[{"commit": "https://github.com/apache/hive/commit/463dae9ee8f694002af492e7d05924423aeaed09", "parent": "https://github.com/apache/hive/commit/d5ca6f6b83ac8e333ad5a621c3b5609b1c56119c", "message": "HIVE-22391: NPE while checking Hive query results cache (Jason Dere, reviewed by gopalv)", "bug_id": "hive_1", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/463dae9ee8f694002af492e7d05924423aeaed09/ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java", "blob_url": "https://github.com/apache/hive/blob/463dae9ee8f694002af492e7d05924423aeaed09/ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java", "sha": "54c24251352aeb513ebb34a2374288d602da3c1f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java?ref=463dae9ee8f694002af492e7d05924423aeaed09", "patch": "@@ -401,7 +401,7 @@ public static void initialize(HiveConf conf) throws IOException {\n         if (metrics != null) {\n           registerMetrics(metrics, instance);\n         }\n-      } catch (IOException err) {\n+      } catch (Exception err) {\n         inited.set(false);\n         throw err;\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/cache/results/QueryResultsCache.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a832b403873d7ffd73dccb90ad92762614be9482", "parent": "https://github.com/apache/hive/commit/aacc83010b1c9dc3c5b68e6d75503bb72066456b", "message": "HIVE-22232: NPE when hive.order.columnalignment is set to false (Jesus Camacho Rodriguez, reviewed by Vineet Garg)\n\nClose apache/hive#783", "bug_id": "hive_2", "file": [{"additions": 21, "raw_url": "https://github.com/apache/hive/raw/a832b403873d7ffd73dccb90ad92762614be9482/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java", "blob_url": "https://github.com/apache/hive/blob/a832b403873d7ffd73dccb90ad92762614be9482/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java", "sha": "6c4edeb905044e2babdf601897047fac05326a00", "changes": 31, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java?ref=a832b403873d7ffd73dccb90ad92762614be9482", "patch": "@@ -160,16 +160,20 @@ private ASTNode convert() throws CalciteSemanticException {\n       }\n \n       HiveAggregate hiveAgg = (HiveAggregate) groupBy;\n-      for (int pos : hiveAgg.getAggregateColumnsOrder()) {\n-        RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n-            groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n-        b.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n-      }\n-      for (int pos = 0; pos < groupBy.getGroupCount(); pos++) {\n-        if (!hiveAgg.getAggregateColumnsOrder().contains(pos)) {\n-          RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n-              groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n-          b.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n+      if (hiveAgg.getAggregateColumnsOrder() != null) {\n+        // Aggregation columns may have been sorted in specific order\n+        for (int pos : hiveAgg.getAggregateColumnsOrder()) {\n+          addRefToBuilder(b, groupBy.getGroupSet().nth(pos));\n+        }\n+        for (int pos = 0; pos < groupBy.getGroupCount(); pos++) {\n+          if (!hiveAgg.getAggregateColumnsOrder().contains(pos)) {\n+            addRefToBuilder(b, groupBy.getGroupSet().nth(pos));\n+          }\n+        }\n+      } else {\n+        // Aggregation columns have not been reordered\n+        for (int i : groupBy.getGroupSet()) {\n+          addRefToBuilder(b, i);\n         }\n       }\n \n@@ -257,6 +261,12 @@ private ASTNode convert() throws CalciteSemanticException {\n     return hiveAST.getAST();\n   }\n \n+  private void addRefToBuilder(ASTBuilder b, int i) {\n+    RexInputRef iRef = new RexInputRef(i,\n+        root.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n+    b.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n+  }\n+\n   private ASTNode buildUDTFAST(String functionName, List<ASTNode> children) {\n     ASTNode node = (ASTNode) ParseDriver.adaptor.create(HiveParser.TOK_FUNCTION, \"TOK_FUNCTION\");\n     node.addChild((ASTNode) ParseDriver.adaptor.create(HiveParser.Identifier, functionName));\n@@ -265,6 +275,7 @@ private ASTNode buildUDTFAST(String functionName, List<ASTNode> children) {\n     }\n     return node;\n   }\n+\n   private void convertOrderLimitToASTNode(HiveSortLimit order) {\n     if (order != null) {\n       HiveSortLimit hiveSortLimit = order;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/a832b403873d7ffd73dccb90ad92762614be9482/ql/src/test/queries/clientpositive/groupby_nocolumnalign.q", "blob_url": "https://github.com/apache/hive/blob/a832b403873d7ffd73dccb90ad92762614be9482/ql/src/test/queries/clientpositive/groupby_nocolumnalign.q", "sha": "8138b3c30b1cd0c0554706e72e972636cc20c4f7", "changes": 17, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/groupby_nocolumnalign.q?ref=a832b403873d7ffd73dccb90ad92762614be9482", "patch": "@@ -0,0 +1,17 @@\n+--! qt:dataset:src\n+-- SORT_QUERY_RESULTS\n+\n+set hive.order.columnalignment=false;\n+\n+CREATE TABLE DEST2_n31_2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE;\n+\n+EXPLAIN\n+FROM SRC\n+INSERT OVERWRITE TABLE DEST2_n31_2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;\n+\n+FROM SRC\n+INSERT OVERWRITE TABLE DEST2_n31_2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;\n+\n+SELECT DEST2_n31_2.* FROM DEST2_n31_2;\n+\n+DROP TABLE DEST2_n31_2;", "filename": "ql/src/test/queries/clientpositive/groupby_nocolumnalign.q"}, {"additions": 464, "raw_url": "https://github.com/apache/hive/raw/a832b403873d7ffd73dccb90ad92762614be9482/ql/src/test/results/clientpositive/groupby_nocolumnalign.q.out", "blob_url": "https://github.com/apache/hive/blob/a832b403873d7ffd73dccb90ad92762614be9482/ql/src/test/results/clientpositive/groupby_nocolumnalign.q.out", "sha": "eefc9b1d74fba7967688bc40ba60de2dbbc6c6d5", "changes": 464, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/groupby_nocolumnalign.q.out?ref=a832b403873d7ffd73dccb90ad92762614be9482", "patch": "@@ -0,0 +1,464 @@\n+PREHOOK: query: CREATE TABLE DEST2_n31_2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@DEST2_n31_2\n+POSTHOOK: query: CREATE TABLE DEST2_n31_2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@DEST2_n31_2\n+PREHOOK: query: EXPLAIN\n+FROM SRC\n+INSERT OVERWRITE TABLE DEST2_n31_2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@dest2_n31_2\n+POSTHOOK: query: EXPLAIN\n+FROM SRC\n+INSERT OVERWRITE TABLE DEST2_n31_2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@dest2_n31_2\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+  Stage-2 depends on stages: Stage-0, Stage-3\n+  Stage-3 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: src\n+            Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE\n+            Select Operator\n+              expressions: key (type: string), value (type: string), substr(value, 5) (type: string)\n+              outputColumnNames: _col0, _col1, _col2\n+              Statistics: Num rows: 500 Data size: 89000 Basic stats: COMPLETE Column stats: COMPLETE\n+              Group By Operator\n+                aggregations: count(DISTINCT _col2)\n+                keys: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+                minReductionHashAggr: 0.99\n+                mode: hash\n+                outputColumnNames: _col0, _col1, _col2, _col3\n+                Statistics: Num rows: 500 Data size: 185000 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+                  sort order: +++\n+                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string)\n+                  Statistics: Num rows: 500 Data size: 185000 Basic stats: COMPLETE Column stats: COMPLETE\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(DISTINCT KEY._col2:0._col0)\n+          keys: KEY._col0 (type: string), KEY._col1 (type: string)\n+          mode: mergepartial\n+          outputColumnNames: _col0, _col1, _col2\n+          Statistics: Num rows: 500 Data size: 93000 Basic stats: COMPLETE Column stats: COMPLETE\n+          Select Operator\n+            expressions: UDFToInteger(_col0) (type: int), _col1 (type: string), CAST( _col2 AS STRING) (type: string)\n+            outputColumnNames: _col0, _col1, _col2\n+            Statistics: Num rows: 500 Data size: 139500 Basic stats: COMPLETE Column stats: COMPLETE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 500 Data size: 139500 Basic stats: COMPLETE Column stats: COMPLETE\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  name: default.dest2_n31_2\n+            Select Operator\n+              expressions: _col0 (type: int), _col1 (type: string), _col2 (type: string)\n+              outputColumnNames: key, val1, val2\n+              Statistics: Num rows: 500 Data size: 139500 Basic stats: COMPLETE Column stats: COMPLETE\n+              Group By Operator\n+                aggregations: compute_stats(key, 'hll'), compute_stats(val1, 'hll'), compute_stats(val2, 'hll')\n+                minReductionHashAggr: 0.99\n+                mode: hash\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 1 Data size: 1304 Basic stats: COMPLETE Column stats: COMPLETE\n+                File Output Operator\n+                  compressed: false\n+                  table:\n+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe\n+\n+  Stage: Stage-0\n+    Move Operator\n+      tables:\n+          replace: true\n+          table:\n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.dest2_n31_2\n+\n+  Stage: Stage-2\n+    Stats Work\n+      Basic Stats Work:\n+      Column Stats Desc:\n+          Columns: key, val1, val2\n+          Column Types: int, string, string\n+          Table: default.dest2_n31_2\n+\n+  Stage: Stage-3\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            Reduce Output Operator\n+              sort order: \n+              Statistics: Num rows: 1 Data size: 1304 Basic stats: COMPLETE Column stats: COMPLETE\n+              value expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:binary>), _col1 (type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:binary>), _col2 (type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:binary>)\n+      Execution mode: vectorized\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1), compute_stats(VALUE._col2)\n+          mode: mergepartial\n+          outputColumnNames: _col0, _col1, _col2\n+          Statistics: Num rows: 1 Data size: 1320 Basic stats: COMPLETE Column stats: COMPLETE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 1320 Basic stats: COMPLETE Column stats: COMPLETE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+PREHOOK: query: FROM SRC\n+INSERT OVERWRITE TABLE DEST2_n31_2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@dest2_n31_2\n+POSTHOOK: query: FROM SRC\n+INSERT OVERWRITE TABLE DEST2_n31_2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@dest2_n31_2\n+POSTHOOK: Lineage: dest2_n31_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: dest2_n31_2.val1 SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: dest2_n31_2.val2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: SELECT DEST2_n31_2.* FROM DEST2_n31_2\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@dest2_n31_2\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT DEST2_n31_2.* FROM DEST2_n31_2\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@dest2_n31_2\n+#### A masked pattern was here ####\n+0\tval_0\t1\n+10\tval_10\t1\n+100\tval_100\t1\n+103\tval_103\t1\n+104\tval_104\t1\n+105\tval_105\t1\n+11\tval_11\t1\n+111\tval_111\t1\n+113\tval_113\t1\n+114\tval_114\t1\n+116\tval_116\t1\n+118\tval_118\t1\n+119\tval_119\t1\n+12\tval_12\t1\n+120\tval_120\t1\n+125\tval_125\t1\n+126\tval_126\t1\n+128\tval_128\t1\n+129\tval_129\t1\n+131\tval_131\t1\n+133\tval_133\t1\n+134\tval_134\t1\n+136\tval_136\t1\n+137\tval_137\t1\n+138\tval_138\t1\n+143\tval_143\t1\n+145\tval_145\t1\n+146\tval_146\t1\n+149\tval_149\t1\n+15\tval_15\t1\n+150\tval_150\t1\n+152\tval_152\t1\n+153\tval_153\t1\n+155\tval_155\t1\n+156\tval_156\t1\n+157\tval_157\t1\n+158\tval_158\t1\n+160\tval_160\t1\n+162\tval_162\t1\n+163\tval_163\t1\n+164\tval_164\t1\n+165\tval_165\t1\n+166\tval_166\t1\n+167\tval_167\t1\n+168\tval_168\t1\n+169\tval_169\t1\n+17\tval_17\t1\n+170\tval_170\t1\n+172\tval_172\t1\n+174\tval_174\t1\n+175\tval_175\t1\n+176\tval_176\t1\n+177\tval_177\t1\n+178\tval_178\t1\n+179\tval_179\t1\n+18\tval_18\t1\n+180\tval_180\t1\n+181\tval_181\t1\n+183\tval_183\t1\n+186\tval_186\t1\n+187\tval_187\t1\n+189\tval_189\t1\n+19\tval_19\t1\n+190\tval_190\t1\n+191\tval_191\t1\n+192\tval_192\t1\n+193\tval_193\t1\n+194\tval_194\t1\n+195\tval_195\t1\n+196\tval_196\t1\n+197\tval_197\t1\n+199\tval_199\t1\n+2\tval_2\t1\n+20\tval_20\t1\n+200\tval_200\t1\n+201\tval_201\t1\n+202\tval_202\t1\n+203\tval_203\t1\n+205\tval_205\t1\n+207\tval_207\t1\n+208\tval_208\t1\n+209\tval_209\t1\n+213\tval_213\t1\n+214\tval_214\t1\n+216\tval_216\t1\n+217\tval_217\t1\n+218\tval_218\t1\n+219\tval_219\t1\n+221\tval_221\t1\n+222\tval_222\t1\n+223\tval_223\t1\n+224\tval_224\t1\n+226\tval_226\t1\n+228\tval_228\t1\n+229\tval_229\t1\n+230\tval_230\t1\n+233\tval_233\t1\n+235\tval_235\t1\n+237\tval_237\t1\n+238\tval_238\t1\n+239\tval_239\t1\n+24\tval_24\t1\n+241\tval_241\t1\n+242\tval_242\t1\n+244\tval_244\t1\n+247\tval_247\t1\n+248\tval_248\t1\n+249\tval_249\t1\n+252\tval_252\t1\n+255\tval_255\t1\n+256\tval_256\t1\n+257\tval_257\t1\n+258\tval_258\t1\n+26\tval_26\t1\n+260\tval_260\t1\n+262\tval_262\t1\n+263\tval_263\t1\n+265\tval_265\t1\n+266\tval_266\t1\n+27\tval_27\t1\n+272\tval_272\t1\n+273\tval_273\t1\n+274\tval_274\t1\n+275\tval_275\t1\n+277\tval_277\t1\n+278\tval_278\t1\n+28\tval_28\t1\n+280\tval_280\t1\n+281\tval_281\t1\n+282\tval_282\t1\n+283\tval_283\t1\n+284\tval_284\t1\n+285\tval_285\t1\n+286\tval_286\t1\n+287\tval_287\t1\n+288\tval_288\t1\n+289\tval_289\t1\n+291\tval_291\t1\n+292\tval_292\t1\n+296\tval_296\t1\n+298\tval_298\t1\n+30\tval_30\t1\n+302\tval_302\t1\n+305\tval_305\t1\n+306\tval_306\t1\n+307\tval_307\t1\n+308\tval_308\t1\n+309\tval_309\t1\n+310\tval_310\t1\n+311\tval_311\t1\n+315\tval_315\t1\n+316\tval_316\t1\n+317\tval_317\t1\n+318\tval_318\t1\n+321\tval_321\t1\n+322\tval_322\t1\n+323\tval_323\t1\n+325\tval_325\t1\n+327\tval_327\t1\n+33\tval_33\t1\n+331\tval_331\t1\n+332\tval_332\t1\n+333\tval_333\t1\n+335\tval_335\t1\n+336\tval_336\t1\n+338\tval_338\t1\n+339\tval_339\t1\n+34\tval_34\t1\n+341\tval_341\t1\n+342\tval_342\t1\n+344\tval_344\t1\n+345\tval_345\t1\n+348\tval_348\t1\n+35\tval_35\t1\n+351\tval_351\t1\n+353\tval_353\t1\n+356\tval_356\t1\n+360\tval_360\t1\n+362\tval_362\t1\n+364\tval_364\t1\n+365\tval_365\t1\n+366\tval_366\t1\n+367\tval_367\t1\n+368\tval_368\t1\n+369\tval_369\t1\n+37\tval_37\t1\n+373\tval_373\t1\n+374\tval_374\t1\n+375\tval_375\t1\n+377\tval_377\t1\n+378\tval_378\t1\n+379\tval_379\t1\n+382\tval_382\t1\n+384\tval_384\t1\n+386\tval_386\t1\n+389\tval_389\t1\n+392\tval_392\t1\n+393\tval_393\t1\n+394\tval_394\t1\n+395\tval_395\t1\n+396\tval_396\t1\n+397\tval_397\t1\n+399\tval_399\t1\n+4\tval_4\t1\n+400\tval_400\t1\n+401\tval_401\t1\n+402\tval_402\t1\n+403\tval_403\t1\n+404\tval_404\t1\n+406\tval_406\t1\n+407\tval_407\t1\n+409\tval_409\t1\n+41\tval_41\t1\n+411\tval_411\t1\n+413\tval_413\t1\n+414\tval_414\t1\n+417\tval_417\t1\n+418\tval_418\t1\n+419\tval_419\t1\n+42\tval_42\t1\n+421\tval_421\t1\n+424\tval_424\t1\n+427\tval_427\t1\n+429\tval_429\t1\n+43\tval_43\t1\n+430\tval_430\t1\n+431\tval_431\t1\n+432\tval_432\t1\n+435\tval_435\t1\n+436\tval_436\t1\n+437\tval_437\t1\n+438\tval_438\t1\n+439\tval_439\t1\n+44\tval_44\t1\n+443\tval_443\t1\n+444\tval_444\t1\n+446\tval_446\t1\n+448\tval_448\t1\n+449\tval_449\t1\n+452\tval_452\t1\n+453\tval_453\t1\n+454\tval_454\t1\n+455\tval_455\t1\n+457\tval_457\t1\n+458\tval_458\t1\n+459\tval_459\t1\n+460\tval_460\t1\n+462\tval_462\t1\n+463\tval_463\t1\n+466\tval_466\t1\n+467\tval_467\t1\n+468\tval_468\t1\n+469\tval_469\t1\n+47\tval_47\t1\n+470\tval_470\t1\n+472\tval_472\t1\n+475\tval_475\t1\n+477\tval_477\t1\n+478\tval_478\t1\n+479\tval_479\t1\n+480\tval_480\t1\n+481\tval_481\t1\n+482\tval_482\t1\n+483\tval_483\t1\n+484\tval_484\t1\n+485\tval_485\t1\n+487\tval_487\t1\n+489\tval_489\t1\n+490\tval_490\t1\n+491\tval_491\t1\n+492\tval_492\t1\n+493\tval_493\t1\n+494\tval_494\t1\n+495\tval_495\t1\n+496\tval_496\t1\n+497\tval_497\t1\n+498\tval_498\t1\n+5\tval_5\t1\n+51\tval_51\t1\n+53\tval_53\t1\n+54\tval_54\t1\n+57\tval_57\t1\n+58\tval_58\t1\n+64\tval_64\t1\n+65\tval_65\t1\n+66\tval_66\t1\n+67\tval_67\t1\n+69\tval_69\t1\n+70\tval_70\t1\n+72\tval_72\t1\n+74\tval_74\t1\n+76\tval_76\t1\n+77\tval_77\t1\n+78\tval_78\t1\n+8\tval_8\t1\n+80\tval_80\t1\n+82\tval_82\t1\n+83\tval_83\t1\n+84\tval_84\t1\n+85\tval_85\t1\n+86\tval_86\t1\n+87\tval_87\t1\n+9\tval_9\t1\n+90\tval_90\t1\n+92\tval_92\t1\n+95\tval_95\t1\n+96\tval_96\t1\n+97\tval_97\t1\n+98\tval_98\t1\n+PREHOOK: query: DROP TABLE DEST2_n31_2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@dest2_n31_2\n+PREHOOK: Output: default@dest2_n31_2\n+POSTHOOK: query: DROP TABLE DEST2_n31_2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@dest2_n31_2\n+POSTHOOK: Output: default@dest2_n31_2", "filename": "ql/src/test/results/clientpositive/groupby_nocolumnalign.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/96f7960c790fadb6bbc9d81fe095288241d86d8c", "parent": "https://github.com/apache/hive/commit/7fc5a88a149cf0767a5846cbb6ace22d8e99a63c", "message": "HIVE-16587: NPE when inserting complex types with nested null values (Naresh P R, reviewed by Sankar Hariappan)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>", "bug_id": "hive_3", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/itests/src/test/resources/testconfiguration.properties", "sha": "84c2042676332b307afc6a34f7bbc7a2279692ba", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c", "patch": "@@ -42,7 +42,8 @@ minitez.query.files=acid_vectorization_original_tez.q,\\\n   multi_count_distinct.q,\\\n   tez-tag.q,\\\n   tez_union_with_udf.q,\\\n-  tez_union_udtf.q\n+  tez_union_udtf.q,\\\n+  tez_complextype_with_null.q\n \n \n minillap.shared.query.files=insert_into1.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "sha": "1795ae562603846a2562857ec30f8ad44d331dcc", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c", "patch": "@@ -1345,11 +1345,11 @@ public static long getWritableSize(ObjectInspector oi, Object value) {\n     if (oi instanceof WritableStringObjectInspector) {\n       WritableStringObjectInspector woi = (WritableStringObjectInspector) oi;\n       return JavaDataModel.get().lengthForStringOfLength(\n-          woi.getPrimitiveWritableObject(value).getLength());\n+        value == null ? 0 : woi.getPrimitiveWritableObject(value).getLength());\n     } else if (oi instanceof WritableBinaryObjectInspector) {\n       WritableBinaryObjectInspector woi = (WritableBinaryObjectInspector) oi;\n       return JavaDataModel.get().lengthForByteArrayOfSize(\n-          woi.getPrimitiveWritableObject(value).getLength());\n+        value == null ? 0 : woi.getPrimitiveWritableObject(value).getLength());\n     } else if (oi instanceof WritableBooleanObjectInspector) {\n       return JavaDataModel.get().primitive1();\n     } else if (oi instanceof WritableByteObjectInspector) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/queries/clientpositive/tez_complextype_with_null.q", "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/queries/clientpositive/tez_complextype_with_null.q", "sha": "7a0f240cdb29fa1c61abb95951bbef919baa2207", "changes": 15, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_complextype_with_null.q?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c", "patch": "@@ -0,0 +1,15 @@\n+CREATE TABLE complex1 (\n+    c0 int,\n+    c1 array<int>,\n+    c2 map<int, string>,\n+    c3 struct<f1:int,f2:string,f3:array<int>>,\n+    c4 array<struct<f1:int,f2:string,f3:array<int>>>);\n+\n+INSERT INTO complex1\n+    SELECT 3,\n+       array(1, 2, null),\n+       map(1, 'one', 2, null),\n+       named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1, 2, null)),\n+       array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2, 3, 4)));\n+\n+select * from complex1;", "filename": "ql/src/test/queries/clientpositive/tez_complextype_with_null.q"}, {"additions": 50, "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out", "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out", "sha": "f20151d746c17b28be1f9dffd97d6bca12913d08", "changes": 50, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c", "patch": "@@ -0,0 +1,50 @@\n+PREHOOK: query: CREATE TABLE complex1 (\n+    c0 int,\n+    c1 array<int>,\n+    c2 map<int, string>,\n+    c3 struct<f1:int,f2:string,f3:array<int>>,\n+    c4 array<struct<f1:int,f2:string,f3:array<int>>>)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@complex1\n+POSTHOOK: query: CREATE TABLE complex1 (\n+    c0 int,\n+    c1 array<int>,\n+    c2 map<int, string>,\n+    c3 struct<f1:int,f2:string,f3:array<int>>,\n+    c4 array<struct<f1:int,f2:string,f3:array<int>>>)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@complex1\n+PREHOOK: query: INSERT INTO complex1\n+    SELECT 3,\n+       array(1, 2, null),\n+       map(1, 'one', 2, null),\n+       named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1, 2, null)),\n+       array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2, 3, 4)))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: default@complex1\n+POSTHOOK: query: INSERT INTO complex1\n+    SELECT 3,\n+       array(1, 2, null),\n+       map(1, 'one', 2, null),\n+       named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1, 2, null)),\n+       array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2, 3, 4)))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: default@complex1\n+POSTHOOK: Lineage: complex1.c0 SIMPLE []\n+POSTHOOK: Lineage: complex1.c1 EXPRESSION []\n+POSTHOOK: Lineage: complex1.c2 EXPRESSION []\n+POSTHOOK: Lineage: complex1.c3 EXPRESSION []\n+POSTHOOK: Lineage: complex1.c4 EXPRESSION []\n+PREHOOK: query: select * from complex1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@complex1\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: select * from complex1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@complex1\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+3\t[1,2,null]\t{1:\"one\",2:null}\t{\"f1\":null,\"f2\":null,\"f3\":[1,2,null]}\t[{\"f1\":11,\"f2\":\"two\",\"f3\":[2,3,4]}]", "filename": "ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8412b3748b7b384689a39b8747c7f8fd41e58f28", "parent": "https://github.com/apache/hive/commit/198ab0e862cdf33e8dff37bf24676ffcb392ed82", "message": "HIVE-21992: REPL DUMP throws NPE when dumping Create Function event (Sankar Hariappan, reviewed by Mahesh Kumar Behera)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>", "bug_id": "hive_4", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/8412b3748b7b384689a39b8747c7f8fd41e58f28/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java", "blob_url": "https://github.com/apache/hive/blob/8412b3748b7b384689a39b8747c7f8fd41e58f28/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java", "sha": "af5746ff48943d6e5808a861cc56ea2cf810432b", "changes": 10, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java?ref=8412b3748b7b384689a39b8747c7f8fd41e58f28", "patch": "@@ -84,22 +84,26 @@ public void testCreateFunctionIncrementalReplication() throws Throwable {\n \n     primary.run(\"CREATE FUNCTION \" + primaryDbName\n         + \".testFunctionOne as 'hivemall.tools.string.StopwordUDF' \"\n-        + \"using jar  'ivy://io.github.myui:hivemall:0.4.0-2'\");\n+        + \"using jar  'ivy://io.github.myui:hivemall:0.4.0-2'\")\n+        .run(\"CREATE FUNCTION \" + primaryDbName\n+            + \".testFunctionTwo as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax'\");\n \n     WarehouseInstance.Tuple incrementalDump =\n         primary.dump(primaryDbName, bootStrapDump.lastReplicationId);\n     replica.load(replicatedDbName, incrementalDump.dumpLocation)\n         .run(\"REPL STATUS \" + replicatedDbName)\n         .verifyResult(incrementalDump.lastReplicationId)\n         .run(\"SHOW FUNCTIONS LIKE '\" + replicatedDbName + \"%'\")\n-        .verifyResult(replicatedDbName + \".testFunctionOne\");\n+        .verifyResults(new String[] { replicatedDbName + \".testFunctionOne\",\n+                                      replicatedDbName + \".testFunctionTwo\" });\n \n     // Test the idempotent behavior of CREATE FUNCTION\n     replica.load(replicatedDbName, incrementalDump.dumpLocation)\n         .run(\"REPL STATUS \" + replicatedDbName)\n         .verifyResult(incrementalDump.lastReplicationId)\n         .run(\"SHOW FUNCTIONS LIKE '\" + replicatedDbName + \"%'\")\n-        .verifyResult(replicatedDbName + \".testFunctionOne\");\n+        .verifyResults(new String[] { replicatedDbName + \".testFunctionOne\",\n+                                      replicatedDbName + \".testFunctionTwo\" });\n   }\n \n   @Test", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java", "blob_url": "https://github.com/apache/hive/blob/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java", "sha": "576eb0699a8aa263f2a4c083a09e5bfef6f3fffc", "changes": 26, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java?ref=8412b3748b7b384689a39b8747c7f8fd41e58f28", "patch": "@@ -51,18 +51,20 @@ public void writeTo(JsonWriter writer, ReplicationSpec additionalPropertiesProvi\n       throws SemanticException, IOException, MetaException {\n     TSerializer serializer = new TSerializer(new TJSONProtocol.Factory());\n     List<ResourceUri> resourceUris = new ArrayList<>();\n-    for (ResourceUri uri : function.getResourceUris()) {\n-      Path inputPath = new Path(uri.getUri());\n-      if (\"hdfs\".equals(inputPath.toUri().getScheme())) {\n-        FileSystem fileSystem = inputPath.getFileSystem(hiveConf);\n-        Path qualifiedUri = PathBuilder.fullyQualifiedHDFSUri(inputPath, fileSystem);\n-        // Initialize ReplChangeManager instance since we will require it to encode file URI.\n-        ReplChangeManager.getInstance(hiveConf);\n-        String checkSum = ReplChangeManager.checksumFor(qualifiedUri, fileSystem);\n-        String newFileUri = ReplChangeManager.encodeFileUri(qualifiedUri.toString(), checkSum, null);\n-        resourceUris.add(new ResourceUri(uri.getResourceType(), newFileUri));\n-      } else {\n-        resourceUris.add(uri);\n+    if (function.getResourceUris() != null) {\n+      for (ResourceUri uri : function.getResourceUris()) {\n+        Path inputPath = new Path(uri.getUri());\n+        if (\"hdfs\".equals(inputPath.toUri().getScheme())) {\n+          FileSystem fileSystem = inputPath.getFileSystem(hiveConf);\n+          Path qualifiedUri = PathBuilder.fullyQualifiedHDFSUri(inputPath, fileSystem);\n+          // Initialize ReplChangeManager instance since we will require it to encode file URI.\n+          ReplChangeManager.getInstance(hiveConf);\n+          String checkSum = ReplChangeManager.checksumFor(qualifiedUri, fileSystem);\n+          String newFileUri = ReplChangeManager.encodeFileUri(qualifiedUri.toString(), checkSum, null);\n+          resourceUris.add(new ResourceUri(uri.getResourceType(), newFileUri));\n+        } else {\n+          resourceUris.add(uri);\n+        }\n       }\n     }\n     Function copyObj = new Function(this.function);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java", "blob_url": "https://github.com/apache/hive/blob/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java", "sha": "bc891f710a8bbfda05b9e80e2a3bfc5e94590e0d", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java?ref=8412b3748b7b384689a39b8747c7f8fd41e58f28", "patch": "@@ -129,9 +129,9 @@ private CreateFunctionDesc build() throws SemanticException {\n       // and not do them lazily. The reason being the function class used for transformations additionally\n       // also creates the corresponding replCopyTasks, which cannot be evaluated lazily. since the query\n       // plan needs to be complete before we execute and not modify it while execution in the driver.\n-      List<ResourceUri> transformedUris = ImmutableList.copyOf(\n-          Lists.transform(metadata.function.getResourceUris(), conversionFunction)\n-      );\n+      List<ResourceUri> transformedUris = (metadata.function.getResourceUris() == null)\n+              ? null\n+              : ImmutableList.copyOf(Lists.transform(metadata.function.getResourceUris(), conversionFunction));\n       replCopyTasks.addAll(conversionFunction.replCopyTasks);\n       String fullQualifiedFunctionName = FunctionUtils.qualifyFunctionName(\n           metadata.function.getFunctionName(), destinationDbName", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "parent": "https://github.com/apache/hive/commit/ec3779978797051fdb345172536aafcd50f1b4ae", "message": "HIVE-21844 : HMS schema Upgrade Script is failing with NPE. (Mahesh Kumar Behera reviewed by  Sankar Hariappan)", "bug_id": "hive_5", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/ColorBuffer.java", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/ColorBuffer.java", "sha": "1730d492d77523dc80a937559dcf93ef707449ac", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/ColorBuffer.java?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -78,6 +78,9 @@ ColorBuffer pad(ColorBuffer str, int len) {\n   }\n \n   ColorBuffer center(String str, int len) {\n+    if (str == null) {\n+      str = \"\";\n+    }\n     StringBuilder buf = new StringBuilder(str);\n     while (buf.length() < len) {\n       buf.append(\" \");", "filename": "beeline/src/java/org/apache/hive/beeline/ColorBuffer.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/Commands.java", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/Commands.java", "sha": "fd0af2ca0c963d0dd2f3ea35c1dc45a179122eb2", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/Commands.java?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1052,7 +1052,9 @@ private boolean executeInternal(String sql, boolean call) {\n             logThread.interrupt();\n           }\n           logThread.join(DEFAULT_QUERY_PROGRESS_THREAD_TIMEOUT);\n-          showRemainingLogsIfAny(stmnt);\n+          if (stmnt != null) {\n+            showRemainingLogsIfAny(stmnt);\n+          }\n         }\n         if (stmnt != null) {\n           stmnt.close();", "filename": "beeline/src/java/org/apache/hive/beeline/Commands.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql", "sha": "20ddd1ada6858a4cccedd26d5f9bbf355fa7a123", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 1.2.0 to 2.0.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 1.2.0 to 2.0.0' AS MESSAGE;\n --SOURCE 021-HIVE-7018.mysql.sql;\n ALTER TABLE `TBLS` DROP FOREIGN KEY `TBLS_FK3`;\n ALTER TABLE `TBLS` DROP KEY `TBLS_N51`;\n@@ -71,5 +71,5 @@ CREATE TABLE AUX_TABLE (\n \n \n UPDATE VERSION SET SCHEMA_VERSION='2.0.0', VERSION_COMMENT='Hive release version 2.0.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 1.2.0 to 2.0.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 1.2.0 to 2.0.0' AS MESSAGE;\n ", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql", "sha": "22a3c377b77081891967df50db29837e649ff7cb", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 2.0.0 to 2.1.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.0.0 to 2.1.0' AS MESSAGE;\n \n --SOURCE 034-HIVE-13076.mysql.sql;\n CREATE TABLE IF NOT EXISTS `KEY_CONSTRAINTS`\n@@ -38,5 +38,5 @@ ALTER TABLE COMPACTION_QUEUE ADD CQ_TBLPROPERTIES varchar(2048);\n ALTER TABLE COMPLETED_COMPACTIONS ADD CC_TBLPROPERTIES varchar(2048);\n \n UPDATE VERSION SET SCHEMA_VERSION='2.1.0', VERSION_COMMENT='Hive release version 2.1.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.0.0 to 2.1.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.0.0 to 2.1.0' AS MESSAGE;\n ", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql", "sha": "3346fe0bc2b3e0967531cdb34702fb152edbbdef", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 2.1.0 to 2.2.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.1.0 to 2.2.0' AS MESSAGE;\n \n --SOURCE 037-HIVE-14496.mysql.sql;\n -- Step 1: Add the column allowing null\n@@ -39,5 +39,5 @@ ALTER TABLE TAB_COL_STATS MODIFY COLUMN_NAME varchar(767) CHARACTER SET latin1 C\n ALTER TABLE PART_COL_STATS MODIFY COLUMN_NAME varchar(767) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL;\n \n UPDATE VERSION SET SCHEMA_VERSION='2.2.0', VERSION_COMMENT='Hive release version 2.2.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.1.0 to 2.2.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.1.0 to 2.2.0' AS MESSAGE;\n ", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql", "sha": "37e817b7cfd41ebcf566b338f85ede77ac9bc63c", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,8 +1,8 @@\n-SELECT 'Upgrading MetaStore schema from 2.2.0 to 2.3.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.2.0 to 2.3.0' AS MESSAGE;\n \n --SOURCE 040-HIVE-16399.mysql.sql;\n CREATE INDEX TC_TXNID_INDEX ON TXN_COMPONENTS (TC_TXNID);\n \n UPDATE VERSION SET SCHEMA_VERSION='2.3.0', VERSION_COMMENT='Hive release version 2.3.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.2.0 to 2.3.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.2.0 to 2.3.0' AS MESSAGE;\n ", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql", "sha": "7140c2af173d8feef8502e9ae75a296d3e9e1552", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 2.3.0 to 3.0.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.3.0 to 3.0.0' AS MESSAGE;\n \n --SOURCE 041-HIVE-16556.mysql.sql;\n --\n@@ -323,4 +323,4 @@ ALTER TABLE `TBLS` ADD COLUMN `OWNER_TYPE` VARCHAR(10) CHARACTER SET latin1 COLL\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='3.0.0', VERSION_COMMENT='Hive release version 3.0.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.3.0 to 3.0.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.3.0 to 3.0.0' AS MESSAGE;", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql", "sha": "3eae9f2850e6d35576f7717a34a259f2b86d3ecf", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 3.0.0 to 3.1.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 3.0.0 to 3.1.0' AS MESSAGE;\n   \n -- HIVE-19440\n ALTER TABLE `GLOBAL_PRIVS` ADD `AUTHORIZER` varchar(128) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL;\n@@ -53,4 +53,4 @@ CREATE TABLE MATERIALIZATION_REBUILD_LOCKS (\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='3.1.0', VERSION_COMMENT='Hive release version 3.1.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 3.0.0 to 3.1.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 3.0.0 to 3.1.0' AS MESSAGE;", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql", "sha": "ebfb90c51e51101cdd2802eb6b06f87c6300d812", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 3.1.0 to 3.2.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 3.1.0 to 3.2.0' AS MESSAGE;\n \n -- HIVE-19267\n CREATE TABLE TXN_WRITE_NOTIFICATION_LOG (\n@@ -25,5 +25,5 @@ ALTER TABLE `CTLGS` ADD `CREATE_TIME` INT(11);\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='3.2.0', VERSION_COMMENT='Hive release version 3.2.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 3.1.0 to 3.2.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 3.1.0 to 3.2.0' AS MESSAGE;\n ", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql", "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql", "sha": "296cb12ef080043855d627e5e76d9f8d160754bb", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a", "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 3.2.0 to 4.0.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 3.2.0 to 4.0.0' AS MESSAGE;\n \n -- HIVE-19416\n ALTER TABLE TBLS ADD WRITE_ID bigint DEFAULT 0;\n@@ -19,5 +19,5 @@ ALTER TABLE COLUMNS_V2 MODIFY COMMENT varchar(4000) CHARACTER SET latin1 COLLATE\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='4.0.0', VERSION_COMMENT='Hive release version 4.0.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 3.2.0 to 4.0.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 3.2.0 to 4.0.0' AS MESSAGE;\n ", "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ae82715f1014c4ed514441311b61ed1891e2a12b", "parent": "https://github.com/apache/hive/commit/0f8119fe797c5b596d22ec4eaaef8aeeb501ccae", "message": "HIVE-21669: HS2 throws NPE when HiveStatement.getQueryId is invoked and query is closed concurrently (Sankar Hariappan, reviewed by Mahesh Kumar Behera)", "bug_id": "hive_6", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/ae82715f1014c4ed514441311b61ed1891e2a12b/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java", "blob_url": "https://github.com/apache/hive/blob/ae82715f1014c4ed514441311b61ed1891e2a12b/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java", "sha": "20e9c3c7e01d2b8300fe92cfca29bf621588801c", "changes": 3, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java?ref=ae82715f1014c4ed514441311b61ed1891e2a12b", "patch": "@@ -1023,9 +1023,6 @@ public String getQueryId() throws SQLException {\n       return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n     } catch (TException e) {\n       throw new SQLException(e);\n-    } catch (Exception e) {\n-      // If concurrently the query is closed before we fetch queryID.\n-      return null;\n     }\n   }\n }", "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/ae82715f1014c4ed514441311b61ed1891e2a12b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java", "blob_url": "https://github.com/apache/hive/blob/ae82715f1014c4ed514441311b61ed1891e2a12b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java", "sha": "48f4fe29ec165424bc5891c6c6980c3e0a85d482", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java?ref=ae82715f1014c4ed514441311b61ed1891e2a12b", "patch": "@@ -854,6 +854,9 @@ public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n       return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n     } catch (HiveSQLException e) {\n       throw new TException(e);\n+    } catch (Exception e) {\n+      // If concurrently the query is closed before we fetch queryID.\n+      return new TGetQueryIdResp((String)null);\n     }\n   }\n ", "filename": "service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/22371f51f365ab609862dc493a86aed17212dac5", "parent": "https://github.com/apache/hive/commit/81117db0e59aa673adc454d0da40d0a146555ae2", "message": "HIVE-21811: Load data into partitioned table throws NPE if DB is enabled for replication (Sankar Hariappan, reviewed by Thejas M Nair)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>", "bug_id": "hive_7", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/22371f51f365ab609862dc493a86aed17212dac5/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java", "blob_url": "https://github.com/apache/hive/blob/22371f51f365ab609862dc493a86aed17212dac5/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java", "sha": "6f47056ca72bb532730a25381a1973be81361ff1", "changes": 12, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java?ref=22371f51f365ab609862dc493a86aed17212dac5", "patch": "@@ -176,6 +176,8 @@ static void internalBeforeClassSetup(Map<String, String> additionalProperties, b\n     hconf.set(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL.varname,\n         \"org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore\");\n     hconf.setBoolVar(HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES, true);\n+    hconf.setBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER, true);\n+    hconf.setBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE, true);\n     System.setProperty(HiveConf.ConfVars.PREEXECHOOKS.varname, \" \");\n     System.setProperty(HiveConf.ConfVars.POSTEXECHOOKS.varname, \" \");\n \n@@ -850,20 +852,20 @@ public void testIncrementalAdds() throws IOException {\n     // Now, we load data into the tables, and see if an incremental\n     // repl drop/load can duplicate it.\n \n-    run(\"LOAD DATA LOCAL INPATH '\" + unptn_locn + \"' OVERWRITE INTO TABLE \" + dbName + \".unptned\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + unptn_locn + \"' OVERWRITE INTO TABLE \" + dbName + \".unptned\", true, driver);\n     verifySetup(\"SELECT * from \" + dbName + \".unptned\", unptn_data, driver);\n     run(\"CREATE TABLE \" + dbName + \".unptned_late AS SELECT * from \" + dbName + \".unptned\", driver);\n     verifySetup(\"SELECT * from \" + dbName + \".unptned_late\", unptn_data, driver);\n \n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=1)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=1)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned WHERE b=1\", ptn_data_1, driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=2)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=2)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned WHERE b=2\", ptn_data_2, driver);\n \n     run(\"CREATE TABLE \" + dbName + \".ptned_late(a string) PARTITIONED BY (b int) STORED AS TEXTFILE\", driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=1)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=1)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned_late WHERE b=1\",ptn_data_1, driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=2)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=2)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned_late WHERE b=2\", ptn_data_2, driver);\n \n     // Perform REPL-DUMP/LOAD", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/22371f51f365ab609862dc493a86aed17212dac5/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java", "blob_url": "https://github.com/apache/hive/blob/22371f51f365ab609862dc493a86aed17212dac5/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java", "sha": "ee0e52879ae4129257e81de4f14cebf3640bb781", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java?ref=22371f51f365ab609862dc493a86aed17212dac5", "patch": "@@ -823,7 +823,7 @@ private void blockPartitionLocationChangesOnReplSource(Database db, Table tbl,\n \n     // Do not allow changing location of a managed table as alter event doesn't capture the\n     // new files list. So, it may cause data inconsistency.\n-    if (ec.isSetProperties()) {\n+    if ((ec != null) && ec.isSetProperties()) {\n       String alterType = ec.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n       if (alterType != null && alterType.equalsIgnoreCase(ALTERLOCATION) &&\n               tbl.getTableType().equalsIgnoreCase(TableType.MANAGED_TABLE.name())) {\n@@ -846,7 +846,7 @@ private void validateTableChangesOnReplSource(Database db, Table oldTbl, Table n\n     // Do not allow changing location of a managed table as alter event doesn't capture the\n     // new files list. So, it may cause data inconsistency. We do this whether or not strict\n     // managed is true on the source cluster.\n-    if (ec.isSetProperties()) {\n+    if ((ec != null) && ec.isSetProperties()) {\n         String alterType = ec.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n         if (alterType != null && alterType.equalsIgnoreCase(ALTERLOCATION) &&\n             oldTbl.getTableType().equalsIgnoreCase(TableType.MANAGED_TABLE.name())) {", "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99", "parent": "https://github.com/apache/hive/commit/793f192de238874d32d4c2f5137e97ebf048cc70", "message": "HIVE-21250 : NPE in HiveProtoLoggingHook for eventPerFile mode. (Harish JP, reviewd by Anishek Agarwal)", "bug_id": "hive_8", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java", "blob_url": "https://github.com/apache/hive/blob/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java", "sha": "ec00ab6d6e9caf8f07169270ae5ba9f306e736e5", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java?ref=5db4c77699ff2adeb30ef2d8ea038cfbd9035d99", "patch": "@@ -305,10 +305,10 @@ private void writeEvent(HiveHookEventProto event) {\n       for (int retryCount = 0; retryCount <= MAX_RETRIES; ++retryCount) {\n         try {\n           if (eventPerFile) {\n-            LOG.debug(\"Event per file enabled. Closing proto event file: {}\", writer.getPath());\n             if (!maybeRolloverWriterForDay()) {\n               writer = logger.getWriter(logFileName + \"_\" + ++logFileCount);\n             }\n+            LOG.debug(\"Event per file enabled. New proto event file: {}\", writer.getPath());\n             writer.writeProto(event);\n             IOUtils.closeQuietly(writer);\n             writer = null;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2", "parent": "https://github.com/apache/hive/commit/12f83719d940034dc8c6273e2772f6b30d07108e", "message": "HIVE-21479: NPE during metastore cache update (Daniel Dai, reviewed by Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>", "bug_id": "hive_9", "file": [{"additions": 18, "raw_url": "https://github.com/apache/hive/raw/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java", "blob_url": "https://github.com/apache/hive/blob/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java", "sha": "3564efe8ed42c1e5a18787a1012d6e1af90e4416", "changes": 31, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java?ref=5708a0b797bf12b4f61afaf0d343ea6bd9b237e2", "patch": "@@ -812,7 +812,7 @@ private void updateTableColStats(RawStore rawStore, String catName, String dbNam\n       rawStore.openTransaction();\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n-        if (!table.isSetPartitionKeys()) {\n+        if (table != null && !table.isSetPartitionKeys()) {\n           List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n           Deadline.startTimer(\"getTableColumnStatistics\");\n \n@@ -856,18 +856,20 @@ private void updateTablePartitionColStats(RawStore rawStore, String catName, Str\n       rawStore.openTransaction();\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n-        List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n-        List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n-        // Get partition column stats for this table\n-        Deadline.startTimer(\"getPartitionColumnStatistics\");\n-        List<ColumnStatistics> partitionColStats =\n-            rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);\n-        Deadline.stopTimer();\n-        sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);\n-        List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);\n-        // Also save partitions for consistency as they have the stats state.\n-        for (Partition part : parts) {\n-          sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);\n+        if (table != null) {\n+          List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n+          List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n+          // Get partition column stats for this table\n+          Deadline.startTimer(\"getPartitionColumnStatistics\");\n+          List<ColumnStatistics> partitionColStats =\n+                  rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);\n+          Deadline.stopTimer();\n+          sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);\n+          List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);\n+          // Also save partitions for consistency as they have the stats state.\n+          for (Partition part : parts) {\n+            sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);\n+          }\n         }\n         committed = rawStore.commitTransaction();\n       } catch (MetaException | NoSuchObjectException e) {\n@@ -886,6 +888,9 @@ private static void updateTableAggregatePartitionColStats(RawStore rawStore, Str\n                                                        String tblName) {\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n+        if (table == null) {\n+          return;\n+        }\n         List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n         List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n         if ((partNames != null) && (partNames.size() > 0)) {", "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/13938db46ff89fc4a425854b4795df604ced8ba9", "parent": "https://github.com/apache/hive/commit/9f2f101feec4283f020b72b918e0961215e76789", "message": "HIVE-21421: HiveStatement.getQueryId throws NPE when query is not running (Sankar Hariappan, reviewed by Mahesh Kumar Behera)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>", "bug_id": "hive_10", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/13938db46ff89fc4a425854b4795df604ced8ba9/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java", "blob_url": "https://github.com/apache/hive/blob/13938db46ff89fc4a425854b4795df604ced8ba9/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java", "sha": "14187cc83bfd6f90496477de828dce773de605ac", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java?ref=13938db46ff89fc4a425854b4795df604ced8ba9", "patch": "@@ -3044,8 +3044,13 @@ private void testInsertOverwrite(HiveStatement stmt) throws SQLException {\n   public void testGetQueryId() throws Exception {\n     HiveStatement stmt = (HiveStatement) con.createStatement();\n     HiveStatement stmt1 = (HiveStatement) con.createStatement();\n-    stmt.executeAsync(\"create database query_id_test with dbproperties ('repl.source.for' = '1, 2, 3')\");\n+\n+    // Returns null if no query is running.\n     String queryId = stmt.getQueryId();\n+    assertTrue(queryId == null);\n+\n+    stmt.executeAsync(\"create database query_id_test with dbproperties ('repl.source.for' = '1, 2, 3')\");\n+    queryId = stmt.getQueryId();\n     assertFalse(queryId.isEmpty());\n     stmt.getUpdateCount();\n ", "filename": "itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/13938db46ff89fc4a425854b4795df604ced8ba9/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java", "blob_url": "https://github.com/apache/hive/blob/13938db46ff89fc4a425854b4795df604ced8ba9/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java", "sha": "d9b625466ca8880c739977f52eabc92ce44aa98d", "changes": 19, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java?ref=13938db46ff89fc4a425854b4795df604ced8ba9", "patch": "@@ -18,8 +18,8 @@\n \n package org.apache.hive.jdbc;\n \n-import com.google.common.annotations.VisibleForTesting;\n import org.apache.commons.codec.binary.Base64;\n+import org.apache.hadoop.hive.common.classification.InterfaceAudience.LimitedPrivate;\n import org.apache.hive.jdbc.logs.InPlaceUpdateStream;\n import org.apache.hive.service.cli.RowSet;\n import org.apache.hive.service.cli.RowSetFactory;\n@@ -37,7 +37,6 @@\n import org.apache.hive.service.rpc.thrift.TGetOperationStatusResp;\n import org.apache.hive.service.rpc.thrift.TGetQueryIdReq;\n import org.apache.hive.service.rpc.thrift.TOperationHandle;\n-import org.apache.hive.service.rpc.thrift.TOperationState;\n import org.apache.hive.service.rpc.thrift.TSessionHandle;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n@@ -1007,12 +1006,26 @@ public void setInPlaceUpdateStream(InPlaceUpdateStream stream) {\n     this.inPlaceUpdateStream = stream;\n   }\n \n-  @VisibleForTesting\n+  /**\n+   * Returns the Query ID if it is running.\n+   * This method is a public API for usage outside of Hive, although it is not part of the\n+   * interface java.sql.Statement.\n+   * @return Valid query ID if it is running else returns NULL.\n+   * @throws SQLException If any internal failures.\n+   */\n+  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n   public String getQueryId() throws SQLException {\n+    if (stmtHandle == null) {\n+      // If query is not running or already closed.\n+      return null;\n+    }\n     try {\n       return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n     } catch (TException e) {\n       throw new SQLException(e);\n+    } catch (Exception e) {\n+      // If concurrently the query is closed before we fetch queryID.\n+      return null;\n     }\n   }\n }", "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/afd7b5b38556f638782606edbe6850ef70e1c8bb", "parent": "https://github.com/apache/hive/commit/6d74222521d2a1333990b9b3577ec9a7f7e619b8", "message": "HIVE-21507: Hive swallows NPE if no delegation token found (Denes Bodo, reviewed by Zoltan Haindrich, Daniel Dai)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>, Daniel Dai <daijyc@gmail.com>", "bug_id": "hive_11", "file": [{"additions": 41, "raw_url": "https://github.com/apache/hive/raw/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java", "blob_url": "https://github.com/apache/hive/blob/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java", "sha": "4c7119f112844ffcc01e7c3dcd628fa2d83684a1", "changes": 62, "status": "modified", "deletions": 21, "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java?ref=afd7b5b38556f638782606edbe6850ef70e1c8bb", "patch": "@@ -760,36 +760,56 @@ SSLConnectionSocketFactory getTwoWaySSLSocketFactory() throws SQLException {\n   }\n \n   // Lookup the delegation token. First in the connection URL, then Configuration\n-  private String getClientDelegationToken(Map<String, String> jdbcConnConf)\n-      throws SQLException {\n+  private String getClientDelegationToken(Map<String, String> jdbcConnConf) throws SQLException {\n     String tokenStr = null;\n-    if (JdbcConnectionParams.AUTH_TOKEN.equalsIgnoreCase(jdbcConnConf.get(JdbcConnectionParams.AUTH_TYPE))) {\n-      // check delegation token in job conf if any\n+    if (!JdbcConnectionParams.AUTH_TOKEN.equalsIgnoreCase(jdbcConnConf.get(JdbcConnectionParams.AUTH_TYPE))) {\n+      return null;\n+    }\n+    DelegationTokenFetcher fetcher = new DelegationTokenFetcher();\n+    try {\n+      tokenStr = fetcher.getTokenStringFromFile();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot get token from environment variable $HADOOP_TOKEN_FILE_LOCATION=\" +\n+              System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION));\n+    }\n+    if (tokenStr == null) {\n       try {\n-        if (System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION) != null) {\n-          try {\n-            Credentials cred = new Credentials();\n-            DataInputStream dis = new DataInputStream(new FileInputStream(System.getenv(UserGroupInformation\n-                    .HADOOP_TOKEN_FILE_LOCATION)));\n-            cred.readTokenStorageStream(dis);\n-            dis.close();\n-            Token<? extends TokenIdentifier> token = cred.getToken(new Text(\"hive\"));\n-            tokenStr = token.encodeToUrlString();\n-          } catch (IOException e) {\n-            LOG.warn(\"Cannot get token from environment variable $HADOOP_TOKEN_FILE_LOCATION=\" +\n-                    System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION));\n-          }\n-        }\n-        if (tokenStr == null) {\n-          tokenStr = SessionUtils.getTokenStrForm(HiveAuthConstants.HS2_CLIENT_TOKEN);\n-        }\n+        return fetcher.getTokenFromSession();\n       } catch (IOException e) {\n         throw new SQLException(\"Error reading token \", e);\n       }\n     }\n     return tokenStr;\n   }\n \n+  static class DelegationTokenFetcher {\n+    String getTokenStringFromFile() throws IOException {\n+      if (System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION) == null) {\n+        return null;\n+      }\n+      Credentials cred = new Credentials();\n+      try (DataInputStream dis = new DataInputStream(new FileInputStream(System.getenv(UserGroupInformation\n+              .HADOOP_TOKEN_FILE_LOCATION)))) {\n+        cred.readTokenStorageStream(dis);\n+      }\n+      return getTokenFromCredential(cred, \"hive\");\n+    }\n+\n+    String getTokenFromCredential(Credentials cred, String key) throws IOException {\n+      Token<? extends TokenIdentifier> token = cred.getToken(new Text(key));\n+      if (token == null) {\n+        LOG.warn(\"Delegation token with key: [hive] cannot be found.\");\n+        return null;\n+      }\n+      return token.encodeToUrlString();\n+    }\n+\n+    String getTokenFromSession() throws IOException {\n+      LOG.debug(\"Fetching delegation token from session.\");\n+      return SessionUtils.getTokenStrForm(HiveAuthConstants.HS2_CLIENT_TOKEN);\n+    }\n+  }\n+\n   private void openSession() throws SQLException {\n     TOpenSessionReq openReq = new TOpenSessionReq();\n ", "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java"}, {"additions": 60, "raw_url": "https://github.com/apache/hive/raw/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java", "blob_url": "https://github.com/apache/hive/blob/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java", "sha": "bcd2608e1ba0101d94574cf3692a3cab66859a01", "changes": 60, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java?ref=afd7b5b38556f638782606edbe6850ef70e1c8bb", "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hive.jdbc;\n+\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.security.Credentials;\n+import org.apache.hadoop.security.token.Token;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+public class TestHiveConnection {\n+\n+  private static final String EXISTING_TOKEN = \"ExistingToken\";\n+  public static final String EXPECTED_TOKEN_STRING_FORM = \"AAAAAA\";\n+  private static HiveConnection.DelegationTokenFetcher fetcher;\n+\n+  @BeforeClass\n+  public static void init() {\n+    fetcher = new HiveConnection.DelegationTokenFetcher();\n+  }\n+\n+  @Test\n+  public void testIfNPEThrownWhileGettingDelegationToken() throws IOException {\n+    try {\n+      String tokenStr = fetcher.getTokenFromCredential(new Credentials(), \"hive\");\n+      Assert.assertEquals(\"Token with id: hive shall not be found.\", null, tokenStr);\n+    } catch (NullPointerException e) {\n+      Assert.fail(\"This NPE is not handled in the code elsewhere so user is not notified about it!\");\n+      e.printStackTrace();\n+    }\n+  }\n+\n+  @Test\n+  public void testIfGettingDelegationTokenFromCredentialWorks() throws IOException {\n+    Credentials creds = new Credentials();\n+    creds.addToken(new Text(EXISTING_TOKEN), new Token<>());\n+\n+    String tokenStr = fetcher.getTokenFromCredential(creds, EXISTING_TOKEN);\n+    Assert.assertEquals(\"Token string form is not as expected.\", EXPECTED_TOKEN_STRING_FORM, tokenStr);\n+  }\n+}", "filename": "jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5131046ca795ecf958e4e24f163e6014588c2222", "parent": "https://github.com/apache/hive/commit/ef7c3963be035635ef75ee202b9826b9beb407d7", "message": "HIVE-21041: NPE, ParseException in getting schema from logical plan (Teddy Choi, reviewed by Jesus Camacho Rodriguez)\n\nChange-Id: Iff9d9b02f934ed800f932ff916a59288a896f169", "bug_id": "hive_12", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/itests/src/test/resources/testconfiguration.properties", "sha": "18e4f7f02c3e3667c8ab3ab09924571b82a73866", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=5131046ca795ecf958e4e24f163e6014588c2222", "patch": "@@ -527,6 +527,7 @@ minillaplocal.query.files=\\\n   external_jdbc_table_partition.q,\\\n   external_jdbc_table_typeconversion.q,\\\n   fullouter_mapjoin_1_optimized.q,\\\n+  get_splits_0.q,\\\n   groupby2.q,\\\n   groupby_groupingset_bug.q,\\\n   hybridgrace_hashjoin_1.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java", "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java", "sha": "07c65af246f4a9d88d12c933f9638e5baa0f5cb5", "changes": 22, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java?ref=5131046ca795ecf958e4e24f163e6014588c2222", "patch": "@@ -545,22 +545,26 @@ public static String getKeywords(Set<String> excludes) {\n \n   public static RelNode parseQuery(HiveConf conf, String viewQuery)\n       throws SemanticException, IOException, ParseException {\n-    return getAnalyzer(conf).genLogicalPlan(parse(viewQuery));\n+    final Context ctx = new Context(conf);\n+    ctx.setIsLoadingMaterializedView(true);\n+    final ASTNode ast = parse(viewQuery, ctx);\n+    final CalcitePlanner analyzer = getAnalyzer(conf, ctx);\n+    return analyzer.genLogicalPlan(ast);\n   }\n \n   public static List<FieldSchema> parseQueryAndGetSchema(HiveConf conf, String viewQuery)\n       throws SemanticException, IOException, ParseException {\n-    final CalcitePlanner analyzer = getAnalyzer(conf);\n-    analyzer.genLogicalPlan(parse(viewQuery));\n+    final Context ctx = new Context(conf);\n+    ctx.setIsLoadingMaterializedView(true);\n+    final ASTNode ast = parse(viewQuery, ctx);\n+    final CalcitePlanner analyzer = getAnalyzer(conf, ctx);\n+    analyzer.genLogicalPlan(ast);\n     return analyzer.getResultSchema();\n   }\n \n-  private static CalcitePlanner getAnalyzer(HiveConf conf) throws SemanticException, IOException {\n-    final QueryState qs =\n-        new QueryState.Builder().withHiveConf(conf).build();\n-    CalcitePlanner analyzer = new CalcitePlanner(qs);\n-    Context ctx = new Context(conf);\n-    ctx.setIsLoadingMaterializedView(true);\n+  private static CalcitePlanner getAnalyzer(HiveConf conf, Context ctx) throws SemanticException {\n+    final QueryState qs = new QueryState.Builder().withHiveConf(conf).build();\n+    final CalcitePlanner analyzer = new CalcitePlanner(qs);\n     analyzer.initCtx(ctx);\n     analyzer.init(false);\n     return analyzer;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/queries/clientpositive/get_splits_0.q", "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/queries/clientpositive/get_splits_0.q", "sha": "e585fda78fd4b35a1b5fdc3ae0740fdbecff2440", "changes": 3, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/get_splits_0.q?ref=5131046ca795ecf958e4e24f163e6014588c2222", "patch": "@@ -0,0 +1,3 @@\n+--! qt:dataset:src\n+select get_splits(\"SELECT * FROM src WHERE value in (SELECT value FROM src)\",0);\n+select get_splits(\"SELECT key AS `key 1`, value AS `value 1` FROM src\",0);", "filename": "ql/src/test/queries/clientpositive/get_splits_0.q"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/results/clientpositive/llap/get_splits_0.q.out", "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/results/clientpositive/llap/get_splits_0.q.out", "sha": "e1ebe952975c089e1030076e3e26d77bf6799eb9", "changes": 0, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/get_splits_0.q.out?ref=5131046ca795ecf958e4e24f163e6014588c2222", "filename": "ql/src/test/results/clientpositive/llap/get_splits_0.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/961224cd6c0a7ee456b470a8990f8843e6e2b449", "parent": "https://github.com/apache/hive/commit/5aac80513d0f1ac931715bf3c9fcbeff830a458b", "message": "HIVE-20886: Fix NPE: GenericUDFLower (Rajesh Balamohan, reviewed by Gopal V)", "bug_id": "hive_13", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/961224cd6c0a7ee456b470a8990f8843e6e2b449/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLower.java", "blob_url": "https://github.com/apache/hive/blob/961224cd6c0a7ee456b470a8990f8843e6e2b449/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLower.java", "sha": "128df018ecaff69c80ff2bce66ca020a79f14397", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLower.java?ref=961224cd6c0a7ee456b470a8990f8843e6e2b449", "patch": "@@ -58,7 +58,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n \n     if (arguments[0].getCategory() != Category.PRIMITIVE) {\n       throw new UDFArgumentException(\n-          \"LOWER only takes primitive types, got \" + argumentOI.getTypeName());\n+          \"LOWER only takes primitive types, got \" + arguments[0].getTypeName());\n     }\n     argumentOI = (PrimitiveObjectInspector) arguments[0];\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFLower.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/961224cd6c0a7ee456b470a8990f8843e6e2b449/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUpper.java", "blob_url": "https://github.com/apache/hive/blob/961224cd6c0a7ee456b470a8990f8843e6e2b449/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUpper.java", "sha": "25a6e04ddeb6cde03536071f546437a1aa1a366b", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUpper.java?ref=961224cd6c0a7ee456b470a8990f8843e6e2b449", "patch": "@@ -58,7 +58,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n \n     if (arguments[0].getCategory() != Category.PRIMITIVE) {\n       throw new UDFArgumentException(\n-          \"UPPER only takes primitive types, got \" + argumentOI.getTypeName());\n+          \"UPPER only takes primitive types, got \" + arguments[0].getTypeName());\n     }\n     argumentOI = (PrimitiveObjectInspector) arguments[0];\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUpper.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d", "parent": "https://github.com/apache/hive/commit/dfc4b8edbd1ad8c394634c67fbd1f06ba03e4d7f", "message": "HIVE-21186: External tables replication throws NPE if hive.repl.replica.external.table.base.dir is not fully qualified HDFS path (Sankar Hariappan, reviewed by Mahesh Kumar Behera)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>", "bug_id": "hive_14", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java", "blob_url": "https://github.com/apache/hive/blob/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java", "sha": "40ce4b4518ff557331c75126e924726cda0dbd8f", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java?ref=4271bbfb02ae81901b1b44e15e0ca6bd1407de9d", "patch": "@@ -216,7 +216,13 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {\n     DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n     fs.mkdirs(externalTableLocation, new FsPermission(\"777\"));\n \n-    List<String> loadWithClause = externalTableBasePathWithClause();\n+    // Create base directory but use HDFS path without schema or authority details.\n+    // Hive should pick up the local cluster's HDFS schema/authority.\n+    externalTableBasePathWithClause();\n+    List<String> loadWithClause = Collections.singletonList(\n+            \"'\" + HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname + \"'='\"\n+                    + REPLICA_EXTERNAL_BASE + \"'\"\n+    );\n \n     WarehouseInstance.Tuple bootstrapTuple = primary.run(\"use \" + primaryDbName)\n         .run(\"create external table a (i int, j int) \"", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java", "blob_url": "https://github.com/apache/hive/blob/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java", "sha": "59b7c1c2ccb38d743bd4582014e790f8f8fb810e", "changes": 22, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java?ref=4271bbfb02ae81901b1b44e15e0ca6bd1407de9d", "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n@@ -38,7 +39,6 @@\n import java.io.InputStreamReader;\n import java.io.OutputStream;\n import java.io.StringWriter;\n-import java.net.URI;\n import java.nio.charset.StandardCharsets;\n import java.util.Base64;\n import java.util.HashSet;\n@@ -62,13 +62,21 @@\n \n   private ReplExternalTables(){}\n \n-  public static String externalTableLocation(HiveConf hiveConf, String location) {\n-    String currentPath = new Path(location).toUri().getPath();\n+  public static String externalTableLocation(HiveConf hiveConf, String location) throws SemanticException {\n     String baseDir = hiveConf.get(HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname);\n-    URI basePath = new Path(baseDir).toUri();\n-    String dataPath = currentPath.replaceFirst(Path.SEPARATOR, basePath.getPath() + Path.SEPARATOR);\n-    Path dataLocation = new Path(basePath.getScheme(), basePath.getAuthority(), dataPath);\n-    LOG.debug(\"incoming location: {} , new location: {}\", location, dataLocation.toString());\n+    Path basePath = new Path(baseDir);\n+    Path currentPath = new Path(location);\n+    String targetPathWithoutSchemeAndAuth = basePath.toUri().getPath() + currentPath.toUri().getPath();\n+    Path dataLocation;\n+    try {\n+      dataLocation = PathBuilder.fullyQualifiedHDFSUri(\n+              new Path(targetPathWithoutSchemeAndAuth),\n+              basePath.getFileSystem(hiveConf)\n+      );\n+    } catch (IOException e) {\n+      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(), e);\n+    }\n+    LOG.info(\"Incoming external table location: {} , new location: {}\", location, dataLocation.toString());\n     return dataLocation.toString();\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/84e5b939190fb18fe0cac232d7e3edf20fc41652", "parent": "https://github.com/apache/hive/commit/f4380f3b5ea7997185ceefb4315bc2e325c99c9f", "message": "HIVE-20412: NPE in HiveMetaHook (Jason Dere, reviewed by Sergey Shelukhin)", "bug_id": "hive_15", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/84e5b939190fb18fe0cac232d7e3edf20fc41652/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaHook.java", "blob_url": "https://github.com/apache/hive/blob/84e5b939190fb18fe0cac232d7e3edf20fc41652/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaHook.java", "sha": "3a827f7cde91f5de621b8090ac5009744b98338c", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaHook.java?ref=84e5b939190fb18fe0cac232d7e3edf20fc41652", "patch": "@@ -110,7 +110,8 @@ public void commitDropTable(Table table, boolean deleteData)\n    * @param table new table definition\n    */\n   public default void preAlterTable(Table table, EnvironmentContext context) throws MetaException {\n-    String alterOpType = context == null ? null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n+    String alterOpType = (context == null || context.getProperties() == null) ?\n+        null : context.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n     // By default allow only ADDPROPS and DROPPROPS.\n     // alterOpType is null in case of stats update.\n     if (alterOpType != null && !allowedAlterTypes.contains(alterOpType)){", "filename": "standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaHook.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2f7abcc6039ca4fddda2c90d5d0184c70c663614", "parent": "https://github.com/apache/hive/commit/1656e1bd892bb47e50ee8352813b0dab6f230bb4", "message": "HIVE-20829: JdbcStorageHandler range split throws NPE (Daniel Dai, reviewed by Thejas Nair)\n\nSigned-off-by: Thejas M Nair <thejas@hortonworks.com>", "bug_id": "hive_16", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java", "sha": "14c5a777965b7158092331e06610e08b9f649b94", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java?ref=2f7abcc6039ca4fddda2c90d5d0184c70c663614", "patch": "@@ -132,7 +132,7 @@\n         intervals.get(intervals.size()-1).setRight(null);\n         splits = new InputSplit[intervals.size()];\n         for (int i = 0; i < intervals.size(); i++) {\n-          splits[i] = new JdbcInputSplit(partitionColumn, intervals.get(i).getLeft(), intervals.get(i).getRight());\n+          splits[i] = new JdbcInputSplit(partitionColumn, intervals.get(i).getLeft(), intervals.get(i).getRight(), tablePaths[0]);\n         }\n       } else {\n         int numRecords = dbAccessor.getTotalNumberOfRecords(job);", "filename": "jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java", "blob_url": "https://github.com/apache/hive/blob/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java", "sha": "e591413aec6c5686c97693d8227d34bed2e68ca0", "changes": 27, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java?ref=2f7abcc6039ca4fddda2c90d5d0184c70c663614", "patch": "@@ -32,7 +32,6 @@\n   private String lowerBound = null;\n   private String upperBound = null;\n \n-\n   public JdbcInputSplit() {\n     super(null, 0, 0, EMPTY_ARRAY);\n     this.limit = -1;\n@@ -51,14 +50,8 @@ public JdbcInputSplit(int limit, int offset, Path dummyPath) {\n     this.offset = offset;\n   }\n \n-  public JdbcInputSplit(int limit, int offset) {\n-    super(null, 0, 0, EMPTY_ARRAY);\n-    this.limit = limit;\n-    this.offset = offset;\n-  }\n-\n-  public JdbcInputSplit(String partitionColumn, String lowerBound, String upperBound) {\n-    super(null, 0, 0, EMPTY_ARRAY);\n+  public JdbcInputSplit(String partitionColumn, String lowerBound, String upperBound, Path dummyPath) {\n+    super(dummyPath, 0, 0, EMPTY_ARRAY);\n     this.partitionColumn = partitionColumn;\n     this.lowerBound = lowerBound;\n     this.upperBound = upperBound;\n@@ -72,7 +65,17 @@ public void write(DataOutput out) throws IOException {\n     if (partitionColumn != null) {\n       out.writeBoolean(true);\n       out.writeUTF(partitionColumn);\n+    } else {\n+      out.writeBoolean(false);\n+    }\n+    if (lowerBound != null) {\n+      out.writeBoolean(true);\n       out.writeUTF(lowerBound);\n+    } else {\n+      out.writeBoolean(false);\n+    }\n+    if (upperBound != null) {\n+      out.writeBoolean(true);\n       out.writeUTF(upperBound);\n     } else {\n       out.writeBoolean(false);\n@@ -88,7 +91,13 @@ public void readFields(DataInput in) throws IOException {\n     boolean partitionColumnExists = in.readBoolean();\n     if (partitionColumnExists) {\n       partitionColumn = in.readUTF();\n+    }\n+    boolean lowerBoundExists = in.readBoolean();\n+    if (lowerBoundExists) {\n       lowerBound = in.readUTF();\n+    }\n+    boolean upperBoundExists = in.readBoolean();\n+    if (upperBoundExists) {\n       upperBound = in.readUTF();\n     }\n   }", "filename": "jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ac247c4304f45ce7766deacb4826907000447c43", "parent": "https://github.com/apache/hive/commit/691e6544833f0e982d848eef181a80f6cf75a3b6", "message": "HIVE-20389: NPE in SessionStateUserAuthenticator when authenticator=SessionStateUserAuthenticator (Daniel Dai, reviewed by Thejas Nair)", "bug_id": "hive_17", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/ac247c4304f45ce7766deacb4826907000447c43/ql/src/java/org/apache/hadoop/hive/ql/security/SessionStateUserAuthenticator.java", "blob_url": "https://github.com/apache/hive/blob/ac247c4304f45ce7766deacb4826907000447c43/ql/src/java/org/apache/hadoop/hive/ql/security/SessionStateUserAuthenticator.java", "sha": "226fbe0313942694614ec5d364126aeb5d98e030", "changes": 6, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/SessionStateUserAuthenticator.java?ref=ac247c4304f45ce7766deacb4826907000447c43", "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.hadoop.hive.ql.security;\n \n-import java.util.ArrayList;\n import java.util.List;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -32,15 +31,14 @@\n  */\n public class SessionStateUserAuthenticator implements HiveAuthenticationProvider {\n \n-  private final List<String> groupNames = new ArrayList<String>();\n-\n   protected Configuration conf;\n   private SessionState sessionState;\n   private List<String> groups;\n \n   @Override\n   public List<String> getGroupNames() {\n-    if (groups == null) {\n+       // In case of embedded hs2, sessionState.getUserName()=null\n+    if (groups == null && sessionState.getUserName() != null) {\n       groups = UserGroupInformation.createRemoteUser(sessionState.getUserName()).getGroups();\n     }\n     return groups;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/SessionStateUserAuthenticator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/042b2ef7df6af8b93adeb936d94c4079153467ff", "parent": "https://github.com/apache/hive/commit/d4b7b93e27a8a28dde8f4584d883faba86f0203c", "message": "HIVE-20249 : LLAP IO: NPE during refCount decrement (Prasanth Jayachandran, reviewed by Sergey Shelukhin)", "bug_id": "hive_18", "file": [{"additions": 14, "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java", "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java", "sha": "013f3538b4b75f62f9e4c40ddbdd9d1b0b8ed8b1", "changes": 26, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff", "patch": "@@ -1352,21 +1352,23 @@ private int getNextFreeListItem(int offset) {\n \n     public void deallocate(LlapAllocatorBuffer buffer, boolean isAfterMove) {\n       assert data != null;\n-      int pos = buffer.byteBuffer.position();\n-      // Note: this is called by someone who has ensured the buffer is not going to be moved.\n-      int headerIx = pos >>> minAllocLog2;\n-      int freeListIx = freeListFromAllocSize(buffer.allocSize);\n-      if (assertsEnabled && !isAfterMove) {\n-        LlapAllocatorBuffer buf = buffers[headerIx];\n-        if (buf != buffer) {\n-          failWithLog(arenaIx + \":\" + headerIx + \" => \"\n+      if (buffer != null && buffer.byteBuffer != null) {\n+        int pos = buffer.byteBuffer.position();\n+        // Note: this is called by someone who has ensured the buffer is not going to be moved.\n+        int headerIx = pos >>> minAllocLog2;\n+        int freeListIx = freeListFromAllocSize(buffer.allocSize);\n+        if (assertsEnabled && !isAfterMove) {\n+          LlapAllocatorBuffer buf = buffers[headerIx];\n+          if (buf != buffer) {\n+            failWithLog(arenaIx + \":\" + headerIx + \" => \"\n               + toDebugString(buffer) + \", \" + toDebugString(buf));\n+          }\n+          assertBufferLooksValid(freeListFromHeader(headers[headerIx]), buf, arenaIx, headerIx);\n+          checkHeader(headerIx, freeListIx, true);\n         }\n-        assertBufferLooksValid(freeListFromHeader(headers[headerIx]), buf, arenaIx, headerIx);\n-        checkHeader(headerIx, freeListIx, true);\n+        buffers[headerIx] = null;\n+        addToFreeListWithMerge(headerIx, freeListIx, buffer, CasLog.Src.DEALLOC);\n       }\n-      buffers[headerIx] = null;\n-      addToFreeListWithMerge(headerIx, freeListIx, buffer, CasLog.Src.DEALLOC);\n     }\n \n     private void addToFreeListWithMerge(int headerIx, int freeListIx,", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java", "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java", "sha": "e012d7dbf9f5dab0bcc88c8cf680dc6ffd8baf71", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff", "patch": "@@ -20,6 +20,7 @@\n import org.apache.orc.impl.RecordReaderUtils;\n \n import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -40,6 +41,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Function;\n+import com.google.common.base.Joiner;\n \n public class LowLevelCacheImpl implements LowLevelCache, BufferUsageManager, LlapIoDebugDump {\n   private static final int DEFAULT_CLEANUP_INTERVAL = 600;\n@@ -457,6 +459,10 @@ public void debugDumpShort(StringBuilder sb) {\n       try {\n         int fileLocked = 0, fileUnlocked = 0, fileEvicted = 0, fileMoving = 0;\n         if (e.getValue().getCache().isEmpty()) continue;\n+        List<LlapDataBuffer> lockedBufs = null;\n+        if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+          lockedBufs = new ArrayList<>();\n+        }\n         for (Map.Entry<Long, LlapDataBuffer> e2 : e.getValue().getCache().entrySet()) {\n           int newRc = e2.getValue().tryIncRef();\n           if (newRc < 0) {\n@@ -470,6 +476,9 @@ public void debugDumpShort(StringBuilder sb) {\n           try {\n             if (newRc > 1) { // We hold one refcount.\n               ++fileLocked;\n+              if (lockedBufs != null) {\n+                lockedBufs.add(e2.getValue());\n+              }\n             } else {\n               ++fileUnlocked;\n             }\n@@ -483,6 +492,9 @@ public void debugDumpShort(StringBuilder sb) {\n         allMoving += fileMoving;\n         sb.append(\"\\n  file \" + e.getKey() + \": \" + fileLocked + \" locked, \" + fileUnlocked\n             + \" unlocked, \" + fileEvicted + \" evicted, \" + fileMoving + \" being moved\");\n+        if (fileLocked > 0 && LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+          LlapIoImpl.LOCKING_LOGGER.trace(\"locked-buffers: {}\", lockedBufs);\n+        }\n       } finally {\n         e.getValue().decRef();\n       }", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java", "sha": "759594aea33b468abcf58a4930a8081176c98a51", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff", "patch": "@@ -1964,7 +1964,9 @@ public void readIndexStreams(OrcIndex index, StripeInformation stripe,\n     } finally {\n       // Release the unreleased buffers. See class comment about refcounts.\n       try {\n-        releaseInitialRefcounts(toRead.next);\n+        if (toRead != null) {\n+          releaseInitialRefcounts(toRead.next);\n+        }\n         releaseBuffers(toRelease.keySet(), true);\n       } catch (Throwable t) {\n         if (!hasError) throw new IOException(t);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c29038af9bc237bc82b83abb4f1370017a8cd379", "parent": "https://github.com/apache/hive/commit/9d522216972598a38b6750eb9b5d4af1b79cd6ba", "message": "HIVE-20698 : Add better message for NPE when inserting rows with null timestamp to druid (Nishant Bangarwa via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_19", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java", "blob_url": "https://github.com/apache/hive/blob/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java", "sha": "cf37e37c1517d35f7e40e169a447ae36cc3bc202", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java?ref=c29038af9bc237bc82b83abb4f1370017a8cd379", "patch": "@@ -357,9 +357,13 @@ protected SegmentAnalysis submitMetadataRequest(String address, SegmentMetadataQ\n     assert values.size() > granularityFieldIndex;\n     Preconditions.checkArgument(\n         fields.get(granularityFieldIndex).getFieldName().equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME));\n-    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,\n+\n+    Timestamp timestamp =\n         ((TimestampObjectInspector) fields.get(granularityFieldIndex).getFieldObjectInspector())\n-            .getPrimitiveJavaObject(values.get(granularityFieldIndex)).toEpochMilli());\n+            .getPrimitiveJavaObject(values.get(granularityFieldIndex));\n+    Preconditions.checkNotNull(timestamp, \"Timestamp column cannot have null value\");\n+    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME, timestamp.toEpochMilli());\n+\n     if (values.size() == columns.length + 2) {\n       // Then partition number if any.\n       final int partitionNumPos = granularityFieldIndex + 1;", "filename": "druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java"}, {"additions": 34, "raw_url": "https://github.com/apache/hive/raw/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java", "blob_url": "https://github.com/apache/hive/blob/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java", "sha": "acde2394d7fb10cfe6f8c1c1349361450f155afb", "changes": 34, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java?ref=c29038af9bc237bc82b83abb4f1370017a8cd379", "patch": "@@ -74,6 +74,7 @@\n import org.apache.hadoop.io.NullWritable;\n import org.apache.hadoop.io.Text;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n \n import com.fasterxml.jackson.core.JsonParseException;\n@@ -90,6 +91,7 @@\n import io.druid.query.select.SelectResultValue;\n import io.druid.query.timeseries.TimeseriesResultValue;\n import io.druid.query.topn.TopNResultValue;\n+import org.junit.rules.ExpectedException;\n \n /**\n  * Basic tests for Druid SerDe. The examples are taken from Druid 0.9.1.1\n@@ -860,6 +862,38 @@ public void testDruidObjectSerializer()\n     serializeObject(tbl, serDe, ROW_OBJECT, DRUID_WRITABLE);\n   }\n \n+  @Rule\n+  public ExpectedException expectedEx = ExpectedException.none();\n+\n+  @Test\n+  public void testDruidObjectSerializerwithNullTimestamp()\n+      throws Exception {\n+    // Create, initialize, and test the SerDe\n+    DruidSerDe serDe = new DruidSerDe();\n+    Configuration conf = new Configuration();\n+    Properties tbl;\n+    // Mixed source (all types)\n+    tbl = createPropertiesSource(COLUMN_NAMES, COLUMN_TYPES);\n+    SerDeUtils.initializeSerDe(serDe, conf, tbl, null);\n+    Object[] row = new Object[] {\n+        null,\n+        new Text(\"dim1_val\"),\n+        new HiveCharWritable(new HiveChar(\"dim2_v\", 6)),\n+        new HiveVarcharWritable(new HiveVarchar(\"dim3_val\", 8)),\n+        new DoubleWritable(10669.3D),\n+        new FloatWritable(10669.45F),\n+        new LongWritable(1113939),\n+        new IntWritable(1112123),\n+        new ShortWritable((short) 12),\n+        new ByteWritable((byte) 0),\n+        null // granularity\n+    };\n+    expectedEx.expect(NullPointerException.class);\n+    expectedEx.expectMessage(\"Timestamp column cannot have null value\");\n+    // should fail as timestamp is null\n+    serializeObject(tbl, serDe, row, DRUID_WRITABLE);\n+  }\n+\n   private static Properties createPropertiesSource(String columnNames, String columnTypes) {\n     Properties tbl = new Properties();\n ", "filename": "druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "parent": "https://github.com/apache/hive/commit/ff98a30ab49c4eafe53974e03c9dd205c14ffee7", "message": "HIVE-20502: Fix NPE while running skewjoin_mapjoin10.q when column stats is used. (Daniel Voros via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>", "bug_id": "hive_20", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java", "sha": "4313a6b4404dfd27ee5d4225b5136583dd1d164f", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -690,6 +690,7 @@ public void setQBJoinTreeProps(JoinDesc joinDesc) {\n     aliasToOpInfo = joinDesc.aliasToOpInfo;\n     leftInputJoin = joinDesc.leftInputJoin;\n     streamAliases = joinDesc.streamAliases;\n+    joinKeys = joinDesc.joinKeys;\n   }\n \n   public void setQBJoinTreeProps(QBJoinTree joinTree) {\n@@ -716,6 +717,12 @@ public void cloneQBJoinTreeProps(JoinDesc joinDesc) {\n     aliasToOpInfo = new HashMap<String, Operator<? extends OperatorDesc>>(joinDesc.aliasToOpInfo);\n     leftInputJoin = joinDesc.leftInputJoin;\n     streamAliases = joinDesc.streamAliases == null ? null : new ArrayList<String>(joinDesc.streamAliases);\n+    if (joinDesc.joinKeys != null) {\n+      joinKeys = new ExprNodeDesc[joinDesc.joinKeys.length][];\n+      for(int i = 0; i < joinDesc.joinKeys.length; i++) {\n+        joinKeys[i] = joinDesc.joinKeys[i].clone();\n+      }\n+    }\n   }\n \n   public MemoryMonitorInfo getMemoryMonitorInfo() {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/JoinDesc.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/quotedid_skew.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/quotedid_skew.q.out", "sha": "a682644577a46c914f48f06b853e98419c5add19", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/quotedid_skew.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/quotedid_skew.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoin_mapjoin2.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoin_mapjoin2.q.out", "sha": "fa0f615144403a8851b15350022c9fc7c8a557d8", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoin_mapjoin2.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -300,6 +300,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoin_mapjoin2.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoin_union_remove_1.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoin_union_remove_1.q.out", "sha": "68a59728cb80edc5119eeafbba6893d66a52c85e", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoin_union_remove_1.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -139,6 +139,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -276,6 +279,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Right Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -438,6 +444,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -597,6 +606,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Right Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoin_union_remove_1.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoin_union_remove_2.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoin_union_remove_2.q.out", "sha": "31df384548d21f93466997c0df3b1025a6798ff6", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoin_union_remove_2.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -191,6 +191,10 @@ STAGE PLANS:\n           condition map:\n                Inner Join 0 to 1\n                Inner Join 0 to 2\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n+            2 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n           Statistics: Num rows: 2 Data size: 660 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoin_union_remove_2.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt1.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt1.q.out", "sha": "c948c785ae46195f5df085c1eb1c3560c6f420ca", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt1.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -323,6 +326,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Right Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -499,6 +505,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n             compressed: false\n@@ -663,6 +672,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Right Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n             compressed: false", "filename": "ql/src/test/results/clientpositive/skewjoinopt1.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt10.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt10.q.out", "sha": "f8116d37bb2f47d8eedf2c83d31da7ffe6588152", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt10.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -203,6 +203,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 key (type: string)\n+            1 key (type: string)\n           outputColumnNames: _col0, _col6\n           Statistics: Num rows: 3 Data size: 13 Basic stats: COMPLETE Column stats: NONE\n           Select Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt10.q.out"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt11.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt11.q.out", "sha": "dd15712ac3ef25927a6eb055979ef83473718558", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt11.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -280,6 +280,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           Select Operator\n@@ -332,6 +335,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           Select Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt11.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt12.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt12.q.out", "sha": "56ea968632bf67e33e7cb18340f9426d9f610feb", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt12.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -159,6 +159,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string), _col1 (type: string)\n+            1 _col0 (type: string), _col1 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt12.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt14.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt14.q.out", "sha": "5c9ca0ca6c1786e7144bba64d4de6bc69dc4614e", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt14.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -216,6 +216,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt14.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt16.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt16.q.out", "sha": "b5350c1211a3b9fc25baea922ca6783958eaceb9", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt16.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -159,6 +159,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string), _col1 (type: string)\n+            1 _col0 (type: string), _col1 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt16.q.out"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt17.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt17.q.out", "sha": "ecf1547871f4a82e0c3d021e74658c1c2f635019", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt17.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -373,6 +376,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string), _col1 (type: string)\n+            1 _col0 (type: string), _col1 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt17.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt19.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt19.q.out", "sha": "59071e153b49a53cb1e2bd980ae6cb354cb79608", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt19.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt19.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt2.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt2.q.out", "sha": "8343e1ba80e2049903044070497f251338903890", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt2.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -159,6 +159,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string), _col1 (type: string)\n+            1 _col0 (type: string), _col1 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -312,6 +315,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Left Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string), _col1 (type: string)\n+            1 _col0 (type: string), _col1 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -495,6 +501,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string), _col1 (type: string)\n+            1 _col0 (type: string), _col1 (type: string)\n           outputColumnNames: _col0\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -669,6 +678,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Left Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string), _col1 (type: string)\n+            1 _col0 (type: string), _col1 (type: string)\n           outputColumnNames: _col0\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt2.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt20.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt20.q.out", "sha": "3b69192c0c0d338db567c94f45d1bde1d75fdae2", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt20.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt20.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt21.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt21.q.out", "sha": "58e7eb8fa7eeaa7985cd7231abd31e171f438d4e", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt21.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -173,6 +173,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt21.q.out"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt3.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt3.q.out", "sha": "6de674cf383ae5aa89a721bf6febb1d51a7a6721", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt3.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -323,6 +326,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Outer Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt3.q.out"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt4.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt4.q.out", "sha": "4aadb5d83d898fbfa399ede0ca766b7a5999b30f", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt4.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -161,6 +161,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator\n@@ -323,6 +326,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt4.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt5.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt5.q.out", "sha": "d98b3639e7eeb6ecf8ee86e47636d3655e0b835d", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt5.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt5.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt6.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt6.q.out", "sha": "f5fcb78fb259ad9b2dfcae61e6e2c4578312aac1", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt6.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -163,6 +163,9 @@ STAGE PLANS:\n         Join Operator\n           condition map:\n                Inner Join 0 to 1\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3\n           Statistics: Num rows: 1 Data size: 330 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt6.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt7.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt7.q.out", "sha": "d6c2db40bb04607946c28a9d1f22ad7248e6cfc3", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt7.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -215,6 +215,10 @@ STAGE PLANS:\n           condition map:\n                Inner Join 0 to 1\n                Inner Join 0 to 2\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n+            2 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n           Statistics: Num rows: 2 Data size: 660 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt7.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt8.q.out", "blob_url": "https://github.com/apache/hive/blob/de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3/ql/src/test/results/clientpositive/skewjoinopt8.q.out", "sha": "a7426b695c86673b7b8328c53b7dbc8b0f69e67a", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/skewjoinopt8.q.out?ref=de9aaf6072424263c7691b9aa1bf61ea1ed2b2d3", "patch": "@@ -213,6 +213,10 @@ STAGE PLANS:\n           condition map:\n                Inner Join 0 to 1\n                Inner Join 0 to 2\n+          keys:\n+            0 _col0 (type: string)\n+            1 _col0 (type: string)\n+            2 _col0 (type: string)\n           outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n           Statistics: Num rows: 2 Data size: 660 Basic stats: COMPLETE Column stats: NONE\n           File Output Operator", "filename": "ql/src/test/results/clientpositive/skewjoinopt8.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cf2e185341cf517ae7c5d36047d345d086f4ecb5", "parent": "https://github.com/apache/hive/commit/1a610cc545d39b9e9116c5b90108197853d0364c", "message": "HIVE-19777 : NPE in TezSessionState (Sergey Shelukhin, reviewed by Jason Dere)", "bug_id": "hive_21", "file": [{"additions": 28, "raw_url": "https://github.com/apache/hive/raw/cf2e185341cf517ae7c5d36047d345d086f4ecb5/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "blob_url": "https://github.com/apache/hive/blob/cf2e185341cf517ae7c5d36047d345d086f4ecb5/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "sha": "08e65a4a6dd7c428938591e074e0eb243e74f026", "changes": 33, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java?ref=cf2e185341cf517ae7c5d36047d345d086f4ecb5", "patch": "@@ -180,7 +180,11 @@ public boolean isOpening() {\n       return false;\n     }\n     try {\n-      session = sessionFuture.get(0, TimeUnit.NANOSECONDS);\n+      TezClient session = sessionFuture.get(0, TimeUnit.NANOSECONDS);\n+      if (session == null) {\n+        return false;\n+      }\n+      this.session = session;\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       return false;\n@@ -202,7 +206,11 @@ public boolean isOpen() {\n       return false;\n     }\n     try {\n-      session = sessionFuture.get(0, TimeUnit.NANOSECONDS);\n+      TezClient session = sessionFuture.get(0, TimeUnit.NANOSECONDS);\n+      if (session == null) {\n+        return false;\n+      }\n+      this.session = session;\n     } catch (InterruptedException e) {\n       Thread.currentThread().interrupt();\n       return false;\n@@ -363,12 +371,23 @@ protected void openInternal(String[] additionalFilesNotFromConf,\n       FutureTask<TezClient> sessionFuture = new FutureTask<>(new Callable<TezClient>() {\n         @Override\n         public TezClient call() throws Exception {\n+          TezClient result = null;\n           try {\n-            return startSessionAndContainers(session, conf, commonLocalResources, tezConfig, true);\n+            result = startSessionAndContainers(\n+                session, conf, commonLocalResources, tezConfig, true);\n           } catch (Throwable t) {\n+            // The caller has already stopped the session.\n             LOG.error(\"Failed to start Tez session\", t);\n             throw (t instanceof Exception) ? (Exception)t : new Exception(t);\n           }\n+          // Check interrupt at the last moment in case we get cancelled quickly.\n+          // This is not bulletproof but should allow us to close session in most cases.\n+          if (Thread.interrupted()) {\n+            LOG.info(\"Interrupted while starting Tez session\");\n+            closeAndIgnoreExceptions(result);\n+            return null;\n+          }\n+          return result;\n         }\n       });\n       new Thread(sessionFuture, \"Tez session start thread\").start();\n@@ -471,7 +490,11 @@ public void endOpen() throws InterruptedException, CancellationException {\n       return;\n     }\n     try {\n-      this.session = this.sessionFuture.get();\n+      TezClient session = this.sessionFuture.get();\n+      if (session == null) {\n+        throw new RuntimeException(\"Initialization was interrupted\");\n+      }\n+      this.session = session;\n     } catch (ExecutionException e) {\n       throw new RuntimeException(e);\n     }\n@@ -645,7 +668,7 @@ void close(boolean keepDagFilesDir) throws Exception {\n     appJarLr = null;\n \n     try {\n-      if (getSession() != null) {\n+      if (session != null) {\n         LOG.info(\"Closing Tez Session\");\n         closeClient(session);\n         session = null;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/221dbe085950e198d0766d60d6b00a70b30e5935", "parent": "https://github.com/apache/hive/commit/b122aea4ec8775c158fff975ffa472be7bfc0711", "message": "HIVE-20038: Update queries on non-bucketed + partitioned tables throws NPE (Prasanth Jayachandran reviewed by Gopal V)", "bug_id": "hive_22", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "blob_url": "https://github.com/apache/hive/blob/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "sha": "949a9e84a73bd474523d713f1a895aa72843cbc4", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java?ref=221dbe085950e198d0766d60d6b00a70b30e5935", "patch": "@@ -224,7 +224,9 @@ public void closeWriters(boolean abort) throws HiveException {\n     private void commit(FileSystem fs, List<Path> commitPaths) throws HiveException {\n       for (int idx = 0; idx < outPaths.length; ++idx) {\n         try {\n-          commitOneOutPath(idx, fs, commitPaths);\n+          if (outPaths[idx] != null) {\n+            commitOneOutPath(idx, fs, commitPaths);\n+          }\n         } catch (IOException e) {\n           throw new HiveException(\"Unable to commit output from: \" +\n               outPaths[idx] + \" to: \" + finalPaths[idx], e);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java"}, {"additions": 41, "raw_url": "https://github.com/apache/hive/raw/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java", "blob_url": "https://github.com/apache/hive/blob/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java", "sha": "bbe9d5a58773a0aeca7d090450805102a2df4c95", "changes": 41, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java?ref=221dbe085950e198d0766d60d6b00a70b30e5935", "patch": "@@ -184,6 +184,47 @@ public void testNoBuckets() throws Exception {\n     assertExpectedFileSet(expectedFiles, getWarehouseDir() + \"/nobuckets\");\n   }\n \n+  @Test\n+  public void testNoBucketsDP() throws Exception {\n+    hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, \"nonstrict\");\n+    int[][] sourceVals1 = {{0,0,0},{3,3,3}};\n+    int[][] sourceVals2 = {{1,1,1},{2,2,2}};\n+    int[][] sourceVals3 = {{3,3,3},{4,4,4}};\n+    int[][] sourceVals4 = {{5,5,5},{6,6,6}};\n+    runStatementOnDriver(\"drop table if exists tmp\");\n+    runStatementOnDriver(\"create table tmp (c1 integer, c2 integer) partitioned by (c3 integer) stored as orc \" +\n+      \"tblproperties('transactional'='false')\");\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals1));\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals2));\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals3));\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals4));\n+    runStatementOnDriver(\"drop table if exists nobuckets\");\n+    runStatementOnDriver(\"create table nobuckets (c1 integer, c2 integer) partitioned by (c3 integer) stored \" +\n+      \"as orc tblproperties('transactional'='true', 'transactional_properties'='default')\");\n+    String stmt = \"insert into nobuckets partition(c3) select * from tmp\";\n+    runStatementOnDriver(stmt);\n+    List<String> rs = runStatementOnDriver(\n+      \"select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by ROW__ID\");\n+    Assert.assertEquals(\"\", 8, rs.size());\n+    LOG.warn(\"after insert\");\n+    for(String s : rs) {\n+      LOG.warn(s);\n+    }\n+\n+    rs = runStatementOnDriver(\n+      \"select * from nobuckets where c2 in (0,3)\");\n+    Assert.assertEquals(3, rs.size());\n+    runStatementOnDriver(\"update nobuckets set c2 = 17 where c2 in(0,3)\");\n+    rs = runStatementOnDriver(\"select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by INPUT__FILE__NAME, ROW__ID\");\n+    LOG.warn(\"after update\");\n+    for(String s : rs) {\n+      LOG.warn(s);\n+    }\n+    rs = runStatementOnDriver(\n+      \"select * from nobuckets where c2=17\");\n+    Assert.assertEquals(3, rs.size());\n+  }\n+\n   /**\n    * See CTAS tests in TestAcidOnTez\n    */", "filename": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1c33fea890bc01a85eb336caf5d73a85652f91a3", "parent": "https://github.com/apache/hive/commit/80c3bb58e24f13f82ca698486645c6c72364d75d", "message": "HIVE-19970: Replication dump has a NPE when table is empty (Mahesh Kumar Behera, reviewed by Peter Vary, Sankar Hariappan)", "bug_id": "hive_23", "file": [{"additions": 42, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java", "sha": "46c623d34bb8de2f0ded145b77e52dc34f3e7d49", "changes": 42, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -91,6 +91,7 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.junit.Assert;\n \n public class TestReplicationScenarios {\n \n@@ -3185,6 +3186,47 @@ public void testLoadCmPathMissing() throws IOException {\n     fs.create(path, false);\n   }\n \n+  @Test\n+  public void testDumpWithTableDirMissing() throws IOException {\n+    String dbName = createDB(testName.getMethodName(), driver);\n+    run(\"CREATE TABLE \" + dbName + \".normal(a int)\", driver);\n+    run(\"INSERT INTO \" + dbName + \".normal values (1)\", driver);\n+\n+    Path path = new Path(System.getProperty(\"test.warehouse.dir\", \"\"));\n+    path = new Path(path, dbName.toLowerCase() + \".db\");\n+    path = new Path(path, \"normal\");\n+    FileSystem fs = path.getFileSystem(hconf);\n+    fs.delete(path);\n+\n+    advanceDumpDir();\n+    CommandProcessorResponse ret = driver.run(\"REPL DUMP \" + dbName);\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    run(\"DROP TABLE \" + dbName + \".normal\", driver);\n+    run(\"drop database \" + dbName, true, driver);\n+  }\n+\n+  @Test\n+  public void testDumpWithPartitionDirMissing() throws IOException {\n+    String dbName = createDB(testName.getMethodName(), driver);\n+    run(\"CREATE TABLE \" + dbName + \".normal(a int) PARTITIONED BY (part int)\", driver);\n+    run(\"INSERT INTO \" + dbName + \".normal partition (part= 124) values (1)\", driver);\n+\n+    Path path = new Path(System.getProperty(\"test.warehouse.dir\",\"\"));\n+    path = new Path(path, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    path = new Path(path, \"part=124\");\n+    FileSystem fs = path.getFileSystem(hconf);\n+    fs.delete(path);\n+\n+    advanceDumpDir();\n+    CommandProcessorResponse ret = driver.run(\"REPL DUMP \" + dbName);\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    run(\"DROP TABLE \" + dbName + \".normal\", driver);\n+    run(\"drop database \" + dbName, true, driver);\n+  }\n+\n   @Test\n   public void testDumpNonReplDatabase() throws IOException {\n     String dbName = createDBNonRepl(testName.getMethodName(), driver);", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java"}, {"additions": 51, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java", "sha": "86c040532f0c07a6e79dd2418496a11545beb9f9", "changes": 52, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -32,6 +32,10 @@\n import org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.CallerArguments;\n import org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.BehaviourInjection;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n import org.junit.rules.TestName;\n import org.junit.rules.TestRule;\n import org.slf4j.Logger;\n@@ -63,10 +67,11 @@\n   protected static final Logger LOG = LoggerFactory.getLogger(TestReplicationScenarios.class);\n   private static WarehouseInstance primary, replica, replicaNonAcid;\n   private String primaryDbName, replicatedDbName;\n+  private static HiveConf conf;\n \n   @BeforeClass\n   public static void classLevelSetup() throws Exception {\n-    Configuration conf = new Configuration();\n+    conf = new HiveConf(TestReplicationScenariosAcidTables.class);\n     conf.set(\"dfs.client.use.datanode.hostname\", \"true\");\n     conf.set(\"hadoop.proxyuser.\" + Utils.getUGI().getShortUserName() + \".hosts\", \"*\");\n     MiniDFSCluster miniDFSCluster =\n@@ -432,4 +437,49 @@ public Boolean apply(@Nullable CallerArguments args) {\n             .run(\"select name from t2 order by name\")\n             .verifyResults(Arrays.asList(\"bob\", \"carl\"));\n   }\n+\n+  @Test\n+  public void testDumpAcidTableWithPartitionDirMissing() throws Throwable {\n+    String dbName = testName.getMethodName();\n+    primary.run(\"CREATE DATABASE \" + dbName + \" WITH DBPROPERTIES ( '\" +\n+            SOURCE_OF_REPLICATION + \"' = '1,2,3')\")\n+    .run(\"CREATE TABLE \" + dbName + \".normal (a int) PARTITIONED BY (part int)\" +\n+            \" STORED AS ORC TBLPROPERTIES ('transactional'='true')\")\n+    .run(\"INSERT INTO \" + dbName + \".normal partition (part= 124) values (1)\");\n+\n+    Path path = new Path(primary.warehouseRoot, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    path = new Path(path, \"part=124\");\n+    FileSystem fs = path.getFileSystem(conf);\n+    fs.delete(path);\n+\n+    CommandProcessorResponse ret = primary.runCommand(\"REPL DUMP \" + dbName +\n+            \" with ('hive.repl.dump.include.acid.tables' = 'true')\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    primary.run(\"DROP TABLE \" + dbName + \".normal\");\n+    primary.run(\"drop database \" + dbName);\n+  }\n+\n+  @Test\n+  public void testDumpAcidTableWithTableDirMissing() throws Throwable {\n+    String dbName = testName.getMethodName();\n+    primary.run(\"CREATE DATABASE \" + dbName + \" WITH DBPROPERTIES ( '\" +\n+            SOURCE_OF_REPLICATION + \"' = '1,2,3')\")\n+            .run(\"CREATE TABLE \" + dbName + \".normal (a int) \" +\n+                    \" STORED AS ORC TBLPROPERTIES ('transactional'='true')\")\n+            .run(\"INSERT INTO \" + dbName + \".normal values (1)\");\n+\n+    Path path = new Path(primary.warehouseRoot, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    FileSystem fs = path.getFileSystem(conf);\n+    fs.delete(path);\n+\n+    CommandProcessorResponse ret = primary.runCommand(\"REPL DUMP \" + dbName +\n+            \" with ('hive.repl.dump.include.acid.tables' = 'true')\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    primary.run(\"DROP TABLE \" + dbName + \".normal\");\n+    primary.run(\"drop database \" + dbName);\n+  }\n }", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java", "sha": "08f013031fe9c1e34da14649bb92a65fae562ee0", "changes": 42, "status": "modified", "deletions": 36, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -68,6 +68,9 @@\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.junit.Assert;\n \n public class TestReplicationScenariosAcrossInstances {\n   @Rule\n@@ -878,41 +881,6 @@ private void verifyIfSrcOfReplPropMissing(Map<String, String> props) {\n     assertFalse(props.containsKey(SOURCE_OF_REPLICATION));\n   }\n \n-  @Test\n-  public void testIfCkptSetForObjectsByBootstrapReplLoad() throws Throwable {\n-    WarehouseInstance.Tuple tuple = primary\n-            .run(\"use \" + primaryDbName)\n-            .run(\"create table t1 (id int)\")\n-            .run(\"insert into table t1 values (10)\")\n-            .run(\"create table t2 (place string) partitioned by (country string)\")\n-            .run(\"insert into table t2 partition(country='india') values ('bangalore')\")\n-            .run(\"insert into table t2 partition(country='uk') values ('london')\")\n-            .run(\"insert into table t2 partition(country='us') values ('sfo')\")\n-            .dump(primaryDbName, null);\n-\n-    replica.load(replicatedDbName, tuple.dumpLocation)\n-            .run(\"use \" + replicatedDbName)\n-            .run(\"repl status \" + replicatedDbName)\n-            .verifyResult(tuple.lastReplicationId)\n-            .run(\"show tables\")\n-            .verifyResults(new String[] { \"t1\", \"t2\" })\n-            .run(\"select country from t2\")\n-            .verifyResults(Arrays.asList(\"india\", \"uk\", \"us\"));\n-\n-    Database db = replica.getDatabase(replicatedDbName);\n-    verifyIfCkptSet(db.getParameters(), tuple.dumpLocation);\n-    Table t1 = replica.getTable(replicatedDbName, \"t1\");\n-    verifyIfCkptSet(t1.getParameters(), tuple.dumpLocation);\n-    Table t2 = replica.getTable(replicatedDbName, \"t2\");\n-    verifyIfCkptSet(t2.getParameters(), tuple.dumpLocation);\n-    Partition india = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"india\"));\n-    verifyIfCkptSet(india.getParameters(), tuple.dumpLocation);\n-    Partition us = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"us\"));\n-    verifyIfCkptSet(us.getParameters(), tuple.dumpLocation);\n-    Partition uk = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"uk\"));\n-    verifyIfCkptSet(uk.getParameters(), tuple.dumpLocation);\n-  }\n-\n   @Test\n   public void testIncrementalDumpMultiIteration() throws Throwable {\n     WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);\n@@ -1182,7 +1150,9 @@ public Boolean apply(@Nullable CallerArguments args) {\n     assertEquals(0, replica.getForeignKeyList(replicatedDbName, \"t2\").size());\n \n     // Retry with different dump should fail.\n-    replica.loadFailure(replicatedDbName, tuple2.dumpLocation);\n+    CommandProcessorResponse ret = replica.runCommand(\"REPL LOAD \" + replicatedDbName +\n+            \" FROM '\" + tuple2.dumpLocation + \"'\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.getErrorCode());\n \n     // Verify if create table is not called on table t1 but called for t2 and t3.\n     // Also, allow constraint creation only on t1 and t3. Foreign key creation on t2 fails.", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java", "sha": "f666df11415dc238e2cd9f7c190fd16ae7132929", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -77,6 +77,7 @@\n   HiveConf hiveConf;\n   MiniDFSCluster miniDFSCluster;\n   private HiveMetaStoreClient client;\n+  public final Path warehouseRoot;\n \n   private static int uniqueIdentifier = 0;\n \n@@ -90,7 +91,7 @@\n     assert miniDFSCluster.isDataNodeUp();\n     DistributedFileSystem fs = miniDFSCluster.getFileSystem();\n \n-    Path warehouseRoot = mkDir(fs, \"/warehouse\" + uniqueIdentifier);\n+    warehouseRoot = mkDir(fs, \"/warehouse\" + uniqueIdentifier);\n     if (StringUtils.isNotEmpty(keyNameForEncryptedZone)) {\n       fs.createEncryptionZone(warehouseRoot, keyNameForEncryptedZone);\n     }\n@@ -199,6 +200,10 @@ public WarehouseInstance run(String command) throws Throwable {\n     return this;\n   }\n \n+  public CommandProcessorResponse runCommand(String command) throws Throwable {\n+    return driver.run(command);\n+  }\n+\n   WarehouseInstance runFailure(String command) throws Throwable {\n     CommandProcessorResponse ret = driver.run(command);\n     if (ret.getException() == null) {", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java", "sha": "b2c9daa436039a0bd2ea57a3cb25c7d09b9c52b6", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -502,7 +502,8 @@\n   //if the error message is changed for REPL_EVENTS_MISSING_IN_METASTORE, then need modification in getNextNotification\n   //method in HiveMetaStoreClient\n   REPL_EVENTS_MISSING_IN_METASTORE(20016, \"Notification events are missing in the meta store.\"),\n-  REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID(20017, \"Target database is bootstrapped from some other path.\"),\n+  REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID(20017, \"Load path {0} not valid as target database is bootstrapped \" +\n+          \"from some other path : {1}.\"),\n   REPL_FILE_MISSING_FROM_SRC_AND_CM_PATH(20018, \"File is missing from both source and cm path.\"),\n   REPL_LOAD_PATH_NOT_FOUND(20019, \"Load path does not exist.\"),\n   REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION(20020,", "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java", "sha": "e48657c35d87a52f4269eae15f7146c333c0e125", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -121,6 +121,10 @@ protected int execute(DriverContext driverContext) {\n         lastReplId = incrementalDump(dumpRoot, dmd, cmRoot);\n       }\n       prepareReturnValues(Arrays.asList(dumpRoot.toUri().toString(), String.valueOf(lastReplId)), dumpSchema);\n+    } catch (RuntimeException e) {\n+      LOG.error(\"failed\", e);\n+      setException(e);\n+      return ErrorMsg.getErrorMsg(e.getMessage()).getErrorCode();\n     } catch (Exception e) {\n       LOG.error(\"failed\", e);\n       setException(e);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java", "sha": "0fd305a0f9abb5f19a8bf226fdcfcb056b1286d2", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -79,7 +79,7 @@ public TaskTracker tasks() throws SemanticException {\n       }\n       return tracker;\n     } catch (Exception e) {\n-      throw new SemanticException(e);\n+      throw new SemanticException(e.getMessage(), e);\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java", "sha": "b1f731ff96da06e7ed677d7685b40bc3cae9511e", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.InvalidOperationException;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.exec.repl.ReplStateLogWork;\n@@ -115,9 +116,8 @@ public static boolean replCkptStatus(String dbName, Map<String, String> props, S\n       if (props.get(REPL_CHECKPOINT_KEY).equals(dumpRoot)) {\n         return true;\n       }\n-      throw new InvalidOperationException(\"REPL LOAD with Dump: \" + dumpRoot\n-              + \" is not allowed as the target DB: \" + dbName\n-              + \" is already bootstrap loaded by another Dump \" + props.get(REPL_CHECKPOINT_KEY));\n+      throw new InvalidOperationException(ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.format(dumpRoot,\n+              props.get(REPL_CHECKPOINT_KEY)));\n     }\n     return false;\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java", "sha": "9e2479938278a01abe39e44fc5512e6f1e56525b", "changes": 24, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -20,6 +20,7 @@\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n import org.apache.hadoop.hive.ql.metadata.PartitionIterable;\n import org.apache.hadoop.hive.ql.parse.ReplicationSpec;\n@@ -29,13 +30,15 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.LinkedList;\n import java.util.List;\n import java.util.concurrent.ArrayBlockingQueue;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.Future;\n \n import static org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.Paths;\n \n@@ -70,10 +73,11 @@\n     this.callersSession = SessionState.get();\n   }\n \n-  void write(final ReplicationSpec forReplicationSpec) throws InterruptedException {\n+  void write(final ReplicationSpec forReplicationSpec) throws InterruptedException, HiveException {\n+    List<Future<?>> futures = new LinkedList<>();\n     ExecutorService producer = Executors.newFixedThreadPool(1,\n         new ThreadFactoryBuilder().setNameFormat(\"partition-submitter-thread-%d\").build());\n-    producer.submit(() -> {\n+    futures.add(producer.submit(() -> {\n       SessionState.setCurrentSessionState(callersSession);\n       for (Partition partition : partitionIterable) {\n         try {\n@@ -83,7 +87,7 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n               \"Error while queuing up the partitions for export of data files\", e);\n         }\n       }\n-    });\n+    }));\n     producer.shutdown();\n \n     ThreadFactory namingThreadFactory =\n@@ -102,7 +106,7 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n         continue;\n       }\n       LOG.debug(\"scheduling partition dump {}\", partition.getName());\n-      consumer.submit(() -> {\n+      futures.add(consumer.submit(() -> {\n         String partitionName = partition.getName();\n         String threadName = Thread.currentThread().getName();\n         LOG.debug(\"Thread: {}, start partition dump {}\", threadName, partitionName);\n@@ -115,11 +119,19 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n                   .export(forReplicationSpec);\n           LOG.debug(\"Thread: {}, finish partition dump {}\", threadName, partitionName);\n         } catch (Exception e) {\n-          throw new RuntimeException(\"Error while export of data files\", e);\n+          throw new RuntimeException(e.getMessage(), e);\n         }\n-      });\n+      }));\n     }\n     consumer.shutdown();\n+    for (Future<?> future : futures) {\n+      try {\n+        future.get();\n+      } catch (Exception e) {\n+        LOG.error(\"failed\", e.getCause());\n+        throw new HiveException(e.getCause().getMessage(), e.getCause());\n+      }\n+    }\n     // may be drive this via configuration as well.\n     consumer.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java", "sha": "976104c210d0ef6bf633817a8d53d04a173ad001", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -22,6 +22,7 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.NotificationEvent;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n@@ -37,6 +38,7 @@\n import org.slf4j.LoggerFactory;\n \n import java.io.DataOutputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.Collection;\n import java.util.Collections;\n@@ -204,7 +206,11 @@ public static boolean shouldReplicate(NotificationEvent tableForEvent,\n   static List<Path> getDataPathList(Path fromPath, ReplicationSpec replicationSpec, HiveConf conf)\n           throws IOException {\n     if (replicationSpec.isTransactionalTableDump()) {\n-      return AcidUtils.getValidDataPaths(fromPath, conf, replicationSpec.getValidWriteIdList());\n+      try {\n+        return AcidUtils.getValidDataPaths(fromPath, conf, replicationSpec.getValidWriteIdList());\n+      } catch (FileNotFoundException e) {\n+        throw new IOException(ErrorMsg.FILE_NOT_FOUND.format(e.getMessage()), e);\n+      }\n     } else {\n       return Collections.singletonList(fromPath);\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java", "sha": "e8eaae6961e7071e18e559291024ca369bfe5eb5", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hive.ql.parse.repl.dump.io;\n \n import java.io.BufferedWriter;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.OutputStreamWriter;\n import java.util.ArrayList;\n@@ -46,6 +47,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import static org.apache.hadoop.hive.ql.ErrorMsg.FILE_NOT_FOUND;\n+\n //TODO: this object is created once to call one method and then immediately destroyed.\n //So it's basically just a roundabout way to pass arguments to a static method. Simplify?\n public class FileOperations {\n@@ -156,6 +159,10 @@ private void exportFilesAsList() throws SemanticException, IOException, LoginExc\n         }\n         done = true;\n       } catch (IOException e) {\n+        if (e instanceof FileNotFoundException) {\n+          logger.error(\"exporting data files in dir : \" + dataPathList + \" to \" + exportRootDataDir + \" failed\");\n+          throw new FileNotFoundException(FILE_NOT_FOUND.format(e.getMessage()));\n+        }\n         repeat++;\n         logger.info(\"writeFilesList failed\", e);\n         if (repeat >= FileUtils.MAX_IO_ERROR_RETRY) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java", "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java", "sha": "93ac74c68b5e14687852a76fdb298557bf79a08a", "changes": 7, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3", "patch": "@@ -125,13 +125,10 @@ public void alterTable(RawStore msdb, Warehouse wh, String catName, String dbnam\n \n     Table oldt = null;\n \n-    List<TransactionalMetaStoreEventListener> transactionalListeners = null;\n-    List<MetaStoreEventListener> listeners = null;\n+    List<TransactionalMetaStoreEventListener> transactionalListeners = handler.getTransactionalListeners();\n+    List<MetaStoreEventListener> listeners = handler.getListeners();\n     Map<String, String> txnAlterTableEventResponses = Collections.emptyMap();\n \n-    transactionalListeners = handler.getTransactionalListeners();\n-    listeners = handler.getListeners();\n-\n     try {\n       boolean rename = false;\n       List<Partition> parts;", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e", "parent": "https://github.com/apache/hive/commit/1b3ac733f53598636870f4f7af09d2938fe0b09f", "message": "HIVE-18786 : NPE in Hive windowing functions (Dongwook Kwon via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_24", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java", "blob_url": "https://github.com/apache/hive/blob/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java", "sha": "b02ca0708b8526912129872a821ab1722d9b32c9", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java?ref=5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e", "patch": "@@ -149,6 +149,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc\n     // This function should be overriden in every sub class\n     // And the sub class should call super.init(m, parameters) to get mode set.\n     mode = m;\n+    partitionEvaluator = null;\n     return null;\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java"}, {"additions": 79, "raw_url": "https://github.com/apache/hive/raw/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java", "blob_url": "https://github.com/apache/hive/blob/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java", "sha": "0747fa15d0b80215155ec6ffcb5d0746a97abef9", "changes": 79, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java?ref=5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e", "patch": "@@ -0,0 +1,79 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import org.apache.hadoop.hive.ql.exec.PTFPartition;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef;\n+import org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef;\n+import org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.Answers;\n+import org.mockito.Mock;\n+import org.mockito.runners.MockitoJUnitRunner;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class TestGenericUDAFEvaluator {\n+\n+  @Mock(answer = Answers.CALLS_REAL_METHODS)\n+  private GenericUDAFEvaluator udafEvaluator;\n+\n+  @Mock\n+  private WindowFrameDef winFrame;\n+\n+  @Mock\n+  private PTFPartition partition1;\n+\n+  @Mock\n+  private ObjectInspector outputOI;\n+\n+  private List<PTFExpressionDef> parameters = Collections.emptyList();\n+\n+  @Test\n+  public void testGetPartitionWindowingEvaluatorWithoutInitCall() {\n+    BasePartitionEvaluator partition1Evaluator1 = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    BasePartitionEvaluator partition1Evaluator2 = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    Assert.assertEquals(partition1Evaluator1, partition1Evaluator2);\n+  }\n+\n+  @Test\n+  public void testGetPartitionWindowingEvaluatorWithInitCall() throws HiveException {\n+    BasePartitionEvaluator partition1Evaluator1 = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    udafEvaluator.init(GenericUDAFEvaluator.Mode.COMPLETE, null);\n+\n+    BasePartitionEvaluator newPartitionEvaluator = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    Assert.assertNotEquals(partition1Evaluator1, newPartitionEvaluator);\n+  }\n+\n+}", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/43e2f9632130d569c93ebeeb11297813ca34b80c", "parent": "https://github.com/apache/hive/commit/f4352e5339694d290b1a146feb2577d4f96d14eb", "message": "HIVE-19628 : possible NPE in LLAP testSigning (Sergey Shelukhin, reviewed by Jason Dere)", "bug_id": "hive_25", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/43e2f9632130d569c93ebeeb11297813ca34b80c/llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java", "blob_url": "https://github.com/apache/hive/blob/43e2f9632130d569c93ebeeb11297813ca34b80c/llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java", "sha": "6be6836bdbcf5dc160bfa2926cf4c4c5f31522dd", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java?ref=43e2f9632130d569c93ebeeb11297813ca34b80c", "patch": "@@ -106,9 +106,9 @@ public void testSigning() throws Exception {\n   private FakeSecretManager rollKey(FakeSecretManager fsm, int idToPreserve) throws IOException {\n     // Adding keys is PITA - there's no way to plug into timed rolling; just create a new fsm.\n     DelegationKey dk = fsm.getDelegationKey(idToPreserve), curDk = fsm.getCurrentKey();\n-    if (curDk.getKeyId() != idToPreserve) {\n+    if (curDk == null || curDk.getKeyId() != idToPreserve) {\n       LOG.warn(\"The current key is not the one we expect; key rolled in background? Signed with \"\n-          + idToPreserve + \" but got \" + curDk.getKeyId());\n+          + idToPreserve + \" but got \" + (curDk == null ? \"null\" : curDk.getKeyId()));\n     }\n     // Regardless of the above, we should have the key we've signed with.\n     assertNotNull(dk);", "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/29f57fc73ef46dc4b5b6ea7c74e493e3cb3e2f7f", "parent": "https://github.com/apache/hive/commit/6875c7655fc4925332ec0e71db1a3b37430a40a7", "message": "HIVE-19424: NPE In MetaDataFormatters (Alice Fan, reviewed by Aihua Xu)", "bug_id": "hive_26", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/29f57fc73ef46dc4b5b6ea7c74e493e3cb3e2f7f/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java", "blob_url": "https://github.com/apache/hive/blob/29f57fc73ef46dc4b5b6ea7c74e493e3cb3e2f7f/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java", "sha": "df0a2370a905a95ece49a3c519d56baddf732f11", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java?ref=29f57fc73ef46dc4b5b6ea7c74e493e3cb3e2f7f", "patch": "@@ -204,15 +204,15 @@ public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf,\n         if (par.getLocation() != null) {\n           tblLoc = par.getDataLocation().toString();\n         }\n-        inputFormattCls = par.getInputFormatClass().getName();\n-        outputFormattCls = par.getOutputFormatClass().getName();\n+        inputFormattCls = par.getInputFormatClass() == null ? null : par.getInputFormatClass().getName();\n+        outputFormattCls = par.getOutputFormatClass() == null ? null : par.getOutputFormatClass().getName();\n       }\n     } else {\n       if (tbl.getPath() != null) {\n         tblLoc = tbl.getDataLocation().toString();\n       }\n-      inputFormattCls = tbl.getInputFormatClass().getName();\n-      outputFormattCls = tbl.getOutputFormatClass().getName();\n+      inputFormattCls = tbl.getInputFormatClass() == null ? null : tbl.getInputFormatClass().getName();\n+      outputFormattCls = tbl.getOutputFormatClass() == null ? null : tbl.getOutputFormatClass().getName();\n     }\n \n     MapBuilder builder = MapBuilder.create();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/29f57fc73ef46dc4b5b6ea7c74e493e3cb3e2f7f/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java", "blob_url": "https://github.com/apache/hive/blob/29f57fc73ef46dc4b5b6ea7c74e493e3cb3e2f7f/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java", "sha": "326cbedcf0194bfa42b66557fc88f6285df1c619", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java?ref=29f57fc73ef46dc4b5b6ea7c74e493e3cb3e2f7f", "patch": "@@ -291,15 +291,15 @@ public void showTableStatus(DataOutputStream outStream,\n             if (par.getLocation() != null) {\n               tblLoc = par.getDataLocation().toString();\n             }\n-            inputFormattCls = par.getInputFormatClass().getName();\n-            outputFormattCls = par.getOutputFormatClass().getName();\n+            inputFormattCls = par.getInputFormatClass() == null ? null : par.getInputFormatClass().getName();\n+            outputFormattCls = par.getOutputFormatClass() == null ? null : par.getOutputFormatClass().getName();\n           }\n         } else {\n           if (tbl.getPath() != null) {\n             tblLoc = tbl.getDataLocation().toString();\n           }\n-          inputFormattCls = tbl.getInputFormatClass().getName();\n-          outputFormattCls = tbl.getOutputFormatClass().getName();\n+          inputFormattCls = tbl.getInputFormatClass() == null ? null : tbl.getInputFormatClass().getName();\n+          outputFormattCls = tbl.getOutputFormatClass() == null ? null : tbl.getOutputFormatClass().getName();\n         }\n \n         String owner = tbl.getOwner();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/fb79870592d775cd836d5611e21ab1c7030aadba", "parent": "https://github.com/apache/hive/commit/68b66a64f0d9b0d587a7ce1e085a0e8e45253adb", "message": "HIVE-13745: UDF current_date\u3001current_timestamp\u3001unix_timestamp NPE (Biao Wu, reviewed by Yongzhi Chen)", "bug_id": "hive_27", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "sha": "44b9eb2824c1d0c475fc56d8737e023513f49d78", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba", "patch": "@@ -1843,6 +1843,7 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n     TESTMODE_BUCKET_CODEC_VERSION(\"hive.test.bucketcodec.version\", 1,\n       \"For testing only.  Will make ACID subsystem write RecordIdentifier.bucketId in specified\\n\" +\n         \"format\", false),\n+    HIVE_QUERY_TIMESTAMP(\"hive.query.timestamp\", System.currentTimeMillis(), \"query execute time.\"),\n \n     HIVEMERGEMAPFILES(\"hive.merge.mapfiles\", true,\n         \"Merge small files at the end of a map-only job\"),", "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "sha": "9f65a771f95a7c0bd3fdb4e56e47c0fc70235850", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba", "patch": "@@ -1924,6 +1924,7 @@ public String getNextValuesTempTableSuffix() {\n    */\n   public void setupQueryCurrentTimestamp() {\n     queryCurrentTimestamp = new Timestamp(System.currentTimeMillis());\n+    sessionConf.setLongVar(ConfVars.HIVE_QUERY_TIMESTAMP, queryCurrentTimestamp.getTime());\n \n     // Provide a facility to set current timestamp during tests\n     if (sessionConf.getBoolVar(ConfVars.HIVE_IN_TEST)) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java"}, {"additions": 25, "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java", "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java", "sha": "91fd08f13e5cdc28cc80acffea0599e14a45a96e", "changes": 26, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba", "patch": "@@ -18,8 +18,12 @@\n package org.apache.hadoop.hive.ql.udf.generic;\n \n import java.sql.Date;\n+import java.sql.Timestamp;\n \n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n+import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -39,6 +43,13 @@\n public class GenericUDFCurrentDate extends GenericUDF {\n \n   protected DateWritable currentDate;\n+  private Configuration conf;\n+\n+  @Override\n+  public void configure(MapredContext context) {\n+    super.configure(context);\n+    conf = context.getJobConf();\n+  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -50,8 +61,21 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentDate == null) {\n+      SessionState ss = SessionState.get();\n+      Timestamp queryTimestamp;\n+      if (ss == null) {\n+        if (conf == null) {\n+          queryTimestamp = new Timestamp(System.currentTimeMillis());\n+        } else {\n+          queryTimestamp = new Timestamp(\n+                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n+        }\n+      } else {\n+        queryTimestamp = ss.getQueryCurrentTimestamp();\n+      }\n+\n       Date dateVal =\n-          Date.valueOf(SessionState.get().getQueryCurrentTimestamp().toString().substring(0, 10));\n+              Date.valueOf(queryTimestamp.toString().substring(0, 10));\n       currentDate = new DateWritable(dateVal);\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java"}, {"additions": 25, "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java", "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java", "sha": "ca43840e372a26accda20386ef4c8679310783fe", "changes": 26, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba", "patch": "@@ -17,7 +17,12 @@\n  */\n package org.apache.hadoop.hive.ql.udf.generic;\n \n+import java.sql.Timestamp;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n+import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -37,6 +42,13 @@\n public class GenericUDFCurrentTimestamp extends GenericUDF {\n \n   protected TimestampWritable currentTimestamp;\n+  private Configuration conf;\n+\n+  @Override\n+  public void configure(MapredContext context) {\n+    super.configure(context);\n+    conf = context.getJobConf();\n+  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -48,7 +60,19 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentTimestamp == null) {\n-      currentTimestamp = new TimestampWritable(SessionState.get().getQueryCurrentTimestamp());\n+      SessionState ss = SessionState.get();\n+      Timestamp queryTimestamp;\n+      if (ss == null) {\n+        if (conf == null) {\n+          queryTimestamp = new Timestamp(System.currentTimeMillis());\n+        } else {\n+          queryTimestamp = new Timestamp(\n+                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n+        }\n+      } else {\n+        queryTimestamp = ss.getQueryCurrentTimestamp();\n+      }\n+      currentTimestamp = new TimestampWritable(queryTimestamp);\n     }\n \n     return PrimitiveObjectInspectorFactory.writableTimestampObjectInspector;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java"}, {"additions": 26, "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java", "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java", "sha": "6ce72f77037d49571eb1bc5fb647bed0559119cf", "changes": 27, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba", "patch": "@@ -18,6 +18,11 @@\n \n package org.apache.hadoop.hive.ql.udf.generic;\n \n+import java.sql.Timestamp;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.apache.hadoop.hive.ql.exec.Description;\n@@ -37,14 +42,34 @@\n public class GenericUDFUnixTimeStamp extends GenericUDFToUnixTimeStamp {\n   private static final Logger LOG = LoggerFactory.getLogger(GenericUDFUnixTimeStamp.class);\n   private LongWritable currentTimestamp; // retValue is transient so store this separately.\n+  private Configuration conf;\n+\n+  @Override\n+  public void configure(MapredContext context) {\n+    super.configure(context);\n+    conf = context.getJobConf();\n+  }\n+\n   @Override\n   protected void initializeInput(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length > 0) {\n       super.initializeInput(arguments);\n     } else {\n       if (currentTimestamp == null) {\n         currentTimestamp = new LongWritable(0);\n-        setValueFromTs(currentTimestamp, SessionState.get().getQueryCurrentTimestamp());\n+        SessionState ss = SessionState.get();\n+        Timestamp queryTimestamp;\n+        if (ss == null) {\n+          if (conf == null) {\n+            queryTimestamp = new Timestamp(System.currentTimeMillis());\n+          } else {\n+            queryTimestamp = new Timestamp(\n+                    HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n+          }\n+        } else {\n+          queryTimestamp = ss.getQueryCurrentTimestamp();\n+        }\n+        setValueFromTs(currentTimestamp, queryTimestamp);\n         String msg = \"unix_timestamp(void) is deprecated. Use current_timestamp instead.\";\n         SessionState.getConsole().printInfo(msg, false);\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/820db608f2878dde1d9c7b3fa3fbfdb3564710d6", "parent": "https://github.com/apache/hive/commit/5eed779c611c7c766b69f992d76683c58b5772c9", "message": "HIVE-19076: Fix NPE and TApplicationException in function related HiveMetastore methods (Marta Kuczora, via Peter Vary)", "bug_id": "hive_28", "file": [{"additions": 47, "raw_url": "https://github.com/apache/hive/raw/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "65ca63c61f4cc7dfece9b656668f9f079f5ad97c", "changes": 49, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=820db608f2878dde1d9c7b3fa3fbfdb3564710d6", "patch": "@@ -6767,13 +6767,28 @@ private static MetaException newMetaException(Exception e) {\n     }\n \n     private void validateFunctionInfo(Function func) throws InvalidObjectException, MetaException {\n+      if (func == null) {\n+        throw new MetaException(\"Function cannot be null.\");\n+      }\n+      if (func.getFunctionName() == null) {\n+        throw new MetaException(\"Function name cannot be null.\");\n+      }\n+      if (func.getDbName() == null) {\n+        throw new MetaException(\"Database name in Function cannot be null.\");\n+      }\n       if (!MetaStoreUtils.validateName(func.getFunctionName(), null)) {\n         throw new InvalidObjectException(func.getFunctionName() + \" is not a valid object name\");\n       }\n       String className = func.getClassName();\n       if (className == null) {\n         throw new InvalidObjectException(\"Function class name cannot be null\");\n       }\n+      if (func.getOwnerType() == null) {\n+        throw new MetaException(\"Function owner type cannot be null.\");\n+      }\n+      if (func.getFunctionType() == null) {\n+        throw new MetaException(\"Function type cannot be null.\");\n+      }\n     }\n \n     @Override\n@@ -6826,11 +6841,17 @@ public void create_function(Function func) throws TException {\n     public void drop_function(String dbName, String funcName)\n         throws NoSuchObjectException, MetaException,\n         InvalidObjectException, InvalidInputException {\n+      if (funcName == null) {\n+        throw new MetaException(\"Function name cannot be null.\");\n+      }\n       boolean success = false;\n       Function func = null;\n       RawStore ms = getMS();\n       Map<String, String> transactionalListenerResponses = Collections.emptyMap();\n       String[] parsedDbName = parseDbName(dbName, conf);\n+      if (parsedDbName[DB_NAME] == null) {\n+        throw new MetaException(\"Database name cannot be null.\");\n+      }\n       try {\n         ms.openTransaction();\n         func = ms.getFunction(parsedDbName[CAT_NAME], parsedDbName[DB_NAME], funcName);\n@@ -6876,21 +6897,42 @@ public void drop_function(String dbName, String funcName)\n \n     @Override\n     public void alter_function(String dbName, String funcName, Function newFunc) throws TException {\n-      validateFunctionInfo(newFunc);\n+      String[] parsedDbName = parseDbName(dbName, conf);\n+      validateForAlterFunction(parsedDbName[DB_NAME], funcName, newFunc);\n       boolean success = false;\n       RawStore ms = getMS();\n-      String[] parsedDbName = parseDbName(dbName, conf);\n       try {\n         ms.openTransaction();\n         ms.alterFunction(parsedDbName[CAT_NAME], parsedDbName[DB_NAME], funcName, newFunc);\n         success = ms.commitTransaction();\n+      } catch (InvalidObjectException e) {\n+        // Throwing MetaException instead of InvalidObjectException as the InvalidObjectException\n+        // is not defined for the alter_function method in the Thrift interface.\n+        throwMetaException(e);\n       } finally {\n         if (!success) {\n           ms.rollbackTransaction();\n         }\n       }\n     }\n \n+    private void validateForAlterFunction(String dbName, String funcName, Function newFunc)\n+        throws MetaException {\n+      if (dbName == null || funcName == null) {\n+        throw new MetaException(\"Database and function name cannot be null.\");\n+      }\n+      try {\n+        validateFunctionInfo(newFunc);\n+      } catch (InvalidObjectException e) {\n+        // The validateFunctionInfo method is used by the create and alter function methods as well\n+        // and it can throw InvalidObjectException. But the InvalidObjectException is not defined\n+        // for the alter_function method in the Thrift interface, therefore a TApplicationException\n+        // will occur at the caller side. Re-throwing the InvalidObjectException as MetaException\n+        // would eliminate the TApplicationException at caller side.\n+        throw newMetaException(e);\n+      }\n+    }\n+\n     @Override\n     public List<String> get_functions(String dbName, String pattern)\n         throws MetaException {\n@@ -6938,6 +6980,9 @@ public GetAllFunctionsResponse get_all_functions()\n \n     @Override\n     public Function get_function(String dbName, String funcName) throws TException {\n+      if (dbName == null || funcName == null) {\n+        throw new MetaException(\"Database and function name cannot be null.\");\n+      }\n       startFunction(\"get_function\", \": \" + dbName + \".\" + funcName);\n \n       RawStore ms = getMS();", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "blob_url": "https://github.com/apache/hive/blob/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "sha": "867771849b5f3ef2cf446cc3bca1a59f5f19a3e2", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=820db608f2878dde1d9c7b3fa3fbfdb3564710d6", "patch": "@@ -2668,6 +2668,9 @@ public boolean isPartitionMarkedForEvent(String catName, String db_name, String\n \n   @Override\n   public void createFunction(Function func) throws TException {\n+    if (func == null) {\n+      throw new MetaException(\"Function cannot be null.\");\n+    }\n     if (!func.isSetCatName()) func.setCatName(getDefaultCatalog(conf));\n     client.create_function(func);\n   }", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java"}, {"additions": 41, "raw_url": "https://github.com/apache/hive/raw/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java", "blob_url": "https://github.com/apache/hive/blob/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java", "sha": "b5705f90a072b2f5da7a04bdf68ea81cf680f5b0", "changes": 258, "status": "modified", "deletions": 217, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java?ref=820db608f2878dde1d9c7b3fa3fbfdb3564710d6", "patch": "@@ -37,9 +37,7 @@\n import org.apache.hadoop.hive.metastore.client.builder.DatabaseBuilder;\n import org.apache.hadoop.hive.metastore.client.builder.FunctionBuilder;\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n-import org.apache.thrift.TApplicationException;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -208,74 +206,39 @@ public void testCreateFunctionEmptyName() throws Exception {\n     client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n+  public void testCreateFunctionNullFunction() throws Exception {\n+    client.createFunction(null);\n+  }\n+\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullFunctionName() throws Exception {\n     Function function = testFunctions[0];\n     function.setFunctionName(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullDatabaseName() throws Exception {\n     Function function = testFunctions[0];\n     function.setDbName(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullOwnerType() throws Exception {\n     Function function = testFunctions[0];\n     function.setFunctionName(\"test_function_2\");\n     function.setOwnerType(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullFunctionType() throws Exception {\n     Function function = testFunctions[0];\n     function.setFunctionName(\"test_function_2\");\n     function.setFunctionType(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n   @Test(expected = NoSuchObjectException.class)\n@@ -331,18 +294,9 @@ public void testGetFunctionNoSuchFunctionInThisDatabase() throws Exception {\n     client.getFunction(OTHER_DATABASE, function.getFunctionName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testGetFunctionNullDatabase() throws Exception {\n-    try {\n-      client.getFunction(null, OTHER_DATABASE);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws MetaException\n-      Assert.fail(\"Expected an NullPointerException or MetaException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (MetaException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.getFunction(null, OTHER_DATABASE);\n   }\n \n   @Test(expected = MetaException.class)\n@@ -371,32 +325,14 @@ public void testDropFunctionNoSuchFunctionInThisDatabase() throws Exception {\n     client.dropFunction(OTHER_DATABASE, function.getFunctionName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropFunctionNullDatabase() throws Exception {\n-    try {\n-      client.dropFunction(null, \"no_such_function\");\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropFunction(null, \"no_such_function\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropFunctionNullFunctionName() throws Exception {\n-    try {\n-      client.dropFunction(DEFAULT_DATABASE, null);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropFunction(DEFAULT_DATABASE, null);\n   }\n \n   @Test\n@@ -601,190 +537,78 @@ public void testAlterFunctionNoSuchFunctionInThisDatabase() throws Exception {\n     client.alterFunction(OTHER_DATABASE, originalFunction.getFunctionName(), newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullDatabase() throws Exception {\n     Function newFunction = getNewFunction();\n-\n-    try {\n-      client.alterFunction(null, OTHER_DATABASE, newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(null, OTHER_DATABASE, newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunctionName() throws Exception {\n     Function newFunction = getNewFunction();\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, null, newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, null, newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunction() throws Exception {\n     Function originalFunction = testFunctions[1];\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, originalFunction.getFunctionName(), null);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, originalFunction.getFunctionName(), null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionInvalidNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionName(\"test_function_2;\");\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionEmptyNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionName(\"\");\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullClassInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setClassName(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunctionNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionName(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullDatabaseNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setDbName(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullOwnerTypeInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setOwnerType(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunctionTypeInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionType(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNoSuchDatabaseInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setDbName(\"no_such_database\");\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-      exception.printStackTrace();\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-      exception.printStackTrace();\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n   @Test(expected = MetaException.class)", "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8494522d810b8a31764729a8dd814f1e6ceab10e", "parent": "https://github.com/apache/hive/commit/eae5225f4301254cd8c5ad127bc242890bd441a8", "message": "HIVE-20098 : Statistics: NPE when getting Date column partition statistics (Andrii via Gopal V)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_29", "file": [{"additions": 8, "raw_url": "https://github.com/apache/hive/raw/8494522d810b8a31764729a8dd814f1e6ceab10e/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java", "blob_url": "https://github.com/apache/hive/blob/8494522d810b8a31764729a8dd814f1e6ceab10e/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java", "sha": "7a0b21b2580d8bb9b256dbc698f125ed15ccdcd3", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java?ref=8494522d810b8a31764729a8dd814f1e6ceab10e", "patch": "@@ -466,8 +466,14 @@ public static ColumnStatisticsObj getPartitionColumnStatisticsObj(\n     } else if (colType.equals(\"date\")) {\n       DateColumnStatsDataInspector dateStats = new DateColumnStatsDataInspector();\n       dateStats.setNumNulls(mStatsObj.getNumNulls());\n-      dateStats.setHighValue(new Date(mStatsObj.getLongHighValue()));\n-      dateStats.setLowValue(new Date(mStatsObj.getLongLowValue()));\n+      Long highValue = mStatsObj.getLongHighValue();\n+      if (highValue != null) {\n+        dateStats.setHighValue(new Date(highValue));\n+      }\n+      Long lowValue = mStatsObj.getLongLowValue();\n+      if (lowValue != null) {\n+        dateStats.setLowValue(new Date(lowValue));\n+      }\n       dateStats.setNumDVs(mStatsObj.getNumDVs());\n       dateStats.setBitVectors((mStatsObj.getBitVector()==null||!enableBitVector)? null : mStatsObj.getBitVector());\n       colStatsData.setDateStats(dateStats);", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d2d50e6940438f9cb1309aea9d6f278ed93b0536", "parent": "https://github.com/apache/hive/commit/e7480d79af6f63b9acb183e5b596228d2226922c", "message": "HIVE-18886: ACID: NPE on unexplained mysql exceptions (Gopal V, reviewed by Eugene Koifman)", "bug_id": "hive_30", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/d2d50e6940438f9cb1309aea9d6f278ed93b0536/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java", "blob_url": "https://github.com/apache/hive/blob/d2d50e6940438f9cb1309aea9d6f278ed93b0536/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java", "sha": "0b3504d3b44a5a623094c099354b2f0f8d50765a", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java?ref=d2d50e6940438f9cb1309aea9d6f278ed93b0536", "patch": "@@ -53,10 +53,10 @@ public static DatabaseProduct determineDatabaseProduct(String productName) throw\n   public static boolean isDeadlock(DatabaseProduct dbProduct, SQLException e) {\n     return e instanceof SQLTransactionRollbackException\n         || ((dbProduct == MYSQL || dbProduct == POSTGRES || dbProduct == SQLSERVER)\n-            && e.getSQLState().equals(\"40001\"))\n-        || (dbProduct == POSTGRES && e.getSQLState().equals(\"40P01\"))\n-        || (dbProduct == ORACLE && (e.getMessage().contains(\"deadlock detected\")\n-            || e.getMessage().contains(\"can't serialize access for this transaction\")));\n+            && \"40001\".equals(e.getSQLState()))\n+        || (dbProduct == POSTGRES && \"40P01\".equals(e.getSQLState()))\n+        || (dbProduct == ORACLE && (e.getMessage() != null && (e.getMessage().contains(\"deadlock detected\")\n+            || e.getMessage().contains(\"can't serialize access for this transaction\"))));\n   }\n \n   /**", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/DatabaseProduct.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1f25c46a2bf50483e09c756803d78e078dc37b92", "parent": "https://github.com/apache/hive/commit/fb22f576da7383f9cb1d24b66f4090d07b6bde07", "message": "HIVE-19158: Fix NPE in the HiveMetastore add partition tests (Marta Kuczora, reviewed by Peter Vary and Sahil Takiar)", "bug_id": "hive_31", "file": [{"additions": 25, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "9c88cf9e063f22a5ba60f16c34ce45457b0c9375", "changes": 26, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -3250,6 +3250,12 @@ public boolean equals(Object obj) {\n                 part.getTableName(), part.toString());\n             throw new MetaException(errorMsg);\n           }\n+          if (part.getValues() == null || part.getValues().isEmpty()) {\n+            throw new MetaException(\"The partition values cannot be null or empty.\");\n+          }\n+          if (part.getValues().contains(null)) {\n+            throw new MetaException(\"Partition value cannot be null.\");\n+          }\n \n           boolean shouldAdd = startAddPartition(ms, part, ifNotExists);\n           if (!shouldAdd) {\n@@ -3410,7 +3416,10 @@ public AddPartitionsResult add_partitions_req(AddPartitionsRequest request)\n     public int add_partitions(final List<Partition> parts) throws MetaException,\n         InvalidObjectException, AlreadyExistsException {\n       startFunction(\"add_partition\");\n-      if (parts.size() == 0) {\n+      if (parts == null) {\n+        throw new MetaException(\"Partition list cannot be null.\");\n+      }\n+      if (parts.isEmpty()) {\n         return 0;\n       }\n \n@@ -3471,6 +3480,9 @@ private int add_partitions_pspec_core(RawStore ms, String catName, String dbName\n                                           boolean ifNotExists)\n         throws TException {\n       boolean success = false;\n+      if (dbName == null || tblName == null) {\n+        throw new MetaException(\"The database and table name cannot be null.\");\n+      }\n       // Ensures that the list doesn't have dups, and keeps track of directories we have created.\n       final Map<PartValEqWrapperLite, Boolean> addedPartitions = new ConcurrentHashMap<>();\n       PartitionSpecProxy partitionSpecProxy = PartitionSpecProxy.Factory.get(partSpecs);\n@@ -3496,12 +3508,18 @@ private int add_partitions_pspec_core(RawStore ms, String catName, String dbName\n           // will be created if the list contains an invalid partition.\n           final Partition part = partitionIterator.getCurrent();\n \n+          if (part.getDbName() == null || part.getTableName() == null) {\n+            throw new MetaException(\"The database and table name must be set in the partition.\");\n+          }\n           if (!part.getTableName().equalsIgnoreCase(tblName) || !part.getDbName().equalsIgnoreCase(dbName)) {\n             String errorMsg = String.format(\n                 \"Partition does not belong to target table %s.%s. It belongs to the table %s.%s : %s\",\n                 dbName, tblName, part.getDbName(), part.getTableName(), part.toString());\n             throw new MetaException(errorMsg);\n           }\n+          if (part.getValues() == null || part.getValues().isEmpty()) {\n+            throw new MetaException(\"The partition values cannot be null or empty.\");\n+          }\n \n           boolean shouldAdd = startAddPartition(ms, part, ifNotExists);\n           if (!shouldAdd) {\n@@ -3733,6 +3751,9 @@ private Partition add_partition_core(final RawStore ms,\n \n         firePreEvent(new PreAddPartitionEvent(tbl, part, this));\n \n+        if (part.getValues() == null || part.getValues().isEmpty()) {\n+          throw new MetaException(\"The partition values cannot be null or empty.\");\n+        }\n         boolean shouldAdd = startAddPartition(ms, part, false);\n         assert shouldAdd; // start would throw if it already existed here\n         boolean madeDir = createLocationForAddedPartition(tbl, part);\n@@ -3789,6 +3810,9 @@ public Partition add_partition_with_environment_context(\n         final Partition part, EnvironmentContext envContext)\n         throws InvalidObjectException, AlreadyExistsException,\n         MetaException {\n+      if (part == null) {\n+        throw new MetaException(\"Partition cannot be null.\");\n+      }\n       startTableFunction(\"add_partition\",\n           part.getCatName(), part.getDbName(), part.getTableName());\n       Partition ret = null;", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "sha": "4f686a028c9e98ac26564d05e658b19806fca83e", "changes": 11, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -649,7 +649,7 @@ public Partition add_partition(Partition new_part) throws TException {\n \n   public Partition add_partition(Partition new_part, EnvironmentContext envContext)\n       throws TException {\n-    if (!new_part.isSetCatName()) new_part.setCatName(getDefaultCatalog(conf));\n+    if (new_part != null && !new_part.isSetCatName()) new_part.setCatName(getDefaultCatalog(conf));\n     Partition p = client.add_partition_with_environment_context(new_part, envContext);\n     return deepCopy(p);\n   }\n@@ -664,6 +664,9 @@ public Partition add_partition(Partition new_part, EnvironmentContext envContext\n    */\n   @Override\n   public int add_partitions(List<Partition> new_parts) throws TException {\n+    if (new_parts == null || new_parts.contains(null)) {\n+      throw new MetaException(\"Partitions cannot be null.\");\n+    }\n     if (new_parts != null && !new_parts.isEmpty() && !new_parts.get(0).isSetCatName()) {\n       final String defaultCat = getDefaultCatalog(conf);\n       new_parts.forEach(p -> p.setCatName(defaultCat));\n@@ -674,6 +677,9 @@ public int add_partitions(List<Partition> new_parts) throws TException {\n   @Override\n   public List<Partition> add_partitions(\n       List<Partition> parts, boolean ifNotExists, boolean needResults) throws TException {\n+    if (parts == null || parts.contains(null)) {\n+      throw new MetaException(\"Partitions cannot be null.\");\n+    }\n     if (parts.isEmpty()) {\n       return needResults ? new ArrayList<>() : null;\n     }\n@@ -688,6 +694,9 @@ public int add_partitions(List<Partition> new_parts) throws TException {\n \n   @Override\n   public int add_partitions_pspec(PartitionSpecProxy partitionSpec) throws TException {\n+    if (partitionSpec == null) {\n+      throw new MetaException(\"PartitionSpec cannot be null.\");\n+    }\n     if (partitionSpec.getCatName() == null) partitionSpec.setCatName(getDefaultCatalog(conf));\n     return client.add_partitions_pspec(partitionSpec.toPartitionSpec());\n   }", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java", "sha": "91d790aa6473c4d3941b11c81938b38dea93c135", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -40,7 +40,7 @@\n   private List<PartitionSpecProxy> partitionSpecProxies;\n   private int size = 0;\n \n-  protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) {\n+  protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) throws MetaException {\n     this.partitionSpecs = partitionSpecs;\n     if (partitionSpecs.isEmpty()) {\n       catName = null;\n@@ -63,13 +63,13 @@ protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) {\n   }\n \n   @Deprecated\n-  protected CompositePartitionSpecProxy(String dbName, String tableName, List<PartitionSpec> partitionSpecs) {\n+  protected CompositePartitionSpecProxy(String dbName, String tableName, List<PartitionSpec> partitionSpecs) throws MetaException {\n     this(DEFAULT_CATALOG_NAME, dbName, tableName, partitionSpecs);\n \n   }\n \n   protected CompositePartitionSpecProxy(String catName, String dbName, String tableName,\n-                                        List<PartitionSpec> partitionSpecs) {\n+                                        List<PartitionSpec> partitionSpecs) throws MetaException {\n     this.catName = catName;\n     this.dbName = dbName;\n     this.tableName = tableName;", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java", "sha": "585b8fd8b6b29cf4bc563b25f67002042c8cd71f", "changes": 22, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.PartitionListComposingSpec;\n import org.apache.hadoop.hive.metastore.api.PartitionSpec;\n \n import java.util.Arrays;\n@@ -33,9 +34,24 @@\n \n   private PartitionSpec partitionSpec;\n \n-  protected PartitionListComposingSpecProxy(PartitionSpec partitionSpec) {\n+  protected PartitionListComposingSpecProxy(PartitionSpec partitionSpec) throws MetaException {\n     assert partitionSpec.isSetPartitionList()\n         : \"Partition-list should have been set.\";\n+    PartitionListComposingSpec partitionList = partitionSpec.getPartitionList();\n+    if (partitionList == null || partitionList.getPartitions() == null) {\n+      throw new MetaException(\"The partition list cannot be null.\");\n+    }\n+    for (Partition partition : partitionList.getPartitions()) {\n+      if (partition == null) {\n+        throw new MetaException(\"Partition cannot be null.\");\n+      }\n+      if (partition.getValues() == null || partition.getValues().isEmpty()) {\n+        throw new MetaException(\"The partition value list cannot be null or empty.\");\n+      }\n+      if (partition.getValues().contains(null)) {\n+        throw new MetaException(\"Partition value cannot be null.\");\n+      }\n+    }\n     this.partitionSpec = partitionSpec;\n   }\n \n@@ -102,6 +118,10 @@ public void setRootLocation(String newRootPath) throws MetaException {\n       throw new MetaException(\"No common root-path. Can't replace root-path!\");\n     }\n \n+    if (newRootPath == null) {\n+      throw new MetaException(\"Root path cannot be null.\");\n+    }\n+\n     for (Partition partition : partitionSpec.getPartitionList().getPartitions()) {\n       String location = partition.getSd().getLocation();\n       if (location.startsWith(oldRootPath)) {", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java", "sha": "18664463633f5de404e4d6b6b0ed3a726082dcf2", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -100,8 +100,9 @@\n      * Factory method. Construct PartitionSpecProxy from raw PartitionSpec.\n      * @param partSpec Raw PartitionSpec from the Thrift API.\n      * @return PartitionSpecProxy instance.\n+     * @throws MetaException\n      */\n-    public static PartitionSpecProxy get(PartitionSpec partSpec) {\n+    public static PartitionSpecProxy get(PartitionSpec partSpec) throws MetaException {\n \n       if (partSpec == null) {\n         return null;\n@@ -123,8 +124,9 @@ public static PartitionSpecProxy get(PartitionSpec partSpec) {\n      * Factory method to construct CompositePartitionSpecProxy.\n      * @param partitionSpecs List of raw PartitionSpecs.\n      * @return A CompositePartitionSpecProxy instance.\n+     * @throws MetaException\n      */\n-    public static PartitionSpecProxy get(List<PartitionSpec> partitionSpecs) {\n+    public static PartitionSpecProxy get(List<PartitionSpec> partitionSpecs) throws MetaException {\n       return new CompositePartitionSpecProxy(partitionSpecs);\n     }\n ", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java", "sha": "5b462066f4f2f196d4b9154b1b996d6a5053794a", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -38,8 +38,11 @@\n \n   private PartitionSpec partitionSpec;\n \n-  public PartitionSpecWithSharedSDProxy(PartitionSpec partitionSpec) {\n+  public PartitionSpecWithSharedSDProxy(PartitionSpec partitionSpec) throws MetaException {\n     assert partitionSpec.isSetSharedSDPartitionSpec();\n+    if (partitionSpec.getSharedSDPartitionSpec().getSd() == null) {\n+      throw new MetaException(\"The shared storage descriptor must be set.\");\n+    }\n     this.partitionSpec = partitionSpec;\n   }\n ", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java"}, {"additions": 25, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java", "sha": "88064d920f30ec1dfa60db51a12baa6848b6c788", "changes": 71, "status": "modified", "deletions": 46, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.hive.metastore.client.builder.TableBuilder;\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -512,7 +511,7 @@ public void testAddPartitionTooManyValues() throws Exception {\n   @Test(expected = MetaException.class)\n   public void testAddPartitionNoPartColOnTable() throws Exception {\n \n-    Table origTable = new TableBuilder()\n+    new TableBuilder()\n         .setDbName(DB_NAME)\n         .setTableName(TABLE_NAME)\n         .addCol(\"test_id\", \"int\", \"test col id\")\n@@ -575,27 +574,19 @@ public void testAddPartitionMorePartColInTable() throws Exception {\n     client.add_partition(partition);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionNullPartition() throws Exception {\n-    try {\n-      client.add_partition(null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+\n+    client.add_partition(null);\n   }\n \n-  @Test\n-  public void testAddPartitionNullValue() throws Exception {\n+  @Test(expected = MetaException.class)\n+  public void testAddPartitionNullValues() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, null);\n-    try {\n-      client.add_partition(partition);\n-    } catch (NullPointerException e) {\n-      // TODO: This works different in remote and embedded mode.\n-      // In embedded mode, no exception happens.\n-    }\n+    partition.setValues(null);\n+    client.add_partition(partition);\n   }\n \n   @Test\n@@ -698,14 +689,10 @@ public void testAddPartitionsWithDefaultAttributes() throws Exception {\n     verifyPartitionAttributesDefaultValues(part, table.getSd().getLocation());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionsNullList() throws Exception {\n-    try {\n-      client.add_partitions(null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+\n+    client.add_partitions(null);\n   }\n \n   @Test\n@@ -1159,31 +1146,23 @@ public void testAddPartitionsMorePartColInTable() throws Exception {\n     client.add_partitions(partitions);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionsNullPartition() throws Exception {\n-    try {\n-      List<Partition> partitions = new ArrayList<>();\n-      partitions.add(null);\n-      client.add_partitions(partitions);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+\n+    List<Partition> partitions = new ArrayList<>();\n+    partitions.add(null);\n+    client.add_partitions(partitions);\n   }\n \n-  @Test\n-  public void testAddPartitionsNullValue() throws Exception {\n+  @Test(expected = MetaException.class)\n+  public void testAddPartitionsNullValues() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, null);\n+    partition.setValues(null);\n     List<Partition> partitions = new ArrayList<>();\n     partitions.add(partition);\n-    try {\n-      client.add_partitions(partitions);\n-    } catch (NullPointerException e) {\n-      // TODO: This works different in remote and embedded mode.\n-      // In embedded mode, no exception happens.\n-    }\n+    client.add_partitions(partitions);\n   }\n \n   @Test\n@@ -1313,9 +1292,9 @@ public void testAddPartsMultipleValues() throws Exception {\n     verifyPartition(table, \"year=2016/month=march\", Lists.newArrayList(\"2016\", \"march\"), 3);\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartsNullList() throws Exception {\n-    // TODO: NPE should not be thrown\n+\n     client.add_partitions(null, false, false);\n   }\n \n@@ -1429,9 +1408,9 @@ public void testAddPartsAlreadyExistsIfExistsTrue() throws Exception {\n     Assert.assertTrue(partitionNames.contains(\"year=2017\"));\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartsNullPartition() throws Exception {\n-    // TODO: NPE should not be thrown\n+\n     List<Partition> partitions = new ArrayList<>();\n     partitions.add(null);\n     client.add_partitions(partitions, false, false);\n@@ -1452,7 +1431,7 @@ private Table createTable(String dbName, String tableName, String location) thro\n \n   private Table createTable(String dbName, String tableName, List<FieldSchema> partCols,\n       String location) throws Exception {\n-    Table table = new TableBuilder()\n+    new TableBuilder()\n         .setDbName(dbName)\n         .setTableName(tableName)\n         .addCol(\"test_id\", \"int\", \"test col id\")", "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java", "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java", "sha": "debcd0eee9d6adde14a7484304ee7d74f1e914fd", "changes": 89, "status": "modified", "deletions": 65, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92", "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;\n import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;\n-import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.metastore.api.InvalidObjectException;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n@@ -45,7 +44,6 @@\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -170,10 +168,9 @@ public void testAddPartitionSpecsMultipleValues() throws Exception {\n \n   // TODO add tests for partitions in other catalogs\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNullSpec() throws Exception {\n \n-    // TODO: NPE should not be thrown.\n     client.add_partitions_pspec(null);\n   }\n \n@@ -186,51 +183,36 @@ public void testAddPartitionSpecEmptyPartList() throws Exception {\n     client.add_partitions_pspec(partitionSpec);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNullPartList() throws Exception {\n \n     createTable();\n     List<Partition> partitions = null;\n     PartitionSpecProxy partitionSpec = buildPartitionSpec(DB_NAME, TABLE_NAME, null, partitions);\n-    try {\n-      client.add_partitions_pspec(partitionSpec);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpec);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNoDB() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, DEFAULT_YEAR_VALUE);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(null, TABLE_NAME, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNoTable() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, DEFAULT_YEAR_VALUE);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, null, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNoDBAndTableInPartition() throws Exception {\n \n     createTable();\n@@ -239,12 +221,7 @@ public void testAddPartitionSpecNoDBAndTableInPartition() throws Exception {\n     partition.setTableName(null);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, TABLE_NAME, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test\n@@ -346,7 +323,7 @@ public void testAddPartitionSpecDiffDBName() throws Exception {\n     }\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNullPart() throws Exception {\n \n     createTable();\n@@ -357,11 +334,7 @@ public void testAddPartitionSpecNullPart() throws Exception {\n     partitions.add(partition2);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, TABLE_NAME, null, partitions);\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-    } catch (NullPointerException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test\n@@ -457,7 +430,7 @@ public void testAddPartitionSpecChangeRootPathFromNull() throws Exception {\n     client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecChangeRootPathToNull() throws Exception {\n \n     Table table = createTable();\n@@ -467,7 +440,6 @@ public void testAddPartitionSpecChangeRootPathToNull() throws Exception {\n         buildPartitionSpec(DB_NAME, TABLE_NAME, rootPath, Lists.newArrayList(partition));\n     partitionSpecProxy.setRootLocation(null);\n     client.add_partitions_pspec(partitionSpecProxy);\n-    // TODO: NPE should not be thrown.\n   }\n \n   @Test(expected = MetaException.class)\n@@ -596,20 +568,15 @@ public void testAddPartitionSpecNullSd() throws Exception {\n     client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecWithSharedSDNullSd() throws Exception {\n \n     createTable();\n     PartitionWithoutSD partition = buildPartitionWithoutSD(Lists.newArrayList(\"2002\"), 0);\n     StorageDescriptor sd = null;\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpecWithSharedSD(Lists.newArrayList(partition), sd);\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test(expected = MetaException.class)\n@@ -734,7 +701,7 @@ public void testAddPartitionsForViewNullPartSd() throws Exception {\n     Assert.assertNull(part.getSd());\n   }\n \n-  @Test\n+  @Test(expected=MetaException.class)\n   public void testAddPartitionSpecWithSharedSDNoValue() throws Exception {\n \n     Table table = createTable();\n@@ -743,12 +710,7 @@ public void testAddPartitionSpecWithSharedSDNoValue() throws Exception {\n     String location = table.getSd().getLocation() + \"/nullValueTest/\";\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpecWithSharedSD(Lists.newArrayList(partition), buildSD(location));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test(expected=MetaException.class)\n@@ -767,18 +729,15 @@ public void testAddPartitionSpecNoValue() throws Exception {\n     client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n-  public void testAddPartitionSpecNullValue() throws Exception {\n+  @Test(expected = MetaException.class)\n+  public void testAddPartitionSpecNullValues() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, null);\n+    partition.setValues(null);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, TABLE_NAME, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-    } catch (NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test\n@@ -938,7 +897,7 @@ public void testAddPartitionSpecMoreThanThreadCountsOneFails() throws Exception\n \n   // Helper methods\n   private void createDB(String dbName) throws TException {\n-    Database db = new DatabaseBuilder().setName(dbName).create(client, metaStore.getConf());\n+    new DatabaseBuilder().setName(dbName).create(client, metaStore.getConf());\n   }\n \n   private Table createTable() throws Exception {\n@@ -948,7 +907,7 @@ private Table createTable() throws Exception {\n \n   private Table createTable(String dbName, String tableName, List<FieldSchema> partCols,\n       String location) throws Exception {\n-    Table table = new TableBuilder()\n+    new TableBuilder()\n         .setDbName(dbName)\n         .setTableName(tableName)\n         .addCol(\"test_id\", \"int\", \"test col id\")\n@@ -1073,7 +1032,7 @@ private PartitionWithoutSD buildPartitionWithoutSD(List<String> values, int inde\n   }\n \n   private PartitionSpecProxy buildPartitionSpec(String dbName, String tableName, String rootPath,\n-      List<Partition> partitions) {\n+      List<Partition> partitions) throws MetaException {\n \n     PartitionSpec partitionSpec = new PartitionSpec();\n     partitionSpec.setDbName(dbName);\n@@ -1104,7 +1063,7 @@ private StorageDescriptor buildSD(String location) {\n   }\n \n   private PartitionSpecProxy buildPartitionSpecWithSharedSD(List<PartitionWithoutSD> partitions,\n-      StorageDescriptor sd) {\n+      StorageDescriptor sd) throws MetaException {\n \n     PartitionSpec partitionSpec = new PartitionSpec();\n     partitionSpec.setDbName(DB_NAME);", "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/54046353e022d7aa5f3656de7d2a66f15608466d", "parent": "https://github.com/apache/hive/commit/fa9e743e7afbcd6409a51955e39e4e2bb3c109d2", "message": "HIVE-18946: Fix columnstats merge NPE (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>", "bug_id": "hive_32", "file": [{"additions": 18, "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/queries/clientpositive/stats_analyze_empty.q", "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/queries/clientpositive/stats_analyze_empty.q", "sha": "6ea6125964fbf8fc7c16c8b80dbc38ecc12e72b6", "changes": 18, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/stats_analyze_empty.q?ref=54046353e022d7aa5f3656de7d2a66f15608466d", "patch": "@@ -0,0 +1,18 @@\n+set hive.stats.autogather=true;\n+set hive.explain.user=true;\n+\n+drop table if exists testdeci2;\n+\n+create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+stored as orc location '/tmp/testdeci2'\n+TBLPROPERTIES (\"transactional\"=\"false\")\n+;\n+\n+\n+analyze table testdeci2 compute statistics for columns;\n+\n+insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2');", "filename": "ql/src/test/queries/clientpositive/stats_analyze_empty.q"}, {"additions": 48, "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/results/clientpositive/stats_analyze_empty.q.out", "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/results/clientpositive/stats_analyze_empty.q.out", "sha": "6eb51e950dbcb24c6a4f9ce08837eb2ebd608cdc", "changes": 48, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/stats_analyze_empty.q.out?ref=54046353e022d7aa5f3656de7d2a66f15608466d", "patch": "@@ -0,0 +1,48 @@\n+PREHOOK: query: drop table if exists testdeci2\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists testdeci2\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+TBLPROPERTIES (\"transactional\"=\"false\")\n+PREHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+TBLPROPERTIES (\"transactional\"=\"false\")\n+POSTHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@testdeci2\n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+PREHOOK: Output: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+POSTHOOK: Output: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: default@testdeci2\n+POSTHOOK: Lineage: testdeci2.amount SCRIPT []\n+POSTHOOK: Lineage: testdeci2.id SCRIPT []\n+POSTHOOK: Lineage: testdeci2.item SCRIPT []\n+POSTHOOK: Lineage: testdeci2.sales_tax SCRIPT []", "filename": "ql/src/test/results/clientpositive/stats_analyze_empty.q.out"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java", "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java", "sha": "517ca7259b7542c6d830b13b2a391c773862879d", "changes": 36, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java?ref=54046353e022d7aa5f3656de7d2a66f15608466d", "patch": "@@ -31,15 +31,15 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new\n         (DecimalColumnStatsDataInspector) aggregateColStats.getStatsData().getDecimalStats();\n     DecimalColumnStatsDataInspector newData =\n         (DecimalColumnStatsDataInspector) newColStats.getStatsData().getDecimalStats();\n-    Decimal lowValue = aggregateData.getLowValue() != null\n-        && (aggregateData.getLowValue().compareTo(newData.getLowValue()) > 0) ? aggregateData\n-        .getLowValue() : newData.getLowValue();\n+\n+    Decimal lowValue = getMin(aggregateData.getLowValue(), newData.getLowValue());\n     aggregateData.setLowValue(lowValue);\n-    Decimal highValue = aggregateData.getHighValue() != null\n-        && (aggregateData.getHighValue().compareTo(newData.getHighValue()) > 0) ? aggregateData\n-        .getHighValue() : newData.getHighValue();\n+\n+    Decimal highValue = getMax(aggregateData.getHighValue(), newData.getHighValue());\n     aggregateData.setHighValue(highValue);\n+\n     aggregateData.setNumNulls(aggregateData.getNumNulls() + newData.getNumNulls());\n+\n     if (aggregateData.getNdvEstimator() == null || newData.getNdvEstimator() == null) {\n       aggregateData.setNumDVs(Math.max(aggregateData.getNumDVs(), newData.getNumDVs()));\n     } else {\n@@ -58,4 +58,28 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new\n       aggregateData.setNumDVs(ndv);\n     }\n   }\n+\n+  Decimal getMax(Decimal firstValue, Decimal secondValue) {\n+    if (firstValue == null && secondValue == null) {\n+      return null;\n+    }\n+\n+    if (firstValue != null && secondValue != null) {\n+      return firstValue.compareTo(secondValue) > 0 ? firstValue : secondValue;\n+    }\n+\n+    return firstValue == null ? secondValue : firstValue;\n+  }\n+\n+  Decimal getMin(Decimal firstValue, Decimal secondValue) {\n+    if (firstValue == null && secondValue == null) {\n+      return null;\n+    }\n+\n+    if (firstValue != null && secondValue != null) {\n+      return firstValue.compareTo(secondValue) > 0 ? secondValue : firstValue;\n+    }\n+\n+    return firstValue == null ? secondValue : firstValue;\n+  }\n }", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java"}, {"additions": 242, "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java", "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java", "sha": "3b74d1e381207cb71c2e5a36220db5201f85ac14", "changes": 242, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java?ref=54046353e022d7aa5f3656de7d2a66f15608466d", "patch": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.hadoop.hive.metastore.columnstats.merge;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.hadoop.hive.metastore.annotation.MetastoreUnitTest;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.columnstats.cache.DecimalColumnStatsDataInspector;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category(MetastoreUnitTest.class)\n+public class DecimalColumnStatsMergerTest {\n+\n+  private static final Decimal DECIMAL_3 = getDecimal(3, 0);\n+  private static final Decimal DECIMAL_5 = getDecimal(5, 0);\n+  private static final Decimal DECIMAL_20 = getDecimal(2, 1);\n+\n+  private DecimalColumnStatsMerger merger = new DecimalColumnStatsMerger();\n+\n+  @Test\n+  public void testMergeNullMinMaxValues() {\n+    ColumnStatisticsObj objNulls = new ColumnStatisticsObj();\n+    createData(objNulls, null, null);\n+\n+    merger.merge(objNulls, objNulls);\n+\n+    Assert.assertNull(objNulls.getStatsData().getDecimalStats().getLowValue());\n+    Assert.assertNull(objNulls.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullLowerValuesOldIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, DECIMAL_3, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullLowerValuesNewIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, DECIMAL_3, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullHigherValuesOldIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, DECIMAL_3);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullHigherValuesNewIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, DECIMAL_3);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeLowValuesFirstWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, DECIMAL_3, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, DECIMAL_5, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeLowValuesSecondWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, DECIMAL_5, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, DECIMAL_3, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeHighValuesFirstWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, DECIMAL_5);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, DECIMAL_3);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_5, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeHighValuesSecondWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, DECIMAL_3);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, DECIMAL_5);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_5, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testDecimalCompareEqual() {\n+    Assert.assertTrue(DECIMAL_3.equals(DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testDecimalCompareDoesntEqual() {\n+    Assert.assertTrue(!DECIMAL_3.equals(DECIMAL_5));\n+  }\n+\n+  @Test\n+  public void testCompareSimple() {\n+    Assert.assertEquals(DECIMAL_5, merger.getMax(DECIMAL_3, DECIMAL_5));\n+  }\n+\n+  @Test\n+  public void testCompareSimpleFlipped() {\n+    Assert.assertEquals(DECIMAL_5, merger.getMax(DECIMAL_5, DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testCompareSimpleReversed() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(DECIMAL_3, DECIMAL_5));\n+  }\n+\n+  @Test\n+  public void testCompareSimpleFlippedReversed() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(DECIMAL_5, DECIMAL_3));\n+  }\n+\n+  /*\n+   * it should pass, but fails because of HIVE-19131, get back to this later!\n+   *\n+   * @Test public void testCompareUnscaledValue() { Assert.assertEquals(DECIMAL_20,\n+   * merger.compareValues(DECIMAL_3, DECIMAL_20)); }\n+   */\n+\n+  @Test\n+  public void testCompareNullsMin() {\n+    Assert.assertNull(merger.getMin(null, null));\n+  }\n+\n+  @Test\n+  public void testCompareNullsMax() {\n+    Assert.assertNull(merger.getMax(null, null));\n+  }\n+\n+  @Test\n+  public void testCompareFirstNullMin() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(null, DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testCompareSecondNullMin() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(DECIMAL_3, null));\n+  }\n+\n+  @Test\n+  public void testCompareFirstNullMax() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMax(null, DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testCompareSecondNullMax() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMax(DECIMAL_3, null));\n+  }\n+\n+  private static Decimal getDecimal(int number, int scale) {\n+    ByteBuffer bb = ByteBuffer.allocate(4);\n+    bb.asIntBuffer().put(number);\n+    return new Decimal(bb, (short) scale);\n+  }\n+\n+  private DecimalColumnStatsDataInspector createData(ColumnStatisticsObj objNulls, Decimal lowValue,\n+      Decimal highValue) {\n+    ColumnStatisticsData statisticsData = new ColumnStatisticsData();\n+    DecimalColumnStatsDataInspector data = new DecimalColumnStatsDataInspector();\n+\n+    statisticsData.setDecimalStats(data);\n+    objNulls.setStatsData(statisticsData);\n+\n+    data.setLowValue(lowValue);\n+    data.setHighValue(highValue);\n+    return data;\n+  }\n+}", "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0a81e1ec309673007bc3e1a37cf88c92edfc285c", "parent": "https://github.com/apache/hive/commit/eb40ea57eac4c3ff46f638cf4ab83bec71b5eda5", "message": "HIVE-19075: Fix NPE when trying to drop or get DB with null name (Marta Kuczora, via Peter Vary)", "bug_id": "hive_33", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "450da4f0f0491b881200ee1123b8064b023ea3e2", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=0a81e1ec309673007bc3e1a37cf88c92edfc285c", "patch": "@@ -1290,6 +1290,9 @@ public Database get_database(final String name) throws NoSuchObjectException, Me\n     @Override\n     public Database get_database_core(String catName, final String name) throws NoSuchObjectException, MetaException {\n       Database db = null;\n+      if (name == null) {\n+        throw new MetaException(\"Database name cannot be null.\");\n+      }\n       try {\n         db = getMS().getDatabase(catName, name);\n       } catch (MetaException | NoSuchObjectException e) {\n@@ -1364,6 +1367,9 @@ private void drop_database_core(RawStore ms, String catName,\n       List<Path> tablePaths = new ArrayList<>();\n       List<Path> partitionPaths = new ArrayList<>();\n       Map<String, String> transactionalListenerResponses = Collections.emptyMap();\n+      if (name == null) {\n+        throw new MetaException(\"Database name cannot be null.\");\n+      }\n       try {\n         ms.openTransaction();\n         db = ms.getDatabase(catName, name);", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java", "blob_url": "https://github.com/apache/hive/blob/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java", "sha": "d558de66d04c529286d91bce2dad8603a4ebfe0b", "changes": 37, "status": "modified", "deletions": 31, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java?ref=0a81e1ec309673007bc3e1a37cf88c92edfc285c", "patch": "@@ -40,7 +40,6 @@\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.hadoop.hive.metastore.utils.SecurityUtils;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -227,51 +226,27 @@ public void testGetDatabaseNoSuchDatabase() throws Exception {\n     client.getDatabase(\"no_such_database\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testGetDatabaseNullName() throws Exception {\n     // Missing database name in the query\n-    try {\n-      client.getDatabase(null);\n-      // TODO: Should have a check on the server side.\n-      Assert.fail(\"Expected a NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.getDatabase(null);\n   }\n \n   @Test(expected = NoSuchObjectException.class)\n   public void testDropDatabaseNoSuchDatabase() throws Exception {\n     client.dropDatabase(\"no_such_database\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropDatabaseNullName() throws Exception {\n     // Missing database in the query\n-    try {\n-      client.dropDatabase(null);\n-      // TODO: Should be checked on server side\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropDatabase(null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropDatabaseDefaultDatabase() throws Exception {\n     // Check if it is possible to drop default database\n-    try {\n-      client.dropDatabase(DEFAULT_DATABASE);\n-      // TODO: Should be checked on server side\n-      Assert.fail(\"Expected an MetaException or TTransportException to be thrown\");\n-    } catch (MetaException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropDatabase(DEFAULT_DATABASE);\n   }\n \n   @Test", "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/80aaa1e65894400d16df608e48bf838238152ec8", "parent": "https://github.com/apache/hive/commit/901a3b3fb04a9d78c64e661ca47e7ebe184abfb8", "message": "HIVE-18898: Fix NPEs in HiveMetastore.dropPartition method (Marta Kuczora, reviewed by Sahil Takiar, Alexander Kolbasov and Peter Vary)", "bug_id": "hive_34", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "84fac2dfa4f29c1cd67d6a06e1f31e6aad016c75", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=80aaa1e65894400d16df608e48bf838238152ec8", "patch": "@@ -3555,6 +3555,16 @@ private boolean drop_partition_common(RawStore ms, String db_name, String tbl_na\n       boolean isExternalTbl = false;\n       Map<String, String> transactionalListenerResponses = Collections.emptyMap();\n \n+      if (db_name == null) {\n+        throw new MetaException(\"The DB name cannot be null.\");\n+      }\n+      if (tbl_name == null) {\n+        throw new MetaException(\"The table name cannot be null.\");\n+      }\n+      if (part_vals == null) {\n+        throw new MetaException(\"The partition values cannot be null.\");\n+      }\n+\n       try {\n         ms.openTransaction();\n         part = ms.getPartition(db_name, tbl_name, part_vals);", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "blob_url": "https://github.com/apache/hive/blob/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "sha": "0e561f82ff28242f1ec62dd0b141b6707506d461", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=80aaa1e65894400d16df608e48bf838238152ec8", "patch": "@@ -993,13 +993,23 @@ public boolean dropPartition(String db_name, String tbl_name,\n   @Override\n   public boolean dropPartition(String db_name, String tbl_name,\n       List<String> part_vals, PartitionDropOptions options) throws TException {\n+    if (options == null) {\n+      options = PartitionDropOptions.instance();\n+    }\n     return dropPartition(db_name, tbl_name, part_vals, options.deleteData,\n                          options.purgeData? getEnvironmentContextWithIfPurgeSet() : null);\n   }\n \n   public boolean dropPartition(String db_name, String tbl_name, List<String> part_vals,\n       boolean deleteData, EnvironmentContext envContext) throws NoSuchObjectException,\n       MetaException, TException {\n+    if (part_vals != null) {\n+      for (String partVal : part_vals) {\n+        if (partVal == null) {\n+          throw new MetaException(\"The partition value must not be null.\");\n+        }\n+      }\n+    }\n     return client.drop_partition_with_environment_context(db_name, tbl_name, part_vals, deleteData,\n         envContext);\n   }", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java", "blob_url": "https://github.com/apache/hive/blob/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java", "sha": "d2ba4be7c055ef3ab3ee76ec771066bb41003854", "changes": 16, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java?ref=80aaa1e65894400d16df608e48bf838238152ec8", "patch": "@@ -264,18 +264,13 @@ public void testDropPartitionNonExistingPartVals() throws Exception {\n     client.dropPartition(DB_NAME, TABLE_NAME, Lists.newArrayList(\"2017\", \"may\"), false);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropPartitionNullVal() throws Exception {\n \n     List<String> partVals = new ArrayList<>();\n     partVals.add(null);\n     partVals.add(null);\n-    try {\n-      client.dropPartition(DB_NAME, TABLE_NAME, partVals, false);\n-      Assert.fail(\"NullPointerException or NoSuchObjectException is expected to be thrown\");\n-    } catch (NullPointerException | NoSuchObjectException e) {\n-      // TODO: Should not throw NPE.\n-    }\n+    client.dropPartition(DB_NAME, TABLE_NAME, partVals, false);\n   }\n \n   @Test(expected = NoSuchObjectException.class)\n@@ -400,10 +395,13 @@ public void testDropPartitionPurgeSetInTable() throws Exception {\n     checkPartitionsAfterDelete(tableName, droppedPartitions, remainingPartitions, true, true);\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test\n   public void testDropPartitionNullPartDropOptions() throws Exception {\n-    // TODO: This should not throw NPE\n+\n     client.dropPartition(DB_NAME, TABLE_NAME, PARTITIONS[0].getValues(), null);\n+    List<Partition> droppedPartitions = Lists.newArrayList(PARTITIONS[0]);\n+    List<Partition> remainingPartitions = Lists.newArrayList(PARTITIONS[1], PARTITIONS[2]);\n+    checkPartitionsAfterDelete(TABLE_NAME, droppedPartitions, remainingPartitions, true, false);\n   }\n \n   // Tests for dropPartition(String db_name, String tbl_name, String name,", "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0870ab9ca6b622b850d83c0a583e1c3a123c33e7", "parent": "https://github.com/apache/hive/commit/68459cf0bfe67dfe72da9095a1dac6b84ede93b0", "message": "HIVE-18892: Fix NPEs in HiveMetastore.exchange_partitions method (Marta Kuczora, reviewed by Sahil Takiar and Peter Vary)", "bug_id": "hive_35", "file": [{"additions": 13, "raw_url": "https://github.com/apache/hive/raw/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "6838dd7d14f78ebcb0e53c9f9d11f666f7ab204b", "changes": 13, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=0870ab9ca6b622b850d83c0a583e1c3a123c33e7", "patch": "@@ -3389,12 +3389,25 @@ public Partition exchange_partition(Map<String, String> partitionSpecs,\n     public List<Partition> exchange_partitions(Map<String, String> partitionSpecs,\n         String sourceDbName, String sourceTableName, String destDbName,\n         String destTableName) throws TException {\n+      if (partitionSpecs == null || sourceDbName == null || sourceTableName == null\n+          || destDbName == null || destTableName == null) {\n+        throw new MetaException(\"The DB and table name for the source and destination tables,\"\n+            + \" and the partition specs must not be null.\");\n+      }\n       boolean success = false;\n       boolean pathCreated = false;\n       RawStore ms = getMS();\n       ms.openTransaction();\n       Table destinationTable = ms.getTable(destDbName, destTableName);\n+      if (destinationTable == null) {\n+        throw new MetaException(\n+            \"The destination table \" + destDbName + \".\" + destTableName + \" not found\");\n+      }\n       Table sourceTable = ms.getTable(sourceDbName, sourceTableName);\n+      if (sourceTable == null) {\n+        throw new MetaException(\n+            \"The source table \" + sourceDbName + \".\" + sourceTableName + \" not found\");\n+      }\n       List<String> partVals = MetaStoreUtils.getPvals(sourceTable.getPartitionKeys(),\n           partitionSpecs);\n       List<String> partValsPresent = new ArrayList<> ();", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"additions": 79, "raw_url": "https://github.com/apache/hive/raw/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java", "blob_url": "https://github.com/apache/hive/blob/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java", "sha": "c9b9e9b28650d84c198b482458561d1b2694f65a", "changes": 288, "status": "modified", "deletions": 209, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java?ref=0870ab9ca6b622b850d83c0a583e1c3a123c33e7", "patch": "@@ -38,7 +38,6 @@\n import org.apache.hadoop.hive.metastore.client.builder.TableBuilder;\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -290,160 +289,100 @@ public void testExchangePartitionsNonExistingPartLocation() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, DB_NAME, \"nonexistingtable\",\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, DB_NAME, \"nonexistingtable\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"nonexistingtable\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"nonexistingtable\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"nonexistingdb\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"nonexistingdb\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptySourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptySourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, \"\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, \"\", sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptyDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptyDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, null, sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, null, sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), null, destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        null, destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)\n@@ -454,15 +393,10 @@ public void testExchangePartitionsEmptyPartSpec() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullPartSpec() throws Exception {\n-    try {\n-      client.exchange_partitions(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+    client.exchange_partitions(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n+        destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)\n@@ -881,160 +815,100 @@ public void testExchangePartitionNonExistingPartLocation() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, DB_NAME, \"nonexistingtable\",\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, DB_NAME, \"nonexistingtable\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"nonexistingtable\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"nonexistingtable\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"nonexistingdb\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"nonexistingdb\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptySourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptySourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, \"\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, \"\", sourceTable.getTableName(), destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptyDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptyDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, null, sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, null, sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), null, destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        null, destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)\n@@ -1045,15 +919,11 @@ public void testExchangePartitionEmptyPartSpec() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullPartSpec() throws Exception {\n-    try {\n-      client.exchange_partition(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+\n+    client.exchange_partition(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n+        destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)", "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/50f52b728f911634e03b8ff6251c15edf3b987cb", "parent": "https://github.com/apache/hive/commit/0d787cbc055eb237bcccd5fdbc144fb6b1d7d4ca", "message": "Revert \"HIVE-13745: UDF current_date\u3001current_timestamp\u3001unix_timestamp NPE (Biao Wu, reviewed by Yongzhi Chen)\"\n\nThis reverts commit fb79870592d775cd836d5611e21ab1c7030aadba.", "bug_id": "hive_36", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "sha": "9df9cca278e9a31c024d570fab2d21c4c126b9b6", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb", "patch": "@@ -1843,7 +1843,6 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n     TESTMODE_BUCKET_CODEC_VERSION(\"hive.test.bucketcodec.version\", 1,\n       \"For testing only.  Will make ACID subsystem write RecordIdentifier.bucketId in specified\\n\" +\n         \"format\", false),\n-    HIVE_QUERY_TIMESTAMP(\"hive.query.timestamp\", System.currentTimeMillis(), \"query execute time.\"),\n \n     HIVEMERGEMAPFILES(\"hive.merge.mapfiles\", true,\n         \"Merge small files at the end of a map-only job\"),", "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "sha": "6bb756cc08148ae4bb9c935f270579e8abeb717a", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb", "patch": "@@ -1924,7 +1924,6 @@ public String getNextValuesTempTableSuffix() {\n    */\n   public void setupQueryCurrentTimestamp() {\n     queryCurrentTimestamp = new Timestamp(System.currentTimeMillis());\n-    sessionConf.setLongVar(ConfVars.HIVE_QUERY_TIMESTAMP, queryCurrentTimestamp.getTime());\n \n     // Provide a facility to set current timestamp during tests\n     if (sessionConf.getBoolVar(ConfVars.HIVE_IN_TEST)) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java", "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java", "sha": "7d3c3f46aa1318cb1eaf152d3b9f0ab36ef00ff7", "changes": 26, "status": "modified", "deletions": 25, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb", "patch": "@@ -18,12 +18,8 @@\n package org.apache.hadoop.hive.ql.udf.generic;\n \n import java.sql.Date;\n-import java.sql.Timestamp;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n-import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -43,13 +39,6 @@\n public class GenericUDFCurrentDate extends GenericUDF {\n \n   protected DateWritable currentDate;\n-  private Configuration conf;\n-\n-  @Override\n-  public void configure(MapredContext context) {\n-    super.configure(context);\n-    conf = context.getJobConf();\n-  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -61,21 +50,8 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentDate == null) {\n-      SessionState ss = SessionState.get();\n-      Timestamp queryTimestamp;\n-      if (ss == null) {\n-        if (conf == null) {\n-          queryTimestamp = new Timestamp(System.currentTimeMillis());\n-        } else {\n-          queryTimestamp = new Timestamp(\n-                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n-        }\n-      } else {\n-        queryTimestamp = ss.getQueryCurrentTimestamp();\n-      }\n-\n       Date dateVal =\n-              Date.valueOf(queryTimestamp.toString().substring(0, 10));\n+          Date.valueOf(SessionState.get().getQueryCurrentTimestamp().toString().substring(0, 10));\n       currentDate = new DateWritable(dateVal);\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java", "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java", "sha": "9da51c84f51d3dae1eac46f8b1e7eef2e482e6c4", "changes": 26, "status": "modified", "deletions": 25, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb", "patch": "@@ -17,12 +17,7 @@\n  */\n package org.apache.hadoop.hive.ql.udf.generic;\n \n-import java.sql.Timestamp;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n-import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -42,13 +37,6 @@\n public class GenericUDFCurrentTimestamp extends GenericUDF {\n \n   protected TimestampWritable currentTimestamp;\n-  private Configuration conf;\n-\n-  @Override\n-  public void configure(MapredContext context) {\n-    super.configure(context);\n-    conf = context.getJobConf();\n-  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -60,19 +48,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentTimestamp == null) {\n-      SessionState ss = SessionState.get();\n-      Timestamp queryTimestamp;\n-      if (ss == null) {\n-        if (conf == null) {\n-          queryTimestamp = new Timestamp(System.currentTimeMillis());\n-        } else {\n-          queryTimestamp = new Timestamp(\n-                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n-        }\n-      } else {\n-        queryTimestamp = ss.getQueryCurrentTimestamp();\n-      }\n-      currentTimestamp = new TimestampWritable(queryTimestamp);\n+      currentTimestamp = new TimestampWritable(SessionState.get().getQueryCurrentTimestamp());\n     }\n \n     return PrimitiveObjectInspectorFactory.writableTimestampObjectInspector;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java", "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java", "sha": "832983105f1f453a756a532cf21bcba8b3ae9fd0", "changes": 27, "status": "modified", "deletions": 26, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb", "patch": "@@ -18,11 +18,6 @@\n \n package org.apache.hadoop.hive.ql.udf.generic;\n \n-import java.sql.Timestamp;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.apache.hadoop.hive.ql.exec.Description;\n@@ -42,34 +37,14 @@\n public class GenericUDFUnixTimeStamp extends GenericUDFToUnixTimeStamp {\n   private static final Logger LOG = LoggerFactory.getLogger(GenericUDFUnixTimeStamp.class);\n   private LongWritable currentTimestamp; // retValue is transient so store this separately.\n-  private Configuration conf;\n-\n-  @Override\n-  public void configure(MapredContext context) {\n-    super.configure(context);\n-    conf = context.getJobConf();\n-  }\n-\n   @Override\n   protected void initializeInput(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length > 0) {\n       super.initializeInput(arguments);\n     } else {\n       if (currentTimestamp == null) {\n         currentTimestamp = new LongWritable(0);\n-        SessionState ss = SessionState.get();\n-        Timestamp queryTimestamp;\n-        if (ss == null) {\n-          if (conf == null) {\n-            queryTimestamp = new Timestamp(System.currentTimeMillis());\n-          } else {\n-            queryTimestamp = new Timestamp(\n-                    HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n-          }\n-        } else {\n-          queryTimestamp = ss.getQueryCurrentTimestamp();\n-        }\n-        setValueFromTs(currentTimestamp, queryTimestamp);\n+        setValueFromTs(currentTimestamp, SessionState.get().getQueryCurrentTimestamp());\n         String msg = \"unix_timestamp(void) is deprecated. Use current_timestamp instead.\";\n         SessionState.getConsole().printInfo(msg, false);\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/35ea45a1e1a7b700df2984a234f444c4a65ccb0d", "parent": "https://github.com/apache/hive/commit/a7c41ba2afb36398134a824bc461a50499114feb", "message": "HIVE-18148: NPE in SparkDynamicPartitionPruningResolver (Rui reviewed by Liyun Zhang and Sahil Takiar)", "bug_id": "hive_37", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/itests/src/test/resources/testconfiguration.properties", "sha": "d51e5cd17631d6372c3ceea452a94e354b8113f5", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d", "patch": "@@ -1506,6 +1506,7 @@ miniSparkOnYarn.only.query.files=spark_combine_equivalent_work.q,\\\n   spark_dynamic_partition_pruning_2.q,\\\n   spark_dynamic_partition_pruning_3.q,\\\n   spark_dynamic_partition_pruning_4.q,\\\n+  spark_dynamic_partition_pruning_5.q,\\\n   spark_dynamic_partition_pruning_mapjoin_only.q,\\\n   spark_constprog_dpp.q,\\\n   spark_dynamic_partition_pruning_recursive_mapjoin.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 113, "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java", "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java", "sha": "130338624edf86fba4ad57323c520d9f467e809f", "changes": 115, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d", "patch": "@@ -17,26 +17,34 @@\n  */\n package org.apache.hadoop.hive.ql.exec.spark;\n \n-import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n-import java.net.URISyntaxException;\n+import java.util.ArrayDeque;\n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.Deque;\n+import java.util.HashSet;\n+import java.util.Set;\n \n import com.google.common.base.Preconditions;\n import org.apache.commons.io.FilenameUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.exec.spark.session.SparkSession;\n import org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManager;\n import org.apache.hadoop.hive.ql.io.HiveKey;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.parse.spark.OptimizeSparkProcContext;\n+import org.apache.hadoop.hive.ql.parse.spark.SparkPartitionPruningSinkOperator;\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.plan.SparkWork;\n import org.apache.hadoop.hive.ql.session.SessionState;\n@@ -210,6 +218,33 @@ public static void collectOp(Collection<Operator<?>> result, Operator<?> root, C\n     }\n   }\n \n+  /**\n+   * Collect operators of type T starting from root. Matching operators will be put into result.\n+   * Set seen can be used to skip search in certain branches.\n+   */\n+  public static <T extends Operator<?>> void collectOp(Operator<?> root, Class<T> cls,\n+      Collection<T> result, Set<Operator<?>> seen) {\n+    if (seen.contains(root)) {\n+      return;\n+    }\n+    Deque<Operator<?>> deque = new ArrayDeque<>();\n+    deque.add(root);\n+    while (!deque.isEmpty()) {\n+      Operator<?> op = deque.remove();\n+      seen.add(op);\n+      if (cls.isInstance(op)) {\n+        result.add((T) op);\n+      }\n+      if (op.getChildOperators() != null) {\n+        for (Operator<?> child : op.getChildOperators()) {\n+          if (!seen.contains(child)) {\n+            deque.add(child);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n   /**\n    * remove currTask from the children of its parentTask\n    * remove currTask from the parent of its childrenTask\n@@ -227,4 +262,80 @@ public static void removeEmptySparkTask(SparkTask currTask) {\n     //remove currTask from childTasks\n     currTask.removeFromChildrenTasks();\n   }\n+\n+  /**\n+   * For DPP sinks w/ common join, we'll split the tree and what's above the branching\n+   * operator is computed multiple times. Therefore it may not be good for performance to support\n+   * nested DPP sinks, i.e. one DPP sink depends on other DPP sinks.\n+   * The following is an example:\n+   *\n+   *             TS          TS\n+   *             |           |\n+   *            ...         FIL\n+   *            |           |  \\\n+   *            RS         RS  SEL\n+   *              \\        /    |\n+   *     TS          JOIN      GBY\n+   *     |         /     \\      |\n+   *    RS        RS    SEL   DPP2\n+   *     \\       /       |\n+   *       JOIN         GBY\n+   *                     |\n+   *                    DPP1\n+   *\n+   * where DPP1 depends on DPP2.\n+   *\n+   * To avoid such case, we'll visit all the branching operators. If a branching operator has any\n+   * further away DPP branches w/ common join in its sub-tree, such branches will be removed.\n+   * In the above example, the branch of DPP1 will be removed.\n+   */\n+  public static void removeNestedDPP(OptimizeSparkProcContext procContext) {\n+    Set<SparkPartitionPruningSinkOperator> allDPPs = new HashSet<>();\n+    Set<Operator<?>> seen = new HashSet<>();\n+    // collect all DPP sinks\n+    for (TableScanOperator root : procContext.getParseContext().getTopOps().values()) {\n+      SparkUtilities.collectOp(root, SparkPartitionPruningSinkOperator.class, allDPPs, seen);\n+    }\n+    // collect all branching operators\n+    Set<Operator<?>> branchingOps = new HashSet<>();\n+    for (SparkPartitionPruningSinkOperator dpp : allDPPs) {\n+      branchingOps.add(dpp.getBranchingOp());\n+    }\n+    // remember the branching ops we have visited\n+    Set<Operator<?>> visited = new HashSet<>();\n+    for (Operator<?> branchingOp : branchingOps) {\n+      if (!visited.contains(branchingOp)) {\n+        visited.add(branchingOp);\n+        seen.clear();\n+        Set<SparkPartitionPruningSinkOperator> nestedDPPs = new HashSet<>();\n+        for (Operator<?> branch : branchingOp.getChildOperators()) {\n+          if (!isDirectDPPBranch(branch)) {\n+            SparkUtilities.collectOp(branch, SparkPartitionPruningSinkOperator.class, nestedDPPs,\n+                seen);\n+          }\n+        }\n+        for (SparkPartitionPruningSinkOperator nestedDPP : nestedDPPs) {\n+          visited.add(nestedDPP.getBranchingOp());\n+          // if a DPP is with MJ, the tree won't be split and so we don't have to remove it\n+          if (!nestedDPP.isWithMapjoin()) {\n+            OperatorUtils.removeBranch(nestedDPP);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  // whether of pattern \"SEL - GBY - DPP\"\n+  private static boolean isDirectDPPBranch(Operator<?> op) {\n+    if (op instanceof SelectOperator && op.getChildOperators() != null\n+        && op.getChildOperators().size() == 1) {\n+      op = op.getChildOperators().get(0);\n+      if (op instanceof GroupByOperator && op.getChildOperators() != null\n+          && op.getChildOperators().size() == 1) {\n+        op = op.getChildOperators().get(0);\n+        return op instanceof SparkPartitionPruningSinkOperator;\n+      }\n+    }\n+    return false;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java", "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java", "sha": "aba15187f9fd662253373043c151e3d8313f5552", "changes": 16, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d", "patch": "@@ -126,11 +126,16 @@ protected void optimizeOperatorPlan(ParseContext pCtx, Set<ReadEntity> inputs,\n     // Run Join releated optimizations\n     runJoinOptimizations(procCtx);\n \n-    // Remove DPP based on expected size of the output data\n-    runRemoveDynamicPruning(procCtx);\n+    if(conf.isSparkDPPAny()){\n+      // Remove DPP based on expected size of the output data\n+      runRemoveDynamicPruning(procCtx);\n \n-    // Remove cyclic dependencies for DPP\n-    runCycleAnalysisForPartitionPruning(procCtx);\n+      // Remove cyclic dependencies for DPP\n+      runCycleAnalysisForPartitionPruning(procCtx);\n+\n+      // Remove nested DPPs\n+      SparkUtilities.removeNestedDPP(procCtx);\n+    }\n \n     // Re-run constant propagation so we fold any new constants introduced by the operator optimizers\n     // Specifically necessary for DPP because we might have created lots of \"and true and true\" conditions\n@@ -161,9 +166,6 @@ private void runRemoveDynamicPruning(OptimizeSparkProcContext procCtx) throws Se\n   }\n \n   private void runCycleAnalysisForPartitionPruning(OptimizeSparkProcContext procCtx) {\n-    if (!conf.isSparkDPPAny()) {\n-      return;\n-    }\n \n     boolean cycleFree = false;\n     while (!cycleFree) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q", "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q", "sha": "488378776e4d5a2ed1f0a30238f72c12c83cc3b1", "changes": 24, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d", "patch": "@@ -0,0 +1,24 @@\n+set hive.spark.dynamic.partition.pruning=true;\n+\n+-- This qfile tests whether we can handle nested DPP sinks\n+\n+create table part1(key string, value string) partitioned by (p string);\n+insert into table part1 partition (p='1') select * from src;\n+\n+create table part2(key string, value string) partitioned by (p string);\n+insert into table part2 partition (p='1') select * from src;\n+\n+create table regular1 as select * from src limit 2;\n+\n+-- nested DPP is removed, upper most DPP is w/ common join\n+explain select * from src join part1 on src.key=part1.p join part2 on src.value=part2.p;\n+\n+-- nested DPP is removed, upper most DPP is w/ map join\n+set hive.auto.convert.join=true;\n+-- ensure regular1 is treated as small table, and partitioned tables are not\n+set hive.auto.convert.join.noconditionaltask.size=20;\n+explain select * from regular1 join part1 on regular1.key=part1.p join part2 on regular1.value=part2.p;\n+\n+drop table part1;\n+drop table part2;\n+drop table regular1;\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q"}, {"additions": 335, "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out", "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out", "sha": "189a43bd159a1376a0f7c8b76153e6c34609b154", "changes": 335, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d", "patch": "@@ -0,0 +1,335 @@\n+PREHOOK: query: create table part1(key string, value string) partitioned by (p string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@part1\n+POSTHOOK: query: create table part1(key string, value string) partitioned by (p string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@part1\n+PREHOOK: query: insert into table part1 partition (p='1') select * from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@part1@p=1\n+POSTHOOK: query: insert into table part1 partition (p='1') select * from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@part1@p=1\n+POSTHOOK: Lineage: part1 PARTITION(p=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: part1 PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: create table part2(key string, value string) partitioned by (p string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@part2\n+POSTHOOK: query: create table part2(key string, value string) partitioned by (p string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@part2\n+PREHOOK: query: insert into table part2 partition (p='1') select * from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@part2@p=1\n+POSTHOOK: query: insert into table part2 partition (p='1') select * from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@part2@p=1\n+POSTHOOK: Lineage: part2 PARTITION(p=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: part2 PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: create table regular1 as select * from src limit 2\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@regular1\n+POSTHOOK: query: create table regular1 as select * from src limit 2\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@regular1\n+POSTHOOK: Lineage: regular1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: regular1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: explain select * from src join part1 on src.key=part1.p join part2 on src.value=part2.p\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select * from src join part1 on src.key=part1.p join part2 on src.value=part2.p\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-2 is a root stage\n+  Stage-1 depends on stages: Stage-2\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-2\n+    Spark\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 6 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (key is not null and value is not null) (type: boolean)\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: string), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                        Group By Operator\n+                          keys: _col0 (type: string)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                          Spark Partition Pruning Sink Operator\n+                            Target column: [1:p (string)]\n+                            partition key expr: [p]\n+                            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                            target works: [Map 1]\n+\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 4), Map 4 (PARTITION-LEVEL SORT, 4)\n+        Reducer 3 <- Map 5 (PARTITION-LEVEL SORT, 4), Reducer 2 (PARTITION-LEVEL SORT, 4)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part1\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col2 (type: string)\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: string)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (key is not null and value is not null) (type: boolean)\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: string), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: string)\n+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col1 (type: string)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part2\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col2 (type: string)\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: string)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col2 (type: string)\n+                  1 _col0 (type: string)\n+                outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+                Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col4 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col4 (type: string)\n+                  Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col4 (type: string)\n+                  1 _col2 (type: string)\n+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                Select Operator\n+                  expressions: _col3 (type: string), _col4 (type: string), _col0 (type: string), _col1 (type: string), _col2 (type: string), _col5 (type: string), _col6 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                  Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: explain select * from regular1 join part1 on regular1.key=part1.p join part2 on regular1.value=part2.p\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select * from regular1 join part1 on regular1.key=part1.p join part2 on regular1.value=part2.p\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-2 is a root stage\n+  Stage-1 depends on stages: Stage-2\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-2\n+    Spark\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: regular1\n+                  Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (key is not null and value is not null) (type: boolean)\n+                    Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: string), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                      Spark HashTable Sink Operator\n+                        keys:\n+                          0 _col2 (type: string)\n+                          1 _col0 (type: string)\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                        Group By Operator\n+                          keys: _col0 (type: string)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                          Spark Partition Pruning Sink Operator\n+                            Target column: [1:p (string)]\n+                            partition key expr: [p]\n+                            Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                            target works: [Map 1]\n+            Local Work:\n+              Map Reduce Local Work\n+\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 4), Map 4 (PARTITION-LEVEL SORT, 4)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part1\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 _col2 (type: string)\n+                        1 _col0 (type: string)\n+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+                      input vertices:\n+                        1 Map 3\n+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col4 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col4 (type: string)\n+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+            Local Work:\n+              Map Reduce Local Work\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part2\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col2 (type: string)\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: string)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col4 (type: string)\n+                  1 _col2 (type: string)\n+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                Select Operator\n+                  expressions: _col3 (type: string), _col4 (type: string), _col0 (type: string), _col1 (type: string), _col2 (type: string), _col5 (type: string), _col6 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                  Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: drop table part1\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@part1\n+PREHOOK: Output: default@part1\n+POSTHOOK: query: drop table part1\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@part1\n+POSTHOOK: Output: default@part1\n+PREHOOK: query: drop table part2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@part2\n+PREHOOK: Output: default@part2\n+POSTHOOK: query: drop table part2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@part2\n+POSTHOOK: Output: default@part2\n+PREHOOK: query: drop table regular1\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@regular1\n+PREHOOK: Output: default@regular1\n+POSTHOOK: query: drop table regular1\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@regular1\n+POSTHOOK: Output: default@regular1", "filename": "ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cce0e3777d178e68f38a0c9335d44a12fff42a6b", "parent": "https://github.com/apache/hive/commit/f0199500f00ae58cf1a9f73f5baebdc5d5eca417", "message": "HIVE-19265 : Potential NPE and hiding actual exception in Hive#copyFiles (Igor Kryvenko via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_38", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/cce0e3777d178e68f38a0c9335d44a12fff42a6b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "blob_url": "https://github.com/apache/hive/blob/cce0e3777d178e68f38a0c9335d44a12fff42a6b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "sha": "466188130184516459dbd307e9cd0cf22844b46d", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java?ref=cce0e3777d178e68f38a0c9335d44a12fff42a6b", "patch": "@@ -3291,7 +3291,9 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,\n         try {\n           files = srcFs.listStatus(src.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);\n         } catch (IOException e) {\n-          pool.shutdownNow();\n+          if (null != pool) {\n+            pool.shutdownNow();\n+          }\n           throw new HiveException(e);\n         }\n       } else {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/006d69d8867b06e27f8890698af4390366b71f3b", "parent": "https://github.com/apache/hive/commit/d0156565adec240735850bedc7729fa43cd05460", "message": "HIVE-18360 : NPE in TezSessionState (Sergey Shelukhin, reviewed by Jason Dere)", "bug_id": "hive_39", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/006d69d8867b06e27f8890698af4390366b71f3b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "blob_url": "https://github.com/apache/hive/blob/006d69d8867b06e27f8890698af4390366b71f3b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "sha": "d66cf1154bb59e45a78fbf2dd90baa8351a3fa7d", "changes": 22, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java?ref=006d69d8867b06e27f8890698af4390366b71f3b", "patch": "@@ -588,17 +588,19 @@ public void ensureLocalResources(Configuration conf, String[] newFilesNotFromCon\n       }\n     }\n \n-    // Finally add the files to AM. The old code seems to do this twice, first for all the new\n-    // resources regardless of type; and then for all the session resources that are not of type\n-    // file (see branch-1 calls to addAppMasterLocalFiles: from updateSession and with resourceMap\n-    // from submit).\n+    // Finally, add the files to the existing AM (if any). The old code seems to do this twice,\n+    // first for all the new resources regardless of type; and then for all the session resources\n+    // that are not of type file (see branch-1 calls to addAppMasterLocalFiles: from updateSession\n+    // and with resourceMap from submit).\n     // TODO: Do we really need all this nonsense?\n-    if (newResources != null && !newResources.isEmpty()) {\n-      session.addAppMasterLocalFiles(DagUtils.createTezLrMap(null, newResources));\n-    }\n-    if (!resources.localizedResources.isEmpty()) {\n-      session.addAppMasterLocalFiles(\n-          DagUtils.getResourcesUpdatableForAm(resources.localizedResources));\n+    if (session != null) {\n+      if (newResources != null && !newResources.isEmpty()) {\n+        session.addAppMasterLocalFiles(DagUtils.createTezLrMap(null, newResources));\n+      }\n+      if (!resources.localizedResources.isEmpty()) {\n+        session.addAppMasterLocalFiles(\n+            DagUtils.getResourcesUpdatableForAm(resources.localizedResources));\n+      }\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6a929c58f943c41685f6e06a519381e696ad91ad", "parent": "https://github.com/apache/hive/commit/b7c64b15c28b78c889e44f58f5e61374696ab9a5", "message": "HIVE-19172 : NPE due to null EnvironmentContext in DDLTask (Nishant Bangarwa via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_40", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/6a929c58f943c41685f6e06a519381e696ad91ad/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "blob_url": "https://github.com/apache/hive/blob/6a929c58f943c41685f6e06a519381e696ad91ad/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "sha": "b9b1830a7b5c284f22c34d310c9afebc3bf93819", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=6a929c58f943c41685f6e06a519381e696ad91ad", "patch": "@@ -3827,6 +3827,9 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n \n     try {\n       EnvironmentContext environmentContext = alterTbl.getEnvironmentContext();\n+      if (environmentContext == null) {\n+        environmentContext = new EnvironmentContext();\n+      }\n       environmentContext.putToProperties(HiveMetaHook.ALTER_TABLE_OPERATION_TYPE, alterTbl.getOp().name());\n       if (allPartitions == null) {\n         db.alterTable(alterTbl.getOldName(), tbl, alterTbl.getIsCascade(), environmentContext);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d9fae049305e20ec8a72e581a2fc938028523402", "parent": "https://github.com/apache/hive/commit/244ca8e5c3192acd017d691ccdbaf0fa06c9fe39", "message": "HIVE-19130: NPE is thrown when REPL LOAD applied drop partition event (Sankar Hariappan, reviewed by Mahesh Kumar Behera, Thejas M Nair)", "bug_id": "hive_41", "file": [{"additions": 55, "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java", "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java", "sha": "70e1aa7f3aaa0d4e95ffefae47ad891f405a9637", "changes": 55, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java?ref=d9fae049305e20ec8a72e581a2fc938028523402", "patch": "@@ -620,4 +620,59 @@ public void testReplLoadFromSourceUsingWithClause() throws Throwable {\n             .run(\"show functions like '\" + replicatedDbName + \"*'\")\n             .verifyResult(null);\n   }\n+\n+  @Test\n+  public void testIncrementalReplWithEventsBatchHavingDropCreateTable() throws Throwable {\n+    // Bootstrap dump with empty db\n+    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);\n+\n+    // Bootstrap load in replica\n+    replica.load(replicatedDbName, bootstrapTuple.dumpLocation)\n+            .status(replicatedDbName)\n+            .verifyResult(bootstrapTuple.lastReplicationId);\n+\n+    // First incremental dump\n+    WarehouseInstance.Tuple firstIncremental = primary.run(\"use \" + primaryDbName)\n+            .run(\"create table table1 (i int)\")\n+            .run(\"create table table2 (id int) partitioned by (country string)\")\n+            .run(\"insert into table1 values (1)\")\n+            .run(\"insert into table2 partition(country='india') values(1)\")\n+            .dump(primaryDbName, bootstrapTuple.lastReplicationId);\n+\n+    // Second incremental dump\n+    WarehouseInstance.Tuple secondIncremental = primary.run(\"use \" + primaryDbName)\n+            .run(\"drop table table1\")\n+            .run(\"drop table table2\")\n+            .run(\"create table table2 (id int) partitioned by (country string)\")\n+            .run(\"alter table table2 add partition(country='india')\")\n+            .run(\"alter table table2 drop partition(country='india')\")\n+            .run(\"insert into table2 partition(country='us') values(2)\")\n+            .run(\"create table table1 (i int)\")\n+            .run(\"insert into table1 values (2)\")\n+            .dump(primaryDbName, firstIncremental.lastReplicationId);\n+\n+    // First incremental load\n+    replica.load(replicatedDbName, firstIncremental.dumpLocation)\n+            .status(replicatedDbName)\n+            .verifyResult(firstIncremental.lastReplicationId)\n+            .run(\"use \" + replicatedDbName)\n+            .run(\"show tables\")\n+            .verifyResults(new String[] {\"table1\", \"table2\"})\n+            .run(\"select * from table1\")\n+            .verifyResults(new String[] {\"1\"})\n+            .run(\"select id from table2 order by id\")\n+            .verifyResults(new String[] {\"1\"});\n+\n+    // Second incremental load\n+    replica.load(replicatedDbName, secondIncremental.dumpLocation)\n+            .status(replicatedDbName)\n+            .verifyResult(secondIncremental.lastReplicationId)\n+            .run(\"use \" + replicatedDbName)\n+            .run(\"show tables\")\n+            .verifyResults(new String[] {\"table1\", \"table2\"})\n+            .run(\"select * from table1\")\n+            .verifyResults(new String[] {\"2\"})\n+            .run(\"select id from table2 order by id\")\n+            .verifyResults(new String[] {\"2\"});\n+  }\n }", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java", "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java", "sha": "accdc1ff0824dd6aac9827e8dbe7ea4b7c56e2f4", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java?ref=d9fae049305e20ec8a72e581a2fc938028523402", "patch": "@@ -231,6 +231,11 @@ WarehouseInstance load(String replicatedDbName, String dumpLocation, List<String\n     return run(replLoadCmd);\n   }\n \n+  WarehouseInstance status(String replicatedDbName) throws Throwable {\n+    String replStatusCmd = \"REPL STATUS \" + replicatedDbName;\n+    return run(replStatusCmd);\n+  }\n+\n   WarehouseInstance status(String replicatedDbName, List<String> withClauseOptions) throws Throwable {\n     String replStatusCmd = \"REPL STATUS \" + replicatedDbName;\n     if (!withClauseOptions.isEmpty()) {", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "sha": "61a04326ac51d6423001eb8cfbe6707decf5416a", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=d9fae049305e20ec8a72e581a2fc938028523402", "patch": "@@ -4522,6 +4522,12 @@ private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws Hi\n       // for dropping. Thus, we need a way to push this filter (replicationSpec.allowEventReplacementInto)\n       // to the  metastore to allow it to do drop a partition or not, depending on a Predicate on the\n       // parameter key values.\n+\n+      if (tbl == null) {\n+        // If table is missing, then partitions are also would've been dropped. Just no-op.\n+        return;\n+      }\n+\n       for (DropTableDesc.PartSpec partSpec : dropTbl.getPartSpecs()){\n         List<Partition> partitions = new ArrayList<>();\n         try {\n@@ -4551,7 +4557,7 @@ private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws Hi\n       console.printInfo(\"Dropped the partition \" + partition.getName());\n       // We have already locked the table, don't lock the partitions.\n       addIfAbsentByName(new WriteEntity(partition, WriteEntity.WriteType.DDL_NO_LOCK));\n-    };\n+    }\n   }\n \n   private void dropTable(Hive db, Table tbl, DropTableDesc dropTbl) throws HiveException {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java", "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java", "sha": "0d2fafb83b9bc008958cb11c5feb88c884e10425", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java?ref=d9fae049305e20ec8a72e581a2fc938028523402", "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.repl.DumpType;\n import org.apache.hadoop.hive.ql.parse.repl.dump.Utils;\n import org.apache.hadoop.hive.ql.parse.repl.dump.io.DBSerializer;\n import org.apache.hadoop.hive.ql.parse.repl.dump.io.JsonWriter;\n@@ -92,6 +93,7 @@\n     private List<Task<? extends Serializable>> tasks;\n     private Logger LOG;\n     private Context ctx;\n+    private DumpType eventType = DumpType.EVENT_UNKNOWN;\n \n     public HiveConf getConf() {\n       return conf;\n@@ -121,6 +123,14 @@ public Context getCtx() {\n       return ctx;\n     }\n \n+    public void setEventType(DumpType eventType) {\n+      this.eventType = eventType;\n+    }\n+\n+    public DumpType getEventType() {\n+      return eventType;\n+    }\n+\n     public SemanticAnalyzerWrapperContext(HiveConf conf, Hive db,\n                                           HashSet<ReadEntity> inputs,\n                                           HashSet<WriteEntity> outputs,", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java"}, {"additions": 28, "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java", "sha": "832f660079e44785a240aa938287ed2ea770f98d", "changes": 31, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java?ref=d9fae049305e20ec8a72e581a2fc938028523402", "patch": "@@ -47,12 +47,14 @@\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.InvalidTableException;\n import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.repl.DumpType;\n import org.apache.hadoop.hive.ql.parse.repl.load.MetaData;\n import org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker;\n import org.apache.hadoop.hive.ql.plan.AddPartitionDesc;\n import org.apache.hadoop.hive.ql.plan.CopyWork;\n import org.apache.hadoop.hive.ql.plan.ImportTableDesc;\n import org.apache.hadoop.hive.ql.plan.DDLWork;\n+import org.apache.hadoop.hive.ql.plan.DropTableDesc;\n import org.apache.hadoop.hive.ql.plan.LoadTableDesc;\n import org.apache.hadoop.hive.ql.plan.LoadTableDesc.LoadFileType;\n import org.apache.hadoop.hive.ql.plan.MoveWork;\n@@ -438,6 +440,13 @@ private static boolean isAcid(Long writeId) {\n     return tableDesc.getCreateTableTask(x.getInputs(), x.getOutputs(), x.getConf());\n   }\n \n+  private static Task<?> dropTableTask(Table table, EximUtil.SemanticAnalyzerWrapperContext x,\n+                                       ReplicationSpec replicationSpec) {\n+    DropTableDesc dropTblDesc = new DropTableDesc(table.getTableName(), table.getTableType(),\n+            true, false, replicationSpec);\n+    return TaskFactory.get(new DDLWork(x.getInputs(), x.getOutputs(), dropTblDesc), x.getConf());\n+  }\n+\n   private static Task<? extends Serializable> alterTableTask(ImportTableDesc tableDesc,\n       EximUtil.SemanticAnalyzerWrapperContext x, ReplicationSpec replicationSpec) {\n     tableDesc.setReplaceMode(true);\n@@ -912,7 +921,7 @@ private static void createReplImportTasks(\n       UpdatedMetaDataTracker updatedMetadata)\n       throws HiveException, URISyntaxException, IOException, MetaException {\n \n-    Task<?> dr = null;\n+    Task<?> dropTblTask = null;\n     WriteEntity.WriteType lockType = WriteEntity.WriteType.DDL_NO_LOCK;\n \n     // Normally, on import, trying to create a table or a partition in a db that does not yet exist\n@@ -934,6 +943,15 @@ private static void createReplImportTasks(\n                 tblDesc.getDatabaseName(), tblDesc.getTableName());\n         return;\n       }\n+\n+      // If the table exists and we found a valid create table event, then need to drop the table first\n+      // and then create it. This case is possible if the event sequence is drop_table(t1) -> create_table(t1).\n+      // We need to drop here to handle the case where the previous incremental load created the table but\n+      // didn't set the last repl ID due to some failure.\n+      if (x.getEventType() == DumpType.EVENT_CREATE_TABLE) {\n+        dropTblTask = dropTableTask(table, x, replicationSpec);\n+        table = null;\n+      }\n     } else {\n       // If table doesn't exist, allow creating a new one only if the database state is older than the update.\n       if ((parentDb != null) && (!replicationSpec.allowReplacementInto(parentDb.getParameters()))) {\n@@ -1000,8 +1018,15 @@ private static void createReplImportTasks(\n           t.addDependentTask(loadTable(fromURI, table, true, new Path(tblDesc.getLocation()), replicationSpec, x, writeId, stmtId, isSourceMm));\n         }\n       }\n-      // Simply create\n-      x.getTasks().add(t);\n+\n+      if (dropTblTask != null) {\n+        // Drop first and then create\n+        dropTblTask.addDependentTask(t);\n+        x.getTasks().add(dropTblTask);\n+      } else {\n+        // Simply create\n+        x.getTasks().add(t);\n+      }\n     } else {\n       // Table existed, and is okay to replicate into, not dropping and re-creating.\n       if (table.isPartitioned()) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java", "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java", "sha": "7f6e80a7d156d7940fe4e24481b6a8b9abd80866", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java?ref=d9fae049305e20ec8a72e581a2fc938028523402", "patch": "@@ -36,6 +36,7 @@\n           new EximUtil.SemanticAnalyzerWrapperContext(\n               context.hiveConf, context.db, readEntitySet, writeEntitySet, importTasks, context.log,\n               context.nestedContext);\n+      x.setEventType(context.dmd.getDumpType());\n \n       // REPL LOAD is not partition level. It is always DB or table level. So, passing null for partition specs.\n       // Also, REPL LOAD doesn't support external table and hence no location set as well.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/bd21f890e21f3baa8b715cb4203405de65b2a30b", "parent": "https://github.com/apache/hive/commit/5a9a328a8129eb8bd116158e06cf37a259cf32c6", "message": "HIVE-19884 : Invalidation cache may throw NPE when there is no data in table used by materialized view (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_42", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/itests/src/test/resources/testconfiguration.properties", "sha": "3ed2cf398a8edde959ccec75d51878724518a72b", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b", "patch": "@@ -555,6 +555,7 @@ minillaplocal.query.files=\\\n   materialized_view_describe.q,\\\n   materialized_view_drop.q,\\\n   materialized_view_rebuild.q,\\\n+  materialized_view_rewrite_empty.q,\\\n   materialized_view_rewrite_1.q,\\\n   materialized_view_rewrite_2.q,\\\n   materialized_view_rewrite_3.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 28, "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q", "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q", "sha": "e5daa8dc7820752b4d55cc92f5a6b56bfd6706a4", "changes": 28, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b", "patch": "@@ -0,0 +1,28 @@\n+-- SORT_QUERY_RESULTS\n+\n+set hive.vectorized.execution.enabled=false;\n+set hive.support.concurrency=true;\n+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n+set hive.strict.checks.cartesian.product=false;\n+set hive.stats.fetch.column.stats=true;\n+set hive.materializedview.rewriting=true;\n+\n+create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true');\n+analyze table emps_mv_rewrite_empty compute statistics for columns;\n+\n+create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150;\n+\n+explain\n+select * from emps_mv_rewrite_empty where empid < 120;\n+\n+select * from emps_mv_rewrite_empty where empid < 120;\n+\n+drop materialized view emps_mv_rewrite_empty_mv1;\n+drop table emps_mv_rewrite_empty;", "filename": "ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q"}, {"additions": 89, "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out", "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out", "sha": "b33d8c3f2d3b0e9eae01b7b878b564350b1a5961", "changes": 89, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b", "patch": "@@ -0,0 +1,89 @@\n+PREHOOK: query: create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true')\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+POSTHOOK: query: create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true')\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@emps_mv_rewrite_empty\n+PREHOOK: query: analyze table emps_mv_rewrite_empty compute statistics for columns\n+PREHOOK: type: ANALYZE_TABLE\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table emps_mv_rewrite_empty compute statistics for columns\n+POSTHOOK: type: ANALYZE_TABLE\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+PREHOOK: query: create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150\n+PREHOOK: type: CREATE_MATERIALIZED_VIEW\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: query: create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150\n+POSTHOOK: type: CREATE_MATERIALIZED_VIEW\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: query: explain\n+select * from emps_mv_rewrite_empty where empid < 120\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select * from emps_mv_rewrite_empty where empid < 120\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: emps_mv_rewrite_empty\n+          Filter Operator\n+            predicate: (empid < 120) (type: boolean)\n+            Select Operator\n+              expressions: empid (type: int), deptno (type: int), name (type: varchar(256)), salary (type: float), commission (type: int)\n+              outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+              ListSink\n+\n+PREHOOK: query: select * from emps_mv_rewrite_empty where empid < 120\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from emps_mv_rewrite_empty where empid < 120\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+PREHOOK: query: drop materialized view emps_mv_rewrite_empty_mv1\n+PREHOOK: type: DROP_MATERIALIZED_VIEW\n+PREHOOK: Input: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: query: drop materialized view emps_mv_rewrite_empty_mv1\n+POSTHOOK: type: DROP_MATERIALIZED_VIEW\n+POSTHOOK: Input: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: query: drop table emps_mv_rewrite_empty\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+POSTHOOK: query: drop table emps_mv_rewrite_empty\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: default@emps_mv_rewrite_empty", "filename": "ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java", "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java", "sha": "fc644f0b637616a670a3585dc4978fb917828747", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b", "patch": "@@ -360,6 +360,15 @@ private void enrichWithInvalidationInfo(Materialization materialization) {\n \n       final ConcurrentSkipListMap<Long, Long> usedTableModifications =\n           tableModifications.get(qNameTableUsed);\n+      if (usedTableModifications == null) {\n+        // This is not necessarily an error, since the table may be empty. To be safe,\n+        // instead of including this materialized view, we just log the information and\n+        // skip it (if table is really empty, it will not matter for performance anyway).\n+        LOG.warn(\"No information found in invalidation cache for table {}, possible tables are: {}\",\n+            qNameTableUsed, tableModifications.keySet());\n+        materialization.setInvalidationTime(Long.MIN_VALUE);\n+        return;\n+      }\n       final ConcurrentSkipListSet<Long> usedUDTableModifications =\n           updateDeleteTableModifications.get(qNameTableUsed);\n       final Entry<Long, Long> tn = usedTableModifications.higherEntry(tableMaterializationTxnList.getHighWatermark());", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java"}, {"additions": 34, "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "sha": "361ede54efcd739516285e7b6a85e1122e8b870a", "changes": 53, "status": "modified", "deletions": 19, "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b", "patch": "@@ -838,11 +838,7 @@ public void commitTxn(CommitTxnRequest rqst)\n     throws NoSuchTxnException, TxnAbortedException, MetaException {\n     MaterializationsRebuildLockHandler materializationsRebuildLockHandler =\n         MaterializationsRebuildLockHandler.get();\n-    String fullyQualifiedName = null;\n-    String dbName = null;\n-    String tblName = null;\n-    long writeId = 0L;\n-    long timestamp = 0L;\n+    List<TransactionRegistryInfo> txnComponents = new ArrayList<>();\n     boolean isUpdateDelete = false;\n     long txnid = rqst.getTxnid();\n     long sourceTxnId = -1;\n@@ -1007,12 +1003,10 @@ public void commitTxn(CommitTxnRequest rqst)\n         s = \"select ctc_database, ctc_table, ctc_writeid, ctc_timestamp from COMPLETED_TXN_COMPONENTS where ctc_txnid = \" + txnid;\n         LOG.debug(\"Going to extract table modification information for invalidation cache <\" + s + \">\");\n         rs = stmt.executeQuery(s);\n-        if (rs.next()) {\n-          dbName = rs.getString(1);\n-          tblName = rs.getString(2);\n-          fullyQualifiedName = Warehouse.getQualifiedName(dbName, tblName);\n-          writeId = rs.getLong(3);\n-          timestamp = rs.getTimestamp(4, Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"))).getTime();\n+        while (rs.next()) {\n+          // We only enter in this loop if the transaction actually affected any table\n+          txnComponents.add(new TransactionRegistryInfo(rs.getString(1), rs.getString(2),\n+              rs.getLong(3), rs.getTimestamp(4, Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"))).getTime()));\n         }\n         s = \"delete from TXN_COMPONENTS where tc_txnid = \" + txnid;\n         LOG.debug(\"Going to execute update <\" + s + \">\");\n@@ -1042,18 +1036,22 @@ public void commitTxn(CommitTxnRequest rqst)\n \n         MaterializationsInvalidationCache materializationsInvalidationCache =\n             MaterializationsInvalidationCache.get();\n-        if (materializationsInvalidationCache.containsMaterialization(dbName, tblName) &&\n-            !materializationsRebuildLockHandler.readyToCommitResource(dbName, tblName, txnid)) {\n-          throw new MetaException(\n-              \"Another process is rebuilding the materialized view \" + fullyQualifiedName);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          if (materializationsInvalidationCache.containsMaterialization(info.dbName, info.tblName) &&\n+              !materializationsRebuildLockHandler.readyToCommitResource(info.dbName, info.tblName, txnid)) {\n+            throw new MetaException(\n+                \"Another process is rebuilding the materialized view \" + info.fullyQualifiedName);\n+          }\n         }\n         LOG.debug(\"Going to commit\");\n         close(rs);\n         dbConn.commit();\n \n         // Update registry with modifications\n-        materializationsInvalidationCache.notifyTableModification(\n-            dbName, tblName, writeId, timestamp, isUpdateDelete);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          materializationsInvalidationCache.notifyTableModification(\n+              info.dbName, info.tblName, info.writeId, info.timestamp, isUpdateDelete);\n+        }\n       } catch (SQLException e) {\n         LOG.debug(\"Going to rollback\");\n         rollbackDBConn(dbConn);\n@@ -1064,8 +1062,8 @@ public void commitTxn(CommitTxnRequest rqst)\n         close(commitIdRs);\n         close(lockHandle, stmt, dbConn);\n         unlockInternal();\n-        if (fullyQualifiedName != null) {\n-          materializationsRebuildLockHandler.unlockResource(dbName, tblName, txnid);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          materializationsRebuildLockHandler.unlockResource(info.dbName, info.tblName, txnid);\n         }\n       }\n     } catch (RetryException e) {\n@@ -4783,4 +4781,21 @@ public boolean isWrapperFor(Class<?> iface) throws SQLException {\n       throw new UnsupportedOperationException();\n     }\n   };\n+\n+  private class TransactionRegistryInfo {\n+    final String dbName;\n+    final String tblName;\n+    final String fullyQualifiedName;\n+    final long writeId;\n+    final long timestamp;\n+\n+    public TransactionRegistryInfo (String dbName, String tblName, long writeId, long timestamp) {\n+      this.dbName = dbName;\n+      this.tblName = tblName;\n+      this.fullyQualifiedName = Warehouse.getQualifiedName(dbName, tblName);\n+      this.writeId = writeId;\n+      this.timestamp = timestamp;\n+    }\n+  }\n+\n }", "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5", "parent": "https://github.com/apache/hive/commit/dec3625b94d164c66087e115942a875a83590149", "message": "HIVE-18975: NPE when inserting NULL value in structure and array with HBase table (Oleksiy Sayankin, reviewed by Ashutosh Chauhan and Yongzhi Chen)", "bug_id": "hive_43", "file": [{"additions": 68, "raw_url": "https://github.com/apache/hive/raw/8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java", "blob_url": "https://github.com/apache/hive/blob/8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java", "sha": "bc4e1466f513bc1d009f6ce64ce24e1cae592e44", "changes": 136, "status": "modified", "deletions": 68, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java?ref=8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5", "patch": "@@ -226,83 +226,83 @@ private void serializeField(\n     return output.toByteArray();\n   }\n \n-  private boolean serialize(\n-      Object obj,\n-      ObjectInspector objInspector,\n-      int level, ByteStream.Output ss) throws IOException {\n+  private boolean serialize(Object obj, ObjectInspector objInspector, int level, ByteStream.Output ss)\n+      throws IOException {\n \n     switch (objInspector.getCategory()) {\n-      case PRIMITIVE:\n-        LazyUtils.writePrimitiveUTF8(ss, obj,\n-            (PrimitiveObjectInspector) objInspector, escaped, escapeChar, needsEscape);\n-        return true;\n-      case LIST:\n-        char separator = (char) separators[level];\n-        ListObjectInspector loi = (ListObjectInspector)objInspector;\n-        List<?> list = loi.getList(obj);\n-        ObjectInspector eoi = loi.getListElementObjectInspector();\n-        if (list == null) {\n-          return false;\n-        } else {\n-          for (int i = 0; i < list.size(); i++) {\n-            if (i > 0) {\n-              ss.write(separator);\n-            }\n-            serialize(list.get(i), eoi, level + 1, ss);\n+    case PRIMITIVE:\n+      LazyUtils.writePrimitiveUTF8(ss, obj, (PrimitiveObjectInspector) objInspector, escaped, escapeChar, needsEscape);\n+      return true;\n+    case LIST:\n+      char separator = (char) separators[level];\n+      ListObjectInspector loi = (ListObjectInspector) objInspector;\n+      List<?> list = loi.getList(obj);\n+      ObjectInspector eoi = loi.getListElementObjectInspector();\n+      if (list == null) {\n+        return false;\n+      } else {\n+        for (int i = 0; i < list.size(); i++) {\n+          if (i > 0) {\n+            ss.write(separator);\n           }\n-        }\n-        return true;\n-      case MAP:\n-        char sep = (char) separators[level];\n-        char keyValueSeparator = (char) separators[level+1];\n-        MapObjectInspector moi = (MapObjectInspector) objInspector;\n-        ObjectInspector koi = moi.getMapKeyObjectInspector();\n-        ObjectInspector voi = moi.getMapValueObjectInspector();\n-\n-        Map<?, ?> map = moi.getMap(obj);\n-        if (map == null) {\n-          return false;\n-        } else {\n-          boolean first = true;\n-          for (Map.Entry<?, ?> entry: map.entrySet()) {\n-            if (first) {\n-              first = false;\n-            } else {\n-              ss.write(sep);\n-            }\n-            serialize(entry.getKey(), koi, level+2, ss);\n-\n-            if ( entry.getValue() != null) {\n-              ss.write(keyValueSeparator);\n-              serialize(entry.getValue(), voi, level+2, ss);\n-            }\n+          Object currentItem = list.get(i);\n+          if (currentItem != null) {\n+            serialize(currentItem, eoi, level + 1, ss);\n           }\n         }\n-        return true;\n-      case STRUCT:\n-        sep = (char)separators[level];\n-        StructObjectInspector soi = (StructObjectInspector)objInspector;\n-        List<? extends StructField> fields = soi.getAllStructFieldRefs();\n-        list = soi.getStructFieldsDataAsList(obj);\n-        if (list == null) {\n-          return false;\n-        } else {\n-          for (int i = 0; i < list.size(); i++) {\n-            if (i > 0) {\n-              ss.write(sep);\n-            }\n+      }\n+      return true;\n+    case MAP:\n+      char sep = (char) separators[level];\n+      char keyValueSeparator = (char) separators[level + 1];\n+      MapObjectInspector moi = (MapObjectInspector) objInspector;\n+      ObjectInspector koi = moi.getMapKeyObjectInspector();\n+      ObjectInspector voi = moi.getMapValueObjectInspector();\n \n-            serialize(list.get(i), fields.get(i).getFieldObjectInspector(),\n-                level + 1, ss);\n+      Map<?, ?> map = moi.getMap(obj);\n+      if (map == null) {\n+        return false;\n+      } else {\n+        boolean first = true;\n+        for (Map.Entry<?, ?> entry : map.entrySet()) {\n+          if (first) {\n+            first = false;\n+          } else {\n+            ss.write(sep);\n+          }\n+          serialize(entry.getKey(), koi, level + 2, ss);\n+          Object currentValue = entry.getValue();\n+          if (currentValue != null) {\n+            ss.write(keyValueSeparator);\n+            serialize(currentValue, voi, level + 2, ss);\n           }\n         }\n-        return true;\n-       case UNION: {\n-        // union type currently not totally supported. See HIVE-2390\n+      }\n+      return true;\n+    case STRUCT:\n+      sep = (char) separators[level];\n+      StructObjectInspector soi = (StructObjectInspector) objInspector;\n+      List<? extends StructField> fields = soi.getAllStructFieldRefs();\n+      list = soi.getStructFieldsDataAsList(obj);\n+      if (list == null) {\n         return false;\n-       }\n-      default:\n-        throw new RuntimeException(\"Unknown category type: \" + objInspector.getCategory());\n+      } else {\n+        for (int i = 0; i < list.size(); i++) {\n+          if (i > 0) {\n+            ss.write(sep);\n+          }\n+          Object currentItem = list.get(i);\n+          if (currentItem != null) {\n+            serialize(currentItem, fields.get(i).getFieldObjectInspector(), level + 1, ss);\n+          }\n+        }\n+      }\n+      return true;\n+    case UNION:\n+      // union type currently not totally supported. See HIVE-2390\n+      return false;\n+    default:\n+      throw new RuntimeException(\"Unknown category type: \" + objInspector.getCategory());\n     }\n   }\n }", "filename": "hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5/hbase-handler/src/test/queries/positive/hbase_queries.q", "blob_url": "https://github.com/apache/hive/blob/8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5/hbase-handler/src/test/queries/positive/hbase_queries.q", "sha": "4604f3e6f59a7b5d707f4e6e7e7fa09c476262b5", "changes": 16, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/queries/positive/hbase_queries.q?ref=8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5", "patch": "@@ -189,6 +189,20 @@ insert into table hbase_table_10 select 5 as id, map(50,cast(null as int), 55, 5\n select * from hbase_table_10;\n \n \n+DROP TABLE IF EXISTS hbase_table_11;\n+CREATE TABLE hbase_table_11(id INT, map_column STRUCT<s_int:INT,s_string:STRING,s_date:DATE>)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,id:id');\n+INSERT INTO hbase_table_11 SELECT 2,NAMED_STRUCT(\"s_int\",CAST(NULL AS INT),\"s_string\",\"s1\",\"s_date\",CAST('2018-03-12' AS DATE)) FROM src LIMIT 1;\n+select * from hbase_table_11;\n+\n+DROP TABLE IF EXISTS hbase_table_12;\n+CREATE TABLE hbase_table_12(id INT, list_column ARRAY <STRING>)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,id:id');\n+INSERT INTO hbase_table_12 SELECT 2, ARRAY(\"a\", CAST (NULL AS STRING),  \"b\") FROM src LIMIT 1;\n+select * from hbase_table_12;\n+\n DROP TABLE hbase_table_1;\n DROP TABLE hbase_table_1_like;\n DROP TABLE hbase_table_2;\n@@ -203,3 +217,5 @@ DROP TABLE empty_hbase_table;\n DROP TABLE empty_normal_table;\n DROP TABLE hbase_table_9;\n DROP TABLE hbase_table_10;\n+DROP TABLE hbase_table_11;\n+DROP TABLE hbase_table_12;", "filename": "hbase-handler/src/test/queries/positive/hbase_queries.q"}, {"additions": 82, "raw_url": "https://github.com/apache/hive/raw/8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5/hbase-handler/src/test/results/positive/hbase_queries.q.out", "blob_url": "https://github.com/apache/hive/blob/8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5/hbase-handler/src/test/results/positive/hbase_queries.q.out", "sha": "41e29f9afbab602bcd202d58c367842682472231", "changes": 82, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/results/positive/hbase_queries.q.out?ref=8f804af05389edd7d5fa91e21ab9bdd71cc2c7a5", "patch": "@@ -1095,6 +1095,72 @@ POSTHOOK: Input: default@hbase_table_10\n 3\t{30:31}\t1234\n 4\t{40:null,45:null}\t1234\n 5\t{50:null,55:58}\t1234\n+PREHOOK: query: DROP TABLE IF EXISTS hbase_table_11\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS hbase_table_11\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE hbase_table_11(id INT, map_column STRUCT<s_int:INT,s_string:STRING,s_date:DATE>)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,id:id')\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@hbase_table_11\n+POSTHOOK: query: CREATE TABLE hbase_table_11(id INT, map_column STRUCT<s_int:INT,s_string:STRING,s_date:DATE>)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,id:id')\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@hbase_table_11\n+PREHOOK: query: INSERT INTO hbase_table_11 SELECT 2,NAMED_STRUCT(\"s_int\",CAST(NULL AS INT),\"s_string\",\"s1\",\"s_date\",CAST('2018-03-12' AS DATE)) FROM src LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_11\n+POSTHOOK: query: INSERT INTO hbase_table_11 SELECT 2,NAMED_STRUCT(\"s_int\",CAST(NULL AS INT),\"s_string\",\"s1\",\"s_date\",CAST('2018-03-12' AS DATE)) FROM src LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_11\n+PREHOOK: query: select * from hbase_table_11\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@hbase_table_11\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from hbase_table_11\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@hbase_table_11\n+#### A masked pattern was here ####\n+2\t{\"s_int\":null,\"s_string\":\"s1\",\"s_date\":\"2018-03-12\"}\n+PREHOOK: query: DROP TABLE IF EXISTS hbase_table_12\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS hbase_table_12\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE hbase_table_12(id INT, list_column ARRAY <STRING>)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,id:id')\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@hbase_table_12\n+POSTHOOK: query: CREATE TABLE hbase_table_12(id INT, list_column ARRAY <STRING>)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES ('hbase.columns.mapping'=':key,id:id')\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@hbase_table_12\n+PREHOOK: query: INSERT INTO hbase_table_12 SELECT 2, ARRAY(\"a\", CAST (NULL AS STRING),  \"b\") FROM src LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_12\n+POSTHOOK: query: INSERT INTO hbase_table_12 SELECT 2, ARRAY(\"a\", CAST (NULL AS STRING),  \"b\") FROM src LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_12\n+PREHOOK: query: select * from hbase_table_12\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@hbase_table_12\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from hbase_table_12\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@hbase_table_12\n+#### A masked pattern was here ####\n+2\t[\"a\",\"\",\"b\"]\n PREHOOK: query: DROP TABLE hbase_table_1\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@hbase_table_1\n@@ -1207,3 +1273,19 @@ POSTHOOK: query: DROP TABLE hbase_table_10\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@hbase_table_10\n POSTHOOK: Output: default@hbase_table_10\n+PREHOOK: query: DROP TABLE hbase_table_11\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@hbase_table_11\n+PREHOOK: Output: default@hbase_table_11\n+POSTHOOK: query: DROP TABLE hbase_table_11\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@hbase_table_11\n+POSTHOOK: Output: default@hbase_table_11\n+PREHOOK: query: DROP TABLE hbase_table_12\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@hbase_table_12\n+PREHOOK: Output: default@hbase_table_12\n+POSTHOOK: query: DROP TABLE hbase_table_12\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@hbase_table_12\n+POSTHOOK: Output: default@hbase_table_12", "filename": "hbase-handler/src/test/results/positive/hbase_queries.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c2e335fc0b4a8144d8d93ff10e9191432ae6547e", "parent": "https://github.com/apache/hive/commit/464a3f61a0c4a1c4e44a1ce427f604295534e969", "message": "HIVE-18606 CTAS on empty table throws NPE from org.apache.hadoop.hive.ql.exec.MoveTask (Eugene Koifman, reviewed by Sergey Shelukhin)", "bug_id": "hive_44", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java", "blob_url": "https://github.com/apache/hive/blob/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java", "sha": "4e804ba04b2ee916f7867a8a4a439d7c467f2523", "changes": 9, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java?ref=c2e335fc0b4a8144d8d93ff10e9191432ae6547e", "patch": "@@ -294,8 +294,13 @@ public int execute(DriverContext driverContext) {\n             //'sourcePath' result of 'select ...' part of CTAS statement\n             assert lfd.getIsDfsDir();\n             FileSystem srcFs = sourcePath.getFileSystem(conf);\n-            List<Path> newFiles = new ArrayList<>();\n-            Hive.moveAcidFiles(srcFs, srcFs.globStatus(sourcePath), targetPath, newFiles);\n+            FileStatus[] srcs = srcFs.globStatus(sourcePath);\n+            if(srcs != null) {\n+              List<Path> newFiles = new ArrayList<>();\n+              Hive.moveAcidFiles(srcFs, srcs, targetPath, newFiles);\n+            } else {\n+              LOG.debug(\"No files found to move from \" + sourcePath + \" to \" + targetPath);\n+            }\n           }\n           else {\n             moveFile(sourcePath, targetPath, lfd.getIsDfsDir());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java", "blob_url": "https://github.com/apache/hive/blob/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java", "sha": "3c6b6be1ba6c5c4b522f31e90dda0b63d0bb8d3a", "changes": 11, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java?ref=c2e335fc0b4a8144d8d93ff10e9191432ae6547e", "patch": "@@ -178,6 +178,7 @@ public void testNoBuckets() throws Exception {\n    */\n   @Test\n   public void testCTAS() throws Exception {\n+    runStatementOnDriver(\"drop table if exists myctas\");\n     int[][] values = {{1,2},{3,4}};\n     runStatementOnDriver(\"insert into \" + Table.NONACIDORCTBL +  makeValuesClause(values));\n     runStatementOnDriver(\"create table myctas stored as ORC TBLPROPERTIES ('transactional\" +\n@@ -221,6 +222,16 @@ public void testCTAS() throws Exception {\n     };\n     checkExpected(rs, expected4, \"Unexpected row count after ctas from union distinct query\");\n   }\n+  @Test\n+  public void testCtasEmpty() throws Exception {\n+    MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);\n+    runStatementOnDriver(\"drop table if exists myctas\");\n+    runStatementOnDriver(\"create table myctas stored as ORC as\" +\n+        \" select a, b from \" + Table.NONACIDORCTBL);\n+    List<String> rs = runStatementOnDriver(\"select ROW__ID, a, b, INPUT__FILE__NAME\" +\n+        \" from myctas order by ROW__ID\");\n+  }\n+\n   /**\n    * Insert into unbucketed acid table from union all query\n    * Union All is flattend so nested subdirs are created and acid move drops them since", "filename": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/583a9511ba8809d81595a5fa4da32ed2c2f8912e", "parent": "https://github.com/apache/hive/commit/c5b3ccc41016afd94035637cb011eacbeb9e5893", "message": "HIVE-17535 Select 1 EXCEPT Select 1 fails with NPE (Vineet Garg,reviewed by Ashutosh Chauhan)", "bug_id": "hive_45", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/itests/src/test/resources/testconfiguration.properties", "sha": "efa690db10836b0c21723bfe51adb22f4ab53bac", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -30,6 +30,7 @@ disabled.query.files=ql_rewrite_gbtoidx.q,\\\n   cbo_rp_subq_not_in.q,\\\n   cbo_rp_subq_exists.q,\\\n   orc_llap.q,\\\n+  min_structvalue.q,\\\n   ql_rewrite_gbtoidx_cbo_2.q,\\\n   rcfile_merge1.q,\\\n   smb_mapjoin_8.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "sha": "28953b9d030f33d1c26789453b97335f4e848a5b", "changes": 19, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -709,7 +709,7 @@ boolean canCBOHandleAst(ASTNode ast, QB qb, PreCboCtx cboCtx) {\n     boolean isSupportedRoot = root == HiveParser.TOK_QUERY || root == HiveParser.TOK_EXPLAIN\n         || qb.isCTAS() || qb.isMaterializedView();\n     // Queries without a source table currently are not supported by CBO\n-    boolean isSupportedType = (qb.getIsQuery() && !qb.containsQueryWithoutSourceTable())\n+    boolean isSupportedType = (qb.getIsQuery())\n         || qb.isCTAS() || qb.isMaterializedView() || cboCtx.type == PreCboCtx.Type.INSERT\n         || cboCtx.type == PreCboCtx.Type.MULTI_INSERT;\n     boolean noBadTokens = HiveCalciteUtil.validateASTForUnsupportedTokens(ast);\n@@ -4164,18 +4164,11 @@ private RelNode genLogicalPlan(QB qb, boolean outerMostQB,\n \n       if (aliasToRel.isEmpty()) {\n         // // This may happen for queries like select 1; (no source table)\n-        // We can do following which is same, as what Hive does.\n-        // With this, we will be able to generate Calcite plan.\n-        // qb.getMetaData().setSrcForAlias(DUMMY_TABLE, getDummyTable());\n-        // RelNode op = genTableLogicalPlan(DUMMY_TABLE, qb);\n-        // qb.addAlias(DUMMY_TABLE);\n-        // qb.setTabAlias(DUMMY_TABLE, DUMMY_TABLE);\n-        // aliasToRel.put(DUMMY_TABLE, op);\n-        // However, Hive trips later while trying to get Metadata for this dummy\n-        // table\n-        // So, for now lets just disable this. Anyway there is nothing much to\n-        // optimize in such cases.\n-        throw new CalciteSemanticException(\"Unsupported\", UnsupportedFeature.Others);\n+        qb.getMetaData().setSrcForAlias(DUMMY_TABLE, getDummyTable());\n+        qb.addAlias(DUMMY_TABLE);\n+        qb.setTabAlias(DUMMY_TABLE, DUMMY_TABLE);\n+        RelNode op = genTableLogicalPlan(DUMMY_TABLE, qb);\n+        aliasToRel.put(DUMMY_TABLE, op);\n \n       }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "d56fd21c63cdce35613c02b115d3f2c4dcaca08e", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -2019,6 +2019,9 @@ private void getMetaData(QB qb, ReadEntity parentInput)\n       }\n \n       if (tab == null) {\n+        if(tabName.equals(DUMMY_DATABASE + \".\" + DUMMY_TABLE)) {\n+          continue;\n+        }\n         ASTNode src = qb.getParseInfo().getSrcForAlias(alias);\n         if (null != src) {\n           throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(src));\n@@ -10611,6 +10614,9 @@ public Operator genPlan(QB qb, boolean skipAmbiguityCheck)\n \n     // Recurse over all the source tables\n     for (String alias : qb.getTabAliases()) {\n+      if(alias.equals(DUMMY_TABLE)) {\n+        continue;\n+      }\n       Operator op = genTablePlan(alias, qb);\n       aliasToOpInfo.put(alias, op);\n     }\n@@ -10738,7 +10744,7 @@ private void rewriteRRForSubQ(String alias, Operator operator, boolean skipAmbig\n     opParseCtx.get(operator).setRowResolver(newRR);\n   }\n \n-  private Table getDummyTable() throws SemanticException {\n+  protected Table getDummyTable() throws SemanticException {\n     Path dummyPath = createDummyFile();\n     Table desc = new Table(DUMMY_DATABASE, DUMMY_TABLE);\n     desc.getTTable().getSd().setLocation(dummyPath.toString());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_missing_from.q", "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_missing_from.q", "sha": "3b49ac6a0a8f00dbc26a4071042d6e05574f0767", "changes": 1, "status": "removed", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/subquery_missing_from.q?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893", "patch": "@@ -1 +0,0 @@\n-select * from src where src.key in (select key);\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientnegative/subquery_missing_from.q"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_select_no_source.q", "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_select_no_source.q", "sha": "75cae51e6af638c5bca38c983bf7dc7161c30fee", "changes": 2, "status": "removed", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/subquery_select_no_source.q?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893", "patch": "@@ -1,2 +0,0 @@\n--- since CBO doesn't allow such queries we can not support subqueries here\n-explain select (select max(p_size) from part);", "filename": "ql/src/test/queries/clientnegative/subquery_select_no_source.q"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_missing_from.q.out", "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_missing_from.q.out", "sha": "b09a8e311f5e7b2ef0c35b28ade81dfb4033fed3", "changes": 3, "status": "removed", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/subquery_missing_from.q.out?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893", "patch": "@@ -1,3 +0,0 @@\n-FAILED: SemanticException Line 0:-1 Invalid SubQuery expression 'key' in definition of SubQuery sq_1 [\n-src.key in (select key)\n-] used as sq_1 at Line 0:-1: From clause is missing in SubQuery.", "filename": "ql/src/test/results/clientnegative/subquery_missing_from.q.out"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_select_no_source.q.out", "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_select_no_source.q.out", "sha": "37c4e57813a61d4f6832df610265962fffc92240", "changes": 1, "status": "removed", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/subquery_select_no_source.q.out?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893", "patch": "@@ -1 +0,0 @@\n-FAILED: CalciteSubquerySemanticException [Error 10249]: Unsupported SubQuery Expression  Currently SubQuery expressions are only allowed as Where and Having Clause predicates", "filename": "ql/src/test/results/clientnegative/subquery_select_no_source.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/mapjoin2.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/mapjoin2.q.out", "sha": "7e7084160df06263534e64b490dcb20d810a26ec", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/beeline/mapjoin2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -53,6 +53,7 @@ POSTHOOK: Input: default@tbl\n #### A masked pattern was here ####\n false\tfalse\ttrue\ttrue\n true\ttrue\tfalse\tfalse\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -62,6 +63,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a left outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -71,6 +73,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a right outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -80,6 +83,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Shuffle Join JOIN[6][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a full outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table", "filename": "ql/src/test/results/clientpositive/beeline/mapjoin2.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out", "sha": "0b73e84e9aea6855e91fd547bfb7a973b55d6613", "changes": 22, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -89,13 +89,17 @@ STAGE PLANS:\n               UDTF Operator\n                 Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                 function name: explode\n-                File Output Operator\n-                  compressed: false\n+                Select Operator\n+                  expressions: col (type: string)\n+                  outputColumnNames: _col0\n                   Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n-                  table:\n-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -204,7 +208,11 @@ STAGE PLANS:\n             UDTF Operator\n               Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY", "filename": "ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/decimal_precision2.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/decimal_precision2.q.out", "sha": "4ce7e1cca7d43da1a88b153e6c6cdeeff5b2adf7", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_precision2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -37,9 +37,9 @@ STAGE PLANS:\n           Row Limit Per Split: 1\n           Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n           Select Operator\n-            expressions: 100 (type: decimal(3,0))\n+            expressions: 100 (type: int)\n             outputColumnNames: _col0\n-            Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE\n+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n             ListSink\n \n PREHOOK: query: explain select 0.000BD\n@@ -59,9 +59,9 @@ STAGE PLANS:\n           Row Limit Per Split: 1\n           Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n           Select Operator\n-            expressions: 0 (type: decimal(1,0))\n+            expressions: 0 (type: int)\n             outputColumnNames: _col0\n-            Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE\n+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n             ListSink\n \n PREHOOK: query: explain select 0.100BD\n@@ -147,7 +147,7 @@ STAGE PLANS:\n           Row Limit Per Split: 1\n           Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n           Select Operator\n-            expressions: 69.0212249755859375 (type: decimal(27,20))\n+            expressions: 69.0212249755859375 (type: decimal(18,16))\n             outputColumnNames: _col0\n             Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE\n             ListSink", "filename": "ql/src/test/results/clientpositive/decimal_precision2.q.out"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/explainuser_1.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/explainuser_1.q.out", "sha": "a47d791fad073baa52e6374252e0c734a9310b8c", "changes": 14, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/explainuser_1.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -5150,16 +5150,18 @@ PREHOOK: query: explain select explode(array('a', 'b'))\n PREHOOK: type: QUERY\n POSTHOOK: query: explain select explode(array('a', 'b'))\n POSTHOOK: type: QUERY\n-Plan not optimized by CBO.\n+Plan optimized by CBO.\n \n Stage-0\n   Fetch Operator\n     limit:-1\n-    UDTF Operator [UDTF_2]\n-      function name:explode\n-      Select Operator [SEL_1]\n-        Output:[\"_col0\"]\n-        TableScan [TS_0]\n+    Select Operator [SEL_3]\n+      Output:[\"_col0\"]\n+      UDTF Operator [UDTF_2]\n+        function name:explode\n+        Select Operator [SEL_1]\n+          Output:[\"_col0\"]\n+          TableScan [TS_0]\n \n PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE\n PREHOOK: type: CREATETABLE", "filename": "ql/src/test/results/clientpositive/llap/explainuser_1.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/mapjoin2.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/mapjoin2.q.out", "sha": "ce65c6ddbfb6b4cdb7c058a05c583835d0631e06", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/mapjoin2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -53,6 +53,7 @@ POSTHOOK: Input: default@tbl\n #### A masked pattern was here ####\n false\tfalse\ttrue\ttrue\n true\ttrue\tfalse\tfalse\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Map 1' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -62,6 +63,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Map 1' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a left outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -71,6 +73,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Map 2' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a right outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -80,6 +83,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Shuffle Join MERGEJOIN[9][tables = [$hdt$_0, $hdt$_1]] in Stage 'Reducer 2' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a full outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table", "filename": "ql/src/test/results/clientpositive/llap/mapjoin2.q.out"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/select_dummy_source.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/select_dummy_source.q.out", "sha": "b7f939fb8e8fd4138e447df9840583a4955ec035", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/select_dummy_source.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -82,7 +82,10 @@ STAGE PLANS:\n             outputColumnNames: _col0\n             UDTF Operator\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY\n@@ -178,7 +181,10 @@ STAGE PLANS:\n             outputColumnNames: _col0\n             UDTF Operator\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY", "filename": "ql/src/test/results/clientpositive/llap/select_dummy_source.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/mapjoin2.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/mapjoin2.q.out", "sha": "7e7084160df06263534e64b490dcb20d810a26ec", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/mapjoin2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -53,6 +53,7 @@ POSTHOOK: Input: default@tbl\n #### A masked pattern was here ####\n false\tfalse\ttrue\ttrue\n true\ttrue\tfalse\tfalse\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -62,6 +63,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a left outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -71,6 +73,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a right outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -80,6 +83,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Shuffle Join JOIN[6][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a full outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table", "filename": "ql/src/test/results/clientpositive/mapjoin2.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/select_dummy_source.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/select_dummy_source.q.out", "sha": "0b73e84e9aea6855e91fd547bfb7a973b55d6613", "changes": 22, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/select_dummy_source.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -89,13 +89,17 @@ STAGE PLANS:\n               UDTF Operator\n                 Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                 function name: explode\n-                File Output Operator\n-                  compressed: false\n+                Select Operator\n+                  expressions: col (type: string)\n+                  outputColumnNames: _col0\n                   Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n-                  table:\n-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -204,7 +208,11 @@ STAGE PLANS:\n             UDTF Operator\n               Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY", "filename": "ql/src/test/results/clientpositive/select_dummy_source.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/timestamptz_1.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/timestamptz_1.q.out", "sha": "b4ef3e41ad814de385e094a8bda12242203469b6", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/timestamptz_1.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -18,7 +18,7 @@ POSTHOOK: query: insert overwrite table tstz1 select cast('2016-01-03 12:26:34 A\n POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n POSTHOOK: Output: default@tstz1\n-POSTHOOK: Lineage: tstz1.t EXPRESSION []\n+POSTHOOK: Lineage: tstz1.t SIMPLE []\n PREHOOK: query: select cast(t as string) from tstz1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@tstz1", "filename": "ql/src/test/results/clientpositive/timestamptz_1.q.out"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/udtf_stack.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/udtf_stack.q.out", "sha": "3192a44e41d9063af2b17f7350795dc9d2834b19", "changes": 18, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udtf_stack.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -182,13 +182,17 @@ STAGE PLANS:\n               UDTF Operator\n                 Statistics: Num rows: 1 Data size: 185 Basic stats: COMPLETE Column stats: COMPLETE\n                 function name: stack\n-                File Output Operator\n-                  compressed: false\n-                  Statistics: Num rows: 1 Data size: 185 Basic stats: COMPLETE Column stats: COMPLETE\n-                  table:\n-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                Select Operator\n+                  expressions: col0 (type: string), col1 (type: string), null (type: void)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator", "filename": "ql/src/test/results/clientpositive/udtf_stack.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out", "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out", "sha": "2d86d8c70834b1dbae3adf3a1d10a2188ffcd385", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e", "patch": "@@ -251,7 +251,7 @@ STAGE PLANS:\n             Select Operator\n               Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n               Group By Operator\n-                aggregations: count(1)\n+                aggregations: count()\n                 Group By Vectorization:\n                     groupByMode: HASH\n                     vectorOutput: false", "filename": "ql/src/test/results/clientpositive/vector_tablesample_rows.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0410bf17a361514f88774cf0545ec07271a26ab8", "parent": "https://github.com/apache/hive/commit/bff9da2cc03da848189c7266ee57069dde3fe668", "message": "HIVE-17961 : NPE during initialization of VectorizedParquetRecordReader when input split is null (Vihang Karajgaonkar, reviewed by Ferdinand Xu)", "bug_id": "hive_46", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java", "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java", "sha": "e4d11fdb126e7687db70770179f8dcaf8d2f5b7b", "changes": 11, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8", "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.VectorizedColumnReaderTestBase;\n import org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport;\n import org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector;\n import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n@@ -332,15 +333,7 @@ public RecordReader getVectorizedRecordReader(Path inputPath) throws Exception {\n       // types.\n       conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n       conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0,1,2,3,6\");\n-      conf.set(ReadSupport.PARQUET_READ_SCHEMA, \"test schema\");\n-      HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, true);\n-      HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, \"//tmp\");\n-      Job vectorJob = new Job(conf, \"read vector\");\n-      ParquetInputFormat.setInputPaths(vectorJob, inputPath);\n-      ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);\n-      InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);\n-      initialVectorizedRowBatchCtx(conf);\n-      return new VectorizedParquetRecordReader(split, new JobConf(conf));\n+      return VectorizedColumnReaderTestBase.createTestParquetReader(\"test schema\", conf);\n     }\n   }\n ", "filename": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java", "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java", "sha": "1d9dba7842835b8024410ae8d15e9b6d2f159675", "changes": 22, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8", "patch": "@@ -118,20 +118,6 @@\n    */\n   protected long totalRowCount = 0;\n \n-  @VisibleForTesting\n-  public VectorizedParquetRecordReader(\n-    InputSplit inputSplit,\n-    JobConf conf) {\n-    try {\n-      serDeStats = new SerDeStats();\n-      projectionPusher = new ProjectionPusher();\n-      initialize(inputSplit, conf);\n-    } catch (Throwable e) {\n-      LOG.error(\"Failed to create the vectorized reader due to exception \" + e);\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n   public VectorizedParquetRecordReader(\n       org.apache.hadoop.mapred.InputSplit oldInputSplit, JobConf conf) {\n     this(oldInputSplit, conf, null, null, null);\n@@ -146,6 +132,10 @@ public VectorizedParquetRecordReader(\n       this.cacheConf = cacheConf;\n       serDeStats = new SerDeStats();\n       projectionPusher = new ProjectionPusher();\n+      colsToInclude = ColumnProjectionUtils.getReadColumnIDs(conf);\n+      //initialize the rowbatchContext\n+      jobConf = conf;\n+      rbCtx = Utilities.getVectorizedRowBatchCtx(jobConf);\n       ParquetInputSplit inputSplit = getSplit(oldInputSplit, conf);\n       if (inputSplit != null) {\n         initialize(inputSplit, conf);\n@@ -171,10 +161,6 @@ private void initPartitionValues(FileSplit fileSplit, JobConf conf) throws IOExc\n   public void initialize(\n     InputSplit oldSplit,\n     JobConf configuration) throws IOException, InterruptedException {\n-    colsToInclude = ColumnProjectionUtils.getReadColumnIDs(configuration);\n-    //initialize the rowbatchContext\n-    jobConf = configuration;\n-    rbCtx = Utilities.getVectorizedRowBatchCtx(jobConf);\n     // the oldSplit may be null during the split phase\n     if (oldSplit == null) {\n       return;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java"}, {"additions": 28, "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java", "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java", "sha": "81d8cffa85fabc4ccdb52a9ff6ece28b33a48ca5", "changes": 32, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8", "patch": "@@ -22,15 +22,21 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.io.IOConstants;\n import org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.FileSplit;\n import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.parquet.hadoop.ParquetInputFormat;\n+import org.apache.parquet.hadoop.ParquetInputSplit;\n import org.junit.AfterClass;\n+import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n import java.io.IOException;\n \n import static junit.framework.TestCase.assertFalse;\n+import static org.apache.parquet.hadoop.api.ReadSupport.PARQUET_READ_SCHEMA;\n \n public class TestVectorizedColumnReader extends VectorizedColumnReaderTestBase {\n   static boolean isDictionaryEncoding = false;\n@@ -97,16 +103,34 @@ public void decimalRead() throws Exception {\n     decimalRead(isDictionaryEncoding);\n   }\n \n+  private class TestVectorizedParquetRecordReader extends VectorizedParquetRecordReader {\n+    public TestVectorizedParquetRecordReader(\n+        org.apache.hadoop.mapred.InputSplit oldInputSplit, JobConf conf) {\n+      super(oldInputSplit, conf);\n+    }\n+    @Override\n+    protected ParquetInputSplit getSplit(\n+        org.apache.hadoop.mapred.InputSplit oldInputSplit, JobConf conf) {\n+      return null;\n+    }\n+  }\n+\n   @Test\n   public void testNullSplitForParquetReader() throws Exception {\n     Configuration conf = new Configuration();\n     conf.set(IOConstants.COLUMNS,\"int32_field\");\n     conf.set(IOConstants.COLUMNS_TYPES,\"int\");\n+    conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n+    conf.set(PARQUET_READ_SCHEMA, \"message test { required int32 int32_field;}\");\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, true);\n     HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, \"//tmp\");\n+    Job vectorJob = new Job(conf, \"read vector\");\n+    ParquetInputFormat.setInputPaths(vectorJob, file);\n     initialVectorizedRowBatchCtx(conf);\n-    VectorizedParquetRecordReader reader =\n-        new VectorizedParquetRecordReader((InputSplit)null, new JobConf(conf));\n-    assertFalse(reader.next(reader.createKey(), reader.createValue()));\n+    FileSplit fsplit = getFileSplit(vectorJob);\n+    JobConf jobConf = new JobConf(conf);\n+    TestVectorizedParquetRecordReader testReader = new TestVectorizedParquetRecordReader(fsplit, jobConf);\n+    Assert.assertNull(\"Test should return null split from getSplit() method\", testReader.getSplit(fsplit, jobConf));\n   }\n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java"}, {"additions": 22, "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java", "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java", "sha": "1a5d0952fa0ed1b85a1cba843d9537ef5b5d0c3c", "changes": 39, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8", "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n import org.apache.hadoop.mapreduce.InputSplit;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.Job;\n@@ -213,18 +214,22 @@ protected static boolean isNull(int index) {\n     return (index % NULL_FREQUENCY == 0);\n   }\n \n-  protected VectorizedParquetRecordReader createParquetReader(String schemaString, Configuration conf)\n+  public static VectorizedParquetRecordReader createTestParquetReader(String schemaString, Configuration conf)\n     throws IOException, InterruptedException, HiveException {\n     conf.set(PARQUET_READ_SCHEMA, schemaString);\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, true);\n     HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, \"//tmp\");\n-\n     Job vectorJob = new Job(conf, \"read vector\");\n     ParquetInputFormat.setInputPaths(vectorJob, file);\n+    initialVectorizedRowBatchCtx(conf);\n+    return new VectorizedParquetRecordReader(getFileSplit(vectorJob),new JobConf(conf));\n+  }\n+\n+  protected static FileSplit getFileSplit(Job vectorJob) throws IOException, InterruptedException {\n     ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);\n     InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);\n-    initialVectorizedRowBatchCtx(conf);\n-    return new VectorizedParquetRecordReader(split, new JobConf(conf));\n+    FileSplit fsplit = new FileSplit(file,0L,split.getLength(),split.getLocations());\n+    return fsplit;\n   }\n \n   protected static void writeData(ParquetWriter<Group> writer, boolean isDictionaryEncoding) throws IOException {\n@@ -295,7 +300,7 @@ protected static void writeData(ParquetWriter<Group> writer, boolean isDictionar\n     writer.close();\n   }\n \n-  protected void initialVectorizedRowBatchCtx(Configuration conf) throws HiveException {\n+  protected static void initialVectorizedRowBatchCtx(Configuration conf) throws HiveException {\n     MapWork mapWork = new MapWork();\n     VectorizedRowBatchCtx rbCtx = new VectorizedRowBatchCtx();\n     rbCtx.init(createStructObjectInspector(conf), new String[0]);\n@@ -304,7 +309,7 @@ protected void initialVectorizedRowBatchCtx(Configuration conf) throws HiveExcep\n     Utilities.setMapWork(conf, mapWork);\n   }\n \n-  private StructObjectInspector createStructObjectInspector(Configuration conf) {\n+  private static StructObjectInspector createStructObjectInspector(Configuration conf) {\n     // Create row related objects\n     String columnNames = conf.get(IOConstants.COLUMNS);\n     List<String> columnNamesList = DataWritableReadSupport.getColumnNames(columnNames);\n@@ -321,7 +326,7 @@ protected void intRead(boolean isDictionaryEncoding) throws InterruptedException\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required int32 int32_field;}\", conf);\n+      createTestParquetReader(\"message test { required int32 int32_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -350,7 +355,7 @@ protected void longRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required int64 int64_field;}\", conf);\n+      createTestParquetReader(\"message test { required int64 int64_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -379,7 +384,7 @@ protected void doubleRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required double double_field;}\", conf);\n+      createTestParquetReader(\"message test { required double double_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -409,7 +414,7 @@ protected void floatRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required float float_field;}\", conf);\n+      createTestParquetReader(\"message test { required float float_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -439,7 +444,7 @@ protected void booleanRead() throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required boolean boolean_field;}\", conf);\n+      createTestParquetReader(\"message test { required boolean boolean_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -468,7 +473,7 @@ protected void binaryRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required binary binary_field_some_null;}\", conf);\n+      createTestParquetReader(\"message test { required binary binary_field_some_null;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -511,7 +516,7 @@ protected void structRead(boolean isDictionaryEncoding) throws Exception {\n       + \"  optional double b;\\n\"\n       + \"}\\n\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -551,7 +556,7 @@ protected void nestedStructRead0(boolean isDictionaryEncoding) throws Exception\n       + \"  }\"\n       + \"optional double e;\\n\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -592,7 +597,7 @@ protected void nestedStructRead1(boolean isDictionaryEncoding) throws Exception\n       + \"    optional int32 c;\\n\"\n       + \"  }\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -628,7 +633,7 @@ protected void structReadSomeNull(boolean isDictionaryEncoding) throws Exception\n       + \"  optional int32 f;\\n\"\n       + \"  optional double g;\\n\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -669,7 +674,7 @@ protected void decimalRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message hive_schema { required value (DECIMAL(5,2));}\", conf);\n+      createTestParquetReader(\"message hive_schema { required value (DECIMAL(5,2));}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/queries/clientpositive/vectorization_parquet_projection.q", "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/queries/clientpositive/vectorization_parquet_projection.q", "sha": "2bcaa987015a98084e6178991e542d13c5d65455", "changes": 17, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/vectorization_parquet_projection.q?ref=0410bf17a361514f88774cf0545ec07271a26ab8", "patch": "@@ -77,3 +77,20 @@ group by m1[\"color\"];\n \n select m1[\"color\"], count(*) from parquet_project_test\n group by m1[\"color\"];\n+\n+\n+create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet;\n+\n+insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red');\n+\n+explain vectorization select count(*) from parquet_nullsplit where len = '1';\n+select count(*) from parquet_nullsplit where len = '1';\n+\n+explain vectorization select count(*) from parquet_nullsplit where len = '99';\n+select count(*) from parquet_nullsplit where len = '99';\n+\n+drop table parquet_nullsplit;\n+drop table parquet_project_test;\n+drop table parquet_types_staging;", "filename": "ql/src/test/queries/clientpositive/vectorization_parquet_projection.q"}, {"additions": 211, "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out", "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out", "sha": "53d059f16125cf6ad48e9bd398eae20f7984f7bf", "changes": 211, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out?ref=0410bf17a361514f88774cf0545ec07271a26ab8", "patch": "@@ -456,3 +456,214 @@ POSTHOOK: Input: default@parquet_project_test\n blue\t7\n green\t7\n red\t8\n+PREHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).key SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).val SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (GROUP, 1)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: parquet_nullsplit\n+                  Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count()\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+            Execution mode: vectorized\n+            Map Vectorization:\n+                enabled: true\n+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true\n+                inputFormatFeatureSupport: []\n+                featureSupportInUse: []\n+                inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+        Reducer 2 \n+            Execution mode: vectorized\n+            Reduce Vectorization:\n+                enabled: true\n+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+1\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (GROUP, 1)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: parquet_nullsplit\n+                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                  Filter Operator\n+                    predicate: (len = '99') (type: boolean)\n+                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                    Select Operator\n+                      Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                      Group By Operator\n+                        aggregations: count()\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        Reduce Output Operator\n+                          sort order: \n+                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                          value expressions: _col0 (type: bigint)\n+            Execution mode: vectorized\n+            Map Vectorization:\n+                enabled: true\n+                inputFormatFeatureSupport: []\n+                featureSupportInUse: []\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+        Reducer 2 \n+            Execution mode: vectorized\n+            Reduce Vectorization:\n+                enabled: true\n+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: drop table parquet_nullsplit\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: drop table parquet_nullsplit\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: drop table parquet_project_test\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_project_test\n+PREHOOK: Output: default@parquet_project_test\n+POSTHOOK: query: drop table parquet_project_test\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_project_test\n+POSTHOOK: Output: default@parquet_project_test\n+PREHOOK: query: drop table parquet_types_staging\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_types_staging\n+PREHOOK: Output: default@parquet_types_staging\n+POSTHOOK: query: drop table parquet_types_staging\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_types_staging\n+POSTHOOK: Output: default@parquet_types_staging", "filename": "ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out"}, {"additions": 193, "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out", "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out", "sha": "e167523cbe06b4a505fde7e3eeeef6378996d949", "changes": 193, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out?ref=0410bf17a361514f88774cf0545ec07271a26ab8", "patch": "@@ -426,3 +426,196 @@ POSTHOOK: Input: default@parquet_project_test\n blue\t7\n green\t7\n red\t8\n+PREHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).key SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).val SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: parquet_nullsplit\n+            Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+            Select Operator\n+              Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+              Group By Operator\n+                aggregations: count()\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: bigint)\n+      Execution mode: vectorized\n+      Map Vectorization:\n+          enabled: true\n+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true\n+          inputFormatFeatureSupport: []\n+          featureSupportInUse: []\n+          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n+          allNative: false\n+          usesVectorUDFAdaptor: false\n+          vectorized: true\n+      Reduce Vectorization:\n+          enabled: false\n+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true\n+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+1\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: parquet_nullsplit\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: (len = '99') (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                Group By Operator\n+                  aggregations: count()\n+                  mode: hash\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    sort order: \n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: _col0 (type: bigint)\n+      Execution mode: vectorized\n+      Map Vectorization:\n+          enabled: true\n+          inputFormatFeatureSupport: []\n+          featureSupportInUse: []\n+          allNative: false\n+          usesVectorUDFAdaptor: false\n+          vectorized: true\n+      Reduce Vectorization:\n+          enabled: false\n+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true\n+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: drop table parquet_nullsplit\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: drop table parquet_nullsplit\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: drop table parquet_project_test\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_project_test\n+PREHOOK: Output: default@parquet_project_test\n+POSTHOOK: query: drop table parquet_project_test\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_project_test\n+POSTHOOK: Output: default@parquet_project_test\n+PREHOOK: query: drop table parquet_types_staging\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_types_staging\n+PREHOOK: Output: default@parquet_types_staging\n+POSTHOOK: query: drop table parquet_types_staging\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_types_staging\n+POSTHOOK: Output: default@parquet_types_staging", "filename": "ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/df321c843d8a06de1785d5e55c9d76ce6f191c2e", "parent": "https://github.com/apache/hive/commit/c29df540d9376ccb02af01b4abd45f200d029baf", "message": "HIVE-17918: NPE during semijoin reduction optimization when LLAP caching disabled (Jason Dere, reviewed by Sergey Shelukhin)", "bug_id": "hive_47", "file": [{"additions": 20, "raw_url": "https://github.com/apache/hive/raw/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java", "blob_url": "https://github.com/apache/hive/blob/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java", "sha": "c51723264b66939ef2e5a355c8726ae449f5c0e5", "changes": 22, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java?ref=df321c843d8a06de1785d5e55c9d76ce6f191c2e", "patch": "@@ -45,10 +45,23 @@ private ObjectCacheFactory() {\n    * Returns the appropriate cache\n    */\n   public static ObjectCache getCache(Configuration conf, String queryId, boolean isPlanCache) {\n+    // LLAP cache can be disabled via config or isPlanCache\n+    return getCache(conf, queryId, isPlanCache, false);\n+  }\n+\n+  /**\n+   * Returns the appropriate cache\n+   * @param conf\n+   * @param queryId\n+   * @param isPlanCache\n+   * @param llapCacheAlwaysEnabled  Whether to always return LLAP cache regardless\n+   *        of config settings disabling LLAP cache. Valid only if running LLAP.\n+   * @return\n+   */\n+  public static ObjectCache getCache(Configuration conf, String queryId, boolean isPlanCache, boolean llapCacheAlwaysEnabled) {\n     if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")) {\n       if (LlapProxy.isDaemon()) { // daemon\n-        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_OBJECT_CACHE_ENABLED)\n-            && !isPlanCache) {\n+        if (isLlapCacheEnabled(conf, isPlanCache, llapCacheAlwaysEnabled)) {\n           // LLAP object cache, unlike others, does not use globals. Thus, get the existing one.\n           return getLlapObjectCache(queryId);\n         } else { // no cache\n@@ -70,6 +83,11 @@ public static ObjectCache getCache(Configuration conf, String queryId, boolean i\n     }\n   }\n \n+  private static boolean isLlapCacheEnabled(Configuration conf, boolean isPlanCache, boolean llapCacheAlwaysEnabled) {\n+    return (llapCacheAlwaysEnabled ||\n+        (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_OBJECT_CACHE_ENABLED) && !isPlanCache));\n+  }\n+\n   private static ObjectCache getLlapObjectCache(String queryId) {\n     // If order of events (i.e. dagstart and fragmentstart) was guaranteed, we could just\n     // create the cache when dag starts, and blindly return it to execution here.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "blob_url": "https://github.com/apache/hive/blob/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "sha": "8ca8be66f6d05f2c273fcba0ed2b3aff2041cc28", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java?ref=df321c843d8a06de1785d5e55c9d76ce6f191c2e", "patch": "@@ -100,7 +100,7 @@ public MapRecordProcessor(final JobConf jconf, final ProcessorContext context) t\n       setLlapOfFragmentId(context);\n     }\n     cache = ObjectCacheFactory.getCache(jconf, queryId, true);\n-    dynamicValueCache = ObjectCacheFactory.getCache(jconf, queryId, false);\n+    dynamicValueCache = ObjectCacheFactory.getCache(jconf, queryId, false, true);\n     execContext = new ExecMapperContext(jconf);\n     execContext.setJc(jconf);\n     cacheKeys = new ArrayList<String>();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java", "blob_url": "https://github.com/apache/hive/blob/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java", "sha": "ce3e07f721b388eacd4527e37e740246a3057513", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java?ref=df321c843d8a06de1785d5e55c9d76ce6f191c2e", "patch": "@@ -90,7 +90,7 @@ public ReduceRecordProcessor(final JobConf jconf, final ProcessorContext context\n \n     String queryId = HiveConf.getVar(jconf, HiveConf.ConfVars.HIVEQUERYID);\n     cache = ObjectCacheFactory.getCache(jconf, queryId, true);\n-    dynamicValueCache = ObjectCacheFactory.getCache(jconf, queryId, false);\n+    dynamicValueCache = ObjectCacheFactory.getCache(jconf, queryId, false, true);\n \n     String cacheKey = processorContext.getTaskVertexName() + REDUCE_PLAN_KEY;\n     cacheKeys = Lists.newArrayList(cacheKey);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/plan/DynamicValue.java", "blob_url": "https://github.com/apache/hive/blob/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/java/org/apache/hadoop/hive/ql/plan/DynamicValue.java", "sha": "a20328cb69c8e72779b7939b81f368f6b414ddab", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/DynamicValue.java?ref=df321c843d8a06de1785d5e55c9d76ce6f191c2e", "patch": "@@ -113,7 +113,7 @@ public Object getValue() {\n     try {\n       // Get object cache\n       String queryId = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID);\n-      ObjectCache cache = ObjectCacheFactory.getCache(conf, queryId, false);\n+      ObjectCache cache = ObjectCacheFactory.getCache(conf, queryId, false, true);\n \n       if (cache == null) {\n         return null;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/DynamicValue.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction.q", "blob_url": "https://github.com/apache/hive/blob/df321c843d8a06de1785d5e55c9d76ce6f191c2e/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction.q", "sha": "1d10dce9d9ea8da9800d7181e4ec0c9a9d03fab5", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction.q?ref=df321c843d8a06de1785d5e55c9d76ce6f191c2e", "patch": "@@ -121,6 +121,7 @@ set hive.tez.dynamic.semijoin.reduction=false;\n EXPLAIN select count(*) from srcpart_small10, srcpart_small, srcpart_date where srcpart_small.key1 = srcpart_small10.key1 and srcpart_date.ds = srcpart_small.ds;\n select count(*) from srcpart_small10, srcpart_small, srcpart_date where srcpart_small.key1 = srcpart_small10.key1 and srcpart_date.ds = srcpart_small.ds;\n set hive.tez.dynamic.semijoin.reduction=true;\n+set hive.llap.object.cache.enabled=false;\n EXPLAIN select count(*) from srcpart_small10, srcpart_small, srcpart_date where srcpart_small.key1 = srcpart_small10.key1 and srcpart_date.ds = srcpart_small.ds;\n select count(*) from srcpart_small10, srcpart_small, srcpart_date where srcpart_small.key1 = srcpart_small10.key1 and srcpart_date.ds = srcpart_small.ds;\n ", "filename": "ql/src/test/queries/clientpositive/dynamic_semijoin_reduction.q"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ae927155d6db5a9e2e07d3d8b51dbf817de817dc", "parent": "https://github.com/apache/hive/commit/0731dab18c85363d4bad8a556c437a587277143c", "message": "HIVE-17007: NPE introduced by HIVE-16871 (Daniel Dai, reviewed by Sushanth Sowmyan)", "bug_id": "hive_48", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/ae927155d6db5a9e2e07d3d8b51dbf817de817dc/metastore/src/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java", "blob_url": "https://github.com/apache/hive/blob/ae927155d6db5a9e2e07d3d8b51dbf817de817dc/metastore/src/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java", "sha": "b6fb4fd14b114824515031cd2937ad69ec79bf1d", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java?ref=ae927155d6db5a9e2e07d3d8b51dbf817de817dc", "patch": "@@ -137,7 +137,7 @@ public static synchronized void removeTableFromCache(String dbName, String tblNa\n   }\n \n   public static synchronized ColumnStatisticsObj getCachedTableColStats(String colStatsCacheKey) {\n-    return tableColStatsCache.get(colStatsCacheKey).deepCopy();\n+    return tableColStatsCache.get(colStatsCacheKey)!=null?tableColStatsCache.get(colStatsCacheKey).deepCopy():null;\n   }\n \n   public static synchronized void removeTableColStatsFromCache(String dbName, String tblName) {\n@@ -426,7 +426,7 @@ public static synchronized int getCachedPartitionCount() {\n   }\n \n   public static synchronized ColumnStatisticsObj getCachedPartitionColStats(String key) {\n-    return partitionColStatsCache.get(key).deepCopy();\n+    return partitionColStatsCache.get(key)!=null?partitionColStatsCache.get(key).deepCopy():null;\n   }\n \n   public static synchronized void addPartitionColStatsToCache(String dbName, String tableName,", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/cache/SharedCache.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6fb647f32ce4e393c4bfcd871821d4da166abaa0", "parent": "https://github.com/apache/hive/commit/41f72dc3eda0e2744ea3787560ef12ec1d994038", "message": "HIVE-16848: NPE during CachedStore refresh (Daniel Dai, reviewed by Vaibhav Gumashta, Thejas Nair)", "bug_id": "hive_49", "file": [{"additions": 23, "raw_url": "https://github.com/apache/hive/raw/6fb647f32ce4e393c4bfcd871821d4da166abaa0/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java", "blob_url": "https://github.com/apache/hive/blob/6fb647f32ce4e393c4bfcd871821d4da166abaa0/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java", "sha": "3ac4fe1604c7b0b455894b8e6293484e9226836e", "changes": 41, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java?ref=6fb647f32ce4e393c4bfcd871821d4da166abaa0", "patch": "@@ -85,6 +85,7 @@\n import org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy;\n import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.hive.common.util.HiveStringUtils;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n@@ -355,10 +356,10 @@ public void run() {\n             }\n           }\n         }\n-      } catch (MetaException e) {\n-        LOG.error(\"Updating CachedStore: error getting database names\", e);\n       } catch (InstantiationException | IllegalAccessException e) {\n         throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n+      } catch (Exception e) {\n+        LOG.error(\"Updating CachedStore: error happen when refresh\", e);\n       } finally {\n         try {\n           if (rawStore != null) {\n@@ -460,15 +461,17 @@ private void updateTableColStats(RawStore rawStore, String dbName, String tblNam\n         ColumnStatistics tableColStats =\n             rawStore.getTableColumnStatistics(dbName, tblName, colNames);\n         Deadline.stopTimer();\n-        if (tableColStatsCacheLock.writeLock().tryLock()) {\n-          // Skip background updates if we detect change\n-          if (isTableColStatsCacheDirty.compareAndSet(true, false)) {\n-            LOG.debug(\"Skipping table column stats cache update; the table column stats list we \"\n-                + \"have is dirty.\");\n-            return;\n+        if (tableColStats != null) {\n+          if (tableColStatsCacheLock.writeLock().tryLock()) {\n+            // Skip background updates if we detect change\n+            if (isTableColStatsCacheDirty.compareAndSet(true, false)) {\n+              LOG.debug(\"Skipping table column stats cache update; the table column stats list we \"\n+                  + \"have is dirty.\");\n+              return;\n+            }\n+            SharedCache.refreshTableColStats(HiveStringUtils.normalizeIdentifier(dbName),\n+                HiveStringUtils.normalizeIdentifier(tblName), tableColStats.getStatsObj());\n           }\n-          SharedCache.refreshTableColStats(HiveStringUtils.normalizeIdentifier(dbName),\n-              HiveStringUtils.normalizeIdentifier(tblName), tableColStats.getStatsObj());\n         }\n       } catch (MetaException | NoSuchObjectException e) {\n         LOG.info(\"Updating CachedStore: unable to read table column stats of table: \" + tblName, e);\n@@ -486,15 +489,17 @@ private void updateTablePartitionColStats(RawStore rawStore, String dbName, Stri\n         Map<String, List<ColumnStatisticsObj>> colStatsPerPartition =\n             rawStore.getColStatsForTablePartitions(dbName, tblName);\n         Deadline.stopTimer();\n-        if (partitionColStatsCacheLock.writeLock().tryLock()) {\n-          // Skip background updates if we detect change\n-          if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {\n-            LOG.debug(\"Skipping partition column stats cache update; the partition column stats \"\n-                + \"list we have is dirty.\");\n-            return;\n+        if (colStatsPerPartition != null) {\n+          if (partitionColStatsCacheLock.writeLock().tryLock()) {\n+            // Skip background updates if we detect change\n+            if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {\n+              LOG.debug(\"Skipping partition column stats cache update; the partition column stats \"\n+                  + \"list we have is dirty.\");\n+              return;\n+            }\n+            SharedCache.refreshPartitionColStats(HiveStringUtils.normalizeIdentifier(dbName),\n+                HiveStringUtils.normalizeIdentifier(tblName), colStatsPerPartition);\n           }\n-          SharedCache.refreshPartitionColStats(HiveStringUtils.normalizeIdentifier(dbName),\n-              HiveStringUtils.normalizeIdentifier(tblName), colStatsPerPartition);\n         }\n       } catch (MetaException | NoSuchObjectException e) {\n         LOG.info(\"Updating CachedStore: unable to read partitions column stats of table: \"", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8df9f65511746eddf26e7a77eb7312faa67d99d5", "parent": "https://github.com/apache/hive/commit/889a60a446e56f28542b4ccf71950908ae349cf5", "message": "HIVE-16845: INSERT OVERWRITE a table with dynamic partitions on S3 fails with NPE (Marta Kuczora, reviewed by Sahil Takiar)", "bug_id": "hive_50", "file": [{"additions": 37, "raw_url": "https://github.com/apache/hive/raw/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q", "blob_url": "https://github.com/apache/hive/blob/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q", "sha": "44360b01de19231de53b32b53286b5502f07899d", "changes": 37, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q?ref=8df9f65511746eddf26e7a77eb7312faa67d99d5", "patch": "@@ -0,0 +1,37 @@\n+set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+SET hive.blobstore.optimizations.enabled=true;\n+SET hive.exec.dynamic.partition.mode=nonstrict;\n+SET mapreduce.input.fileinputformat.split.maxsize=10;\n+SET hive.merge.mapfiles=true;\n+set hive.optimize.sort.dynamic.partition=false;\n+\n+CREATE TABLE tmp_table_merge_move (id string, name string, dt string, pid int);\n+\n+INSERT INTO tmp_table_merge_move values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10000), ('u3','name3','2017-04-10',10000), ('u4','name4','2017-04-10',10001), ('u5','name5','2017-04-10',10002);\n+\n+CREATE EXTERNAL TABLE s3_table_merge_move (user_id string, event_name string) PARTITIONED BY (reported_date string, product_id int) LOCATION '${hiveconf:test.blobstore.path.unique}/s3_table_merge_move/';\n+\n+INSERT OVERWRITE TABLE s3_table_merge_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge_move t;\n+\n+select * from s3_table_merge_move order by user_id;\n+\n+SET hive.blobstore.optimizations.enabled=false;\n+\n+INSERT OVERWRITE TABLE s3_table_merge_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge_move t;\n+\n+select * from s3_table_merge_move order by user_id;\n+\n+DROP TABLE s3_table_merge_move;\n+DROP TABLE tmp_table_merge_move;\n\\ No newline at end of file", "filename": "itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q", "blob_url": "https://github.com/apache/hive/blob/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q", "sha": "25562d94bfdaf4f98de981c87a0d71ff589172f0", "changes": 37, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q?ref=8df9f65511746eddf26e7a77eb7312faa67d99d5", "patch": "@@ -0,0 +1,37 @@\n+set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+SET hive.blobstore.optimizations.enabled=true;\n+SET hive.exec.dynamic.partition.mode=nonstrict;\n+SET mapreduce.input.fileinputformat.split.maxsize=10;\n+SET hive.merge.mapfiles=true;\n+set hive.optimize.sort.dynamic.partition=false;\n+\n+CREATE TABLE tmp_table_merge (id string, name string, dt string, pid int);\n+\n+INSERT INTO tmp_table_merge values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10000), ('u3','name3','2017-04-10',10000), ('u4','name4','2017-04-10',10001), ('u5','name5','2017-04-10',10001);\n+\n+CREATE EXTERNAL TABLE s3_table_merge (user_id string, event_name string) PARTITIONED BY (reported_date string, product_id int) LOCATION '${hiveconf:test.blobstore.path.unique}/s3_table_merge/';\n+\n+INSERT OVERWRITE TABLE s3_table_merge PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge t;\n+\n+select * from s3_table_merge order by user_id;\n+\n+SET hive.blobstore.optimizations.enabled=false;\n+\n+INSERT OVERWRITE TABLE s3_table_merge PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge t;\n+\n+select * from s3_table_merge order by user_id;\n+\n+DROP TABLE s3_table_merge;\n+DROP TABLE tmp_table_merge;\n\\ No newline at end of file", "filename": "itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_move_only.q", "blob_url": "https://github.com/apache/hive/blob/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_move_only.q", "sha": "cb1a32bc94c9762b9c3676e385f8dde03e24c20f", "changes": 37, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_move_only.q?ref=8df9f65511746eddf26e7a77eb7312faa67d99d5", "patch": "@@ -0,0 +1,37 @@\n+set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+SET hive.blobstore.optimizations.enabled=true;\n+SET hive.exec.dynamic.partition.mode=nonstrict;\n+SET mapreduce.input.fileinputformat.split.maxsize=10;\n+SET hive.merge.mapfiles=true;\n+set hive.optimize.sort.dynamic.partition=false;\n+\n+CREATE TABLE tmp_table_move (id string, name string, dt string, pid int);\n+\n+INSERT INTO tmp_table_move values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10001), ('u3','name3','2017-04-10',10002), ('u4','name4','2017-04-12',10001), ('u5','name5','2017-04-12',10002);\n+\n+CREATE EXTERNAL TABLE s3_table_move (user_id string, event_name string) PARTITIONED BY (reported_date string, product_id int) LOCATION '${hiveconf:test.blobstore.path.unique}/s3_table_move/';\n+\n+INSERT OVERWRITE TABLE s3_table_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_move t;\n+\n+select * from s3_table_move order by user_id;\n+\n+SET hive.blobstore.optimizations.enabled=false;\n+\n+INSERT OVERWRITE TABLE s3_table_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_move t;\n+\n+select * from s3_table_move order by user_id;\n+\n+DROP TABLE s3_table_move;\n+DROP TABLE tmp_table_move;\n\\ No newline at end of file", "filename": "itests/hive-blobstore/src/test/queries/clientpositive/insert_overwrite_dynamic_partitions_move_only.q"}, {"additions": 138, "raw_url": "https://github.com/apache/hive/raw/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q.out", "blob_url": "https://github.com/apache/hive/blob/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q.out", "sha": "bfebad6a9ad453629e0d3e7dcc6ec35359647253", "changes": 138, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q.out?ref=8df9f65511746eddf26e7a77eb7312faa67d99d5", "patch": "@@ -0,0 +1,138 @@\n+PREHOOK: query: CREATE TABLE tmp_table_merge_move (id string, name string, dt string, pid int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tmp_table_merge_move\n+POSTHOOK: query: CREATE TABLE tmp_table_merge_move (id string, name string, dt string, pid int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tmp_table_merge_move\n+PREHOOK: query: INSERT INTO tmp_table_merge_move values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10000), ('u3','name3','2017-04-10',10000), ('u4','name4','2017-04-10',10001), ('u5','name5','2017-04-10',10002)\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@tmp_table_merge_move\n+POSTHOOK: query: INSERT INTO tmp_table_merge_move values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10000), ('u3','name3','2017-04-10',10000), ('u4','name4','2017-04-10',10001), ('u5','name5','2017-04-10',10002)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@tmp_table_merge_move\n+POSTHOOK: Lineage: tmp_table_merge_move.dt SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_merge_move.id SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_merge_move.name SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_merge_move.pid EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col4, type:string, comment:), ]\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Input: ### test.blobstore.path ###/s3_table_merge_move\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@s3_table_merge_move\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Input: ### test.blobstore.path ###/s3_table_merge_move\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@s3_table_merge_move\n+PREHOOK: query: INSERT OVERWRITE TABLE s3_table_merge_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge_move t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tmp_table_merge_move\n+PREHOOK: Output: default@s3_table_merge_move\n+POSTHOOK: query: INSERT OVERWRITE TABLE s3_table_merge_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge_move t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tmp_table_merge_move\n+POSTHOOK: Output: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Output: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Output: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10002\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10000).event_name SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10000).user_id SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10001).event_name SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10001).user_id SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10002).event_name SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10002).user_id SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+PREHOOK: query: select * from s3_table_merge_move order by user_id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@s3_table_merge_move\n+PREHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10000\n+PREHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10001\n+PREHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10002\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from s3_table_merge_move order by user_id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@s3_table_merge_move\n+POSTHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10002\n+#### A masked pattern was here ####\n+u1\tname1\t2017-04-10\t10000\n+u2\tname2\t2017-04-10\t10000\n+u3\tname3\t2017-04-10\t10000\n+u4\tname4\t2017-04-10\t10001\n+u5\tname5\t2017-04-10\t10002\n+PREHOOK: query: INSERT OVERWRITE TABLE s3_table_merge_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge_move t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tmp_table_merge_move\n+PREHOOK: Output: default@s3_table_merge_move\n+POSTHOOK: query: INSERT OVERWRITE TABLE s3_table_merge_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge_move t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tmp_table_merge_move\n+POSTHOOK: Output: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Output: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Output: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10002\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10000).event_name SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10000).user_id SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10001).event_name SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10001).user_id SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10002).event_name SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge_move PARTITION(reported_date=2017-04-10,product_id=10002).user_id SIMPLE [(tmp_table_merge_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+PREHOOK: query: select * from s3_table_merge_move order by user_id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@s3_table_merge_move\n+PREHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10000\n+PREHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10001\n+PREHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10002\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from s3_table_merge_move order by user_id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@s3_table_merge_move\n+POSTHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Input: default@s3_table_merge_move@reported_date=2017-04-10/product_id=10002\n+#### A masked pattern was here ####\n+u1\tname1\t2017-04-10\t10000\n+u2\tname2\t2017-04-10\t10000\n+u3\tname3\t2017-04-10\t10000\n+u4\tname4\t2017-04-10\t10001\n+u5\tname5\t2017-04-10\t10002\n+PREHOOK: query: DROP TABLE s3_table_merge_move\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@s3_table_merge_move\n+PREHOOK: Output: default@s3_table_merge_move\n+POSTHOOK: query: DROP TABLE s3_table_merge_move\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@s3_table_merge_move\n+POSTHOOK: Output: default@s3_table_merge_move\n+PREHOOK: query: DROP TABLE tmp_table_merge_move\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@tmp_table_merge_move\n+PREHOOK: Output: default@tmp_table_merge_move\n+POSTHOOK: query: DROP TABLE tmp_table_merge_move\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@tmp_table_merge_move\n+POSTHOOK: Output: default@tmp_table_merge_move", "filename": "itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_move.q.out"}, {"additions": 128, "raw_url": "https://github.com/apache/hive/raw/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q.out", "blob_url": "https://github.com/apache/hive/blob/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q.out", "sha": "1bffae39829b9bb041a4b33df975297c16bf378a", "changes": 128, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q.out?ref=8df9f65511746eddf26e7a77eb7312faa67d99d5", "patch": "@@ -0,0 +1,128 @@\n+PREHOOK: query: CREATE TABLE tmp_table_merge (id string, name string, dt string, pid int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tmp_table_merge\n+POSTHOOK: query: CREATE TABLE tmp_table_merge (id string, name string, dt string, pid int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tmp_table_merge\n+PREHOOK: query: INSERT INTO tmp_table_merge values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10000), ('u3','name3','2017-04-10',10000), ('u4','name4','2017-04-10',10001), ('u5','name5','2017-04-10',10001)\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@tmp_table_merge\n+POSTHOOK: query: INSERT INTO tmp_table_merge values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10000), ('u3','name3','2017-04-10',10000), ('u4','name4','2017-04-10',10001), ('u5','name5','2017-04-10',10001)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@tmp_table_merge\n+POSTHOOK: Lineage: tmp_table_merge.dt SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_merge.id SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_merge.name SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_merge.pid EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col4, type:string, comment:), ]\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Input: ### test.blobstore.path ###/s3_table_merge\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@s3_table_merge\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Input: ### test.blobstore.path ###/s3_table_merge\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@s3_table_merge\n+PREHOOK: query: INSERT OVERWRITE TABLE s3_table_merge PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tmp_table_merge\n+PREHOOK: Output: default@s3_table_merge\n+POSTHOOK: query: INSERT OVERWRITE TABLE s3_table_merge PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tmp_table_merge\n+POSTHOOK: Output: default@s3_table_merge@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Output: default@s3_table_merge@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10000).event_name SIMPLE [(tmp_table_merge)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10000).user_id SIMPLE [(tmp_table_merge)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10001).event_name SIMPLE [(tmp_table_merge)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10001).user_id SIMPLE [(tmp_table_merge)t.FieldSchema(name:id, type:string, comment:null), ]\n+PREHOOK: query: select * from s3_table_merge order by user_id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@s3_table_merge\n+PREHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10000\n+PREHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10001\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from s3_table_merge order by user_id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@s3_table_merge\n+POSTHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10001\n+#### A masked pattern was here ####\n+u1\tname1\t2017-04-10\t10000\n+u2\tname2\t2017-04-10\t10000\n+u3\tname3\t2017-04-10\t10000\n+u4\tname4\t2017-04-10\t10001\n+u5\tname5\t2017-04-10\t10001\n+PREHOOK: query: INSERT OVERWRITE TABLE s3_table_merge PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tmp_table_merge\n+PREHOOK: Output: default@s3_table_merge\n+POSTHOOK: query: INSERT OVERWRITE TABLE s3_table_merge PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_merge t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tmp_table_merge\n+POSTHOOK: Output: default@s3_table_merge@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Output: default@s3_table_merge@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10000).event_name SIMPLE [(tmp_table_merge)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10000).user_id SIMPLE [(tmp_table_merge)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10001).event_name SIMPLE [(tmp_table_merge)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_merge PARTITION(reported_date=2017-04-10,product_id=10001).user_id SIMPLE [(tmp_table_merge)t.FieldSchema(name:id, type:string, comment:null), ]\n+PREHOOK: query: select * from s3_table_merge order by user_id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@s3_table_merge\n+PREHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10000\n+PREHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10001\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from s3_table_merge order by user_id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@s3_table_merge\n+POSTHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Input: default@s3_table_merge@reported_date=2017-04-10/product_id=10001\n+#### A masked pattern was here ####\n+u1\tname1\t2017-04-10\t10000\n+u2\tname2\t2017-04-10\t10000\n+u3\tname3\t2017-04-10\t10000\n+u4\tname4\t2017-04-10\t10001\n+u5\tname5\t2017-04-10\t10001\n+PREHOOK: query: DROP TABLE s3_table_merge\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@s3_table_merge\n+PREHOOK: Output: default@s3_table_merge\n+POSTHOOK: query: DROP TABLE s3_table_merge\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@s3_table_merge\n+POSTHOOK: Output: default@s3_table_merge\n+PREHOOK: query: DROP TABLE tmp_table_merge\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@tmp_table_merge\n+PREHOOK: Output: default@tmp_table_merge\n+POSTHOOK: query: DROP TABLE tmp_table_merge\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@tmp_table_merge\n+POSTHOOK: Output: default@tmp_table_merge", "filename": "itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_merge_only.q.out"}, {"additions": 148, "raw_url": "https://github.com/apache/hive/raw/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_move_only.q.out", "blob_url": "https://github.com/apache/hive/blob/8df9f65511746eddf26e7a77eb7312faa67d99d5/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_move_only.q.out", "sha": "530c0368caa3aa2acf0ff4d4c8a52091c3ae00c2", "changes": 148, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_move_only.q.out?ref=8df9f65511746eddf26e7a77eb7312faa67d99d5", "patch": "@@ -0,0 +1,148 @@\n+PREHOOK: query: CREATE TABLE tmp_table_move (id string, name string, dt string, pid int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tmp_table_move\n+POSTHOOK: query: CREATE TABLE tmp_table_move (id string, name string, dt string, pid int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tmp_table_move\n+PREHOOK: query: INSERT INTO tmp_table_move values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10001), ('u3','name3','2017-04-10',10002), ('u4','name4','2017-04-12',10001), ('u5','name5','2017-04-12',10002)\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@tmp_table_move\n+POSTHOOK: query: INSERT INTO tmp_table_move values ('u1','name1','2017-04-10',10000), ('u2','name2','2017-04-10',10001), ('u3','name3','2017-04-10',10002), ('u4','name4','2017-04-12',10001), ('u5','name5','2017-04-12',10002)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@tmp_table_move\n+POSTHOOK: Lineage: tmp_table_move.dt SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_move.id SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_move.name SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: tmp_table_move.pid EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col4, type:string, comment:), ]\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Input: ### test.blobstore.path ###/s3_table_move\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@s3_table_move\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Input: ### test.blobstore.path ###/s3_table_move\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@s3_table_move\n+PREHOOK: query: INSERT OVERWRITE TABLE s3_table_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_move t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tmp_table_move\n+PREHOOK: Output: default@s3_table_move\n+POSTHOOK: query: INSERT OVERWRITE TABLE s3_table_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_move t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tmp_table_move\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-10/product_id=10002\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-12/product_id=10001\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-12/product_id=10002\n+PREHOOK: query: select * from s3_table_move order by user_id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@s3_table_move\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10000\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10001\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10002\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10001\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10002\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from s3_table_move order by user_id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@s3_table_move\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10002\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10001\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10002\n+#### A masked pattern was here ####\n+u1\tname1\t2017-04-10\t10000\n+u2\tname2\t2017-04-10\t10001\n+u3\tname3\t2017-04-10\t10002\n+u4\tname4\t2017-04-12\t10001\n+u5\tname5\t2017-04-12\t10002\n+PREHOOK: query: INSERT OVERWRITE TABLE s3_table_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_move t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tmp_table_move\n+PREHOOK: Output: default@s3_table_move\n+POSTHOOK: query: INSERT OVERWRITE TABLE s3_table_move PARTITION (reported_date, product_id)\n+SELECT\n+  t.id as user_id,\n+  t.name as event_name,\n+  t.dt as reported_date,\n+  t.pid as product_id\n+FROM tmp_table_move t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tmp_table_move\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-10/product_id=10002\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-12/product_id=10001\n+POSTHOOK: Output: default@s3_table_move@reported_date=2017-04-12/product_id=10002\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-10,product_id=10000).event_name SIMPLE [(tmp_table_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-10,product_id=10000).user_id SIMPLE [(tmp_table_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-10,product_id=10001).event_name SIMPLE [(tmp_table_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-10,product_id=10001).user_id SIMPLE [(tmp_table_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-10,product_id=10002).event_name SIMPLE [(tmp_table_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-10,product_id=10002).user_id SIMPLE [(tmp_table_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-12,product_id=10001).event_name SIMPLE [(tmp_table_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-12,product_id=10001).user_id SIMPLE [(tmp_table_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-12,product_id=10002).event_name SIMPLE [(tmp_table_move)t.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: s3_table_move PARTITION(reported_date=2017-04-12,product_id=10002).user_id SIMPLE [(tmp_table_move)t.FieldSchema(name:id, type:string, comment:null), ]\n+PREHOOK: query: select * from s3_table_move order by user_id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@s3_table_move\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10000\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10001\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10002\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10001\n+PREHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10002\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from s3_table_move order by user_id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@s3_table_move\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10000\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10001\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-10/product_id=10002\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10001\n+POSTHOOK: Input: default@s3_table_move@reported_date=2017-04-12/product_id=10002\n+#### A masked pattern was here ####\n+u1\tname1\t2017-04-10\t10000\n+u2\tname2\t2017-04-10\t10001\n+u3\tname3\t2017-04-10\t10002\n+u4\tname4\t2017-04-12\t10001\n+u5\tname5\t2017-04-12\t10002\n+PREHOOK: query: DROP TABLE s3_table_move\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@s3_table_move\n+PREHOOK: Output: default@s3_table_move\n+POSTHOOK: query: DROP TABLE s3_table_move\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@s3_table_move\n+POSTHOOK: Output: default@s3_table_move\n+PREHOOK: query: DROP TABLE tmp_table_move\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@tmp_table_move\n+PREHOOK: Output: default@tmp_table_move\n+POSTHOOK: query: DROP TABLE tmp_table_move\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@tmp_table_move\n+POSTHOOK: Output: default@tmp_table_move", "filename": "itests/hive-blobstore/src/test/results/clientpositive/insert_overwrite_dynamic_partitions_move_only.q.out"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/8df9f65511746eddf26e7a77eb7312faa67d99d5/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java", "blob_url": "https://github.com/apache/hive/blob/8df9f65511746eddf26e7a77eb7312faa67d99d5/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java", "sha": "b454a2ad0c229468410c62b8e65964e3504379c9", "changes": 11, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java?ref=8df9f65511746eddf26e7a77eb7312faa67d99d5", "patch": "@@ -284,7 +284,16 @@ private void generateActualTasks(HiveConf conf, List<Task<? extends Serializable\n         // make the MoveTask as the child of the MR Task\n         resTsks.add(mrAndMvTask);\n \n-        MoveWork mvWork = (MoveWork) mvTask.getWork();\n+        // Originally the mvTask and the child move task of the mrAndMvTask contain the same\n+        // MoveWork object.\n+        // If the blobstore optimizations are on and the input/output paths are merged\n+        // in the move only MoveWork, the mvTask and the child move task of the mrAndMvTask\n+        // will contain different MoveWork objects, which causes problems.\n+        // Not just in this case, but also in general the child move task of the mrAndMvTask should\n+        // be used, because that is the correct move task for the \"merge and move\" use case.\n+        Task<? extends Serializable> mergeAndMoveMoveTask = mrAndMvTask.getChildTasks().get(0);\n+        MoveWork mvWork = (MoveWork) mergeAndMoveMoveTask.getWork();\n+\n         LoadFileDesc lfd = mvWork.getLoadFileWork();\n \n         Path targetDir = lfd.getTargetDir();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ConditionalResolverMergeFiles.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c49a6e79751414f045706eb3132ae50e7b3d2bc4", "parent": "https://github.com/apache/hive/commit/149e4fad4e78508da8793ac293a161aaf3106e01", "message": "HIVE-16249: With column stats, mergejoin.q throws NPE (Pengcheng Xiong, reviewed by Ashutosh Chauhan)", "bug_id": "hive_51", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/c49a6e79751414f045706eb3132ae50e7b3d2bc4/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/FilterSelectivityEstimator.java", "blob_url": "https://github.com/apache/hive/blob/c49a6e79751414f045706eb3132ae50e7b3d2bc4/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/FilterSelectivityEstimator.java", "sha": "a25b58bb1e9277e66379ba8f2596fa49d8b40348", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/FilterSelectivityEstimator.java?ref=c49a6e79751414f045706eb3132ae50e7b3d2bc4", "patch": "@@ -29,6 +29,7 @@\n import org.apache.calcite.rel.metadata.RelMetadataQuery;\n import org.apache.calcite.rex.RexCall;\n import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexLiteral;\n import org.apache.calcite.rex.RexNode;\n import org.apache.calcite.rex.RexVisitorImpl;\n import org.apache.calcite.sql.SqlKind;\n@@ -301,4 +302,15 @@ private SqlKind getOp(RexCall call) {\n \n     return op;\n   }\n+\n+  public Double visitLiteral(RexLiteral literal) {\n+    if (literal.isAlwaysFalse()) {\n+      return 0.0;\n+    } else if (literal.isAlwaysTrue()) {\n+      return 1.0;\n+    } else {\n+      assert false;\n+    }\n+    return null;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/FilterSelectivityEstimator.java"}, {"additions": 49, "raw_url": "https://github.com/apache/hive/raw/c49a6e79751414f045706eb3132ae50e7b3d2bc4/ql/src/test/queries/clientpositive/optimize_filter_literal.q", "blob_url": "https://github.com/apache/hive/blob/c49a6e79751414f045706eb3132ae50e7b3d2bc4/ql/src/test/queries/clientpositive/optimize_filter_literal.q", "sha": "d13197ad40895a118c9823dd15148ae3af3c1111", "changes": 49, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/optimize_filter_literal.q?ref=c49a6e79751414f045706eb3132ae50e7b3d2bc4", "patch": "@@ -0,0 +1,49 @@\n+set hive.strict.checks.bucketing=false;\n+\n+set hive.mapred.mode=nonstrict;\n+set hive.explain.user=false;\n+set hive.join.emit.interval=100000;\n+set hive.optimize.ppd=true;\n+set hive.ppd.remove.duplicatefilters=true;\n+set hive.tez.dynamic.partition.pruning=true;\n+set hive.optimize.metadataonly=false;\n+set hive.optimize.index.filter=true;\n+set hive.vectorized.execution.enabled=true;\n+\n+-- SORT_QUERY_RESULTS\n+\n+CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;\n+CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS ORCFILE;\n+CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;\n+\n+load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');\n+load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');\n+\n+load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n+load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n+load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n+load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n+\n+\n+\n+set hive.optimize.bucketingsorting=false;\n+insert overwrite table tab_part partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin_part;\n+\n+analyze table tab_part partition (ds='2008-04-08') compute statistics for columns;\n+\n+CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORCFILE;\n+insert overwrite table tab partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin;\n+\n+analyze table tab partition (ds='2008-04-08') compute statistics for columns;\n+\n+set hive.join.emit.interval=2;\n+\n+set mapred.reduce.tasks=3;\n+\n+select * from\n+(select * from tab where tab.key = 0)a\n+full outer join\n+(select * from tab_part where tab_part.key = 98)b join tab_part c on a.key = b.key and b.key = c.key;\n+", "filename": "ql/src/test/queries/clientpositive/optimize_filter_literal.q"}, {"additions": 151, "raw_url": "https://github.com/apache/hive/raw/c49a6e79751414f045706eb3132ae50e7b3d2bc4/ql/src/test/results/clientpositive/optimize_filter_literal.q.out", "blob_url": "https://github.com/apache/hive/blob/c49a6e79751414f045706eb3132ae50e7b3d2bc4/ql/src/test/results/clientpositive/optimize_filter_literal.q.out", "sha": "00bb01b8e16c6569f95b601bf4f54eeb22329174", "changes": 151, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/optimize_filter_literal.q.out?ref=c49a6e79751414f045706eb3132ae50e7b3d2bc4", "patch": "@@ -0,0 +1,151 @@\n+PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@srcbucket_mapjoin\n+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@srcbucket_mapjoin\n+PREHOOK: query: CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS ORCFILE\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tab_part\n+POSTHOOK: query: CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS ORCFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tab_part\n+PREHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@srcbucket_mapjoin_part\n+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@srcbucket_mapjoin_part\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin\n+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin_part\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin_part\n+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n+PREHOOK: query: insert overwrite table tab_part partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin_part\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@srcbucket_mapjoin_part\n+PREHOOK: Input: default@srcbucket_mapjoin_part@ds=2008-04-08\n+PREHOOK: Output: default@tab_part@ds=2008-04-08\n+POSTHOOK: query: insert overwrite table tab_part partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin_part\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@srcbucket_mapjoin_part\n+POSTHOOK: Input: default@srcbucket_mapjoin_part@ds=2008-04-08\n+POSTHOOK: Output: default@tab_part@ds=2008-04-08\n+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]\n+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]\n+PREHOOK: query: analyze table tab_part partition (ds='2008-04-08') compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab_part\n+PREHOOK: Input: default@tab_part@ds=2008-04-08\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table tab_part partition (ds='2008-04-08') compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab_part\n+POSTHOOK: Input: default@tab_part@ds=2008-04-08\n+#### A masked pattern was here ####\n+PREHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORCFILE\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tab\n+POSTHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORCFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tab\n+PREHOOK: query: insert overwrite table tab partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@srcbucket_mapjoin\n+PREHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08\n+PREHOOK: Output: default@tab@ds=2008-04-08\n+POSTHOOK: query: insert overwrite table tab partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@srcbucket_mapjoin\n+POSTHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08\n+POSTHOOK: Output: default@tab@ds=2008-04-08\n+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]\n+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]\n+PREHOOK: query: analyze table tab partition (ds='2008-04-08') compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab\n+PREHOOK: Input: default@tab@ds=2008-04-08\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table tab partition (ds='2008-04-08') compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab\n+POSTHOOK: Input: default@tab@ds=2008-04-08\n+#### A masked pattern was here ####\n+Warning: Shuffle Join JOIN[13][tables = [$hdt$_1, $hdt$_2, $hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product\n+PREHOOK: query: select * from\n+(select * from tab where tab.key = 0)a\n+full outer join\n+(select * from tab_part where tab_part.key = 98)b join tab_part c on a.key = b.key and b.key = c.key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab\n+PREHOOK: Input: default@tab@ds=2008-04-08\n+PREHOOK: Input: default@tab_part\n+PREHOOK: Input: default@tab_part@ds=2008-04-08\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from\n+(select * from tab where tab.key = 0)a\n+full outer join\n+(select * from tab_part where tab_part.key = 98)b join tab_part c on a.key = b.key and b.key = c.key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab\n+POSTHOOK: Input: default@tab@ds=2008-04-08\n+POSTHOOK: Input: default@tab_part\n+POSTHOOK: Input: default@tab_part@ds=2008-04-08\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/optimize_filter_literal.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d2e870438a99fe84369154d6f399f7864a36319a", "parent": "https://github.com/apache/hive/commit/f572ce6d2c708ae0a08254d042ce5739c096ba18", "message": "HIVE-16831: Add unit tests for NPE fixes in HIVE-12054 (Sunitha Beeram, reviewed by Carl Steinbach)", "bug_id": "hive_52", "file": [{"additions": 11, "raw_url": "https://github.com/apache/hive/raw/d2e870438a99fe84369154d6f399f7864a36319a/ql/src/test/queries/clientpositive/orc_empty_table.q", "blob_url": "https://github.com/apache/hive/blob/d2e870438a99fe84369154d6f399f7864a36319a/ql/src/test/queries/clientpositive/orc_empty_table.q", "sha": "05bba281b8c07b92d077b46b8989adddaf956cb3", "changes": 11, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_empty_table.q?ref=d2e870438a99fe84369154d6f399f7864a36319a", "patch": "@@ -0,0 +1,11 @@\n+CREATE TABLE test_orc_empty_table_with_struct (struct_field STRUCT<int_field: INT>) STORED AS ORC;\n+SELECT count(*) FROM test_orc_empty_table_with_struct;\n+\n+CREATE TABLE test_orc_empty_table_with_map (map_field MAP<STRING,STRING>) STORED AS ORC;\n+SELECT count(*) FROM test_orc_empty_table_with_map;\n+\n+CREATE TABLE test_orc_empty_table_with_list (list_field ARRAY<INT>) STORED AS ORC;\n+SELECT count(*) FROM test_orc_empty_table_with_list;\n+\n+CREATE TABLE test_orc_empty_table_with_union (union_field UNIONTYPE<int, double>) STORED AS ORC;\n+SELECT count(*) FROM test_orc_empty_table_with_union;", "filename": "ql/src/test/queries/clientpositive/orc_empty_table.q"}, {"additions": 68, "raw_url": "https://github.com/apache/hive/raw/d2e870438a99fe84369154d6f399f7864a36319a/ql/src/test/results/clientpositive/orc_empty_table.q.out", "blob_url": "https://github.com/apache/hive/blob/d2e870438a99fe84369154d6f399f7864a36319a/ql/src/test/results/clientpositive/orc_empty_table.q.out", "sha": "e95589ed87c50a003cdf120d8adc5ecf37971b38", "changes": 68, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_empty_table.q.out?ref=d2e870438a99fe84369154d6f399f7864a36319a", "patch": "@@ -0,0 +1,68 @@\n+PREHOOK: query: CREATE TABLE test_orc_empty_table_with_struct (struct_field STRUCT<int_field: INT>) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@test_orc_empty_table_with_struct\n+POSTHOOK: query: CREATE TABLE test_orc_empty_table_with_struct (struct_field STRUCT<int_field: INT>) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test_orc_empty_table_with_struct\n+PREHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_struct\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_orc_empty_table_with_struct\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_struct\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_orc_empty_table_with_struct\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: CREATE TABLE test_orc_empty_table_with_map (map_field MAP<STRING,STRING>) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@test_orc_empty_table_with_map\n+POSTHOOK: query: CREATE TABLE test_orc_empty_table_with_map (map_field MAP<STRING,STRING>) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test_orc_empty_table_with_map\n+PREHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_map\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_orc_empty_table_with_map\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_map\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_orc_empty_table_with_map\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: CREATE TABLE test_orc_empty_table_with_list (list_field ARRAY<INT>) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@test_orc_empty_table_with_list\n+POSTHOOK: query: CREATE TABLE test_orc_empty_table_with_list (list_field ARRAY<INT>) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test_orc_empty_table_with_list\n+PREHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_list\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_orc_empty_table_with_list\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_list\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_orc_empty_table_with_list\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: CREATE TABLE test_orc_empty_table_with_union (union_field UNIONTYPE<int, double>) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@test_orc_empty_table_with_union\n+POSTHOOK: query: CREATE TABLE test_orc_empty_table_with_union (union_field UNIONTYPE<int, double>) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test_orc_empty_table_with_union\n+PREHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_union\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_orc_empty_table_with_union\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM test_orc_empty_table_with_union\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_orc_empty_table_with_union\n+#### A masked pattern was here ####\n+0", "filename": "ql/src/test/results/clientpositive/orc_empty_table.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c", "parent": "https://github.com/apache/hive/commit/35d707950ddd210c37533be3da51cea730bac881", "message": "HIVE-16142: ATSHook NPE via LLAP (Pengcheng Xiong, reviewed by Ashutosh Chauhan)", "bug_id": "hive_53", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java", "blob_url": "https://github.com/apache/hive/blob/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java", "sha": "d35e3ba56a2395ad8665e8df7c29a0ada830a657", "changes": 9, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java?ref=48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c", "patch": "@@ -797,10 +797,11 @@ private JSONObject outputPlan(Object work, PrintStream out,\n                 operator.getOperatorId());\n             if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                 && operator instanceof ReduceSinkOperator) {\n-              ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\n-                  OUTPUT_OPERATORS,\n-                  Arrays.toString(((ReduceSinkOperator) operator).getConf().getOutputOperators()\n-                      .toArray()));\n+              List<String> outputOperators = ((ReduceSinkOperator) operator).getConf().getOutputOperators();\n+              if (outputOperators != null) {\n+                ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(OUTPUT_OPERATORS,\n+                    Arrays.toString(outputOperators.toArray()));\n+              }\n             }\n           }\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef", "parent": "https://github.com/apache/hive/commit/bda64ee87c74a06b3cf19b08c41d67f192f22018", "message": "HIVE-15992: LLAP: NPE in LlapTaskCommunicator.getCompletedLogsUrl for unsuccessful attempt (Rajesh Balamohan reviewed by Prasanth Jayachandran)", "bug_id": "hive_54", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java", "blob_url": "https://github.com/apache/hive/blob/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java", "sha": "3aae7a42fac9f67cd3a7ce9e915b569ffd994115", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java?ref=7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef", "patch": "@@ -541,7 +541,7 @@ public void indicateError(Throwable t) {\n   @Override\n   public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n     String url = \"\";\n-    if (timelineServerUri != null) {\n+    if (timelineServerUri != null && containerNodeId != null) {\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n       BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n       ContainerId containerId = biMap.inverse().get(attemptID);\n@@ -559,7 +559,7 @@ public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerN\n   @Override\n   public String getCompletedLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n     String url = \"\";\n-    if (timelineServerUri != null) {\n+    if (timelineServerUri != null && containerNodeId != null) {\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n       BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n       ContainerId containerId = biMap.inverse().get(attemptID);", "filename": "llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e9347ab9778608c415174e40ed63bcf36054b66d", "parent": "https://github.com/apache/hive/commit/22659ca43c40417f5761e8615e71bff605478753", "message": "HIVE-17272: when hive.vectorized.execution.enabled is true, query on empty partitioned table fails with NPE (Aihua Xu, reviewed by Vihang Karajgaonkar)", "bug_id": "hive_55", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/e9347ab9778608c415174e40ed63bcf36054b66d/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java", "blob_url": "https://github.com/apache/hive/blob/e9347ab9778608c415174e40ed63bcf36054b66d/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java", "sha": "e8c73a944a127bf717cc3d13f8a5ae30ef680220", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java?ref=e9347ab9778608c415174e40ed63bcf36054b66d", "patch": "@@ -385,6 +385,9 @@ public VectorPartitionContext createAndInitPartitionContext(PartitionDesc partDe\n           throws SerDeException, Exception {\n \n     VectorPartitionDesc vectorPartDesc = partDesc.getVectorPartitionDesc();\n+    if (vectorPartDesc == null) {\n+      return null;\n+    }\n     VectorPartitionContext vectorPartitionContext;\n     VectorMapOperatorReadType vectorMapOperatorReadType =\n         vectorPartDesc.getVectorMapOperatorReadType();\n@@ -631,6 +634,10 @@ private void setRowIdentiferToNull(VectorizedRowBatch batch) {\n   private void setupPartitionContextVars(String nominalPath) throws HiveException {\n \n     currentVectorPartContext = fileToPartitionContextMap.get(nominalPath);\n+    if (currentVectorPartContext == null) {\n+      return;\n+    }\n+\n     PartitionDesc partDesc = currentVectorPartContext.getPartDesc();\n     VectorPartitionDesc vectorPartDesc = partDesc.getVectorPartitionDesc();\n     currentReadType = vectorPartDesc.getVectorMapOperatorReadType();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/e9347ab9778608c415174e40ed63bcf36054b66d/ql/src/test/queries/clientpositive/vectorized_parquet.q", "blob_url": "https://github.com/apache/hive/blob/e9347ab9778608c415174e40ed63bcf36054b66d/ql/src/test/queries/clientpositive/vectorized_parquet.q", "sha": "db02ec04a5b412c973d7dbbba4e4fb375806f52d", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/vectorized_parquet.q?ref=e9347ab9778608c415174e40ed63bcf36054b66d", "patch": "@@ -46,3 +46,6 @@ select ctinyint,\n   stddev_pop(cdouble)\n   from alltypes_parquet\n   group by ctinyint;\n+\n+CREATE TABLE empty_parquet(x int) PARTITIONED BY (y int) stored as parquet;\n+select * from empty_parquet t1 join empty_parquet t2 where t1.x=t2.x;", "filename": "ql/src/test/queries/clientpositive/vectorized_parquet.q"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/e9347ab9778608c415174e40ed63bcf36054b66d/ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out", "blob_url": "https://github.com/apache/hive/blob/e9347ab9778608c415174e40ed63bcf36054b66d/ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out", "sha": "8a84d3dc2259be5fbb3a0bf82cd7ff3951c10754", "changes": 16, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out?ref=e9347ab9778608c415174e40ed63bcf36054b66d", "patch": "@@ -348,3 +348,19 @@ NULL\t1073418988\t-16379\t3115\tNULL\t305051.4870777435\n 55\t626923679\t-15887\t21\t55.0\t9826.38569192808\n 61\t626923679\t-15894\t29\t61.0\t8785.714950987198\n 62\t626923679\t-14307\t17\t62.0\t9491.752726667326\n+PREHOOK: query: CREATE TABLE empty_parquet(x int) PARTITIONED BY (y int) stored as parquet\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@empty_parquet\n+POSTHOOK: query: CREATE TABLE empty_parquet(x int) PARTITIONED BY (y int) stored as parquet\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@empty_parquet\n+PREHOOK: query: select * from empty_parquet t1 join empty_parquet t2 where t1.x=t2.x\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@empty_parquet\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from empty_parquet t1 join empty_parquet t2 where t1.x=t2.x\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@empty_parquet\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/llap/vectorized_parquet.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/3db22685748d689dfa2929644c82c0801a73726c", "parent": "https://github.com/apache/hive/commit/57ae3aca05d21628df620f33a9f03966f33c8d7b", "message": "HIVE-16599: NPE in runtime filtering cost when handling SMB Joins (Deepak Jaiswal, reviewed by Jason Dere)", "bug_id": "hive_56", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/3db22685748d689dfa2929644c82c0801a73726c/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "blob_url": "https://github.com/apache/hive/blob/3db22685748d689dfa2929644c82c0801a73726c/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "sha": "f469cd29fbd529097fef6b20e97135e32fcd80b3", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java?ref=3db22685748d689dfa2929644c82c0801a73726c", "patch": "@@ -671,7 +671,6 @@ private static void removeSemijoinOptimizationFromSMBJoins(\n           continue;\n         }\n \n-        assert parent instanceof SelectOperator;\n         while (parent != null) {\n           if (parent instanceof TableScanOperator) {\n             tsOps.add((TableScanOperator) parent);\n@@ -685,10 +684,11 @@ private static void removeSemijoinOptimizationFromSMBJoins(\n     // a semijoin filter on any of them, if so, remove it.\n \n     ParseContext pctx = procCtx.parseContext;\n+    Set<ReduceSinkOperator> rsSet = new HashSet<>(pctx.getRsToSemiJoinBranchInfo().keySet());\n     for (TableScanOperator ts : tsOps) {\n-      for (ReduceSinkOperator rs : pctx.getRsToSemiJoinBranchInfo().keySet()) {\n+      for (ReduceSinkOperator rs : rsSet) {\n         SemiJoinBranchInfo sjInfo = pctx.getRsToSemiJoinBranchInfo().get(rs);\n-        if (ts == sjInfo.getTsOp()) {\n+        if (sjInfo != null && ts == sjInfo.getTsOp()) {\n           // match!\n           if (LOG.isDebugEnabled()) {\n             LOG.debug(\"Semijoin optimization found going to SMB join. Removing semijoin \"", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/3db22685748d689dfa2929644c82c0801a73726c/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java", "blob_url": "https://github.com/apache/hive/blob/3db22685748d689dfa2929644c82c0801a73726c/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java", "sha": "01fab9c5d6f4049e39e394c78fb623ddf37306a2", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java?ref=3db22685748d689dfa2929644c82c0801a73726c", "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n import org.apache.hadoop.hive.ql.exec.UDF;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n import org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory;\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;\n@@ -843,7 +844,12 @@ public static ColumnOrigin findColumnOrigin(ExprNodeDesc expr, Operator<?> op) {\n       ExprNodeColumnDesc parentCol = ExprNodeDescUtils.getColumnExpr(parentExpr);\n       if (parentCol != null) {\n         for (Operator<?> currParent : op.getParentOperators()) {\n-          if (currParent.getSchema().getTableNames().contains(parentCol.getTabAlias())) {\n+          RowSchema schema = currParent.getSchema();\n+          if (schema == null) {\n+            // Happens in case of TezDummyStoreOperator\n+            return null;\n+          }\n+          if (schema.getTableNames().contains(parentCol.getTabAlias())) {\n             parentOp = currParent;\n             break;\n           }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java"}, {"additions": 48, "raw_url": "https://github.com/apache/hive/raw/3db22685748d689dfa2929644c82c0801a73726c/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_2.q", "blob_url": "https://github.com/apache/hive/blob/3db22685748d689dfa2929644c82c0801a73726c/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_2.q", "sha": "97b3d8433909a5481ebd1e13b3c4267d3fcbdc01", "changes": 48, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_2.q?ref=3db22685748d689dfa2929644c82c0801a73726c", "patch": "@@ -31,6 +31,7 @@ EXPLAIN\n SELECT\n COUNT(*)\n FROM table_1 t1\n+\n INNER JOIN table_18 t2 ON (((t2.tinyint_col_15) = (t1.bigint_col_7)) AND\n ((t2.decimal2709_col_9) = (t1.decimal2016_col_26))) AND\n ((t2.tinyint_col_20) = (t1.tinyint_col_3))\n@@ -42,3 +43,50 @@ WHERE (t1.timestamp_col_9) = (tt2.timestamp_col_18));\n \n drop table table_1;\n drop table table_18;\n+\n+-- Hive 15699\n+CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;\n+\n+CREATE TABLE src2 as select * from src1;\n+insert into src2 select * from src2;\n+insert into src2 select * from src2;\n+\n+load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');\n+load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');\n+\n+set hive.strict.checks.bucketing=false;\n+set hive.join.emit.interval=2;\n+set hive.stats.fetch.column.stats=true;\n+set hive.optimize.bucketingsorting=false;\n+set hive.stats.autogather=true;\n+\n+CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;\n+insert overwrite table tab partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin;\n+\n+set hive.convert.join.bucket.mapjoin.tez = true;\n+set hive.auto.convert.sortmerge.join = true;\n+\n+set hive.auto.convert.join.noconditionaltask.size=0;\n+set hive.mapjoin.hybridgrace.minwbsize=125;\n+set hive.mapjoin.hybridgrace.minnumpartitions=4;\n+\n+set hive.llap.memory.oversubscription.max.executors.per.query=3;\n+\n+CREATE TABLE tab2 (key int, value string, ds string);\n+\n+set hive.exec.dynamic.partition.mode=nonstrict\n+insert into tab2select key, value, ds from tab;\n+analyze table tab2 compute statistics;\n+analyze table tab2 compute statistics for columns;\n+\n+\n+explain\n+select\n+  count(*)\n+  from\n+  (select x.key as key, min(x.value) as value from tab2 x group by x.key) a\n+  join\n+  (select x.key as key, min(x.value) as value from tab2 x group by x.key) b\n+  on\n+  a.key = b.key join src1 c on a.value = c.value where c.key < 0;", "filename": "ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_2.q"}, {"additions": 305, "raw_url": "https://github.com/apache/hive/raw/3db22685748d689dfa2929644c82c0801a73726c/ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_2.q.out", "blob_url": "https://github.com/apache/hive/blob/3db22685748d689dfa2929644c82c0801a73726c/ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_2.q.out", "sha": "650dc9ffd72a832f88e4a5e7b401d1e811b62655", "changes": 305, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_2.q.out?ref=3db22685748d689dfa2929644c82c0801a73726c", "patch": "@@ -40,6 +40,7 @@ PREHOOK: query: EXPLAIN\n SELECT\n COUNT(*)\n FROM table_1 t1\n+\n INNER JOIN table_18 t2 ON (((t2.tinyint_col_15) = (t1.bigint_col_7)) AND\n ((t2.decimal2709_col_9) = (t1.decimal2016_col_26))) AND\n ((t2.tinyint_col_20) = (t1.tinyint_col_3))\n@@ -53,6 +54,7 @@ POSTHOOK: query: EXPLAIN\n SELECT\n COUNT(*)\n FROM table_1 t1\n+\n INNER JOIN table_18 t2 ON (((t2.tinyint_col_15) = (t1.bigint_col_7)) AND\n ((t2.decimal2709_col_9) = (t1.decimal2016_col_26))) AND\n ((t2.tinyint_col_20) = (t1.tinyint_col_3))\n@@ -299,3 +301,306 @@ POSTHOOK: query: drop table table_18\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@table_18\n POSTHOOK: Output: default@table_18\n+PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@srcbucket_mapjoin\n+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@srcbucket_mapjoin\n+PREHOOK: query: CREATE TABLE src2 as select * from src1\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src1\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@src2\n+POSTHOOK: query: CREATE TABLE src2 as select * from src1\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src1\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@src2\n+POSTHOOK: Lineage: src2.key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src2.value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: insert into src2 select * from src2\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src2\n+PREHOOK: Output: default@src2\n+POSTHOOK: query: insert into src2 select * from src2\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src2\n+POSTHOOK: Output: default@src2\n+POSTHOOK: Lineage: src2.key SIMPLE [(src2)src2.FieldSchema(name:key, type:string, comment:null), ]\n+POSTHOOK: Lineage: src2.value SIMPLE [(src2)src2.FieldSchema(name:value, type:string, comment:null), ]\n+PREHOOK: query: insert into src2 select * from src2\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src2\n+PREHOOK: Output: default@src2\n+POSTHOOK: query: insert into src2 select * from src2\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src2\n+POSTHOOK: Output: default@src2\n+POSTHOOK: Lineage: src2.key SIMPLE [(src2)src2.FieldSchema(name:key, type:string, comment:null), ]\n+POSTHOOK: Lineage: src2.value SIMPLE [(src2)src2.FieldSchema(name:value, type:string, comment:null), ]\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin\n+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08\n+PREHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08\n+POSTHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08\n+PREHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tab\n+POSTHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tab\n+PREHOOK: query: insert overwrite table tab partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@srcbucket_mapjoin\n+PREHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08\n+PREHOOK: Output: default@tab@ds=2008-04-08\n+POSTHOOK: query: insert overwrite table tab partition (ds='2008-04-08')\n+select key,value from srcbucket_mapjoin\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@srcbucket_mapjoin\n+POSTHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08\n+POSTHOOK: Output: default@tab@ds=2008-04-08\n+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]\n+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]\n+PREHOOK: query: CREATE TABLE tab2 (key int, value string, ds string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tab2\n+POSTHOOK: query: CREATE TABLE tab2 (key int, value string, ds string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tab2\n+Warning: Value had a \\n character in it.\n+PREHOOK: query: analyze table tab2 compute statistics\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab2\n+PREHOOK: Output: default@tab2\n+POSTHOOK: query: analyze table tab2 compute statistics\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab2\n+POSTHOOK: Output: default@tab2\n+PREHOOK: query: analyze table tab2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab2\n+PREHOOK: Output: default@tab2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table tab2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab2\n+POSTHOOK: Output: default@tab2\n+#### A masked pattern was here ####\n+PREHOOK: query: explain\n+select\n+  count(*)\n+  from\n+  (select x.key as key, min(x.value) as value from tab2 x group by x.key) a\n+  join\n+  (select x.key as key, min(x.value) as value from tab2 x group by x.key) b\n+  on\n+  a.key = b.key join src1 c on a.value = c.value where c.key < 0\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select\n+  count(*)\n+  from\n+  (select x.key as key, min(x.value) as value from tab2 x group by x.key) a\n+  join\n+  (select x.key as key, min(x.value) as value from tab2 x group by x.key) b\n+  on\n+  a.key = b.key join src1 c on a.value = c.value where c.key < 0\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+#### A masked pattern was here ####\n+      Edges:\n+        Map 8 <- Reducer 5 (BROADCAST_EDGE)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 6 (SIMPLE_EDGE)\n+        Reducer 3 <- Map 8 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)\n+        Reducer 4 <- Reducer 3 (CUSTOM_SIMPLE_EDGE)\n+        Reducer 5 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: x\n+                  filterExpr: key is not null (type: boolean)\n+                  Statistics: Num rows: 1 Data size: 88 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Filter Operator\n+                    predicate: key is not null (type: boolean)\n+                    Statistics: Num rows: 1 Data size: 88 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Group By Operator\n+                      aggregations: min(value)\n+                      keys: key (type: int)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: COMPLETE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: int)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: int)\n+                        Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: COMPLETE\n+                        value expressions: _col1 (type: string)\n+            Execution mode: llap\n+            LLAP IO: no inputs\n+        Map 6 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: x\n+                  filterExpr: key is not null (type: boolean)\n+                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Filter Operator\n+                    predicate: key is not null (type: boolean)\n+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Group By Operator\n+                      keys: key (type: int)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: int)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: int)\n+                        Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+            Execution mode: llap\n+            LLAP IO: no inputs\n+        Map 8 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: c\n+                  filterExpr: ((UDFToDouble(key) < 0.0) and value is not null and (value BETWEEN DynamicValue(RS_21_x__col1_min) AND DynamicValue(RS_21_x__col1_max) and in_bloom_filter(value, DynamicValue(RS_21_x__col1_bloom_filter)))) (type: boolean)\n+                  Statistics: Num rows: 25 Data size: 4375 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Filter Operator\n+                    predicate: ((UDFToDouble(key) < 0.0) and value is not null and (value BETWEEN DynamicValue(RS_21_x__col1_min) AND DynamicValue(RS_21_x__col1_max) and in_bloom_filter(value, DynamicValue(RS_21_x__col1_bloom_filter)))) (type: boolean)\n+                    Statistics: Num rows: 8 Data size: 1400 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Statistics: Num rows: 8 Data size: 1400 Basic stats: COMPLETE Column stats: COMPLETE\n+                      Reduce Output Operator\n+                        key expressions: _col1 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col1 (type: string)\n+                        Statistics: Num rows: 8 Data size: 1400 Basic stats: COMPLETE Column stats: COMPLETE\n+            Execution mode: llap\n+            LLAP IO: no inputs\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                keys: KEY._col0 (type: int)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+            Execution mode: llap\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: min(VALUE._col0)\n+                keys: KEY._col0 (type: int)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: COMPLETE\n+                Filter Operator\n+                  predicate: _col1 is not null (type: boolean)\n+                  Statistics: Num rows: 1 Data size: 188 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Merge Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    keys:\n+                      0 _col0 (type: int)\n+                      1 _col0 (type: int)\n+                    outputColumnNames: _col1\n+                    Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Reduce Output Operator\n+                      key expressions: _col1 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col1 (type: string)\n+                      Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Select Operator\n+                      expressions: _col1 (type: string)\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE\n+                      Group By Operator\n+                        aggregations: min(_col0), max(_col0), bloom_filter(_col0, expectedEntries=2)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n+                        Reduce Output Operator\n+                          sort order: \n+                          Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n+                          value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)\n+        Reducer 3 \n+            Execution mode: llap\n+            Reduce Operator Tree:\n+              Merge Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col1 (type: string)\n+                  1 _col1 (type: string)\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                Group By Operator\n+                  aggregations: count()\n+                  mode: hash\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Reduce Output Operator\n+                    sort order: \n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                    value expressions: _col0 (type: bigint)\n+        Reducer 4 \n+            Execution mode: llap\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 5 \n+            Execution mode: llap\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: min(VALUE._col0), max(VALUE._col1), bloom_filter(VALUE._col2, expectedEntries=2)\n+                mode: final\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+", "filename": "ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_2.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c8685b40361f89c81de13c905b999929d498c17f", "parent": "https://github.com/apache/hive/commit/b799bde78919bdd8803b248d6aff57009767e1df", "message": "HIVE-15855: throws NPE when using Hplsql UDF (Fei Hui, reviewed by Ferdinand Xu)", "bug_id": "hive_57", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/c8685b40361f89c81de13c905b999929d498c17f/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "blob_url": "https://github.com/apache/hive/blob/c8685b40361f89c81de13c905b999929d498c17f/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "sha": "2ad3ea3a942089d626114e82ba12a41c4038a613", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/hplsql/src/main/java/org/apache/hive/hplsql/Exec.java?ref=c8685b40361f89c81de13c905b999929d498c17f", "patch": "@@ -144,7 +144,9 @@ public Var setVariable(String name, Var value) {\n     else {\n       var = new Var(value);\n       var.setName(name);\n-      exec.currentScope.addVariable(var);\n+      if(exec.currentScope != null) {\n+        exec.currentScope.addVariable(var);\n+      }\n     }    \n     return var;\n   }\n@@ -172,7 +174,9 @@ public Var setVariableToNull(String name) {\n     else {\n       var = new Var();\n       var.setName(name);\n-      exec.currentScope.addVariable(var);\n+      if(exec.currentScope != null) {\n+        exec.currentScope.addVariable(var);\n+      }\n     }    \n     return var;\n   }", "filename": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f4d7471ec45389103b81c8e3edc9fbf96086835e", "parent": "https://github.com/apache/hive/commit/803e1e0b89140350c40a7ec5c42f217532550495", "message": "HIVE-15387: NPE in HiveServer2 webUI Historical SQL Operations section (Barna Zsombor Klara via Chaoyu Tang)", "bug_id": "hive_58", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/f4d7471ec45389103b81c8e3edc9fbf96086835e/service/src/resources/hive-webapps/hiveserver2/hiveserver2.jsp", "blob_url": "https://github.com/apache/hive/blob/f4d7471ec45389103b81c8e3edc9fbf96086835e/service/src/resources/hive-webapps/hiveserver2/hiveserver2.jsp", "sha": "33797edc03ecce05c03c22c74db15c3529f773f2", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/resources/hive-webapps/hiveserver2/hiveserver2.jsp?ref=f4d7471ec45389103b81c8e3edc9fbf96086835e", "patch": "@@ -193,7 +193,7 @@ for (HiveSession hiveSession: hiveSessions) {\n         <td><%= operation.getState() %></td>\n         <td><%= operation.getElapsedTime()/1000 %></td>\n         <td><%= operation.getEndTime() == null ? \"In Progress\" : new Date(operation.getEndTime()) %></td>\n-        <td><%= operation.getRuntime()/1000 %></td>\n+        <td><%= operation.getRuntime() == null ? \"n/a\" : operation.getRuntime()/1000 %></td>\n         <% String link = \"/query_page?operationId=\" + operation.getOperationId(); %>\n         <td>  <a href= <%= link %>>Drilldown</a> </td>\n     </tr>", "filename": "service/src/resources/hive-webapps/hiveserver2/hiveserver2.jsp"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b00621da53a733d3903521683b0c0ccf1201a140", "parent": "https://github.com/apache/hive/commit/3d71a90a72d850040d42f7a4ba55a7dc36924e5b", "message": "HIVE-15649 : LLAP IO may NPE on all-column read (Sergey Shelukhin, reviewed by Prasanth Jayachandran)", "bug_id": "hive_59", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/itests/src/test/resources/testconfiguration.properties", "sha": "e966959a5261f9f1bdfe3c0aab829ba38ee699c3", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=b00621da53a733d3903521683b0c0ccf1201a140", "patch": "@@ -439,6 +439,7 @@ minillap.query.files=acid_bucket_pruning.q,\\\n   dynamic_partition_pruning_2.q,\\\n   tez_union_dynamic_partition.q,\\\n   load_fs2.q,\\\n+  llap_stats.q,\\\n   multi_count_distinct_null.q\n \n minillaplocal.query.files=acid_globallimit.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java", "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java", "sha": "2f79828f416d19283b3f0ed52dfc3c827d48c85c", "changes": 18, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java?ref=b00621da53a733d3903521683b0c0ccf1201a140", "patch": "@@ -87,6 +87,7 @@\n   private final boolean[] includedColumns;\n   private final ReadPipeline rp;\n   private final ExecutorService executor;\n+  private final int columnCount;\n \n   private TypeDescription fileSchema;\n \n@@ -119,6 +120,14 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols\n     MapWork mapWork = Utilities.getMapWork(job);\n     VectorizedRowBatchCtx ctx = mapWork.getVectorizedRowBatchCtx();\n     rbCtx = ctx != null ? ctx : LlapInputFormat.createFakeVrbCtx(mapWork);\n+    if (includedCols == null) {\n+      // Assume including everything means the VRB will have everything.\n+      this.columnCount = rbCtx.getRowColumnTypeInfos().length;\n+    } else {\n+      this.columnCount = columnIds.size();\n+    }\n+\n+\n \n     int partitionColumnCount = rbCtx.getPartitionColumnCount();\n     if (partitionColumnCount > 0) {\n@@ -159,7 +168,8 @@ public boolean init() {\n   private boolean checkOrcSchemaEvolution() {\n     SchemaEvolution schemaEvolution = new SchemaEvolution(\n         fileSchema, rp.getReaderSchema(), includedColumns);\n-    for (Integer colId : columnIds) {\n+    for (int i = 0; i < columnCount; ++i) {\n+      int colId = columnIds == null ? i : columnIds.get(i);\n       if (!schemaEvolution.isPPDSafeConversion(colId)) {\n         LlapIoImpl.LOG.warn(\"Unsupported schema evolution! Disabling Llap IO for {}\", split);\n         return false;\n@@ -197,14 +207,14 @@ public boolean next(NullWritable key, VectorizedRowBatch value) throws IOExcepti\n       counters.incrTimeCounter(LlapIOCounters.CONSUMER_TIME_NS, firstReturnTime);\n       return false;\n     }\n-    if (columnIds.size() != cvb.cols.length) {\n-      throw new RuntimeException(\"Unexpected number of columns, VRB has \" + columnIds.size()\n+    if (columnCount != cvb.cols.length) {\n+      throw new RuntimeException(\"Unexpected number of columns, VRB has \" + columnCount\n           + \" included, but the reader returned \" + cvb.cols.length);\n     }\n     // VRB was created from VrbCtx, so we already have pre-allocated column vectors\n     for (int i = 0; i < cvb.cols.length; ++i) {\n       // Return old CVs (if any) to caller. We assume these things all have the same schema.\n-      cvb.swapColumnVector(i, value.cols, columnIds.get(i));\n+      cvb.swapColumnVector(i, value.cols, columnIds == null ? i : columnIds.get(i));\n     }\n     value.selectedInUse = false;\n     value.size = cvb.size;", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java"}, {"additions": 26, "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/queries/clientpositive/llap_stats.q", "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/queries/clientpositive/llap_stats.q", "sha": "49b52bd4a6342b7fb7391846e06a477ff7c6f30f", "changes": 26, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/llap_stats.q?ref=b00621da53a733d3903521683b0c0ccf1201a140", "patch": "@@ -0,0 +1,26 @@\n+set hive.mapred.mode=nonstrict;\n+SET hive.vectorized.execution.enabled=true;\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+\n+SET hive.llap.io.enabled=false;\n+\n+SET hive.exec.orc.default.buffer.size=32768;\n+SET hive.exec.orc.default.row.index.stride=1000;\n+SET hive.optimize.index.filter=true;\n+set hive.auto.convert.join=false;\n+\n+DROP TABLE llap_stats;\n+\n+CREATE TABLE llap_stats(ctinyint TINYINT, csmallint SMALLINT) partitioned by (cint int) STORED AS ORC;\n+\n+insert into table llap_stats partition(cint)\n+select cint, ctinyint, csmallint from alltypesorc where cint is not null limit 10;\n+\n+select * from llap_stats;\n+\n+SET hive.llap.io.enabled=true;\n+\n+explain analyze table llap_stats partition (cint) compute statistics for columns;\n+analyze table llap_stats partition (cint) compute statistics for columns;\n+\n+DROP TABLE llap_stats;", "filename": "ql/src/test/queries/clientpositive/llap_stats.q"}, {"additions": 192, "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/results/clientpositive/llap/llap_stats.q.out", "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/results/clientpositive/llap/llap_stats.q.out", "sha": "f6921f13067352cf02e7a1a5e6d194b6d87f02d0", "changes": 192, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/llap_stats.q.out?ref=b00621da53a733d3903521683b0c0ccf1201a140", "patch": "@@ -0,0 +1,192 @@\n+PREHOOK: query: DROP TABLE llap_stats\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE llap_stats\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE llap_stats(ctinyint TINYINT, csmallint SMALLINT) partitioned by (cint int) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@llap_stats\n+POSTHOOK: query: CREATE TABLE llap_stats(ctinyint TINYINT, csmallint SMALLINT) partitioned by (cint int) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@llap_stats\n+PREHOOK: query: insert into table llap_stats partition(cint)\n+select cint, ctinyint, csmallint from alltypesorc where cint is not null limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@llap_stats\n+POSTHOOK: query: insert into table llap_stats partition(cint)\n+select cint, ctinyint, csmallint from alltypesorc where cint is not null limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@llap_stats@cint=-13326\n+POSTHOOK: Output: default@llap_stats@cint=-15431\n+POSTHOOK: Output: default@llap_stats@cint=-15549\n+POSTHOOK: Output: default@llap_stats@cint=-15813\n+POSTHOOK: Output: default@llap_stats@cint=-4213\n+POSTHOOK: Output: default@llap_stats@cint=-7824\n+POSTHOOK: Output: default@llap_stats@cint=-9566\n+POSTHOOK: Output: default@llap_stats@cint=15007\n+POSTHOOK: Output: default@llap_stats@cint=4963\n+POSTHOOK: Output: default@llap_stats@cint=7021\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-13326).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-13326).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15431).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15431).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15549).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15549).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15813).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15813).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-4213).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-4213).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-7824).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-7824).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-9566).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-9566).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=15007).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=15007).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=4963).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=4963).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=7021).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=7021).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+PREHOOK: query: select * from llap_stats\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@llap_stats\n+PREHOOK: Input: default@llap_stats@cint=-13326\n+PREHOOK: Input: default@llap_stats@cint=-15431\n+PREHOOK: Input: default@llap_stats@cint=-15549\n+PREHOOK: Input: default@llap_stats@cint=-15813\n+PREHOOK: Input: default@llap_stats@cint=-4213\n+PREHOOK: Input: default@llap_stats@cint=-7824\n+PREHOOK: Input: default@llap_stats@cint=-9566\n+PREHOOK: Input: default@llap_stats@cint=15007\n+PREHOOK: Input: default@llap_stats@cint=4963\n+PREHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from llap_stats\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@llap_stats\n+POSTHOOK: Input: default@llap_stats@cint=-13326\n+POSTHOOK: Input: default@llap_stats@cint=-15431\n+POSTHOOK: Input: default@llap_stats@cint=-15549\n+POSTHOOK: Input: default@llap_stats@cint=-15813\n+POSTHOOK: Input: default@llap_stats@cint=-4213\n+POSTHOOK: Input: default@llap_stats@cint=-7824\n+POSTHOOK: Input: default@llap_stats@cint=-9566\n+POSTHOOK: Input: default@llap_stats@cint=15007\n+POSTHOOK: Input: default@llap_stats@cint=4963\n+POSTHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+-17\t-50\t-13326\n+-17\t-11\t-15431\n+-17\t61\t-15549\n+-17\t-28\t-15813\n+-17\tNULL\t-4213\n+-17\t27\t-7824\n+-17\t31\t-9566\n+-17\t-34\t15007\n+-17\t31\t4963\n+-17\t29\t7021\n+PREHOOK: query: explain analyze table llap_stats partition (cint) compute statistics for columns\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain analyze table llap_stats partition (cint) compute statistics for columns\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+  Stage-2 depends on stages: Stage-0\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Tez\n+#### A masked pattern was here ####\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: llap_stats\n+                  Statistics: Num rows: 10 Data size: 116 Basic stats: COMPLETE Column stats: PARTIAL\n+                  Select Operator\n+                    expressions: cint (type: int), ctinyint (type: tinyint), csmallint (type: smallint)\n+                    outputColumnNames: cint, ctinyint, csmallint\n+                    Statistics: Num rows: 10 Data size: 116 Basic stats: COMPLETE Column stats: PARTIAL\n+                    Group By Operator\n+                      aggregations: compute_stats(ctinyint, 16), compute_stats(csmallint, 16)\n+                      keys: cint (type: int)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Statistics: Num rows: 5 Data size: 4780 Basic stats: COMPLETE Column stats: PARTIAL\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: int)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: int)\n+                        Statistics: Num rows: 5 Data size: 4780 Basic stats: COMPLETE Column stats: PARTIAL\n+                        value expressions: _col1 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>), _col2 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>)\n+            Execution mode: llap\n+            LLAP IO: all inputs\n+        Reducer 2 \n+            Execution mode: llap\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1)\n+                keys: KEY._col0 (type: int)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 5 Data size: 4820 Basic stats: COMPLETE Column stats: PARTIAL\n+                Select Operator\n+                  expressions: _col1 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint,ndvbitvector:string>), _col2 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint,ndvbitvector:string>), _col0 (type: int)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 5 Data size: 4820 Basic stats: COMPLETE Column stats: PARTIAL\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 5 Data size: 4820 Basic stats: COMPLETE Column stats: PARTIAL\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-2\n+    Column Stats Work\n+      Column Stats Desc:\n+          Columns: ctinyint, csmallint\n+          Column Types: tinyint, smallint\n+          Table: default.llap_stats\n+\n+PREHOOK: query: analyze table llap_stats partition (cint) compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@llap_stats\n+PREHOOK: Input: default@llap_stats@cint=-13326\n+PREHOOK: Input: default@llap_stats@cint=-15431\n+PREHOOK: Input: default@llap_stats@cint=-15549\n+PREHOOK: Input: default@llap_stats@cint=-15813\n+PREHOOK: Input: default@llap_stats@cint=-4213\n+PREHOOK: Input: default@llap_stats@cint=-7824\n+PREHOOK: Input: default@llap_stats@cint=-9566\n+PREHOOK: Input: default@llap_stats@cint=15007\n+PREHOOK: Input: default@llap_stats@cint=4963\n+PREHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table llap_stats partition (cint) compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@llap_stats\n+POSTHOOK: Input: default@llap_stats@cint=-13326\n+POSTHOOK: Input: default@llap_stats@cint=-15431\n+POSTHOOK: Input: default@llap_stats@cint=-15549\n+POSTHOOK: Input: default@llap_stats@cint=-15813\n+POSTHOOK: Input: default@llap_stats@cint=-4213\n+POSTHOOK: Input: default@llap_stats@cint=-7824\n+POSTHOOK: Input: default@llap_stats@cint=-9566\n+POSTHOOK: Input: default@llap_stats@cint=15007\n+POSTHOOK: Input: default@llap_stats@cint=4963\n+POSTHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+PREHOOK: query: DROP TABLE llap_stats\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@llap_stats\n+PREHOOK: Output: default@llap_stats\n+POSTHOOK: query: DROP TABLE llap_stats\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@llap_stats\n+POSTHOOK: Output: default@llap_stats", "filename": "ql/src/test/results/clientpositive/llap/llap_stats.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c31c2963d6db464c482a56c532011f2d00b527aa", "parent": "https://github.com/apache/hive/commit/881deac63e835c4cfa8b9232b7aa280fe82acea7", "message": "HIVE-15647 Combination of a boolean condition and null-safe comparison leads to NPE", "bug_id": "hive_60", "file": [{"additions": 8, "raw_url": "https://github.com/apache/hive/raw/c31c2963d6db464c482a56c532011f2d00b527aa/ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/c31c2963d6db464c482a56c532011f2d00b527aa/ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerProcFactory.java", "sha": "d4df1e83e05947a190dae9c400ca35c961e014f3", "changes": 9, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerProcFactory.java?ref=c31c2963d6db464c482a56c532011f2d00b527aa", "patch": "@@ -367,7 +367,14 @@ private static void extractFinalCandidates(ExprNodeDesc expr,\n \n     ExprInfo exprInfo = ctx.getExprInfo(expr);\n     if (exprInfo != null && exprInfo.isCandidate) {\n-      ctx.addFinalCandidate(exprInfo.alias, exprInfo.convertedExpr != null ?\n+      String alias = exprInfo.alias;\n+      if ((alias == null) && (exprInfo.convertedExpr != null)) {\n+    \tExprInfo convertedExprInfo = ctx.getExprInfo(exprInfo.convertedExpr);\n+    \tif (convertedExprInfo != null) {\n+    \t\talias = convertedExprInfo.alias;\n+    \t}\n+      }\n+      ctx.addFinalCandidate(alias, exprInfo.convertedExpr != null ?\n               exprInfo.convertedExpr : expr);\n       return;\n     } else if (!FunctionRegistry.isOpAnd(expr) &&", "filename": "ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerProcFactory.java"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/c31c2963d6db464c482a56c532011f2d00b527aa/ql/src/test/queries/clientpositive/filter_cond_pushdown_HIVE_15647.q", "blob_url": "https://github.com/apache/hive/blob/c31c2963d6db464c482a56c532011f2d00b527aa/ql/src/test/queries/clientpositive/filter_cond_pushdown_HIVE_15647.q", "sha": "7a1d32c0a6a0cd9666d45c4b7850561461cdbcbb", "changes": 24, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/filter_cond_pushdown_HIVE_15647.q?ref=c31c2963d6db464c482a56c532011f2d00b527aa", "patch": "@@ -0,0 +1,24 @@\n+set hive.cbo.enable=false;\n+\n+CREATE TABLE sales_HIVE_15647 (store_id INTEGER, store_number INTEGER, customer_id INTEGER);\n+CREATE TABLE store_HIVE_15647 (store_id INTEGER, salad_bar BOOLEAN);\n+\n+explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar)) and ((sales.store_number) <=> (sales.customer_id));\n+\n+explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar)) and ((sales.store_number) = (sales.customer_id));\n+\n+explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar = true)) and ((sales.store_number) <=> (sales.customer_id));\n+\n+explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar = false)) and ((sales.store_number) > (sales.customer_id));\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/filter_cond_pushdown_HIVE_15647.q"}, {"additions": 372, "raw_url": "https://github.com/apache/hive/raw/c31c2963d6db464c482a56c532011f2d00b527aa/ql/src/test/results/clientpositive/filter_cond_pushdown_HIVE_15647.q.out", "blob_url": "https://github.com/apache/hive/blob/c31c2963d6db464c482a56c532011f2d00b527aa/ql/src/test/results/clientpositive/filter_cond_pushdown_HIVE_15647.q.out", "sha": "866c8fed9ceba4b2330c3b61c702aaf6e2866a34", "changes": 372, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/filter_cond_pushdown_HIVE_15647.q.out?ref=c31c2963d6db464c482a56c532011f2d00b527aa", "patch": "@@ -0,0 +1,372 @@\n+PREHOOK: query: CREATE TABLE sales_HIVE_15647 (store_id INTEGER, store_number INTEGER, customer_id INTEGER)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@sales_HIVE_15647\n+POSTHOOK: query: CREATE TABLE sales_HIVE_15647 (store_id INTEGER, store_number INTEGER, customer_id INTEGER)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@sales_HIVE_15647\n+PREHOOK: query: CREATE TABLE store_HIVE_15647 (store_id INTEGER, salad_bar BOOLEAN)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@store_HIVE_15647\n+POSTHOOK: query: CREATE TABLE store_HIVE_15647 (store_id INTEGER, salad_bar BOOLEAN)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@store_HIVE_15647\n+PREHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar)) and ((sales.store_number) <=> (sales.customer_id))\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar)) and ((sales.store_number) <=> (sales.customer_id))\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1\n+  Stage-0 depends on stages: Stage-2\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: sales\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: ((store_number = customer_id) and store_id is not null) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          TableScan\n+            alias: store\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: (store_id is not null and salad_bar) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          keys:\n+            0 store_id (type: int)\n+            1 store_id (type: int)\n+          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          Group By Operator\n+            aggregations: count()\n+            mode: hash\n+            outputColumnNames: _col0\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            Reduce Output Operator\n+              sort order: \n+              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+              value expressions: _col0 (type: bigint)\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar)) and ((sales.store_number) = (sales.customer_id))\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar)) and ((sales.store_number) = (sales.customer_id))\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1\n+  Stage-0 depends on stages: Stage-2\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: sales\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: ((store_number = customer_id) and store_id is not null) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          TableScan\n+            alias: store\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: (store_id is not null and salad_bar) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          keys:\n+            0 store_id (type: int)\n+            1 store_id (type: int)\n+          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          Group By Operator\n+            aggregations: count()\n+            mode: hash\n+            outputColumnNames: _col0\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            Reduce Output Operator\n+              sort order: \n+              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+              value expressions: _col0 (type: bigint)\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar = true)) and ((sales.store_number) <=> (sales.customer_id))\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar = true)) and ((sales.store_number) <=> (sales.customer_id))\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1\n+  Stage-0 depends on stages: Stage-2\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: sales\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: ((store_number = customer_id) and store_id is not null) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          TableScan\n+            alias: store\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: (store_id is not null and (salad_bar = true)) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          keys:\n+            0 store_id (type: int)\n+            1 store_id (type: int)\n+          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          Group By Operator\n+            aggregations: count()\n+            mode: hash\n+            outputColumnNames: _col0\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            Reduce Output Operator\n+              sort order: \n+              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+              value expressions: _col0 (type: bigint)\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar = false)) and ((sales.store_number) > (sales.customer_id))\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select count(*) from\n+sales_HIVE_15647 as sales \n+join store_HIVE_15647 as store on sales.store_id = store.store_id\n+where ((store.salad_bar = false)) and ((sales.store_number) > (sales.customer_id))\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1\n+  Stage-0 depends on stages: Stage-2\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: sales\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: (store_id is not null and (store_number > customer_id)) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          TableScan\n+            alias: store\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: (store_id is not null and (salad_bar = false)) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: store_id (type: int)\n+                sort order: +\n+                Map-reduce partition columns: store_id (type: int)\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          keys:\n+            0 store_id (type: int)\n+            1 store_id (type: int)\n+          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+          Group By Operator\n+            aggregations: count()\n+            mode: hash\n+            outputColumnNames: _col0\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            Reduce Output Operator\n+              sort order: \n+              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+              value expressions: _col0 (type: bigint)\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+", "filename": "ql/src/test/results/clientpositive/filter_cond_pushdown_HIVE_15647.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a496e581152425773080aac48cf479e493cd5b74", "parent": "https://github.com/apache/hive/commit/e5a6b30241c166c82d082effd72967dc25804f97", "message": "HIVE-16433: Not nullify variable \"rj\" to avoid NPE due to race condition in ExecDriver (Zhihai Xu via Jimmy Xiang)", "bug_id": "hive_61", "file": [{"additions": 26, "raw_url": "https://github.com/apache/hive/raw/a496e581152425773080aac48cf479e493cd5b74/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java", "blob_url": "https://github.com/apache/hive/blob/a496e581152425773080aac48cf479e493cd5b74/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java", "sha": "20ecbcdc6fd7a224316ddfd9d3992c5b9cbe261d", "changes": 38, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=a496e581152425773080aac48cf479e493cd5b74", "patch": "@@ -117,6 +117,8 @@\n   protected transient JobConf job;\n   public static MemoryMXBean memoryMXBean;\n   protected HadoopJobExecHelper jobExecHelper;\n+  private transient boolean isShutdown = false;\n+  private transient boolean jobKilled = false;\n \n   protected static transient final Logger LOG = LoggerFactory.getLogger(ExecDriver.class);\n \n@@ -413,10 +415,7 @@ public int execute(DriverContext driverContext) {\n \n       if (driverContext.isShutdown()) {\n         LOG.warn(\"Task was cancelled\");\n-        if (rj != null) {\n-          rj.killJob();\n-          rj = null;\n-        }\n+        killJob();\n         return 5;\n       }\n \n@@ -449,7 +448,7 @@ public int execute(DriverContext driverContext) {\n \n         if (rj != null) {\n           if (returnVal != 0) {\n-            rj.killJob();\n+            killJob();\n           }\n           jobID = rj.getID().toString();\n         }\n@@ -857,22 +856,37 @@ public void logPlanProgress(SessionState ss) throws IOException {\n     ss.getHiveHistory().logPlanProgress(queryPlan);\n   }\n \n+  public boolean isTaskShutdown() {\n+    return isShutdown;\n+  }\n+\n   @Override\n   public void shutdown() {\n     super.shutdown();\n-    if (rj != null) {\n+    killJob();\n+    isShutdown = true;\n+  }\n+\n+  @Override\n+  public String getExternalHandle() {\n+    return this.jobID;\n+  }\n+\n+  private void killJob() {\n+    boolean needToKillJob = false;\n+    synchronized(this) {\n+      if (rj != null && !jobKilled) {\n+        jobKilled = true;\n+        needToKillJob = true;\n+      }\n+    }\n+    if (needToKillJob) {\n       try {\n         rj.killJob();\n       } catch (Exception e) {\n         LOG.warn(\"failed to kill job \" + rj.getID(), e);\n       }\n-      rj = null;\n     }\n   }\n-\n-  @Override\n-  public String getExternalHandle() {\n-    return this.jobID;\n-  }\n }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8b0b83fd57553b4cb52129ff36c398e18230b649", "parent": "https://github.com/apache/hive/commit/6b87af7477219a3b62acb4b8ff4e614d45816d68", "message": "HIVE-10495 : Hive index creation code throws NPE if index table is null (Bing Li via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_62", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/8b0b83fd57553b4cb52129ff36c398e18230b649/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/8b0b83fd57553b4cb52129ff36c398e18230b649/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "58b9044930046758a83ee499692e5593cd82f9e0", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=8b0b83fd57553b4cb52129ff36c398e18230b649", "patch": "@@ -4706,7 +4706,8 @@ public boolean drop_partition_by_name_with_environment_context(final String db_n\n     @Override\n     public Index add_index(final Index newIndex, final Table indexTable)\n         throws InvalidObjectException, AlreadyExistsException, MetaException, TException {\n-      startFunction(\"add_index\", \": \" + newIndex.toString() + \" \" + indexTable.toString());\n+      String tableName = indexTable != null ? indexTable.getTableName() : \"\";\n+      startFunction(\"add_index\", \": \" + newIndex.toString() + \" \" + tableName);\n       Index ret = null;\n       Exception ex = null;\n       try {\n@@ -4725,7 +4726,6 @@ public Index add_index(final Index newIndex, final Table indexTable)\n           throw newMetaException(e);\n         }\n       } finally {\n-        String tableName = indexTable != null ? indexTable.getTableName() : null;\n         endFunction(\"add_index\", ret != null, ex, tableName);\n       }\n       return ret;", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/597ca1bdcd7d662be9cafb5238b6ae402a2972f1", "parent": "https://github.com/apache/hive/commit/7299c080f3619a858e56b3826b4f91c0bcf18c6b", "message": "HIVE-15471: LLAP UI: NPE when getting thread metrics (Rajesh Balamohan, reviewed by Gopal V)", "bug_id": "hive_63", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/597ca1bdcd7d662be9cafb5238b6ae402a2972f1/llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java", "blob_url": "https://github.com/apache/hive/blob/597ca1bdcd7d662be9cafb5238b6ae402a2972f1/llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java", "sha": "92c891343c6294d528aaa44c75e480c81cbbd813", "changes": 17, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java?ref=597ca1bdcd7d662be9cafb5238b6ae402a2972f1", "patch": "@@ -50,6 +50,7 @@\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n \n+import com.google.common.collect.Maps;\n import org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl;\n import org.apache.hadoop.metrics2.MetricsCollector;\n import org.apache.hadoop.metrics2.MetricsInfo;\n@@ -82,6 +83,8 @@\n   private long maxTimeLost = Long.MIN_VALUE;\n   private long maxTimeToKill = Long.MIN_VALUE;\n \n+  private final Map<String, Integer> executorNames;\n+\n   final MutableGaugeLong[] executorThreadCpuTime;\n   final MutableGaugeLong[] executorThreadUserTime;\n   @Metric\n@@ -152,6 +155,7 @@ private LlapDaemonExecutorMetrics(String displayName, JvmMetrics jm, String sess\n           \"ops\", \"latency\", interval);\n     }\n \n+    this.executorNames = Maps.newHashMap();\n     for (int i = 0; i < numExecutors; i++) {\n       MetricsInfo mic = new LlapDaemonCustomMetricsInfo(ExecutorThreadCPUTime.name() + \"_\" + i,\n           ExecutorThreadCPUTime.description());\n@@ -161,6 +165,7 @@ private LlapDaemonExecutorMetrics(String displayName, JvmMetrics jm, String sess\n       this.userMetricsInfoMap.put(i, miu);\n       this.executorThreadCpuTime[i] = registry.newGauge(mic, 0L);\n       this.executorThreadUserTime[i] = registry.newGauge(miu, 0L);\n+      this.executorNames.put(ContainerRunnerImpl.THREAD_NAME_FORMAT_PREFIX + i, i);\n     }\n   }\n \n@@ -304,13 +309,15 @@ private void updateThreadMetrics(MetricsRecordBuilder rb) {\n       final ThreadInfo[] infos = threadMXBean.getThreadInfo(ids);\n       for (int i = 0; i < ids.length; i++) {\n         ThreadInfo threadInfo = infos[i];\n+        if (threadInfo == null) {\n+          continue;\n+        }\n         String threadName = threadInfo.getThreadName();\n         long threadId = ids[i];\n-        for (int j = 0; j < numExecutors; j++) {\n-          if (threadName.equals(ContainerRunnerImpl.THREAD_NAME_FORMAT_PREFIX + j)) {\n-            executorThreadCpuTime[j].set(threadMXBean.getThreadCpuTime(threadId));\n-            executorThreadUserTime[j].set(threadMXBean.getThreadUserTime(threadId));\n-          }\n+        Integer id = executorNames.get(threadName);\n+        if (id != null) {\n+          executorThreadCpuTime[id].set(threadMXBean.getThreadCpuTime(threadId));\n+          executorThreadUserTime[id].set(threadMXBean.getThreadUserTime(threadId));\n         }\n       }\n ", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/metrics/LlapDaemonExecutorMetrics.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2e0493bdd4d59bf7b010988e82558e53f29e3d13", "parent": "https://github.com/apache/hive/commit/421815759cffbde92281df003a4b30fee492c370", "message": "HIVE-16251 : Vectorization: new octet_length function (HIVE15979) get NPE\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_64", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/OctetLength.java", "blob_url": "https://github.com/apache/hive/blob/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/OctetLength.java", "sha": "3b41ed43fe483f4791975ad4c23da6fa01b18c2f", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/OctetLength.java?ref=2e0493bdd4d59bf7b010988e82558e53f29e3d13", "patch": "@@ -25,8 +25,8 @@\n \n public class OctetLength extends VectorExpression {\n   private static final long serialVersionUID = 1L;\n-  private transient int colNum;\n-  private transient int outputColumn;\n+  private int colNum;\n+  private int outputColumn;\n \n   public OctetLength(int colNum, int outputColumn) {\n     this();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/OctetLength.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/queries/clientpositive/vector_udf_character_length.q", "blob_url": "https://github.com/apache/hive/blob/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/queries/clientpositive/vector_udf_character_length.q", "sha": "19a5260ddcc16a1bdb847fbd7a11e51f4936bade", "changes": 10, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/vector_udf_character_length.q?ref=2e0493bdd4d59bf7b010988e82558e53f29e3d13", "patch": "@@ -18,12 +18,14 @@ DROP TABLE dest1;\n CREATE TABLE dest1(name STRING) STORED AS TEXTFILE;\n LOAD DATA LOCAL INPATH '../../data/files/kv4.txt' INTO TABLE dest1;\n INSERT INTO dest1 VALUES(NULL);\n+CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1;\n \n-EXPLAIN SELECT character_length(dest1.name) FROM dest1;\n+EXPLAIN SELECT character_length(dest2.name) FROM dest2;\n -- SORT_BEFORE_DIFF\n-SELECT character_length(dest1.name) FROM dest1;\n+SELECT character_length(dest2.name) FROM dest2;\n \n-EXPLAIN SELECT char_length(dest1.name) FROM dest1;\n+EXPLAIN SELECT char_length(dest2.name) FROM dest2;\n -- SORT_BEFORE_DIFF\n-SELECT char_length(dest1.name) FROM dest1;\n+SELECT char_length(dest2.name) FROM dest2;\n DROP TABLE dest1;\n+DROP TABLE dest2;", "filename": "ql/src/test/queries/clientpositive/vector_udf_character_length.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/queries/clientpositive/vector_udf_octet_length.q", "blob_url": "https://github.com/apache/hive/blob/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/queries/clientpositive/vector_udf_octet_length.q", "sha": "06a49852a2710bac5a9b4a314fb7ff8b718638aa", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/vector_udf_octet_length.q?ref=2e0493bdd4d59bf7b010988e82558e53f29e3d13", "patch": "@@ -15,7 +15,9 @@ DROP TABLE dest1;\n CREATE TABLE dest1(name STRING) STORED AS TEXTFILE;\n LOAD DATA LOCAL INPATH '../../data/files/kv4.txt' INTO TABLE dest1;\n INSERT INTO dest1 VALUES(NULL);\n-EXPLAIN SELECT octet_length(dest1.name) FROM dest1;\n+CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1;\n+EXPLAIN SELECT octet_length(dest2.name) FROM dest2;\n -- SORT_BEFORE_DIFF\n-SELECT octet_length(dest1.name) FROM dest1;\n+SELECT octet_length(dest2.name) FROM dest2;\n DROP TABLE dest1;\n+DROP TABLE dest2;", "filename": "ql/src/test/queries/clientpositive/vector_udf_octet_length.q"}, {"additions": 43, "raw_url": "https://github.com/apache/hive/raw/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/llap/vector_udf_character_length.q.out", "blob_url": "https://github.com/apache/hive/blob/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/llap/vector_udf_character_length.q.out", "sha": "b26db2d7b16420d82f8e933273bf26e1372f7e55", "changes": 67, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/vector_udf_character_length.q.out?ref=2e0493bdd4d59bf7b010988e82558e53f29e3d13", "patch": "@@ -162,9 +162,20 @@ POSTHOOK: query: INSERT INTO dest1 VALUES(NULL)\n POSTHOOK: type: QUERY\n POSTHOOK: Output: default@dest1\n POSTHOOK: Lineage: dest1.name SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n-PREHOOK: query: EXPLAIN SELECT character_length(dest1.name) FROM dest1\n+PREHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@dest1\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@dest1\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@dest2\n+POSTHOOK: Lineage: dest2.name SIMPLE [(dest1)dest1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: EXPLAIN SELECT character_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-POSTHOOK: query: EXPLAIN SELECT character_length(dest1.name) FROM dest1\n+POSTHOOK: query: EXPLAIN SELECT character_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n@@ -178,41 +189,41 @@ STAGE PLANS:\n         Map 1 \n             Map Operator Tree:\n                 TableScan\n-                  alias: dest1\n-                  Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                  alias: dest2\n+                  Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                   Select Operator\n                     expressions: character_length(name) (type: int)\n                     outputColumnNames: _col0\n-                    Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                     File Output Operator\n                       compressed: false\n-                      Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                       table:\n                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                           output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-            Execution mode: llap\n-            LLAP IO: no inputs\n+            Execution mode: vectorized, llap\n+            LLAP IO: all inputs\n \n   Stage: Stage-0\n     Fetch Operator\n       limit: -1\n       Processor Tree:\n         ListSink\n \n-PREHOOK: query: SELECT character_length(dest1.name) FROM dest1\n+PREHOOK: query: SELECT character_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-PREHOOK: Input: default@dest1\n+PREHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-POSTHOOK: query: SELECT character_length(dest1.name) FROM dest1\n+POSTHOOK: query: SELECT character_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n-POSTHOOK: Input: default@dest1\n+POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n 2\n NULL\n-PREHOOK: query: EXPLAIN SELECT char_length(dest1.name) FROM dest1\n+PREHOOK: query: EXPLAIN SELECT char_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-POSTHOOK: query: EXPLAIN SELECT char_length(dest1.name) FROM dest1\n+POSTHOOK: query: EXPLAIN SELECT char_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n@@ -226,35 +237,35 @@ STAGE PLANS:\n         Map 1 \n             Map Operator Tree:\n                 TableScan\n-                  alias: dest1\n-                  Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                  alias: dest2\n+                  Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                   Select Operator\n                     expressions: character_length(name) (type: int)\n                     outputColumnNames: _col0\n-                    Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                     File Output Operator\n                       compressed: false\n-                      Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                       table:\n                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                           output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-            Execution mode: llap\n-            LLAP IO: no inputs\n+            Execution mode: vectorized, llap\n+            LLAP IO: all inputs\n \n   Stage: Stage-0\n     Fetch Operator\n       limit: -1\n       Processor Tree:\n         ListSink\n \n-PREHOOK: query: SELECT char_length(dest1.name) FROM dest1\n+PREHOOK: query: SELECT char_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-PREHOOK: Input: default@dest1\n+PREHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-POSTHOOK: query: SELECT char_length(dest1.name) FROM dest1\n+POSTHOOK: query: SELECT char_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n-POSTHOOK: Input: default@dest1\n+POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n 2\n NULL\n@@ -266,3 +277,11 @@ POSTHOOK: query: DROP TABLE dest1\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@dest1\n POSTHOOK: Output: default@dest1\n+PREHOOK: query: DROP TABLE dest2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@dest2\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: DROP TABLE dest2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@dest2\n+POSTHOOK: Output: default@dest2", "filename": "ql/src/test/results/clientpositive/llap/vector_udf_character_length.q.out"}, {"additions": 31, "raw_url": "https://github.com/apache/hive/raw/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/llap/vector_udf_octet_length.q.out", "blob_url": "https://github.com/apache/hive/blob/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/llap/vector_udf_octet_length.q.out", "sha": "10ace12ef5eaf4c631dae6af79c6dbcb00c2156c", "changes": 43, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/vector_udf_octet_length.q.out?ref=2e0493bdd4d59bf7b010988e82558e53f29e3d13", "patch": "@@ -145,9 +145,20 @@ POSTHOOK: query: INSERT INTO dest1 VALUES(NULL)\n POSTHOOK: type: QUERY\n POSTHOOK: Output: default@dest1\n POSTHOOK: Lineage: dest1.name SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n-PREHOOK: query: EXPLAIN SELECT octet_length(dest1.name) FROM dest1\n+PREHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@dest1\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@dest1\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@dest2\n+POSTHOOK: Lineage: dest2.name SIMPLE [(dest1)dest1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: EXPLAIN SELECT octet_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-POSTHOOK: query: EXPLAIN SELECT octet_length(dest1.name) FROM dest1\n+POSTHOOK: query: EXPLAIN SELECT octet_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n@@ -161,35 +172,35 @@ STAGE PLANS:\n         Map 1 \n             Map Operator Tree:\n                 TableScan\n-                  alias: dest1\n-                  Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                  alias: dest2\n+                  Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                   Select Operator\n                     expressions: octet_length(name) (type: int)\n                     outputColumnNames: _col0\n-                    Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                     File Output Operator\n                       compressed: false\n-                      Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                       table:\n                           input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                           output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-            Execution mode: llap\n-            LLAP IO: no inputs\n+            Execution mode: vectorized, llap\n+            LLAP IO: all inputs\n \n   Stage: Stage-0\n     Fetch Operator\n       limit: -1\n       Processor Tree:\n         ListSink\n \n-PREHOOK: query: SELECT octet_length(dest1.name) FROM dest1\n+PREHOOK: query: SELECT octet_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-PREHOOK: Input: default@dest1\n+PREHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-POSTHOOK: query: SELECT octet_length(dest1.name) FROM dest1\n+POSTHOOK: query: SELECT octet_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n-POSTHOOK: Input: default@dest1\n+POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n 6\n NULL\n@@ -201,3 +212,11 @@ POSTHOOK: query: DROP TABLE dest1\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@dest1\n POSTHOOK: Output: default@dest1\n+PREHOOK: query: DROP TABLE dest2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@dest2\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: DROP TABLE dest2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@dest2\n+POSTHOOK: Output: default@dest2", "filename": "ql/src/test/results/clientpositive/llap/vector_udf_octet_length.q.out"}, {"additions": 41, "raw_url": "https://github.com/apache/hive/raw/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/vector_udf_character_length.q.out", "blob_url": "https://github.com/apache/hive/blob/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/vector_udf_character_length.q.out", "sha": "558bfc80f2afb1a2b57c2c53447a422324e6db56", "changes": 61, "status": "modified", "deletions": 20, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vector_udf_character_length.q.out?ref=2e0493bdd4d59bf7b010988e82558e53f29e3d13", "patch": "@@ -197,9 +197,20 @@ POSTHOOK: query: INSERT INTO dest1 VALUES(NULL)\n POSTHOOK: type: QUERY\n POSTHOOK: Output: default@dest1\n POSTHOOK: Lineage: dest1.name SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n-PREHOOK: query: EXPLAIN SELECT character_length(dest1.name) FROM dest1\n+PREHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@dest1\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@dest1\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@dest2\n+POSTHOOK: Lineage: dest2.name SIMPLE [(dest1)dest1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: EXPLAIN SELECT character_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-POSTHOOK: query: EXPLAIN SELECT character_length(dest1.name) FROM dest1\n+POSTHOOK: query: EXPLAIN SELECT character_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n@@ -210,39 +221,40 @@ STAGE PLANS:\n     Map Reduce\n       Map Operator Tree:\n           TableScan\n-            alias: dest1\n-            Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+            alias: dest2\n+            Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n             Select Operator\n               expressions: character_length(name) (type: int)\n               outputColumnNames: _col0\n-              Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+              Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n               File Output Operator\n                 compressed: false\n-                Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                 table:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+      Execution mode: vectorized\n \n   Stage: Stage-0\n     Fetch Operator\n       limit: -1\n       Processor Tree:\n         ListSink\n \n-PREHOOK: query: SELECT character_length(dest1.name) FROM dest1\n+PREHOOK: query: SELECT character_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-PREHOOK: Input: default@dest1\n+PREHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-POSTHOOK: query: SELECT character_length(dest1.name) FROM dest1\n+POSTHOOK: query: SELECT character_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n-POSTHOOK: Input: default@dest1\n+POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n NULL\n 2\n-PREHOOK: query: EXPLAIN SELECT char_length(dest1.name) FROM dest1\n+PREHOOK: query: EXPLAIN SELECT char_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-POSTHOOK: query: EXPLAIN SELECT char_length(dest1.name) FROM dest1\n+POSTHOOK: query: EXPLAIN SELECT char_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n@@ -253,33 +265,34 @@ STAGE PLANS:\n     Map Reduce\n       Map Operator Tree:\n           TableScan\n-            alias: dest1\n-            Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+            alias: dest2\n+            Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n             Select Operator\n               expressions: character_length(name) (type: int)\n               outputColumnNames: _col0\n-              Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+              Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n               File Output Operator\n                 compressed: false\n-                Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                 table:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+      Execution mode: vectorized\n \n   Stage: Stage-0\n     Fetch Operator\n       limit: -1\n       Processor Tree:\n         ListSink\n \n-PREHOOK: query: SELECT char_length(dest1.name) FROM dest1\n+PREHOOK: query: SELECT char_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-PREHOOK: Input: default@dest1\n+PREHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-POSTHOOK: query: SELECT char_length(dest1.name) FROM dest1\n+POSTHOOK: query: SELECT char_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n-POSTHOOK: Input: default@dest1\n+POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n NULL\n 2\n@@ -291,3 +304,11 @@ POSTHOOK: query: DROP TABLE dest1\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@dest1\n POSTHOOK: Output: default@dest1\n+PREHOOK: query: DROP TABLE dest2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@dest2\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: DROP TABLE dest2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@dest2\n+POSTHOOK: Output: default@dest2", "filename": "ql/src/test/results/clientpositive/vector_udf_character_length.q.out"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/vector_udf_octet_length.q.out", "blob_url": "https://github.com/apache/hive/blob/2e0493bdd4d59bf7b010988e82558e53f29e3d13/ql/src/test/results/clientpositive/vector_udf_octet_length.q.out", "sha": "37a6786cbdcced1846489a26de879b59910a91f0", "changes": 40, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vector_udf_octet_length.q.out?ref=2e0493bdd4d59bf7b010988e82558e53f29e3d13", "patch": "@@ -180,9 +180,20 @@ POSTHOOK: query: INSERT INTO dest1 VALUES(NULL)\n POSTHOOK: type: QUERY\n POSTHOOK: Output: default@dest1\n POSTHOOK: Lineage: dest1.name SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n-PREHOOK: query: EXPLAIN SELECT octet_length(dest1.name) FROM dest1\n+PREHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@dest1\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: CREATE TABLE dest2 STORED AS ORC AS SELECT * FROM dest1\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@dest1\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@dest2\n+POSTHOOK: Lineage: dest2.name SIMPLE [(dest1)dest1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: EXPLAIN SELECT octet_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-POSTHOOK: query: EXPLAIN SELECT octet_length(dest1.name) FROM dest1\n+POSTHOOK: query: EXPLAIN SELECT octet_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n@@ -193,33 +204,34 @@ STAGE PLANS:\n     Map Reduce\n       Map Operator Tree:\n           TableScan\n-            alias: dest1\n-            Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+            alias: dest2\n+            Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n             Select Operator\n               expressions: octet_length(name) (type: int)\n               outputColumnNames: _col0\n-              Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+              Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n               File Output Operator\n                 compressed: false\n-                Statistics: Num rows: 1 Data size: 9 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 2 Data size: 90 Basic stats: COMPLETE Column stats: NONE\n                 table:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+      Execution mode: vectorized\n \n   Stage: Stage-0\n     Fetch Operator\n       limit: -1\n       Processor Tree:\n         ListSink\n \n-PREHOOK: query: SELECT octet_length(dest1.name) FROM dest1\n+PREHOOK: query: SELECT octet_length(dest2.name) FROM dest2\n PREHOOK: type: QUERY\n-PREHOOK: Input: default@dest1\n+PREHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-POSTHOOK: query: SELECT octet_length(dest1.name) FROM dest1\n+POSTHOOK: query: SELECT octet_length(dest2.name) FROM dest2\n POSTHOOK: type: QUERY\n-POSTHOOK: Input: default@dest1\n+POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n NULL\n 6\n@@ -231,3 +243,11 @@ POSTHOOK: query: DROP TABLE dest1\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@dest1\n POSTHOOK: Output: default@dest1\n+PREHOOK: query: DROP TABLE dest2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@dest2\n+PREHOOK: Output: default@dest2\n+POSTHOOK: query: DROP TABLE dest2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@dest2\n+POSTHOOK: Output: default@dest2", "filename": "ql/src/test/results/clientpositive/vector_udf_octet_length.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/70cc5eface64b5417916e42312befc022f4a06c0", "parent": "https://github.com/apache/hive/commit/1a39cbfcaeda4392e231b70afa343f2862e91f26", "message": "HIVE-15275 \"beeline -f <file>\" will throw NPE (Aihua Xu, reviewed by Vihang Karajgaonkar, Yongzhi Chen)", "bug_id": "hive_65", "file": [{"additions": 62, "raw_url": "https://github.com/apache/hive/raw/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/BeeLine.java", "blob_url": "https://github.com/apache/hive/blob/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/BeeLine.java", "sha": "65818dd48fa91b7cac2d3c07a3caa082b78e6365", "changes": 155, "status": "modified", "deletions": 93, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/BeeLine.java?ref=70cc5eface64b5417916e42312befc022f4a06c0", "patch": "@@ -23,12 +23,10 @@\n package org.apache.hive.beeline;\n \n import java.io.ByteArrayInputStream;\n-import java.io.ByteArrayOutputStream;\n import java.io.Closeable;\n import java.io.EOFException;\n import java.io.File;\n import java.io.FileInputStream;\n-import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.PrintStream;\n@@ -80,7 +78,6 @@\n import jline.console.completer.StringsCompleter;\n import jline.console.completer.FileNameCompleter;\n import jline.console.ConsoleReader;\n-import jline.console.history.History;\n import jline.console.history.FileHistory;\n \n import org.apache.commons.cli.CommandLine;\n@@ -146,7 +143,7 @@\n   private String dbName = null;\n   private String currentDatabase = null;\n \n-  private History history;\n+  private FileHistory history;\n   // Indicates if this instance of beeline is running in compatibility mode, or beeline mode\n   private boolean isBeeLine = true;\n \n@@ -517,14 +514,17 @@ public static void main(String[] args) throws IOException {\n   public static void mainWithInputRedirection(String[] args, InputStream inputStream)\n       throws IOException {\n     BeeLine beeLine = new BeeLine();\n-    int status = beeLine.begin(args, inputStream);\n+    try {\n+      int status = beeLine.begin(args, inputStream);\n \n-    if (!Boolean.getBoolean(BeeLineOpts.PROPERTY_NAME_EXIT)) {\n-        System.exit(status);\n+      if (!Boolean.getBoolean(BeeLineOpts.PROPERTY_NAME_EXIT)) {\n+          System.exit(status);\n+      }\n+    } finally {\n+      beeLine.close();\n     }\n   }\n \n-\n   public BeeLine() {\n     this(true);\n   }\n@@ -539,12 +539,11 @@ DatabaseConnection getDatabaseConnection() {\n \n \n   Connection getConnection() throws SQLException {\n-    if (getDatabaseConnections().current() == null) {\n-      throw new IllegalArgumentException(loc(\"no-current-connection\"));\n-    }\n-    if (getDatabaseConnections().current().getConnection() == null) {\n+    if (getDatabaseConnections().current() == null\n+        || getDatabaseConnections().current().getConnection() == null) {\n       throw new IllegalArgumentException(loc(\"no-current-connection\"));\n     }\n+\n     return getDatabaseConnections().current().getConnection();\n   }\n \n@@ -983,38 +982,36 @@ public int begin(String[] args, InputStream inputStream) throws IOException {\n       // nothing\n     }\n \n-    try {\n-      //this method also initializes the consoleReader which is\n-      //needed by initArgs for certain execution paths\n-      ConsoleReader reader = initializeConsoleReader(inputStream);\n-      if (isBeeLine) {\n-        int code = initArgs(args);\n-        if (code != 0) {\n-          return code;\n-        }\n-      } else {\n-        int code = initArgsFromCliVars(args);\n-        if (code != 0 || exit) {\n-          return code;\n-        }\n-        defaultConnect(false);\n-      }\n+    setupHistory();\n \n-      if (getOpts().isHelpAsked()) {\n-        return 0;\n-      }\n-      if (getOpts().getScriptFile() != null) {\n-        return executeFile(getOpts().getScriptFile());\n+    //this method also initializes the consoleReader which is\n+    //needed by initArgs for certain execution paths\n+    ConsoleReader reader = initializeConsoleReader(inputStream);\n+    if (isBeeLine) {\n+      int code = initArgs(args);\n+      if (code != 0) {\n+        return code;\n       }\n-      try {\n-        info(getApplicationTitle());\n-      } catch (Exception e) {\n-        // ignore\n+    } else {\n+      int code = initArgsFromCliVars(args);\n+      if (code != 0 || exit) {\n+        return code;\n       }\n-      return execute(reader, false);\n-    } finally {\n-        close();\n+      defaultConnect(false);\n+    }\n+\n+    if (getOpts().isHelpAsked()) {\n+      return 0;\n     }\n+    if (getOpts().getScriptFile() != null) {\n+      return executeFile(getOpts().getScriptFile());\n+    }\n+    try {\n+      info(getApplicationTitle());\n+    } catch (Exception e) {\n+      // ignore\n+    }\n+    return execute(reader, false);\n   }\n \n   /*\n@@ -1120,7 +1117,7 @@ public int defaultConnect(boolean exitOnError) {\n   }\n \n   private int executeFile(String fileName) {\n-    InputStream initStream = null;\n+    InputStream fileStream = null;\n     try {\n       if (!isBeeLine) {\n         org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(fileName);\n@@ -1132,17 +1129,16 @@ private int executeFile(String fileName) {\n         } else {\n           fs = FileSystem.get(path.toUri(), conf);\n         }\n-        initStream = fs.open(path);\n+        fileStream = fs.open(path);\n       } else {\n-        initStream = new FileInputStream(fileName);\n+        fileStream = new FileInputStream(fileName);\n       }\n-      return execute(initializeConsoleReader(initStream), !getOpts().getForce());\n+      return execute(initializeConsoleReader(fileStream), !getOpts().getForce());\n     } catch (Throwable t) {\n       handleException(t);\n       return ERRNO_OTHER;\n     } finally {\n-      IOUtils.closeStream(initStream);\n-      consoleReader = null;\n+      IOUtils.closeStream(fileStream);\n       output(\"\");   // dummy new line\n     }\n   }\n@@ -1181,6 +1177,25 @@ public void close() {\n     commands.closeall(null);\n   }\n \n+  private void setupHistory() throws IOException {\n+    if (this.history != null) {\n+       return;\n+    }\n+\n+    this.history = new FileHistory(new File(getOpts().getHistoryFile()));\n+    // add shutdown hook to flush the history to history file\n+    ShutdownHookManager.addShutdownHook(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          history.flush();\n+        } catch (IOException e) {\n+          error(e);\n+        }\n+      }\n+    });\n+  }\n+\n   public ConsoleReader initializeConsoleReader(InputStream inputStream) throws IOException {\n     if (inputStream != null) {\n       // ### NOTE: fix for sf.net bug 879425.\n@@ -1197,29 +1212,9 @@ public ConsoleReader initializeConsoleReader(InputStream inputStream) throws IOE\n     //disable the expandEvents for the purpose of backward compatibility\n     consoleReader.setExpandEvents(false);\n \n-    // setup history\n-    ByteArrayOutputStream hist = new ByteArrayOutputStream();\n-    if (new File(getOpts().getHistoryFile()).isFile()) {\n-      try {\n-        // save the current contents of the history buffer. This gets\n-        // around a bug in JLine where setting the output before the\n-        // input will clobber the history input, but setting the\n-        // input before the output will cause the previous commands\n-        // to not be saved to the buffer.\n-        try (FileInputStream historyIn = new FileInputStream(getOpts().getHistoryFile())) {\n-          int n;\n-          while ((n = historyIn.read()) != -1) {\n-            hist.write(n);\n-          }\n-        }\n-      } catch (Exception e) {\n-        handleException(e);\n-      }\n-    }\n-\n     try {\n       // now set the output for the history\n-      consoleReader.setHistory(new FileHistory(new File(getOpts().getHistoryFile())));\n+      consoleReader.setHistory(this.history);\n     } catch (Exception e) {\n       handleException(e);\n     }\n@@ -1228,32 +1223,6 @@ public ConsoleReader initializeConsoleReader(InputStream inputStream) throws IOE\n       // from script.. no need to load history and no need of completer, either\n       return consoleReader;\n     }\n-    try {\n-      // now load in the previous history\n-      if (hist != null) {\n-        History h = consoleReader.getHistory();\n-        if (!(h instanceof FileHistory)) {\n-          consoleReader.getHistory().add(hist.toString());\n-        }\n-      }\n-    } catch (Exception e) {\n-        handleException(e);\n-    }\n-\n-    // add shutdown hook to flush the history to history file\n-    ShutdownHookManager.addShutdownHook(new Runnable() {\n-        @Override\n-        public void run() {\n-            History h = consoleReader.getHistory();\n-            if (h instanceof FileHistory) {\n-                try {\n-                    ((FileHistory) h).flush();\n-                } catch (IOException e) {\n-                    error(e);\n-                }\n-            }\n-        }\n-    });\n \n     consoleReader.addCompleter(new BeeLineCompleter(this));\n     return consoleReader;", "filename": "beeline/src/java/org/apache/hive/beeline/BeeLine.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java", "blob_url": "https://github.com/apache/hive/blob/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java", "sha": "13fea293eb9df66d61cfd2fdf1a0c0626a571e1b", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java?ref=70cc5eface64b5417916e42312befc022f4a06c0", "patch": "@@ -32,6 +32,10 @@ public static void main(String[] args) throws IOException {\n \n   public int runWithArgs(String[] cmd, InputStream inputStream) throws IOException {\n     beeLine = new BeeLine(false);\n-    return beeLine.begin(cmd, inputStream);\n+    try {\n+      return beeLine.begin(cmd, inputStream);\n+    } finally {\n+      beeLine.close();\n+    }\n   }\n }", "filename": "beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java", "blob_url": "https://github.com/apache/hive/blob/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java", "sha": "623e667b3cc588d6ada5432e87e23f5ab48c59b8", "changes": 9, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java?ref=70cc5eface64b5417916e42312befc022f4a06c0", "patch": "@@ -22,6 +22,7 @@\n import java.io.File;\n import java.io.PrintStream;\n import java.io.PrintWriter;\n+import java.lang.reflect.Method;\n \n import org.junit.AfterClass;\n import org.junit.Assert;\n@@ -58,11 +59,14 @@ public void testNumHistories() throws Exception {\n     BeeLine beeline = new BeeLine();\n     beeline.getOpts().setHistoryFile(fileName);\n     beeline.setOutputStream(ops);\n+    Method method = beeline.getClass().getDeclaredMethod(\"setupHistory\");\n+    method.setAccessible(true);\n+    method.invoke(beeline);\n     beeline.initializeConsoleReader(null);\n     beeline.dispatch(\"!history\");\n     String output = os.toString(\"UTF-8\");\n     int numHistories = output.split(\"\\n\").length;\n-    Assert.assertEquals(numHistories, 10);\n+    Assert.assertEquals(10, numHistories);\n     beeline.close();\n   }\n \n@@ -73,6 +77,9 @@ public void testHistory() throws Exception {\n     BeeLine beeline = new BeeLine();\n     beeline.getOpts().setHistoryFile(fileName);\n     beeline.setOutputStream(ops);\n+    Method method = beeline.getClass().getDeclaredMethod(\"setupHistory\");\n+    method.setAccessible(true);\n+    method.invoke(beeline);\n     beeline.initializeConsoleReader(null);\n     beeline.dispatch(\"!history\");\n     String output = os.toString(\"UTF-8\");", "filename": "beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6", "parent": "https://github.com/apache/hive/commit/db21c3ff6c09ca920b9406efe95694d110219483", "message": "HIVE-16877 : NPE when issue query like alter table ... cascade onto non-partitioned table (Wang Haihua via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_66", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java", "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java", "sha": "226ba1895fc1b7d613083887c9bd79ea883c32ec", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6", "patch": "@@ -471,6 +471,8 @@\n   INVALID_JOIN_CONDITION(10407, \"Error parsing condition in join\"),\n   INVALID_TARGET_COLUMN_IN_SET_CLAUSE(10408, \"Target column \\\"{0}\\\" of set clause is not found in table \\\"{1}\\\".\", true),\n   HIVE_GROUPING_FUNCTION_EXPR_NOT_IN_GROUPBY(10409, \"Expression in GROUPING function not present in GROUP BY\"),\n+  ALTER_TABLE_NON_PARTITIONED_TABLE_CASCADE_NOT_SUPPORTED(10410,\n+      \"Alter table with non-partitioned table does not support cascade\"),\n   //========================== 20000 range starts here ========================//\n   SCRIPT_INIT_ERROR(20000, \"Unable to initialize custom script.\"),\n   SCRIPT_IO_ERROR(20001, \"An error occurred while reading or writing to your custom script. \"", "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "sha": "55f07afabace4f836deab2f4505fad9112552e78", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6", "patch": "@@ -1494,6 +1494,12 @@ private void addInputsOutputsAlterTable(String tableName, Map<String, String> pa\n     }\n \n     Table tab = getTable(tableName, true);\n+    // cascade only occurs with partitioned table\n+    if (isCascade && !tab.isPartitioned()) {\n+      throw new SemanticException(\n+          ErrorMsg.ALTER_TABLE_NON_PARTITIONED_TABLE_CASCADE_NOT_SUPPORTED);\n+    }\n+\n     // Determine the lock type to acquire\n     WriteEntity.WriteType writeType = WriteEntity.determineAlterTableWriteType(op);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q", "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q", "sha": "47ce383c8c656f3d648034e931d7f0a30608a06d", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6", "patch": "@@ -0,0 +1,4 @@\n+drop table if exists alter_table_non_partition_cascade;\n+create table alter_table_non_partitioned_cascade(c1 string, c2 string);\n+describe alter_table_non_partitioned_cascade;\n+alter table alter_table_non_partitioned_cascade add columns (c3 string) cascade;\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out", "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out", "sha": "afa648dfde19b83762abc557b5b7cb10a98fa901", "changes": 21, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6", "patch": "@@ -0,0 +1,21 @@\n+PREHOOK: query: drop table if exists alter_table_non_partition_cascade\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists alter_table_non_partition_cascade\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table alter_table_non_partitioned_cascade(c1 string, c2 string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@alter_table_non_partitioned_cascade\n+POSTHOOK: query: create table alter_table_non_partitioned_cascade(c1 string, c2 string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@alter_table_non_partitioned_cascade\n+PREHOOK: query: describe alter_table_non_partitioned_cascade\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@alter_table_non_partitioned_cascade\n+POSTHOOK: query: describe alter_table_non_partitioned_cascade\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@alter_table_non_partitioned_cascade\n+c1                  \tstring              \t                    \n+c2                  \tstring              \t                    \n+FAILED: SemanticException [Error 10410]: Alter table with non-partitioned table does not support cascade", "filename": "ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1d159ffd3c5016b78ca2814b837c02ab3f4be1de", "parent": "https://github.com/apache/hive/commit/7f4a3e17ec2fa886276a7f278e5846e0e7ebc8a6", "message": "HIVE-16122: NPE Hive Druid split introduced by HIVE-15928 (Slim Bouguerra, reviewed by Jesus Camacho Rodriguez)", "bug_id": "hive_67", "file": [{"additions": 17, "raw_url": "https://github.com/apache/hive/raw/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java", "blob_url": "https://github.com/apache/hive/blob/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java", "sha": "5159b426df985a15817ec13071be783ab18cc51a", "changes": 19, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java?ref=1d159ffd3c5016b78ca2814b837c02ab3f4be1de", "patch": "@@ -49,22 +49,37 @@ public HiveDruidSplit(String druidQuery, Path dummyPath, String hosts[]) {\n   public void write(DataOutput out) throws IOException {\n     super.write(out);\n     out.writeUTF(druidQuery);\n+    out.writeInt(hosts.length);\n+    for (String host : hosts) {\n+      out.writeUTF(host);\n+    }\n   }\n \n   @Override\n   public void readFields(DataInput in) throws IOException {\n     super.readFields(in);\n     druidQuery = in.readUTF();\n+    int length = in.readInt();\n+    String[] listHosts = new String[length];\n+    for (int i = 0; i < length; i++) {\n+      listHosts[i] = in.readUTF();\n+    }\n+    hosts = listHosts;\n   }\n \n   public String getDruidQuery() {\n     return druidQuery;\n   }\n \n+  @Override\n+  public String[] getLocations() throws IOException {\n+    return hosts;\n+  }\n+\n   @Override\n   public String toString() {\n-    return \"HiveDruidSplit{\" + druidQuery + \", \" \n-            + (hosts == null ? \"empty hosts\" : Arrays.toString(hosts))  + \"}\";\n+    return \"HiveDruidSplit{\" + druidQuery + \", \"\n+            + (hosts == null ? \"empty hosts\" : Arrays.toString(hosts)) + \"}\";\n   }\n \n }", "filename": "druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java"}, {"additions": 46, "raw_url": "https://github.com/apache/hive/raw/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java", "blob_url": "https://github.com/apache/hive/blob/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java", "sha": "234c783d25354cfa48890f659cc0083eff1c0c66", "changes": 46, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java?ref=1d159ffd3c5016b78ca2814b837c02ab3f4be1de", "patch": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.druid.io;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+public class TestHiveDruidSplit {\n+  @Test\n+  public void testSerDeser() throws IOException {\n+    HiveDruidSplit hiveDruidSplit = new HiveDruidSplit(\"query string\", new Path(\"test-path\"), new String []{\"host:8080\", \"host2:8090\"});\n+    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+    DataOutput dataOutput = new DataOutputStream(byteArrayOutputStream);\n+    hiveDruidSplit.write(dataOutput);\n+    ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArrayOutputStream.toByteArray());\n+    HiveDruidSplit actualHiveDruidSplit = new HiveDruidSplit();\n+    actualHiveDruidSplit.readFields(new DataInputStream(byteArrayInputStream));\n+    Assert.assertEquals(actualHiveDruidSplit.getDruidQuery(), \"query string\");\n+    Assert.assertArrayEquals(actualHiveDruidSplit.getLocations(),  new String []{\"host:8080\", \"host2:8090\"});\n+  }\n+\n+}\n\\ No newline at end of file", "filename": "druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2feaa5dc99febb8fd0367d8bfa4fe20d44930adc", "parent": "https://github.com/apache/hive/commit/12130c3ee71c2499d8224624a0e4cc7109727579", "message": "HIVE-15278 : PTF+MergeJoin = NPE (Sergey Shelukhin, reviewed by Gunther Hagleitner)", "bug_id": "hive_68", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/2feaa5dc99febb8fd0367d8bfa4fe20d44930adc/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/2feaa5dc99febb8fd0367d8bfa4fe20d44930adc/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java", "sha": "0b8eae8955e3f014a07a61732224c4f0fb31b6e2", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java?ref=2feaa5dc99febb8fd0367d8bfa4fe20d44930adc", "patch": "@@ -384,11 +384,15 @@ private void fetchNextGroup(Byte t) throws HiveException {\n       this.nextKeyWritables[t] = null;\n     }\n   }\n+  \n+  @Override\n+  public void close(boolean abort) throws HiveException {\n+    joinFinalLeftData(); // Do this WITHOUT checking for parents\n+    super.close(abort);\n+  }\n \n   @Override\n   public void closeOp(boolean abort) throws HiveException {\n-    joinFinalLeftData();\n-\n     super.closeOp(abort);\n \n     // clean up", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/2feaa5dc99febb8fd0367d8bfa4fe20d44930adc/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "blob_url": "https://github.com/apache/hive/blob/2feaa5dc99febb8fd0367d8bfa4fe20d44930adc/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "sha": "8b04cd44fa780b545d102b3b533cb495835b4b71", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java?ref=2feaa5dc99febb8fd0367d8bfa4fe20d44930adc", "patch": "@@ -667,6 +667,9 @@ protected boolean allInitializedParentsAreClosed() {\n   // since it is called by its parents' main thread, so no\n   // more than 1 thread should call this close() function.\n   public void close(boolean abort) throws HiveException {\n+    if (isLogDebugEnabled) {\n+      LOG.debug(\"close called for operator \" + this);\n+    }\n \n     if (state == State.CLOSE) {\n       return;\n@@ -683,12 +686,13 @@ public void close(boolean abort) throws HiveException {\n     // set state as CLOSE as long as all parents are closed\n     // state == CLOSE doesn't mean all children are also in state CLOSE\n     state = State.CLOSE;\n-    if (isLogDebugEnabled) {\n-      LOG.debug(id + \" finished. closing... \");\n+    if (isLogInfoEnabled) {\n+      LOG.info(\"Closing operator \" + this);\n     }\n \n     abort |= abortOp.get();\n \n+\n     // call the operator specific close routine\n     closeOp(abort);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/35be3f160135d009900ced22586989a5c122c2ef", "parent": "https://github.com/apache/hive/commit/09339d71c92feae450df9dddc9be0d9e3b711aa6", "message": "HIVE-15162: NPE in ATSHook (Jason Dere reviewed by Sergey Shelukhin)", "bug_id": "hive_69", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/35be3f160135d009900ced22586989a5c122c2ef/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "blob_url": "https://github.com/apache/hive/blob/35be3f160135d009900ced22586989a5c122c2ef/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "sha": "923ef08d8f6418afc37d068a36720d163d8edfa0", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/Driver.java?ref=35be3f160135d009900ced22586989a5c122c2ef", "patch": "@@ -1729,7 +1729,7 @@ public int execute(boolean deferClose) throws CommandNeedRetryException {\n \n       SessionState ss = SessionState.get();\n       hookContext = new HookContext(plan, queryState, ctx.getPathToCS(), ss.getUserName(),\n-          ss.getUserIpAddress(), operationId);\n+          ss.getUserIpAddress(), operationId, ss.getSessionId());\n       hookContext.setHookType(HookContext.HookType.PRE_EXEC_HOOK);\n \n       for (Hook peh : getHooks(HiveConf.ConfVars.PREEXECHOOKS)) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/35be3f160135d009900ced22586989a5c122c2ef/ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java", "blob_url": "https://github.com/apache/hive/blob/35be3f160135d009900ced22586989a5c122c2ef/ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java", "sha": "8ee5c04c9de3c1f59bf7dfbe3e3fddcc2892cd41", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java?ref=35be3f160135d009900ced22586989a5c122c2ef", "patch": "@@ -138,7 +138,7 @@ public void run() {\n               explain.initialize(queryState, plan, null, null);\n               String query = plan.getQueryStr();\n               JSONObject explainPlan = explain.getJSONPlan(null, work);\n-              String logID = conf.getLogIdVar(SessionState.get().getSessionId());\n+              String logID = conf.getLogIdVar(hookContext.getSessionId());\n               fireAndForget(conf, createPreHookEvent(queryId, query, explainPlan, queryStartTime,\n                 user, requestuser, numMrJobs, numTezJobs, opId, logID));\n               break;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/35be3f160135d009900ced22586989a5c122c2ef/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java", "blob_url": "https://github.com/apache/hive/blob/35be3f160135d009900ced22586989a5c122c2ef/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java", "sha": "3b4cc2c799fcafa8c5d8c154522eb31acb46fc8b", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java?ref=35be3f160135d009900ced22586989a5c122c2ef", "patch": "@@ -61,10 +61,11 @@\n   // unique id set for operation when run from HS2, base64 encoded value of\n   // TExecuteStatementResp.TOperationHandle.THandleIdentifier.guid\n   private final String operationId;\n+  private final String sessionId;\n \n   public HookContext(QueryPlan queryPlan, QueryState queryState,\n       Map<String, ContentSummary> inputPathToContentSummary, String userName, String ipAddress,\n-      String operationId) throws Exception {\n+      String operationId, String sessionId) throws Exception {\n     this.queryPlan = queryPlan;\n     this.queryState = queryState;\n     this.conf = queryState.getConf();\n@@ -82,6 +83,7 @@ public HookContext(QueryPlan queryPlan, QueryState queryState,\n     this.userName = userName;\n     this.ipAddress = ipAddress;\n     this.operationId = operationId;\n+    this.sessionId = sessionId;\n   }\n \n   public QueryPlan getQueryPlan() {\n@@ -199,4 +201,8 @@ public String getOperationId() {\n   public QueryState getQueryState() {\n     return queryState;\n   }\n+\n+  public String getSessionId() {\n+    return sessionId;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/509308f642f4af8eb44a9fb7f0f105198df9fac6", "parent": "https://github.com/apache/hive/commit/8aee8d4f2b124fcfa093724b4de0a54287a8084f", "message": "HIVE-16788: ODBC call SQLForeignKeys leads to NPE if you use PK arguments rather than FK arguments (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)", "bug_id": "hive_70", "file": [{"additions": 11, "raw_url": "https://github.com/apache/hive/raw/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "blob_url": "https://github.com/apache/hive/blob/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "sha": "4676e15942d72b0db56bedf0ff30aa60964c28d8", "changes": 12, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=509308f642f4af8eb44a9fb7f0f105198df9fac6", "patch": "@@ -8539,7 +8539,17 @@ private String getPrimaryKeyConstraintName(String db_name, String tbl_name) thro\n     final String parent_tbl_name = parent_tbl_name_input;\n     final String foreign_db_name = foreign_db_name_input;\n     final String foreign_tbl_name = foreign_tbl_name_input;\n-    return new GetListHelper<SQLForeignKey>(foreign_db_name, foreign_tbl_name, allowSql, allowJdo) {\n+    final String db_name;\n+    final String tbl_name;\n+    if (foreign_tbl_name == null) {\n+      // The FK table name might be null if we are retrieving the constraint from the PK side\n+      db_name = parent_db_name_input;\n+      tbl_name = parent_tbl_name_input;\n+    } else {\n+      db_name = foreign_db_name_input;\n+      tbl_name = foreign_tbl_name_input;\n+    }\n+    return new GetListHelper<SQLForeignKey>(db_name, tbl_name, allowSql, allowJdo) {\n \n       @Override\n       protected List<SQLForeignKey> getSqlResult(GetHelper<List<SQLForeignKey>> ctx) throws MetaException {", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java"}, {"additions": 46, "raw_url": "https://github.com/apache/hive/raw/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java", "blob_url": "https://github.com/apache/hive/blob/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java", "sha": "b28ea7359357406fcd0ffc01a864ff572ab5f278", "changes": 49, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java?ref=509308f642f4af8eb44a9fb7f0f105198df9fac6", "patch": "@@ -63,6 +63,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.collect.ImmutableList;\n+\n import javax.jdo.Query;\n \n public class TestObjectStore {\n@@ -204,22 +206,63 @@ public void testDatabaseOps() throws MetaException, InvalidObjectException, NoSu\n   public void testTableOps() throws MetaException, InvalidObjectException, NoSuchObjectException, InvalidInputException {\n     Database db1 = new Database(DB1, \"description\", \"locationurl\", null);\n     objectStore.createDatabase(db1);\n-    StorageDescriptor sd = new StorageDescriptor(null, \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null), null, null, null);\n+    StorageDescriptor sd1 = new StorageDescriptor(ImmutableList.of(new FieldSchema(\"pk_col\", \"double\", null)),\n+            \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null),\n+            null, null, null);\n     HashMap<String,String> params = new HashMap<String,String>();\n     params.put(\"EXTERNAL\", \"false\");\n-    Table tbl1 = new Table(TABLE1, DB1, \"owner\", 1, 2, 3, sd, null, params, null, null, \"MANAGED_TABLE\");\n+    Table tbl1 = new Table(TABLE1, DB1, \"owner\", 1, 2, 3, sd1, null, params, null, null, \"MANAGED_TABLE\");\n     objectStore.createTable(tbl1);\n \n     List<String> tables = objectStore.getAllTables(DB1);\n     Assert.assertEquals(1, tables.size());\n     Assert.assertEquals(TABLE1, tables.get(0));\n \n-    Table newTbl1 = new Table(\"new\" + TABLE1, DB1, \"owner\", 1, 2, 3, sd, null, params, null, null, \"MANAGED_TABLE\");\n+    StorageDescriptor sd2 = new StorageDescriptor(ImmutableList.of(new FieldSchema(\"fk_col\", \"double\", null)),\n+            \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null),\n+            null, null, null);\n+    Table newTbl1 = new Table(\"new\" + TABLE1, DB1, \"owner\", 1, 2, 3, sd2, null, params, null, null, \"MANAGED_TABLE\");\n     objectStore.alterTable(DB1, TABLE1, newTbl1);\n     tables = objectStore.getTables(DB1, \"new*\");\n     Assert.assertEquals(1, tables.size());\n     Assert.assertEquals(\"new\" + TABLE1, tables.get(0));\n \n+    objectStore.createTable(tbl1);\n+    tables = objectStore.getAllTables(DB1);\n+    Assert.assertEquals(2, tables.size());\n+\n+    List<SQLForeignKey> foreignKeys = objectStore.getForeignKeys(DB1, TABLE1, null, null);\n+    Assert.assertEquals(0, foreignKeys.size());\n+\n+    SQLPrimaryKey pk = new SQLPrimaryKey(DB1, TABLE1, \"pk_col\", 1,\n+            \"pk_const_1\", false, false, false);\n+    objectStore.addPrimaryKeys(ImmutableList.of(pk));\n+    SQLForeignKey fk = new SQLForeignKey(DB1, TABLE1, \"pk_col\",\n+            DB1, \"new\" + TABLE1, \"fk_col\", 1,\n+            0, 0, \"fk_const_1\", \"pk_const_1\", false, false, false);\n+    objectStore.addForeignKeys(ImmutableList.of(fk));\n+\n+    // Retrieve from PK side\n+    foreignKeys = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n+    Assert.assertEquals(1, foreignKeys.size());\n+\n+    List<SQLForeignKey> fks = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n+    if (fks != null) {\n+      for (SQLForeignKey fkcol : fks) {\n+        objectStore.dropConstraint(fkcol.getFktable_db(), fkcol.getFktable_name(), fkcol.getFk_name());\n+      }\n+    }\n+    // Retrieve from FK side\n+    foreignKeys = objectStore.getForeignKeys(DB1, TABLE1, null, null);\n+    Assert.assertEquals(0, foreignKeys.size());\n+    // Retrieve from PK side\n+    foreignKeys = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n+    Assert.assertEquals(0, foreignKeys.size());\n+\n+    objectStore.dropTable(DB1, TABLE1);\n+    tables = objectStore.getAllTables(DB1);\n+    Assert.assertEquals(1, tables.size());\n+\n     objectStore.dropTable(DB1, \"new\" + TABLE1);\n     tables = objectStore.getAllTables(DB1);\n     Assert.assertEquals(0, tables.size());", "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c", "parent": "https://github.com/apache/hive/commit/7cca0978af944b4a76dd40e014e628f82f43c42f", "message": "HIVE-15778: DROP INDEX (non-existent) throws NPE when using DbNotificationListener (Vamsee Yarlagadda, reviewed by Aihua Xu)", "bug_id": "hive_71", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "f8c3c4e48db0df9d6c18801bcd61f9e5dc6eb7c2", "changes": 9, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c", "patch": "@@ -4614,9 +4614,12 @@ private boolean drop_index_by_name_core(final RawStore ms,\n           deleteTableData(tblPath);\n           // ok even if the data is not deleted\n         }\n-        for (MetaStoreEventListener listener : listeners) {\n-          DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n-          listener.onDropIndex(dropIndexEvent);\n+        // Skip the event listeners if the index is NULL\n+        if (index != null) {\n+          for (MetaStoreEventListener listener : listeners) {\n+            DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n+            listener.onDropIndex(dropIndexEvent);\n+          }\n         }\n       }\n       return success;", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/43e0497677a55f18fbf8bd3ba9ea41b87fcdb033", "parent": "https://github.com/apache/hive/commit/06b2b0e998183be6ed102ca57de1e00d39db3e95", "message": "HIVE-15649 : LLAP IO may NPE on all-column read (Sergey Shelukhin, reviewed by Prasanth Jayachandran) ADDENDUM", "bug_id": "hive_72", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/43e0497677a55f18fbf8bd3ba9ea41b87fcdb033/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java", "blob_url": "https://github.com/apache/hive/blob/43e0497677a55f18fbf8bd3ba9ea41b87fcdb033/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java", "sha": "9ef9ca486760d448ce4fdb35b31cf4d0e37c18c2", "changes": 19, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java?ref=43e0497677a55f18fbf8bd3ba9ea41b87fcdb033", "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.hadoop.hive.llap.io.api.impl;\n \n+import java.util.ArrayList;\n+\n import java.io.IOException;\n import java.util.LinkedList;\n import java.util.List;\n@@ -98,7 +100,6 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols\n     this.executor = executor;\n     this.jobConf = job;\n     this.split = split;\n-    this.columnIds = includedCols;\n     this.sarg = ConvertAstToSearchArg.createFromConf(job);\n     this.columnNames = ColumnProjectionUtils.getReadColumnNames(job);\n     final String fragmentId = LlapTezUtils.getFragmentId(job);\n@@ -122,12 +123,13 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols\n     rbCtx = ctx != null ? ctx : LlapInputFormat.createFakeVrbCtx(mapWork);\n     if (includedCols == null) {\n       // Assume including everything means the VRB will have everything.\n-      this.columnCount = rbCtx.getRowColumnTypeInfos().length;\n-    } else {\n-      this.columnCount = columnIds.size();\n+      includedCols = new ArrayList<>(rbCtx.getRowColumnTypeInfos().length);\n+      for (int i = 0; i < rbCtx.getRowColumnTypeInfos().length; ++i) {\n+        includedCols.add(i);\n+      }\n     }\n-\n-\n+    this.columnIds = includedCols;\n+    this.columnCount = columnIds.size();\n \n     int partitionColumnCount = rbCtx.getPartitionColumnCount();\n     if (partitionColumnCount > 0) {\n@@ -169,8 +171,7 @@ private boolean checkOrcSchemaEvolution() {\n     SchemaEvolution schemaEvolution = new SchemaEvolution(\n         fileSchema, rp.getReaderSchema(), includedColumns);\n     for (int i = 0; i < columnCount; ++i) {\n-      int colId = columnIds == null ? i : columnIds.get(i);\n-      if (!schemaEvolution.isPPDSafeConversion(colId)) {\n+      if (!schemaEvolution.isPPDSafeConversion(columnIds.get(i))) {\n         LlapIoImpl.LOG.warn(\"Unsupported schema evolution! Disabling Llap IO for {}\", split);\n         return false;\n       }\n@@ -214,7 +215,7 @@ public boolean next(NullWritable key, VectorizedRowBatch value) throws IOExcepti\n     // VRB was created from VrbCtx, so we already have pre-allocated column vectors\n     for (int i = 0; i < cvb.cols.length; ++i) {\n       // Return old CVs (if any) to caller. We assume these things all have the same schema.\n-      cvb.swapColumnVector(i, value.cols, columnIds == null ? i : columnIds.get(i));\n+      cvb.swapColumnVector(i, value.cols, columnIds.get(i));\n     }\n     value.selectedInUse = false;\n     value.size = cvb.size;", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/044f1fa3ce868d4880adbe8c92a24e15e8b88dba", "parent": "https://github.com/apache/hive/commit/f562dfb5207e8246e5f12696e0d7f373c3e3bf4c", "message": "HIVE-14974: TestBeeLineHistory throws NPE in ShutdownHook (Prasanth Jayachandran reviewed by Siddharth Seth)", "bug_id": "hive_73", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/044f1fa3ce868d4880adbe8c92a24e15e8b88dba/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java", "blob_url": "https://github.com/apache/hive/blob/044f1fa3ce868d4880adbe8c92a24e15e8b88dba/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java", "sha": "5f99a0edb401e1d220b81b2953ca839fd93d0a7b", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java?ref=044f1fa3ce868d4880adbe8c92a24e15e8b88dba", "patch": "@@ -33,7 +33,7 @@\n  */\n public class TestBeeLineHistory {\n \n-  private static final String fileName = \"history\";\n+  private static final String fileName = System.getProperty(\"test.tmp.dir\") + \"/history\";\n \n   @BeforeClass\n   public static void beforeTests() throws Exception {", "filename": "beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1a3e4be3dbd485f2630c7249254727ce58374d1c", "parent": "https://github.com/apache/hive/commit/c9224d58cce6e0b0520598894e962c48ce9d97e3", "message": "HIVE-14814: metastoreClient is used directly in Hive cause NPE (Prasanth Jayachandran reviewed by Eugene Koifman)", "bug_id": "hive_74", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/1a3e4be3dbd485f2630c7249254727ce58374d1c/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "blob_url": "https://github.com/apache/hive/blob/1a3e4be3dbd485f2630c7249254727ce58374d1c/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "sha": "de6adb5047bf6457d031050b6e72d54a6534ccfa", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java?ref=1a3e4be3dbd485f2630c7249254727ce58374d1c", "patch": "@@ -1864,7 +1864,7 @@ public Void call() throws Exception {\n         for (Partition p : partitionsMap.values()) {\n           partNames.add(p.getName());\n         }\n-        metaStoreClient.addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n+        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n           partNames, AcidUtils.toDataOperationType(operation));\n       }\n       LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4e3a62071d8a8d77892edc0ef3f282b190cd96fd", "parent": "https://github.com/apache/hive/commit/c3a9fa170377bf2d80dc6e6efa46187c6c5236b2", "message": "HIVE-15542: NPE in StatsUtils::getColStatistics when all values in DATE column are NULL (Rajesh Balamohan, reviewed by Gopal V)", "bug_id": "hive_75", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/4e3a62071d8a8d77892edc0ef3f282b190cd96fd/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "blob_url": "https://github.com/apache/hive/blob/4e3a62071d8a8d77892edc0ef3f282b190cd96fd/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "sha": "0da7ea434e0d75c1d07044ec5dfe7de673e22768", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java?ref=4e3a62071d8a8d77892edc0ef3f282b190cd96fd", "patch": "@@ -756,8 +756,12 @@ public static ColStatistics getColStatistics(ColumnStatisticsObj cso, String tab\n       }\n     } else if (colTypeLowerCase.equals(serdeConstants.DATE_TYPE_NAME)) {\n       cs.setAvgColLen(JavaDataModel.get().lengthOfDate());\n-      cs.setRange(csd.getDateStats().getLowValue().getDaysSinceEpoch(),\n-              csd.getDateStats().getHighValue().getDaysSinceEpoch());\n+      cs.setNumNulls(csd.getDateStats().getNumNulls());\n+      Long lowVal = (csd.getDateStats().getLowValue() != null) ? csd.getDateStats().getLowValue()\n+          .getDaysSinceEpoch() : null;\n+      Long highVal = (csd.getDateStats().getHighValue() != null) ? csd.getDateStats().getHighValue()\n+          .getDaysSinceEpoch() : null;\n+      cs.setRange(lowVal, highVal);\n     } else {\n       // Columns statistics for complex datatypes are not supported yet\n       return null;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/4e3a62071d8a8d77892edc0ef3f282b190cd96fd/ql/src/test/queries/clientpositive/analyze_tbl_date.q", "blob_url": "https://github.com/apache/hive/blob/4e3a62071d8a8d77892edc0ef3f282b190cd96fd/ql/src/test/queries/clientpositive/analyze_tbl_date.q", "sha": "6726b838544bb4bde7c35b95e9c3872a126998e4", "changes": 14, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/analyze_tbl_date.q?ref=4e3a62071d8a8d77892edc0ef3f282b190cd96fd", "patch": "@@ -0,0 +1,14 @@\n+set hive.fetch.task.conversion=none;\n+\n+create table test_table(d date);\n+\n+insert into test_table values(null), (null), (null);\n+\n+analyze table test_table compute statistics for columns;\n+\n+describe formatted test_table;\n+\n+explain select * from test_table where d is not null;\n+\n+select * from test_table where d is not null;\n+", "filename": "ql/src/test/queries/clientpositive/analyze_tbl_date.q"}, {"additions": 101, "raw_url": "https://github.com/apache/hive/raw/4e3a62071d8a8d77892edc0ef3f282b190cd96fd/ql/src/test/results/clientpositive/analyze_tbl_date.q.out", "blob_url": "https://github.com/apache/hive/blob/4e3a62071d8a8d77892edc0ef3f282b190cd96fd/ql/src/test/results/clientpositive/analyze_tbl_date.q.out", "sha": "a0cdbca8c14fc282a467fb8e48953e2fe77d960b", "changes": 101, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/analyze_tbl_date.q.out?ref=4e3a62071d8a8d77892edc0ef3f282b190cd96fd", "patch": "@@ -0,0 +1,101 @@\n+PREHOOK: query: create table test_table(d date)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@test_table\n+POSTHOOK: query: create table test_table(d date)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test_table\n+PREHOOK: query: insert into test_table values(null), (null), (null)\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@test_table\n+POSTHOOK: query: insert into test_table values(null), (null), (null)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@test_table\n+POSTHOOK: Lineage: test_table.d EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+PREHOOK: query: analyze table test_table compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table test_table compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_table\n+#### A masked pattern was here ####\n+PREHOOK: query: describe formatted test_table\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@test_table\n+POSTHOOK: query: describe formatted test_table\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@test_table\n+# col_name            \tdata_type           \tcomment             \n+\t \t \n+d                   \tdate                \t                    \n+\t \t \n+# Detailed Table Information\t \t \n+Database:           \tdefault             \t \n+#### A masked pattern was here ####\n+Retention:          \t0                   \t \n+#### A masked pattern was here ####\n+Table Type:         \tMANAGED_TABLE       \t \n+Table Parameters:\t \t \n+\tCOLUMN_STATS_ACCURATE\t{\\\"BASIC_STATS\\\":\\\"true\\\",\\\"COLUMN_STATS\\\":{\\\"d\\\":\\\"true\\\"}}\n+\tnumFiles            \t1                   \n+\tnumRows             \t3                   \n+\trawDataSize         \t6                   \n+\ttotalSize           \t9                   \n+#### A masked pattern was here ####\n+\t \t \n+# Storage Information\t \t \n+SerDe Library:      \torg.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\t \n+InputFormat:        \torg.apache.hadoop.mapred.TextInputFormat\t \n+OutputFormat:       \torg.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\t \n+Compressed:         \tNo                  \t \n+Num Buckets:        \t-1                  \t \n+Bucket Columns:     \t[]                  \t \n+Sort Columns:       \t[]                  \t \n+Storage Desc Params:\t \t \n+\tserialization.format\t1                   \n+PREHOOK: query: explain select * from test_table where d is not null\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select * from test_table where d is not null\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: test_table\n+            Statistics: Num rows: 3 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: d is not null (type: boolean)\n+              Statistics: Num rows: 3 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: d (type: date)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 3 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 3 Data size: 6 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select * from test_table where d is not null\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from test_table where d is not null\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_table\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/analyze_tbl_date.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/406e935f27f60bb01c53d54bdb2c91429c95207e", "parent": "https://github.com/apache/hive/commit/0705323db28edf13777d29d3a0add48f19936db0", "message": "HIVE-14621 : LLAP: memory.mode = none has NPE (Sergey Shelukhin, reviewed by Prasanth Jayachandran)", "bug_id": "hive_76", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "sha": "cb0d96f50a2d38822cacb09fd98833c925d2af77", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -2733,10 +2733,10 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether the LLAP IO layer is enabled for non-vectorized queries that read inputs\\n\" +\n         \"that can be vectorized\"),\n     LLAP_IO_MEMORY_MODE(\"hive.llap.io.memory.mode\", \"cache\",\n-        new StringSet(\"cache\", \"allocator\", \"none\"),\n+        new StringSet(\"cache\", \"none\"),\n         \"LLAP IO memory usage; 'cache' (the default) uses data and metadata cache with a\\n\" +\n-        \"custom off-heap allocator, 'allocator' uses the custom allocator without the caches,\\n\" +\n-        \"'none' doesn't use either (this mode may result in significant performance degradation)\"),\n+        \"custom off-heap allocator, 'none' doesn't use either (this mode may result in\\n\" +\n+        \"significant performance degradation)\"),\n     LLAP_ALLOCATOR_MIN_ALLOC(\"hive.llap.io.allocator.alloc.min\", \"16Kb\", new SizeValidator(),\n         \"Minimum allocation possible from LLAP buddy allocator. Allocations below that are\\n\" +\n         \"padded to minimum allocation. For ORC, should generally be the same as the expected\\n\" +", "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java", "sha": "b6fd3e35fcbc4146a03acbf48f4cca8afee21e07", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -26,10 +26,10 @@\n  * Eviction dispatcher - uses double dispatch to route eviction notifications to correct caches.\n  */\n public final class EvictionDispatcher implements EvictionListener {\n-  private final LowLevelCacheImpl dataCache;\n+  private final LowLevelCache dataCache;\n   private final OrcMetadataCache metadataCache;\n \n-  public EvictionDispatcher(LowLevelCacheImpl dataCache, OrcMetadataCache metadataCache) {\n+  public EvictionDispatcher(LowLevelCache dataCache, OrcMetadataCache metadataCache) {\n     this.dataCache = dataCache;\n     this.metadataCache = metadataCache;\n   }", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java", "sha": "19c589a9fd7835c7693344355a3ce8034bdde89c", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -59,4 +59,7 @@ DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseOffset,\n    */\n   long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] chunks,\n       long baseOffset, Priority priority, LowLevelCacheCounters qfCounters);\n+\n+  /** Notifies the cache that a particular buffer should be removed due to eviction. */\n+  void notifyEvicted(MemoryBuffer buffer);\n }", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java", "sha": "ea458cab4a3f6b4c8289eff97ee4bcfd6994aa84", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -67,7 +67,7 @@ public LowLevelCacheImpl(LlapDaemonCacheMetrics metrics, LowLevelCachePolicy cac\n     this.doAssumeGranularBlocks = doAssumeGranularBlocks;\n   }\n \n-  public void init() {\n+  public void startThreads() {\n     if (cleanupInterval < 0) return;\n     cleanupThread = new CleanupThread(cleanupInterval);\n     cleanupThread.start();\n@@ -368,7 +368,8 @@ public static LlapDataBuffer allocateFake() {\n     return fake;\n   }\n \n-  public final void notifyEvicted(LlapDataBuffer buffer) {\n+  @Override\n+  public final void notifyEvicted(MemoryBuffer buffer) {\n     allocator.deallocateEvicted(buffer);\n     newEvictions.incrementAndGet();\n   }", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java", "sha": "d8f59d1fa98e163a39e90b3d623b4c4bebe87169", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -68,6 +68,7 @@ public void deallocate(MemoryBuffer buffer) {\n     LlapDataBuffer buf = (LlapDataBuffer)buffer;\n     ByteBuffer bb = buf.byteBuffer;\n     buf.byteBuffer = null;\n+    if (!bb.isDirect()) return;\n     Field field = cleanerField;\n     if (field == null) return;\n     try {", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java"}, {"additions": 31, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java", "sha": "d1eee045499fadcc238a7ca8e28939222e9b1500", "changes": 33, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -20,12 +20,15 @@\n import java.util.List;\n \n import org.apache.hadoop.hive.common.io.Allocator;\n+import org.apache.hadoop.hive.common.io.DataCache.BooleanRef;\n+import org.apache.hadoop.hive.common.io.DataCache.DiskRangeListFactory;\n+import org.apache.hadoop.hive.common.io.DiskRange;\n+import org.apache.hadoop.hive.common.io.DiskRangeList;\n import org.apache.hadoop.hive.common.io.encoded.MemoryBuffer;\n-import org.apache.hadoop.hive.llap.DebugUtils;\n import org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl;\n import org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics;\n \n-public class SimpleBufferManager implements BufferUsageManager {\n+public class SimpleBufferManager implements BufferUsageManager, LowLevelCache {\n   private final Allocator allocator;\n   private final LlapDaemonCacheMetrics metrics;\n \n@@ -73,4 +76,30 @@ public boolean incRefBuffer(MemoryBuffer buffer) {\n   public Allocator getAllocator() {\n     return allocator;\n   }\n+\n+  @Override\n+  public DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseOffset,\n+      DiskRangeListFactory factory, LowLevelCacheCounters qfCounters, BooleanRef gotAllData) {\n+    return range; // Nothing changes - no cache.\n+  }\n+\n+  @Override\n+  public long[] putFileData(Object fileKey, DiskRange[] ranges,\n+      MemoryBuffer[] chunks, long baseOffset, Priority priority,\n+      LowLevelCacheCounters qfCounters) {\n+    for (int i = 0; i < chunks.length; ++i) {\n+      LlapDataBuffer buffer = (LlapDataBuffer)chunks[i];\n+      if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+        LlapIoImpl.LOCKING_LOGGER.trace(\"Locking {} at put time (no cache)\", buffer);\n+      }\n+      boolean canLock = lockBuffer(buffer);\n+      assert canLock;\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  public void notifyEvicted(MemoryBuffer buffer) {\n+    throw new UnsupportedOperationException(\"Buffer manager doesn't have cache\");\n+  }\n }", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java", "sha": "8048624a2b7c0562525d2ce147f8ca1a9de1c8ff", "changes": 36, "status": "modified", "deletions": 19, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.hive.llap.cache.BufferUsageManager;\n import org.apache.hadoop.hive.llap.cache.EvictionAwareAllocator;\n import org.apache.hadoop.hive.llap.cache.EvictionDispatcher;\n+import org.apache.hadoop.hive.llap.cache.LowLevelCache;\n import org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl;\n import org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager;\n import org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy;\n@@ -71,7 +72,7 @@\n   public static final Logger CACHE_LOGGER = LoggerFactory.getLogger(\"LlapIoCache\");\n   public static final Logger LOCKING_LOGGER = LoggerFactory.getLogger(\"LlapIoLocking\");\n \n-  private static final String MODE_CACHE = \"cache\", MODE_ALLOCATOR = \"allocator\";\n+  private static final String MODE_CACHE = \"cache\";\n \n   private final ColumnVectorProducer cvp;\n   private final ExecutorService executor;\n@@ -82,9 +83,8 @@\n \n   private LlapIoImpl(Configuration conf) throws IOException {\n     String ioMode = HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_IO_MEMORY_MODE);\n-    boolean useLowLevelCache = LlapIoImpl.MODE_CACHE.equalsIgnoreCase(ioMode),\n-        useAllocOnly = !useLowLevelCache && LlapIoImpl.MODE_ALLOCATOR.equalsIgnoreCase(ioMode);\n-    LOG.info(\"Initializing LLAP IO in {} mode\", ioMode);\n+    boolean useLowLevelCache = LlapIoImpl.MODE_CACHE.equalsIgnoreCase(ioMode);\n+    LOG.info(\"Initializing LLAP IO in {} mode\", useLowLevelCache ? LlapIoImpl.MODE_CACHE : \"none\");\n     String displayName = \"LlapDaemonCacheMetrics-\" + MetricsUtils.getHostName();\n     String sessionId = conf.get(\"llap.daemon.metrics.sessionid\");\n     this.cacheMetrics = LlapDaemonCacheMetrics.create(displayName, sessionId);\n@@ -109,7 +109,7 @@ private LlapIoImpl(Configuration conf) throws IOException {\n         sessionId);\n \n     OrcMetadataCache metadataCache = null;\n-    LowLevelCacheImpl orcCache = null;\n+    LowLevelCache cache = null;\n     BufferUsageManager bufferManager = null;\n     if (useLowLevelCache) {\n       // Memory manager uses cache policy to trigger evictions, so create the policy first.\n@@ -122,23 +122,21 @@ private LlapIoImpl(Configuration conf) throws IOException {\n       // Cache uses allocator to allocate and deallocate, create allocator and then caches.\n       EvictionAwareAllocator allocator = new BuddyAllocator(conf, memManager, cacheMetrics);\n       this.allocator = allocator;\n-      orcCache = new LowLevelCacheImpl(cacheMetrics, cachePolicy, allocator, true);\n+      LowLevelCacheImpl cacheImpl = new LowLevelCacheImpl(\n+          cacheMetrics, cachePolicy, allocator, true);\n+      cache = cacheImpl;\n       boolean useGapCache = HiveConf.getBoolVar(conf, ConfVars.LLAP_CACHE_ENABLE_ORC_GAP_CACHE);\n       metadataCache = new OrcMetadataCache(memManager, cachePolicy, useGapCache);\n       // And finally cache policy uses cache to notify it of eviction. The cycle is complete!\n-      cachePolicy.setEvictionListener(new EvictionDispatcher(orcCache, metadataCache));\n-      cachePolicy.setParentDebugDumper(orcCache);\n-      orcCache.init(); // Start the cache threads.\n-      bufferManager = orcCache; // Cache also serves as buffer manager.\n+      cachePolicy.setEvictionListener(new EvictionDispatcher(cache, metadataCache));\n+      cachePolicy.setParentDebugDumper(cacheImpl);\n+      cacheImpl.startThreads(); // Start the cache threads.\n+      bufferManager = cacheImpl; // Cache also serves as buffer manager.\n     } else {\n-      if (useAllocOnly) {\n-        LowLevelCacheMemoryManager memManager = new LowLevelCacheMemoryManager(\n-            conf, null, cacheMetrics);\n-        allocator = new BuddyAllocator(conf, memManager, cacheMetrics);\n-      } else {\n-        allocator = new SimpleAllocator(conf);\n-      }\n-      bufferManager = new SimpleBufferManager(allocator, cacheMetrics);\n+      this.allocator = new SimpleAllocator(conf);\n+      SimpleBufferManager sbm = new SimpleBufferManager(allocator, cacheMetrics);\n+      bufferManager = sbm;\n+      cache = sbm;\n     }\n     // IO thread pool. Listening is used for unhandled errors for now (TODO: remove?)\n     int numThreads = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_IO_THREADPOOL_SIZE);\n@@ -148,7 +146,7 @@ private LlapIoImpl(Configuration conf) throws IOException {\n         new ThreadFactoryBuilder().setNameFormat(\"IO-Elevator-Thread-%d\").setDaemon(true).build());\n     // TODO: this should depends on input format and be in a map, or something.\n     this.cvp = new OrcColumnVectorProducer(\n-        metadataCache, orcCache, bufferManager, conf, cacheMetrics, ioMetrics);\n+        metadataCache, cache, bufferManager, conf, cacheMetrics, ioMetrics);\n     LOG.info(\"LLAP IO initialized\");\n \n     registerMXBeans();", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java", "sha": "12275acd5a39dcb3e5536c011d12c1c8afd38d55", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -25,7 +25,6 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.llap.cache.BufferUsageManager;\n import org.apache.hadoop.hive.llap.cache.LowLevelCache;\n-import org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl;\n import org.apache.hadoop.hive.llap.counters.QueryFragmentCounters;\n import org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch;\n import org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl;\n@@ -48,7 +47,7 @@\n   private LlapDaemonIOMetrics ioMetrics;\n \n   public OrcColumnVectorProducer(OrcMetadataCache metadataCache,\n-      LowLevelCacheImpl lowLevelCache, BufferUsageManager bufferManager,\n+      LowLevelCache lowLevelCache, BufferUsageManager bufferManager,\n       Configuration conf, LlapDaemonCacheMetrics cacheMetrics, LlapDaemonIOMetrics ioMetrics) {\n     LlapIoImpl.LOG.info(\"Initializing ORC column vector producer\");\n ", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java", "sha": "eb8ee6cd4c10e82f8adcfed407c58854828aceb9", "changes": 11, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -835,12 +835,11 @@ public DataWrapperForOrc() throws IOException {\n     @Override\n     public DiskRangeList getFileData(Object fileKey, DiskRangeList range,\n         long baseOffset, DiskRangeListFactory factory, BooleanRef gotAllData) {\n-      DiskRangeList result = (lowLevelCache == null) ? range\n-          : lowLevelCache.getFileData(fileKey, range, baseOffset, factory, counters, gotAllData);\n+      DiskRangeList result = lowLevelCache.getFileData(\n+          fileKey, range, baseOffset, factory, counters, gotAllData);\n       if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {\n-        LlapIoImpl.ORC_LOGGER.trace(\"Disk ranges after data cache (file \" + fileKey\n-            + \", base offset \" + baseOffset + \"): \"\n-            + RecordReaderUtils.stringifyDiskRanges(range.next));\n+        LlapIoImpl.ORC_LOGGER.trace(\"Disk ranges after data cache (file \" + fileKey +\n+            \", base offset \" + baseOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(range));\n       }\n       if (gotAllData.value) return result;\n       return (metadataCache == null) ? range\n@@ -851,7 +850,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList range,\n     public long[] putFileData(Object fileKey, DiskRange[] ranges,\n         MemoryBuffer[] data, long baseOffset) {\n       if (data != null) {\n-        return (lowLevelCache == null) ? null : lowLevelCache.putFileData(\n+        return lowLevelCache.putFileData(\n             fileKey, ranges, data, baseOffset, Priority.NORMAL, counters);\n       } else if (metadataCache != null) {\n         metadataCache.putIncompleteCbs(fileKey, ranges, baseOffset);", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/main/resources/llap-daemon-log4j2.properties", "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/main/resources/llap-daemon-log4j2.properties", "sha": "0c953d1fa58f79d60bb059cf1110269285570358", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/main/resources/llap-daemon-log4j2.properties?ref=406e935f27f60bb01c53d54bdb2c91429c95207e", "patch": "@@ -100,7 +100,10 @@ appender.query-routing.routes.route-mdc.file-mdc.app.layout.type = PatternLayout\n appender.query-routing.routes.route-mdc.file-mdc.app.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n \n # list of all loggers\n-loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, HistoryLogger, LlapIoImpl, LlapIoOrc, LlapIoCache, LlapIoLocking\n+loggers = EncodedReader, NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, HistoryLogger, LlapIoImpl, LlapIoOrc, LlapIoCache, LlapIoLocking\n+\n+logger.EncodedReader.name = org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl\n+logger.EncodedReader.level = INFO\n \n logger.LlapIoImpl.name = LlapIoImpl\n logger.LlapIoImpl.level = INFO\n@@ -109,7 +112,7 @@ logger.LlapIoOrc.name = LlapIoOrc\n logger.LlapIoOrc.level = WARN\n \n logger.LlapIoCache.name = LlapIoCache\n-logger.LlapIOCache.level = WARN\n+logger.LlapIoCache.level = WARN\n \n logger.LlapIoLocking.name = LlapIoLocking\n logger.LlapIoLocking.level = WARN", "filename": "llap-server/src/main/resources/llap-daemon-log4j2.properties"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2f686d4c0c20540079660de202c619e42ed5cd4f", "parent": "https://github.com/apache/hive/commit/9343fee5d10ab5ab64692d9723a6c35e77adefc3", "message": "HIVE-14617: NPE in UDF MapValues() if input is null (reviewed by Chao)", "bug_id": "hive_77", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java", "blob_url": "https://github.com/apache/hive/blob/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java", "sha": "3bd5864499ea7d9e4d6e8e7f2284ec96f445ad06", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java?ref=2f686d4c0c20540079660de202c619e42ed5cd4f", "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.hive.ql.udf.generic;\n \n import java.util.ArrayList;\n+import java.util.Map;\n \n import org.apache.hadoop.hive.ql.exec.Description;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n@@ -61,7 +62,10 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     retArray.clear();\n     Object mapObj = arguments[0].get();\n-    retArray.addAll(mapOI.getMap(mapObj).values());\n+    Map<?, ?> map = mapOI.getMap(mapObj);\n+    if (map != null) {\n+      retArray.addAll(map.values());\n+    }\n     return retArray;\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java"}, {"additions": 56, "raw_url": "https://github.com/apache/hive/raw/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java", "blob_url": "https://github.com/apache/hive/blob/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java", "sha": "44676ed6f877666b89f2f3e7109a5d924597e901", "changes": 56, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java?ref=2f686d4c0c20540079660de202c619e42ed5cd4f", "patch": "@@ -0,0 +1,56 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestGenericUDFMapValues {\n+\n+  @Test\n+  public void testNullMap() throws HiveException, IOException {\n+    ObjectInspector[] inputOIs = {\n+        ObjectInspectorFactory.getStandardMapObjectInspector(\n+            PrimitiveObjectInspectorFactory.writableStringObjectInspector,\n+            PrimitiveObjectInspectorFactory.writableStringObjectInspector),\n+    };\n+\n+    Map<String, String> input = null;\n+    DeferredObject[] args = {\n+        new DeferredJavaObject(input)\n+    };\n+\n+  GenericUDFMapValues udf = new GenericUDFMapValues();\n+    StandardListObjectInspector oi = (StandardListObjectInspector) udf.initialize(inputOIs);\n+    Object res = udf.evaluate(args);\n+    Assert.assertTrue(oi.getList(res).isEmpty());\n+    udf.close();\n+  }\n+\n+}", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4", "parent": "https://github.com/apache/hive/commit/89080f557ad95141e3752d6f5d43455dc0ffb2c6", "message": "HIVE-12954: NPE with str_to_map on null strings (Marta Kuczora, reviewed by Aihua Xu)", "bug_id": "hive_78", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java", "blob_url": "https://github.com/apache/hive/blob/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java", "sha": "093f2a3d6130cca01d137ebe1eb3d3c8416c54d5", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java?ref=6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4", "patch": "@@ -83,11 +83,23 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     ret.clear();\n     String text = (String) soi_text.convert(arguments[0].get());\n+    if (text == null) {\n+      return ret;\n+    }\n+\n     String delimiter1 = (soi_de1 == null) ?\n       default_de1 : (String) soi_de1.convert(arguments[1].get());\n     String delimiter2 = (soi_de2 == null) ?\n       default_de2 : (String) soi_de2.convert(arguments[2].get());\n \n+    if (delimiter1 == null) {\n+      delimiter1 = default_de1;\n+    }\n+\n+    if (delimiter2 == null) {\n+      delimiter2 = default_de2;\n+    }\n+\n     String[] keyValuePairs = text.split(delimiter1);\n \n     for (String keyValuePair : keyValuePairs) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java"}, {"additions": 152, "raw_url": "https://github.com/apache/hive/raw/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java", "blob_url": "https://github.com/apache/hive/blob/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java", "sha": "1653936284a437f553f435a480e70f0c6e761aab", "changes": 152, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java?ref=6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4", "patch": "@@ -0,0 +1,152 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+\n+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.junit.Test;\n+\n+public class TestGenericUDFStringToMap {\n+\n+  @Test\n+  public void testStringToMapWithCustomDelimiters() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDF(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key1\", \"value1\");\n+    expResult.put(\"key2\", \"value2\");\n+    expResult.put(\"key3\", \"value3\");\n+    runAndVerify(\"key1=value1;key2=value2;key3=value3\", \";\", \"=\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithDefaultDelimiters() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key1\", \"value1\");\n+    expResult.put(\"key2\", \"value2\");\n+    expResult.put(\"key3\", \"value3\");\n+    runAndVerify(\"key1:value1,key2:value2,key3:value3\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithNullDelimiters() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDF(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key1\", \"value1\");\n+    expResult.put(\"key2\", \"value2\");\n+    expResult.put(\"key3\", \"value3\");\n+    runAndVerify(\"key1:value1,key2:value2,key3:value3\", null, null, expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithNullText() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    runAndVerify(null, expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithEmptyText() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"\", null);\n+    runAndVerify(\"\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapNoKey() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"\", \"value\");\n+    runAndVerify(\":value\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapNoValue() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key\", \"\");\n+    runAndVerify(\"key:\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapNotMatchingDelimiter() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key=value\", null);\n+    runAndVerify(\"key=value\", expResult, udf);\n+  }\n+\n+  private void initGenericUDF(GenericUDFStringToMap udf)\n+      throws UDFArgumentException {\n+\n+    ObjectInspector valueOI0 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector valueOI1 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector valueOI2 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector[] arguments = { valueOI0, valueOI1, valueOI2 };\n+    udf.initialize(arguments);\n+  }\n+\n+  private void initGenericUDFWithNoDelimiters(GenericUDFStringToMap udf)\n+      throws UDFArgumentException {\n+\n+    ObjectInspector valueOI0 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector[] arguments = { valueOI0 };\n+    udf.initialize(arguments);\n+  }\n+\n+  private void runAndVerify(String text, String delimiter1, String delimiter2,\n+      Map<String, String> expResult, GenericUDF udf) throws HiveException {\n+\n+    DeferredObject valueObj0 = new DeferredJavaObject(text);\n+    DeferredObject valueObj1 = new DeferredJavaObject(delimiter1);\n+    DeferredObject valueObj2 = new DeferredJavaObject(delimiter2);\n+    DeferredObject[] args = { valueObj0, valueObj1, valueObj2 };\n+\n+    @SuppressWarnings(\"unchecked\")\n+    LinkedHashMap<Object, Object> output = (LinkedHashMap<Object, Object>) udf.evaluate(args);\n+    assertTrue(\"str_to_map() test\", expResult.equals(output));\n+  }\n+\n+  private void runAndVerify(String text, Map<String, String> expResult,\n+      GenericUDF udf) throws HiveException {\n+\n+    DeferredObject valueObj0 = new DeferredJavaObject(text);\n+    DeferredObject[] args = { valueObj0 };\n+    @SuppressWarnings(\"unchecked\")\n+    LinkedHashMap<Object, Object> output = (LinkedHashMap<Object, Object>) udf.evaluate(args);\n+    assertTrue(\"str_to_map() test\", expResult.equals(output));\n+  }\n+}\n\\ No newline at end of file", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/deeddf4a453f4ac1bd2c588558e8d327c065619b", "parent": "https://github.com/apache/hive/commit/1d12007718e91286a6082b50fd0b273ae80c5b3a", "message": "HIVE-14139: NPE dropping permanent function (Rui reviewed by Sergey Shelukhin)", "bug_id": "hive_79", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/deeddf4a453f4ac1bd2c588558e8d327c065619b/ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java", "blob_url": "https://github.com/apache/hive/blob/deeddf4a453f4ac1bd2c588558e8d327c065619b/ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java", "sha": "c800a214bc3b97d6800d519fdb489608fe2fa6c9", "changes": 11, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java?ref=deeddf4a453f4ac1bd2c588558e8d327c065619b", "patch": "@@ -510,11 +510,12 @@ public void unregisterFunction(String functionName) throws HiveException {\n   private void removePersistentFunctionUnderLock(FunctionInfo fi) {\n     Class<?> functionClass = getPermanentUdfClass(fi);\n     Integer refCount = persistent.get(functionClass);\n-    assert refCount != null;\n-    if (refCount == 1) {\n-      persistent.remove(functionClass);\n-    } else {\n-      persistent.put(functionClass, Integer.valueOf(refCount - 1));\n+    if (refCount != null) {\n+      if (refCount == 1) {\n+        persistent.remove(functionClass);\n+      } else {\n+        persistent.put(functionClass, Integer.valueOf(refCount - 1));\n+      }\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/3eeb94229af5577e945cb3f5080b83948296ef0c", "parent": "https://github.com/apache/hive/commit/acdc31b8d3c2b85e4b7882422f5a35cf8665eb8d", "message": "HIVE-13852: NPE in TaskLocationHints during LLAP GetSplits request (Jason Dere, reviewed by Siddharth Seth)", "bug_id": "hive_80", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/3eeb94229af5577e945cb3f5080b83948296ef0c/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java", "blob_url": "https://github.com/apache/hive/blob/3eeb94229af5577e945cb3f5080b83948296ef0c/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java", "sha": "140dbdab5162ac865a9d6427aaf4a972cebbd827", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java?ref=3eeb94229af5577e945cb3f5080b83948296ef0c", "patch": "@@ -91,7 +91,9 @@\n import org.apache.tez.dag.api.DAG;\n import org.apache.tez.dag.api.TaskLocationHint;\n import org.apache.tez.dag.api.TaskSpecBuilder;\n+import org.apache.tez.dag.api.TezConfiguration;\n import org.apache.tez.dag.api.Vertex;\n+import org.apache.tez.mapreduce.grouper.TezSplitGrouper;\n import org.apache.tez.runtime.api.Event;\n import org.apache.tez.runtime.api.events.InputConfigureVertexTasksEvent;\n import org.apache.tez.runtime.api.impl.EventMetaData;\n@@ -205,6 +207,7 @@ public PlanFragment createPlanFragment(String query, int num) throws HiveExcepti\n     HiveConf.setVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_MODE, \"llap\");\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_TEZ_GENERATE_CONSISTENT_SPLITS, true);\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS, true);\n+    conf.setBoolean(TezSplitGrouper.TEZ_GROUPING_NODE_LOCAL_ONLY, true);\n     // Tez/LLAP requires RPC query plan\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_RPC_QUERY_PLAN, true);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e91f69e213f6ee78ad8299cda81079104f7141bb", "parent": "https://github.com/apache/hive/commit/e459a67283900393a79e4f69853103cc4fd8a726", "message": "HIVE-13855: select INPUT__FILE__NAME throws NPE exception (Aihua Xu, reviewed by Yongzhi Chen)", "bug_id": "hive_81", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/e91f69e213f6ee78ad8299cda81079104f7141bb/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "blob_url": "https://github.com/apache/hive/blob/e91f69e213f6ee78ad8299cda81079104f7141bb/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "sha": "528d663e701eaa13e2cd7da328973637457ff642", "changes": 12, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=e91f69e213f6ee78ad8299cda81079104f7141bb", "patch": "@@ -387,7 +387,10 @@ private static BaseWork getBaseWork(Configuration conf, String name) {\n \n       path = getPlanPath(conf, name);\n       LOG.info(\"PLAN PATH = \" + path);\n-      assert path != null;\n+      if (path == null) { // Map/reduce plan may not be generated\n+        return null;\n+      }\n+\n       BaseWork gWork = gWorkMap.get(conf).get(path);\n       if (gWork == null) {\n         Path localPath = path;\n@@ -443,12 +446,11 @@ private static BaseWork getBaseWork(Configuration conf, String name) {\n       return gWork;\n     } catch (FileNotFoundException fnf) {\n       // happens. e.g.: no reduce work.\n-      LOG.debug(\"File not found: \" + fnf.getMessage());\n-      LOG.info(\"No plan file found: \"+path);\n+      LOG.debug(\"No plan file found: \" + path, fnf);\n       return null;\n     } catch (Exception e) {\n-      String msg = \"Failed to load plan: \" + path + \": \" + e;\n-      LOG.error(msg, e);\n+      String msg = \"Failed to load plan: \" + path;\n+      LOG.error(\"Failed to load plan: \" + path, e);\n       throw new RuntimeException(msg, e);\n     } finally {\n       SerializationUtilities.releaseKryo(kryo);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c9cc105cd7557a57f4207d3ea056e5222e5e3f32", "parent": "https://github.com/apache/hive/commit/f73b774186021158d4dc99b8a9330656d003b90e", "message": "HIVE-11978 : LLAP: NPE in Expr toString (Sergey Shelukhin)", "bug_id": "hive_82", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c9cc105cd7557a57f4207d3ea056e5222e5e3f32/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java", "blob_url": "https://github.com/apache/hive/blob/c9cc105cd7557a57f4207d3ea056e5222e5e3f32/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java", "sha": "b5d2ddfab2ce49c0ee582efb3b6ea9e35f934e91", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java?ref=c9cc105cd7557a57f4207d3ea056e5222e5e3f32", "patch": "@@ -141,7 +141,7 @@ public String toString() {\n         if (i > 0) {\n           sb.append(\", \");\n         }\n-        sb.append(chidren.get(i).toString());\n+        sb.append(chidren.get(i));\n       }\n     }\n     sb.append(\")\");", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c53c9be7181fc47bb5422473edbba1ad9ae81042", "parent": "https://github.com/apache/hive/commit/59539885725a96cca4b3f0759a5b26e0d8198dc8", "message": "HIVE-14773: NPE aggregating column statistics for date column in partitioned table (Pengcheng Xiong, reviewed by Gopal V)", "bug_id": "hive_83", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c53c9be7181fc47bb5422473edbba1ad9ae81042/metastore/src/java/org/apache/hadoop/hive/metastore/IExtrapolatePartStatus.java", "blob_url": "https://github.com/apache/hive/blob/c53c9be7181fc47bb5422473edbba1ad9ae81042/metastore/src/java/org/apache/hadoop/hive/metastore/IExtrapolatePartStatus.java", "sha": "d0569fb8d8a98dc4b77b4979f30dec4ff9ee0405", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/IExtrapolatePartStatus.java?ref=c53c9be7181fc47bb5422473edbba1ad9ae81042", "patch": "@@ -39,6 +39,7 @@\n       put(\"int\", new Integer[] { 0, 1, 6, 7, 12, 15 });\n       put(\"smallint\", new Integer[] { 0, 1, 6, 7, 12, 15 });\n       put(\"tinyint\", new Integer[] { 0, 1, 6, 7, 12, 15 });\n+      put(\"date\", new Integer[] { 0, 1, 6, 7, 12, 15 });\n       put(\"timestamp\", new Integer[] { 0, 1, 6, 7, 12, 15 });\n       put(\"long\", new Integer[] { 0, 1, 6, 7, 12, 15 });\n       put(\"double\", new Integer[] { 2, 3, 6, 7, 13, 15 });", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/IExtrapolatePartStatus.java"}, {"additions": 29, "raw_url": "https://github.com/apache/hive/raw/c53c9be7181fc47bb5422473edbba1ad9ae81042/metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java", "blob_url": "https://github.com/apache/hive/blob/c53c9be7181fc47bb5422473edbba1ad9ae81042/metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java", "sha": "b259dfa10159971a8ff83f4d55fa4be797dc8807", "changes": 29, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java?ref=c53c9be7181fc47bb5422473edbba1ad9ae81042", "patch": "@@ -579,6 +579,35 @@ public static void fillColumnStatisticsData(String colType, ColumnStatisticsData\n         longStats.setNumDVs(lowerBound);\n       }\n       data.setLongStats(longStats);\n+    } else if (colType.equals(\"date\")) {\n+      DateColumnStatsData dateStats = new DateColumnStatsData();\n+      dateStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));\n+      if (lhigh != null) {\n+        dateStats.setHighValue(new Date(MetaStoreDirectSql.extractSqlLong(lhigh)));\n+      }\n+      if (llow != null) {\n+        dateStats.setLowValue(new Date(MetaStoreDirectSql.extractSqlLong(llow)));\n+      }\n+      long lowerBound = MetaStoreDirectSql.extractSqlLong(dist);\n+      long higherBound = MetaStoreDirectSql.extractSqlLong(sumDist);\n+      if (useDensityFunctionForNDVEstimation && lhigh != null && llow != null && avgLong != null\n+          && MetaStoreDirectSql.extractSqlDouble(avgLong) != 0.0) {\n+        // We have estimation, lowerbound and higherbound. We use estimation if\n+        // it is between lowerbound and higherbound.\n+        long estimation = MetaStoreDirectSql\n+            .extractSqlLong((MetaStoreDirectSql.extractSqlLong(lhigh) - MetaStoreDirectSql\n+                .extractSqlLong(llow)) / MetaStoreDirectSql.extractSqlDouble(avgLong));\n+        if (estimation < lowerBound) {\n+          dateStats.setNumDVs(lowerBound);\n+        } else if (estimation > higherBound) {\n+          dateStats.setNumDVs(higherBound);\n+        } else {\n+          dateStats.setNumDVs(estimation);\n+        }\n+      } else {\n+        dateStats.setNumDVs(lowerBound);\n+      }\n+      data.setDateStats(dateStats);\n     } else if (colType.equals(\"double\") || colType.equals(\"float\")) {\n       DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n       doubleStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/c53c9be7181fc47bb5422473edbba1ad9ae81042/ql/src/test/queries/clientpositive/extrapolate_part_stats_date.q", "blob_url": "https://github.com/apache/hive/blob/c53c9be7181fc47bb5422473edbba1ad9ae81042/ql/src/test/queries/clientpositive/extrapolate_part_stats_date.q", "sha": "1f38a6526de26c95c157632d4f2ac8178a8d43c7", "changes": 14, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/extrapolate_part_stats_date.q?ref=c53c9be7181fc47bb5422473edbba1ad9ae81042", "patch": "@@ -0,0 +1,14 @@\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+set hive.stats.fetch.column.stats=true;\n+\n+create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc;\n+insert into date_dim partition(d_date_sk=2416945) values('1905-04-09');\n+insert into date_dim partition(d_date_sk=2416946) values('1905-04-10');\n+insert into date_dim partition(d_date_sk=2416947) values('1905-04-11');\n+analyze table date_dim partition(d_date_sk) compute statistics for columns;\n+\n+explain select count(*) from date_dim where d_date > date \"1900-01-02\" and d_date_sk= 2416945;\n+\n+insert into date_dim partition(d_date_sk=2416948) values('1905-04-12');\n+\n+explain extended select d_date from date_dim;", "filename": "ql/src/test/queries/clientpositive/extrapolate_part_stats_date.q"}, {"additions": 302, "raw_url": "https://github.com/apache/hive/raw/c53c9be7181fc47bb5422473edbba1ad9ae81042/ql/src/test/results/clientpositive/extrapolate_part_stats_date.q.out", "blob_url": "https://github.com/apache/hive/blob/c53c9be7181fc47bb5422473edbba1ad9ae81042/ql/src/test/results/clientpositive/extrapolate_part_stats_date.q.out", "sha": "1dce4df9c49a6ceb5a4ba40f2048b5d6a08a6870", "changes": 302, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/extrapolate_part_stats_date.q.out?ref=c53c9be7181fc47bb5422473edbba1ad9ae81042", "patch": "@@ -0,0 +1,302 @@\n+PREHOOK: query: create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@date_dim\n+POSTHOOK: query: create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@date_dim\n+PREHOOK: query: insert into date_dim partition(d_date_sk=2416945) values('1905-04-09')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@date_dim@d_date_sk=2416945\n+POSTHOOK: query: insert into date_dim partition(d_date_sk=2416945) values('1905-04-09')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@date_dim@d_date_sk=2416945\n+POSTHOOK: Lineage: date_dim PARTITION(d_date_sk=2416945).d_date EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+PREHOOK: query: insert into date_dim partition(d_date_sk=2416946) values('1905-04-10')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__2\n+PREHOOK: Output: default@date_dim@d_date_sk=2416946\n+POSTHOOK: query: insert into date_dim partition(d_date_sk=2416946) values('1905-04-10')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__2\n+POSTHOOK: Output: default@date_dim@d_date_sk=2416946\n+POSTHOOK: Lineage: date_dim PARTITION(d_date_sk=2416946).d_date EXPRESSION [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+PREHOOK: query: insert into date_dim partition(d_date_sk=2416947) values('1905-04-11')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__3\n+PREHOOK: Output: default@date_dim@d_date_sk=2416947\n+POSTHOOK: query: insert into date_dim partition(d_date_sk=2416947) values('1905-04-11')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__3\n+POSTHOOK: Output: default@date_dim@d_date_sk=2416947\n+POSTHOOK: Lineage: date_dim PARTITION(d_date_sk=2416947).d_date EXPRESSION [(values__tmp__table__3)values__tmp__table__3.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+PREHOOK: query: analyze table date_dim partition(d_date_sk) compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Input: default@date_dim@d_date_sk=2416945\n+PREHOOK: Input: default@date_dim@d_date_sk=2416946\n+PREHOOK: Input: default@date_dim@d_date_sk=2416947\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table date_dim partition(d_date_sk) compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Input: default@date_dim@d_date_sk=2416945\n+POSTHOOK: Input: default@date_dim@d_date_sk=2416946\n+POSTHOOK: Input: default@date_dim@d_date_sk=2416947\n+#### A masked pattern was here ####\n+PREHOOK: query: explain select count(*) from date_dim where d_date > date \"1900-01-02\" and d_date_sk= 2416945\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select count(*) from date_dim where d_date > date \"1900-01-02\" and d_date_sk= 2416945\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: date_dim\n+            Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: COMPLETE\n+            Filter Operator\n+              predicate: (d_date > 1900-01-02) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: COMPLETE\n+              Select Operator\n+                Statistics: Num rows: 1 Data size: 56 Basic stats: COMPLETE Column stats: COMPLETE\n+                Group By Operator\n+                  aggregations: count()\n+                  mode: hash\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Reduce Output Operator\n+                    sort order: \n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                    value expressions: _col0 (type: bigint)\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: insert into date_dim partition(d_date_sk=2416948) values('1905-04-12')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__4\n+PREHOOK: Output: default@date_dim@d_date_sk=2416948\n+POSTHOOK: query: insert into date_dim partition(d_date_sk=2416948) values('1905-04-12')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__4\n+POSTHOOK: Output: default@date_dim@d_date_sk=2416948\n+POSTHOOK: Lineage: date_dim PARTITION(d_date_sk=2416948).d_date EXPRESSION [(values__tmp__table__4)values__tmp__table__4.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+PREHOOK: query: explain extended select d_date from date_dim\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain extended select d_date from date_dim\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Partition Description:\n+          Partition\n+            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+            partition values:\n+              d_date_sk 2416945\n+            properties:\n+              COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"d_date\":\"true\"}}\n+              bucket_count -1\n+              columns d_date\n+              columns.comments \n+              columns.types date\n+#### A masked pattern was here ####\n+              name default.date_dim\n+              numFiles 1\n+              numRows 1\n+              partition_columns d_date_sk\n+              partition_columns.types bigint\n+              rawDataSize 56\n+              serialization.ddl struct date_dim { date d_date}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              totalSize 193\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+          \n+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns d_date\n+                columns.comments \n+                columns.types date\n+#### A masked pattern was here ####\n+                name default.date_dim\n+                partition_columns d_date_sk\n+                partition_columns.types bigint\n+                serialization.ddl struct date_dim { date d_date}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              name: default.date_dim\n+            name: default.date_dim\n+          Partition\n+            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+            partition values:\n+              d_date_sk 2416946\n+            properties:\n+              COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"d_date\":\"true\"}}\n+              bucket_count -1\n+              columns d_date\n+              columns.comments \n+              columns.types date\n+#### A masked pattern was here ####\n+              name default.date_dim\n+              numFiles 1\n+              numRows 1\n+              partition_columns d_date_sk\n+              partition_columns.types bigint\n+              rawDataSize 56\n+              serialization.ddl struct date_dim { date d_date}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              totalSize 193\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+          \n+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns d_date\n+                columns.comments \n+                columns.types date\n+#### A masked pattern was here ####\n+                name default.date_dim\n+                partition_columns d_date_sk\n+                partition_columns.types bigint\n+                serialization.ddl struct date_dim { date d_date}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              name: default.date_dim\n+            name: default.date_dim\n+          Partition\n+            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+            partition values:\n+              d_date_sk 2416947\n+            properties:\n+              COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"d_date\":\"true\"}}\n+              bucket_count -1\n+              columns d_date\n+              columns.comments \n+              columns.types date\n+#### A masked pattern was here ####\n+              name default.date_dim\n+              numFiles 1\n+              numRows 1\n+              partition_columns d_date_sk\n+              partition_columns.types bigint\n+              rawDataSize 56\n+              serialization.ddl struct date_dim { date d_date}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              totalSize 193\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+          \n+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns d_date\n+                columns.comments \n+                columns.types date\n+#### A masked pattern was here ####\n+                name default.date_dim\n+                partition_columns d_date_sk\n+                partition_columns.types bigint\n+                serialization.ddl struct date_dim { date d_date}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              name: default.date_dim\n+            name: default.date_dim\n+          Partition\n+            input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+            partition values:\n+              d_date_sk 2416948\n+            properties:\n+              COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\"}\n+              bucket_count -1\n+              columns d_date\n+              columns.comments \n+              columns.types date\n+#### A masked pattern was here ####\n+              name default.date_dim\n+              numFiles 1\n+              numRows 1\n+              partition_columns d_date_sk\n+              partition_columns.types bigint\n+              rawDataSize 56\n+              serialization.ddl struct date_dim { date d_date}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              totalSize 193\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+          \n+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns d_date\n+                columns.comments \n+                columns.types date\n+#### A masked pattern was here ####\n+                name default.date_dim\n+                partition_columns d_date_sk\n+                partition_columns.types bigint\n+                serialization.ddl struct date_dim { date d_date}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde\n+              name: default.date_dim\n+            name: default.date_dim\n+      Processor Tree:\n+        TableScan\n+          alias: date_dim\n+          Statistics: Num rows: 4 Data size: 224 Basic stats: COMPLETE Column stats: PARTIAL\n+          GatherStats: false\n+          Select Operator\n+            expressions: d_date (type: date)\n+            outputColumnNames: _col0\n+            Statistics: Num rows: 4 Data size: 224 Basic stats: COMPLETE Column stats: PARTIAL\n+            ListSink\n+", "filename": "ql/src/test/results/clientpositive/extrapolate_part_stats_date.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d", "parent": "https://github.com/apache/hive/commit/5edbf31a755668b213c2a21fb8a4a2e902e081f6", "message": "HIVE-11580: ThriftUnionObjectInspector#toString throws NPE (Jimmy, reviewed by Chao)", "bug_id": "hive_84", "file": [{"additions": 13, "raw_url": "https://github.com/apache/hive/raw/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java", "blob_url": "https://github.com/apache/hive/blob/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java", "sha": "600abbb2af19ae7e071d81bc8466ad8d1ab0204a", "changes": 26, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java?ref=6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d", "patch": "@@ -18,26 +18,27 @@\n \n package org.apache.hadoop.hive.serde2.objectinspector;\n \n-import com.google.common.primitives.UnsignedBytes;\n-import org.apache.hadoop.util.ReflectionUtils;\n-import org.apache.thrift.TFieldIdEnum;\n-import org.apache.thrift.TUnion;\n-import org.apache.thrift.meta_data.FieldMetaData;\n-\n import java.lang.reflect.Field;\n import java.lang.reflect.Type;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.TUnion;\n+import org.apache.thrift.meta_data.FieldMetaData;\n+\n+import com.google.common.primitives.UnsignedBytes;\n+\n /**\n  * Always use the ObjectInspectorFactory to create new ObjectInspector objects,\n  * instead of directly creating an instance of this class.\n  */\n public class ThriftUnionObjectInspector extends ReflectionStructObjectInspector implements UnionObjectInspector {\n \n   private static final String FIELD_METADATA_MAP = \"metaDataMap\";\n-  private  List<ObjectInspector> ois;\n+  private List<ObjectInspector> ois;\n+  private List<StandardStructObjectInspector.MyField> fields;\n \n   @Override\n   public boolean shouldIgnoreField(String name) {\n@@ -88,10 +89,14 @@ protected void init(Class<?> objectClass,\n \n     try {\n       final Map<? extends TFieldIdEnum, FieldMetaData> fieldMap = (Map<? extends TFieldIdEnum, FieldMetaData>) fieldMetaData.get(null);\n+      fields = new ArrayList<StandardStructObjectInspector.MyField>(fieldMap.size());\n       this.ois = new ArrayList<ObjectInspector>();\n       for(Map.Entry<? extends TFieldIdEnum, FieldMetaData> metadata : fieldMap.entrySet()) {\n-        final Type fieldType = ThriftObjectInspectorUtils.getFieldType(objectClass, metadata.getValue().fieldName);\n+        int fieldId = metadata.getKey().getThriftFieldId();\n+        String fieldName = metadata.getValue().fieldName;\n+        final Type fieldType = ThriftObjectInspectorUtils.getFieldType(objectClass, fieldName);\n         final ObjectInspector reflectionObjectInspector = ObjectInspectorFactory.getReflectionObjectInspector(fieldType, options);\n+        fields.add(new StandardStructObjectInspector.MyField(fieldId, fieldName, reflectionObjectInspector));\n         this.ois.add(reflectionObjectInspector);\n       }\n     } catch (IllegalAccessException e) {\n@@ -112,10 +117,5 @@ public Category getCategory() {\n   public String getTypeName() {\n     return ObjectInspectorUtils.getStandardUnionTypeName(this);\n   }\n-\n-  @Override\n-  public Object create() {\n-    return ReflectionUtils.newInstance(objectClass, null);\n-  }\n }\n ", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java", "blob_url": "https://github.com/apache/hive/blob/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java", "sha": "85f2bd63fb86019ee76103cb6b4c3b64b836f730", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java?ref=6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d", "patch": "@@ -23,14 +23,15 @@\n import java.util.List;\n import java.util.Set;\n \n-import junit.framework.TestCase;\n-\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n import org.apache.hadoop.hive.serde2.thrift.test.Complex;\n import org.apache.hadoop.hive.serde2.thrift.test.IntString;\n+import org.apache.hadoop.hive.serde2.thrift.test.PropValueUnion;\n import org.apache.hadoop.hive.serde2.thrift.test.SetIntString;\n \n+import junit.framework.TestCase;\n+\n /**\n  * TestThriftObjectInspectors.\n  *\n@@ -60,6 +61,11 @@ public void testThriftObjectInspectors() throws Throwable {\n         assertNull(soi.getStructFieldData(null, fields.get(i)));\n       }\n \n+      ObjectInspector oi = ObjectInspectorFactory\n+          .getReflectionObjectInspector(PropValueUnion.class,\n+          ObjectInspectorFactory.ObjectInspectorOptions.THRIFT);\n+      assertNotNull(oi.toString());\n+\n       // real object\n       Complex c = new Complex();\n       c.setAint(1);", "filename": "serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ba864a241b08c7916be1eefe08f7972451aeda15", "parent": "https://github.com/apache/hive/commit/0ebcd938cf0da7ef74ef3534a3aef4b7976a1163", "message": "HIVE-13588: NPE is thrown from MapredLocalTask.executeInChildVM (Chaoyu Tang, reviewed by Yongzhi Chen)", "bug_id": "hive_85", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java", "blob_url": "https://github.com/apache/hive/blob/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java", "sha": "24bf506e5bc43c9cef35a993dabbbe71dcfb63da", "changes": 16, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java?ref=ba864a241b08c7916be1eefe08f7972451aeda15", "patch": "@@ -319,10 +319,18 @@ public int executeInChildVM(DriverContext driverContext) {\n \n       CachingPrintStream errPrintStream = new CachingPrintStream(System.err);\n \n-      StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out,\n-        OperationLog.getCurrentOperationLog().getPrintStream());\n-      StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, errPrintStream,\n-        OperationLog.getCurrentOperationLog().getPrintStream());\n+      StreamPrinter outPrinter;\n+      StreamPrinter errPrinter;\n+      OperationLog operationLog = OperationLog.getCurrentOperationLog();\n+      if (operationLog != null) {\n+        outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out,\n+            operationLog.getPrintStream());\n+        errPrinter = new StreamPrinter(executor.getErrorStream(), null, errPrintStream,\n+            operationLog.getPrintStream());\n+      } else {\n+        outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out);\n+        errPrinter = new StreamPrinter(executor.getErrorStream(), null, errPrintStream);\n+      }\n \n       outPrinter.start();\n       errPrinter.start();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/test/results/clientpositive/auto_sortmerge_join_8.q.out", "blob_url": "https://github.com/apache/hive/blob/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/test/results/clientpositive/auto_sortmerge_join_8.q.out", "sha": "23a36851e78a9929be8ef63c57b3a69d6258595c", "changes": 2, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_sortmerge_join_8.q.out?ref=ba864a241b08c7916be1eefe08f7972451aeda15", "patch": "@@ -1411,8 +1411,6 @@ PREHOOK: Input: default@bucket_small\n PREHOOK: Input: default@bucket_small@ds=2008-04-08\n PREHOOK: Input: default@bucket_small@ds=2008-04-09\n #### A masked pattern was here ####\n-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n-ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n POSTHOOK: query: select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@bucket_big", "filename": "ql/src/test/results/clientpositive/auto_sortmerge_join_8.q.out"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/test/results/clientpositive/llap/tez_join_hash.q.out", "blob_url": "https://github.com/apache/hive/blob/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/test/results/clientpositive/llap/tez_join_hash.q.out", "sha": "54ca9d219c532b76c92c91e620f357b99916f58d", "changes": 4, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/tez_join_hash.q.out?ref=ba864a241b08c7916be1eefe08f7972451aeda15", "patch": "@@ -652,10 +652,6 @@ PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12\n PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11\n PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12\n #### A masked pattern was here ####\n-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n-ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n-ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n POSTHOOK: query: select key, count(*) from (select x.key as key, y.value as value from\n srcpart x join srcpart y on (x.key = y.key)\n union all", "filename": "ql/src/test/results/clientpositive/llap/tez_join_hash.q.out"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/test/results/clientpositive/tez/tez_join_hash.q.out", "blob_url": "https://github.com/apache/hive/blob/ba864a241b08c7916be1eefe08f7972451aeda15/ql/src/test/results/clientpositive/tez/tez_join_hash.q.out", "sha": "8d0aba19d63a51c3d21f356078b74363ce8fc967", "changes": 4, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_join_hash.q.out?ref=ba864a241b08c7916be1eefe08f7972451aeda15", "patch": "@@ -638,10 +638,6 @@ PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12\n PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11\n PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12\n #### A masked pattern was here ####\n-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n-ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n-ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n POSTHOOK: query: select key, count(*) from (select x.key as key, y.value as value from\n srcpart x join srcpart y on (x.key = y.key)\n union all", "filename": "ql/src/test/results/clientpositive/tez/tez_join_hash.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b", "parent": "https://github.com/apache/hive/commit/f33e1537292d7f1b1f486d7b31c6485b27ce623e", "message": "HIVE-11433: NPE for a multiple inner join query", "bug_id": "hive_86", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java", "blob_url": "https://github.com/apache/hive/blob/89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java", "sha": "5f13277570900bc9baae8f9fc7935f9984e51476", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java?ref=89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b", "patch": "@@ -153,7 +153,7 @@ public static CharTypeInfo getCharTypeInfo(ASTNode node)\n \n   static int getIndex(String[] list, String elem) {\n     for(int i=0; i < list.length; i++) {\n-      if (list[i].toLowerCase().equals(elem)) {\n+      if (list[i] != null && list[i].toLowerCase().equals(elem)) {\n         return i;\n       }\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c48bed2e95a60e347b2450c92cfe3581024ea94b", "parent": "https://github.com/apache/hive/commit/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c", "message": "HIVE-11260. LLAP: Fix NPE in AMReporter. (Siddharth Seth)", "bug_id": "hive_87", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/c48bed2e95a60e347b2450c92cfe3581024ea94b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java", "blob_url": "https://github.com/apache/hive/blob/c48bed2e95a60e347b2450c92cfe3581024ea94b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java", "sha": "2fd2546406c89f14dc903d6b0aa43b9832666100", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java?ref=c48bed2e95a60e347b2450c92cfe3581024ea94b", "patch": "@@ -203,8 +203,9 @@ public void unregisterTask(String amLocation, int port) {\n       amNodeInfo = knownAppMasters.get(amNodeId);\n       if (amNodeInfo == null) {\n         LOG.info((\"Ignoring duplicate unregisterRequest for am at: \" + amLocation + \":\" + port));\n+      } else {\n+        amNodeInfo.decrementAndGetTaskCount();\n       }\n-      amNodeInfo.decrementAndGetTaskCount();\n       // Not removing this here. Will be removed when taken off the queue and discovered to have 0\n       // pending tasks.\n     }", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c", "parent": "https://github.com/apache/hive/commit/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692", "message": "HIVE-11200 : LLAP: Cache BuddyAllocator throws NPE (Sergey Shelukhin)", "bug_id": "hive_88", "file": [{"additions": 66, "raw_url": "https://github.com/apache/hive/raw/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java", "blob_url": "https://github.com/apache/hive/blob/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java", "sha": "fca624917d58800d2a4ef354bcfe2f30bc29b402", "changes": 87, "status": "modified", "deletions": 21, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java?ref=838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c", "patch": "@@ -118,6 +118,9 @@ public void allocateMultiple(LlapMemoryBuffer[] dest, int size)\n     }\n     // First try to quickly lock some of the correct-sized free lists and allocate from them.\n     int arenaCount = allocatedArenas.get();\n+    if (arenaCount < 0) {\n+      arenaCount = -arenaCount - 1; // Next arena is being allocated.\n+    }\n     long threadId = arenaCount > 1 ? Thread.currentThread().getId() : 0;\n     {\n       int startIndex = (int)(threadId % arenaCount), index = startIndex;\n@@ -317,18 +320,21 @@ private int allocateWithSplit(int arenaIx, int freeListIx,\n       FreeList freeList = freeLists[freeListIx];\n       int remaining = -1;\n       freeList.lock.lock();\n-      // TODO: write some comments for this method\n       try {\n+        // Try to allocate from target-sized free list, maybe we'll get lucky.\n         ix = allocateFromFreeListUnderLock(\n             arenaIx, freeList, freeListIx, dest, ix, allocationSize);\n         remaining = dest.length - ix;\n         if (remaining == 0) return ix;\n       } finally {\n         freeList.lock.unlock();\n       }\n-      byte headerData = makeHeader(freeListIx, true);\n-      int headerStep = 1 << freeListIx;\n-      int splitListIx = freeListIx + 1;\n+      byte headerData = makeHeader(freeListIx, true); // Header for newly allocated used blocks.\n+      int headerStep = 1 << freeListIx; // Number of headers (smallest blocks) per target block.\n+      int splitListIx = freeListIx + 1; // Next free list from which we will be splitting.\n+      // Each iteration of this loop tries to split blocks from one level of the free list into\n+      // target size blocks; if we cannot satisfy the allocation from the free list containing the\n+      // blocks of a particular size, we'll try to split yet larger blocks, until we run out.\n       while (remaining > 0 && splitListIx < freeLists.length) {\n         int splitWaysLog2 = (splitListIx - freeListIx);\n         assert splitWaysLog2 > 0;\n@@ -338,28 +344,33 @@ private int allocateWithSplit(int arenaIx, int freeListIx,\n         FreeList splitList = freeLists[splitListIx];\n         splitList.lock.lock();\n         try {\n-          int headerIx = splitList.listHead;\n+          int headerIx = splitList.listHead; // Index of the next free block to split.\n           while (headerIx >= 0 && remaining > 0) {\n             int origOffset = offsetFromHeaderIndex(headerIx), offset = origOffset;\n-            int toTake = Math.min(splitWays, remaining); // We split it splitWays and take toTake.\n+            // We will split the block at headerIx [splitWays] ways, and take [toTake] blocks,\n+            // which will leave [lastSplitBlocksRemaining] free blocks of target size.\n+            int toTake = Math.min(splitWays, remaining);\n             remaining -= toTake;\n             lastSplitBlocksRemaining = splitWays - toTake; // Whatever remains.\n-            // Take toTake blocks by splitting the block at origOffset.\n+            // Take toTake blocks by splitting the block at offset.\n             for (; toTake > 0; ++ix, --toTake, headerIx += headerStep, offset += allocationSize) {\n               headers[headerIx] = headerData;\n               // TODO: this could be done out of the lock, we only need to take the blocks out.\n               ((LlapDataBuffer)dest[ix]).initialize(arenaIx, data, offset, allocationSize);\n             }\n             lastSplitNextHeader = headerIx; // If anything remains, this is where it starts.\n-            headerIx = data.getInt(origOffset + 4); // Get next item from the free list.\n+            headerIx = getNextFreeListItem(origOffset);\n           }\n           replaceListHeadUnderLock(splitList, headerIx); // In the end, update free list head.\n         } finally {\n           splitList.lock.unlock();\n         }\n         if (remaining == 0) {\n-          // We have just obtained all we needed by splitting at lastSplitBlockOffset; now\n-          // we need to put the space remaining from that block into lower free lists.\n+          // We have just obtained all we needed by splitting some block; now we need\n+          // to put the space remaining from that block into lower free lists.\n+          // We'll put at most one block into each list, since 2 blocks can always be combined\n+          // to make a larger-level block. Each bit in the remaining target-sized blocks count\n+          // is one block in a list offset from target-sized list by bit index.\n           int newListIndex = freeListIx;\n           while (lastSplitBlocksRemaining > 0) {\n             if ((lastSplitBlocksRemaining & 1) == 1) {\n@@ -394,17 +405,43 @@ private void replaceListHeadUnderLock(FreeList freeList, int headerIx) {\n \n     private int allocateWithExpand(\n         int arenaIx, int freeListIx, LlapMemoryBuffer[] dest, int ix, int size) {\n-      if (data == null) {\n-        synchronized (this) {\n-          // Never goes from non-null to null, so this is the only place we need sync.\n-          if (data == null) {\n-            init();\n-            allocatedArenas.incrementAndGet();\n-            metrics.incrAllocatedArena();\n+      while (true) {\n+        int arenaCount = allocatedArenas.get(), allocArenaCount = arenaCount;\n+        if (arenaCount < 0)  {\n+          allocArenaCount = -arenaCount - 1; // Someone is allocating an arena.\n+        }\n+        if (allocArenaCount > arenaIx) {\n+          // Someone already allocated this arena; just do the usual thing.\n+          return allocateWithSplit(arenaIx, freeListIx, dest, ix, size);\n+        }\n+        if ((arenaIx + 1) == -arenaCount) {\n+          // Someone is allocating this arena. Wait a bit and recheck.\n+          try {\n+            synchronized (this) {\n+              this.wait(100);\n+            }\n+          } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt(); // Restore interrupt, won't handle here.\n           }\n+          continue;\n         }\n+        // Either this arena is being allocated, or it is already allocated, or it is next. The\n+        // caller should not try to allocate another arena before waiting for the previous one.\n+        assert arenaCount == arenaIx :\n+          \"Arena count \" + arenaCount + \" but \" + arenaIx + \" is not being allocated\";\n+        if (!allocatedArenas.compareAndSet(arenaCount, -arenaCount - 1)) {\n+          continue; // CAS race, look again.\n+        }\n+        assert data == null;\n+        init();\n+        boolean isCommited = allocatedArenas.compareAndSet(-arenaCount - 1, arenaCount + 1);\n+        assert isCommited;\n+        synchronized (this) {\n+          this.notifyAll();\n+        }\n+        metrics.incrAllocatedArena();\n+        return allocateWithSplit(arenaIx, freeListIx, dest, ix, size);\n       }\n-      return allocateWithSplit(arenaIx, freeListIx, dest, ix, size);\n     }\n \n     public int offsetFromHeaderIndex(int lastSplitNextHeader) {\n@@ -418,14 +455,22 @@ public int allocateFromFreeListUnderLock(int arenaIx, FreeList freeList,\n         int offset = offsetFromHeaderIndex(current);\n         // Noone else has this either allocated or in a different free list; no sync needed.\n         headers[current] = makeHeader(freeListIx, true);\n-        current = data.getInt(offset + 4);\n+        current = getNextFreeListItem(offset);\n         ((LlapDataBuffer)dest[ix]).initialize(arenaIx, data, offset, size);\n         ++ix;\n       }\n       replaceListHeadUnderLock(freeList, current);\n       return ix;\n     }\n \n+    private int getPrevFreeListItem(int offset) {\n+      return data.getInt(offset);\n+    }\n+\n+    private int getNextFreeListItem(int offset) {\n+      return data.getInt(offset + 4);\n+    }\n+\n     private byte makeHeader(int freeListIx, boolean isInUse) {\n       return (byte)(((freeListIx + 1) << 1) | (isInUse ? 1 : 0));\n     }\n@@ -462,7 +507,7 @@ public void deallocate(LlapDataBuffer buffer) {\n     private void addBlockToFreeListUnderLock(FreeList freeList, int headerIx) {\n       if (freeList.listHead >= 0) {\n         int oldHeadOffset = offsetFromHeaderIndex(freeList.listHead);\n-        assert data.getInt(oldHeadOffset) == -1;\n+        assert getPrevFreeListItem(oldHeadOffset) == -1;\n         data.putInt(oldHeadOffset, headerIx);\n       }\n       int offset = offsetFromHeaderIndex(headerIx);\n@@ -473,7 +518,7 @@ private void addBlockToFreeListUnderLock(FreeList freeList, int headerIx) {\n \n     private void removeBlockFromFreeList(FreeList freeList, int headerIx) {\n       int bOffset = offsetFromHeaderIndex(headerIx),\n-          bpHeaderIx = data.getInt(bOffset), bnHeaderIx = data.getInt(bOffset + 4);\n+          bpHeaderIx = getPrevFreeListItem(bOffset), bnHeaderIx = getNextFreeListItem(bOffset);\n       if (freeList.listHead == headerIx) {\n         assert bpHeaderIx == -1;\n         freeList.listHead = bnHeaderIx;", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java"}, {"additions": 31, "raw_url": "https://github.com/apache/hive/raw/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java", "blob_url": "https://github.com/apache/hive/blob/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java", "sha": "50d5e19094ad8960d61dcc183bc078d33599387a", "changes": 31, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java?ref=838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c", "patch": "@@ -136,6 +136,37 @@ public Void call() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testMTTArenas() {\n+    final int min = 3, max = 4, maxAlloc = 1 << max, minAllocCount = 2048, threadCount = 4;\n+    Configuration conf = createConf(1 << min, maxAlloc, maxAlloc, (1 << min) * minAllocCount);\n+    final BuddyAllocator a = new BuddyAllocator(conf, new DummyMemoryManager(),\n+        LlapDaemonCacheMetrics.create(\"test\", \"1\"));\n+    ExecutorService executor = Executors.newFixedThreadPool(threadCount);\n+    final CountDownLatch cdlIn = new CountDownLatch(threadCount), cdlOut = new CountDownLatch(1);\n+    Callable<Void> testCallable = new Callable<Void>() {\n+      public Void call() throws Exception {\n+        syncThreadStart(cdlIn, cdlOut);\n+        allocSameSize(a, minAllocCount / threadCount, min);\n+        return null;\n+      }\n+    };\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<Void>[] allocTasks = new FutureTask[threadCount];\n+    for (int i = 0; i < threadCount; ++i) {\n+      allocTasks[i] = new FutureTask<>(testCallable);\n+      executor.execute(allocTasks[i]);\n+    }\n+    try {\n+      cdlIn.await(); // Wait for all threads to be ready.\n+      cdlOut.countDown(); // Release them at the same time.\n+      for (int i = 0; i < threadCount; ++i) {\n+        allocTasks[i].get();\n+      }\n+    } catch (Throwable t) {\n+      throw new RuntimeException(t);\n+    }\n+  }\n   private void syncThreadStart(final CountDownLatch cdlIn, final CountDownLatch cdlOut) {\n     cdlIn.countDown();\n     try {", "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f2dcdaa472ca238e481df549446a2613588ba28e", "parent": "https://github.com/apache/hive/commit/ea87e0f26112f0cdbe2fa06e3090fb7b49d81569", "message": "HIVE-14303: CommonJoinOperator.checkAndGenObject should return directly to avoid NPE if ExecReducer.close is called twice. (Zhihai Xu, reviewed by Chao Sun)", "bug_id": "hive_89", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/f2dcdaa472ca238e481df549446a2613588ba28e/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/f2dcdaa472ca238e481df549446a2613588ba28e/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "sha": "7e9007c7de7a72265d9b8150f30c6dde8b5268af", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java?ref=f2dcdaa472ca238e481df549446a2613588ba28e", "patch": "@@ -154,6 +154,8 @@\n   protected transient int heartbeatInterval;\n   protected static final int NOTSKIPBIGTABLE = -1;\n \n+  private transient boolean closeOpCalled = false;\n+\n   /** Kryo ctor. */\n   protected CommonJoinOperator() {\n     super();\n@@ -226,6 +228,7 @@ public CommonJoinOperator(CommonJoinOperator<T> clone) {\n   @SuppressWarnings(\"unchecked\")\n   protected void initializeOp(Configuration hconf) throws HiveException {\n     super.initializeOp(hconf);\n+    closeOpCalled = false;\n     this.handleSkewJoin = conf.getHandleSkewJoin();\n     this.hconf = hconf;\n \n@@ -851,6 +854,12 @@ private void genAllOneUniqueJoinObject()\n   }\n \n   protected void checkAndGenObject() throws HiveException {\n+    if (closeOpCalled) {\n+      LOG.warn(\"checkAndGenObject is called after operator \" +\n+          id + \" \" + getName() + \" called closeOp\");\n+      return;\n+    }\n+\n     if (condn[0].getType() == JoinDesc.UNIQUE_JOIN) {\n \n       // Check if results need to be emitted.\n@@ -951,6 +960,7 @@ protected void reportProgress() {\n    */\n   @Override\n   public void closeOp(boolean abort) throws HiveException {\n+    closeOpCalled = true;\n     for (AbstractRowContainer<List<Object>> alw : storage) {\n       if (alw != null) {\n         alw.clearRows(); // clean up the temp files", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/48aefe4506ea5f6934a129c09ca9e13269adf895", "parent": "https://github.com/apache/hive/commit/15220e8b52bf934500ff8d98a131ae1059cfe6dc", "message": "HIVE-13311: MetaDataFormatUtils throws NPE when HiveDecimal.create is null (Reuben Kuhnert, reviewed by Sergio Pena)", "bug_id": "hive_90", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/48aefe4506ea5f6934a129c09ca9e13269adf895/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java", "blob_url": "https://github.com/apache/hive/blob/48aefe4506ea5f6934a129c09ca9e13269adf895/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java", "sha": "e76fb91a86fadf099198b3b9371e36801a93c321", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java?ref=48aefe4506ea5f6934a129c09ca9e13269adf895", "patch": "@@ -150,7 +150,13 @@ private static String convertToString(Decimal val) {\n     if (val == null) {\n       return \"\";\n     }\n-    return HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale()).toString();\n+\n+    HiveDecimal result = HiveDecimal.create(new BigInteger(val.getUnscaled()), val.getScale());\n+    if (result != null) {\n+      return result.toString();\n+    } else {\n+      return \"\";\n+    }\n   }\n \n   private static String convertToString(org.apache.hadoop.hive.metastore.api.Date val) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatUtils.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f8ab8852a49060c0769d243bc354e5afc1586c3f", "parent": "https://github.com/apache/hive/commit/cd42bd08004da92ab01d4a1b4ec28261ca44238f", "message": "HIVE-10889 : LLAP: HIVE-10778 has NPE (Sergey Shelukhin)", "bug_id": "hive_91", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/f8ab8852a49060c0769d243bc354e5afc1586c3f/ql/src/java/org/apache/hadoop/hive/ql/exec/GlobalWorkMapFactory.java", "blob_url": "https://github.com/apache/hive/blob/f8ab8852a49060c0769d243bc354e5afc1586c3f/ql/src/java/org/apache/hadoop/hive/ql/exec/GlobalWorkMapFactory.java", "sha": "59ee3476177a14b87a82d37d978ffaba44969b78", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/GlobalWorkMapFactory.java?ref=f8ab8852a49060c0769d243bc354e5afc1586c3f", "patch": "@@ -82,7 +82,8 @@ public int size() {\n   DummyMap<Path, BaseWork> dummy = new DummyMap<Path, BaseWork>();\n \n   public Map<Path, BaseWork> get(Configuration conf) {\n-    if (LlapIoProxy.isDaemon() || SessionState.get().isHiveServerQuery()\n+    if (LlapIoProxy.isDaemon()\n+        || (SessionState.get() != null && SessionState.get().isHiveServerQuery())\n         || HiveConf.getVar(conf, ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n       if (threadLocalWorkMap == null) {\n         threadLocalWorkMap = new ThreadLocal<Map<Path, BaseWork>>() {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/GlobalWorkMapFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/df3b030ae0937e217fb719e201947c15c3dbb483", "parent": "https://github.com/apache/hive/commit/40a81e80ccda292b1455bc70eaeba9ce40b2b00d", "message": "LLAP: NPE when calling abort on the TezProcessor. (Siddharth Seth)", "bug_id": "hive_92", "file": [{"additions": 19, "raw_url": "https://github.com/apache/hive/raw/df3b030ae0937e217fb719e201947c15c3dbb483/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java", "blob_url": "https://github.com/apache/hive/blob/df3b030ae0937e217fb719e201947c15c3dbb483/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java", "sha": "9baa0c1ca59fd8f86961bfa63316902286671a5d", "changes": 21, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java?ref=df3b030ae0937e217fb719e201947c15c3dbb483", "patch": "@@ -21,6 +21,7 @@\n import java.text.NumberFormat;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -50,6 +51,7 @@\n   protected boolean isMap = false;\n \n   protected RecordProcessor rproc = null;\n+  private final AtomicBoolean aborted = new AtomicBoolean(false);\n \n   protected JobConf jobConf;\n \n@@ -115,26 +117,34 @@ private void setupMRLegacyConfigs(ProcessorContext processorContext) {\n     String taskAttemptIdStr = taskAttemptIdBuilder.toString();\n     this.jobConf.set(\"mapred.task.id\", taskAttemptIdStr);\n     this.jobConf.set(\"mapreduce.task.attempt.id\", taskAttemptIdStr);\n-    this.jobConf.setInt(\"mapred.task.partition\",processorContext.getTaskIndex());\n+    this.jobConf.setInt(\"mapred.task.partition\", processorContext.getTaskIndex());\n   }\n \n   @Override\n   public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs)\n       throws Exception {\n \n+    if (aborted.get()) {\n+      return;\n+    }\n+\n       perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_RUN_PROCESSOR);\n       // in case of broadcast-join read the broadcast edge inputs\n       // (possibly asynchronously)\n \n       LOG.info(\"Running task: \" + getContext().getUniqueIdentifier());\n \n+    synchronized (this) {\n       if (isMap) {\n         rproc = new MapRecordProcessor(jobConf, getContext());\n       } else {\n         rproc = new ReduceRecordProcessor(jobConf, getContext());\n       }\n+    }\n \n+    if (!aborted.get()) {\n       initializeAndRunProcessor(inputs, outputs);\n+    }\n   }\n \n   protected void initializeAndRunProcessor(Map<String, LogicalInput> inputs,\n@@ -174,7 +184,14 @@ protected void initializeAndRunProcessor(Map<String, LogicalInput> inputs,\n   }\n \n   public void abort() {\n-    rproc.abort();\n+    aborted.set(true);\n+    RecordProcessor rProcLocal;\n+    synchronized (this) {\n+      rProcLocal = rproc;\n+    }\n+    if (rProcLocal != null) {\n+      rProcLocal.abort();\n+    }\n   }\n \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b6502b5ea35f316ed10e71d845a7b5c6ab4ad151", "parent": "https://github.com/apache/hive/commit/62bae5e1a5cc563c5ef3f650927f2a63038c5a50", "message": "HIVE-13237: Select parquet struct field with upper case throws NPE (Jimmy, reviewed by Xuefu)", "bug_id": "hive_93", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/b6502b5ea35f316ed10e71d845a7b5c6ab4ad151/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java", "blob_url": "https://github.com/apache/hive/blob/b6502b5ea35f316ed10e71d845a7b5c6ab4ad151/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java", "sha": "5f852d0368048e5a5819cf74f6ee5762fa794983", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java?ref=b6502b5ea35f316ed10e71d845a7b5c6ab4ad151", "patch": "@@ -63,7 +63,7 @@ public ArrayWritableObjectInspector(final StructTypeInfo rowTypeInfo) {\n \n       final StructFieldImpl field = new StructFieldImpl(name, getObjectInspector(fieldInfo), i);\n       fields.add(field);\n-      fieldsByName.put(name, field);\n+      fieldsByName.put(name.toLowerCase(), field);\n     }\n   }\n \n@@ -158,7 +158,7 @@ public Object getStructFieldData(final Object data, final StructField fieldRef)\n \n   @Override\n   public StructField getStructFieldRef(final String name) {\n-    return fieldsByName.get(name);\n+    return fieldsByName.get(name.toLowerCase());\n   }\n \n   @Override", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ArrayWritableObjectInspector.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ac9c5991b5bf8184319657315638af73b22b1c02", "parent": "https://github.com/apache/hive/commit/305c2f6ef0eb552d43a4f6c3bcb0e0284be78529", "message": "HIVE-13099: Non-SQLOperations lead to Web UI NPE (Mohit via Jimmy)", "bug_id": "hive_94", "file": [{"additions": 27, "raw_url": "https://github.com/apache/hive/raw/ac9c5991b5bf8184319657315638af73b22b1c02/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java", "blob_url": "https://github.com/apache/hive/blob/ac9c5991b5bf8184319657315638af73b22b1c02/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java", "sha": "56a9c18bc118b8d67f87e5500ec7d4442a90a869", "changes": 47, "status": "modified", "deletions": 20, "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java?ref=ac9c5991b5bf8184319657315638af73b22b1c02", "patch": "@@ -181,22 +181,6 @@ private Operation getOperationInternal(OperationHandle operationHandle) {\n     return handleToOperation.get(operationHandle);\n   }\n \n-  private Operation removeTimedOutOperation(OperationHandle operationHandle) {\n-    Operation operation = handleToOperation.get(operationHandle);\n-    if (operation != null && operation.isTimedOut(System.currentTimeMillis())) {\n-      handleToOperation.remove(operationHandle, operation);\n-      synchronized (webuiLock) {\n-        String opKey = operationHandle.getHandleIdentifier().toString();\n-        SQLOperationDisplay display = liveSqlOperations.remove(opKey);\n-        if (historicSqlOperations != null) {\n-          historicSqlOperations.put(opKey, display);\n-        }\n-      }\n-      return operation;\n-    }\n-    return null;\n-  }\n-\n   private void addOperation(Operation operation) {\n     handleToOperation.put(operation.getHandle(), operation);\n     if (operation instanceof SQLOperation) {\n@@ -208,15 +192,38 @@ private void addOperation(Operation operation) {\n   }\n \n   private Operation removeOperation(OperationHandle opHandle) {\n-    Operation result = handleToOperation.remove(opHandle);\n+    Operation operation = handleToOperation.remove(opHandle);\n+    if (operation instanceof SQLOperation) {\n+      removeSaveSqlOperationDisplay(opHandle);\n+    }\n+    return operation;\n+  }\n+\n+  private Operation removeTimedOutOperation(OperationHandle operationHandle) {\n+    Operation operation = handleToOperation.get(operationHandle);\n+    if (operation != null && operation.isTimedOut(System.currentTimeMillis())) {\n+      handleToOperation.remove(operationHandle, operation);\n+      if (operation instanceof SQLOperation) {\n+        removeSaveSqlOperationDisplay(operationHandle);\n+      }\n+      return operation;\n+    }\n+    return null;\n+  }\n+\n+  private void removeSaveSqlOperationDisplay(OperationHandle operationHandle) {\n     synchronized (webuiLock) {\n-      String opKey = opHandle.getHandleIdentifier().toString();\n+      String opKey = operationHandle.getHandleIdentifier().toString();\n+      // remove from list of live operations\n       SQLOperationDisplay display = liveSqlOperations.remove(opKey);\n-      if (historicSqlOperations != null) {\n+      if (display == null) {\n+        LOG.debug(\"Unexpected display object value of null for operation {}\",\n+            opKey);\n+      } else if (historicSqlOperations != null) {\n+        // add to list of saved historic operations\n         historicSqlOperations.put(opKey, display);\n       }\n     }\n-    return result;\n   }\n \n   public OperationStatus getOperationStatus(OperationHandle opHandle)", "filename": "service/src/java/org/apache/hive/service/cli/operation/OperationManager.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5d94d4c205bad635f157f1eedd782aeb88c27cf6", "parent": "https://github.com/apache/hive/commit/4d67088efa9b6438ff178e0b55382f167be06925", "message": "HIVE-12673: Orcfiledump throws NPE when no files are available (Rajesh Balamohan reviewed by Prasanth Jayachandran)", "bug_id": "hive_95", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/5d94d4c205bad635f157f1eedd782aeb88c27cf6/ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java", "blob_url": "https://github.com/apache/hive/blob/5d94d4c205bad635f157f1eedd782aeb88c27cf6/ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java", "sha": "00de5451d2779af2f077c3c88d3135ba5c49c85d", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java?ref=5d94d4c205bad635f157f1eedd782aeb88c27cf6", "patch": "@@ -54,6 +54,9 @@ public static void printJsonMetaData(List<String> files,\n       Configuration conf,\n       List<Integer> rowIndexCols, boolean prettyPrint, boolean printTimeZone)\n       throws JSONException, IOException {\n+    if (files.isEmpty()) {\n+      return;\n+    }\n     JSONStringer writer = new JSONStringer();\n     boolean multiFile = files.size() > 1;\n     if (multiFile) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54", "parent": "https://github.com/apache/hive/commit/38b172cdce32fa0b9014f16b44e8dc96bdc143d3", "message": "HIVE-12990 : LLAP: ORC cache NPE without FileID support (Sergey Shelukhin, reviewed by Prasanth Jayachandran)", "bug_id": "hive_96", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java", "blob_url": "https://github.com/apache/hive/blob/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java", "sha": "b36cf6499bcf443bb168c839e486822e05fe14e8", "changes": 11, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java?ref=49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54", "patch": "@@ -394,18 +394,16 @@ protected Void performDataRead() throws IOException {\n             counters.incrCounter(Counter.METADATA_CACHE_MISS);\n             ensureMetadataReader();\n             long startTimeHdfs = counters.startTimeCounter();\n-            stripeMetadata = new OrcStripeMetadata(\n-                stripeKey, metadataReader, stripe, stripeIncludes, sargColumns);\n+            stripeMetadata = new OrcStripeMetadata(new OrcBatchKey(fileId, stripeIx, 0),\n+                metadataReader, stripe, stripeIncludes, sargColumns);\n             counters.incrTimeCounter(Counter.HDFS_TIME_US, startTimeHdfs);\n             if (hasFileId && metadataCache != null) {\n               stripeMetadata = metadataCache.putStripeMetadata(stripeMetadata);\n               if (DebugUtils.isTraceOrcEnabled()) {\n                 LlapIoImpl.LOG.info(\"Caching stripe \" + stripeKey.stripeIx\n                     + \" metadata with includes: \" + DebugUtils.toString(stripeIncludes));\n               }\n-              stripeKey = new OrcBatchKey(fileId, -1, 0);\n             }\n-\n           }\n           consumer.setStripeMetadata(stripeMetadata);\n         }\n@@ -658,16 +656,15 @@ private OrcFileMetadata getOrReadFileMetadata() throws IOException {\n         StripeInformation si = fileMetadata.getStripes().get(stripeIx);\n         if (value == null) {\n           long startTime = counters.startTimeCounter();\n-          value = new OrcStripeMetadata(stripeKey, metadataReader, si, globalInc, sargColumns);\n+          value = new OrcStripeMetadata(new OrcBatchKey(fileId, stripeIx, 0),\n+              metadataReader, si, globalInc, sargColumns);\n           counters.incrTimeCounter(Counter.HDFS_TIME_US, startTime);\n           if (hasFileId && metadataCache != null) {\n             value = metadataCache.putStripeMetadata(value);\n             if (DebugUtils.isTraceOrcEnabled()) {\n               LlapIoImpl.LOG.info(\"Caching stripe \" + stripeKey.stripeIx\n                   + \" metadata with includes: \" + DebugUtils.toString(globalInc));\n             }\n-            // Create new key object to reuse for gets; we've used the old one to put in cache.\n-            stripeKey = new OrcBatchKey(fileId, 0, 0);\n           }\n         }\n         // We might have got an old value from cache; recheck it has indexes.", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java", "sha": "6cec80ee968a503c48cdd487fe11ab31cc2aa592", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54", "patch": "@@ -106,7 +106,7 @@ public DiskRangeList createCacheChunk(MemoryBuffer buffer, long offset, long end\n   private ByteBufferAllocatorPool pool;\n   private boolean isDebugTracingEnabled;\n \n-  public EncodedReaderImpl(long fileId, List<OrcProto.Type> types, CompressionCodec codec,\n+  public EncodedReaderImpl(Long fileId, List<OrcProto.Type> types, CompressionCodec codec,\n       int bufferSize, long strideRate, DataCache cache, DataReader dataReader, PoolFactory pf)\n           throws IOException {\n     this.fileId = fileId;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7", "parent": "https://github.com/apache/hive/commit/fad946bb2c11a9159b78426865975c0782e5f663", "message": "HIVE-14173: NPE was thrown after enabling directsql in the middle of session (Chaoyu Tang, reviewed by Sergey Shelukhin)", "bug_id": "hive_97", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "blob_url": "https://github.com/apache/hive/blob/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "sha": "82de857d221889d2c62252aabc0741a83d5f5313", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7", "patch": "@@ -2661,6 +2661,10 @@ public GetHelper(String dbName, String tblName, boolean allowSql, boolean allowJ\n       // the fallback from failed SQL to JDO is not possible.\n       boolean isConfigEnabled = HiveConf.getBoolVar(getConf(), ConfVars.METASTORE_TRY_DIRECT_SQL)\n           && (HiveConf.getBoolVar(getConf(), ConfVars.METASTORE_TRY_DIRECT_SQL_DDL) || !isInTxn);\n+      if (isConfigEnabled && directSql == null) {\n+        directSql = new MetaStoreDirectSql(pm, getConf());\n+      }\n+\n       if (!allowJdo && isConfigEnabled && !directSql.isCompatibleDatastore()) {\n         throw new MetaException(\"SQL is not operational\"); // test path; SQL is enabled and broken.\n       }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/fb35bae5ae2fad93de3deef9023f52cba8e4783b", "parent": "https://github.com/apache/hive/commit/72cea13e4d968fad86be733c1f1aa65aafbb1fc4", "message": "HIVE-14109: query execuction throws NPE when hive.exec.submitviachild is set to true (Aihua Xu, reviewed by Sergio Pe\u00f1a)", "bug_id": "hive_98", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java", "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java", "sha": "285f9ad76e07b66dc338c5d33859ff47ac08d9a8", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b", "patch": "@@ -126,7 +126,8 @@\n   public ExecDriver() {\n     super();\n     console = new LogHelper(LOG);\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, this);\n+    job = new JobConf(ExecDriver.class);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);\n   }\n \n   @Override\n@@ -175,7 +176,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext\n     initializeFiles(\"tmparchives\", getResource(conf, SessionState.ResourceType.ARCHIVE));\n \n     conf.stripHiddenConfigurations(job);\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, this);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);\n   }\n \n   /**\n@@ -185,7 +186,7 @@ public ExecDriver(MapredWork plan, JobConf job, boolean isSilent) throws HiveExc\n     setWork(plan);\n     this.job = job;\n     console = new LogHelper(LOG, isSilent);\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, this);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);\n   }\n \n   /**\n@@ -671,6 +672,7 @@ public static void main(String[] args) throws IOException, HiveException {\n     String queryId = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID, \"\").trim();\n     if(queryId.isEmpty()) {\n       queryId = \"unknown-\" + System.currentTimeMillis();\n+      HiveConf.setVar(conf, HiveConf.ConfVars.HIVEQUERYID, queryId);\n     }\n     System.setProperty(HiveConf.ConfVars.HIVEQUERYID.toString(), queryId);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java", "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java", "sha": "cfb4a2816817925fe7351f7d17bd8ab35742890b", "changes": 18, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b", "patch": "@@ -34,7 +34,6 @@\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n import org.apache.hadoop.hive.ql.Context;\n import org.apache.hadoop.hive.ql.MapRedStats;\n-import org.apache.hadoop.hive.ql.QueryState;\n import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskHandle;\n@@ -78,7 +77,7 @@\n   public transient JobID jobId;\n   private final LogHelper console;\n   private final HadoopJobExecHook callBackObj;\n-  private final QueryState queryState;\n+  private final String queryId;\n \n   /**\n    * Update counters relevant to this task.\n@@ -139,9 +138,9 @@ public void setJobId(JobID jobId) {\n     this.jobId = jobId;\n   }\n \n-  public HadoopJobExecHelper(QueryState queryState, JobConf job, LogHelper console,\n+  public HadoopJobExecHelper(JobConf job, LogHelper console,\n       Task<? extends Serializable> task, HadoopJobExecHook hookCallBack) {\n-    this.queryState = queryState;\n+    this.queryId = HiveConf.getVar(job, HiveConf.ConfVars.HIVEQUERYID, \"unknown-\" + System.currentTimeMillis());\n     this.job = job;\n     this.console = console;\n     this.task = task;\n@@ -259,7 +258,6 @@ private MapRedStats progress(ExecDriverTaskHandle th) throws IOException, LockEx\n \n           String logMapper;\n           String logReducer;\n-          String queryId = queryState.getQueryId();\n           TaskReport[] mappers = jc.getMapTaskReports(rj.getID());\n           if (mappers == null) {\n             logMapper = \"no information for number of mappers; \";\n@@ -364,11 +362,11 @@ private MapRedStats progress(ExecDriverTaskHandle th) throws IOException, LockEx\n       String output = report.toString();\n       SessionState ss = SessionState.get();\n       if (ss != null) {\n-        ss.getHiveHistory().setTaskCounters(queryState.getQueryId(), getId(), ctrs);\n-        ss.getHiveHistory().setTaskProperty(queryState.getQueryId(), getId(),\n+        ss.getHiveHistory().setTaskCounters(queryId, getId(), ctrs);\n+        ss.getHiveHistory().setTaskProperty(queryId, getId(),\n             Keys.TASK_HADOOP_PROGRESS, output);\n         if (ss.getConf().getBoolVar(HiveConf.ConfVars.HIVE_LOG_INCREMENTAL_PLAN_PROGRESS)) {\n-          ss.getHiveHistory().progressTask(queryState.getQueryId(), this.task);\n+          ss.getHiveHistory().progressTask(queryId, this.task);\n           this.callBackObj.logPlanProgress(ss);\n         }\n       }\n@@ -397,7 +395,7 @@ private MapRedStats progress(ExecDriverTaskHandle th) throws IOException, LockEx\n       } else {\n         SessionState ss = SessionState.get();\n         if (ss != null) {\n-          ss.getHiveHistory().setTaskCounters(queryState.getQueryId(), getId(), ctrs);\n+          ss.getHiveHistory().setTaskCounters(queryId, getId(), ctrs);\n         }\n         success = rj.isSuccessful();\n       }\n@@ -441,7 +439,7 @@ public void jobInfo(RunningJob rj) {\n       console.printInfo(\"Job running in-process (local Hadoop)\");\n     } else {\n       if (SessionState.get() != null) {\n-        SessionState.get().getHiveHistory().setTaskProperty(queryState.getQueryId(),\n+        SessionState.get().getHiveHistory().setTaskProperty(queryId,\n             getId(), Keys.TASK_HADOOP_ID, rj.getID().toString());\n       }\n       console.printInfo(getJobStartMsg(rj.getID()) + \", Tracking URL = \"", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java", "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java", "sha": "23a13d66c2dc1434c96164a7e2a8869fd5b4fcae", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b", "patch": "@@ -128,7 +128,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext\n     job = new JobConf(conf, ExecDriver.class);\n     execContext = new ExecMapperContext(job);\n     //we don't use the HadoopJobExecHooks for local tasks\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, null);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, null);\n   }\n \n   public static String now() {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java", "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java", "sha": "376bab2c39f775a6d6c931f55f60c90c87e41957", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b", "patch": "@@ -65,7 +65,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan,\n       DriverContext driverContext, CompilationOpContext opContext) {\n     super.initialize(queryState, queryPlan, driverContext, opContext);\n     job = new JobConf(conf, MergeFileTask.class);\n-    jobExecHelper = new HadoopJobExecHelper(queryState, job, this.console, this, this);\n+    jobExecHelper = new HadoopJobExecHelper(job, this.console, this, this);\n   }\n \n   @Override", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java", "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java", "sha": "6131581b1965adccd3f49f8fd730648acffbc78e", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b", "patch": "@@ -89,7 +89,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan,\n       DriverContext driverContext, CompilationOpContext opContext) {\n     super.initialize(queryState, queryPlan, driverContext, opContext);\n     job = new JobConf(conf, PartialScanTask.class);\n-    jobExecHelper = new HadoopJobExecHelper(queryState, job, this.console, this, this);\n+    jobExecHelper = new HadoopJobExecHelper(job, this.console, this, this);\n   }\n \n   @Override", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java", "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java", "sha": "2d29afcbac0fcc6ce7e9554ad78d2b98127103a5", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b", "patch": "@@ -64,7 +64,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan,\n       DriverContext driverContext, CompilationOpContext opContext) {\n     super.initialize(queryState, queryPlan, driverContext, opContext);\n     job = new JobConf(conf, ColumnTruncateTask.class);\n-    jobExecHelper = new HadoopJobExecHelper(queryState, job, this.console, this, this);\n+    jobExecHelper = new HadoopJobExecHelper(job, this.console, this, this);\n   }\n \n   @Override", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0b96c9b7662d4326b02af5b2d84321978e65ea46", "parent": "https://github.com/apache/hive/commit/cee2619e095b8172d77370b7baf5c04da6bea2c7", "message": "HIVE-11470 : NPE in DynamicPartFileRecordWriterContainer on null part-keys (Mithun Radhakrishnan via Sushanth Sowmyan)", "bug_id": "hive_99", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/0b96c9b7662d4326b02af5b2d84321978e65ea46/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java", "blob_url": "https://github.com/apache/hive/blob/0b96c9b7662d4326b02af5b2d84321978e65ea46/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java", "sha": "a7c9f29ecc2dc7fdf99c401af0be7bff54d50efe", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java?ref=0b96c9b7662d4326b02af5b2d84321978e65ea46", "patch": "@@ -62,6 +62,8 @@\n   private final Map<String, ObjectInspector> dynamicObjectInspectors;\n   private Map<String, OutputJobInfo> dynamicOutputJobInfo;\n \n+  private String HIVE_DEFAULT_PARTITION_VALUE = null;\n+\n   /**\n    * @param baseWriter RecordWriter to contain\n    * @param context current TaskAttemptContext\n@@ -86,6 +88,7 @@ public DynamicPartitionFileRecordWriterContainer(\n     this.dynamicContexts = new HashMap<String, org.apache.hadoop.mapred.TaskAttemptContext>();\n     this.dynamicObjectInspectors = new HashMap<String, ObjectInspector>();\n     this.dynamicOutputJobInfo = new HashMap<String, OutputJobInfo>();\n+    this.HIVE_DEFAULT_PARTITION_VALUE = HiveConf.getVar(context.getConfiguration(), HiveConf.ConfVars.DEFAULTPARTITIONNAME);\n   }\n \n   @Override\n@@ -136,7 +139,8 @@ protected LocalFileWriter getLocalFileWriter(HCatRecord value) throws IOExceptio\n     // be done before we delete cols.\n     List<String> dynamicPartValues = new ArrayList<String>();\n     for (Integer colToAppend : dynamicPartCols) {\n-      dynamicPartValues.add(value.get(colToAppend).toString());\n+      Object partitionValue = value.get(colToAppend);\n+      dynamicPartValues.add(partitionValue == null? HIVE_DEFAULT_PARTITION_VALUE : partitionValue.toString());\n     }\n \n     String dynKey = dynamicPartValues.toString();", "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/DynamicPartitionFileRecordWriterContainer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0b810991a12bb7c8d52f64b781772a1deabcbe53", "parent": "https://github.com/apache/hive/commit/6e513b06c12e508af655a6bea4aef55b86619cc6", "message": "HIVE-12740: NPE with HS2 when using null input format (Vikram Dixit K via Gunther Hagleitner)", "bug_id": "hive_100", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "blob_url": "https://github.com/apache/hive/blob/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "sha": "fce11c86fcdf1e9c72575ce5025208e1018a3591", "changes": 17, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=0b810991a12bb7c8d52f64b781772a1deabcbe53", "patch": "@@ -215,6 +215,7 @@\n   public static final String MAPRED_MAPPER_CLASS = \"mapred.mapper.class\";\n   public static final String MAPRED_REDUCER_CLASS = \"mapred.reducer.class\";\n   public static final String HIVE_ADDED_JARS = \"hive.added.jars\";\n+  public static final String VECTOR_MODE = \"VECTOR_MODE\";\n   public static String MAPNAME = \"Map \";\n   public static String REDUCENAME = \"Reducer \";\n \n@@ -3238,12 +3239,18 @@ private static void resetUmaskInConf(Configuration conf, boolean unsetUmask, Str\n    * but vectorization disallowed eg. for FetchOperator execution.\n    */\n   public static boolean isVectorMode(Configuration conf) {\n-    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) &&\n-        Utilities.getPlanPath(conf) != null && Utilities\n-        .getMapWork(conf).getVectorMode()) {\n-      return true;\n+    if (conf.get(VECTOR_MODE) != null) {\n+      // this code path is necessary, because with HS2 and client\n+      // side split generation we end up not finding the map work.\n+      // This is because of thread local madness (tez split\n+      // generation is multi-threaded - HS2 plan cache uses thread\n+      // locals).\n+      return conf.getBoolean(VECTOR_MODE, false);\n+    } else {\n+      return HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED)\n+        && Utilities.getPlanPath(conf) != null\n+        && Utilities.getMapWork(conf).getVectorMode();\n     }\n-    return false;\n   }\n \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "blob_url": "https://github.com/apache/hive/blob/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "sha": "e8864aee6428d3cc6b165acbc188cac8d22ffd61", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=0b810991a12bb7c8d52f64b781772a1deabcbe53", "patch": "@@ -614,6 +614,15 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       }\n     } else {\n       // Setup client side split generation.\n+\n+      // we need to set this, because with HS2 and client side split\n+      // generation we end up not finding the map work. This is\n+      // because of thread local madness (tez split generation is\n+      // multi-threaded - HS2 plan cache uses thread locals). Setting\n+      // VECTOR_MODE causes the split gen code to use the conf instead\n+      // of the map work.\n+      conf.setBoolean(Utilities.VECTOR_MODE, mapWork.getVectorMode());\n+\n       dataSource = MRInputHelpers.configureMRInputWithLegacySplitGeneration(conf, new Path(tezDir,\n           \"split_\" + mapWork.getName().replaceAll(\" \", \"_\")), true);\n       numTasks = dataSource.getNumberOfShards();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java"}], "repo": "hive"}]
[{"commit": "https://github.com/apache/hive/commit/0cbf45cfc046f39bed4533ab83542002e79b4f5b", "parent": "https://github.com/apache/hive/commit/b340ecb5e163277e86def59b454b8c041ece39d5", "message": "HIVE-12577: NPE in LlapTaskCommunicator when unregistering containers (Siddarth Seth, reviewed by Sergey Shelukhin)", "bug_id": "hive_101", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java", "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java", "previous_filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/TaskCommunicator.java", "sha": "2884e40fe787a1b0884b6832f6a05fa6945baf4a", "changes": 11, "status": "renamed", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b", "patch": "@@ -16,8 +16,6 @@\n \n import javax.net.SocketFactory;\n \n-import java.io.ByteArrayInputStream;\n-import java.io.DataInputStream;\n import java.io.IOException;\n import java.security.PrivilegedAction;\n import java.util.HashSet;\n@@ -49,7 +47,6 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n import org.apache.hadoop.hive.llap.LlapNodeId;\n-import org.apache.hadoop.hive.llap.configuration.LlapConfiguration;\n import org.apache.hadoop.hive.llap.daemon.LlapDaemonProtocolBlockingPB;\n import org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolClientImpl;\n import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto;\n@@ -71,9 +68,9 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-public class TaskCommunicator extends AbstractService {\n+public class LlapDaemonProtocolClientProxy extends AbstractService {\n \n-  private static final Logger LOG = LoggerFactory.getLogger(TaskCommunicator.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(LlapDaemonProtocolClientProxy.class);\n \n   private final ConcurrentMap<String, LlapDaemonProtocolBlockingPB> hostProxies;\n \n@@ -85,9 +82,9 @@\n   private volatile ListenableFuture<Void> requestManagerFuture;\n   private final Token<LlapTokenIdentifier> llapToken;\n \n-  public TaskCommunicator(\n+  public LlapDaemonProtocolClientProxy(\n       int numThreads, Configuration conf, Token<LlapTokenIdentifier> llapToken) {\n-    super(TaskCommunicator.class.getSimpleName());\n+    super(LlapDaemonProtocolClientProxy.class.getSimpleName());\n     this.hostProxies = new ConcurrentHashMap<>();\n     this.socketFactory = NetUtils.getDefaultSocketFactory(conf);\n     this.llapToken = llapToken;", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java"}, {"additions": 143, "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java", "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java", "sha": "5c370eef18e3037ebdc8abee4497cca04e461d1c", "changes": 187, "status": "modified", "deletions": 44, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b", "patch": "@@ -22,8 +22,10 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicLong;\n \n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.BiMap;\n import com.google.common.collect.HashBiMap;\n@@ -79,6 +81,9 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(LlapTaskCommunicator.class);\n \n+  private static final boolean isInfoEnabled = LOG.isInfoEnabled();\n+  private static final boolean isDebugEnabed = LOG.isDebugEnabled();\n+\n   private final SubmitWorkRequestProto BASE_SUBMIT_WORK_REQUEST;\n   private final ConcurrentMap<String, ByteBuffer> credentialMap;\n \n@@ -88,11 +93,17 @@\n   private final SourceStateTracker sourceStateTracker;\n   private final Set<LlapNodeId> nodesForQuery = new HashSet<>();\n \n-  private TaskCommunicator communicator;\n+  private LlapDaemonProtocolClientProxy communicator;\n   private long deleteDelayOnDagComplete;\n   private final LlapTaskUmbilicalProtocol umbilical;\n   private final Token<LlapTokenIdentifier> token;\n \n+  // These two structures track the list of known nodes, and the list of nodes which are sending in keep-alive heartbeats.\n+  // Primarily for debugging purposes a.t.m, since there's some unexplained TASK_TIMEOUTS which are currently being observed.\n+  private final ConcurrentMap<LlapNodeId, Long> knownNodeMap = new ConcurrentHashMap<>();\n+  private final ConcurrentMap<LlapNodeId, PingingNodeInfo> pingedNodeMap = new ConcurrentHashMap<>();\n+\n+\n   private volatile String currentDagName;\n \n   public LlapTaskCommunicator(\n@@ -131,7 +142,7 @@ public void initialize() throws Exception {\n     super.initialize();\n     Configuration conf = getConf();\n     int numThreads = HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_COMMUNICATOR_NUM_THREADS);\n-    this.communicator = new TaskCommunicator(numThreads, conf, token);\n+    this.communicator = new LlapDaemonProtocolClientProxy(numThreads, conf, token);\n     this.deleteDelayOnDagComplete = HiveConf.getTimeVar(\n         conf, ConfVars.LLAP_FILE_CLEANUP_DELAY_SECONDS, TimeUnit.SECONDS);\n     LOG.info(\"Running LlapTaskCommunicator with \"\n@@ -235,6 +246,7 @@ public void registerRunningTaskAttempt(final ContainerId containerId, final Task\n     }\n \n     LlapNodeId nodeId = LlapNodeId.getInstance(host, port);\n+    registerKnownNode(nodeId);\n     entityTracker.registerTaskAttempt(containerId, taskSpec.getTaskAttemptID(), host, port);\n     nodesForQuery.add(nodeId);\n \n@@ -254,7 +266,7 @@ public void registerRunningTaskAttempt(final ContainerId containerId, final Task\n     getContext()\n         .taskStartedRemotely(taskSpec.getTaskAttemptID(), containerId);\n     communicator.sendSubmitWork(requestProto, host, port,\n-        new TaskCommunicator.ExecuteRequestCallback<SubmitWorkResponseProto>() {\n+        new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<SubmitWorkResponseProto>() {\n           @Override\n           public void setResponse(SubmitWorkResponseProto response) {\n             if (response.hasSubmissionState()) {\n@@ -333,14 +345,14 @@ private void sendTaskTerminated(final TezTaskAttemptID taskAttemptId,\n     LOG.info(\n         \"DBG: Attempting to send terminateRequest for fragment {} due to internal preemption invoked by {}\",\n         taskAttemptId.toString(), invokedByContainerEnd ? \"containerEnd\" : \"taskEnd\");\n-    LlapNodeId nodeId = entityTracker.getNodeIfForTaskAttempt(taskAttemptId);\n+    LlapNodeId nodeId = entityTracker.getNodeIdForTaskAttempt(taskAttemptId);\n     // NodeId can be null if the task gets unregistered due to failure / being killed by the daemon itself\n     if (nodeId != null) {\n       TerminateFragmentRequestProto request =\n           TerminateFragmentRequestProto.newBuilder().setDagName(currentDagName)\n               .setFragmentIdentifierString(taskAttemptId.toString()).build();\n       communicator.sendTerminateFragment(request, nodeId.getHostname(), nodeId.getPort(),\n-          new TaskCommunicator.ExecuteRequestCallback<TerminateFragmentResponseProto>() {\n+          new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<TerminateFragmentResponseProto>() {\n             @Override\n             public void setResponse(TerminateFragmentResponseProto response) {\n             }\n@@ -365,7 +377,7 @@ public void dagComplete(final String dagName) {\n     for (final LlapNodeId llapNodeId : nodesForQuery) {\n       LOG.info(\"Sending dagComplete message for {}, to {}\", dagName, llapNodeId);\n       communicator.sendQueryComplete(request, llapNodeId.getHostname(), llapNodeId.getPort(),\n-          new TaskCommunicator.ExecuteRequestCallback<LlapDaemonProtocolProtos.QueryCompleteResponseProto>() {\n+          new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<LlapDaemonProtocolProtos.QueryCompleteResponseProto>() {\n             @Override\n             public void setResponse(LlapDaemonProtocolProtos.QueryCompleteResponseProto response) {\n             }\n@@ -391,7 +403,7 @@ public void onVertexStateUpdated(VertexStateUpdate vertexStateUpdate) {\n   public void sendStateUpdate(final String host, final int port,\n                               final SourceStateUpdatedRequestProto request) {\n     communicator.sendSourceStateUpdate(request, host, port,\n-        new TaskCommunicator.ExecuteRequestCallback<SourceStateUpdatedResponseProto>() {\n+        new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<SourceStateUpdatedResponseProto>() {\n           @Override\n           public void setResponse(SourceStateUpdatedResponseProto response) {\n           }\n@@ -409,6 +421,79 @@ public void indicateError(Throwable t) {\n   }\n \n \n+  private static class PingingNodeInfo {\n+    final AtomicLong logTimestamp;\n+    final AtomicInteger pingCount;\n+\n+    PingingNodeInfo(long currentTs) {\n+      logTimestamp = new AtomicLong(currentTs);\n+      pingCount = new AtomicInteger(1);\n+    }\n+  }\n+\n+  public void registerKnownNode(LlapNodeId nodeId) {\n+    Long old = knownNodeMap.putIfAbsent(nodeId,\n+        TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS));\n+    if (old == null) {\n+      if (isInfoEnabled) {\n+        LOG.info(\"Added new known node: {}\", nodeId);\n+      }\n+    }\n+  }\n+\n+  public void registerPingingNode(LlapNodeId nodeId) {\n+    long currentTs = TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS);\n+    PingingNodeInfo ni = new PingingNodeInfo(currentTs);\n+    PingingNodeInfo old = pingedNodeMap.put(nodeId, ni);\n+    if (old == null) {\n+      if (isInfoEnabled) {\n+        LOG.info(\"Added new pinging node: [{}]\", nodeId);\n+      }\n+    } else {\n+      old.pingCount.incrementAndGet();\n+    }\n+    // The node should always be known by this point. Log occasionally if it is not known.\n+    if (!knownNodeMap.containsKey(nodeId)) {\n+      if (old == null) {\n+        // First time this is seen. Log it.\n+        LOG.warn(\"Received ping from unknownNode: [{}], count={}\", nodeId, ni.pingCount.get());\n+      } else {\n+        // Pinged before. Log only occasionally.\n+        if (currentTs > old.logTimestamp.get() + 5000l) { // 5 seconds elapsed. Log again.\n+          LOG.warn(\"Received ping from unknownNode: [{}], count={}\", nodeId, old.pingCount.get());\n+          old.logTimestamp.set(currentTs);\n+        }\n+      }\n+\n+    }\n+  }\n+\n+\n+  private final AtomicLong nodeNotFoundLogTime = new AtomicLong(0);\n+\n+  void nodePinged(String hostname, int port) {\n+    LlapNodeId nodeId = LlapNodeId.getInstance(hostname, port);\n+    registerPingingNode(nodeId);\n+    BiMap<ContainerId, TezTaskAttemptID> biMap =\n+        entityTracker.getContainerAttemptMapForNode(nodeId);\n+    if (biMap != null) {\n+      synchronized (biMap) {\n+        for (Map.Entry<ContainerId, TezTaskAttemptID> entry : biMap.entrySet()) {\n+          getContext().taskAlive(entry.getValue());\n+          getContext().containerAlive(entry.getKey());\n+        }\n+      }\n+    } else {\n+      long currentTs = TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS);\n+      if (currentTs > nodeNotFoundLogTime.get() + 5000l) {\n+        LOG.warn(\"Received ping from node without any registered tasks or containers: \" + hostname +\n+            \":\" + port +\n+            \". Could be caused by pre-emption by the AM,\" +\n+            \" or a mismatched hostname. Enable debug logging for mismatched host names\");\n+        nodeNotFoundLogTime.set(currentTs);\n+      }\n+    }\n+  }\n \n   private void resetCurrentDag(String newDagName) {\n     // Working on the assumption that a single DAG runs at a time per AM.\n@@ -454,6 +539,8 @@ private ByteBuffer serializeCredentials(Credentials credentials) throws IOExcept\n     return ByteBuffer.wrap(containerTokens_dob.getData(), 0, containerTokens_dob.getLength());\n   }\n \n+\n+\n   protected class LlapTaskUmbilicalProtocolImpl implements LlapTaskUmbilicalProtocol {\n \n     private final TezTaskUmbilicalProtocol tezUmbilical;\n@@ -475,7 +562,7 @@ public TezHeartbeatResponse heartbeat(TezHeartbeatRequest request) throws IOExce\n \n     @Override\n     public void nodeHeartbeat(Text hostname, int port) throws IOException {\n-      entityTracker.nodePinged(hostname.toString(), port);\n+      nodePinged(hostname.toString(), port);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Received heartbeat from [\" + hostname + \":\" + port +\"]\");\n       }\n@@ -502,17 +589,28 @@ public ProtocolSignature getProtocolSignature(String protocol, long clientVersio\n     }\n   }\n \n-  private final class EntityTracker {\n-    private final ConcurrentMap<TezTaskAttemptID, LlapNodeId> attemptToNodeMap = new ConcurrentHashMap<>();\n-    private final ConcurrentMap<ContainerId, LlapNodeId> containerToNodeMap = new ConcurrentHashMap<>();\n-    private final ConcurrentMap<LlapNodeId, BiMap<ContainerId, TezTaskAttemptID>> nodeMap = new ConcurrentHashMap<>();\n+  /**\n+   * Track the association between known containers and taskAttempts, along with the nodes they are assigned to.\n+   */\n+  @VisibleForTesting\n+  static final class EntityTracker {\n+    @VisibleForTesting\n+    final ConcurrentMap<TezTaskAttemptID, LlapNodeId> attemptToNodeMap = new ConcurrentHashMap<>();\n+    @VisibleForTesting\n+    final ConcurrentMap<ContainerId, LlapNodeId> containerToNodeMap = new ConcurrentHashMap<>();\n+    @VisibleForTesting\n+    final ConcurrentMap<LlapNodeId, BiMap<ContainerId, TezTaskAttemptID>> nodeMap = new ConcurrentHashMap<>();\n \n     void registerTaskAttempt(ContainerId containerId, TezTaskAttemptID taskAttemptId, String host, int port) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Registering \" + containerId + \", \" + taskAttemptId + \" for node: \" + host + \":\" + port);\n       }\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(host, port);\n       attemptToNodeMap.putIfAbsent(taskAttemptId, llapNodeId);\n+\n+      registerContainer(containerId, host, port);\n+\n+      // nodeMap registration.\n       BiMap<ContainerId, TezTaskAttemptID> tmpMap = HashBiMap.create();\n       BiMap<ContainerId, TezTaskAttemptID> old = nodeMap.putIfAbsent(llapNodeId, tmpMap);\n       BiMap<ContainerId, TezTaskAttemptID> usedInstance;\n@@ -538,10 +636,9 @@ void unregisterTaskAttempt(TezTaskAttemptID attemptId) {\n         synchronized(bMap) {\n           matched = bMap.inverse().remove(attemptId);\n         }\n-      }\n-      // Removing here. Registration into the map has to make sure to put\n-      if (bMap.isEmpty()) {\n-        nodeMap.remove(llapNodeId);\n+        if (bMap.isEmpty()) {\n+          nodeMap.remove(llapNodeId);\n+        }\n       }\n \n       // Remove the container mapping\n@@ -552,23 +649,29 @@ void unregisterTaskAttempt(TezTaskAttemptID attemptId) {\n     }\n \n     void registerContainer(ContainerId containerId, String hostname, int port) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Registering \" + containerId + \" for node: \" + hostname + \":\" + port);\n+      }\n       containerToNodeMap.putIfAbsent(containerId, LlapNodeId.getInstance(hostname, port));\n+      // nodeMap registration is not required, since there's no taskId association.\n     }\n \n     LlapNodeId getNodeIdForContainer(ContainerId containerId) {\n       return containerToNodeMap.get(containerId);\n     }\n \n-    LlapNodeId getNodeIfForTaskAttempt(TezTaskAttemptID taskAttemptId) {\n+    LlapNodeId getNodeIdForTaskAttempt(TezTaskAttemptID taskAttemptId) {\n       return attemptToNodeMap.get(taskAttemptId);\n     }\n \n     ContainerId getContainerIdForAttempt(TezTaskAttemptID taskAttemptId) {\n-      LlapNodeId llapNodeId = getNodeIfForTaskAttempt(taskAttemptId);\n+      LlapNodeId llapNodeId = getNodeIdForTaskAttempt(taskAttemptId);\n       if (llapNodeId != null) {\n         BiMap<TezTaskAttemptID, ContainerId> bMap = nodeMap.get(llapNodeId).inverse();\n         if (bMap != null) {\n-          return bMap.get(taskAttemptId);\n+          synchronized (bMap) {\n+            return bMap.get(taskAttemptId);\n+          }\n         } else {\n           return null;\n         }\n@@ -582,7 +685,9 @@ TezTaskAttemptID getTaskAttemptIdForContainer(ContainerId containerId) {\n       if (llapNodeId != null) {\n         BiMap<ContainerId, TezTaskAttemptID> bMap = nodeMap.get(llapNodeId);\n         if (bMap != null) {\n-          return bMap.get(containerId);\n+          synchronized (bMap) {\n+            return bMap.get(containerId);\n+          }\n         } else {\n           return null;\n         }\n@@ -604,10 +709,9 @@ void unregisterContainer(ContainerId containerId) {\n         synchronized(bMap) {\n           matched = bMap.remove(containerId);\n         }\n-      }\n-      // Removing here. Registration into the map has to make sure to put\n-      if (bMap.isEmpty()) {\n-        nodeMap.remove(llapNodeId);\n+        if (bMap.isEmpty()) {\n+          nodeMap.remove(llapNodeId);\n+        }\n       }\n \n       // Remove the container mapping\n@@ -616,25 +720,20 @@ void unregisterContainer(ContainerId containerId) {\n       }\n     }\n \n-    private final AtomicLong nodeNotFoundLogTime = new AtomicLong(0);\n-    void nodePinged(String hostname, int port) {\n-      LlapNodeId nodeId = LlapNodeId.getInstance(hostname, port);\n-      BiMap<ContainerId, TezTaskAttemptID> biMap = nodeMap.get(nodeId);\n-      if (biMap != null) {\n-        synchronized(biMap) {\n-          for (Map.Entry<ContainerId, TezTaskAttemptID> entry : biMap.entrySet()) {\n-            getContext().taskAlive(entry.getValue());\n-            getContext().containerAlive(entry.getKey());\n-          }\n-        }\n-      } else {\n-        if (System.currentTimeMillis() > nodeNotFoundLogTime.get() + 5000l) {\n-          LOG.warn(\"Received ping from unknown node: \" + hostname + \":\" + port +\n-              \". Could be caused by pre-emption by the AM,\" +\n-              \" or a mismatched hostname. Enable debug logging for mismatched host names\");\n-          nodeNotFoundLogTime.set(System.currentTimeMillis());\n-        }\n-      }\n+    /**\n+     * Return a {@link BiMap} containing container->taskAttemptId mapping for the host specified.\n+     * </p>\n+     * <p/>\n+     * This method return the internal structure used by the EntityTracker. Users must synchronize\n+     * on the structure to ensure correct usage.\n+     *\n+     * @param llapNodeId\n+     * @return\n+     */\n+    BiMap<ContainerId, TezTaskAttemptID> getContainerAttemptMapForNode(LlapNodeId llapNodeId) {\n+      BiMap<ContainerId, TezTaskAttemptID> biMap = nodeMap.get(llapNodeId);\n+      return biMap;\n     }\n+\n   }\n-}\n\\ No newline at end of file\n+}", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java", "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java", "previous_filename": "llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestTaskCommunicator.java", "sha": "a6af8c2405c9ead687ac5ddc5531f55a7c6c258d", "changes": 18, "status": "renamed", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b", "patch": "@@ -28,7 +28,7 @@\n import org.apache.hadoop.hive.llap.LlapNodeId;\n import org.junit.Test;\n \n-public class TestTaskCommunicator {\n+public class TestLlapDaemonProtocolClientProxy {\n \n   @Test (timeout = 5000)\n   public void testMultipleNodes() {\n@@ -38,8 +38,8 @@ public void testMultipleNodes() {\n     LlapNodeId nodeId2 = LlapNodeId.getInstance(\"host2\", 1025);\n \n     Message mockMessage = mock(Message.class);\n-    TaskCommunicator.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n-        TaskCommunicator.ExecuteRequestCallback.class);\n+    LlapDaemonProtocolClientProxy.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n+        LlapDaemonProtocolClientProxy.ExecuteRequestCallback.class);\n \n     // Request two messages\n     requestManager.queueRequest(\n@@ -66,8 +66,8 @@ public void testSingleInvocationPerNode() {\n     LlapNodeId nodeId1 = LlapNodeId.getInstance(\"host1\", 1025);\n \n     Message mockMessage = mock(Message.class);\n-    TaskCommunicator.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n-        TaskCommunicator.ExecuteRequestCallback.class);\n+    LlapDaemonProtocolClientProxy.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n+        LlapDaemonProtocolClientProxy.ExecuteRequestCallback.class);\n \n     // First request for host.\n     requestManager.queueRequest(\n@@ -101,7 +101,7 @@ public void testSingleInvocationPerNode() {\n   }\n \n \n-  static class RequestManagerForTest extends TaskCommunicator.RequestManager {\n+  static class RequestManagerForTest extends LlapDaemonProtocolClientProxy.RequestManager {\n \n     int numSubmissionsCounters = 0;\n     private Map<LlapNodeId, MutableInt> numInvocationsPerNode = new HashMap<>();\n@@ -110,7 +110,7 @@ public RequestManagerForTest(int numThreads) {\n       super(numThreads);\n     }\n \n-    protected void submitToExecutor(TaskCommunicator.CallableRequest request, LlapNodeId nodeId) {\n+    protected void submitToExecutor(LlapDaemonProtocolClientProxy.CallableRequest request, LlapNodeId nodeId) {\n       numSubmissionsCounters++;\n       MutableInt nodeCount = numInvocationsPerNode.get(nodeId);\n       if (nodeCount == null) {\n@@ -127,10 +127,10 @@ void reset() {\n \n   }\n \n-  static class CallableRequestForTest extends TaskCommunicator.CallableRequest<Message, Message> {\n+  static class CallableRequestForTest extends LlapDaemonProtocolClientProxy.CallableRequest<Message, Message> {\n \n     protected CallableRequestForTest(LlapNodeId nodeId, Message message,\n-                                     TaskCommunicator.ExecuteRequestCallback<Message> callback) {\n+                                     LlapDaemonProtocolClientProxy.ExecuteRequestCallback<Message> callback) {\n       super(nodeId, message, callback);\n     }\n ", "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java"}, {"additions": 100, "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java", "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java", "sha": "8f3d10474b95fa9f609272ba59356979f9e5642c", "changes": 100, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b", "patch": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ *  you may not use this file except in compliance with the License.\n+ *  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.llap.tezplugins;\n+\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+\n+import org.apache.hadoop.hive.llap.LlapNodeId;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.tez.dag.records.TezTaskAttemptID;\n+import org.junit.Test;\n+\n+public class TestLlapTaskCommunicator {\n+\n+  @Test (timeout = 5000)\n+  public void testEntityTracker1() {\n+    LlapTaskCommunicator.EntityTracker entityTracker = new LlapTaskCommunicator.EntityTracker();\n+\n+    String host1 = \"host1\";\n+    String host2 = \"host2\";\n+    String host3 = \"host3\";\n+    int port = 1451;\n+\n+\n+    // Simple container registration and un-registration without any task attempt being involved.\n+    ContainerId containerId101 = constructContainerId(101);\n+    entityTracker.registerContainer(containerId101, host1, port);\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForContainer(containerId101));\n+\n+    entityTracker.unregisterContainer(containerId101);\n+    assertNull(entityTracker.getContainerAttemptMapForNode(LlapNodeId.getInstance(host1, port)));\n+    assertNull(entityTracker.getNodeIdForContainer(containerId101));\n+    assertEquals(0, entityTracker.nodeMap.size());\n+    assertEquals(0, entityTracker.attemptToNodeMap.size());\n+    assertEquals(0, entityTracker.containerToNodeMap.size());\n+\n+\n+    // Simple task registration and un-registration.\n+    ContainerId containerId1 = constructContainerId(1);\n+    TezTaskAttemptID taskAttemptId1 = constructTaskAttemptId(1);\n+    entityTracker.registerTaskAttempt(containerId1, taskAttemptId1, host1, port);\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForContainer(containerId1));\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForTaskAttempt(taskAttemptId1));\n+\n+    entityTracker.unregisterTaskAttempt(taskAttemptId1);\n+    assertNull(entityTracker.getContainerAttemptMapForNode(LlapNodeId.getInstance(host1, port)));\n+    assertNull(entityTracker.getNodeIdForContainer(containerId1));\n+    assertNull(entityTracker.getNodeIdForTaskAttempt(taskAttemptId1));\n+    assertEquals(0, entityTracker.nodeMap.size());\n+    assertEquals(0, entityTracker.attemptToNodeMap.size());\n+    assertEquals(0, entityTracker.containerToNodeMap.size());\n+\n+    // Register taskAttempt, unregister container. TaskAttempt should also be unregistered\n+    ContainerId containerId201 = constructContainerId(201);\n+    TezTaskAttemptID taskAttemptId201 = constructTaskAttemptId(201);\n+    entityTracker.registerTaskAttempt(containerId201, taskAttemptId201, host1, port);\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForContainer(containerId201));\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForTaskAttempt(taskAttemptId201));\n+\n+    entityTracker.unregisterContainer(containerId201);\n+    assertNull(entityTracker.getContainerAttemptMapForNode(LlapNodeId.getInstance(host1, port)));\n+    assertNull(entityTracker.getNodeIdForContainer(containerId201));\n+    assertNull(entityTracker.getNodeIdForTaskAttempt(taskAttemptId201));\n+    assertEquals(0, entityTracker.nodeMap.size());\n+    assertEquals(0, entityTracker.attemptToNodeMap.size());\n+    assertEquals(0, entityTracker.containerToNodeMap.size());\n+\n+    entityTracker.unregisterTaskAttempt(taskAttemptId201); // No errors\n+  }\n+\n+\n+  private ContainerId constructContainerId(int id) {\n+    ContainerId containerId = mock(ContainerId.class);\n+    doReturn(id).when(containerId).getId();\n+    doReturn((long)id).when(containerId).getContainerId();\n+    return containerId;\n+  }\n+\n+  private TezTaskAttemptID constructTaskAttemptId(int id) {\n+    TezTaskAttemptID taskAttemptId = mock(TezTaskAttemptID.class);\n+    doReturn(id).when(taskAttemptId).getId();\n+    return taskAttemptId;\n+  }\n+\n+}", "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/76a3d5e37acfed87ce5eb153616579acee9a8852", "parent": "https://github.com/apache/hive/commit/368bcaead844983c0726b81ff391009623e26e15", "message": "HIVE-13932:Hive SMB Map Join with small set of LIMIT failed with NPE (Yongzhi Chen, reviewed by Chaoyu Tang)", "bug_id": "hive_102", "file": [{"additions": 99, "raw_url": "https://github.com/apache/hive/raw/76a3d5e37acfed87ce5eb153616579acee9a8852/data/files/smbdata.txt", "blob_url": "https://github.com/apache/hive/blob/76a3d5e37acfed87ce5eb153616579acee9a8852/data/files/smbdata.txt", "sha": "4422a548fbd8bfbdd42a06a267c1923ce3c6ef6a", "changes": 99, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/smbdata.txt?ref=76a3d5e37acfed87ce5eb153616579acee9a8852", "patch": "@@ -0,0 +1,99 @@\n+1\n+2\n+3\n+4\n+5\n+6\n+7\n+8\n+9\n+10\n+11\n+12\n+13\n+14\n+15\n+16\n+17\n+18\n+19\n+20\n+21\n+22\n+23\n+24\n+25\n+26\n+27\n+28\n+29\n+30\n+31\n+32\n+33\n+34\n+35\n+36\n+37\n+38\n+39\n+40\n+41\n+42\n+43\n+44\n+45\n+46\n+47\n+48\n+49\n+50\n+51\n+52\n+53\n+54\n+55\n+56\n+57\n+58\n+59\n+60\n+61\n+62\n+63\n+64\n+65\n+66\n+67\n+68\n+69\n+70\n+71\n+72\n+73\n+74\n+75\n+76\n+77\n+78\n+79\n+80\n+81\n+82\n+83\n+84\n+85\n+86\n+87\n+88\n+89\n+90\n+91\n+92\n+93\n+94\n+95\n+96\n+97\n+98\n+99", "filename": "data/files/smbdata.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/76a3d5e37acfed87ce5eb153616579acee9a8852/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/76a3d5e37acfed87ce5eb153616579acee9a8852/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java", "sha": "27b16735500697e6d1883e5763cdb9cad1ec948c", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java?ref=76a3d5e37acfed87ce5eb153616579acee9a8852", "patch": "@@ -350,7 +350,7 @@ private void joinFinalLeftData() throws HiveException {\n       joinOneGroup();\n       dataInCache = false;\n       for (byte pos = 0; pos < order.length; pos++) {\n-        if (this.candidateStorage[pos].hasRows()) {\n+        if (this.candidateStorage[pos] != null && this.candidateStorage[pos].hasRows()) {\n           dataInCache = true;\n           break;\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/76a3d5e37acfed87ce5eb153616579acee9a8852/ql/src/test/queries/clientpositive/smblimit.q", "blob_url": "https://github.com/apache/hive/blob/76a3d5e37acfed87ce5eb153616579acee9a8852/ql/src/test/queries/clientpositive/smblimit.q", "sha": "1db21675223e42606b209dd49b783610a2cc33d6", "changes": 12, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/smblimit.q?ref=76a3d5e37acfed87ce5eb153616579acee9a8852", "patch": "@@ -0,0 +1,12 @@\n+drop table if exists hlp1;\n+drop table if exists btl;\n+create table hlp1(c string);\n+load data local inpath '../../data/files/smbdata.txt' into table hlp1;\n+create table btl(c string) clustered by (c) sorted by (c) into 5 buckets;\n+insert overwrite table btl select * from hlp1;\n+SET hive.auto.convert.sortmerge.join = true;\n+SET hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;\n+SET hive.optimize.bucketmapjoin = true;\n+SET hive.optimize.bucketmapjoin.sortedmerge = true;\n+select 1 from btl join btl t1 on btl.c=t1.c limit 1;\n+", "filename": "ql/src/test/queries/clientpositive/smblimit.q"}, {"additions": 50, "raw_url": "https://github.com/apache/hive/raw/76a3d5e37acfed87ce5eb153616579acee9a8852/ql/src/test/results/clientpositive/smblimit.q.out", "blob_url": "https://github.com/apache/hive/blob/76a3d5e37acfed87ce5eb153616579acee9a8852/ql/src/test/results/clientpositive/smblimit.q.out", "sha": "64ca6041aec435f411b03773667c9d90ebde0265", "changes": 50, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smblimit.q.out?ref=76a3d5e37acfed87ce5eb153616579acee9a8852", "patch": "@@ -0,0 +1,50 @@\n+PREHOOK: query: drop table if exists hlp1\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists hlp1\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: drop table if exists btl\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists btl\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table hlp1(c string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@hlp1\n+POSTHOOK: query: create table hlp1(c string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@hlp1\n+PREHOOK: query: load data local inpath '../../data/files/smbdata.txt' into table hlp1\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@hlp1\n+POSTHOOK: query: load data local inpath '../../data/files/smbdata.txt' into table hlp1\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@hlp1\n+PREHOOK: query: create table btl(c string) clustered by (c) sorted by (c) into 5 buckets\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@btl\n+POSTHOOK: query: create table btl(c string) clustered by (c) sorted by (c) into 5 buckets\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@btl\n+PREHOOK: query: insert overwrite table btl select * from hlp1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@hlp1\n+PREHOOK: Output: default@btl\n+POSTHOOK: query: insert overwrite table btl select * from hlp1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@hlp1\n+POSTHOOK: Output: default@btl\n+POSTHOOK: Lineage: btl.c SIMPLE [(hlp1)hlp1.FieldSchema(name:c, type:string, comment:null), ]\n+PREHOOK: query: select 1 from btl join btl t1 on btl.c=t1.c limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@btl\n+#### A masked pattern was here ####\n+POSTHOOK: query: select 1 from btl join btl t1 on btl.c=t1.c limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@btl\n+#### A masked pattern was here ####\n+1", "filename": "ql/src/test/results/clientpositive/smblimit.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/bef879d0a3e1827bffbd278a883e721124ee0eea", "parent": "https://github.com/apache/hive/commit/1d02ab578dbd47103a70710abd4d949ea8cea9d2", "message": "HIVE-12557 : NPE while removing entry in LRFU cache (Sergey Shelukhin, reviewed by Prasanth Jayachandran)", "bug_id": "hive_103", "file": [{"additions": 60, "raw_url": "https://github.com/apache/hive/raw/bef879d0a3e1827bffbd278a883e721124ee0eea/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java", "blob_url": "https://github.com/apache/hive/blob/bef879d0a3e1827bffbd278a883e721124ee0eea/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java", "sha": "40cb92d24550489bb9df653e179700594fcb9679", "changes": 75, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java?ref=bef879d0a3e1827bffbd278a883e721124ee0eea", "patch": "@@ -140,7 +140,9 @@ public void notifyUnlock(LlapCacheableBuffer buffer) {\n       } else if (heapSize == heap.length) {\n         // The buffer is not in the (full) heap. Demote the top item of the heap into the list.\n         LlapCacheableBuffer demoted = heap[0];\n-        synchronized (listLock) {\n+        listLock.lock();\n+        try {\n+          assert demoted.indexInHeap == 0; // Noone could have moved it, we have the heap lock.\n           demoted.indexInHeap = LlapCacheableBuffer.IN_LIST;\n           demoted.prev = null;\n           if (listHead != null) {\n@@ -151,6 +153,8 @@ public void notifyUnlock(LlapCacheableBuffer buffer) {\n             listHead = listTail = demoted;\n             demoted.next = null;\n           }\n+        } finally {\n+          listLock.unlock();\n         }\n         // Now insert the buffer in its place and restore heap property.\n         buffer.indexInHeap = 0;\n@@ -340,44 +344,62 @@ private void removeFromListAndUnlock(LlapCacheableBuffer buffer) {\n   }\n \n   private void removeFromListUnderLock(LlapCacheableBuffer buffer) {\n-    if (buffer == listTail) {\n+    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n+    boolean isTail = buffer == listTail, isHead = buffer == listHead;\n+    if ((isTail != (buffer.next == null)) || (isHead != (buffer.prev == null))) {\n+      debugDumpListOnError(buffer);\n+      throw new AssertionError(\"LRFU list is corrupted.\");\n+    }\n+    if (isTail) {\n       listTail = buffer.prev;\n     } else {\n       buffer.next.prev = buffer.prev;\n     }\n-    if (buffer == listHead) {\n+    if (isHead) {\n       listHead = buffer.next;\n     } else {\n       buffer.prev.next = buffer.next;\n     }\n-    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n   }\n \n   private void removeFromListUnderLockNoStateUpdate(\n       LlapCacheableBuffer from, LlapCacheableBuffer to) {\n-    if (to == listTail) {\n+    boolean isToTail = to == listTail, isFromHead = from == listHead;\n+    if ((isToTail != (to.next == null)) || (isFromHead != (from.prev == null))) {\n+      debugDumpListOnError(from, to);\n+      throw new AssertionError(\"LRFU list is corrupted.\");\n+    }\n+    if (isToTail) {\n       listTail = from.prev;\n     } else {\n       to.next.prev = from.prev;\n     }\n-    if (from == listHead) {\n+    if (isFromHead) {\n       listHead = to.next;\n     } else {\n       from.prev.next = to.next;\n     }\n   }\n \n-  public String debugDumpHeap() {\n-    StringBuilder result = new StringBuilder(\"List: \");\n-    if (listHead == null) {\n-      result.append(\"<empty>\");\n-    } else {\n-      LlapCacheableBuffer listItem = listHead;\n-      while (listItem != null) {\n-        result.append(listItem.toStringForCache()).append(\" -> \");\n-        listItem = listItem.next;\n+  private void debugDumpListOnError(LlapCacheableBuffer... buffers) {\n+    // Hopefully this will be helpful in case of NPEs.\n+    StringBuilder listDump = new StringBuilder(\"Invalid list removal. List: \");\n+    try {\n+      dumpList(listDump, listHead, listTail);\n+      int i = 0;\n+      for (LlapCacheableBuffer buffer : buffers) {\n+        listDump.append(\"; list from the buffer #\").append(i).append(\" being removed: \");\n+        dumpList(listDump, buffer, null);\n       }\n+    } catch (Throwable t) {\n+      LlapIoImpl.LOG.error(\"Error dumping the lists on error\", t);\n     }\n+    LlapIoImpl.LOG.error(listDump.toString());\n+  }\n+\n+  public String debugDumpHeap() {\n+    StringBuilder result = new StringBuilder(\"List: \");\n+    dumpList(result, listHead, listTail);\n     result.append(\"\\nHeap:\");\n     if (heapSize == 0) {\n       result.append(\" <empty>\\n\");\n@@ -421,6 +443,29 @@ public String debugDumpHeap() {\n     return result.toString();\n   }\n \n+  private static void dumpList(StringBuilder result,\n+      LlapCacheableBuffer listHeadLocal, LlapCacheableBuffer listTailLocal) {\n+    if (listHeadLocal == null) {\n+      result.append(\"<empty>\");\n+      return;\n+    }\n+    LlapCacheableBuffer listItem = listHeadLocal;\n+    while (listItem.prev != null) {\n+      listItem = listItem.prev;  // To detect incorrect lists.\n+    }\n+    while (listItem != null) {\n+      result.append(listItem.toStringForCache());\n+      if (listItem == listTailLocal) {\n+        result.append(\"(tail)\"); // To detect incorrect lists.\n+      }\n+      if (listItem == listHeadLocal) {\n+        result.append(\"(head)\"); // To detect incorrect lists.\n+      }\n+      result.append(\" -> \");\n+      listItem = listItem.next;\n+    }\n+  }\n+\n   @Override\n   public String debugDumpForOom() {\n     String result = debugDumpHeap();", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1d02ab578dbd47103a70710abd4d949ea8cea9d2", "parent": "https://github.com/apache/hive/commit/a603ed8d73e7f2bf4588a67f1b40709cc54fcbfe", "message": "HIVE-12532 : LLAP Cache: Uncompressed data cache has NPE (Sergey Shelukhin, reviewed by Prasanth Jayachandran)", "bug_id": "hive_104", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/1d02ab578dbd47103a70710abd4d949ea8cea9d2/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/1d02ab578dbd47103a70710abd4d949ea8cea9d2/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java", "sha": "a8b51b92e0cab002b83a7538d4304d8ccfa65f12", "changes": 9, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=1d02ab578dbd47103a70710abd4d949ea8cea9d2", "patch": "@@ -778,19 +778,18 @@ private DiskRangeList preReadUncompressedStream(long baseOffset,\n     }\n     // Account for maximum cache buffer size.\n     long streamLen = streamEnd - streamOffset;\n-    int partSize = determineUncompressedPartSize(), //\n+    int partSize = determineUncompressedPartSize(),\n         partCount = (int)(streamLen / partSize) + (((streamLen % partSize) != 0) ? 1 : 0);\n \n     CacheChunk lastUncompressed = null;\n     MemoryBuffer[] singleAlloc = new MemoryBuffer[1];\n-    /*\n-Starting pre-read for [12187411,17107411) at start: 12187411 end: 12449555 cache buffer: 0x5f64a8f6(2)\n-Processing uncompressed file data at [12187411, 12449555)\n-  */\n     for (int i = 0; i < partCount; ++i) {\n       long partOffset = streamOffset + (i * partSize),\n            partEnd = Math.min(partOffset + partSize, streamEnd);\n       long hasEntirePartTo = partOffset; // We have 0 bytes of data for this part, for now.\n+      if (current == null) {\n+        break; // We have no data from this point on (could be unneeded), skip.\n+      }\n       assert partOffset <= current.getOffset();\n       if (partOffset == current.getOffset() && current instanceof CacheChunk) {\n         // We assume cache chunks would always match the way we read, so check and skip it.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/60cb16bb1c79de7481f42735efdb86b908aa1526", "parent": "https://github.com/apache/hive/commit/0f4065e51f64db008c1908b4e63b8828d88ef3fb", "message": "HIVE-12476: Metastore NPE on Oracle with Direct SQL (Jason Dere, reviewed by Sushanth Sowmyan)", "bug_id": "hive_105", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/60cb16bb1c79de7481f42735efdb86b908aa1526/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java", "blob_url": "https://github.com/apache/hive/blob/60cb16bb1c79de7481f42735efdb86b908aa1526/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java", "sha": "d76e77fe0aab8d63b22bf4cbd98a0a8946af1787", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java?ref=60cb16bb1c79de7481f42735efdb86b908aa1526", "patch": "@@ -646,6 +646,10 @@ private boolean isViewTable(String dbName, String tblName) throws MetaException\n       public void apply(Partition t, Object[] fields) {\n         t.putToParameters((String)fields[1], (String)fields[2]);\n       }});\n+    // Perform conversion of null map values\n+    for (Partition t : partitions.values()) {\n+      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n+    }\n \n     queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n         + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n@@ -673,6 +677,10 @@ public void apply(Partition t, Object[] fields) {\n       public void apply(StorageDescriptor t, Object[] fields) {\n         t.putToParameters((String)fields[1], (String)fields[2]);\n       }});\n+    // Perform conversion of null map values\n+    for (StorageDescriptor t : sds.values()) {\n+      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n+    }\n \n     queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n         + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n@@ -810,6 +818,10 @@ public void apply(List<FieldSchema> t, Object[] fields) {\n       public void apply(SerDeInfo t, Object[] fields) {\n         t.putToParameters((String)fields[1], (String)fields[2]);\n       }});\n+    // Perform conversion of null map values\n+    for (SerDeInfo t : serdes.values()) {\n+      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n+    }\n \n     return orderedResult;\n   }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cf1d1604b902a925f0094fc04c16d37c89e276ba", "parent": "https://github.com/apache/hive/commit/a3c02d0d6b155a3cc308ed7e60aa3cfdc22b4ea3", "message": "HIVE-12196 NPE when converting bad timestamp value (Aihua Xu, reviewed by Chaoyu Tang)", "bug_id": "hive_106", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/cf1d1604b902a925f0094fc04c16d37c89e276ba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java", "blob_url": "https://github.com/apache/hive/blob/cf1d1604b902a925f0094fc04c16d37c89e276ba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java", "sha": "331ee6b69ca0ede683f66f868e9a2907d8d20842", "changes": 11, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java?ref=cf1d1604b902a925f0094fc04c16d37c89e276ba", "patch": "@@ -77,12 +77,15 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {\n       return null;\n     }\n \n-    String tzStr = textConverter.convert(o1).toString();\n-    TimeZone timezone = TimeZone.getTimeZone(tzStr);\n+    Object converted_o0 = timestampConverter.convert(o0);\n+    if (converted_o0 == null) {\n+      return null;\n+    }\n \n-    Timestamp timestamp = ((TimestampWritable) timestampConverter.convert(o0))\n-        .getTimestamp();\n+    Timestamp timestamp = ((TimestampWritable) converted_o0).getTimestamp();\n \n+    String tzStr = textConverter.convert(o1).toString();\n+    TimeZone timezone = TimeZone.getTimeZone(tzStr);\n     int offset = timezone.getOffset(timestamp.getTime());\n     if (invert()) {\n       offset = -offset;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/cf1d1604b902a925f0094fc04c16d37c89e276ba/ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q", "blob_url": "https://github.com/apache/hive/blob/cf1d1604b902a925f0094fc04c16d37c89e276ba/ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q", "sha": "ca0a6a80c876d17753a3bd39502c59703fbd0cd2", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q?ref=cf1d1604b902a925f0094fc04c16d37c89e276ba", "patch": "@@ -22,3 +22,8 @@ from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), ''),\n from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), '---'),\n from_utc_timestamp(cast(null as timestamp), 'PST'),\n from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), cast(null as string));\n+\n+select\n+from_utc_timestamp('2012-02-11-04:30:00', 'UTC'),\n+from_utc_timestamp('2012-02-11-04:30:00', 'PST');\n+", "filename": "ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/cf1d1604b902a925f0094fc04c16d37c89e276ba/ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out", "blob_url": "https://github.com/apache/hive/blob/cf1d1604b902a925f0094fc04c16d37c89e276ba/ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out", "sha": "be2cb1ad8730e8884261fa1c37441f4369b393a3", "changes": 13, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out?ref=cf1d1604b902a925f0094fc04c16d37c89e276ba", "patch": "@@ -80,3 +80,16 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 2012-02-10 20:30:00\t2012-02-11 08:30:00\t2012-02-11 12:30:00\t2012-02-11 04:30:00\t2012-02-11 04:30:00\t2012-02-11 04:30:00\tNULL\tNULL\n+PREHOOK: query: select\n+from_utc_timestamp('2012-02-11-04:30:00', 'UTC'),\n+from_utc_timestamp('2012-02-11-04:30:00', 'PST')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: select\n+from_utc_timestamp('2012-02-11-04:30:00', 'UTC'),\n+from_utc_timestamp('2012-02-11-04:30:00', 'PST')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+NULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "parent": "https://github.com/apache/hive/commit/64c96e1e92a490a069f9cc924b2ec476187f98ea", "message": "HIVE-13621: compute stats in certain cases fails with NPE (Vikram Dixit K, Pengcheng Xiong, reviewed by Gunther Hagleitner)", "bug_id": "hive_107", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/itests/src/test/resources/testconfiguration.properties", "sha": "c891d40cbfba32e4c5035cd514dadf9d4f519223", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "patch": "@@ -70,6 +70,7 @@ disabled.query.files=ql_rewrite_gbtoidx.q,\\\n   smb_mapjoin_8.q\n \n minitez.query.files.shared=acid_globallimit.q,\\\n+  deleteAnalyze.q,\\\n   empty_join.q,\\\n   alter_merge_2_orc.q,\\\n   alter_merge_orc.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java", "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java", "sha": "d1cff0691559ad6f4f340eb6a0aff9e80b4d77af", "changes": 27, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "patch": "@@ -1205,17 +1205,22 @@ static StorageDescriptorParts deserializeTable(String dbName, String tableName,\n       if (decimalData.isSetBitVectors()) {\n         builder.setBitVectors(decimalData.getBitVectors());\n       }\n-      builder.setDecimalStats(\n-          HbaseMetastoreProto.ColumnStats.DecimalStats\n-              .newBuilder()\n-              .setLowValue(\n-                  HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n-                      .setUnscaled(ByteString.copyFrom(decimalData.getLowValue().getUnscaled()))\n-                      .setScale(decimalData.getLowValue().getScale()).build())\n-              .setHighValue(\n-                  HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n-                      .setUnscaled(ByteString.copyFrom(decimalData.getHighValue().getUnscaled()))\n-                      .setScale(decimalData.getHighValue().getScale()).build())).build();\n+      if (decimalData.getLowValue() != null && decimalData.getHighValue() != null) {\n+        builder.setDecimalStats(\n+            HbaseMetastoreProto.ColumnStats.DecimalStats\n+                .newBuilder()\n+                .setLowValue(\n+                    HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n+                        .setUnscaled(ByteString.copyFrom(decimalData.getLowValue().getUnscaled()))\n+                        .setScale(decimalData.getLowValue().getScale()).build())\n+                .setHighValue(\n+                    HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n+                        .setUnscaled(ByteString.copyFrom(decimalData.getHighValue().getUnscaled()))\n+                        .setScale(decimalData.getHighValue().getScale()).build())).build();\n+      } else {\n+        builder.setDecimalStats(HbaseMetastoreProto.ColumnStats.DecimalStats.newBuilder().clear()\n+            .build());\n+      }\n       break;\n \n     default:", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "sha": "636f079471a7f92eef99ee94a156f0b1946cd25c", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "patch": "@@ -412,7 +412,7 @@ private void completeInitialization(Collection<Future<?>> fs) throws HiveExcepti\n   }\n \n   /**\n-   * This metod can be used to retrieve the results from async operations\n+   * This method can be used to retrieve the results from async operations\n    * started at init time - before the operator pipeline is started.\n    *\n    * @param os", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "sha": "3944e10d38836d6fd5dd65b12468147d6574a106", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "patch": "@@ -1792,6 +1792,7 @@ private long computeNewRowCount(List<Long> rowCountParents, long denom) {\n         }\n       }\n \n+      denom = denom == 0 ? 1 : denom;\n       factor = (double) max / (double) denom;\n \n       for (int i = 0; i < rowCountParents.size(); i++) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java"}, {"additions": 31, "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/queries/clientpositive/deleteAnalyze.q", "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/queries/clientpositive/deleteAnalyze.q", "sha": "7e5371cc6154c235dfada3ba419f0b838d24434c", "changes": 31, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/deleteAnalyze.q?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "patch": "@@ -0,0 +1,31 @@\n+set hive.stats.autogather=true;\n+\n+dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/testdeci2;\n+\n+create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+stored as orc location '${system:test.tmp.dir}/testdeci2';\n+\n+insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2');\n+\n+describe formatted testdeci2;\n+\n+dfs -rmr ${system:test.tmp.dir}/testdeci2/000000_0;\n+\n+describe formatted testdeci2 amount;\n+\n+analyze table testdeci2 compute statistics for columns;\n+\n+set hive.stats.fetch.column.stats=true;\n+\n+analyze table testdeci2 compute statistics for columns;\n+\n+explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2;", "filename": "ql/src/test/queries/clientpositive/deleteAnalyze.q"}, {"additions": 173, "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/deleteAnalyze.q.out", "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/deleteAnalyze.q.out", "sha": "7b9391b36f0371b5c2639973091c2c27efef6192", "changes": 173, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/deleteAnalyze.q.out?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "patch": "@@ -0,0 +1,173 @@\n+PREHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@testdeci2\n+PREHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@testdeci2\n+POSTHOOK: Lineage: testdeci2.amount EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.id EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.item SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col4, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.sales_tax EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]\n+PREHOOK: query: describe formatted testdeci2\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tcomment             \n+\t \t \n+id                  \tint                 \t                    \n+amount              \tdecimal(10,3)       \t                    \n+sales_tax           \tdecimal(10,3)       \t                    \n+item                \tstring              \t                    \n+\t \t \n+# Detailed Table Information\t \t \n+Database:           \tdefault             \t \n+#### A masked pattern was here ####\n+Retention:          \t0                   \t \n+#### A masked pattern was here ####\n+Table Type:         \tMANAGED_TABLE       \t \n+Table Parameters:\t \t \n+\tnumFiles            \t1                   \n+\ttotalSize           \t578                 \n+#### A masked pattern was here ####\n+\t \t \n+# Storage Information\t \t \n+SerDe Library:      \torg.apache.hadoop.hive.ql.io.orc.OrcSerde\t \n+InputFormat:        \torg.apache.hadoop.hive.ql.io.orc.OrcInputFormat\t \n+OutputFormat:       \torg.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\t \n+Compressed:         \tNo                  \t \n+Num Buckets:        \t-1                  \t \n+Bucket Columns:     \t[]                  \t \n+Sort Columns:       \t[]                  \t \n+Storage Desc Params:\t \t \n+\tserialization.format\t1                   \n+#### A masked pattern was here ####\n+PREHOOK: query: describe formatted testdeci2 amount\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2 amount\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tmin                 \tmax                 \tnum_nulls           \tdistinct_count      \tavg_col_len         \tmax_col_len         \tnum_trues           \tnum_falses          \tcomment             \n+\t \t \t \t \t \t \t \t \t \t \n+amount              \tdecimal(10,3)       \t                    \t                    \t                    \t                    \t                    \t                    \t                    \t                    \tfrom deserializer   \n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+            Filter Operator\n+              predicate: item is not null (type: boolean)\n+              Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+              Select Operator\n+                expressions: id (type: int), item (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  key expressions: _col1 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col1 (type: string)\n+                  Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col0 (type: int)\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+            Filter Operator\n+              predicate: ((id = 2) and item is not null) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+              Select Operator\n+                expressions: amount (type: decimal(10,3)), sales_tax (type: decimal(10,3)), item (type: string)\n+                outputColumnNames: _col1, _col2, _col3\n+                Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  key expressions: _col3 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col3 (type: string)\n+                  Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col1 (type: decimal(10,3)), _col2 (type: decimal(10,3))\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          keys:\n+            0 _col1 (type: string)\n+            1 _col3 (type: string)\n+          outputColumnNames: _col0, _col3, _col4\n+          Statistics: Num rows: 5 Data size: 1140 Basic stats: COMPLETE Column stats: COMPLETE\n+          Select Operator\n+            expressions: _col0 (type: int), COALESCE(_col3,0) (type: decimal(13,3)), COALESCE(_col4,0) (type: decimal(13,3))\n+            outputColumnNames: _col0, _col1, _col2\n+            Statistics: Num rows: 5 Data size: 1140 Basic stats: COMPLETE Column stats: COMPLETE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 5 Data size: 1140 Basic stats: COMPLETE Column stats: COMPLETE\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+", "filename": "ql/src/test/results/clientpositive/deleteAnalyze.q.out"}, {"additions": 140, "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out", "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out", "sha": "47f2a201b078d95988f82730800ad86f6f945cef", "changes": 140, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac", "patch": "@@ -0,0 +1,140 @@\n+PREHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@testdeci2\n+PREHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@testdeci2\n+POSTHOOK: Lineage: testdeci2.amount EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.id EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.item SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col4, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.sales_tax EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]\n+PREHOOK: query: describe formatted testdeci2\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tcomment             \n+\t \t \n+id                  \tint                 \t                    \n+amount              \tdecimal(10,3)       \t                    \n+sales_tax           \tdecimal(10,3)       \t                    \n+item                \tstring              \t                    \n+\t \t \n+# Detailed Table Information\t \t \n+Database:           \tdefault             \t \n+#### A masked pattern was here ####\n+Retention:          \t0                   \t \n+#### A masked pattern was here ####\n+Table Type:         \tMANAGED_TABLE       \t \n+Table Parameters:\t \t \n+\tnumFiles            \t1                   \n+\ttotalSize           \t578                 \n+#### A masked pattern was here ####\n+\t \t \n+# Storage Information\t \t \n+SerDe Library:      \torg.apache.hadoop.hive.ql.io.orc.OrcSerde\t \n+InputFormat:        \torg.apache.hadoop.hive.ql.io.orc.OrcInputFormat\t \n+OutputFormat:       \torg.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\t \n+Compressed:         \tNo                  \t \n+Num Buckets:        \t-1                  \t \n+Bucket Columns:     \t[]                  \t \n+Sort Columns:       \t[]                  \t \n+Storage Desc Params:\t \t \n+\tserialization.format\t1                   \n+#### A masked pattern was here ####\n+PREHOOK: query: describe formatted testdeci2 amount\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2 amount\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tmin                 \tmax                 \tnum_nulls           \tdistinct_count      \tavg_col_len         \tmax_col_len         \tnum_trues           \tnum_falses          \tcomment             \n+\t \t \t \t \t \t \t \t \t \t \n+amount              \tdecimal(10,3)       \t                    \t                    \t                    \t                    \t                    \t                    \t                    \t                    \tfrom deserializer   \n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+POSTHOOK: type: QUERY\n+Plan optimized by CBO.\n+\n+Vertex dependency in root stage\n+Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)\n+\n+Stage-0\n+  Fetch Operator\n+    limit:-1\n+    Stage-1\n+      Reducer 2\n+      File Output Operator [FS_10]\n+        Select Operator [SEL_9] (rows=5 width=228)\n+          Output:[\"_col0\",\"_col1\",\"_col2\"]\n+          Merge Join Operator [MERGEJOIN_15] (rows=5 width=228)\n+            Conds:RS_6._col1=RS_7._col3(Inner),Output:[\"_col0\",\"_col3\",\"_col4\"]\n+          <-Map 1 [SIMPLE_EDGE]\n+            SHUFFLE [RS_6]\n+              PartitionCols:_col1\n+              Select Operator [SEL_2] (rows=5 width=88)\n+                Output:[\"_col0\",\"_col1\"]\n+                Filter Operator [FIL_13] (rows=5 width=88)\n+                  predicate:item is not null\n+                  TableScan [TS_0] (rows=5 width=88)\n+                    default@testdeci2,s,Tbl:COMPLETE,Col:COMPLETE,Output:[\"id\",\"item\"]\n+          <-Map 3 [SIMPLE_EDGE]\n+            SHUFFLE [RS_7]\n+              PartitionCols:_col3\n+              Select Operator [SEL_5] (rows=1 width=312)\n+                Output:[\"_col1\",\"_col2\",\"_col3\"]\n+                Filter Operator [FIL_14] (rows=1 width=312)\n+                  predicate:((id = 2) and item is not null)\n+                  TableScan [TS_3] (rows=1 width=312)\n+                    default@testdeci2,s,Tbl:COMPLETE,Col:COMPLETE,Output:[\"id\",\"amount\",\"sales_tax\",\"item\"]\n+", "filename": "ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/02629e9794e228dcaa8d446423a256d75f71d6dd", "parent": "https://github.com/apache/hive/commit/47617d31f347a0ba78ebfc903738b39dd960b19b", "message": "HIVE-12202 NPE thrown when reading legacy ACID delta files(Elliot West via Eugene Koifman)", "bug_id": "hive_108", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/02629e9794e228dcaa8d446423a256d75f71d6dd/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/02629e9794e228dcaa8d446423a256d75f71d6dd/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "sha": "7c7074d156fb5f0ace46fb561e869235024d335a", "changes": 14, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java?ref=02629e9794e228dcaa8d446423a256d75f71d6dd", "patch": "@@ -33,7 +33,6 @@\n import java.io.DataOutput;\n import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Collections;\n import java.util.List;\n \n /**\n@@ -115,11 +114,14 @@\n     private List<Integer> stmtIds;\n     \n     public DeltaMetaData() {\n-      this(0,0,null);\n+      this(0,0,new ArrayList<Integer>());\n     }\n     DeltaMetaData(long minTxnId, long maxTxnId, List<Integer> stmtIds) {\n       this.minTxnId = minTxnId;\n       this.maxTxnId = maxTxnId;\n+      if (stmtIds == null) {\n+        throw new IllegalArgumentException(\"stmtIds == null\");\n+      }\n       this.stmtIds = stmtIds;\n     }\n     long getMinTxnId() {\n@@ -136,9 +138,6 @@ public void write(DataOutput out) throws IOException {\n       out.writeLong(minTxnId);\n       out.writeLong(maxTxnId);\n       out.writeInt(stmtIds.size());\n-      if(stmtIds == null) {\n-        return;\n-      }\n       for(Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n@@ -147,11 +146,8 @@ public void write(DataOutput out) throws IOException {\n     public void readFields(DataInput in) throws IOException {\n       minTxnId = in.readLong();\n       maxTxnId = in.readLong();\n+      stmtIds.clear();\n       int numStatements = in.readInt();\n-      if(numStatements <= 0) {\n-        return;\n-      }\n-      stmtIds = new ArrayList<>();\n       for(int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1ad1dc8581ae3454237a2b232767e73ec6b74a83", "parent": "https://github.com/apache/hive/commit/cdbd1c8517e70614ec9dfd0bfdc978b200a946c2", "message": "HIVE-12318 qtest failing due to NPE in logStats (Jimmmy Xiang via gates)", "bug_id": "hive_109", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/1ad1dc8581ae3454237a2b232767e73ec6b74a83/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "blob_url": "https://github.com/apache/hive/blob/1ad1dc8581ae3454237a2b232767e73ec6b74a83/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "sha": "9a86a35a2d28c348f9ee505530422f9709ae46a3", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java?ref=1ad1dc8581ae3454237a2b232767e73ec6b74a83", "patch": "@@ -896,7 +896,7 @@ public void logStats() {\n     if (isLogInfoEnabled && !statsMap.isEmpty()) {\n       StringBuilder sb = new StringBuilder();\n       for (Map.Entry<String, LongWritable> e : statsMap.entrySet()) {\n-        sb.append(e.getKey()).append(\":\").append(statsMap.get(e).toString()).append(\", \");\n+        sb.append(e.getKey()).append(\":\").append(e.getValue()).append(\", \");\n       }\n       LOG.info(sb.toString());\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0f3998af3c5e9d8168b32d5085165892c2942101", "parent": "https://github.com/apache/hive/commit/c587404d4d2f0995af822aa6871d4aadfbf6aeda", "message": "HIVE-14990 : run all tests for MM tables and fix the issues that are found - issue with FetchOperator, drop, NPE (Sergey Shelukhin)", "bug_id": "hive_110", "file": [{"additions": 20, "raw_url": "https://github.com/apache/hive/raw/0f3998af3c5e9d8168b32d5085165892c2942101/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "blob_url": "https://github.com/apache/hive/blob/0f3998af3c5e9d8168b32d5085165892c2942101/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "sha": "8ad70590037dcac358c83b28f8e151d26ca9b9ce", "changes": 26, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=0f3998af3c5e9d8168b32d5085165892c2942101", "patch": "@@ -1161,6 +1161,12 @@ public boolean dropTable(String dbName, String tableName) throws MetaException,\n           pm.deletePersistentAll(partGrants);\n         }\n \n+        // TODO# temporary; will be removed with ACID. Otherwise, need to do direct delete w/o get.\n+        List<MTableWrite> mtw = getTableWrites(dbName, tableName, -1, -1);\n+        if (mtw != null && mtw.size() > 0) {\n+          pm.deletePersistentAll(mtw);\n+        }\n+\n         List<MPartitionColumnPrivilege> partColGrants = listTableAllPartitionColumnGrants(dbName,\n             tableName);\n         if (partColGrants != null && partColGrants.size() > 0) {\n@@ -8866,17 +8872,25 @@ public MTableWrite getTableWrite(\n   public List<MTableWrite> getTableWrites(\n       String dbName, String tblName, long from, long to) throws MetaException {\n     boolean success = false;\n+    dbName = HiveStringUtils.normalizeIdentifier(dbName);\n+    tblName = HiveStringUtils.normalizeIdentifier(tblName);\n     Query query = null;\n     openTransaction();\n     try {\n-      query = pm.newQuery(MTableWrite.class,\n-          \"table.tableName == t1 && table.database.name == t2 && writeId > t3 && writeId < t4\");\n-      query.declareParameters(\n-          \"java.lang.String t1, java.lang.String t2, java.lang.Long t3, java.lang.Long t4\");\n+      String queryStr = \"table.tableName == t1 && table.database.name == t2 && writeId > t3\",\n+          argStr = \"java.lang.String t1, java.lang.String t2, java.lang.Long t3\";\n+      if (to >= 0) {\n+        queryStr += \" && writeId < t4\";\n+        argStr += \", java.lang.Long t4\";\n+      }\n+      query = pm.newQuery(MTableWrite.class, queryStr);\n+      query.declareParameters(argStr);\n       query.setOrdering(\"writeId asc\");\n       @SuppressWarnings(\"unchecked\")\n-      List<MTableWrite> writes =\n-        (List<MTableWrite>) query.executeWithArray(tblName, dbName, from, to);\n+      List<MTableWrite> writes = (List<MTableWrite>)(to >= 0\n+         ? query.executeWithArray(tblName, dbName, from, to)\n+         : query.executeWithArray(tblName, dbName, from));\n+      pm.retrieveAll(writes);\n       success = true;\n       return (writes == null || writes.isEmpty()) ? null : new ArrayList<>(writes);\n     } finally {", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/0f3998af3c5e9d8168b32d5085165892c2942101/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java", "blob_url": "https://github.com/apache/hive/blob/0f3998af3c5e9d8168b32d5085165892c2942101/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java", "sha": "dadae4d950b4a24b32043540098c1ce15feaf294", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java?ref=0f3998af3c5e9d8168b32d5085165892c2942101", "patch": "@@ -374,6 +374,7 @@ public boolean doNext(WritableComparable key, Writable value) throws IOException\n       Utilities.copyTableJobPropertiesToConf(currDesc.getTableDesc(), job);\n       InputFormat inputFormat = getInputFormatFromCache(formatter, job);\n       String inputs = processCurrPathForMmWriteIds(inputFormat);\n+      Utilities.LOG14535.info(\"Setting fetch inputs to \" + inputs);\n       if (inputs == null) return null;\n       job.set(\"mapred.input.dir\", inputs);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/69a793b3475d51d61d69aa8ac5d5fee70b90bf80", "parent": "https://github.com/apache/hive/commit/f6bd00244c1a6804e71b8294fa1588857ad736da", "message": "HIVE-12178 : LLAP: NPE in LRFU policy (Sergey Shelukhin, reviewed by Prasanth Jayachandran)", "bug_id": "hive_111", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "blob_url": "https://github.com/apache/hive/blob/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "sha": "f0650482b389a7cffc5b2aaef352de292b3d23d7", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=69a793b3475d51d61d69aa8ac5d5fee70b90bf80", "patch": "@@ -2305,7 +2305,8 @@ public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {\n     LLAP_USE_LRFU(\"hive.llap.io.use.lrfu\", false,\n         \"Whether ORC low-level cache should use LRFU cache policy instead of default (FIFO).\"),\n     LLAP_LRFU_LAMBDA(\"hive.llap.io.lrfu.lambda\", 0.01f,\n-        \"Lambda for ORC low-level cache LRFU cache policy.\"),\n+        \"Lambda for ORC low-level cache LRFU cache policy. Must be in [0, 1]. 0 makes LRFU\\n\" +\n+        \"behave like LFU, 1 makes it behave like LRU, values in between balance accordingly.\"),\n     LLAP_ORC_ENABLE_TIME_COUNTERS(\"hive.llap.io.orc.time.counters\", true,\n         \"Whether to enable time counters for LLAP IO layer (time spent in HDFS, etc.)\"),\n     LLAP_AUTO_ALLOW_UBER(\"hive.llap.auto.allow.uber\", true,", "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java", "blob_url": "https://github.com/apache/hive/blob/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java", "sha": "76e7605acc669c8be0e190216691c0780e6ef0f2", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java?ref=69a793b3475d51d61d69aa8ac5d5fee70b90bf80", "patch": "@@ -205,7 +205,8 @@ public long evictSomeBlocks(long memoryToReserve) {\n           listHead = listTail = null; // We have evicted the entire list.\n         } else {\n           // Splice the section that we have evicted out of the list.\n-          removeFromListUnderLock(nextCandidate.next, firstCandidate);\n+          // We have already updated the state above so no need to do that again.\n+          removeFromListUnderLockNoStateUpdate(nextCandidate.next, firstCandidate);\n         }\n       }\n     } finally {\n@@ -333,7 +334,6 @@ private void removeFromListAndUnlock(LlapCacheableBuffer buffer) {\n     try {\n       if (buffer.indexInHeap != LlapCacheableBuffer.IN_LIST) return;\n       removeFromListUnderLock(buffer);\n-      buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n     } finally {\n       listLock.unlock();\n     }\n@@ -350,9 +350,11 @@ private void removeFromListUnderLock(LlapCacheableBuffer buffer) {\n     } else {\n       buffer.prev.next = buffer.next;\n     }\n+    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n   }\n \n-  private void removeFromListUnderLock(LlapCacheableBuffer from, LlapCacheableBuffer to) {\n+  private void removeFromListUnderLockNoStateUpdate(\n+      LlapCacheableBuffer from, LlapCacheableBuffer to) {\n     if (to == listTail) {\n       listTail = from.prev;\n     } else {", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java"}, {"additions": 50, "raw_url": "https://github.com/apache/hive/raw/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java", "blob_url": "https://github.com/apache/hive/blob/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java", "sha": "bb815e3273883326fd175b9ef35cd8f5d6b94844", "changes": 57, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java?ref=69a793b3475d51d61d69aa8ac5d5fee70b90bf80", "patch": "@@ -17,17 +17,14 @@\n  */\n package org.apache.hadoop.hive.llap.cache;\n \n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNotNull;\n-import static org.junit.Assert.assertNotSame;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertSame;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n \n+import java.lang.reflect.Field;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n import java.util.Random;\n+import java.util.concurrent.locks.ReentrantLock;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -41,6 +38,45 @@\n public class TestLowLevelLrfuCachePolicy {\n   private static final Log LOG = LogFactory.getLog(TestLowLevelLrfuCachePolicy.class);\n \n+  @Test\n+  public void testRegression_HIVE_12178() throws Exception {\n+    LOG.info(\"Testing wrong list status after eviction\");\n+    EvictionTracker et = new EvictionTracker();\n+    int memSize = 2, lambda = 1; // Set lambda to 1 so the heap size becomes 1 (LRU).\n+    Configuration conf = createConf(1, memSize, (double)lambda);\n+    final LowLevelLrfuCachePolicy lrfu = new LowLevelLrfuCachePolicy(conf);\n+    Field f = LowLevelLrfuCachePolicy.class.getDeclaredField(\"listLock\");\n+    f.setAccessible(true);\n+    ReentrantLock listLock = (ReentrantLock)f.get(lrfu);\n+    LowLevelCacheMemoryManager mm = new LowLevelCacheMemoryManager(conf, lrfu,\n+        LlapDaemonCacheMetrics.create(\"test\", \"1\"));\n+    lrfu.setEvictionListener(et);\n+    final LlapDataBuffer buffer1 = LowLevelCacheImpl.allocateFake();\n+    LlapDataBuffer buffer2 = LowLevelCacheImpl.allocateFake();\n+    assertTrue(cache(mm, lrfu, et, buffer1));\n+    assertTrue(cache(mm, lrfu, et, buffer2));\n+    // buffer2 is now in the heap, buffer1 is in the list. \"Use\" buffer1 again;\n+    // before we notify though, lock the list, so lock cannot remove it from the list.\n+    buffer1.incRef();\n+    assertEquals(LlapCacheableBuffer.IN_LIST, buffer1.indexInHeap);\n+    listLock.lock();\n+    try {\n+      Thread otherThread = new Thread(new Runnable() {\n+        public void run() {\n+          lrfu.notifyLock(buffer1);\n+        }\n+      });\n+      otherThread.start();\n+      otherThread.join();\n+    } finally {\n+      listLock.unlock();\n+    }\n+    // Now try to evict with locked buffer still in the list.\n+    mm.reserveMemory(1, false);\n+    assertSame(buffer2, et.evicted.get(0));\n+    unlock(lrfu, buffer1);\n+  }\n+\n   @Test\n   public void testHeapSize2() {\n     testHeapSize(2);\n@@ -100,13 +136,20 @@ public void testLfuExtreme() {\n     verifyOrder(mm, lfu, et, inserted);\n   }\n \n-  private Configuration createConf(int min, int heapSize) {\n+  private Configuration createConf(int min, int heapSize, Double lambda) {\n     Configuration conf = new Configuration();\n     conf.setInt(HiveConf.ConfVars.LLAP_ORC_CACHE_MIN_ALLOC.varname, min);\n     conf.setInt(HiveConf.ConfVars.LLAP_ORC_CACHE_MAX_SIZE.varname, heapSize);\n+    if (lambda != null) {\n+      conf.setDouble(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, lambda.doubleValue());\n+    }\n     return conf;\n   }\n \n+  private Configuration createConf(int min, int heapSize) {\n+    return createConf(min, heapSize, null);\n+  }\n+\n   @Test\n   public void testLruExtreme() {\n     int heapSize = 4;", "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a31511ab71728e354e290e3a6decebb40f532041", "parent": "https://github.com/apache/hive/commit/1b9114f8c4ca0f3d5f956b00d36ed7a80efb35ec", "message": "HIVE-11578: Fix NPE in ExplainWork (Rajesh Balamohan, via Gopal V)", "bug_id": "hive_112", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/a31511ab71728e354e290e3a6decebb40f532041/ql/src/java/org/apache/hadoop/hive/ql/plan/ExplainWork.java", "blob_url": "https://github.com/apache/hive/blob/a31511ab71728e354e290e3a6decebb40f532041/ql/src/java/org/apache/hadoop/hive/ql/plan/ExplainWork.java", "sha": "132cb8d3e6f7b21abc8bd645e28491909b2a075b", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ExplainWork.java?ref=a31511ab71728e354e290e3a6decebb40f532041", "patch": "@@ -79,7 +79,9 @@ public ExplainWork(Path resFile,\n     this.fetchTask = fetchTask;\n     this.astTree = astTree;\n     this.analyzer = analyzer;\n-    this.inputs = analyzer.getInputs();\n+    if (analyzer != null) {\n+      this.inputs = analyzer.getInputs();\n+    }\n     this.extended = extended;\n     this.formatted = formatted;\n     this.dependency = dependency;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExplainWork.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2a65989a48e043404b7060296be8da9d3494911e", "parent": "https://github.com/apache/hive/commit/4ff5b258c93a1996f320cab19c58465a2aab38bc", "message": "HIVE-11849: NPE in HiveHBaseTableShapshotInputFormat in query with just count(*) (Enis Soztutar via Jason Dere)", "bug_id": "hive_113", "file": [{"additions": 13, "raw_url": "https://github.com/apache/hive/raw/2a65989a48e043404b7060296be8da9d3494911e/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableSnapshotInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/2a65989a48e043404b7060296be8da9d3494911e/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableSnapshotInputFormat.java", "sha": "aa3a02f6516f6758a72b2fa3457cff41a73feaa4", "changes": 21, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableSnapshotInputFormat.java?ref=2a65989a48e043404b7060296be8da9d3494911e", "patch": "@@ -24,6 +24,9 @@\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n import org.apache.hadoop.hbase.mapred.TableInputFormat;\n import org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat;\n+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;\n+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;\n+import org.apache.hadoop.hbase.util.Base64;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.mapred.FileInputFormat;\n import org.apache.hadoop.mapred.InputFormat;\n@@ -41,15 +44,17 @@\n   TableSnapshotInputFormat delegate = new TableSnapshotInputFormat();\n \n   private static void setColumns(JobConf job) throws IOException {\n-    // hbase mapred API doesn't support scan at the moment.\n     Scan scan = HiveHBaseInputFormatUtil.getScan(job);\n-    byte[][] families = scan.getFamilies();\n-    StringBuilder sb = new StringBuilder();\n-    for (int i = 0; i < families.length; i++) {\n-      if (i > 0) sb.append(\" \");\n-      sb.append(Bytes.toString(families[i]));\n-    }\n-    job.set(TableInputFormat.COLUMN_LIST, sb.toString());\n+    job.set(org.apache.hadoop.hbase.mapreduce.TableInputFormat.SCAN,\n+      convertScanToString(scan));\n+  }\n+\n+  // TODO: Once HBASE-11163 is completed, use that API, or switch to\n+  // using mapreduce version of the APIs. rather than mapred\n+  // Copied from HBase's TableMapreduceUtil since it is not public API\n+  static String convertScanToString(Scan scan) throws IOException {\n+    ClientProtos.Scan proto = ProtobufUtil.toScan(scan);\n+    return Base64.encodeBytes(proto.toByteArray());\n   }\n \n   @Override", "filename": "hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableSnapshotInputFormat.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/2a65989a48e043404b7060296be8da9d3494911e/hbase-handler/src/test/queries/positive/hbase_handler_snapshot.q", "blob_url": "https://github.com/apache/hive/blob/2a65989a48e043404b7060296be8da9d3494911e/hbase-handler/src/test/queries/positive/hbase_handler_snapshot.q", "sha": "ebdc63ca47b0568cb64fe72eebb66ca6b442ee8b", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/queries/positive/hbase_handler_snapshot.q?ref=2a65989a48e043404b7060296be8da9d3494911e", "patch": "@@ -2,3 +2,7 @@ SET hive.hbase.snapshot.name=src_hbase_snapshot;\n SET hive.hbase.snapshot.restoredir=/tmp;\n \n SELECT * FROM src_hbase LIMIT 5;\n+\n+SELECT value FROM src_hbase LIMIT 5;\n+\n+select count(*) from src_hbase;", "filename": "hbase-handler/src/test/queries/positive/hbase_handler_snapshot.q"}, {"additions": 22, "raw_url": "https://github.com/apache/hive/raw/2a65989a48e043404b7060296be8da9d3494911e/hbase-handler/src/test/results/positive/hbase_handler_snapshot.q.out", "blob_url": "https://github.com/apache/hive/blob/2a65989a48e043404b7060296be8da9d3494911e/hbase-handler/src/test/results/positive/hbase_handler_snapshot.q.out", "sha": "731646c2087a887635c2b2e7d2c095dbe80d1f07", "changes": 22, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/results/positive/hbase_handler_snapshot.q.out?ref=2a65989a48e043404b7060296be8da9d3494911e", "patch": "@@ -11,3 +11,25 @@ POSTHOOK: Input: default@src_hbase\n 100\tval_100\n 103\tval_103\n 104\tval_104\n+PREHOOK: query: SELECT value FROM src_hbase LIMIT 5\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src_hbase\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT value FROM src_hbase LIMIT 5\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src_hbase\n+#### A masked pattern was here ####\n+val_0\n+val_10\n+val_100\n+val_103\n+val_104\n+PREHOOK: query: select count(*) from src_hbase\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src_hbase\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from src_hbase\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src_hbase\n+#### A masked pattern was here ####\n+309", "filename": "hbase-handler/src/test/results/positive/hbase_handler_snapshot.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/fbbb7cf1fa5691037243a6db3993f294ffb00eeb", "parent": "https://github.com/apache/hive/commit/e150af9457079c87c267094f3861528286e951ea", "message": "HIVE-11636 NPE in stats conversion with HBase metastore (Sergey Shelukhin via gates)", "bug_id": "hive_114", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "df64124536a576300b579a20e34a706d4b4cbdcd", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=fbbb7cf1fa5691037243a6db3993f294ffb00eeb", "patch": "@@ -4281,8 +4281,8 @@ public TableStatsResult get_table_statistics_req(TableStatsRequest request)\n       }\n       try {\n         ColumnStatistics cs = getMS().getTableColumnStatistics(dbName, tblName, lowerCaseColNames);\n-        result = new TableStatsResult(\n-            cs == null ? Lists.<ColumnStatisticsObj>newArrayList() : cs.getStatsObj());\n+        result = new TableStatsResult((cs == null || cs.getStatsObj() == null)\n+            ? Lists.<ColumnStatisticsObj>newArrayList() : cs.getStatsObj());\n       } finally {\n         endFunction(\"get_table_statistics_req: \", result == null, null, tblName);\n       }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java", "blob_url": "https://github.com/apache/hive/blob/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java", "sha": "df0fac31ad2bce2a49626c6cc8305496a8629030", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java?ref=fbbb7cf1fa5691037243a6db3993f294ffb00eeb", "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hive.metastore.Warehouse;\n import org.apache.hadoop.hive.metastore.api.AggrStats;\n import org.apache.hadoop.hive.metastore.api.ColumnStatistics;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n@@ -1693,9 +1694,11 @@ public AggrStats get_aggr_stats_for(String dbName, String tblName, List<String>\n       partVals.add(partNameToVals(partName));\n     }\n     boolean commit = false;\n+    boolean hasAnyStats = false;\n     openTransaction();\n     try {\n       AggrStats aggrStats = new AggrStats();\n+      aggrStats.setPartsFound(0);\n       for (String colName : colNames) {\n         try {\n           AggrStats oneCol =\n@@ -1704,6 +1707,7 @@ public AggrStats get_aggr_stats_for(String dbName, String tblName, List<String>\n             assert oneCol.getColStatsSize() == 1;\n             aggrStats.setPartsFound(oneCol.getPartsFound());\n             aggrStats.addToColStats(oneCol.getColStats().get(0));\n+            hasAnyStats = true;\n           }\n         } catch (CacheLoader.InvalidCacheLoadException e) {\n           LOG.debug(\"Found no stats for column \" + colName);\n@@ -1712,6 +1716,10 @@ public AggrStats get_aggr_stats_for(String dbName, String tblName, List<String>\n         }\n       }\n       commit = true;\n+      if (!hasAnyStats) {\n+        // Set the required field.\n+        aggrStats.setColStats(new ArrayList<ColumnStatisticsObj>());\n+      }\n       return aggrStats;\n     } catch (IOException e) {\n       LOG.error(\"Unable to fetch aggregate column statistics\", e);", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd", "parent": "https://github.com/apache/hive/commit/251991568c5e9e38b3480e9ef5dc972b9da112db", "message": "HIVE-11380: NPE when FileSinkOperator is not initialized (Yongzhi Chen, reviewed by Sergio Pena)", "bug_id": "hive_115", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "blob_url": "https://github.com/apache/hive/blob/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "sha": "2604d5d82fd5b4b67e6c7cb048c9840c4837d4b2", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java?ref=8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd", "patch": "@@ -569,7 +569,7 @@ protected void createBucketFiles(FSPaths fsp) throws HiveException {\n       assert filesIdx == numFiles;\n \n       // in recent hadoop versions, use deleteOnExit to clean tmp files.\n-      if (isNativeTable) {\n+      if (isNativeTable && fs != null && fsp != null) {\n         autoDelete = fs.deleteOnExit(fsp.outPaths[0]);\n       }\n     } catch (Exception e) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cf4268487a5d65346b79994a2bfada70b20c428e", "parent": "https://github.com/apache/hive/commit/e732616392ddf5139f4d32bfb9fc51f352114887", "message": "HIVE-13090 : Hive metastore crashes on NPE with ZooKeeperTokenStore (Piotr Wikie\u0142, Thejas Nair, reviewed by Ashutosh Chauhan)", "bug_id": "hive_116", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java", "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java", "sha": "65a10e344e3fb24e5efd3b051bb1eaf26b0179a3", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e", "patch": "@@ -122,6 +122,7 @@ public void testTokenStorage() throws Exception {\n \n     assertTrue(ts.removeToken(tokenId));\n     assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());\n+    assertNull(ts.getToken(tokenId));\n   }\n \n   public void testAclNoAuth() throws Exception {", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java", "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java", "sha": "87b418ebf29f8744a57bdb809a1d0fec7ed47ab5", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e", "patch": "@@ -330,7 +330,6 @@ public void run() {\n       } catch (Throwable t) {\n         LOGGER.error(\"ExpiredTokenRemover thread received unexpected exception. \"\n             + t, t);\n-        Runtime.getRuntime().exit(-1);\n       }\n     }\n   }", "filename": "shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java", "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java", "sha": "528e55d28cb7f56b0bdc4edb4f829acb6d6a61ca", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e", "patch": "@@ -396,6 +396,10 @@ public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {\n   @Override\n   public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {\n     byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));\n+    if(tokenBytes == null) {\n+      // The token is already removed.\n+      return null;\n+    }\n     try {\n       return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);\n     } catch (Exception ex) {", "filename": "shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/512462165b3adc772e2ac38dc56624ed2519d970", "parent": "https://github.com/apache/hive/commit/83752a6bd7308b15398caf9743cf3d800781dac9", "message": "HIVE-14694 : UDF rand throws NPE when input data is NULL (Niklaus Xiao via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_117", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/512462165b3adc772e2ac38dc56624ed2519d970/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRand.java", "blob_url": "https://github.com/apache/hive/blob/512462165b3adc772e2ac38dc56624ed2519d970/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRand.java", "sha": "2bf0a57219e15130935a22e0f87796136b1f2869", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRand.java?ref=512462165b3adc772e2ac38dc56624ed2519d970", "patch": "@@ -54,7 +54,11 @@ public DoubleWritable evaluate() {\n \n   public DoubleWritable evaluate(LongWritable seed) {\n     if (random == null) {\n-      random = new Random(seed.get());\n+      long seedValue = 0;\n+      if (seed != null) {\n+        seedValue = seed.get();\n+      }\n+      random = new Random(seedValue);\n     }\n     result.set(random.nextDouble());\n     return result;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRand.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ec4673bbc61b2555ef2d992266055a331443b4d6", "parent": "https://github.com/apache/hive/commit/20824f27b1649ae8c2670a177463dae0f188183c", "message": "HIVE-14658 : UDF abs throws NPE when input arg type is string (Niklaus Xiao via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_118", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java", "blob_url": "https://github.com/apache/hive/blob/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java", "sha": "a8e278683a0b6c6b79b392535bc668cd27f32c1d", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java?ref=ec4673bbc61b2555ef2d992266055a331443b4d6", "patch": "@@ -130,6 +130,9 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     case STRING:\n     case DOUBLE:\n       valObject = inputConverter.convert(valObject);\n+      if (valObject == null) {\n+        return null;\n+      }\n       resultDouble.set(Math.abs(((DoubleWritable) valObject).get()));\n       return resultDouble;\n     case DECIMAL:", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java", "blob_url": "https://github.com/apache/hive/blob/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java", "sha": "6dbb33f9feaff8d06a88b8db603dd207ac0ab5d7", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java?ref=ec4673bbc61b2555ef2d992266055a331443b4d6", "patch": "@@ -133,6 +133,12 @@ public void testText() throws HiveException {\n     output = (DoubleWritable) udf.evaluate(args);\n \n     assertEquals(\"abs() test for String failed \", \"123.45\", output.toString());\n+\n+    valueObj = new DeferredJavaObject(new Text(\"foo\"));\n+    args[0] = valueObj;\n+    output = (DoubleWritable) udf.evaluate(args);\n+\n+    assertEquals(\"abs() test for String failed \", null, output);\n   }\n \n   public void testHiveDecimal() throws HiveException {", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/735ba0d872ddfbe0470497576904d721350548a4", "parent": "https://github.com/apache/hive/commit/8f9d964007f668f11084d55fe5608294edb3434f", "message": "HIVE-10816: NPE in ExecDriver::handleSampling when submitted via child JVM (Rui reviewed by Xuefu)", "bug_id": "hive_119", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java", "blob_url": "https://github.com/apache/hive/blob/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java", "sha": "dc1b601b732dc9cda814c41da6ae42d1179d4f56", "changes": 9, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java?ref=735ba0d872ddfbe0470497576904d721350548a4", "patch": "@@ -112,7 +112,7 @@ public void collect(HiveKey key, Object value) throws IOException {\n     return partitionKeys;\n   }\n \n-  public void writePartitionKeys(Path path, HiveConf conf, JobConf job) throws IOException {\n+  public void writePartitionKeys(Path path, JobConf job) throws IOException {\n     byte[][] partitionKeys = getPartitionKeys(job.getNumReduceTasks());\n     int numPartition = partitionKeys.length + 1;\n     if (numPartition != job.getNumReduceTasks()) {\n@@ -133,10 +133,11 @@ public void writePartitionKeys(Path path, HiveConf conf, JobConf job) throws IOE\n   }\n \n   // random sampling\n-  public static FetchOperator createSampler(FetchWork work, HiveConf conf, JobConf job,\n+  public static FetchOperator createSampler(FetchWork work, JobConf job,\n       Operator<?> operator) throws HiveException {\n-    int sampleNum = conf.getIntVar(HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n-    float samplePercent = conf.getFloatVar(HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n+    int sampleNum = HiveConf.getIntVar(job, HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n+    float samplePercent =\n+        HiveConf.getFloatVar(job, HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n     if (samplePercent < 0.0 || samplePercent > 1.0) {\n       throw new IllegalArgumentException(\"Percentile value must be within the range of 0 to 1.\");\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java", "blob_url": "https://github.com/apache/hive/blob/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java", "sha": "a2cf71281e8cd960e5f297638005da010b74d6ae", "changes": 12, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=735ba0d872ddfbe0470497576904d721350548a4", "patch": "@@ -376,7 +376,7 @@ public int execute(DriverContext driverContext) {\n \n       if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n         try {\n-          handleSampling(driverContext, mWork, job, conf);\n+          handleSampling(ctx, mWork, job);\n           job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n         } catch (IllegalStateException e) {\n           console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n@@ -496,7 +496,7 @@ public int execute(DriverContext driverContext) {\n     return (returnVal);\n   }\n \n-  private void handleSampling(DriverContext context, MapWork mWork, JobConf job, HiveConf conf)\n+  private void handleSampling(Context context, MapWork mWork, JobConf job)\n       throws Exception {\n     assert mWork.getAliasToWork().keySet().size() == 1;\n \n@@ -512,7 +512,7 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n       inputPaths.add(new Path(path));\n     }\n \n-    Path tmpPath = context.getCtx().getExternalTmpPath(inputPaths.get(0));\n+    Path tmpPath = context.getExternalTmpPath(inputPaths.get(0));\n     Path partitionFile = new Path(tmpPath, \".partitions\");\n     ShimLoader.getHadoopShims().setTotalOrderPartitionFile(job, partitionFile);\n     PartitionKeySampler sampler = new PartitionKeySampler();\n@@ -541,9 +541,9 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n       fetchWork.setSource(ts);\n \n       // random sampling\n-      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, conf, job, ts);\n+      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, job, ts);\n       try {\n-        ts.initialize(conf, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n+        ts.initialize(job, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n         OperatorUtils.setChildrenCollector(ts.getChildOperators(), sampler);\n         while (fetcher.pushRow()) { }\n       } finally {\n@@ -552,7 +552,7 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n     } else {\n       throw new IllegalArgumentException(\"Invalid sampling type \" + mWork.getSamplingType());\n     }\n-    sampler.writePartitionKeys(partitionFile, conf, job);\n+    sampler.writePartitionKeys(partitionFile, job);\n   }\n \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/81e8e665247f8bff927cb206b5bb178455047d73", "parent": "https://github.com/apache/hive/commit/cee1183d3044c4fa0a7882c3499d59721f1b5492", "message": "HIVE-10527: NPE in SparkUtilities::isDedicatedCluster [Spark Branch] (Rui reviewed by Jimmy)", "bug_id": "hive_120", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/81e8e665247f8bff927cb206b5bb178455047d73/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java", "blob_url": "https://github.com/apache/hive/blob/81e8e665247f8bff927cb206b5bb178455047d73/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java", "sha": "21398d8d9d15278dca6a620a9ac2b4be71cedc85", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java?ref=81e8e665247f8bff927cb206b5bb178455047d73", "patch": "@@ -112,6 +112,7 @@ public static HiveSparkClient createHiveSparkClient(HiveConf hiveconf)\n     String sparkMaster = hiveConf.get(\"spark.master\");\n     if (sparkMaster == null) {\n       sparkMaster = sparkConf.get(\"spark.master\");\n+      hiveConf.set(\"spark.master\", sparkMaster);\n     }\n     if (sparkMaster.equals(\"yarn-cluster\")) {\n       sparkConf.put(\"spark.yarn.maxAppAttempts\", \"1\");", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b219752dfd12de862e9d9ce637e60720f3ab1b44", "parent": "https://github.com/apache/hive/commit/ab1f405d192f65f7443bb08785a35f906c55fcfe", "message": "HIVE-12815: column stats NPE for a query w/o a table (Prasanth Jayachandran reviewed by Sergey Shelukhin)", "bug_id": "hive_121", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b219752dfd12de862e9d9ce637e60720f3ab1b44/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "blob_url": "https://github.com/apache/hive/blob/b219752dfd12de862e9d9ce637e60720f3ab1b44/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "sha": "b4cf58f5b8bf0e28f711cbeb71984ab3f5012e13", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java?ref=b219752dfd12de862e9d9ce637e60720f3ab1b44", "patch": "@@ -1443,7 +1443,7 @@ public static long getBasicStatForTable(Table table, String statType) {\n   public static long getDataSizeFromColumnStats(long numRows, List<ColStatistics> colStats) {\n     long result = 0;\n \n-    if (numRows <= 0) {\n+    if (numRows <= 0 || colStats == null || colStats.isEmpty()) {\n       return result;\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/b219752dfd12de862e9d9ce637e60720f3ab1b44/ql/src/test/queries/clientpositive/annotate_stats_table.q", "blob_url": "https://github.com/apache/hive/blob/b219752dfd12de862e9d9ce637e60720f3ab1b44/ql/src/test/queries/clientpositive/annotate_stats_table.q", "sha": "b5389ffdb4fa510cca8c98599d8f9c3b931a8020", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/annotate_stats_table.q?ref=b219752dfd12de862e9d9ce637e60720f3ab1b44", "patch": "@@ -51,3 +51,6 @@ explain select deptid from emp_orc;\n \n -- basicStatState: COMPLETE colStatState: COMPLETE\n explain select lastname,deptid from emp_orc;\n+\n+create table tmp as select 1;\n+explain create table tmp as select 1;", "filename": "ql/src/test/queries/clientpositive/annotate_stats_table.q"}, {"additions": 99, "raw_url": "https://github.com/apache/hive/raw/b219752dfd12de862e9d9ce637e60720f3ab1b44/ql/src/test/results/clientpositive/annotate_stats_table.q.out", "blob_url": "https://github.com/apache/hive/blob/b219752dfd12de862e9d9ce637e60720f3ab1b44/ql/src/test/results/clientpositive/annotate_stats_table.q.out", "sha": "cec3a1f8ed10300e31726a2d0528ab9fa7c8f05d", "changes": 99, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/annotate_stats_table.q.out?ref=b219752dfd12de862e9d9ce637e60720f3ab1b44", "patch": "@@ -289,3 +289,102 @@ STAGE PLANS:\n             Statistics: Num rows: 48 Data size: 4560 Basic stats: COMPLETE Column stats: COMPLETE\n             ListSink\n \n+PREHOOK: query: create table tmp as select 1\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@tmp\n+POSTHOOK: query: create table tmp as select 1\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@tmp\n+PREHOOK: query: explain create table tmp as select 1\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: query: explain create table tmp as select 1\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-6 depends on stages: Stage-1 , consists of Stage-3, Stage-2, Stage-4\n+  Stage-3\n+  Stage-0 depends on stages: Stage-3, Stage-2, Stage-5\n+  Stage-7 depends on stages: Stage-0\n+  Stage-2\n+  Stage-4\n+  Stage-5 depends on stages: Stage-4\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: _dummy_table\n+            Row Limit Per Split: 1\n+            Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n+            Select Operator\n+              expressions: 1 (type: int)\n+              outputColumnNames: _col0\n+              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+              File Output Operator\n+                compressed: false\n+                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                table:\n+                    input format: org.apache.hadoop.mapred.TextInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    name: default.tmp\n+\n+  Stage: Stage-6\n+    Conditional Operator\n+\n+  Stage: Stage-3\n+    Move Operator\n+      files:\n+          hdfs directory: true\n+#### A masked pattern was here ####\n+\n+  Stage: Stage-0\n+    Move Operator\n+      files:\n+          hdfs directory: true\n+#### A masked pattern was here ####\n+\n+  Stage: Stage-7\n+      Create Table Operator:\n+        Create Table\n+          columns: _c0 int\n+          input format: org.apache.hadoop.mapred.TextInputFormat\n+          output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\n+          serde name: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          name: default.tmp\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            File Output Operator\n+              compressed: false\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  name: default.tmp\n+\n+  Stage: Stage-4\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            File Output Operator\n+              compressed: false\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  name: default.tmp\n+\n+  Stage: Stage-5\n+    Move Operator\n+      files:\n+          hdfs directory: true\n+#### A masked pattern was here ####\n+", "filename": "ql/src/test/results/clientpositive/annotate_stats_table.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2d87c12d914044540a4f5ed7fe500e3c245fbead", "parent": "https://github.com/apache/hive/commit/bc0138c436add2335d2045b6c7bf86bc6a15cc27", "message": "HIVE-10590 fix potential NPE in HiveMetaStore.equals (Alexander Pivovarov, reviewed by Ashutosh Chauhan)", "bug_id": "hive_122", "file": [{"additions": 8, "raw_url": "https://github.com/apache/hive/raw/2d87c12d914044540a4f5ed7fe500e3c245fbead/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/2d87c12d914044540a4f5ed7fe500e3c245fbead/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "sha": "e7960487680dd26059ee512dd28b66c90848aa57", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=2d87c12d914044540a4f5ed7fe500e3c245fbead", "patch": "@@ -2024,8 +2024,14 @@ public boolean equals(Object obj) {\n         if (!p1.isSetValues() || !p2.isSetValues()) return p1.isSetValues() == p2.isSetValues();\n         if (p1.getValues().size() != p2.getValues().size()) return false;\n         for (int i = 0; i < p1.getValues().size(); ++i) {\n-          String v1 = p1.getValues().get(i), v2 = p2.getValues().get(i);\n-          if ((v1 == null && v2 != null) || !v1.equals(v2)) return false;\n+          String v1 = p1.getValues().get(i);\n+          String v2 = p2.getValues().get(i);\n+          if (v1 == null && v2 == null) {\n+            continue;\n+          }\n+          if (v1 == null || !v1.equals(v2)) {\n+            return false;\n+          }\n         }\n         return true;\n       }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/3633db25fadd39fdcad15d95af1cf69cc6e2429e", "parent": "https://github.com/apache/hive/commit/0af1d6e7d36fbd8071a35692f51d72eb86b0e9da", "message": "HIVE-10538: Fix NPE in FileSinkOperator from hashcode mismatch (Peter Slawski reviewed by Prasanth Jayachandran)", "bug_id": "hive_123", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/itests/src/test/resources/testconfiguration.properties", "sha": "eeb46cc16dc9c2a0e7f05b503efa9ee87cd1f067", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -3,6 +3,7 @@ minimr.query.files=auto_sortmerge_join_16.q,\\\n   bucket4.q,\\\n   bucket5.q,\\\n   bucket6.q,\\\n+  bucket_many.q,\\\n   bucket_num_reducers.q,\\\n   bucket_num_reducers2.q,\\\n   bucketizedhiveinputformat.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java", "sha": "859a28fc965ed75c965c41af2e1442f3eda99523", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -125,7 +125,7 @@\n   protected transient Object[] cachedValues;\n   protected transient List<List<Integer>> distinctColIndices;\n   protected transient Random random;\n-  protected transient int bucketNumber;\n+  protected transient int bucketNumber = -1;\n \n   /**\n    * This two dimensional array holds key data and a corresponding Union object\n@@ -552,6 +552,7 @@ private BytesWritable makeValueWritable(Object row) throws Exception {\n     // in case of bucketed table, insert the bucket number as the last column in value\n     if (bucketEval != null) {\n       length -= 1;\n+      assert bucketNumber >= 0;\n       cachedValues[length] = new Text(String.valueOf(bucketNumber));\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/queries/clientpositive/bucket_many.q", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/queries/clientpositive/bucket_many.q", "sha": "1f0b795f870f699963890d3ffddc0f47ce5bb214", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/bucket_many.q?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -0,0 +1,16 @@\n+set hive.enforce.bucketing = true;\n+set mapred.reduce.tasks = 16;\n+\n+create table bucket_many(key int, value string) clustered by (key) into 256 buckets;\n+\n+explain extended\n+insert overwrite table bucket_many\n+select * from src;\n+\n+insert overwrite table bucket_many\n+select * from src;\n+\n+explain\n+select * from bucket_many tablesample (bucket 1 out of 256) s;\n+\n+select * from bucket_many tablesample (bucket 1 out of 256) s;", "filename": "ql/src/test/queries/clientpositive/bucket_many.q"}, {"additions": 230, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/bucket_many.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/bucket_many.q.out", "sha": "9f09163ea1cdcd5f1097fe47b02b6ba5eb8e4012", "changes": 230, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucket_many.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -0,0 +1,230 @@\n+PREHOOK: query: create table bucket_many(key int, value string) clustered by (key) into 256 buckets\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@bucket_many\n+POSTHOOK: query: create table bucket_many(key int, value string) clustered by (key) into 256 buckets\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@bucket_many\n+PREHOOK: query: explain extended\n+insert overwrite table bucket_many\n+select * from src\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain extended\n+insert overwrite table bucket_many\n+select * from src\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  \n+TOK_QUERY\n+   TOK_FROM\n+      TOK_TABREF\n+         TOK_TABNAME\n+            src\n+   TOK_INSERT\n+      TOK_DESTINATION\n+         TOK_TAB\n+            TOK_TABNAME\n+               bucket_many\n+      TOK_SELECT\n+         TOK_SELEXPR\n+            TOK_ALLCOLREF\n+\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+  Stage-2 depends on stages: Stage-0\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: src\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            GatherStats: false\n+            Select Operator\n+              expressions: key (type: string), value (type: string)\n+              outputColumnNames: _col0, _col1\n+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+              Reduce Output Operator\n+                sort order: \n+                Map-reduce partition columns: UDFToInteger(_col0) (type: int)\n+                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                tag: -1\n+                value expressions: _col0 (type: string), _col1 (type: string)\n+                auto parallelism: false\n+      Path -> Alias:\n+#### A masked pattern was here ####\n+      Path -> Partition:\n+#### A masked pattern was here ####\n+          Partition\n+            base file name: src\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns key,value\n+              columns.comments 'default','default'\n+              columns.types string:string\n+#### A masked pattern was here ####\n+              name default.src\n+              numFiles 1\n+              numRows 500\n+              rawDataSize 5312\n+              serialization.ddl struct src { string key, string value}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 5812\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                COLUMN_STATS_ACCURATE true\n+                bucket_count -1\n+                columns key,value\n+                columns.comments 'default','default'\n+                columns.types string:string\n+#### A masked pattern was here ####\n+                name default.src\n+                numFiles 1\n+                numRows 500\n+                rawDataSize 5312\n+                serialization.ddl struct src { string key, string value}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                totalSize 5812\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.src\n+            name: default.src\n+      Truncated Path -> Alias:\n+        /src [src]\n+      Needs Tagging: false\n+      Reduce Operator Tree:\n+        Select Operator\n+          expressions: UDFToInteger(VALUE._col0) (type: int), VALUE._col1 (type: string)\n+          outputColumnNames: _col0, _col1\n+          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            GlobalTableId: 1\n+#### A masked pattern was here ####\n+            NumFilesPerFileSink: 16\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+#### A masked pattern was here ####\n+            table:\n+                input format: org.apache.hadoop.mapred.TextInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                properties:\n+                  bucket_count 256\n+                  bucket_field_name key\n+                  columns key,value\n+                  columns.comments \n+                  columns.types int:string\n+#### A masked pattern was here ####\n+                  name default.bucket_many\n+                  serialization.ddl struct bucket_many { i32 key, string value}\n+                  serialization.format 1\n+                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                name: default.bucket_many\n+            TotalFiles: 256\n+            GatherStats: true\n+            MultiFileSpray: true\n+\n+  Stage: Stage-0\n+    Move Operator\n+      tables:\n+          replace: true\n+#### A masked pattern was here ####\n+          table:\n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count 256\n+                bucket_field_name key\n+                columns key,value\n+                columns.comments \n+                columns.types int:string\n+#### A masked pattern was here ####\n+                name default.bucket_many\n+                serialization.ddl struct bucket_many { i32 key, string value}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.bucket_many\n+\n+  Stage: Stage-2\n+    Stats-Aggr Operator\n+#### A masked pattern was here ####\n+\n+PREHOOK: query: insert overwrite table bucket_many\n+select * from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@bucket_many\n+POSTHOOK: query: insert overwrite table bucket_many\n+select * from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@bucket_many\n+POSTHOOK: Lineage: bucket_many.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: bucket_many.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: explain\n+select * from bucket_many tablesample (bucket 1 out of 256) s\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select * from bucket_many tablesample (bucket 1 out of 256) s\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: (((hash(key) & 2147483647) % 256) = 0) (type: boolean)\n+              Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: int), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select * from bucket_many tablesample (bucket 1 out of 256) s\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@bucket_many\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from bucket_many tablesample (bucket 1 out of 256) s\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@bucket_many\n+#### A masked pattern was here ####\n+256\tval_256\n+0\tval_0\n+0\tval_0\n+0\tval_0\n+256\tval_256", "filename": "ql/src/test/results/clientpositive/bucket_many.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_gby.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_gby.q.out", "sha": "9ca8a881cbbc09c31a2218bc8fbb7fa56bbe61db", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/cbo_gby.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -11,10 +11,10 @@ POSTHOOK: Input: default@cbo_t1\n POSTHOOK: Input: default@cbo_t1@dt=2014\n #### A masked pattern was here ####\n 1\t4\t12\n+ 1 \t4\t2\n NULL\tNULL\tNULL\n  1\t4\t2\n 1 \t4\t2\n- 1 \t4\t2\n PREHOOK: query: select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x\n PREHOOK: type: QUERY\n PREHOOK: Input: default@cbo_t1\n@@ -25,9 +25,9 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@cbo_t1\n POSTHOOK: Input: default@cbo_t1@dt=2014\n #### A masked pattern was here ####\n+5.0\t2\t3\n NULL\tNULL\t1\n 5.0\t12\t1\n-5.0\t2\t3\n PREHOOK: query: select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c\n PREHOOK: type: QUERY\n PREHOOK: Input: default@cbo_t1", "filename": "ql/src/test/results/clientpositive/spark/cbo_gby.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out", "sha": "ded043f626b0926075688a03b659b651756a27c7", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -79,9 +79,9 @@ POSTHOOK: Input: default@cbo_t1\n POSTHOOK: Input: default@cbo_t1@dt=2014\n #### A masked pattern was here ####\n NULL\t0\tNULL\n+1 \t2\t1.0\n  1 \t2\t1.0\n  1\t2\t1.0\n-1 \t2\t1.0\n 1\t12\t1.0\n PREHOOK: query: select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_float order by a\n PREHOOK: type: QUERY", "filename": "ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out", "sha": "9fe3b7227c04480249534b0595d2e4e1c2a5e0d4", "changes": 38, "status": "modified", "deletions": 19, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -204,16 +204,16 @@ POSTHOOK: query: SELECT DEST1.* FROM DEST1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest1\n #### A masked pattern was here ####\n-[\"166\"]\t1\n-[\"169\"]\t4\n+[\"118\"]\t2\n+[\"180\"]\t1\n+[\"201\"]\t1\n+[\"202\"]\t1\n [\"238\"]\t2\n-[\"258\"]\t1\n-[\"306\"]\t1\n-[\"384\"]\t3\n-[\"392\"]\t1\n-[\"435\"]\t1\n-[\"455\"]\t1\n-[\"468\"]\t4\n+[\"273\"]\t3\n+[\"282\"]\t2\n+[\"419\"]\t1\n+[\"432\"]\t1\n+[\"467\"]\t1\n PREHOOK: query: SELECT DEST2.* FROM DEST2\n PREHOOK: type: QUERY\n PREHOOK: Input: default@dest2\n@@ -222,13 +222,13 @@ POSTHOOK: query: SELECT DEST2.* FROM DEST2\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-{\"120\":\"val_120\"}\t2\n-{\"129\":\"val_129\"}\t2\n-{\"160\":\"val_160\"}\t1\n-{\"26\":\"val_26\"}\t2\n-{\"27\":\"val_27\"}\t1\n-{\"288\":\"val_288\"}\t2\n-{\"298\":\"val_298\"}\t3\n-{\"30\":\"val_30\"}\t1\n-{\"311\":\"val_311\"}\t3\n-{\"74\":\"val_74\"}\t1\n+{\"0\":\"val_0\"}\t3\n+{\"138\":\"val_138\"}\t4\n+{\"170\":\"val_170\"}\t1\n+{\"19\":\"val_19\"}\t1\n+{\"222\":\"val_222\"}\t1\n+{\"223\":\"val_223\"}\t2\n+{\"226\":\"val_226\"}\t1\n+{\"489\":\"val_489\"}\t4\n+{\"8\":\"val_8\"}\t1\n+{\"80\":\"val_80\"}\t1", "filename": "ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out", "sha": "41d60f584d71ee2fd1fffbb94d090230c6a0f9f3", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -93,9 +93,9 @@ POSTHOOK: query: SELECT col1, col2 FROM src LATERAL VIEW explode2(array(1,2,3))\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n #### A masked pattern was here ####\n-2\t2\n-1\t1\n 3\t3\n+1\t1\n+2\t2\n PREHOOK: query: DROP TEMPORARY FUNCTION explode2\n PREHOOK: type: DROPFUNCTION\n PREHOOK: Output: explode2", "filename": "ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_remove_25.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_remove_25.q.out", "sha": "5853cc03cd4db548b727eee098362a769d63d4df", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/union_remove_25.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -424,7 +424,7 @@ Partition Parameters:\n \tnumFiles            \t2                   \n \tnumRows             \t-1                  \n \trawDataSize         \t-1                  \n-\ttotalSize           \t6814                \n+\ttotalSize           \t6826                \n #### A masked pattern was here ####\n \t \t \n # Storage Information\t \t ", "filename": "ql/src/test/results/clientpositive/spark/union_remove_25.q.out"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_top_level.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_top_level.q.out", "sha": "a64fc9572e40f28d8b564564461626aade96dd2b", "changes": 16, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/union_top_level.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -348,18 +348,18 @@ POSTHOOK: Input: default@src\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-10\tval_10\n-10\tval_10\n+0\tval_0\n+0\tval_0\n+100\tval_100\n+100\tval_100\n+100\tval_100\n+100\tval_100\n+100\tval_100\n+100\tval_100\n 100\tval_100\n 100\tval_100\n-103\tval_103\n-103\tval_103\n-104\tval_104\n-104\tval_104\n 104\tval_104\n 104\tval_104\n-111\tval_111\n-111\tval_111\n PREHOOK: query: -- ctas\n explain\n create table union_top as", "filename": "ql/src/test/results/clientpositive/spark/union_top_level.q.out"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out", "sha": "aaac8aac04ec925019b04e482a34e73e869512e7", "changes": 16, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -191,13 +191,13 @@ POSTHOOK: query: SELECT\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@over1korc\n #### A masked pattern was here ####\n+65636\t50.0\t50.0\t50\n+65550\t50.0\t50.0\t50\n+65592\t50.0\t50.0\t50\n+65744\t50.0\t50.0\t50\n+65668\t50.0\t50.0\t50\n+65722\t50.0\t50.0\t50\n 65598\t50.0\t50.0\t50\n-65694\t50.0\t50.0\t50\n-65678\t50.0\t50.0\t50\n-65684\t50.0\t50.0\t50\n+65568\t50.0\t50.0\t50\n 65596\t50.0\t50.0\t50\n-65692\t50.0\t50.0\t50\n-65630\t50.0\t50.0\t50\n-65674\t50.0\t50.0\t50\n-65628\t50.0\t50.0\t50\n-65776\t50.0\t50.0\t50\n+65738\t50.0\t50.0\t50", "filename": "ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out", "sha": "44ecd09a326e5ca2558c2cf679d62c38ebba9814", "changes": 16, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -191,13 +191,13 @@ POSTHOOK: query: SELECT\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@over1korc\n #### A masked pattern was here ####\n-65788\t50.0\t50.0\t50\n+65636\t50.0\t50.0\t50\n+65550\t50.0\t50.0\t50\n+65592\t50.0\t50.0\t50\n+65744\t50.0\t50.0\t50\n+65722\t50.0\t50.0\t50\n+65668\t50.0\t50.0\t50\n 65598\t50.0\t50.0\t50\n-65694\t50.0\t50.0\t50\n-65678\t50.0\t50.0\t50\n-65684\t50.0\t50.0\t50\n 65596\t50.0\t50.0\t50\n-65692\t50.0\t50.0\t50\n-65630\t50.0\t50.0\t50\n-65674\t50.0\t50.0\t50\n-65628\t50.0\t50.0\t50\n+65568\t50.0\t50.0\t50\n+65738\t50.0\t50.0\t50", "filename": "ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out", "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out", "sha": "304458215b4dcbc4d49321ba5f14ca5a87f2ec26", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e", "patch": "@@ -768,7 +768,7 @@ FROM alltypesorc_string\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@alltypesorc_string\n #### A masked pattern was here ####\n-1123143.857\n+1123143.8569999998\n PREHOOK: query: EXPLAIN SELECT\n   avg(ctimestamp1),\n   variance(ctimestamp1),\n@@ -868,4 +868,4 @@ FROM alltypesorc_string\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@alltypesorc_string\n #### A masked pattern was here ####\n-2.8798560435897438E13\t8.970772952794215E19\t8.970772952794215E19\t9.206845925236167E19\t9.471416447815086E9\t9.471416447815086E9\t9.471416447815086E9\t9.595231068211004E9\n+2.8798560435897438E13\t8.970772952794214E19\t8.970772952794214E19\t9.206845925236167E19\t9.471416447815086E9\t9.471416447815086E9\t9.471416447815086E9\t9.595231068211004E9", "filename": "ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0d143bdc3815c1ac2f7c2b40206c24a935791cab", "parent": "https://github.com/apache/hive/commit/d2b453fc2e71b1f466b6de2201b218a69579cb70", "message": "Revert \"LLAP: NPE caused by HIVE-10397 (Prasanth Jayachandran)\"\n\nThis reverts commit 9c33b85497bdb693f16d15fc109d0f800bc4077b.", "bug_id": "hive_124", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/0d143bdc3815c1ac2f7c2b40206c24a935791cab/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java", "blob_url": "https://github.com/apache/hive/blob/0d143bdc3815c1ac2f7c2b40206c24a935791cab/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java", "sha": "e819b27c51bee1f402c477a33502cb8627bcb191", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java?ref=0d143bdc3815c1ac2f7c2b40206c24a935791cab", "patch": "@@ -94,10 +94,10 @@\n       InputSplit[] groupedSplits =\n           tezGrouper.getGroupedSplits(conf, rawSplits, bucketTaskMap.get(bucketId),\n               HiveInputFormat.class.getName(), splitSizeEstimator);\n-      String splitSizeEstStr = splitSizeEstimator == null ? \"null\" : splitSizeEstimator.getClass().getSimpleName();\n+\n       LOG.info(\"Original split size is \" + rawSplits.length + \" grouped split size is \"\n           + groupedSplits.length + \", for bucket: \" + bucketId + \" SplitSizeEstimator: \" +\n-          splitSizeEstStr);\n+          splitSizeEstimator.getClass().getSimpleName());\n \n       for (InputSplit inSplit : groupedSplits) {\n         bucketGroupedSplitMultimap.put(bucketId, inSplit);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5534653dba4a3dd13d514c3390c0fed01948a2cb", "parent": "https://github.com/apache/hive/commit/85fb4d23acc4ea976c65a59bd8d6c84d090eeee5", "message": "HIVE-10527: NPE in SparkUtilities::isDedicatedCluster [Spark Branch] (Rui reviewed by Jimmy)", "bug_id": "hive_125", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5534653dba4a3dd13d514c3390c0fed01948a2cb/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java", "blob_url": "https://github.com/apache/hive/blob/5534653dba4a3dd13d514c3390c0fed01948a2cb/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java", "sha": "21398d8d9d15278dca6a620a9ac2b4be71cedc85", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java?ref=5534653dba4a3dd13d514c3390c0fed01948a2cb", "patch": "@@ -112,6 +112,7 @@ public static HiveSparkClient createHiveSparkClient(HiveConf hiveconf)\n     String sparkMaster = hiveConf.get(\"spark.master\");\n     if (sparkMaster == null) {\n       sparkMaster = sparkConf.get(\"spark.master\");\n+      hiveConf.set(\"spark.master\", sparkMaster);\n     }\n     if (sparkMaster.equals(\"yarn-cluster\")) {\n       sparkConf.put(\"spark.yarn.maxAppAttempts\", \"1\");", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c547c94f5f44498dc766f475c19b73174c5795cb", "parent": "https://github.com/apache/hive/commit/fef58e7133fda276dffcf9cd085df118f717368a", "message": "HIVE-10470: LLAP: NPE in IO when returning 0 rows with no projection (Prasanth Jayachandran)", "bug_id": "hive_126", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c547c94f5f44498dc766f475c19b73174c5795cb/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/c547c94f5f44498dc766f475c19b73174c5795cb/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java", "sha": "016f47029db26e28692cc51734978d944f43271a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java?ref=c547c94f5f44498dc766f475c19b73174c5795cb", "patch": "@@ -323,7 +323,7 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n \n     if (listToRead.get() == null) {\n       // No data to read for this stripe. Check if we have some included index-only columns.\n-      if (hasIndexOnlyCols) {\n+      if (hasIndexOnlyCols && (includedRgs == null)) {\n         OrcEncodedColumnBatch ecb = ECB_POOL.take();\n         ecb.init(fileId, stripeIx, OrcEncodedColumnBatch.ALL_RGS, colRgs.length);\n         consumer.consumeData(ecb);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/c547c94f5f44498dc766f475c19b73174c5795cb/ql/src/test/queries/clientpositive/orc_llap.q", "blob_url": "https://github.com/apache/hive/blob/c547c94f5f44498dc766f475c19b73174c5795cb/ql/src/test/queries/clientpositive/orc_llap.q", "sha": "165904494e5c96f610a5814de64a6f88a44a7966", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_llap.q?ref=c547c94f5f44498dc766f475c19b73174c5795cb", "patch": "@@ -55,6 +55,8 @@ explain\n select count(1) from orc_llap_small y join orc_llap_small x;\n select count(1) from orc_llap_small y join orc_llap_small x;\n \n+-- All row groups pruned\n+select count(*) from orc_llap_small where cint < 60000000;\n \n -- Hash cannot be vectorized, so run hash as the last step on a temp table\n drop table llap_temp_table;", "filename": "ql/src/test/queries/clientpositive/orc_llap.q"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/c547c94f5f44498dc766f475c19b73174c5795cb/ql/src/test/results/clientpositive/orc_llap.q.out", "blob_url": "https://github.com/apache/hive/blob/c547c94f5f44498dc766f475c19b73174c5795cb/ql/src/test/results/clientpositive/orc_llap.q.out", "sha": "b71fb6bb325c2fa2a45345bc0de5bd383fc48c19", "changes": 11, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_llap.q.out?ref=c547c94f5f44498dc766f475c19b73174c5795cb", "patch": "@@ -212,6 +212,17 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@orc_llap_small\n #### A masked pattern was here ####\n 225\n+PREHOOK: query: -- All row groups pruned\n+select count(*) from orc_llap_small where cint < 60000000\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_llap_small\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- All row groups pruned\n+select count(*) from orc_llap_small where cint < 60000000\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_llap_small\n+#### A masked pattern was here ####\n+0\n PREHOOK: query: -- Hash cannot be vectorized, so run hash as the last step on a temp table\n drop table llap_temp_table\n PREHOOK: type: DROPTABLE", "filename": "ql/src/test/results/clientpositive/orc_llap.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/529814e3d0bfaefdf3f1625d014d9e05149f5ccf", "parent": "https://github.com/apache/hive/commit/c6aa5552c1bf5c0e0ecf06e338f5724fb6b93a58", "message": "HIVE-14303: CommonJoinOperator.checkAndGenObject should return directly at CLOSE state to avoid NPE if ExecReducer.close is called twice. (Zhihai Xu, reviewed by Xuefu Zhang)", "bug_id": "hive_127", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/529814e3d0bfaefdf3f1625d014d9e05149f5ccf/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/529814e3d0bfaefdf3f1625d014d9e05149f5ccf/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "sha": "5446c4058c8c30d2107c3076a6de0c18abed3d9b", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java?ref=529814e3d0bfaefdf3f1625d014d9e05149f5ccf", "patch": "@@ -682,6 +682,12 @@ private void genAllOneUniqueJoinObject()\n   }\n \n   protected void checkAndGenObject() throws HiveException {\n+    if (state == State.CLOSE) {\n+      LOG.warn(\"checkAndGenObject is called after operator \" +\n+          id + \" \" + getName() + \" closed\");\n+      return;\n+    }\n+\n     if (condn[0].getType() == JoinDesc.UNIQUE_JOIN) {\n \n       // Check if results need to be emitted.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cac5804de034ad54821e0524091cff0f4a97476b", "parent": "https://github.com/apache/hive/commit/0f1c112fc489fac3781470589517fd1025a306ed", "message": "HIVE-12684: NPE in stats annotation when all values in decimal column are NULLs (Prasanth Jayachandran reviewed by Pengcheng Xiong)", "bug_id": "hive_128", "file": [{"additions": 13, "raw_url": "https://github.com/apache/hive/raw/cac5804de034ad54821e0524091cff0f4a97476b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "blob_url": "https://github.com/apache/hive/blob/cac5804de034ad54821e0524091cff0f4a97476b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java", "sha": "2f78fe8995c00d16e340a4e36cffc4233db70cbd", "changes": 20, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java?ref=cac5804de034ad54821e0524091cff0f4a97476b", "patch": "@@ -709,13 +709,19 @@ public static ColStatistics getColStatistics(ColumnStatisticsObj cso, String tab\n       cs.setAvgColLen(JavaDataModel.get().lengthOfDecimal());\n       cs.setCountDistint(csd.getDecimalStats().getNumDVs());\n       cs.setNumNulls(csd.getDecimalStats().getNumNulls());\n-      Decimal val = csd.getDecimalStats().getHighValue();\n-      BigDecimal maxVal = HiveDecimal.\n-          create(new BigInteger(val.getUnscaled()), val.getScale()).bigDecimalValue();\n-      val = csd.getDecimalStats().getLowValue();\n-      BigDecimal minVal = HiveDecimal.\n-          create(new BigInteger(val.getUnscaled()), val.getScale()).bigDecimalValue();\n-      cs.setRange(minVal, maxVal);\n+      Decimal highValue = csd.getDecimalStats().getHighValue();\n+      Decimal lowValue = csd.getDecimalStats().getLowValue();\n+      if (highValue != null && highValue.getUnscaled() != null\n+          && lowValue != null && lowValue.getUnscaled() != null) {\n+        HiveDecimal maxHiveDec = HiveDecimal.create(new BigInteger(highValue.getUnscaled()), highValue.getScale());\n+        BigDecimal maxVal = maxHiveDec == null ? null : maxHiveDec.bigDecimalValue();\n+        HiveDecimal minHiveDec = HiveDecimal.create(new BigInteger(lowValue.getUnscaled()), lowValue.getScale());\n+        BigDecimal minVal = minHiveDec == null ? null : minHiveDec.bigDecimalValue();\n+\n+        if (minVal != null && maxVal != null) {\n+          cs.setRange(minVal, maxVal);\n+        }\n+      }\n     } else if (colTypeLowerCase.equals(serdeConstants.DATE_TYPE_NAME)) {\n       cs.setAvgColLen(JavaDataModel.get().lengthOfDate());\n     } else {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/cac5804de034ad54821e0524091cff0f4a97476b/ql/src/test/queries/clientpositive/decimal_stats.q", "blob_url": "https://github.com/apache/hive/blob/cac5804de034ad54821e0524091cff0f4a97476b/ql/src/test/queries/clientpositive/decimal_stats.q", "sha": "2370e7d39f24f1e9839c0e8f71db20d8be2bc77e", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/decimal_stats.q?ref=cac5804de034ad54821e0524091cff0f4a97476b", "patch": "@@ -0,0 +1,16 @@\n+set hive.stats.fetch.column.stats=true;\n+drop table if exists decimal_1;\n+\n+create table decimal_1 (t decimal(4,2), u decimal(5), v decimal);\n+\n+desc decimal_1;\n+\n+insert overwrite table decimal_1\n+  select cast('17.29' as decimal(4,2)), 3.1415926BD, null from src;\n+\n+analyze table decimal_1 compute statistics for columns;\n+\n+desc formatted decimal_1 v;\n+\n+explain select * from decimal_1 order by 1 limit 100;\n+drop table decimal_1;", "filename": "ql/src/test/queries/clientpositive/decimal_stats.q"}, {"additions": 106, "raw_url": "https://github.com/apache/hive/raw/cac5804de034ad54821e0524091cff0f4a97476b/ql/src/test/results/clientpositive/decimal_stats.q.out", "blob_url": "https://github.com/apache/hive/blob/cac5804de034ad54821e0524091cff0f4a97476b/ql/src/test/results/clientpositive/decimal_stats.q.out", "sha": "dabf7f8d22e86fee6b6a7c647d38725dd24968dc", "changes": 106, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_stats.q.out?ref=cac5804de034ad54821e0524091cff0f4a97476b", "patch": "@@ -0,0 +1,106 @@\n+PREHOOK: query: drop table if exists decimal_1\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists decimal_1\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table decimal_1 (t decimal(4,2), u decimal(5), v decimal)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@decimal_1\n+POSTHOOK: query: create table decimal_1 (t decimal(4,2), u decimal(5), v decimal)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@decimal_1\n+PREHOOK: query: desc decimal_1\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@decimal_1\n+POSTHOOK: query: desc decimal_1\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@decimal_1\n+t                   \tdecimal(4,2)        \t                    \n+u                   \tdecimal(5,0)        \t                    \n+v                   \tdecimal(10,0)       \t                    \n+PREHOOK: query: insert overwrite table decimal_1\n+  select cast('17.29' as decimal(4,2)), 3.1415926BD, null from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@decimal_1\n+POSTHOOK: query: insert overwrite table decimal_1\n+  select cast('17.29' as decimal(4,2)), 3.1415926BD, null from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@decimal_1\n+POSTHOOK: Lineage: decimal_1.t EXPRESSION []\n+POSTHOOK: Lineage: decimal_1.u EXPRESSION []\n+POSTHOOK: Lineage: decimal_1.v EXPRESSION []\n+PREHOOK: query: analyze table decimal_1 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_1\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table decimal_1 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_1\n+#### A masked pattern was here ####\n+PREHOOK: query: desc formatted decimal_1 v\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@decimal_1\n+POSTHOOK: query: desc formatted decimal_1 v\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@decimal_1\n+# col_name            \tdata_type           \tmin                 \tmax                 \tnum_nulls           \tdistinct_count      \tavg_col_len         \tmax_col_len         \tnum_trues           \tnum_falses          \tcomment             \n+\t \t \t \t \t \t \t \t \t \t \n+v                   \tdecimal(10,0)       \t                    \t                    \t500                 \t1                   \t                    \t                    \t                    \t                    \tfrom deserializer   \n+PREHOOK: query: explain select * from decimal_1 order by 1 limit 100\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select * from decimal_1 order by 1 limit 100\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: decimal_1\n+            Statistics: Num rows: 500 Data size: 112000 Basic stats: COMPLETE Column stats: COMPLETE\n+            Select Operator\n+              expressions: t (type: decimal(4,2)), u (type: decimal(5,0)), v (type: decimal(10,0))\n+              outputColumnNames: _col0, _col1, _col2\n+              Statistics: Num rows: 500 Data size: 112000 Basic stats: COMPLETE Column stats: COMPLETE\n+              Reduce Output Operator\n+                key expressions: 1 (type: int)\n+                sort order: +\n+                Statistics: Num rows: 500 Data size: 112000 Basic stats: COMPLETE Column stats: COMPLETE\n+                TopN Hash Memory Usage: 0.1\n+                value expressions: _col0 (type: decimal(4,2)), _col1 (type: decimal(5,0)), _col2 (type: decimal(10,0))\n+      Reduce Operator Tree:\n+        Select Operator\n+          expressions: VALUE._col0 (type: decimal(4,2)), VALUE._col1 (type: decimal(5,0)), VALUE._col2 (type: decimal(10,0))\n+          outputColumnNames: _col0, _col1, _col2\n+          Statistics: Num rows: 500 Data size: 112000 Basic stats: COMPLETE Column stats: COMPLETE\n+          Limit\n+            Number of rows: 100\n+            Statistics: Num rows: 100 Data size: 22400 Basic stats: COMPLETE Column stats: COMPLETE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 100 Data size: 22400 Basic stats: COMPLETE Column stats: COMPLETE\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: 100\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: drop table decimal_1\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@decimal_1\n+PREHOOK: Output: default@decimal_1\n+POSTHOOK: query: drop table decimal_1\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@decimal_1\n+POSTHOOK: Output: default@decimal_1", "filename": "ql/src/test/results/clientpositive/decimal_stats.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cd5147c7e759bce87c8af9370dbcf79fdc717eca", "parent": "https://github.com/apache/hive/commit/391335380893ffba7e7b85d1d97d17555c044b89", "message": "HIVE-14298: NPE could be thrown in HMS when an ExpressionTree could not be made from a filter (Chaoyu Tang, reviewed by Sergio Pena)", "bug_id": "hive_129", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/cd5147c7e759bce87c8af9370dbcf79fdc717eca/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java", "blob_url": "https://github.com/apache/hive/blob/cd5147c7e759bce87c8af9370dbcf79fdc717eca/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java", "sha": "8eeb1c4fa165b0e53257e999468824d68061edff", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java?ref=cd5147c7e759bce87c8af9370dbcf79fdc717eca", "patch": "@@ -984,6 +984,11 @@ private static String generateSqlFilter(Table table, ExpressionTree tree, List<O\n         List<String> joins, boolean dbHasJoinCastBug, String defaultPartName, DB dbType)\n             throws MetaException {\n       assert table != null;\n+      if (tree == null) {\n+        // consistent with other APIs like makeExpressionTree, null is returned to indicate that\n+        // the filter could not pushed down due to parsing issue etc\n+        return null;\n+      }\n       if (tree.getRoot() == null) {\n         return \"\";\n       }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cf0729c13699546f38e9392c2039fa9214347690", "parent": "https://github.com/apache/hive/commit/231247f35804bc918747a12ea630b30d33478ce2", "message": "HIVE-12208: Vectorized JOIN NPE on dynamically partitioned hash-join + map-join (Gunther Hagleitner, reviewed by Matt McCline)", "bug_id": "hive_130", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/cf0729c13699546f38e9392c2039fa9214347690/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/cf0729c13699546f38e9392c2039fa9214347690/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java", "sha": "cab0fc8a109c08c4345c5b1002175c2d52555a4a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java?ref=cf0729c13699546f38e9392c2039fa9214347690", "patch": "@@ -183,7 +183,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {\n \n   @SuppressWarnings(\"unchecked\")\n   @Override\n-  protected final void completeInitializationOp(Object[] os) throws HiveException {\n+  protected void completeInitializationOp(Object[] os) throws HiveException {\n     if (os.length != 0) {\n       Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> pair =\n           (Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]>) os[0];", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/cf0729c13699546f38e9392c2039fa9214347690/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java", "blob_url": "https://github.com/apache/hive/blob/cf0729c13699546f38e9392c2039fa9214347690/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java", "sha": "1d5a9de1053882c83c284456a17b54460e734ffb", "changes": 22, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java?ref=cf0729c13699546f38e9392c2039fa9214347690", "patch": "@@ -614,46 +614,34 @@ protected void initializeOp(Configuration hconf) throws HiveException {\n   }\n \n   @Override\n-  protected Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> loadHashTable(\n-      ExecMapperContext mapContext, MapredContext mrContext) throws HiveException {\n-\n-    Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> pair;\n+  protected void completeInitializationOp(Object[] os) throws HiveException {\n+    // setup mapJoinTables and serdes\n+    super.completeInitializationOp(os);\n \n     VectorMapJoinDesc vectorDesc = conf.getVectorDesc();\n     HashTableImplementationType hashTableImplementationType = vectorDesc.hashTableImplementationType();\n     switch (vectorDesc.hashTableImplementationType()) {\n     case OPTIMIZED:\n       {\n-        // Using Tez's HashTableLoader, create either a MapJoinBytesTableContainer or\n-        // HybridHashTableContainer.\n-        pair = super.loadHashTable(mapContext, mrContext);\n-\n         // Create our vector map join optimized hash table variation *above* the\n         // map join table container.\n-        MapJoinTableContainer[] mapJoinTableContainers = pair.getLeft();\n         vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf,\n-                mapJoinTableContainers[posSingleVectorMapJoinSmallTable]);\n+                mapJoinTables[posSingleVectorMapJoinSmallTable]);\n       }\n       break;\n \n     case FAST:\n       {\n-        // Use our VectorMapJoinFastHashTableLoader to create a VectorMapJoinTableContainer.\n-        pair = super.loadHashTable(mapContext, mrContext);\n-\n         // Get our vector map join fast hash table variation from the\n         // vector map join table container.\n-        MapJoinTableContainer[] mapJoinTableContainers = pair.getLeft();\n         VectorMapJoinTableContainer vectorMapJoinTableContainer =\n-                (VectorMapJoinTableContainer) mapJoinTableContainers[posSingleVectorMapJoinSmallTable];\n+                (VectorMapJoinTableContainer) mapJoinTables[posSingleVectorMapJoinSmallTable];\n         vectorMapJoinHashTable = vectorMapJoinTableContainer.vectorMapJoinHashTable();\n       }\n       break;\n     default:\n       throw new RuntimeException(\"Unknown vector map join hash table implementation type \" + hashTableImplementationType.name());\n     }\n-\n-    return pair;\n   }\n \n   /*", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/50177ef69486730c10ee9460870eefe51050826b", "parent": "https://github.com/apache/hive/commit/6577f55cd7f21568994638399f9c31bef578b5cc", "message": "HIVE-12202 NPE thrown when reading legacy ACID delta files(missed a file)(Elliot West via Eugene Koifman)", "bug_id": "hive_131", "file": [{"additions": 88, "raw_url": "https://github.com/apache/hive/raw/50177ef69486730c10ee9460870eefe51050826b/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/50177ef69486730c10ee9460870eefe51050826b/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "sha": "6a776701fc4228bb9cdfa16bcf2d027ca8456eb7", "changes": 88, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java?ref=50177ef69486730c10ee9460870eefe51050826b", "patch": "@@ -0,0 +1,88 @@\n+package org.apache.hadoop.hive.ql.io;\n+\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.Assert.assertThat;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import java.io.DataInput;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.hive.ql.io.AcidInputFormat.DeltaMetaData;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.Mock;\n+import org.mockito.runners.MockitoJUnitRunner;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class TestAcidInputFormat {\n+\n+  @Mock\n+  private DataInput mockDataInput;\n+\n+  @Test\n+  public void testDeltaMetaDataReadFieldsNoStatementIds() throws Exception {\n+    when(mockDataInput.readLong()).thenReturn(1L, 2L);\n+    when(mockDataInput.readInt()).thenReturn(0);\n+\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData();\n+    deltaMetaData.readFields(mockDataInput);\n+\n+    verify(mockDataInput, times(1)).readInt();\n+    assertThat(deltaMetaData.getMinTxnId(), is(1L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2L));\n+    assertThat(deltaMetaData.getStmtIds().isEmpty(), is(true));\n+  }\n+\n+  @Test\n+  public void testDeltaMetaDataReadFieldsWithStatementIds() throws Exception {\n+    when(mockDataInput.readLong()).thenReturn(1L, 2L);\n+    when(mockDataInput.readInt()).thenReturn(2, 100, 101);\n+\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData();\n+    deltaMetaData.readFields(mockDataInput);\n+\n+    verify(mockDataInput, times(3)).readInt();\n+    assertThat(deltaMetaData.getMinTxnId(), is(1L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2L));\n+    assertThat(deltaMetaData.getStmtIds().size(), is(2));\n+    assertThat(deltaMetaData.getStmtIds().get(0), is(100));\n+    assertThat(deltaMetaData.getStmtIds().get(1), is(101));\n+  }\n+\n+  @Test\n+  public void testDeltaMetaConstructWithState() throws Exception {\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, Arrays.asList(97, 98, 99));\n+\n+    assertThat(deltaMetaData.getMinTxnId(), is(2000L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2001L));\n+    assertThat(deltaMetaData.getStmtIds().size(), is(3));\n+    assertThat(deltaMetaData.getStmtIds().get(0), is(97));\n+    assertThat(deltaMetaData.getStmtIds().get(1), is(98));\n+    assertThat(deltaMetaData.getStmtIds().get(2), is(99));\n+  }\n+\n+  @Test\n+  public void testDeltaMetaDataReadFieldsWithStatementIdsResetsState() throws Exception {\n+    when(mockDataInput.readLong()).thenReturn(1L, 2L);\n+    when(mockDataInput.readInt()).thenReturn(2, 100, 101);\n+\n+    List<Integer> statementIds = new ArrayList<>();\n+    statementIds.add(97);\n+    statementIds.add(98);\n+    statementIds.add(99);\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, statementIds);\n+    deltaMetaData.readFields(mockDataInput);\n+\n+    verify(mockDataInput, times(3)).readInt();\n+    assertThat(deltaMetaData.getMinTxnId(), is(1L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2L));\n+    assertThat(deltaMetaData.getStmtIds().size(), is(2));\n+    assertThat(deltaMetaData.getStmtIds().get(0), is(100));\n+    assertThat(deltaMetaData.getStmtIds().get(1), is(101));\n+  }\n+\n+}", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692", "parent": "https://github.com/apache/hive/commit/fcc45db48ba0ad22a520ce5ec75e615d40a8c277", "message": "HIVE-11222 : LLAP: occasional NPE in parallel queries in ORC reader (Sergey Shelukhin, reviewed by Prasanth Jayachandran)", "bug_id": "hive_132", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java", "blob_url": "https://github.com/apache/hive/blob/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java", "sha": "5cf0780291687438b4ad396443854051660fd9d6", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java?ref=3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692", "patch": "@@ -550,7 +550,9 @@ private OrcFileMetadata getOrReadFileMetadata() throws IOException {\n           }\n           // Create new key object to reuse for gets; we've used the old one to put in cache.\n           stripeKey = new OrcBatchKey(fileId, 0, 0);\n-        } else {\n+        }\n+        // We might have got an old value from cache; recheck it has indexes.\n+        if (!value.hasAllIndexes(globalInc)) {\n           if (DebugUtils.isTraceOrcEnabled()) {\n             LlapIoImpl.LOG.info(\"Updating indexes in stripe \" + stripeKey.stripeIx\n                 + \" metadata for includes: \" + DebugUtils.toString(globalInc));", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "sha": "3b9856234a0a03b83b393685fca2ac7c7e0a50d8", "changes": 17, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692", "patch": "@@ -46,6 +46,7 @@\n import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n import org.apache.hadoop.hive.ql.io.filters.BloomFilterIO;\n+import org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry;\n import org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.ByteBufferAllocatorPool;\n import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n@@ -705,18 +706,24 @@ public SargApplier(SearchArgument sarg, String[] columnNames, long rowIndexStrid\n       boolean hasSelected = false, hasSkipped = false;\n       for (int rowGroup = 0; rowGroup < result.length; ++rowGroup) {\n         for (int pred = 0; pred < leafValues.length; ++pred) {\n-          if (filterColumns[pred] != -1) {\n-            OrcProto.ColumnStatistics stats =\n-                indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();\n+          int columnIx = filterColumns[pred];\n+          if (columnIx != -1) {\n+            if (indexes[columnIx] == null) {\n+              throw new AssertionError(\"Index is not populated for \" + columnIx);\n+            }\n+            RowIndexEntry entry = indexes[columnIx].getEntry(rowGroup);\n+            if (entry == null) {\n+              throw new AssertionError(\"RG is not populated for \" + columnIx + \" rg \" + rowGroup);\n+            }\n+            OrcProto.ColumnStatistics stats = entry.getStatistics();\n             OrcProto.BloomFilter bf = null;\n             if (bloomFilterIndices != null && bloomFilterIndices[filterColumns[pred]] != null) {\n               bf = bloomFilterIndices[filterColumns[pred]].getBloomFilter(rowGroup);\n             }\n             leafValues[pred] = evaluatePredicateProto(stats, sargLeaves.get(pred), bf);\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Stats = \" + stats);\n-              LOG.trace(\"Setting \" + sargLeaves.get(pred) + \" to \" +\n-                  leafValues[pred]);\n+              LOG.trace(\"Setting \" + sargLeaves.get(pred) + \" to \" + leafValues[pred]);\n             }\n           } else {\n             // the column is a virtual column", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a250582f11204def2523876b812f8996c5174d93", "parent": "https://github.com/apache/hive/commit/1369cb48c22ef165c26ee35103ca94e2fd13675a", "message": "HIVE-11011 : LLAP: test auto_sortmerge_join_5 on MiniTez fails with NPE (Sergey Shelukhin, reviewed by Vikram Dixit K)", "bug_id": "hive_133", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/a250582f11204def2523876b812f8996c5174d93/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/a250582f11204def2523876b812f8996c5174d93/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java", "sha": "4e7db0730531848e0613dc431c698cebd18fdc4b", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java?ref=a250582f11204def2523876b812f8996c5174d93", "patch": "@@ -642,7 +642,6 @@ public void initializeLocalWork(Configuration hconf) throws HiveException {\n       }\n     }\n     super.initializeLocalWork(hconf);\n-    return;\n   }\n \n   public boolean isBigTableWork() {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/a250582f11204def2523876b812f8996c5174d93/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java", "blob_url": "https://github.com/apache/hive/blob/a250582f11204def2523876b812f8996c5174d93/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java", "sha": "ee6af0490d00a14fdcb79d4216405e74ad6396c3", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java?ref=a250582f11204def2523876b812f8996c5174d93", "patch": "@@ -658,4 +658,8 @@ public void clearConnectedOperators() {\n   public void setConnectedOperators(int tag, DummyStoreOperator dummyOp) {\n     connectedOperators.put(tag, dummyOp);\n   }\n+\n+  public Map<Integer, DummyStoreOperator> getConnectedOperators() {\n+    return connectedOperators;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/a250582f11204def2523876b812f8996c5174d93/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "blob_url": "https://github.com/apache/hive/blob/a250582f11204def2523876b812f8996c5174d93/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "sha": "9ce8b8c7bfc5167d7f41ee876b1565aeaa363c2c", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java?ref=a250582f11204def2523876b812f8996c5174d93", "patch": "@@ -209,6 +209,8 @@ public Object call() {\n         }\n       }\n \n+      ((TezContext) (MapredContext.get())).setDummyOpsMap(mapOp.getConnectedOperators());\n+\n       // initialize map operator\n       mapOp.setConf(mapWork);\n       l4j.info(\"Main input name is \" + mapWork.getName());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ab095f0bc24447ab73843a1ae23a32f7b6c4bd1a", "parent": "https://github.com/apache/hive/commit/f9d1b6ab77ab15b8337c17fbe38557c1f7b5ce58", "message": "HIVE-13008 - WebHcat DDL commands in secure mode NPE when default FileSystem doesn't support delegation tokens (Eugene Koifman, reviewed by Chris Nauroth, Thejas Nair)", "bug_id": "hive_134", "file": [{"additions": 33, "raw_url": "https://github.com/apache/hive/raw/ab095f0bc24447ab73843a1ae23a32f7b6c4bd1a/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java", "blob_url": "https://github.com/apache/hive/blob/ab095f0bc24447ab73843a1ae23a32f7b6c4bd1a/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java", "sha": "13f3c9bd5e523e770dd8ccfd75a442bbbf93b680", "changes": 46, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java?ref=ab095f0bc24447ab73843a1ae23a32f7b6c4bd1a", "patch": "@@ -20,10 +20,14 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n import java.security.PrivilegedExceptionAction;\n+import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.commons.lang3.ArrayUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.apache.hadoop.conf.Configuration;\n@@ -79,7 +83,7 @@ public Path open(String user, Configuration conf)\n       this.user = user;\n       File t = File.createTempFile(\"templeton\", null);\n       tokenPath = new Path(t.toURI());\n-      Token fsToken = getFSDelegationToken(user, conf);\n+      Token[] fsToken = getFSDelegationToken(user, conf);\n       String hcatTokenStr;\n       try {\n         hcatTokenStr = buildHcatDelegationToken(user);\n@@ -130,30 +134,44 @@ public void addArgs(List<String> args) {\n     }\n   }\n \n-  class TokenWrapper {\n-    Token<?> token;\n+  private static class TokenWrapper {\n+    Token<?>[] tokens = new Token<?>[0];\n   }\n \n-  private Token<?> getFSDelegationToken(String user,\n+  private Token<?>[] getFSDelegationToken(String user,\n                       final Configuration conf)\n     throws IOException, InterruptedException {\n     LOG.info(\"user: \" + user + \" loginUser: \" + UserGroupInformation.getLoginUser().getUserName());\n     final UserGroupInformation ugi = UgiFactory.getUgi(user);\n \n     final TokenWrapper twrapper = new TokenWrapper();\n     ugi.doAs(new PrivilegedExceptionAction<Object>() {\n-      public Object run() throws IOException {\n-        FileSystem fs = FileSystem.get(conf);\n-        //todo: according to JavaDoc this seems like private API: addDelegationToken should be used\n-        twrapper.token = fs.getDelegationToken(ugi.getShortUserName());\n+      public Object run() throws IOException, URISyntaxException {\n+        Credentials creds = new Credentials();\n+        //get Tokens for default FS.  Not all FSs support delegation tokens, e.g. WASB\n+        collectTokens(FileSystem.get(conf), twrapper, creds, ugi.getShortUserName());\n+        //get tokens for all other known FSs since Hive tables may result in different ones\n+        //passing \"creds\" prevents duplicate tokens from being added\n+        Collection<String> URIs = conf.getStringCollection(\"mapreduce.job.hdfs-servers\");\n+        for(String uri : URIs) {\n+          LOG.debug(\"Getting tokens for \" + uri);\n+          collectTokens(FileSystem.get(new URI(uri), conf), twrapper, creds, ugi.getShortUserName());\n+        }\n         return null;\n       }\n     });\n-    return twrapper.token;\n-\n+    return twrapper.tokens;\n   }\n-\n-  private void writeProxyDelegationTokens(final Token<?> fsToken,\n+  private static void collectTokens(FileSystem fs, TokenWrapper twrapper, Credentials creds, String userName) throws IOException {\n+    Token[] tokens = fs.addDelegationTokens(userName, creds);\n+    if(tokens != null && tokens.length > 0) {\n+      twrapper.tokens = ArrayUtils.addAll(twrapper.tokens, tokens);\n+    }\n+  }\n+  /**\n+   * @param fsTokens not null\n+   */\n+  private void writeProxyDelegationTokens(final Token<?> fsTokens[],\n                       final Token<?> msToken,\n                       final Configuration conf,\n                       String user,\n@@ -168,7 +186,9 @@ private void writeProxyDelegationTokens(final Token<?> fsToken,\n     ugi.doAs(new PrivilegedExceptionAction<Object>() {\n       public Object run() throws IOException {\n         Credentials cred = new Credentials();\n-        cred.addToken(fsToken.getService(), fsToken);\n+        for(Token<?> fsToken : fsTokens) {\n+          cred.addToken(fsToken.getService(), fsToken);\n+        }\n         cred.addToken(msToken.getService(), msToken);\n         cred.writeTokenStorageFile(tokenPath, conf);\n         return null;", "filename": "hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/SecureProxySupport.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c156b32b49aeb5943e45a68fc7600c9244afb128", "parent": "https://github.com/apache/hive/commit/72088ca7c39136dd68dbbefb3897cd7abfdd982c", "message": "HIVE-10595 Dropping a table can cause NPEs in the compactor (Alan Gates, reviewed by Eugene Koifman)", "bug_id": "hive_135", "file": [{"additions": 19, "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "sha": "16d2c81333dbc2e3a74b28018933f127f9b146fb", "changes": 20, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128", "patch": "@@ -26,10 +26,12 @@\n import org.apache.hadoop.hive.common.ValidReadTxnList;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n import org.apache.hadoop.hive.metastore.api.ShowLocksRequest;\n import org.apache.hadoop.hive.metastore.api.ShowLocksResponse;\n import org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.hive.metastore.txn.CompactionInfo;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -183,7 +185,23 @@ public void run() {\n   private void clean(CompactionInfo ci) throws MetaException {\n     LOG.info(\"Starting cleaning for \" + ci.getFullPartitionName());\n     try {\n-      StorageDescriptor sd = resolveStorageDescriptor(resolveTable(ci), resolvePartition(ci));\n+      Table t = resolveTable(ci);\n+      if (t == null) {\n+        // The table was dropped before we got around to cleaning it.\n+        LOG.info(\"Unable to find table \" + ci.getFullTableName() + \", assuming it was dropped\");\n+        return;\n+      }\n+      Partition p = null;\n+      if (ci.partName != null) {\n+        p = resolvePartition(ci);\n+        if (p == null) {\n+          // The partition was dropped before we got around to cleaning it.\n+          LOG.info(\"Unable to find partition \" + ci.getFullPartitionName() +\n+              \", assuming it was dropped\");\n+          return;\n+        }\n+      }\n+      StorageDescriptor sd = resolveStorageDescriptor(t, p);\n       final String location = sd.getLocation();\n \n       // Create a bogus validTxnList with a high water mark set to MAX_LONG and no open", "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java", "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java", "sha": "38cd95ea305edd26bb425b1555ce125930aa3de3", "changes": 12, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128", "patch": "@@ -32,13 +32,13 @@\n import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.hive.metastore.txn.CompactionInfo;\n import org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler;\n-import org.apache.hadoop.hive.metastore.txn.TxnHandler;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n \n import java.io.IOException;\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n@@ -105,13 +105,15 @@ protected Table resolveTable(CompactionInfo ci) throws MetaException {\n    * one partition.\n    */\n   protected Partition resolvePartition(CompactionInfo ci) throws Exception {\n-    Partition p = null;\n     if (ci.partName != null) {\n-      List<String> names = new ArrayList<String>(1);\n-      names.add(ci.partName);\n       List<Partition> parts = null;\n       try {\n-        parts = rs.getPartitionsByNames(ci.dbname, ci.tableName, names);\n+        parts = rs.getPartitionsByNames(ci.dbname, ci.tableName,\n+            Collections.singletonList(ci.partName));\n+        if (parts == null || parts.size() == 0) {\n+          // The partition got dropped before we went looking for it.\n+          return null;\n+        }\n       } catch (Exception e) {\n         LOG.error(\"Unable to find partition \" + ci.getFullPartitionName() + \", \" + e.getMessage());\n         throw e;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "sha": "847d75199d6d614bd17ea852a4e3e87bf6911be7", "changes": 11, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128", "patch": "@@ -85,13 +85,13 @@ public void run() {\n           LOG.debug(\"Found \" + potentials.size() + \" potential compactions, \" +\n               \"checking to see if we should compact any of them\");\n           for (CompactionInfo ci : potentials) {\n-            LOG.debug(\"Checking to see if we should compact \" + ci.getFullPartitionName());\n+            LOG.info(\"Checking to see if we should compact \" + ci.getFullPartitionName());\n             try {\n               Table t = resolveTable(ci);\n               if (t == null) {\n                 // Most likely this means it's a temp table\n-                LOG.debug(\"Can't find table \" + ci.getFullTableName() + \", assuming it's a temp \" +\n-                    \"table and moving on.\");\n+                LOG.info(\"Can't find table \" + ci.getFullTableName() + \", assuming it's a temp \" +\n+                    \"table or has been dropped and moving on.\");\n                 continue;\n               }\n \n@@ -121,6 +121,11 @@ public void run() {\n \n               // Figure out who we should run the file operations as\n               Partition p = resolvePartition(ci);\n+              if (p == null && ci.partName != null) {\n+                LOG.info(\"Can't find partition \" + ci.getFullPartitionName() +\n+                    \", assuming it has been dropped and moving on.\");\n+                continue;\n+              }\n               StorageDescriptor sd = resolveStorageDescriptor(t, p);\n               String runAs = findUserToRunAs(sd.getLocation(), t);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "sha": "f26225a72c34252c8fdf615bd34b59532376c5de", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128", "patch": "@@ -94,6 +94,12 @@ public void run() {\n         Table t1 = null;\n         try {\n           t1 = resolveTable(ci);\n+          if (t1 == null) {\n+            LOG.info(\"Unable to find table \" + ci.getFullTableName() +\n+                \", assuming it was dropped and moving on.\");\n+            txnHandler.markCleaned(ci);\n+            continue;\n+          }\n         } catch (MetaException e) {\n           txnHandler.markCleaned(ci);\n           continue;\n@@ -106,6 +112,12 @@ public void run() {\n         Partition p = null;\n         try {\n           p = resolvePartition(ci);\n+          if (p == null && ci.partName != null) {\n+            LOG.info(\"Unable to find partition \" + ci.getFullPartitionName() +\n+                \", assuming it was dropped and moving on.\");\n+            txnHandler.markCleaned(ci);\n+            continue;\n+          }\n         } catch (Exception e) {\n           txnHandler.markCleaned(ci);\n           continue;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java"}, {"additions": 54, "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "sha": "ffdbb9aacf87610d1c340e97fb715e9c1c25152d", "changes": 56, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128", "patch": "@@ -17,17 +17,17 @@\n  */\n package org.apache.hadoop.hive.ql.txn.compactor;\n \n-import junit.framework.Assert;\n+import org.junit.Assert;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.MetaStoreThread;\n import org.apache.hadoop.hive.metastore.api.*;\n import org.apache.hadoop.hive.metastore.txn.CompactionInfo;\n import org.junit.Test;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n@@ -428,4 +428,56 @@ public void cleanupAfterMajorPartitionCompactionNoBase() throws Exception {\n     Assert.assertEquals(1, paths.size());\n     Assert.assertEquals(\"base_25\", paths.get(0).getName());\n   }\n+\n+  @Test\n+  public void droppedTable() throws Exception {\n+    Table t = newTable(\"default\", \"dt\", false);\n+\n+    addDeltaFile(t, null, 1L, 22L, 22);\n+    addDeltaFile(t, null, 23L, 24L, 2);\n+    addBaseFile(t, null, 25L, 25);\n+\n+    burnThroughTransactions(25);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dt\", CompactionType.MINOR);\n+    txnHandler.compact(rqst);\n+    CompactionInfo ci = txnHandler.findNextToCompact(\"fred\");\n+    txnHandler.markCompacted(ci);\n+    txnHandler.setRunAs(ci.id, System.getProperty(\"user.name\"));\n+\n+    ms.dropTable(\"default\", \"dt\");\n+\n+    startCleaner();\n+\n+    // Check there are no compactions requests left.\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(0, rsp.getCompactsSize());\n+  }\n+\n+  @Test\n+  public void droppedPartition() throws Exception {\n+    Table t = newTable(\"default\", \"dp\", true);\n+    Partition p = newPartition(t, \"today\");\n+\n+    addDeltaFile(t, p, 1L, 22L, 22);\n+    addDeltaFile(t, p, 23L, 24L, 2);\n+    addBaseFile(t, p, 25L, 25);\n+\n+    burnThroughTransactions(25);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dp\", CompactionType.MAJOR);\n+    rqst.setPartitionname(\"ds=today\");\n+    txnHandler.compact(rqst);\n+    CompactionInfo ci = txnHandler.findNextToCompact(\"fred\");\n+    txnHandler.markCompacted(ci);\n+    txnHandler.setRunAs(ci.id, System.getProperty(\"user.name\"));\n+\n+    ms.dropPartition(\"default\", \"dp\", Collections.singletonList(\"today\"), true);\n+\n+    startCleaner();\n+\n+    // Check there are no compactions requests left.\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(0, rsp.getCompactsSize());\n+  }\n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java"}, {"additions": 62, "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java", "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java", "sha": "00b13de0992eee9b1aa7144bb2f9ae6549e4e55e", "changes": 63, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128", "patch": "@@ -17,7 +17,7 @@\n  */\n package org.apache.hadoop.hive.ql.txn.compactor;\n \n-import junit.framework.Assert;\n+import org.junit.Assert;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.conf.HiveConf;\n@@ -27,6 +27,7 @@\n import org.junit.Test;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -653,4 +654,64 @@ public void noCompactTableDynamicPartitioning() throws Exception {\n     Assert.assertEquals(0, compacts.size());\n   }\n \n+  @Test\n+  public void dropTable() throws Exception {\n+    Table t = newTable(\"default\", \"dt\", false);\n+\n+    addBaseFile(t, null, 20L, 20);\n+    addDeltaFile(t, null, 21L, 22L, 2);\n+    addDeltaFile(t, null, 23L, 24L, 2);\n+\n+    burnThroughTransactions(23);\n+\n+    long txnid = openTxn();\n+    LockComponent comp = new LockComponent(LockType.SHARED_WRITE, LockLevel.PARTITION, \"default\");\n+    comp.setTablename(\"dt\");\n+    List<LockComponent> components = new ArrayList<LockComponent>(1);\n+    components.add(comp);\n+    LockRequest req = new LockRequest(components, \"me\", \"localhost\");\n+    req.setTxnid(txnid);\n+    LockResponse res = txnHandler.lock(req);\n+    txnHandler.commitTxn(new CommitTxnRequest(txnid));\n+\n+    ms.dropTable(\"default\", \"dt\");\n+\n+    startInitiator();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n+\n+  @Test\n+  public void dropPartition() throws Exception {\n+    Table t = newTable(\"default\", \"dp\", true);\n+    Partition p = newPartition(t, \"today\");\n+\n+    addBaseFile(t, p, 20L, 20);\n+    addDeltaFile(t, p, 21L, 22L, 2);\n+    addDeltaFile(t, p, 23L, 24L, 2);\n+\n+    burnThroughTransactions(23);\n+\n+    long txnid = openTxn();\n+    LockComponent comp = new LockComponent(LockType.SHARED_WRITE, LockLevel.PARTITION, \"default\");\n+    comp.setTablename(\"dp\");\n+    comp.setPartitionname(\"ds=today\");\n+    List<LockComponent> components = new ArrayList<LockComponent>(1);\n+    components.add(comp);\n+    LockRequest req = new LockRequest(components, \"me\", \"localhost\");\n+    req.setTxnid(txnid);\n+    LockResponse res = txnHandler.lock(req);\n+    txnHandler.commitTxn(new CommitTxnRequest(txnid));\n+\n+    ms.dropPartition(\"default\", \"dp\", Collections.singletonList(\"today\"), true);\n+\n+    startInitiator();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n+\n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java"}, {"additions": 45, "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java", "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java", "sha": "bebac541265a32d518e634f09b347b4ea4416f00", "changes": 45, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128", "patch": "@@ -29,6 +29,7 @@\n import java.io.*;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -799,4 +800,48 @@ public void majorWithAborted() throws Exception {\n     Assert.assertEquals(\"delta_23_25\", stat[3].getPath().getName());\n     Assert.assertEquals(\"delta_26_27\", stat[4].getPath().getName());\n   }\n+\n+  @Test\n+  public void droppedTable() throws Exception {\n+    Table t = newTable(\"default\", \"dt\", false);\n+\n+    addDeltaFile(t, null, 1L, 2L, 2);\n+    addDeltaFile(t, null, 3L, 4L, 2);\n+    burnThroughTransactions(4);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dt\", CompactionType.MAJOR);\n+    txnHandler.compact(rqst);\n+\n+    ms.dropTable(\"default\", \"dt\");\n+\n+    startWorker();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n+\n+  @Test\n+  public void droppedPartition() throws Exception {\n+    Table t = newTable(\"default\", \"dp\", true);\n+    Partition p = newPartition(t, \"today\");\n+\n+    addBaseFile(t, p, 20L, 20);\n+    addDeltaFile(t, p, 21L, 22L, 2);\n+    addDeltaFile(t, p, 23L, 24L, 2);\n+\n+    burnThroughTransactions(25);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dp\", CompactionType.MINOR);\n+    rqst.setPartitionname(\"ds=today\");\n+    txnHandler.compact(rqst);\n+\n+    ms.dropPartition(\"default\", \"dp\", Collections.singletonList(\"today\"), true);\n+\n+    startWorker();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/31591bf1833fb3858c599addbe53aa57ac381ba4", "parent": "https://github.com/apache/hive/commit/aaa112dbfd0b52285925dad4160f34487b8dfd82", "message": "HIVE-10428 : NPE in RegexSerDe using HCat (Jason Dere via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_136", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/31591bf1833fb3858c599addbe53aa57ac381ba4/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java", "blob_url": "https://github.com/apache/hive/blob/31591bf1833fb3858c599addbe53aa57ac381ba4/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java", "sha": "31001814097966b2b40ccebd4300322939e38ea5", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java?ref=31591bf1833fb3858c599addbe53aa57ac381ba4", "patch": "@@ -164,6 +164,8 @@ private static Properties getSerdeProperties(HCatTableInfo info, HCatSchema s)\n       MetaStoreUtils.getColumnNamesFromFieldSchema(fields));\n     props.setProperty(org.apache.hadoop.hive.serde.serdeConstants.LIST_COLUMN_TYPES,\n       MetaStoreUtils.getColumnTypesFromFieldSchema(fields));\n+    props.setProperty(\"columns.comments\",\n+      MetaStoreUtils.getColumnCommentsFromFieldSchema(fields));\n \n     // setting these props to match LazySimpleSerde\n     props.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, \"\\\\N\");", "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/31591bf1833fb3858c599addbe53aa57ac381ba4/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java", "blob_url": "https://github.com/apache/hive/blob/31591bf1833fb3858c599addbe53aa57ac381ba4/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java", "sha": "38dc4062f8c5f485618f67a38c9c58b742d57438", "changes": 11, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java?ref=31591bf1833fb3858c599addbe53aa57ac381ba4", "patch": "@@ -1115,6 +1115,17 @@ public static String getColumnTypesFromFieldSchema(\n     return sb.toString();\n   }\n \n+  public static String getColumnCommentsFromFieldSchema(List<FieldSchema> fieldSchemas) {\n+    StringBuilder sb = new StringBuilder();\n+    for (int i = 0; i < fieldSchemas.size(); i++) {\n+      if (i > 0) {\n+        sb.append(SerDeUtils.COLUMN_COMMENTS_DELIMITER);\n+      }\n+      sb.append(fieldSchemas.get(i).getComment());\n+    }\n+    return sb.toString();\n+  }\n+\n   public static void makeDir(Path path, HiveConf hiveConf) throws MetaException {\n     FileSystem fs;\n     try {", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/31591bf1833fb3858c599addbe53aa57ac381ba4/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java", "blob_url": "https://github.com/apache/hive/blob/31591bf1833fb3858c599addbe53aa57ac381ba4/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java", "sha": "8dada5a324545bf80e26c5766545ff735f7fce40", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java?ref=31591bf1833fb3858c599addbe53aa57ac381ba4", "patch": "@@ -62,6 +62,7 @@\n   public static final char QUOTE = '\"';\n   public static final char COLON = ':';\n   public static final char COMMA = ',';\n+  public static final char COLUMN_COMMENTS_DELIMITER = '\\0';\n   public static final String LBRACKET = \"[\";\n   public static final String RBRACKET = \"]\";\n   public static final String LBRACE = \"{\";", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/46d8a07469d617463b5173c8983aeff5e564d554", "parent": "https://github.com/apache/hive/commit/c307716392754169451954b77353a3aec3fc21ef", "message": "HIVE-13065: Hive throws NPE when writing map type data to a HBase backed table (Yongzhi Chen, reviewed by Aihua Xu)", "bug_id": "hive_137", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/46d8a07469d617463b5173c8983aeff5e564d554/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java", "blob_url": "https://github.com/apache/hive/blob/46d8a07469d617463b5173c8983aeff5e564d554/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java", "sha": "c6f3b0ff3abfd78bb4139eb471beeb0b66211f2a", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java?ref=46d8a07469d617463b5173c8983aeff5e564d554", "patch": "@@ -271,8 +271,11 @@ private boolean serialize(\n               ss.write(sep);\n             }\n             serialize(entry.getKey(), koi, level+2, ss);\n-            ss.write(keyValueSeparator);\n-            serialize(entry.getValue(), voi, level+2, ss);\n+\n+            if ( entry.getValue() != null) {\n+              ss.write(keyValueSeparator);\n+              serialize(entry.getValue(), voi, level+2, ss);\n+            }\n           }\n         }\n         return true;", "filename": "hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseRowSerializer.java"}, {"additions": 26, "raw_url": "https://github.com/apache/hive/raw/46d8a07469d617463b5173c8983aeff5e564d554/hbase-handler/src/test/queries/positive/hbase_queries.q", "blob_url": "https://github.com/apache/hive/blob/46d8a07469d617463b5173c8983aeff5e564d554/hbase-handler/src/test/queries/positive/hbase_queries.q", "sha": "49fa829eed4b860908f1ff4cf8360c846f3d69d5", "changes": 26, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/queries/positive/hbase_queries.q?ref=46d8a07469d617463b5173c8983aeff5e564d554", "patch": "@@ -164,6 +164,30 @@ SELECT COUNT(*) FROM hbase_table_1_like;\n \n SHOW CREATE TABLE hbase_table_1_like;\n \n+DROP TABLE IF EXISTS hbase_table_9;\n+CREATE TABLE hbase_table_9 (id bigint, data map<string, string>, str string)\n+stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+with serdeproperties (\"hbase.columns.mapping\" = \":key,cf:map_col#s:s,cf:str_col\");\n+\n+insert overwrite table hbase_table_9 select 1 as id, map('abcd', null) as data , null as str from src limit 1;\n+insert into table hbase_table_9 select 2 as id, map('efgh', null) as data , '1234' as str from src limit 1;\n+insert into table hbase_table_9 select 3 as id, map('hij', '') as data , '1234' as str from src limit 1;\n+insert into table hbase_table_9 select 4 as id, map('klm', 'avalue') as data , '1234' as str from src limit 1;\n+insert into table hbase_table_9 select 5 as id, map('key1',null, 'key2', 'avalue') as data , '1234' as str from src limit 1;\n+select * from hbase_table_9;\n+\n+DROP TABLE IF EXISTS hbase_table_10;\n+CREATE TABLE hbase_table_10 (id bigint, data map<int, int>, str string)\n+stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+with serdeproperties (\"hbase.columns.mapping\" = \":key,cf:map_col2,cf:str2_col\");\n+insert overwrite table hbase_table_10 select 1 as id, map(10, cast(null as int)) as data , null as str from src limit 1;\n+insert into table hbase_table_10 select 2 as id, map(20, cast(null as int)) as data , '1234' as str from src limit 1;\n+insert into table hbase_table_10 select 3 as id, map(30, 31) as data , '1234' as str from src limit 1;\n+insert into table hbase_table_10 select 4 as id, map(40, cast(null as int), 45, cast(null as int)) as data , '1234' as str from src limit 1;\n+insert into table hbase_table_10 select 5 as id, map(50,cast(null as int), 55, 58) as data , '1234' as str from src limit 1;\n+select * from hbase_table_10;\n+\n+\n DROP TABLE hbase_table_1;\n DROP TABLE hbase_table_1_like;\n DROP TABLE hbase_table_2;\n@@ -176,3 +200,5 @@ DROP TABLE hbase_table_7;\n DROP TABLE hbase_table_8;\n DROP TABLE empty_hbase_table;\n DROP TABLE empty_normal_table;\n+DROP TABLE hbase_table_9;\n+DROP TABLE hbase_table_10;", "filename": "hbase-handler/src/test/queries/positive/hbase_queries.q"}, {"additions": 154, "raw_url": "https://github.com/apache/hive/raw/46d8a07469d617463b5173c8983aeff5e564d554/hbase-handler/src/test/results/positive/hbase_queries.q.out", "blob_url": "https://github.com/apache/hive/blob/46d8a07469d617463b5173c8983aeff5e564d554/hbase-handler/src/test/results/positive/hbase_queries.q.out", "sha": "a99f561828fb8466a70ad639e73aaf65ac199b72", "changes": 154, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/results/positive/hbase_queries.q.out?ref=46d8a07469d617463b5173c8983aeff5e564d554", "patch": "@@ -919,6 +919,144 @@ WITH SERDEPROPERTIES (\n TBLPROPERTIES (\n   'hbase.table.name'='hbase_table_0', \n #### A masked pattern was here ####\n+PREHOOK: query: DROP TABLE IF EXISTS hbase_table_9\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS hbase_table_9\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE hbase_table_9 (id bigint, data map<string, string>, str string)\n+stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+with serdeproperties (\"hbase.columns.mapping\" = \":key,cf:map_col#s:s,cf:str_col\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@hbase_table_9\n+POSTHOOK: query: CREATE TABLE hbase_table_9 (id bigint, data map<string, string>, str string)\n+stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+with serdeproperties (\"hbase.columns.mapping\" = \":key,cf:map_col#s:s,cf:str_col\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@hbase_table_9\n+PREHOOK: query: insert overwrite table hbase_table_9 select 1 as id, map('abcd', null) as data , null as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_9\n+POSTHOOK: query: insert overwrite table hbase_table_9 select 1 as id, map('abcd', null) as data , null as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_9\n+PREHOOK: query: insert into table hbase_table_9 select 2 as id, map('efgh', null) as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_9\n+POSTHOOK: query: insert into table hbase_table_9 select 2 as id, map('efgh', null) as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_9\n+PREHOOK: query: insert into table hbase_table_9 select 3 as id, map('hij', '') as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_9\n+POSTHOOK: query: insert into table hbase_table_9 select 3 as id, map('hij', '') as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_9\n+PREHOOK: query: insert into table hbase_table_9 select 4 as id, map('klm', 'avalue') as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_9\n+POSTHOOK: query: insert into table hbase_table_9 select 4 as id, map('klm', 'avalue') as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_9\n+PREHOOK: query: insert into table hbase_table_9 select 5 as id, map('key1',null, 'key2', 'avalue') as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_9\n+POSTHOOK: query: insert into table hbase_table_9 select 5 as id, map('key1',null, 'key2', 'avalue') as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_9\n+PREHOOK: query: select * from hbase_table_9\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@hbase_table_9\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from hbase_table_9\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@hbase_table_9\n+#### A masked pattern was here ####\n+1\t{\"abcd\":null}\tNULL\n+2\t{\"efgh\":null}\t1234\n+3\t{\"hij\":\"\"}\t1234\n+4\t{\"klm\":\"avalue\"}\t1234\n+5\t{\"key1\":null,\"key2\":\"avalue\"}\t1234\n+PREHOOK: query: DROP TABLE IF EXISTS hbase_table_10\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS hbase_table_10\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE hbase_table_10 (id bigint, data map<int, int>, str string)\n+stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+with serdeproperties (\"hbase.columns.mapping\" = \":key,cf:map_col2,cf:str2_col\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@hbase_table_10\n+POSTHOOK: query: CREATE TABLE hbase_table_10 (id bigint, data map<int, int>, str string)\n+stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+with serdeproperties (\"hbase.columns.mapping\" = \":key,cf:map_col2,cf:str2_col\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@hbase_table_10\n+PREHOOK: query: insert overwrite table hbase_table_10 select 1 as id, map(10, cast(null as int)) as data , null as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_10\n+POSTHOOK: query: insert overwrite table hbase_table_10 select 1 as id, map(10, cast(null as int)) as data , null as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_10\n+PREHOOK: query: insert into table hbase_table_10 select 2 as id, map(20, cast(null as int)) as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_10\n+POSTHOOK: query: insert into table hbase_table_10 select 2 as id, map(20, cast(null as int)) as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_10\n+PREHOOK: query: insert into table hbase_table_10 select 3 as id, map(30, 31) as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_10\n+POSTHOOK: query: insert into table hbase_table_10 select 3 as id, map(30, 31) as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_10\n+PREHOOK: query: insert into table hbase_table_10 select 4 as id, map(40, cast(null as int), 45, cast(null as int)) as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_10\n+POSTHOOK: query: insert into table hbase_table_10 select 4 as id, map(40, cast(null as int), 45, cast(null as int)) as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_10\n+PREHOOK: query: insert into table hbase_table_10 select 5 as id, map(50,cast(null as int), 55, 58) as data , '1234' as str from src limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@hbase_table_10\n+POSTHOOK: query: insert into table hbase_table_10 select 5 as id, map(50,cast(null as int), 55, 58) as data , '1234' as str from src limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@hbase_table_10\n+PREHOOK: query: select * from hbase_table_10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@hbase_table_10\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from hbase_table_10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@hbase_table_10\n+#### A masked pattern was here ####\n+1\t{10:null}\tNULL\n+2\t{20:null}\t1234\n+3\t{30:31}\t1234\n+4\t{40:null,45:null}\t1234\n+5\t{50:null,55:58}\t1234\n PREHOOK: query: DROP TABLE hbase_table_1\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@hbase_table_1\n@@ -1015,3 +1153,19 @@ POSTHOOK: query: DROP TABLE empty_normal_table\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@empty_normal_table\n POSTHOOK: Output: default@empty_normal_table\n+PREHOOK: query: DROP TABLE hbase_table_9\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@hbase_table_9\n+PREHOOK: Output: default@hbase_table_9\n+POSTHOOK: query: DROP TABLE hbase_table_9\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@hbase_table_9\n+POSTHOOK: Output: default@hbase_table_9\n+PREHOOK: query: DROP TABLE hbase_table_10\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@hbase_table_10\n+PREHOOK: Output: default@hbase_table_10\n+POSTHOOK: query: DROP TABLE hbase_table_10\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@hbase_table_10\n+POSTHOOK: Output: default@hbase_table_10", "filename": "hbase-handler/src/test/results/positive/hbase_queries.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/eb2b7b81f815238cc2b67d701d45aa7618fc8d13", "parent": "https://github.com/apache/hive/commit/c8f15f7b802fa0c2f2426a3b29093aba4aebc57f", "message": "HIVE-12798 : CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver.vector* queries failures due to NPE in Vectorizer.onExpressionHasNullSafes() (Hari Subramaniyan, reviewed by Matt McCline )", "bug_id": "hive_138", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/eb2b7b81f815238cc2b67d701d45aa7618fc8d13/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java", "blob_url": "https://github.com/apache/hive/blob/eb2b7b81f815238cc2b67d701d45aa7618fc8d13/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java", "sha": "ee080aad9c365aa73be4c518fc136e43158edf27", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java?ref=eb2b7b81f815238cc2b67d701d45aa7618fc8d13", "patch": "@@ -1911,6 +1911,9 @@ private boolean isBigTableOnlyResults(MapJoinDesc desc) {\n \n   private boolean onExpressionHasNullSafes(MapJoinDesc desc) {\n     boolean[] nullSafes = desc.getNullSafes();\n+    if (nullSafes == null) {\n+\treturn false;\n+    }\n     for (boolean nullSafe : nullSafes) {\n       if (nullSafe) {\n         return true;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/3c1eae0c44450ebbf26c88066de871ea7f479981", "parent": "https://github.com/apache/hive/commit/1030ae719e2bbea47d8f2835244589621ab126fb", "message": "HIVE-11596: nvl(x, y) throws NPE if type x and type y doesn't match, rather than throwing the meaningful error (Aihua Xu, reviewed by Chao Sun)", "bug_id": "hive_139", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBasePad.java", "blob_url": "https://github.com/apache/hive/blob/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBasePad.java", "sha": "33839f76df0c2edc3d5f6c586507dd79921b343f", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBasePad.java?ref=3c1eae0c44450ebbf26c88066de871ea7f479981", "patch": "@@ -94,8 +94,8 @@ protected abstract void performOp(byte[] data, byte[] txt, byte[] padTxt, int le\n   private Converter checkTextArguments(ObjectInspector[] arguments, int i)\n     throws UDFArgumentException {\n     if (arguments[i].getCategory() != ObjectInspector.Category.PRIMITIVE) {\n-      throw new UDFArgumentTypeException(i + 1, \"Only primitive type arguments are accepted but \"\n-      + arguments[i].getTypeName() + \" is passed. as  arguments\");\n+      throw new UDFArgumentTypeException(i, \"Only primitive type arguments are accepted but \"\n+      + arguments[i].getTypeName() + \" is passed.\");\n     }\n \n     Converter converter = ObjectInspectorConverters.getConverter((PrimitiveObjectInspector) arguments[i],\n@@ -107,8 +107,8 @@ private Converter checkTextArguments(ObjectInspector[] arguments, int i)\n   private Converter checkIntArguments(ObjectInspector[] arguments, int i)\n     throws UDFArgumentException {\n     if (arguments[i].getCategory() != ObjectInspector.Category.PRIMITIVE) {\n-      throw new UDFArgumentTypeException(i + 1, \"Only primitive type arguments are accepted but \"\n-      + arguments[i].getTypeName() + \" is passed. as  arguments\");\n+      throw new UDFArgumentTypeException(i, \"Only primitive type arguments are accepted but \"\n+      + arguments[i].getTypeName() + \" is passed.\");\n     }\n     PrimitiveCategory inputType = ((PrimitiveObjectInspector) arguments[i]).getPrimitiveCategory();\n     Converter converter;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBasePad.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java", "blob_url": "https://github.com/apache/hive/blob/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java", "sha": "0a16da88e0d5990b3d742b329934c5ab9fff5a68", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java?ref=3c1eae0c44450ebbf26c88066de871ea7f479981", "patch": "@@ -42,7 +42,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n     returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);\n     if (!(returnOIResolver.update(arguments[0]) && returnOIResolver\n         .update(arguments[1]))) {\n-      throw new UDFArgumentTypeException(2,\n+      throw new UDFArgumentTypeException(1,\n           \"The first and seconds arguments of function NLV should have the same type, \"\n           + \"but they are different: \\\"\" + arguments[0].getTypeName()\n           + \"\\\" and \\\"\" + arguments[1].getTypeName() + \"\\\"\");", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/queries/clientnegative/nvl_mismatch_type.q", "blob_url": "https://github.com/apache/hive/blob/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/queries/clientnegative/nvl_mismatch_type.q", "sha": "ace180c5c94bee16180a67528f72ef5c2d78d082", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/nvl_mismatch_type.q?ref=3c1eae0c44450ebbf26c88066de871ea7f479981", "patch": "@@ -0,0 +1,20 @@\n+\n+create table over1k(\n+           t tinyint,\n+           si smallint,\n+           i int,\n+           b bigint,\n+           f float,\n+           d double,\n+           bo boolean,\n+           s string,\n+           ts timestamp,\n+           dec decimal(4,2),\n+           bin binary)\n+       row format delimited\n+       fields terminated by '|';\n+\n+load data local inpath '../../data/files/over1k' into table over1k;\n+\n+-- Integers\n+select nvl(t, true) from over1k limit 5;", "filename": "ql/src/test/queries/clientnegative/nvl_mismatch_type.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/char_pad_convert_fail0.q.out", "blob_url": "https://github.com/apache/hive/blob/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/char_pad_convert_fail0.q.out", "sha": "ed2f516311fa0347213d848345958fbe61bc1701", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/char_pad_convert_fail0.q.out?ref=3c1eae0c44450ebbf26c88066de871ea7f479981", "patch": "@@ -40,4 +40,4 @@ POSTHOOK: query: load data local inpath '../../data/files/over1k' into table ove\n POSTHOOK: type: LOAD\n #### A masked pattern was here ####\n POSTHOOK: Output: default@over1k\n-FAILED: SemanticException [Error 10016]: Line 7:29 Argument type mismatch '' '': Only primitive type arguments are accepted but array<int> is passed. as  arguments\n+FAILED: SemanticException [Error 10016]: Line 7:15 Argument type mismatch '3': Only primitive type arguments are accepted but array<int> is passed.", "filename": "ql/src/test/results/clientnegative/char_pad_convert_fail0.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/char_pad_convert_fail1.q.out", "blob_url": "https://github.com/apache/hive/blob/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/char_pad_convert_fail1.q.out", "sha": "5f17cfb3738ccc75081bb35fc7d3699515067f69", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/char_pad_convert_fail1.q.out?ref=3c1eae0c44450ebbf26c88066de871ea7f479981", "patch": "@@ -40,4 +40,4 @@ POSTHOOK: query: load data local inpath '../../data/files/over1k' into table ove\n POSTHOOK: type: LOAD\n #### A masked pattern was here ####\n POSTHOOK: Output: default@over1k\n-FAILED: SemanticException [Error 10016]: Line 7:26 Argument type mismatch '4': Only primitive type arguments are accepted but array<int> is passed. as  arguments\n+FAILED: SemanticException [Error 10016]: Line 7:12 Argument type mismatch '3': Only primitive type arguments are accepted but array<int> is passed.", "filename": "ql/src/test/results/clientnegative/char_pad_convert_fail1.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/char_pad_convert_fail3.q.out", "blob_url": "https://github.com/apache/hive/blob/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/char_pad_convert_fail3.q.out", "sha": "a25ea1532863f34fd383085beb8da8457d69ddb2", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/char_pad_convert_fail3.q.out?ref=3c1eae0c44450ebbf26c88066de871ea7f479981", "patch": "@@ -40,4 +40,4 @@ POSTHOOK: query: load data local inpath '../../data/files/over1k' into table ove\n POSTHOOK: type: LOAD\n #### A masked pattern was here ####\n POSTHOOK: Output: default@over1k\n-FAILED: SemanticException [Error 10016]: Line 7:38 Argument type mismatch '4': Only primitive type arguments are accepted but uniontype<string> is passed. as  arguments\n+FAILED: SemanticException [Error 10016]: Line 7:12 Argument type mismatch ''Union'': Only primitive type arguments are accepted but uniontype<string> is passed.", "filename": "ql/src/test/results/clientnegative/char_pad_convert_fail3.q.out"}, {"additions": 43, "raw_url": "https://github.com/apache/hive/raw/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/nvl_mismatch_type.q.out", "blob_url": "https://github.com/apache/hive/blob/3c1eae0c44450ebbf26c88066de871ea7f479981/ql/src/test/results/clientnegative/nvl_mismatch_type.q.out", "sha": "6e883857e94b1fc073f95dc475420c14c0bb5657", "changes": 43, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/nvl_mismatch_type.q.out?ref=3c1eae0c44450ebbf26c88066de871ea7f479981", "patch": "@@ -0,0 +1,43 @@\n+PREHOOK: query: create table over1k(\n+           t tinyint,\n+           si smallint,\n+           i int,\n+           b bigint,\n+           f float,\n+           d double,\n+           bo boolean,\n+           s string,\n+           ts timestamp,\n+           dec decimal(4,2),\n+           bin binary)\n+       row format delimited\n+       fields terminated by '|'\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@over1k\n+POSTHOOK: query: create table over1k(\n+           t tinyint,\n+           si smallint,\n+           i int,\n+           b bigint,\n+           f float,\n+           d double,\n+           bo boolean,\n+           s string,\n+           ts timestamp,\n+           dec decimal(4,2),\n+           bin binary)\n+       row format delimited\n+       fields terminated by '|'\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@over1k\n+PREHOOK: query: load data local inpath '../../data/files/over1k' into table over1k\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@over1k\n+POSTHOOK: query: load data local inpath '../../data/files/over1k' into table over1k\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@over1k\n+FAILED: SemanticException [Error 10016]: Line 4:14 Argument type mismatch 'true': The first and seconds arguments of function NLV should have the same type, but they are different: \"tinyint\" and \"boolean\"", "filename": "ql/src/test/results/clientnegative/nvl_mismatch_type.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6b0131b04dea38bda5237ab2b7ec1be5cf0c59af", "parent": "https://github.com/apache/hive/commit/c8648aa3a21da73ef1e17253829094bc46a9c324", "message": "Revert \"HIVE-14303: CommonJoinOperator.checkAndGenObject should return directly at CLOSE state to avoid NPE if ExecReducer.close is called twice. (Zhihai Xu, reviewed by Xuefu Zhang)\"\n\nThis reverts commit 529814e3d0bfaefdf3f1625d014d9e05149f5ccf, due to test failures.", "bug_id": "hive_140", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/6b0131b04dea38bda5237ab2b7ec1be5cf0c59af/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/6b0131b04dea38bda5237ab2b7ec1be5cf0c59af/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "sha": "43231af4e7432821981a5d64f2f8b4679bd8c52d", "changes": 6, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java?ref=6b0131b04dea38bda5237ab2b7ec1be5cf0c59af", "patch": "@@ -682,12 +682,6 @@ private void genAllOneUniqueJoinObject()\n   }\n \n   protected void checkAndGenObject() throws HiveException {\n-    if (state == State.CLOSE) {\n-      LOG.warn(\"checkAndGenObject is called after operator \" +\n-          id + \" \" + getName() + \" closed\");\n-      return;\n-    }\n-\n     if (condn[0].getType() == JoinDesc.UNIQUE_JOIN) {\n \n       // Check if results need to be emitted.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/854950b2a0ce5b3885b2be0e7359f04b483f687c", "parent": "https://github.com/apache/hive/commit/e74dc320ed4c2cae0ab34cf89fb695c1a5f2e31f", "message": "HIVE-11216: UDF GenericUDFMapKeys throws NPE when a null map value is passed in (Yibing Shi via Chaoyu Tang, reviewed by Szehon Ho", "bug_id": "hive_141", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/854950b2a0ce5b3885b2be0e7359f04b483f687c/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapKeys.java", "blob_url": "https://github.com/apache/hive/blob/854950b2a0ce5b3885b2be0e7359f04b483f687c/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapKeys.java", "sha": "d0cff080c24d7214e0109e25c69642bbfbaa6869", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapKeys.java?ref=854950b2a0ce5b3885b2be0e7359f04b483f687c", "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.hive.ql.udf.generic;\n \n import java.util.ArrayList;\n+import java.util.Map;\n \n import org.apache.hadoop.hive.ql.exec.Description;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n@@ -61,7 +62,10 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     retArray.clear();\n     Object mapObj = arguments[0].get();\n-    retArray.addAll(mapOI.getMap(mapObj).keySet());\n+    Map<?,?> mapVal = mapOI.getMap(mapObj);\n+    if (mapVal != null) {\n+      retArray.addAll(mapVal.keySet());\n+    }\n     return retArray;\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapKeys.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947", "parent": "https://github.com/apache/hive/commit/d89a7d1e7fe7fb51aeb514e4357ae149158b2a34", "message": "HIVE-11221: In Tez mode, alter table concatenate orc files can intermittently fail with NPE (Prasanth Jayachandran reviewed by Vikram Dixit)", "bug_id": "hive_142", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/itests/src/test/resources/testconfiguration.properties", "sha": "97715fc4e0548d661e61397ca95bf990204acf73", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=b61e6b52b54c9f8914aa6e4e042ff2921ce6a947", "patch": "@@ -139,6 +139,7 @@ minitez.query.files.shared=alter_merge_2_orc.q,\\\n   orc_merge6.q,\\\n   orc_merge7.q,\\\n   orc_merge8.q,\\\n+  orc_merge9.q,\\\n   orc_merge_incompat1.q,\\\n   orc_merge_incompat2.q,\\\n   orc_vectorization_ppd.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 29, "raw_url": "https://github.com/apache/hive/raw/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java", "blob_url": "https://github.com/apache/hive/blob/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java", "sha": "fce152306ad2f4df85e1b29083773f15911decc9", "changes": 42, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java?ref=b61e6b52b54c9f8914aa6e4e042ff2921ce6a947", "patch": "@@ -17,9 +17,8 @@\n  */\n package org.apache.hadoop.hive.ql.exec.tez;\n \n-import java.io.IOException;\n+import java.util.List;\n import java.util.Map;\n-import java.util.Map.Entry;\n import java.util.concurrent.Callable;\n \n import org.apache.commons.logging.Log;\n@@ -41,11 +40,14 @@\n import org.apache.hadoop.util.StringUtils;\n import org.apache.tez.mapreduce.input.MRInputLegacy;\n import org.apache.tez.mapreduce.processor.MRTaskReporter;\n+import org.apache.tez.runtime.api.Input;\n import org.apache.tez.runtime.api.LogicalInput;\n import org.apache.tez.runtime.api.LogicalOutput;\n import org.apache.tez.runtime.api.ProcessorContext;\n import org.apache.tez.runtime.library.api.KeyValueReader;\n \n+import com.google.common.collect.Lists;\n+\n /**\n  * Record processor for fast merging of files.\n  */\n@@ -219,22 +221,36 @@ private boolean processRow(Object key, Object value) {\n   }\n \n   private MRInputLegacy getMRInput(Map<String, LogicalInput> inputs) throws Exception {\n-    // there should be only one MRInput\n-    MRInputLegacy theMRInput = null;\n-    LOG.info(\"VDK: the inputs are: \" + inputs);\n-    for (Entry<String, LogicalInput> inp : inputs.entrySet()) {\n-      if (inp.getValue() instanceof MRInputLegacy) {\n-        if (theMRInput != null) {\n+    LOG.info(\"The inputs are: \" + inputs);\n+\n+    // start the mr input and wait for ready event. number of MRInput is expected to be 1\n+    List<Input> li = Lists.newArrayList();\n+    int numMRInputs = 0;\n+    for (LogicalInput inp : inputs.values()) {\n+      if (inp instanceof MRInputLegacy) {\n+        numMRInputs++;\n+        if (numMRInputs > 1) {\n           throw new IllegalArgumentException(\"Only one MRInput is expected\");\n         }\n-        // a better logic would be to find the alias\n-        theMRInput = (MRInputLegacy) inp.getValue();\n+        inp.start();\n+        li.add(inp);\n       } else {\n-        throw new IOException(\"Expecting only one input of type MRInputLegacy. Found type: \"\n-            + inp.getClass().getCanonicalName());\n+        throw new IllegalArgumentException(\"Expecting only one input of type MRInputLegacy.\" +\n+            \" Found type: \" + inp.getClass().getCanonicalName());\n       }\n     }\n-    theMRInput.init();\n+\n+    // typically alter table .. concatenate is run on only one partition/one table,\n+    // so it doesn't matter if we wait for all inputs or any input to be ready.\n+    processorContext.waitForAnyInputReady(li);\n+\n+    final MRInputLegacy theMRInput;\n+    if (li.size() == 1) {\n+      theMRInput = (MRInputLegacy) li.get(0);\n+      theMRInput.init();\n+    } else {\n+      throw new IllegalArgumentException(\"MRInputs count is expected to be 1\");\n+    }\n \n     return theMRInput;\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java"}, {"additions": 186, "raw_url": "https://github.com/apache/hive/raw/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/test/results/clientpositive/tez/orc_merge9.q.out", "blob_url": "https://github.com/apache/hive/blob/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/test/results/clientpositive/tez/orc_merge9.q.out", "sha": "bdf0fd30332f7790f18ad5b242154641d14f0c4e", "changes": 186, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/orc_merge9.q.out?ref=b61e6b52b54c9f8914aa6e4e042ff2921ce6a947", "patch": "@@ -0,0 +1,186 @@\n+PREHOOK: query: create table ts_merge (\n+userid bigint,\n+string1 string,\n+subtype double,\n+decimal1 decimal(38,18),\n+ts timestamp\n+) stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: create table ts_merge (\n+userid bigint,\n+string1 string,\n+subtype double,\n+decimal1 decimal(38,18),\n+ts timestamp\n+) stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@ts_merge\n+PREHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' overwrite into table ts_merge\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' overwrite into table ts_merge\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@ts_merge\n+PREHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' into table ts_merge\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' into table ts_merge\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@ts_merge\n+Found 2 items\n+#### A masked pattern was here ####\n+PREHOOK: query: select count(*) from ts_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from ts_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+50000\n+PREHOOK: query: alter table ts_merge concatenate\n+PREHOOK: type: ALTER_TABLE_MERGE\n+PREHOOK: Input: default@ts_merge\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: alter table ts_merge concatenate\n+POSTHOOK: type: ALTER_TABLE_MERGE\n+POSTHOOK: Input: default@ts_merge\n+POSTHOOK: Output: default@ts_merge\n+PREHOOK: query: select count(*) from ts_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from ts_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+50000\n+Found 1 items\n+#### A masked pattern was here ####\n+PREHOOK: query: -- incompatible merge test (stripe statistics missing)\n+\n+create table a_merge like alltypesorc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: -- incompatible merge test (stripe statistics missing)\n+\n+create table a_merge like alltypesorc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@a_merge\n+PREHOOK: query: insert overwrite table a_merge select * from alltypesorc\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: insert overwrite table a_merge select * from alltypesorc\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@a_merge\n+POSTHOOK: Lineage: a_merge.cbigint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean1, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean2, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: a_merge.csmallint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:csmallint, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring2, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp2, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctinyint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+PREHOOK: query: load data local inpath '../../data/files/alltypesorc' into table a_merge\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: load data local inpath '../../data/files/alltypesorc' into table a_merge\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@a_merge\n+Found 2 items\n+#### A masked pattern was here ####\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+24576\n+PREHOOK: query: alter table a_merge concatenate\n+PREHOOK: type: ALTER_TABLE_MERGE\n+PREHOOK: Input: default@a_merge\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: alter table a_merge concatenate\n+POSTHOOK: type: ALTER_TABLE_MERGE\n+POSTHOOK: Input: default@a_merge\n+POSTHOOK: Output: default@a_merge\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+24576\n+Found 2 items\n+#### A masked pattern was here ####\n+PREHOOK: query: insert into table a_merge select * from alltypesorc\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: insert into table a_merge select * from alltypesorc\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@a_merge\n+POSTHOOK: Lineage: a_merge.cbigint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean1, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean2, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: a_merge.csmallint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:csmallint, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring2, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp2, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctinyint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+Found 3 items\n+#### A masked pattern was here ####\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+36864\n+PREHOOK: query: alter table a_merge concatenate\n+PREHOOK: type: ALTER_TABLE_MERGE\n+PREHOOK: Input: default@a_merge\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: alter table a_merge concatenate\n+POSTHOOK: type: ALTER_TABLE_MERGE\n+POSTHOOK: Input: default@a_merge\n+POSTHOOK: Output: default@a_merge\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+36864\n+Found 2 items\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/tez/orc_merge9.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/47b18039d48169378ff0df34b92451a9cc43405d", "parent": "https://github.com/apache/hive/commit/63b31c991350eb791a80ca34964d87d6e7de6ea2", "message": "HIVE-11013 : MiniTez tez_join_hash test on the branch fails with NPE (initializeOp not called?) (Sergey Shelukhin, reviewed by Vikram Dixit K) ADDENDUM", "bug_id": "hive_143", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/47b18039d48169378ff0df34b92451a9cc43405d/ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java", "blob_url": "https://github.com/apache/hive/blob/47b18039d48169378ff0df34b92451a9cc43405d/ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java", "sha": "f09138dd3b33701f64bc758afb580a72b68eee1a", "changes": 5, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java?ref=47b18039d48169378ff0df34b92451a9cc43405d", "patch": "@@ -144,9 +144,4 @@ public void setDummyOps(List<HashTableDummyOperator> dummyOps) {\n   public void addDummyOp(HashTableDummyOperator dummyOp) {\n     getMainWork().addDummyOp(dummyOp);\n   }\n-\n-  @Override\n-  public List<HashTableDummyOperator> getDummyOps() {\n-    return getMainWork().getDummyOps();\n-  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/63b31c991350eb791a80ca34964d87d6e7de6ea2", "parent": "https://github.com/apache/hive/commit/afeed299451a651e5ac957b01999dae564f096a4", "message": "HIVE-11013 : MiniTez tez_join_hash test on the branch fails with NPE (initializeOp not called?) (Sergey Shelukhin, reviewed by Vikram Dixit K)", "bug_id": "hive_144", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/63b31c991350eb791a80ca34964d87d6e7de6ea2/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java", "blob_url": "https://github.com/apache/hive/blob/63b31c991350eb791a80ca34964d87d6e7de6ea2/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java", "sha": "fa697efaf7d1175df9bf3b30d5b297df56be96e6", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java?ref=63b31c991350eb791a80ca34964d87d6e7de6ea2", "patch": "@@ -26,6 +26,8 @@\n import java.util.Set;\n import java.util.Stack;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.ql.exec.HashTableDummyOperator;\n import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.mapred.JobConf;\n@@ -38,6 +40,7 @@\n  */\n @SuppressWarnings({\"serial\"})\n public abstract class BaseWork extends AbstractOperatorDesc {\n+  static final private Log LOG = LogFactory.getLog(BaseWork.class);\n \n   // dummyOps is a reference to all the HashTableDummy operators in the\n   // plan. These have to be separately initialized when we setup a task.\n@@ -86,6 +89,10 @@ public void setName(String name) {\n   }\n \n   public void setDummyOps(List<HashTableDummyOperator> dummyOps) {\n+    if (this.dummyOps != null && !this.dummyOps.isEmpty()\n+        && (dummyOps == null || dummyOps.isEmpty())) {\n+      LOG.info(\"Removing dummy operators from \" + name + \" \" + this.getClass().getSimpleName());\n+    }\n     this.dummyOps = dummyOps;\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/63b31c991350eb791a80ca34964d87d6e7de6ea2/ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java", "blob_url": "https://github.com/apache/hive/blob/63b31c991350eb791a80ca34964d87d6e7de6ea2/ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java", "sha": "2cf45349ea7e13e4fb019dd9c6c55d6f78089a63", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java?ref=63b31c991350eb791a80ca34964d87d6e7de6ea2", "patch": "@@ -135,6 +135,11 @@ public void setDummyOps(List<HashTableDummyOperator> dummyOps) {\n     getMainWork().setDummyOps(dummyOps);\n   }\n \n+  @Override\n+  public List<HashTableDummyOperator> getDummyOps() {\n+    return getMainWork().getDummyOps();\n+  }\n+\n   @Override\n   public void addDummyOp(HashTableDummyOperator dummyOp) {\n     getMainWork().addDummyOp(dummyOp);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "parent": "https://github.com/apache/hive/commit/638597af7f11f22a3896df7a2ba19bbba7e699f8", "message": "HIVE-11015 : LLAP: MiniTez tez_smb_main, tez_bmj_schema_evolution fail with NPE (Sergey Shelukhin, reviewed by Vikram Dixit K)", "bug_id": "hive_145", "file": [{"additions": 34, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java", "sha": "1a125cbb98fb03ebf451a8bd5b7e39e26f5ec26e", "changes": 62, "status": "modified", "deletions": 28, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -38,6 +38,7 @@\n import org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics;\n import org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol;\n import org.apache.hadoop.hive.llap.tezplugins.Converters;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.ipc.RPC;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.Credentials;\n@@ -198,38 +199,43 @@ public LlapTaskUmbilicalProtocol run() throws Exception {\n         new AtomicLong(0),\n         request.getContainerIdString());\n \n-    synchronized (this) {\n-      if (shouldRunTask) {\n-        taskRunner = new TezTaskRunner2(conf, taskUgi, fragmentInfo.getLocalDirs(),\n-            taskSpec,\n-            request.getAppAttemptNumber(),\n-            serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor,\n-            objectRegistry,\n-            pid,\n-            executionContext, memoryAvailable);\n-      }\n-    }\n-    if (taskRunner == null) {\n-      LOG.info(\"Not starting task {} since it was killed earlier\", taskSpec.getTaskAttemptID());\n-      return new TaskRunner2Result(EndReason.KILL_REQUESTED, null, false);\n-    }\n-\n+    String attemptId = fragmentInfo.getFragmentIdentifierString();\n+    IOContextMap.setThreadAttemptId(attemptId);\n     try {\n-      TaskRunner2Result result = taskRunner.run();\n-      if (result.isContainerShutdownRequested()) {\n-        LOG.warn(\"Unexpected container shutdown requested while running task. Ignoring\");\n+      synchronized (this) {\n+        if (shouldRunTask) {\n+          taskRunner = new TezTaskRunner2(conf, taskUgi, fragmentInfo.getLocalDirs(),\n+              taskSpec,\n+              request.getAppAttemptNumber(),\n+              serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor,\n+              objectRegistry,\n+              pid,\n+              executionContext, memoryAvailable);\n+        }\n+      }\n+      if (taskRunner == null) {\n+        LOG.info(\"Not starting task {} since it was killed earlier\", taskSpec.getTaskAttemptID());\n+        return new TaskRunner2Result(EndReason.KILL_REQUESTED, null, false);\n       }\n-      isCompleted.set(true);\n-      return result;\n \n-    } finally {\n-      // TODO Fix UGI and FS Handling. Closing UGI here causes some errors right now.\n-      //        FileSystem.closeAllForUGI(taskUgi);\n-      LOG.info(\"ExecutionTime for Container: \" + request.getContainerIdString() + \"=\" +\n-          runtimeWatch.stop().elapsedMillis());\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"canFinish post completion: \" + taskSpec.getTaskAttemptID() + \": \" + canFinish());\n+      try {\n+        TaskRunner2Result result = taskRunner.run();\n+        if (result.isContainerShutdownRequested()) {\n+          LOG.warn(\"Unexpected container shutdown requested while running task. Ignoring\");\n+        }\n+        isCompleted.set(true);\n+        return result;\n+      } finally {\n+        // TODO Fix UGI and FS Handling. Closing UGI here causes some errors right now.\n+        //        FileSystem.closeAllForUGI(taskUgi);\n+        LOG.info(\"ExecutionTime for Container: \" + request.getContainerIdString() + \"=\" +\n+            runtimeWatch.stop().elapsedMillis());\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"canFinish post completion: \" + taskSpec.getTaskAttemptID() + \": \" + canFinish());\n+        }\n       }\n+    } finally {\n+      IOContextMap.clearThreadAttempt(attemptId);\n     }\n   }\n ", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java", "sha": "0e7e79dc864a63aaf69e37d90dcf41cc33826739", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.io.IOContext;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.plan.FilterDesc;\n import org.apache.hadoop.hive.ql.plan.api.OperatorType;\n@@ -61,7 +62,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {\n       }\n \n       conditionInspector = null;\n-      ioContext = IOContext.get(hconf);\n+      ioContext = IOContextMap.get(hconf);\n     } catch (Throwable e) {\n       throw new HiveException(e);\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java", "sha": "fc5abfef583e9a2c4c6a0866b30421b724dd8415", "changes": 10, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -22,8 +22,8 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.ql.exec.FetchOperator;\n-import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.io.IOContext;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.hive.ql.plan.MapredLocalWork;\n import org.apache.hadoop.mapred.JobConf;\n \n@@ -63,11 +63,11 @@ public void setCurrentBigBucketFile(String currentBigBucketFile) {\n \n   public ExecMapperContext(JobConf jc) {\n     this.jc = jc;\n-    ioCxt = IOContext.get(jc);\n+    ioCxt = IOContextMap.get(jc);\n   }\n \n   public void clear() {\n-    IOContext.clear();\n+    IOContextMap.clear();\n     ioCxt = null;\n   }\n \n@@ -151,8 +151,4 @@ public void setFetchOperators(Map<String, FetchOperator> fetchOperators) {\n   public IOContext getIoCxt() {\n     return ioCxt;\n   }\n-\n-  public void setIoCxt(IOContext ioCxt) {\n-    this.ioCxt = ioCxt;\n-  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "sha": "e205f1ebaf14434a92957538d443c33678987a6e", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -201,7 +201,7 @@ public Object call() {\n             mergeMapOp.setChildren(jconf);\n \n             DummyStoreOperator dummyOp = getJoinParentOp(mergeMapOp);\n-\t          mapOp.setConnectedOperators(mergeMapWork.getTag(), dummyOp);\n+            mapOp.setConnectedOperators(mergeMapWork.getTag(), dummyOp);\n \n             mergeMapOp.passExecContext(new ExecMapperContext(jconf));\n             mergeMapOp.initializeLocalWork(jconf);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java", "sha": "738ca9ce32b4b7a6bfd5f239fd12aa989724c67f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -162,7 +162,7 @@ protected void updateIOContext()\n   }\n \n   public IOContext getIOContext() {\n-    return IOContext.get(jobConf);\n+    return IOContextMap.get(jobConf);\n   }\n \n   private void initIOContext(long startPos, boolean isBlockPointer,", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java", "sha": "0d9b6445f143e9abab18cad0124eda11d6538be4", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -264,7 +264,6 @@ public static boolean canWrapForLlap(Class<? extends InputFormat> inputFormatCla\n \n   public RecordReader getRecordReader(InputSplit split, JobConf job,\n       Reporter reporter) throws IOException {\n-\n     HiveInputSplit hsplit = (HiveInputSplit) split;\n \n     InputSplit inputSplit = hsplit.getInputSplit();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java", "sha": "019db8db83af4bbb3c9f2209cae52669befe6695", "changes": 55, "status": "modified", "deletions": 55, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -18,13 +18,7 @@\n \n package org.apache.hadoop.hive.ql.io;\n \n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.ql.exec.Utilities;\n \n /**\n  * IOContext basically contains the position information of the current\n@@ -35,55 +29,6 @@\n  * nextBlockStart refers the end of current row and beginning of next row.\n  */\n public class IOContext {\n-  public static final String DEFAULT_CONTEXT = \"\";\n-\n-  private static final ThreadLocal<Map<String,IOContext>> threadLocalMap\n-      = new ThreadLocal<Map<String,IOContext>>() {\n-    @Override\n-    protected synchronized Map<String,IOContext> initialValue() {\n-      Map<String, IOContext> map = new HashMap<String, IOContext>(); \n-      map.put(DEFAULT_CONTEXT, new IOContext());\n-      return map;\n-    }\n-  };\n-\n-  /**\n-   * Spark uses this thread local TODO: no it doesn't?\n-   */\n-  private static final ThreadLocal<IOContext> threadLocal = new ThreadLocal<IOContext>(){\n-    @Override\n-    protected IOContext initialValue() { return new IOContext(); }\n-  };\n-\n-  private static IOContext get() {\n-      return IOContext.threadLocalMap.get().get(DEFAULT_CONTEXT);\n-  }\n-\n-  /**\n-   * Tez and MR use this map but are single threaded per JVM thus no synchronization is required.\n-   */\n-  private static final Map<String, IOContext> inputNameIOContextMap = new HashMap<String, IOContext>();\n-\n-  public static IOContext get(Configuration conf) {\n-    String inputName = conf.get(Utilities.INPUT_NAME);\n-    Map<String, IOContext> inputNameIOContextMap = threadLocalMap.get();\n-\n-    if (inputName == null) {\n-      inputName = DEFAULT_CONTEXT;\n-    }\n-\n-    if (!inputNameIOContextMap.containsKey(inputName)) {\n-      IOContext ioContext = new IOContext();\n-      inputNameIOContextMap.put(inputName, ioContext);\n-    }\n-\n-    return inputNameIOContextMap.get(inputName);\n-  }\n-\n-  public static void clear() {\n-      threadLocal.remove();\n-  }\n-\n   private long currentBlockStart;\n   private long nextBlockStart;\n   private long currentRow;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java"}, {"additions": 117, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java", "sha": "57e7e2a910023ce2f4a700c2a3dc6b7654fb489a", "changes": 117, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -0,0 +1,117 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+\n+/**\n+ * There used to be a global static map of IOContext-s inside IOContext (Hive style!).\n+ * Unfortunately, due to variety of factors, this is now a giant fustercluck.\n+ * 1) Spark doesn't apparently care about multiple inputs, but has multiple threads, so one\n+ *    threadlocal IOContext was added for it.\n+ * 2) LLAP has lots of tasks in the same process so globals no longer cut it either.\n+ * 3) However, Tez runs 2+ threads for one task (e.g. TezTaskEventRouter and TezChild), and these\n+ *    surprisingly enough need the same context. Tez, in its infinite wisdom, doesn't allow them\n+ *    to communicate in any way nor provide any shared context.\n+ * So we are going to...\n+ * 1) Keep the good ol' global map for MR and Tez. Hive style!\n+ * 2) Keep the threadlocal for Spark. Hive style!\n+ * 3) Create inheritable (TADA!) threadlocal with attemptId, only set in LLAP; that will propagate\n+ *    to all the little Tez threads, and we will keep a map per attempt. Hive style squared!\n+ */\n+public class IOContextMap {\n+  public static final String DEFAULT_CONTEXT = \"\";\n+  private static final Log LOG = LogFactory.getLog(IOContextMap.class);\n+\n+  /** Used for Tez and MR */\n+  private static final ConcurrentHashMap<String, IOContext> globalMap =\n+      new ConcurrentHashMap<String, IOContext>();\n+\n+  /** Used for Spark */\n+  private static final ThreadLocal<IOContext> sparkThreadLocal = new ThreadLocal<IOContext>(){\n+    @Override\n+    protected IOContext initialValue() { return new IOContext(); }\n+  };\n+\n+  /** Used for Tez+LLAP */\n+  private static final ConcurrentHashMap<String, ConcurrentHashMap<String, IOContext>> attemptMap =\n+      new ConcurrentHashMap<String, ConcurrentHashMap<String, IOContext>>();\n+\n+  // TODO: This depends on Tez creating separate threads, as it does now. If that changes, some\n+  //       other way to propagate/find out attempt ID would be needed (e.g. see TEZ-2587).\n+  private static final InheritableThreadLocal<String> threadAttemptId =\n+      new InheritableThreadLocal<>();\n+\n+  public static void setThreadAttemptId(String attemptId) {\n+    assert attemptId != null;\n+    threadAttemptId.set(attemptId);\n+  }\n+\n+  public static void clearThreadAttempt(String attemptId) {\n+    assert attemptId != null;\n+    String attemptIdCheck = threadAttemptId.get();\n+    if (!attemptId.equals(attemptIdCheck)) {\n+      LOG.error(\"Thread is clearing context for \"\n+          + attemptId + \", but \" + attemptIdCheck + \" expected\");\n+    }\n+    attemptMap.remove(attemptId);\n+    threadAttemptId.remove();\n+  }\n+\n+  public static IOContext get(Configuration conf) {\n+    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n+      return sparkThreadLocal.get();\n+    }\n+    String inputName = conf.get(Utilities.INPUT_NAME);\n+    if (inputName == null) {\n+      inputName = DEFAULT_CONTEXT;\n+    }\n+    String attemptId = threadAttemptId.get();\n+    ConcurrentHashMap<String, IOContext> map;\n+    if (attemptId == null) {\n+      map = globalMap;\n+    } else {\n+      map = attemptMap.get(attemptId);\n+      if (map == null) {\n+        map = new ConcurrentHashMap<>();\n+        ConcurrentHashMap<String, IOContext> oldMap = attemptMap.putIfAbsent(attemptId, map);\n+        if (oldMap != null) {\n+          map = oldMap;\n+        }\n+      }\n+    }\n+\n+    IOContext ioContext = map.get(inputName);\n+    if (ioContext != null) return ioContext;\n+    ioContext = new IOContext();\n+    IOContext oldContext = map.putIfAbsent(inputName, ioContext);\n+    return (oldContext == null) ? ioContext : oldContext;\n+  }\n+\n+  public static void clear() {\n+    sparkThreadLocal.remove();\n+    globalMap.clear();\n+  }\n+}", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java", "sha": "c3a36c021127935790a90eff7136b91b0cb77e4e", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.Driver;\n import org.apache.hadoop.hive.ql.io.IOContext;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;\n import org.apache.hadoop.hive.ql.plan.CollectDesc;\n import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n@@ -272,7 +273,7 @@ public void testMapOperator() throws Throwable {\n       JobConf hconf = new JobConf(TestOperators.class);\n       HiveConf.setVar(hconf, HiveConf.ConfVars.HADOOPMAPFILENAME,\n           \"hdfs:///testDir/testFile\");\n-      IOContext.get(hconf).setInputPath(\n+      IOContextMap.get(hconf).setInputPath(\n           new Path(\"hdfs:///testDir/testFile\"));\n \n       // initialize pathToAliases", "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java", "sha": "9dc4f5b8ad64b75111f5d39a008e697ba5bb18e9", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -116,7 +116,7 @@ public void doClose() throws IOException {\n \n   private void resetIOContext() {\n     conf.set(Utilities.INPUT_NAME, \"TestHiveBinarySearchRecordReader\");\n-    ioContext = IOContext.get(conf);\n+    ioContext = IOContextMap.get(conf);\n     ioContext.setUseSorted(false);\n     ioContext.setBinarySearching(false);\n     ioContext.setEndBinarySearch(false);", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java"}, {"additions": 207, "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java", "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java", "sha": "dad55360d5f72bd1b645283b59a2c515bc2040ec", "changes": 207, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917", "patch": "@@ -0,0 +1,207 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.FutureTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.junit.Test;\n+\n+import com.google.common.collect.Sets;\n+\n+public class TestIOContextMap {\n+\n+  private void syncThreadStart(final CountDownLatch cdlIn, final CountDownLatch cdlOut) {\n+    cdlIn.countDown();\n+    try {\n+      cdlOut.await();\n+    } catch (InterruptedException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  @Test\n+  public void testMRTezGlobalMap() throws Exception {\n+    // Tests concurrent modification, and that results are the same per input across threads\n+    // but different between inputs.\n+    final int THREAD_COUNT = 2, ITER_COUNT = 1000;\n+    final AtomicInteger countdown = new AtomicInteger(ITER_COUNT);\n+    final CountDownLatch phase1End = new CountDownLatch(THREAD_COUNT);\n+    final IOContext[] results = new IOContext[ITER_COUNT];\n+    ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);\n+    final CountDownLatch cdlIn = new CountDownLatch(THREAD_COUNT), cdlOut = new CountDownLatch(1);\n+\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<Void>[] tasks = new FutureTask[THREAD_COUNT];\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i] = new FutureTask<Void>(new Callable<Void>() {\n+        public Void call() throws Exception {\n+          Configuration conf = new Configuration();\n+          syncThreadStart(cdlIn, cdlOut);\n+          // Phase 1 - create objects.\n+          while (true) {\n+            int nextIx = countdown.decrementAndGet();\n+            if (nextIx < 0) break;\n+            conf.set(Utilities.INPUT_NAME, \"Input \" + nextIx);\n+            results[nextIx] = IOContextMap.get(conf);\n+            if (nextIx == 0) break;\n+          }\n+          phase1End.countDown();\n+          phase1End.await();\n+          // Phase 2 - verify we get the expected objects created by all threads.\n+          for (int i = 0; i < ITER_COUNT; ++i) {\n+            conf.set(Utilities.INPUT_NAME, \"Input \" + i);\n+            IOContext ctx = IOContextMap.get(conf);\n+            assertSame(results[i], ctx);\n+          }\n+          return null;\n+        }\n+      });\n+      executor.execute(tasks[i]);\n+    }\n+\n+    cdlIn.await(); // Wait for all threads to be ready.\n+    cdlOut.countDown(); // Release them at the same time.\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i].get();\n+    }\n+    Set<IOContext> resultSet = Sets.newIdentityHashSet();\n+    for (int i = 0; i < results.length; ++i) {\n+      assertTrue(resultSet.add(results[i])); // All the objects must be different.\n+    }\n+  }\n+\n+  @Test\n+  public void testTezLlapAttemptMap() throws Exception {\n+    // Tests that different threads get the same object per attempt per input, and different\n+    // between attempts/inputs; that attempt is inherited between threads; and that clearing\n+    // the attempt produces a different result.\n+    final int THREAD_COUNT = 2, ITER_COUNT = 1000, ATTEMPT_COUNT = 3;\n+    final AtomicInteger countdown = new AtomicInteger(ITER_COUNT);\n+    final IOContext[] results = new IOContext[ITER_COUNT * ATTEMPT_COUNT];\n+    ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);\n+    final CountDownLatch cdlIn = new CountDownLatch(THREAD_COUNT), cdlOut = new CountDownLatch(1);\n+\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<Void>[] tasks = new FutureTask[THREAD_COUNT];\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i] = new FutureTask<Void>(new Callable<Void>() {\n+        public Void call() throws Exception {\n+          final Configuration conf = new Configuration(), conf2 = new Configuration();\n+          syncThreadStart(cdlIn, cdlOut);\n+          while (true) {\n+            int nextIx = countdown.decrementAndGet();\n+            if (nextIx < 0) break;\n+            String input1 = \"Input \" + nextIx;\n+            conf.set(Utilities.INPUT_NAME, input1);\n+            for (int j = 0; j < ATTEMPT_COUNT; ++j) {\n+              String attemptId = \"Attempt \" + nextIx + \":\" + j;\n+              IOContextMap.setThreadAttemptId(attemptId);\n+              final IOContext r1 = results[(nextIx * ATTEMPT_COUNT) + j] = IOContextMap.get(conf);\n+              // For some attempts, check inheritance.\n+              if ((nextIx % (ITER_COUNT / 10)) == 0) {\n+                String input2 = \"Input2 \" + nextIx;\n+                conf2.set(Utilities.INPUT_NAME, input2);\n+                final AtomicReference<IOContext> ref2 = new AtomicReference<>();\n+                Thread t = new Thread(new Runnable() {\n+                  public void run() {\n+                    assertSame(r1, IOContextMap.get(conf));\n+                    ref2.set(IOContextMap.get(conf2));\n+                  }\n+                });\n+                t.start();\n+                t.join();\n+                assertSame(ref2.get(), IOContextMap.get(conf2));\n+              }\n+              // Don't clear the attempt ID, or the stuff will be cleared.\n+            }\n+            if (nextIx == 0) break;\n+          }\n+          return null;\n+        }\n+      });\n+      executor.execute(tasks[i]);\n+    }\n+\n+    cdlIn.await(); // Wait for all threads to be ready.\n+    cdlOut.countDown(); // Release them at the same time.\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i].get();\n+    }\n+    Configuration conf = new Configuration();\n+    Set<IOContext> resultSet = Sets.newIdentityHashSet();\n+    for (int i = 0; i < ITER_COUNT; ++i) {\n+      conf.set(Utilities.INPUT_NAME, \"Input \" + i);\n+      for (int j = 0; j < ATTEMPT_COUNT; ++j) {\n+        String attemptId = \"Attempt \" + i + \":\" + j;\n+        IOContext result = results[(i * ATTEMPT_COUNT) + j];\n+        assertTrue(resultSet.add(result)); // All the objects must be different.\n+        IOContextMap.setThreadAttemptId(attemptId);\n+        assertSame(result, IOContextMap.get(conf)); // Matching result for attemptId + input.\n+        IOContextMap.clearThreadAttempt(attemptId);\n+        IOContextMap.setThreadAttemptId(attemptId);\n+        assertNotSame(result, IOContextMap.get(conf)); // Different result after clearing.\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSparkThreadLocal() throws Exception {\n+    // Test that input name does not change IOContext returned, and that each thread gets its own.\n+    final Configuration conf1 = new Configuration();\n+    conf1.set(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE.varname, \"spark\");\n+    final Configuration conf2 = new Configuration(conf1);\n+    conf2.set(Utilities.INPUT_NAME, \"Other input\");\n+    final int THREAD_COUNT = 2;\n+    ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);\n+    final CountDownLatch cdlIn = new CountDownLatch(THREAD_COUNT), cdlOut = new CountDownLatch(1);\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<IOContext>[] tasks = new FutureTask[THREAD_COUNT];\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i] = new FutureTask<IOContext>(new Callable<IOContext>() {\n+        public IOContext call() throws Exception {\n+          syncThreadStart(cdlIn, cdlOut);\n+          IOContext c1 = IOContextMap.get(conf1), c2 = IOContextMap.get(conf2);\n+          assertSame(c1, c2);\n+          return c1;\n+        }\n+      });\n+      executor.execute(tasks[i]);\n+    }\n+\n+    cdlIn.await(); // Wait for all threads to be ready.\n+    cdlOut.countDown(); // Release them at the same time.\n+    Set<IOContext> results = Sets.newIdentityHashSet();\n+    for (int i = 0; i < tasks.length; ++i) {\n+      assertTrue(results.add(tasks[i].get())); // All the objects must be different.\n+    }\n+  }\n+\n+}", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f2600e9da833f409afc33be9f92db4dbe9ad2c9b", "parent": "https://github.com/apache/hive/commit/19d70748fc5e897a58124e2fb8c938b99cdb101b", "message": "HIVE-10963 : Hive throws NPE rather than meaningful error message when window is missing (Aihua Xu via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_146", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "blob_url": "https://github.com/apache/hive/blob/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "sha": "bff97721c6ad540a1f8598e617d1c5f452d1cdeb", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java?ref=f2600e9da833f409afc33be9f92db4dbe9ad2c9b", "patch": "@@ -2378,9 +2378,9 @@ private int getWindowSpecIndx(ASTNode wndAST) {\n         WindowSpec wndSpec = ((WindowFunctionSpec) wExpSpec).getWindowSpec();\n         List<RexNode> partitionKeys = getPartitionKeys(wndSpec.getPartition(), converter, inputRR);\n         List<RexFieldCollation> orderKeys = getOrderKeys(wndSpec.getOrder(), converter, inputRR);\n-        RexWindowBound upperBound = getBound(wndSpec.windowFrame.start, converter);\n-        RexWindowBound lowerBound = getBound(wndSpec.windowFrame.end, converter);\n-        boolean isRows = ((wndSpec.windowFrame.start instanceof RangeBoundarySpec) || (wndSpec.windowFrame.end instanceof RangeBoundarySpec)) ? true\n+        RexWindowBound upperBound = getBound(wndSpec.getWindowFrame().start, converter);\n+        RexWindowBound lowerBound = getBound(wndSpec.getWindowFrame().end, converter);\n+        boolean isRows = ((wndSpec.getWindowFrame().start instanceof RangeBoundarySpec) || (wndSpec.getWindowFrame().end instanceof RangeBoundarySpec)) ? true\n             : false;\n \n         w = cluster.getRexBuilder().makeOver(calciteAggFnRetType, calciteAggFn, calciteAggFnArgs,", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFInvocationSpec.java", "blob_url": "https://github.com/apache/hive/blob/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFInvocationSpec.java", "sha": "29b85105c487816e838918f51cf9c12c05806aa3", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/PTFInvocationSpec.java?ref=f2600e9da833f409afc33be9f92db4dbe9ad2c9b", "patch": "@@ -202,6 +202,7 @@ public PTFQueryInputSpec getQueryInput() {\n   public static class PartitioningSpec {\n     PartitionSpec partSpec;\n     OrderSpec orderSpec;\n+\n     public PartitionSpec getPartSpec() {\n       return partSpec;\n     }\n@@ -250,6 +251,13 @@ public boolean equals(Object obj) {\n       }\n       return true;\n     }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\"PartitioningSpec=[%s%s]\",\n+          partSpec == null ? \"\" : partSpec,\n+          orderSpec == null ? \"\" : orderSpec);\n+    }\n   }\n \n   /*", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/PTFInvocationSpec.java"}, {"additions": 22, "raw_url": "https://github.com/apache/hive/raw/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java", "blob_url": "https://github.com/apache/hive/blob/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java", "sha": "a181f7c1a7bc66f37277e94fd3ffbef91290dc6f", "changes": 46, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java?ref=f2600e9da833f409afc33be9f92db4dbe9ad2c9b", "patch": "@@ -58,20 +58,21 @@\n  * building RowResolvers.\n  */\n public class WindowingSpec {\n-  HashMap<String, WindowExpressionSpec> aliasToWdwExpr;\n-  HashMap<String, WindowSpec> windowSpecs;\n-  ArrayList<WindowExpressionSpec> windowExpressions;\n+  private HashMap<String, WindowExpressionSpec> aliasToWdwExpr;\n+  private HashMap<String, WindowSpec> windowSpecs;\n+  private ArrayList<WindowExpressionSpec> windowExpressions;\n+\n+  public WindowingSpec() {\n+    aliasToWdwExpr = new HashMap<String, WindowExpressionSpec>();\n+    windowSpecs = new HashMap<String, WindowSpec>();\n+    windowExpressions = new ArrayList<WindowExpressionSpec>();\n+  }\n \n   public void addWindowSpec(String name, WindowSpec wdwSpec) {\n-    windowSpecs = windowSpecs == null ? new HashMap<String, WindowSpec>() : windowSpecs;\n     windowSpecs.put(name, wdwSpec);\n   }\n \n   public void addWindowFunction(WindowFunctionSpec wFn) {\n-    windowExpressions = windowExpressions == null ?\n-        new ArrayList<WindowExpressionSpec>() : windowExpressions;\n-    aliasToWdwExpr = aliasToWdwExpr == null ?\n-        new HashMap<String, WindowExpressionSpec>() : aliasToWdwExpr;\n     windowExpressions.add(wFn);\n     aliasToWdwExpr.put(wFn.getAlias(), wFn);\n   }\n@@ -80,26 +81,14 @@ public void addWindowFunction(WindowFunctionSpec wFn) {\n     return aliasToWdwExpr;\n   }\n \n-  public void setAliasToWdwExpr(HashMap<String, WindowExpressionSpec> aliasToWdwExpr) {\n-    this.aliasToWdwExpr = aliasToWdwExpr;\n-  }\n-\n   public HashMap<String, WindowSpec> getWindowSpecs() {\n     return windowSpecs;\n   }\n \n-  public void setWindowSpecs(HashMap<String, WindowSpec> windowSpecs) {\n-    this.windowSpecs = windowSpecs;\n-  }\n-\n   public ArrayList<WindowExpressionSpec> getWindowExpressions() {\n     return windowExpressions;\n   }\n \n-  public void setWindowExpressions(ArrayList<WindowExpressionSpec> windowExpressions) {\n-    this.windowExpressions = windowExpressions;\n-  }\n-\n   public PartitioningSpec getQueryPartitioningSpec() {\n     /*\n      * Why no null and class checks?\n@@ -171,7 +160,7 @@ private void fillInWindowSpec(String sourceId, WindowSpec dest, ArrayList<String\n       WindowSpec source = getWindowSpecs().get(sourceId);\n       if (source == null || source.equals(dest))\n       {\n-        throw new SemanticException(String.format(\"Window Spec %s refers to an unknown source \" ,\n+        throw new SemanticException(String.format(\"%s refers to an unknown source\" ,\n             dest));\n       }\n \n@@ -445,9 +434,10 @@ public String toString() {\n    */\n   public static class WindowSpec\n   {\n-    String sourceId;\n-    PartitioningSpec partitioning;\n-    WindowFrameSpec windowFrame;\n+    private String sourceId;\n+    private PartitioningSpec partitioning;\n+    private WindowFrameSpec windowFrame;\n+\n     public String getSourceId() {\n       return sourceId;\n     }\n@@ -496,6 +486,14 @@ protected void ensureOrderSpec() {\n         setOrder(order);\n       }\n     }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\"Window Spec=[%s%s%s]\",\n+          sourceId == null ? \"\" : \"Name='\" + sourceId + \"'\",\n+          partitioning == null ? \"\" : partitioning,\n+          windowFrame == null ? \"\" : windowFrame);\n+    }\n   };\n \n   /*", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/test/queries/clientnegative/ptf_negative_NoWindowDefn.q", "blob_url": "https://github.com/apache/hive/blob/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/test/queries/clientnegative/ptf_negative_NoWindowDefn.q", "sha": "8defb3a66b9162607cc94aa88526d6253a10a9d9", "changes": 9, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/ptf_negative_NoWindowDefn.q?ref=f2600e9da833f409afc33be9f92db4dbe9ad2c9b", "patch": "@@ -0,0 +1,9 @@\n+-- testNoWindowDefn\n+select p_mfgr, p_name, p_size,\n+sum(p_size) over (w1) as s1,\n+sum(p_size) over (w2) as s2\n+from part\n+distribute by p_mfgr\n+sort by p_mfgr\n+window w1 as (rows between 2 preceding and 2 following);\n+", "filename": "ql/src/test/queries/clientnegative/ptf_negative_NoWindowDefn.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/test/results/clientnegative/ptf_negative_NoWindowDefn.q.out", "blob_url": "https://github.com/apache/hive/blob/f2600e9da833f409afc33be9f92db4dbe9ad2c9b/ql/src/test/results/clientnegative/ptf_negative_NoWindowDefn.q.out", "sha": "74b97afe5ed0407345b4f885aa4345a2a9feb424", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/ptf_negative_NoWindowDefn.q.out?ref=f2600e9da833f409afc33be9f92db4dbe9ad2c9b", "patch": "@@ -0,0 +1 @@\n+FAILED: SemanticException Window Spec=[Name='w2'] refers to an unknown source", "filename": "ql/src/test/results/clientnegative/ptf_negative_NoWindowDefn.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/3626a7951772a8e205bd33c01cb817af7e8223e2", "parent": "https://github.com/apache/hive/commit/7983593fbc094a90679f8b1fc775b0dd4d5feab3", "message": "HIVE-10906 : Value based UDAF function without orderby expression throws NPE (Aihua Xu via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>", "bug_id": "hive_147", "file": [{"additions": 27, "raw_url": "https://github.com/apache/hive/raw/3626a7951772a8e205bd33c01cb817af7e8223e2/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java", "blob_url": "https://github.com/apache/hive/blob/3626a7951772a8e205bd33c01cb817af7e8223e2/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java", "sha": "953f3ae8ab583e2a7be28a0f1a05871f38c09d58", "changes": 38, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java?ref=3626a7951772a8e205bd33c01cb817af7e8223e2", "patch": "@@ -145,7 +145,7 @@ public void validateAndMakeEffective() throws SemanticException {\n       }\n \n       // 2. A Window Spec with no Parition Spec, is Partitioned on a Constant(number 0)\n-      applyContantPartition(wdwSpec);\n+      applyConstantPartition(wdwSpec);\n \n       // 3. For missing Wdw Frames or for Frames with only a Start Boundary, completely\n       //    specify them by the rules in {@link effectiveWindowFrame}\n@@ -154,8 +154,8 @@ public void validateAndMakeEffective() throws SemanticException {\n       // 4. Validate the effective Window Frames with the rules in {@link validateWindowFrame}\n       validateWindowFrame(wdwSpec);\n \n-      // 5. If there is no Order, then add the Partition expressions as the Order.\n-      wdwSpec.ensureOrderSpec();\n+      // 5. Add the Partition expressions as the Order if there is no Order and validate Order spec.\n+      setAndValidateOrderSpec(wdwSpec);\n     }\n   }\n \n@@ -196,7 +196,7 @@ private void fillInWindowSpec(String sourceId, WindowSpec dest, ArrayList<String\n     }\n   }\n \n-  private void applyContantPartition(WindowSpec wdwSpec) {\n+  private void applyConstantPartition(WindowSpec wdwSpec) {\n     PartitionSpec partSpec = wdwSpec.getPartition();\n     if ( partSpec == null ) {\n       partSpec = new PartitionSpec();\n@@ -277,20 +277,36 @@ private void validateWindowFrame(WindowSpec wdwSpec) throws SemanticException {\n         end.getAmt() == BoundarySpec.UNBOUNDED_AMOUNT ) {\n       throw new SemanticException(\"End of a WindowFrame cannot be UNBOUNDED PRECEDING\");\n     }\n-\n-    validateValueBoundary(wFrame.getStart(), wdwSpec.getOrder());\n-    validateValueBoundary(wFrame.getEnd(), wdwSpec.getOrder());\n   }\n \n-  private void validateValueBoundary(BoundarySpec bs, OrderSpec order) throws SemanticException {\n-    if ( bs instanceof ValueBoundarySpec ) {\n-      ValueBoundarySpec vbs = (ValueBoundarySpec) bs;\n+  /**\n+   * Add default order spec if there is no order and validate order spec for valued based\n+   * windowing since only one sort key is allowed.\n+   * @param wdwSpec\n+   * @throws SemanticException\n+   */\n+  private void setAndValidateOrderSpec(WindowSpec wdwSpec) throws SemanticException {\n+    wdwSpec.ensureOrderSpec();\n+\n+    WindowFrameSpec wFrame = wdwSpec.getWindowFrame();\n+    OrderSpec order = wdwSpec.getOrder();\n+\n+    BoundarySpec start = wFrame.getStart();\n+    BoundarySpec end = wFrame.getEnd();\n+\n+    if (start instanceof ValueBoundarySpec || end instanceof ValueBoundarySpec) {\n       if ( order != null ) {\n         if ( order.getExpressions().size() > 1 ) {\n           throw new SemanticException(\"Range based Window Frame can have only 1 Sort Key\");\n         }\n+\n+        if (start instanceof ValueBoundarySpec) {\n+          ((ValueBoundarySpec)start).setExpression(order.getExpressions().get(0).getExpression());\n+        }\n+        if (end instanceof ValueBoundarySpec) {\n+          ((ValueBoundarySpec)end).setExpression(order.getExpressions().get(0).getExpression());\n+        }\n       }\n-      vbs.setExpression(order.getExpressions().get(0).getExpression());\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/3626a7951772a8e205bd33c01cb817af7e8223e2/ql/src/test/queries/clientpositive/windowing_windowspec3.q", "blob_url": "https://github.com/apache/hive/blob/3626a7951772a8e205bd33c01cb817af7e8223e2/ql/src/test/queries/clientpositive/windowing_windowspec3.q", "sha": "c87aaff2634efdd3e8861926582d4d1227312cde", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/windowing_windowspec3.q?ref=3626a7951772a8e205bd33c01cb817af7e8223e2", "patch": "@@ -16,6 +16,10 @@ create table emp(empno smallint,\n \n load data local inpath '../../data/files/emp2.txt' into table emp;\n \n+-- No order by\n+select hirets, salary, sum(salary) over (partition by hirets range between current row and unbounded following) from emp;\n+\n+\n -- Support date datatype\n select deptno, empno, hiredate, salary,\n     sum(salary) over (partition by deptno order by hiredate range 90 preceding),\n@@ -25,5 +29,3 @@ select deptno, empno, hiredate, salary,\n     sum(salary) over (partition by deptno order by hiredate range between 10 following and unbounded following),\n     sum(salary) over (partition by deptno order by hiredate range between unbounded preceding and 10 following)\n from emp;\n-\n-", "filename": "ql/src/test/queries/clientpositive/windowing_windowspec3.q"}, {"additions": 26, "raw_url": "https://github.com/apache/hive/raw/3626a7951772a8e205bd33c01cb817af7e8223e2/ql/src/test/results/clientpositive/windowing_windowspec3.q.out", "blob_url": "https://github.com/apache/hive/blob/3626a7951772a8e205bd33c01cb817af7e8223e2/ql/src/test/results/clientpositive/windowing_windowspec3.q.out", "sha": "bf7797a2b1a91162795cc5c11eb3ffaaa02cab6d", "changes": 26, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/windowing_windowspec3.q.out?ref=3626a7951772a8e205bd33c01cb817af7e8223e2", "patch": "@@ -42,6 +42,32 @@ POSTHOOK: query: load data local inpath '../../data/files/emp2.txt' into table e\n POSTHOOK: type: LOAD\n #### A masked pattern was here ####\n POSTHOOK: Output: default@emp\n+PREHOOK: query: -- No order by\n+select hirets, salary, sum(salary) over (partition by hirets range between current row and unbounded following) from emp\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@emp\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- No order by\n+select hirets, salary, sum(salary) over (partition by hirets range between current row and unbounded following) from emp\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@emp\n+#### A masked pattern was here ####\n+NULL\t1500.0\t3000.0\n+NULL\t1500.0\t3000.0\n+1980-12-17 00:00:00\t800.0\t800.0\n+1981-02-20 00:00:00\t1600.0\t1600.0\n+1981-02-22 00:00:00\t1250.0\t1250.0\n+1981-04-02 00:00:00\t2975.0\t2975.0\n+1981-05-01 00:00:00\t2850.0\t2850.0\n+1981-06-09 00:00:00\t2450.0\t2450.0\n+1981-09-08 00:00:00\t1500.0\t1500.0\n+1981-09-28 00:00:00\t1250.0\t1250.0\n+1981-11-17 00:00:00\t5000.0\t5000.0\n+1981-12-03 00:00:00\t3000.0\t3950.0\n+1981-12-03 00:00:00\t950.0\t3950.0\n+1982-01-23 00:00:00\t1300.0\t1300.0\n+1982-12-09 00:00:00\t3000.0\t3000.0\n+1983-01-12 00:00:00\t1100.0\t1100.0\n PREHOOK: query: -- Support date datatype\n select deptno, empno, hiredate, salary,\n     sum(salary) over (partition by deptno order by hiredate range 90 preceding),", "filename": "ql/src/test/results/clientpositive/windowing_windowspec3.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/9c33b85497bdb693f16d15fc109d0f800bc4077b", "parent": "https://github.com/apache/hive/commit/f8fecf12089b53f9ffa5a023ccb048b849e9ab45", "message": "LLAP: NPE caused by HIVE-10397 (Prasanth Jayachandran)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1675014 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_148", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9c33b85497bdb693f16d15fc109d0f800bc4077b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java", "blob_url": "https://github.com/apache/hive/blob/9c33b85497bdb693f16d15fc109d0f800bc4077b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java", "sha": "0820893c8968b72a6ab9b7ddb55b965e036006af", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java?ref=9c33b85497bdb693f16d15fc109d0f800bc4077b", "patch": "@@ -94,10 +94,10 @@\n       InputSplit[] groupedSplits =\n           tezGrouper.getGroupedSplits(conf, rawSplits, bucketTaskMap.get(bucketId),\n               HiveInputFormat.class.getName(), splitSizeEstimator);\n-\n+      String splitSizeEstStr = splitSizeEstimator == null ? \"null\" : splitSizeEstimator.getClass().getSimpleName();\n       LOG.info(\"Original split size is \" + rawSplits.length + \" grouped split size is \"\n           + groupedSplits.length + \", for bucket: \" + bucketId + \" SplitSizeEstimator: \" +\n-          splitSizeEstimator.getClass().getSimpleName());\n+          splitSizeEstStr);\n \n       for (InputSplit inSplit : groupedSplits) {\n         bucketGroupedSplitMultimap.put(bucketId, inSplit);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/74312da18a8903ee0f3be21d4dfd5e0efc62e8d7", "parent": "https://github.com/apache/hive/commit/626a9daecf884d0a925724612e8a41fffc167316", "message": "HIVE-9953: fix NPE in WindowingTableFunction (Alexander Pivovarov via Jason Dere)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1667085 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_149", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/74312da18a8903ee0f3be21d4dfd5e0efc62e8d7/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java", "blob_url": "https://github.com/apache/hive/blob/74312da18a8903ee0f3be21d4dfd5e0efc62e8d7/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java", "sha": "0423466d539ab571f1e9bbca89842c3b9009ba20", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java?ref=74312da18a8903ee0f3be21d4dfd5e0efc62e8d7", "patch": "@@ -1190,7 +1190,7 @@ public boolean isEqual(Object v1, Object v2) {\n           (PrimitiveObjectInspector) expressionDef.getOI());\n       String s2 = PrimitiveObjectInspectorUtils.getString(v2,\n           (PrimitiveObjectInspector) expressionDef.getOI());\n-      return (s1 == null && s2 == null) || s1.equals(s2);\n+      return (s1 == null && s2 == null) || (s1 != null && s1.equals(s2));\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4bbaca8b33ed31cb67862f7c815ea7b6bbe5a2b4", "parent": "https://github.com/apache/hive/commit/289863694d242a16cdd5e8ed82bc8b4ef460bfdc", "message": "Fix split generation NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1661819 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_150", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/4bbaca8b33ed31cb67862f7c815ea7b6bbe5a2b4/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/4bbaca8b33ed31cb67862f7c815ea7b6bbe5a2b4/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java", "sha": "e6532c2308c904599fd2ef2fe1ea996957de4e95", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java?ref=4bbaca8b33ed31cb67862f7c815ea7b6bbe5a2b4", "patch": "@@ -214,6 +214,10 @@ public void configure(JobConf job) {\n     LOG.info(\"Wrapping \" + inputFormat);\n     @SuppressWarnings(\"unchecked\")\n     LlapIo<VectorizedRowBatch> llapIo = LlapIoProxy.getIo();\n+    if (llapIo == null) {\n+      LOG.info(\"Not using LLAP because IO is not initialized\");\n+      return inputFormat;\n+    }\n     return castInputFormat(llapIo.getInputFormat(inputFormat));\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5eaf74bacfc3f8402865e782f184f0106ea07d6f", "parent": "https://github.com/apache/hive/commit/828f1193c4f1a2b8867263ae169cd7ca046051b8", "message": "HIVE-9361 - Intermittent NPE in SessionHiveMetaStoreClient.alterTempTable\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1654854 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_151", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5eaf74bacfc3f8402865e782f184f0106ea07d6f/ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java", "blob_url": "https://github.com/apache/hive/blob/5eaf74bacfc3f8402865e782f184f0106ea07d6f/ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java", "sha": "d20f1963d93453f604249197ec246a4ad38a81d4", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java?ref=5eaf74bacfc3f8402865e782f184f0106ea07d6f", "patch": "@@ -387,7 +387,7 @@ private void alterTempTable(String dbname, String tbl_name,\n \n     org.apache.hadoop.hive.metastore.api.Table newtCopy = deepCopyAndLowerCaseTable(newt);\n     MetaStoreUtils.updateUnpartitionedTableStatsFast(newtCopy,\n-        wh.getFileStatusesForSD(newtCopy.getSd()), false, true);\n+        getWh().getFileStatusesForSD(newtCopy.getSd()), false, true);\n     Table newTable = new Table(newtCopy);\n     String newDbName = newTable.getDbName();\n     String newTableName = newTable.getTableName();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/51145e8d84640ed5fc43bfcf2b9e8f7ce2c3486f", "parent": "https://github.com/apache/hive/commit/eb06ce3f3f02bd4c1fb23bc1c22635db8c8e91be", "message": "HIVE-8576: Guaranteed NPE in StatsRulesProcFactory (Lars Francke via Prasanth J)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633978 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_152", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/51145e8d84640ed5fc43bfcf2b9e8f7ce2c3486f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/51145e8d84640ed5fc43bfcf2b9e8f7ce2c3486f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "sha": "357a474676853c095bd268f24c8eb19b09720c92", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java?ref=51145e8d84640ed5fc43bfcf2b9e8f7ce2c3486f", "patch": "@@ -1394,7 +1394,7 @@ private float getSelectivityComplexTree(Operator<? extends OperatorDesc> op) {\n      */\n     private List<Integer> getPrimaryKeyCandidates(List<Operator<? extends OperatorDesc>> ops) {\n       List<Integer> result = Lists.newArrayList();\n-      if (ops != null || !ops.isEmpty()) {\n+      if (ops != null && !ops.isEmpty()) {\n         for (int i = 0; i < ops.size(); i++) {\n           Operator<? extends OperatorDesc> op = ops.get(i);\n           if (op instanceof ReduceSinkOperator) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c50a627fe687027d08413a7aed9ec83e101b9ec2", "parent": "https://github.com/apache/hive/commit/3bebb7898ec5496502bffcda7993523da0585aa1", "message": "HIVE-8008: NPE while reading null decimal value (Chao via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1623261 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_153", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java", "blob_url": "https://github.com/apache/hive/blob/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java", "sha": "ad48f69c6fc72e09fd0ce03e50a658d8a3e4a342", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java?ref=c50a627fe687027d08413a7aed9ec83e101b9ec2", "patch": "@@ -254,16 +254,16 @@ public static BigDecimal enforcePrecisionScale(BigDecimal bd, int maxPrecision,\n       return null;\n     }\n \n+    if (bd.scale() > maxScale) {\n+      bd = bd.setScale(maxScale, RoundingMode.HALF_UP);\n+    }\n+\n     int maxIntDigits = maxPrecision - maxScale;\n     int intDigits = bd.precision() - bd.scale();\n     if (intDigits > maxIntDigits) {\n       return null;\n     }\n \n-    if (bd.scale() > maxScale) {\n-      bd = bd.setScale(maxScale, RoundingMode.HALF_UP);\n-    }\n-\n     return bd;\n   }\n }", "filename": "common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java", "blob_url": "https://github.com/apache/hive/blob/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java", "sha": "46a73f2eba706bf413dde31a26e302b3e4c7bd65", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java?ref=c50a627fe687027d08413a7aed9ec83e101b9ec2", "patch": "@@ -68,6 +68,13 @@ public void testPrecisionScaleEnforcement() {\n     Assert.assertEquals(\"0.02\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"0.015\"), 3, 2).toString());\n     Assert.assertEquals(\"0.01\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"0.0145\"), 3, 2).toString());\n \n+    // Rounding numbers that increase int digits\n+    Assert.assertEquals(\"10\",\n+        HiveDecimal.enforcePrecisionScale(new BigDecimal(\"9.5\"), 2, 0).toString());\n+    Assert.assertNull(HiveDecimal.enforcePrecisionScale(new BigDecimal(\"9.5\"), 1, 0));\n+    Assert.assertEquals(\"9\",\n+        HiveDecimal.enforcePrecisionScale(new BigDecimal(\"9.4\"), 1, 0).toString());\n+\n     // Integers with no scale values are not modified (zeros are not null)\n     Assert.assertEquals(\"0\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"0\"), 1, 0).toString());\n     Assert.assertEquals(\"30\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"30\"), 2, 0).toString());", "filename": "common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/626a4413a00f362fbeae6f86bb5ade2ce85ca274", "parent": "https://github.com/apache/hive/commit/153de35ee6504888d8b472b322837b2543c2ceeb", "message": " HIVE-7234: Select on decimal column throws NPE (Missing files)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1604690 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_154", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/626a4413a00f362fbeae6f86bb5ade2ce85ca274/data/files/decimal_10_0.txt", "blob_url": "https://github.com/apache/hive/blob/626a4413a00f362fbeae6f86bb5ade2ce85ca274/data/files/decimal_10_0.txt", "sha": "121e88f44d05bd3b6a5c169c832ce4da2a5cbcf9", "changes": 2, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/decimal_10_0.txt?ref=626a4413a00f362fbeae6f86bb5ade2ce85ca274", "patch": "@@ -0,0 +1,2 @@\n+999999999.999\n+9999999999.999\n\\ No newline at end of file", "filename": "data/files/decimal_10_0.txt"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2b969787ab1c215d4aecacfc14051089249702d1", "parent": "https://github.com/apache/hive/commit/7a3f05de797e980119629d6803635a2871cb01a9", "message": "HIVE-7234: Select on decimal column throws NPE (Ashish via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1604611 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_155", "file": [{"additions": 9, "raw_url": "https://github.com/apache/hive/raw/2b969787ab1c215d4aecacfc14051089249702d1/ql/src/test/queries/clientpositive/decimal_10_0.q", "blob_url": "https://github.com/apache/hive/blob/2b969787ab1c215d4aecacfc14051089249702d1/ql/src/test/queries/clientpositive/decimal_10_0.q", "sha": "02b547c6a14dc8bfb52e936a06843afefcf6ba50", "changes": 9, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/decimal_10_0.q?ref=2b969787ab1c215d4aecacfc14051089249702d1", "patch": "@@ -0,0 +1,9 @@\n+DROP TABLE IF EXISTS DECIMAL;\n+\n+CREATE TABLE DECIMAL (dec decimal);\n+\n+LOAD DATA LOCAL INPATH '../../data/files/decimal_10_0.txt' OVERWRITE INTO TABLE DECIMAL;\n+\n+SELECT dec FROM DECIMAL;\n+\n+DROP TABLE DECIMAL;\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/decimal_10_0.q"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/2b969787ab1c215d4aecacfc14051089249702d1/ql/src/test/results/clientpositive/decimal_10_0.q.out", "blob_url": "https://github.com/apache/hive/blob/2b969787ab1c215d4aecacfc14051089249702d1/ql/src/test/results/clientpositive/decimal_10_0.q.out", "sha": "722a0a0f8129be7e827500bf65340f28f0703673", "changes": 37, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_10_0.q.out?ref=2b969787ab1c215d4aecacfc14051089249702d1", "patch": "@@ -0,0 +1,37 @@\n+PREHOOK: query: DROP TABLE IF EXISTS DECIMAL\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE DECIMAL (dec decimal)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: CREATE TABLE DECIMAL (dec decimal)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@DECIMAL\n+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/decimal_10_0.txt' OVERWRITE INTO TABLE DECIMAL\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@decimal\n+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/decimal_10_0.txt' OVERWRITE INTO TABLE DECIMAL\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@decimal\n+PREHOOK: query: SELECT dec FROM DECIMAL\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT dec FROM DECIMAL\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal\n+#### A masked pattern was here ####\n+1000000000\n+NULL\n+PREHOOK: query: DROP TABLE DECIMAL\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@decimal\n+PREHOOK: Output: default@decimal\n+POSTHOOK: query: DROP TABLE DECIMAL\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@decimal\n+POSTHOOK: Output: default@decimal", "filename": "ql/src/test/results/clientpositive/decimal_10_0.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/2b969787ab1c215d4aecacfc14051089249702d1/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java", "blob_url": "https://github.com/apache/hive/blob/2b969787ab1c215d4aecacfc14051089249702d1/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java", "sha": "fcf1ac6693124a6854fc4109c2fd5259e41cecc9", "changes": 19, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java?ref=2b969787ab1c215d4aecacfc14051089249702d1", "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.hadoop.hive.serde2.lazy;\n \n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.ByteBuffer;\n import java.nio.charset.CharacterCodingException;\n \n import org.apache.commons.logging.Log;\n@@ -33,6 +36,7 @@\n \n   private final int precision;\n   private final int scale;\n+  private static final byte[] nullBytes = new byte[]{0x0, 0x0, 0x0, 0x0};\n \n   public LazyHiveDecimal(LazyHiveDecimalObjectInspector oi) {\n     super(oi);\n@@ -93,4 +97,19 @@ public HiveDecimalWritable getWritableObject() {\n     return data;\n   }\n \n+  /**\n+   * Writes HiveDecimal object to output stream as string\n+   * @param outputStream\n+   * @param hiveDecimal\n+   * @throws IOException\n+   */\n+  public static void writeUTF8(OutputStream outputStream, HiveDecimal hiveDecimal)\n+    throws IOException {\n+    if (hiveDecimal == null) {\n+      outputStream.write(nullBytes);\n+    } else {\n+      ByteBuffer b = Text.encode(hiveDecimal.toString());\n+      outputStream.write(b.array(), 0, b.limit());\n+    }\n+  }\n }", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/2b969787ab1c215d4aecacfc14051089249702d1/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java", "blob_url": "https://github.com/apache/hive/blob/2b969787ab1c215d4aecacfc14051089249702d1/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java", "sha": "1d6242265dc958ef0db1784c21f09d8217e35d8f", "changes": 6, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java?ref=2b969787ab1c215d4aecacfc14051089249702d1", "patch": "@@ -28,7 +28,6 @@\n import java.util.Properties;\n \n import org.apache.commons.codec.binary.Base64;\n-import org.apache.hadoop.hive.common.type.HiveDecimal;\n import org.apache.hadoop.hive.serde.serdeConstants;\n import org.apache.hadoop.hive.serde2.SerDeException;\n import org.apache.hadoop.hive.serde2.io.HiveCharWritable;\n@@ -261,9 +260,8 @@ public static void writePrimitiveUTF8(OutputStream out, Object o,\n       break;\n     }\n     case DECIMAL: {\n-      HiveDecimal bd = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);\n-      ByteBuffer b = Text.encode(bd.toString());\n-      out.write(b.array(), 0, b.limit());\n+      LazyHiveDecimal.writeUTF8(out,\n+        ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o));\n       break;\n     }\n     default: {", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1", "parent": "https://github.com/apache/hive/commit/7c2f45fd25a19e4231e59c545e010b947bf3f988", "message": "HIVE-5638: NPE in ConvertJoinMapJoin on Tez (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1535269 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_156", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractSMBJoinProc.java", "blob_url": "https://github.com/apache/hive/blob/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractSMBJoinProc.java", "sha": "0b7b1a321b3018c8106c797f78554e3836f80784", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractSMBJoinProc.java?ref=2a5b148c3a89ceb15a848549a9ded6a2da93a2c1", "patch": "@@ -473,7 +473,7 @@ protected boolean canConvertJoinToBucketMapJoin(\n     JoinDesc joinDesc = joinOp.getConf();\n     JoinCondDesc[] joinCondns = joinDesc.getConds();\n     Set<Integer> joinCandidates = MapJoinProcessor.getBigTableCandidates(joinCondns);\n-    if (joinCandidates == null) {\n+    if (joinCandidates.isEmpty()) {\n       // This is a full outer join. This can never be a map-join\n       // of any type. So return false.\n       return false;\n@@ -527,7 +527,7 @@ protected MapJoinOperator convertJoinToBucketMapJoin(\n     SortBucketJoinProcCtx joinContext,\n     ParseContext parseContext) throws SemanticException {\n     MapJoinOperator mapJoinOp = MapJoinProcessor.convertMapJoin(\n-      parseContext.getConf(), \n+      parseContext.getConf(),\n       parseContext.getOpParseCtx(),\n       joinOp,\n       pGraphContext.getJoinContext().get(joinOp),", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractSMBJoinProc.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java", "blob_url": "https://github.com/apache/hive/blob/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java", "sha": "eeca808532c18618333038039ae015136d6df2c4", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java?ref=2a5b148c3a89ceb15a848549a9ded6a2da93a2c1", "patch": "@@ -237,7 +237,7 @@ private static String genMapJoinLocalWork(MapredWork newWork, MapJoinOperator ma\n    * @return the alias to the big table\n    * @throws SemanticException\n    */\n-  public static String genMapJoinOpAndLocalWork(HiveConf conf, MapredWork newWork, \n+  public static String genMapJoinOpAndLocalWork(HiveConf conf, MapredWork newWork,\n     JoinOperator op, int mapJoinPos)\n       throws SemanticException {\n     LinkedHashMap<Operator<? extends OperatorDesc>, OpParseContext> opParseCtxMap =\n@@ -663,7 +663,7 @@ public MapJoinOperator generateMapJoinOperator(ParseContext pctx, JoinOperator o\n    * If see a right outer join, set lastSeenRightOuterJoin to true, clear the\n    * bigTableCandidates, and add right side to the bigTableCandidates, it means\n    * the right side of a right outer join always win. If see a full outer join,\n-   * return null immediately (no one can be the big table, can not do a\n+   * return empty set immediately (no one can be the big table, can not do a\n    * mapjoin).\n    *\n    *\n@@ -689,7 +689,8 @@ public MapJoinOperator generateMapJoinOperator(ParseContext pctx, JoinOperator o\n         // changed in future, these 2 are not missing.\n         seenOuterJoin = true;\n         lastSeenRightOuterJoin = false;\n-        return null;\n+        // empty set - cannot convert\n+        return new HashSet<Integer>();\n       } else if (joinType == JoinDesc.LEFT_OUTER_JOIN\n           || joinType == JoinDesc.LEFT_SEMI_JOIN) {\n         seenOuterJoin = true;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationOptimizer.java", "blob_url": "https://github.com/apache/hive/blob/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationOptimizer.java", "sha": "f38b8ccaeca08e0a0929fe21d4cb8276984e2cee", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationOptimizer.java?ref=2a5b148c3a89ceb15a848549a9ded6a2da93a2c1", "patch": "@@ -168,7 +168,7 @@ private void findPossibleAutoConvertedJoinOperators() throws SemanticException {\n       int numAliases = order.length;\n       Set<Integer> bigTableCandidates =\n           MapJoinProcessor.getBigTableCandidates(joinDesc.getConds());\n-      if (bigTableCandidates == null) {\n+      if (bigTableCandidates.isEmpty()) {\n         continue;\n       }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationOptimizer.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java", "blob_url": "https://github.com/apache/hive/blob/2a5b148c3a89ceb15a848549a9ded6a2da93a2c1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java", "sha": "58ffd19749bc0bfd42d9cf9f1c7541e82639526e", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java?ref=2a5b148c3a89ceb15a848549a9ded6a2da93a2c1", "patch": "@@ -420,7 +420,7 @@ public static boolean cannotConvert(String bigTableAlias,\n           .getConds());\n \n       // no table could be the big table; there is no need to convert\n-      if (bigTableCandidates == null) {\n+      if (bigTableCandidates.isEmpty()) {\n         return null;\n       }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/3e06c49cf283b1005b2ad0f199bc9a4f5e8329fc", "parent": "https://github.com/apache/hive/commit/e9b48de7db9acdf8dc68b954fca7fe1c9aa62c3a", "message": "HIVE-4935 : Potential NPE in MetadataOnlyOptimizer (Yin Huai via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1507342 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_157", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/3e06c49cf283b1005b2ad0f199bc9a4f5e8329fc/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MetadataOnlyOptimizer.java", "blob_url": "https://github.com/apache/hive/blob/3e06c49cf283b1005b2ad0f199bc9a4f5e8329fc/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MetadataOnlyOptimizer.java", "sha": "dc7ae6a0bbd86b103ed0272cbc6173704fac5aa2", "changes": 12, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MetadataOnlyOptimizer.java?ref=3e06c49cf283b1005b2ad0f199bc9a4f5e8329fc", "patch": "@@ -247,12 +247,6 @@ private String encode(Map<String, String> partSpec) {\n       return partSpec.toString().replaceAll(\"[:/#\\\\?]\", \"_\");\n     }\n \n-    private void convertToMetadataOnlyQuery(MapredWork work,\n-        TableScanOperator tso) {\n-      String alias = getAliasForTableScanOperator(work, tso);\n-      processAlias(work, alias);\n-    }\n-\n     @Override\n     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)\n         throws SemanticException {\n@@ -305,8 +299,10 @@ public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)\n \n       while (iterator.hasNext()) {\n         TableScanOperator tso = iterator.next();\n-        LOG.info(\"Metadata only table scan for \" + tso.getConf().getAlias());\n-        convertToMetadataOnlyQuery((MapredWork) task.getWork(), tso);\n+        MapredWork work = (MapredWork) task.getWork();\n+        String alias = getAliasForTableScanOperator(work, tso);\n+        LOG.info(\"Metadata only table scan for \" + alias);\n+        processAlias(work, alias);\n       }\n \n       return null;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MetadataOnlyOptimizer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/9a6d96991bab71551aae020d807d940d509b379d", "parent": "https://github.com/apache/hive/commit/51dfd1f94a145ef3b63ccda0246478bdca5b02f2", "message": "HIVE-4502 : NPE - subquery smb joins fails (Navis via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1505870 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_158", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java", "sha": "128ce77b57c1129c97c687e13253744cb3d85aa7", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -86,6 +86,7 @@\n \n   protected String id;\n   protected T work;\n+\n   public static enum FeedType {\n     DYNAMIC_PARTITIONS, // list of dynamic partitions\n   };", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/lib/DefaultGraphWalker.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/lib/DefaultGraphWalker.java", "sha": "69ea4a6587da7e3d3637649e89cddc2e43936e4d", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lib/DefaultGraphWalker.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -75,6 +75,13 @@ public DefaultGraphWalker(Dispatcher disp) {\n    * @throws SemanticException\n    */\n   public void dispatch(Node nd, Stack<Node> ndStack) throws SemanticException {\n+    dispatchAndReturn(nd, ndStack);\n+  }\n+\n+  /**\n+   * Returns dispatch result\n+   */\n+  public <T> T dispatchAndReturn(Node nd, Stack<Node> ndStack) throws SemanticException {\n     Object[] nodeOutputs = null;\n     if (nd.getChildren() != null) {\n       nodeOutputs = new Object[nd.getChildren().size()];\n@@ -86,6 +93,7 @@ public void dispatch(Node nd, Stack<Node> ndStack) throws SemanticException {\n \n     Object retVal = dispatcher.dispatch(nd, ndStack, nodeOutputs);\n     retMap.put(nd, retVal);\n+    return (T) retVal;\n   }\n \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/lib/DefaultGraphWalker.java"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java", "sha": "7f0253c112792a6c886715f5a333b17db76eddcd", "changes": 49, "status": "modified", "deletions": 36, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -91,6 +91,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     ParseContext parseCtx = ctx.getParseCtx();\n     boolean chDir = false;\n     Task<? extends Serializable> currTask = ctx.getCurrTask();\n+    ctx.addRootIfPossible(currTask);\n+\n     FileSinkOperator fsOp = (FileSinkOperator) nd;\n     boolean isInsertTable = // is INSERT OVERWRITE TABLE\n     fsOp.getConf().getTableInfo().getTableName() != null &&\n@@ -106,7 +108,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     if (fileSinkDescs != null) {\n       Task<? extends Serializable> childTask = fileSinkDescs.get(fsOp.getConf());\n       processLinkedFileDesc(ctx, childTask);\n-      return null;\n+      return true;\n     }\n \n     // Has the user enabled merging of files for map-only jobs or for all jobs\n@@ -181,34 +183,20 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n       }\n     }\n \n-    return null;\n+    return true;\n   }\n \n   /*\n    * Multiple file sink descriptors are linked.\n    * Use the task created by the first linked file descriptor\n    */\n   private void processLinkedFileDesc(GenMRProcContext ctx,\n-    Task<? extends Serializable> childTask)\n-    throws SemanticException {\n-    Operator<? extends OperatorDesc> currTopOp = ctx.getCurrTopOp();\n-    String currAliasId = ctx.getCurrAliasId();\n-    List<Operator<? extends OperatorDesc>> seenOps = ctx.getSeenOps();\n-    List<Task<? extends Serializable>> rootTasks = ctx.getRootTasks();\n+      Task<? extends Serializable> childTask) throws SemanticException {\n     Task<? extends Serializable> currTask = ctx.getCurrTask();\n-\n-    if (currTopOp != null) {\n-      if (!seenOps.contains(currTopOp)) {\n-        seenOps.add(currTopOp);\n-        GenMapRedUtils.setTaskPlan(currAliasId, currTopOp,\n-          (MapredWork) currTask.getWork(), false, ctx);\n-      }\n-\n-      if (!rootTasks.contains(currTask)\n-          && (currTask.getParentTasks() == null\n-              || currTask.getParentTasks().isEmpty())) {\n-        rootTasks.add(currTask);\n-      }\n+    Operator<? extends OperatorDesc> currTopOp = ctx.getCurrTopOp();\n+    if (currTopOp != null && !ctx.isSeenOp(currTask, currTopOp)) {\n+      String currAliasId = ctx.getCurrAliasId();\n+      GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currTask, false, ctx);\n     }\n \n     if (childTask != null) {\n@@ -702,8 +690,6 @@ private String processFS(FileSinkOperator fsOp, Stack<Node> stack,\n     String currAliasId = ctx.getCurrAliasId();\n     HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap =\n         ctx.getOpTaskMap();\n-    List<Operator<? extends OperatorDesc>> seenOps = ctx.getSeenOps();\n-    List<Task<? extends Serializable>> rootTasks = ctx.getRootTasks();\n \n     // Set the move task to be dependent on the current task\n     if (mvTask != null) {\n@@ -717,22 +703,13 @@ private String processFS(FileSinkOperator fsOp, Stack<Node> stack,\n     if (currTopOp != null) {\n       Task<? extends Serializable> mapTask = opTaskMap.get(null);\n       if (mapTask == null) {\n-        if (!seenOps.contains(currTopOp)) {\n-          seenOps.add(currTopOp);\n-          GenMapRedUtils.setTaskPlan(currAliasId, currTopOp,\n-              (MapredWork) currTask.getWork(), false, ctx);\n+        if (!ctx.isSeenOp(currTask, currTopOp)) {\n+          GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currTask, false, ctx);\n         }\n         opTaskMap.put(null, currTask);\n-        if (!rootTasks.contains(currTask)\n-            && (currTask.getParentTasks() == null\n-                || currTask.getParentTasks().isEmpty())) {\n-          rootTasks.add(currTask);\n-        }\n       } else {\n-        if (!seenOps.contains(currTopOp)) {\n-          seenOps.add(currTopOp);\n-          GenMapRedUtils.setTaskPlan(currAliasId, currTopOp,\n-              (MapredWork) mapTask.getWork(), false, ctx);\n+        if (!ctx.isSeenOp(currTask, currTopOp)) {\n+          GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, mapTask, false, ctx);\n         } else {\n           UnionOperator currUnionOp = ctx.getCurrUnionOp();\n           if (currUnionOp != null) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java", "sha": "4f3eb0640b168753ba8c96af1575e18f51dd9d66", "changes": 5, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -53,8 +53,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         .getMapCurrCtx();\n     GenMapRedCtx mapredCtx = mapCurrCtx.get(stack.get(stack.size() - 2));\n     mapCurrCtx.put((Operator<? extends OperatorDesc>) nd, new GenMapRedCtx(\n-        mapredCtx.getCurrTask(), mapredCtx.getCurrTopOp(), mapredCtx\n-        .getCurrAliasId()));\n-    return null;\n+        mapredCtx.getCurrTask(), mapredCtx.getCurrAliasId()));\n+    return true;\n   }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMROperator.java"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java", "sha": "82a0b491a6ffa4dc118dba5367be94ba6e21d4ef", "changes": 85, "status": "modified", "deletions": 55, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -55,7 +55,6 @@\n    */\n   public static class GenMapRedCtx {\n     Task<? extends Serializable> currTask;\n-    Operator<? extends OperatorDesc> currTopOp;\n     String currAliasId;\n \n     public GenMapRedCtx() {\n@@ -64,15 +63,10 @@ public GenMapRedCtx() {\n     /**\n      * @param currTask\n      *          the current task\n-     * @param currTopOp\n-     *          the current top operator being traversed\n      * @param currAliasId\n-     *          the current alias for the to operator\n      */\n-    public GenMapRedCtx(Task<? extends Serializable> currTask,\n-        Operator<? extends OperatorDesc> currTopOp, String currAliasId) {\n+    public GenMapRedCtx(Task<? extends Serializable> currTask, String currAliasId) {\n       this.currTask = currTask;\n-      this.currTopOp = currTopOp;\n       this.currAliasId = currAliasId;\n     }\n \n@@ -83,13 +77,6 @@ public GenMapRedCtx(Task<? extends Serializable> currTask,\n       return currTask;\n     }\n \n-    /**\n-     * @return current top operator\n-     */\n-    public Operator<? extends OperatorDesc> getCurrTopOp() {\n-      return currTopOp;\n-    }\n-\n     /**\n      * @return current alias\n      */\n@@ -103,13 +90,13 @@ public String getCurrAliasId() {\n    *\n    */\n   public static class GenMRUnionCtx {\n-    Task<? extends Serializable> uTask;\n+    final Task<? extends Serializable> uTask;\n     List<String> taskTmpDir;\n     List<TableDesc> tt_desc;\n     List<Operator<? extends OperatorDesc>> listTopOperators;\n \n-    public GenMRUnionCtx() {\n-      uTask = null;\n+    public GenMRUnionCtx(Task<? extends Serializable> uTask) {\n+      this.uTask = uTask;\n       taskTmpDir = new ArrayList<String>();\n       tt_desc = new ArrayList<TableDesc>();\n       listTopOperators = new ArrayList<Operator<? extends OperatorDesc>>();\n@@ -119,10 +106,6 @@ public GenMRUnionCtx() {\n       return uTask;\n     }\n \n-    public void setUTask(Task<? extends Serializable> uTask) {\n-      this.uTask = uTask;\n-    }\n-\n     public void addTaskTmpDir(String taskTmpDir) {\n       this.taskTmpDir.add(taskTmpDir);\n     }\n@@ -156,8 +139,10 @@ public void addListTopOperators(Operator<? extends OperatorDesc> topOperator) {\n   private HiveConf conf;\n   private\n     HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap;\n+  private\n+    HashMap<Task<? extends Serializable>, List<Operator<? extends OperatorDesc>>> taskToSeenOps;\n+\n   private HashMap<UnionOperator, GenMRUnionCtx> unionTaskMap;\n-  private List<Operator<? extends OperatorDesc>> seenOps;\n   private List<FileSinkOperator> seenFileSinkOps;\n \n   private ParseContext parseCtx;\n@@ -169,7 +154,6 @@ public void addListTopOperators(Operator<? extends OperatorDesc> topOperator) {\n   private Operator<? extends OperatorDesc> currTopOp;\n   private UnionOperator currUnionOp;\n   private String currAliasId;\n-  private List<Operator<? extends OperatorDesc>> rootOps;\n   private DependencyCollectionTask dependencyTaskForMultiInsert;\n \n   // If many fileSinkDescs are linked to each other, it is a good idea to keep track of\n@@ -213,14 +197,13 @@ public GenMRProcContext() {\n   public GenMRProcContext(\n       HiveConf conf,\n       HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap,\n-      List<Operator<? extends OperatorDesc>> seenOps, ParseContext parseCtx,\n+      ParseContext parseCtx,\n       List<Task<MoveWork>> mvTask,\n       List<Task<? extends Serializable>> rootTasks,\n       LinkedHashMap<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx,\n       Set<ReadEntity> inputs, Set<WriteEntity> outputs) {\n     this.conf = conf;\n     this.opTaskMap = opTaskMap;\n-    this.seenOps = seenOps;\n     this.mvTask = mvTask;\n     this.parseCtx = parseCtx;\n     this.rootTasks = rootTasks;\n@@ -231,9 +214,9 @@ public GenMRProcContext(\n     currTopOp = null;\n     currUnionOp = null;\n     currAliasId = null;\n-    rootOps = new ArrayList<Operator<? extends OperatorDesc>>();\n-    rootOps.addAll(parseCtx.getTopOps().values());\n     unionTaskMap = new HashMap<UnionOperator, GenMRUnionCtx>();\n+    taskToSeenOps = new HashMap<Task<? extends Serializable>,\n+        List<Operator<? extends OperatorDesc>>>();\n     dependencyTaskForMultiInsert = null;\n     linkedFileDescTasks = null;\n   }\n@@ -255,11 +238,17 @@ public void setOpTaskMap(\n     this.opTaskMap = opTaskMap;\n   }\n \n-  /**\n-   * @return operators already visited\n-   */\n-  public List<Operator<? extends OperatorDesc>> getSeenOps() {\n-    return seenOps;\n+  public boolean isSeenOp(Task task, Operator operator) {\n+    List<Operator<?extends OperatorDesc>> seenOps = taskToSeenOps.get(task);\n+    return seenOps != null && seenOps.contains(operator);\n+  }\n+\n+  public void addSeenOp(Task task, Operator operator) {\n+    List<Operator<?extends OperatorDesc>> seenOps = taskToSeenOps.get(task);\n+    if (seenOps == null) {\n+      taskToSeenOps.put(task, seenOps = new ArrayList<Operator<? extends OperatorDesc>>());\n+    }\n+    seenOps.add(operator);\n   }\n \n   /**\n@@ -269,14 +258,6 @@ public void setOpTaskMap(\n     return seenFileSinkOps;\n   }\n \n-  /**\n-   * @param seenOps\n-   *          operators already visited\n-   */\n-  public void setSeenOps(List<Operator<? extends OperatorDesc>> seenOps) {\n-    this.seenOps = seenOps;\n-  }\n-\n   /**\n    * @param seenFileSinkOps\n    *          file sink operators already visited\n@@ -285,21 +266,6 @@ public void setSeenFileSinkOps(List<FileSinkOperator> seenFileSinkOps) {\n     this.seenFileSinkOps = seenFileSinkOps;\n   }\n \n-  /**\n-   * @return top operators for tasks\n-   */\n-  public List<Operator<? extends OperatorDesc>> getRootOps() {\n-    return rootOps;\n-  }\n-\n-  /**\n-   * @param rootOps\n-   *          top operators for tasks\n-   */\n-  public void setRootOps(List<Operator<? extends OperatorDesc>> rootOps) {\n-    this.rootOps = rootOps;\n-  }\n-\n   /**\n    * @return current parse context\n    */\n@@ -345,6 +311,15 @@ public void setRootTasks(List<Task<? extends Serializable>> rootTasks) {\n     this.rootTasks = rootTasks;\n   }\n \n+  public boolean addRootIfPossible(Task<? extends Serializable> task) {\n+    if (task.getParentTasks() == null || task.getParentTasks().isEmpty()) {\n+      if (!rootTasks.contains(task)) {\n+        return rootTasks.add(task);\n+      }\n+    }\n+    return false;\n+  }\n+\n   /**\n    * @return operator to task mappings\n    */", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRProcContext.java"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java", "sha": "489200326ec77d2e1f1dbf7d7e9340cfcfcfef59", "changes": 30, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -19,7 +19,6 @@\n package org.apache.hadoop.hive.ql.optimizer;\n \n import java.io.Serializable;\n-import java.util.HashMap;\n import java.util.Map;\n import java.util.Stack;\n \n@@ -64,19 +63,20 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     GenMapRedCtx mapredCtx = mapCurrCtx.get(stack.get(stack.size() - 2));\n     Task<? extends Serializable> currTask = mapredCtx.getCurrTask();\n     MapredWork currPlan = (MapredWork) currTask.getWork();\n-    Operator<? extends OperatorDesc> currTopOp = mapredCtx.getCurrTopOp();\n     String currAliasId = mapredCtx.getCurrAliasId();\n+\n+    if (op.getNumChild() != 1) {\n+      throw new IllegalStateException(\"Expecting operator \" + op + \" to have one child. \" +\n+          \"But found multiple children : \" + op.getChildOperators());\n+    }\n     Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);\n-    HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap = ctx\n-        .getOpTaskMap();\n-    Task<? extends Serializable> opMapTask = opTaskMap.get(reducer);\n+    Task<? extends Serializable> oldTask = ctx.getOpTaskMap().get(reducer);\n \n-    ctx.setCurrTopOp(currTopOp);\n     ctx.setCurrAliasId(currAliasId);\n     ctx.setCurrTask(currTask);\n \n     // If the plan for this reducer does not exist, initialize the plan\n-    if (opMapTask == null) {\n+    if (oldTask == null) {\n       if (currPlan.getReducer() == null) {\n         GenMapRedUtils.initPlan(op, ctx);\n       } else {\n@@ -85,14 +85,18 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     } else {\n       // This will happen in case of joins. The current plan can be thrown away\n       // after being merged with the original plan\n-      GenMapRedUtils.joinPlan(op, null, opMapTask, ctx, -1, false);\n-      currTask = opMapTask;\n+      GenMapRedUtils.joinPlan(currTask, oldTask, ctx);\n+      currTask = oldTask;\n       ctx.setCurrTask(currTask);\n     }\n \n-    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(),\n-        ctx.getCurrAliasId()));\n-    return null;\n-  }\n+    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrAliasId()));\n \n+    if (GenMapRedUtils.hasBranchFinished(nodeOutputs)) {\n+      ctx.addRootIfPossible(currTask);\n+      return false;\n+    }\n+\n+    return true;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java", "sha": "6c34bfe7c834bf6bea05c5ca43d4237279d92504", "changes": 20, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -57,28 +57,32 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n         .getMapCurrCtx();\n     GenMapRedCtx mapredCtx = mapCurrCtx.get(op.getParentOperators().get(0));\n     Task<? extends Serializable> currTask = mapredCtx.getCurrTask();\n-    Operator<? extends OperatorDesc> currTopOp = mapredCtx.getCurrTopOp();\n     String currAliasId = mapredCtx.getCurrAliasId();\n     Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);\n     Map<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap = ctx\n         .getOpTaskMap();\n-    Task<? extends Serializable> opMapTask = opTaskMap.get(reducer);\n+    Task<? extends Serializable> oldTask = opTaskMap.get(reducer);\n \n-    ctx.setCurrTopOp(currTopOp);\n     ctx.setCurrAliasId(currAliasId);\n     ctx.setCurrTask(currTask);\n \n-    if (opMapTask == null) {\n+    if (oldTask == null) {\n       GenMapRedUtils.splitPlan(op, ctx);\n     } else {\n-      GenMapRedUtils.joinPlan(op, currTask, opMapTask, ctx, -1, true);\n-      currTask = opMapTask;\n+      GenMapRedUtils.splitPlan(op, currTask, oldTask, ctx);\n+      currTask = oldTask;\n       ctx.setCurrTask(currTask);\n     }\n \n-    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(),\n+    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(),\n         ctx.getCurrAliasId()));\n-    return null;\n+\n+    if (GenMapRedUtils.hasBranchFinished(nodeOutputs)) {\n+      ctx.addRootIfPossible(currTask);\n+      return false;\n+    }\n+\n+    return true;\n   }\n \n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java", "sha": "edab749418023885a13aa9f020f44bfc3a5b8748", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -101,11 +101,11 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n       ctx.setCurrTask(reducerTask);\n     }\n \n-    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(),\n+    mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(),\n         ctx.getCurrAliasId()));\n \n     // the union operator has been processed\n     ctx.setCurrUnionOp(null);\n-    return null;\n+    return true;\n   }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java", "sha": "f5dd43f03b0b0081da7d86f2256d5f506867b0f7", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -84,7 +84,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n       if (currOp == op) {\n         String currAliasId = alias;\n         ctx.setCurrAliasId(currAliasId);\n-        mapCurrCtx.put(op, new GenMapRedCtx(currTask, currTopOp, currAliasId));\n+        mapCurrCtx.put(op, new GenMapRedCtx(currTask, currAliasId));\n \n         QBParseInfo parseInfo = parseCtx.getQB().getParseInfo();\n         if (parseInfo.isAnalyzeCommand()) {\n@@ -139,12 +139,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n             Table source = parseCtx.getQB().getMetaData().getTableForAlias(alias);\n             PrunedPartitionList partList = new PrunedPartitionList(source, confirmedPartns,\n                 new HashSet<Partition>(), null);\n-            GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currWork, false, ctx, partList);\n+            GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currTask, false, ctx, partList);\n           } else { // non-partitioned table\n-            GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currWork, false, ctx);\n+            GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currTask, false, ctx);\n           }\n         }\n-        return null;\n+        return true;\n       }\n     }\n     assert false;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java", "sha": "52e9e6bf615feebf5664755cb252efcfbb4752de", "changes": 24, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -82,14 +82,13 @@ private Object processMapOnlyUnion(UnionOperator union, Stack<Node> stack,\n     UnionParseContext uPrsCtx = uCtx.getUnionParseContext(union);\n     ctx.getMapCurrCtx().put(\n         (Operator<? extends OperatorDesc>) union,\n-        new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(),\n+        new GenMapRedCtx(ctx.getCurrTask(),\n             ctx.getCurrAliasId()));\n \n     // if the union is the first time seen, set current task to GenMRUnionCtx\n     uCtxTask = ctx.getUnionTask(union);\n     if (uCtxTask == null) {\n-      uCtxTask = new GenMRUnionCtx();\n-      uCtxTask.setUTask(ctx.getCurrTask());\n+      uCtxTask = new GenMRUnionCtx(ctx.getCurrTask());\n       ctx.setUnionTask(union, uCtxTask);\n     }\n \n@@ -101,7 +100,7 @@ private Object processMapOnlyUnion(UnionOperator union, Stack<Node> stack,\n       }\n     }\n \n-    return null;\n+    return true;\n   }\n \n   /**\n@@ -192,14 +191,11 @@ private void processSubQueryUnionMerge(GenMRProcContext ctx,\n     // The current plan can be thrown away after being merged with the union\n     // plan\n     Task<? extends Serializable> uTask = uCtxTask.getUTask();\n-    MapredWork plan = (MapredWork) uTask.getWork();\n     ctx.setCurrTask(uTask);\n-    List<Operator<? extends OperatorDesc>> seenOps = ctx.getSeenOps();\n     Operator<? extends OperatorDesc> topOp = ctx.getCurrTopOp();\n-    if (!seenOps.contains(topOp) && topOp != null) {\n-      seenOps.add(topOp);\n+    if (topOp != null && !ctx.isSeenOp(uTask, topOp)) {\n       GenMapRedUtils.setTaskPlan(ctx.getCurrAliasId(), ctx\n-          .getCurrTopOp(), plan, false, ctx);\n+          .getCurrTopOp(), uTask, false, ctx);\n     }\n   }\n \n@@ -230,8 +226,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n       // All inputs of this UnionOperator are in the same Reducer.\n       // We do not need to break the operator tree.\n       mapCurrCtx.put((Operator<? extends OperatorDesc>) nd,\n-          new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(),\n-              ctx.getCurrAliasId()));\n+        new GenMapRedCtx(ctx.getCurrTask(),ctx.getCurrAliasId()));\n       return null;\n     }\n \n@@ -255,10 +250,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     // union is encountered for the first time\n     GenMRUnionCtx uCtxTask = ctx.getUnionTask(union);\n     if (uCtxTask == null) {\n-      uCtxTask = new GenMRUnionCtx();\n       uPlan = GenMapRedUtils.getMapRedWork(parseCtx);\n       uTask = TaskFactory.get(uPlan, parseCtx.getConf());\n-      uCtxTask.setUTask(uTask);\n+      uCtxTask = new GenMRUnionCtx(uTask);\n       ctx.setUnionTask(union, uCtxTask);\n     }\n     else {\n@@ -293,9 +287,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     ctx.setCurrTask(uTask);\n \n     mapCurrCtx.put((Operator<? extends OperatorDesc>) nd,\n-        new GenMapRedCtx(ctx.getCurrTask(), null, null));\n+        new GenMapRedCtx(ctx.getCurrTask(), null));\n \n-    return null;\n+    return true;\n   }\n \n   private boolean shouldBeRootTask(", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java"}, {"additions": 117, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "sha": "b5a9291f9c25673b116bc60508570844d0c47db3", "changes": 237, "status": "modified", "deletions": 120, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.hive.ql.exec.UnionOperator;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.exec.mr.ExecDriver;\n+import org.apache.hadoop.hive.ql.exec.mr.MapRedTask;\n import org.apache.hadoop.hive.ql.hooks.ReadEntity;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n@@ -60,7 +61,6 @@\n import org.apache.hadoop.hive.ql.plan.FetchWork;\n import org.apache.hadoop.hive.ql.plan.FileSinkDesc;\n import org.apache.hadoop.hive.ql.plan.FilterDesc.sampleDesc;\n-import org.apache.hadoop.hive.ql.plan.MapJoinDesc;\n import org.apache.hadoop.hive.ql.plan.MapredLocalWork;\n import org.apache.hadoop.hive.ql.plan.MapredWork;\n import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n@@ -81,6 +81,10 @@\n     LOG = LogFactory.getLog(\"org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils\");\n   }\n \n+  private static boolean needsTagging(Operator<? extends OperatorDesc> reducer) {\n+    return (reducer.getClass() == JoinOperator.class ||\n+        reducer.getClass() == DemuxOperator.class);\n+  }\n   /**\n    * Initialize the current plan by adding it to root tasks.\n    *\n@@ -107,25 +111,15 @@ public static void initPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx)\n \n     plan.setNumReduceTasks(desc.getNumReducers());\n \n-    List<Task<? extends Serializable>> rootTasks = opProcCtx.getRootTasks();\n-\n-    if (!rootTasks.contains(currTask)\n-        && (currTask.getParentTasks() == null\n-            || currTask.getParentTasks().isEmpty())) {\n-      rootTasks.add(currTask);\n-    }\n-    if (reducer.getClass() == JoinOperator.class ||\n-        reducer.getClass() == DemuxOperator.class) {\n+    if (needsTagging(reducer)) {\n       plan.setNeedsTagging(true);\n     }\n \n     assert currTopOp != null;\n-    List<Operator<? extends OperatorDesc>> seenOps = opProcCtx.getSeenOps();\n     String currAliasId = opProcCtx.getCurrAliasId();\n \n-    if (!seenOps.contains(currTopOp)) {\n-      seenOps.add(currTopOp);\n-      setTaskPlan(currAliasId, currTopOp, plan, false, opProcCtx);\n+    if (!opProcCtx.isSeenOp(currTask, currTopOp)) {\n+      setTaskPlan(currAliasId, currTopOp, currTask, false, opProcCtx);\n     }\n \n     currTopOp = null;\n@@ -160,25 +154,22 @@ public static void initUnionPlan(ReduceSinkOperator op, UnionOperator currUnionO\n \n     plan.setNumReduceTasks(desc.getNumReducers());\n \n-    if (reducer.getClass() == JoinOperator.class ||\n-        reducer.getClass() == DemuxOperator.class) {\n+    if (needsTagging(reducer)) {\n       plan.setNeedsTagging(true);\n     }\n \n     initUnionPlan(opProcCtx, currUnionOp, unionTask, false);\n   }\n \n   private static void setUnionPlan(GenMRProcContext opProcCtx,\n-      boolean local, MapredWork plan, GenMRUnionCtx uCtx,\n+      boolean local, Task<? extends Serializable> currTask, GenMRUnionCtx uCtx,\n       boolean mergeTask) throws SemanticException {\n     Operator<? extends OperatorDesc> currTopOp = opProcCtx.getCurrTopOp();\n \n     if (currTopOp != null) {\n-      List<Operator<? extends OperatorDesc>> seenOps = opProcCtx.getSeenOps();\n       String currAliasId = opProcCtx.getCurrAliasId();\n-      if (!seenOps.contains(currTopOp) || mergeTask) {\n-        seenOps.add(currTopOp);\n-        setTaskPlan(currAliasId, currTopOp, plan, local, opProcCtx);\n+      if (mergeTask || !opProcCtx.isSeenOp(currTask, currTopOp)) {\n+        setTaskPlan(currAliasId, currTopOp, currTask, local, opProcCtx);\n       }\n       currTopOp = null;\n       opProcCtx.setCurrTopOp(currTopOp);\n@@ -194,6 +185,7 @@ private static void setUnionPlan(GenMRProcContext opProcCtx,\n         List<Operator<? extends OperatorDesc>> topOperators =\n             uCtx.getListTopOperators();\n \n+        MapredWork plan = (MapredWork) currTask.getWork();\n         for (int pos = 0; pos < size; pos++) {\n           String taskTmpDir = taskTmpDirLst.get(pos);\n           TableDesc tt_desc = tt_descLst.get(pos);\n@@ -217,14 +209,12 @@ private static void setUnionPlan(GenMRProcContext opProcCtx,\n   public static void initUnionPlan(GenMRProcContext opProcCtx, UnionOperator currUnionOp,\n       Task<? extends Serializable> currTask, boolean local)\n       throws SemanticException {\n-    MapredWork plan = (MapredWork) currTask.getWork();\n-\n     // In case of lateral views followed by a join, the same tree\n     // can be traversed more than one\n     if (currUnionOp != null) {\n       GenMRUnionCtx uCtx = opProcCtx.getUnionTask(currUnionOp);\n       assert uCtx != null;\n-      setUnionPlan(opProcCtx, local, plan, uCtx, false);\n+      setUnionPlan(opProcCtx, local, currTask, uCtx, false);\n     }\n   }\n \n@@ -236,12 +226,11 @@ public static void joinUnionPlan(GenMRProcContext opProcCtx,\n       Task<? extends Serializable> currentUnionTask,\n       Task<? extends Serializable> existingTask, boolean local)\n       throws SemanticException {\n-    MapredWork plan = (MapredWork) existingTask.getWork();\n     assert currUnionOp != null;\n     GenMRUnionCtx uCtx = opProcCtx.getUnionTask(currUnionOp);\n     assert uCtx != null;\n \n-    setUnionPlan(opProcCtx, local, plan, uCtx, true);\n+    setUnionPlan(opProcCtx, local, existingTask, uCtx, true);\n \n     List<Task<? extends Serializable>> parTasks = null;\n     if (opProcCtx.getRootTasks().contains(currentUnionTask)) {\n@@ -276,104 +265,105 @@ public static void joinUnionPlan(GenMRProcContext opProcCtx,\n   }\n \n   /**\n-   * Merge the current task with the task for the current reducer.\n+   * Merge the current task into the old task for the reducer\n    *\n-   * @param op\n-   *          operator being processed\n+   * @param currTask\n+   *          the current task for the current reducer\n    * @param oldTask\n    *          the old task for the current reducer\n-   * @param task\n-   *          the current task for the current reducer\n    * @param opProcCtx\n    *          processing context\n-   * @param pos\n-   *          position of the parent in the stack\n    */\n-  public static void joinPlan(Operator<? extends OperatorDesc> op,\n-      Task<? extends Serializable> oldTask, Task<? extends Serializable> task,\n-      GenMRProcContext opProcCtx, int pos, boolean split)\n+  public static void joinPlan(Task<? extends Serializable> currTask,\n+      Task<? extends Serializable> oldTask, GenMRProcContext opProcCtx)\n       throws SemanticException {\n-    Task<? extends Serializable> currTask = task;\n-    MapredWork plan = (MapredWork) currTask.getWork();\n+    assert currTask != null && oldTask != null;\n+\n     Operator<? extends OperatorDesc> currTopOp = opProcCtx.getCurrTopOp();\n     List<Task<? extends Serializable>> parTasks = null;\n-\n     // terminate the old task and make current task dependent on it\n-    if (split) {\n-      assert oldTask != null;\n-      splitTasks(op, oldTask, currTask, opProcCtx, true, false, 0);\n-    } else {\n-      if ((oldTask != null) && (oldTask.getParentTasks() != null)\n-          && !oldTask.getParentTasks().isEmpty()) {\n-        parTasks = new ArrayList<Task<? extends Serializable>>();\n-        parTasks.addAll(oldTask.getParentTasks());\n-\n-        Object[] parTaskArr = parTasks.toArray();\n-        for (Object element : parTaskArr) {\n-          ((Task<? extends Serializable>) element).removeDependentTask(oldTask);\n-        }\n+    if (currTask.getParentTasks() != null\n+        && !currTask.getParentTasks().isEmpty()) {\n+      parTasks = new ArrayList<Task<? extends Serializable>>();\n+      parTasks.addAll(currTask.getParentTasks());\n+\n+      Object[] parTaskArr = parTasks.toArray();\n+      for (Object element : parTaskArr) {\n+        ((Task<? extends Serializable>) element).removeDependentTask(currTask);\n       }\n     }\n \n     if (currTopOp != null) {\n-      List<Operator<? extends OperatorDesc>> seenOps = opProcCtx.getSeenOps();\n-      String currAliasId = opProcCtx.getCurrAliasId();\n-\n-      if (!seenOps.contains(currTopOp)) {\n-        seenOps.add(currTopOp);\n-        boolean local = false;\n-        if (pos != -1) {\n-          local = (pos == ((MapJoinDesc) op.getConf()).getPosBigTable()) ? false\n-              : true;\n-        }\n-        setTaskPlan(currAliasId, currTopOp, plan, local, opProcCtx);\n-      }\n-      currTopOp = null;\n-      opProcCtx.setCurrTopOp(currTopOp);\n+      mergeInput(currTopOp, opProcCtx, oldTask, false);\n     }\n \n-    if ((oldTask != null) && (parTasks != null)) {\n+    if (parTasks != null) {\n       for (Task<? extends Serializable> parTask : parTasks) {\n-        parTask.addDependentTask(currTask);\n-        if (opProcCtx.getRootTasks().contains(currTask)) {\n-          opProcCtx.getRootTasks().remove(currTask);\n-        }\n+        parTask.addDependentTask(oldTask);\n       }\n     }\n \n-    opProcCtx.setCurrTask(currTask);\n+    if (oldTask instanceof MapRedTask && currTask instanceof MapRedTask) {\n+      ((MapRedTask)currTask).getWork().mergingInto(((MapRedTask) oldTask).getWork());\n+    }\n+\n+    opProcCtx.setCurrTopOp(null);\n+    opProcCtx.setCurrTask(oldTask);\n   }\n \n   /**\n-   * Split the current plan by creating a temporary destination.\n+   * If currTopOp is not set for input of the task, add input for to the task\n+   */\n+  static boolean mergeInput(Operator<? extends OperatorDesc> currTopOp,\n+      GenMRProcContext opProcCtx, Task<? extends Serializable> task, boolean local)\n+      throws SemanticException {\n+    if (!opProcCtx.isSeenOp(task, currTopOp)) {\n+      String currAliasId = opProcCtx.getCurrAliasId();\n+      setTaskPlan(currAliasId, currTopOp, task, local, opProcCtx);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * Met cRS in pRS(parentTask)-cRS-OP(childTask) case\n+   * Split and link two tasks by temporary file : pRS-FS / TS-cRS-OP\n+   */\n+  static void splitPlan(ReduceSinkOperator cRS,\n+      Task<? extends Serializable> parentTask, Task<? extends Serializable> childTask,\n+      GenMRProcContext opProcCtx) throws SemanticException {\n+    assert parentTask != null && childTask != null;\n+    splitTasks(cRS, parentTask, childTask, opProcCtx);\n+  }\n+\n+  /**\n+   * Met cRS in pOP(parentTask with RS)-cRS-cOP(noTask) case\n+   * Create new child task for cRS-cOP and link two tasks by temporary file : pOP-FS / TS-cRS-cOP\n    *\n-   * @param op\n+   * @param cRS\n    *          the reduce sink operator encountered\n    * @param opProcCtx\n    *          processing context\n    */\n-  public static void splitPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx)\n+  static void splitPlan(ReduceSinkOperator cRS, GenMRProcContext opProcCtx)\n       throws SemanticException {\n     // Generate a new task\n     ParseContext parseCtx = opProcCtx.getParseCtx();\n-    MapredWork cplan = getMapRedWork(parseCtx);\n-    Task<? extends Serializable> redTask = TaskFactory.get(cplan, parseCtx\n+    Task<? extends Serializable> parentTask = opProcCtx.getCurrTask();\n+\n+    MapredWork childPlan = getMapRedWork(parseCtx);\n+    Task<? extends Serializable> childTask = TaskFactory.get(childPlan, parseCtx\n         .getConf());\n-    Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);\n+    Operator<? extends OperatorDesc> reducer = cRS.getChildOperators().get(0);\n \n     // Add the reducer\n-    cplan.setReducer(reducer);\n-    ReduceSinkDesc desc = op.getConf();\n-\n-    cplan.setNumReduceTasks(new Integer(desc.getNumReducers()));\n+    childPlan.setReducer(reducer);\n+    ReduceSinkDesc desc = cRS.getConf();\n+    childPlan.setNumReduceTasks(new Integer(desc.getNumReducers()));\n \n-    HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap =\n-        opProcCtx.getOpTaskMap();\n-    opTaskMap.put(reducer, redTask);\n-    Task<? extends Serializable> currTask = opProcCtx.getCurrTask();\n+    opProcCtx.getOpTaskMap().put(reducer, childTask);\n \n-    splitTasks(op, currTask, redTask, opProcCtx, true, false, 0);\n-    opProcCtx.getRootOps().add(op);\n+    splitTasks(cRS, parentTask, childTask, opProcCtx);\n   }\n \n   /**\n@@ -391,9 +381,9 @@ public static void splitPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx)\n    *          processing context\n    */\n   public static void setTaskPlan(String alias_id,\n-      Operator<? extends OperatorDesc> topOp, MapredWork plan, boolean local,\n+      Operator<? extends OperatorDesc> topOp, Task<?> task, boolean local,\n       GenMRProcContext opProcCtx) throws SemanticException {\n-    setTaskPlan(alias_id, topOp, plan, local, opProcCtx, null);\n+    setTaskPlan(alias_id, topOp, task, local, opProcCtx, null);\n   }\n \n   private static ReadEntity getParentViewInfo(String alias_id,\n@@ -435,8 +425,9 @@ private static ReadEntity getParentViewInfo(String alias_id,\n    *          pruned partition list. If it is null it will be computed on-the-fly.\n    */\n   public static void setTaskPlan(String alias_id,\n-      Operator<? extends OperatorDesc> topOp, MapredWork plan, boolean local,\n+      Operator<? extends OperatorDesc> topOp, Task<?> task, boolean local,\n       GenMRProcContext opProcCtx, PrunedPartitionList pList) throws SemanticException {\n+    MapredWork plan = (MapredWork) task.getWork();\n     ParseContext parseCtx = opProcCtx.getParseCtx();\n     Set<ReadEntity> inputs = opProcCtx.getInputs();\n \n@@ -684,6 +675,7 @@ public static void setTaskPlan(String alias_id,\n       }\n       plan.setMapLocalWork(localPlan);\n     }\n+    opProcCtx.addSeenOp(task, topOp);\n   }\n \n   /**\n@@ -854,20 +846,20 @@ public static MapredWork getMapRedWorkFromConf(HiveConf conf) {\n \n   @SuppressWarnings(\"nls\")\n   /**\n-   * Merge the tasks - by creating a temporary file between them.\n+   * Split two tasks by creating a temporary file between them.\n+   *\n    * @param op reduce sink operator being processed\n-   * @param oldTask the parent task\n-   * @param task the child task\n+   * @param parentTask the parent task\n+   * @param childTask the child task\n    * @param opProcCtx context\n-   * @param setReducer does the reducer needs to be set\n-   * @param pos position of the parent\n    **/\n-  public static void splitTasks(Operator<? extends OperatorDesc> op,\n-      Task<? extends Serializable> parentTask,\n-      Task<? extends Serializable> childTask, GenMRProcContext opProcCtx,\n-      boolean setReducer, boolean local, int posn) throws SemanticException {\n-    childTask.getWork();\n-    Operator<? extends OperatorDesc> currTopOp = opProcCtx.getCurrTopOp();\n+  private static void splitTasks(ReduceSinkOperator op,\n+      Task<? extends Serializable> parentTask, Task<? extends Serializable> childTask,\n+      GenMRProcContext opProcCtx) throws SemanticException {\n+    if (op.getNumParent() != 1) {\n+      throw new IllegalStateException(\"Expecting operator \" + op + \" to have one parent. \" +\n+          \"But found multiple parents : \" + op.getParentOperators());\n+    }\n \n     ParseContext parseCtx = opProcCtx.getParseCtx();\n     parentTask.addDependentTask(childTask);\n@@ -883,7 +875,7 @@ public static void splitTasks(Operator<? extends OperatorDesc> op,\n     Context baseCtx = parseCtx.getContext();\n     String taskTmpDir = baseCtx.getMRTmpFileURI();\n \n-    Operator<? extends OperatorDesc> parent = op.getParentOperators().get(posn);\n+    Operator<? extends OperatorDesc> parent = op.getParentOperators().get(0);\n     TableDesc tt_desc = PlanUtils.getIntermediateFileTableDesc(PlanUtils\n         .getFieldSchemasFromRowSchema(parent.getSchema(), \"temporarycol\"));\n \n@@ -925,41 +917,46 @@ public static void splitTasks(Operator<? extends OperatorDesc> op,\n     childOpList = new ArrayList<Operator<? extends OperatorDesc>>();\n     childOpList.add(op);\n     ts_op.setChildOperators(childOpList);\n-    op.getParentOperators().set(posn, ts_op);\n+    op.getParentOperators().set(0, ts_op);\n \n     Map<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx =\n         opProcCtx.getMapCurrCtx();\n-    mapCurrCtx.put(ts_op, new GenMapRedCtx(childTask, null, null));\n+    mapCurrCtx.put(ts_op, new GenMapRedCtx(childTask, null));\n \n     String streamDesc = taskTmpDir;\n     MapredWork cplan = (MapredWork) childTask.getWork();\n \n-    if (setReducer) {\n-      Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);\n+    Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);\n \n-      if (reducer.getClass() == JoinOperator.class) {\n-        String origStreamDesc;\n-        streamDesc = \"$INTNAME\";\n-        origStreamDesc = streamDesc;\n-        int pos = 0;\n-        while (cplan.getAliasToWork().get(streamDesc) != null) {\n-          streamDesc = origStreamDesc.concat(String.valueOf(++pos));\n-        }\n+    if (needsTagging(reducer)) {\n+      String origStreamDesc;\n+      streamDesc = \"$INTNAME\";\n+      origStreamDesc = streamDesc;\n+      int pos = 0;\n+      while (cplan.getAliasToWork().get(streamDesc) != null) {\n+        streamDesc = origStreamDesc.concat(String.valueOf(++pos));\n       }\n \n       // TODO: Allocate work to remove the temporary files and make that\n       // dependent on the redTask\n-      if (reducer.getClass() == JoinOperator.class ||\n-          reducer.getClass() == DemuxOperator.class) {\n-        cplan.setNeedsTagging(true);\n-      }\n+      cplan.setNeedsTagging(true);\n     }\n \n     // Add the path to alias mapping\n-    setTaskPlan(taskTmpDir, streamDesc, ts_op, cplan, local, tt_desc);\n+    setTaskPlan(taskTmpDir, streamDesc, ts_op, cplan, false, tt_desc);\n     opProcCtx.setCurrTopOp(null);\n     opProcCtx.setCurrAliasId(null);\n     opProcCtx.setCurrTask(childTask);\n+    opProcCtx.addRootIfPossible(parentTask);\n+  }\n+\n+  static boolean hasBranchFinished(Object... children) {\n+    for (Object child : children) {\n+      if (child == null) {\n+        return false;\n+      }\n+    }\n+    return true;\n   }\n \n   private GenMapRedUtils() {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java", "sha": "8604dfa6781c15b656cde09b3566db21b0f6b320", "changes": 81, "status": "modified", "deletions": 60, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -144,38 +144,16 @@ public static void setupBucketMapJoinInfo(MapredWork plan,\n      *          position of the parent\n      */\n     private static void initMapJoinPlan(AbstractMapJoinOperator<? extends MapJoinDesc> op,\n-        GenMRProcContext opProcCtx, int pos)\n+        Task<? extends Serializable> currTask,\n+        GenMRProcContext opProcCtx, boolean local)\n         throws SemanticException {\n-      Map<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx =\n-          opProcCtx.getMapCurrCtx();\n-      int parentPos = (pos == -1) ? 0 : pos;\n-      GenMapRedCtx mapredCtx = mapCurrCtx.get(op.getParentOperators().get(\n-          parentPos));\n-      Task<? extends Serializable> currTask = mapredCtx.getCurrTask();\n-      MapredWork plan = (MapredWork) currTask.getWork();\n-      HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap =\n-          opProcCtx.getOpTaskMap();\n-      Operator<? extends OperatorDesc> currTopOp = opProcCtx.getCurrTopOp();\n-\n-      MapJoinDesc desc = (MapJoinDesc) op.getConf();\n \n       // The map is overloaded to keep track of mapjoins also\n-      opTaskMap.put(op, currTask);\n-\n-      List<Task<? extends Serializable>> rootTasks = opProcCtx.getRootTasks();\n-      if(!rootTasks.contains(currTask)\n-         && (currTask.getParentTasks() == null\n-             || currTask.getParentTasks().isEmpty())) {\n-        rootTasks.add(currTask);\n-      }\n-\n-      assert currTopOp != null;\n-      opProcCtx.getSeenOps().add(currTopOp);\n+      opProcCtx.getOpTaskMap().put(op, currTask);\n \n+      Operator<? extends OperatorDesc> currTopOp = opProcCtx.getCurrTopOp();\n       String currAliasId = opProcCtx.getCurrAliasId();\n-      boolean local = (pos == desc.getPosBigTable()) ? false : true;\n-      GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, plan, local, opProcCtx);\n-      setupBucketMapJoinInfo(plan, op);\n+      GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currTask, local, opProcCtx);\n     }\n \n     /**\n@@ -191,29 +169,12 @@ private static void initMapJoinPlan(AbstractMapJoinOperator<? extends MapJoinDes\n      * @param pos\n      *          position of the parent in the stack\n      */\n-    public static void joinMapJoinPlan(AbstractMapJoinOperator<? extends OperatorDesc> op,\n+    private static void joinMapJoinPlan(AbstractMapJoinOperator<? extends MapJoinDesc> op,\n         Task<? extends Serializable> oldTask,\n-        GenMRProcContext opProcCtx, int pos)\n+        GenMRProcContext opProcCtx, boolean local)\n         throws SemanticException {\n-      MapredWork plan = (MapredWork) oldTask.getWork();\n       Operator<? extends OperatorDesc> currTopOp = opProcCtx.getCurrTopOp();\n-\n-      List<Operator<? extends OperatorDesc>> seenOps = opProcCtx.getSeenOps();\n-      String currAliasId = opProcCtx.getCurrAliasId();\n-\n-      if (!seenOps.contains(currTopOp)) {\n-        seenOps.add(currTopOp);\n-        boolean local = false;\n-        if (pos != -1) {\n-          local = (pos == ((MapJoinDesc) op.getConf()).getPosBigTable()) ? false\n-              : true;\n-        }\n-        GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, plan, local, opProcCtx);\n-        setupBucketMapJoinInfo(plan, op);\n-      }\n-      currTopOp = null;\n-      opProcCtx.setCurrTopOp(currTopOp);\n-      opProcCtx.setCurrTask(oldTask);\n+      GenMapRedUtils.mergeInput(currTopOp, opProcCtx, oldTask, local);\n     }\n \n     /*\n@@ -236,38 +197,38 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n \n       Map<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx = ctx\n           .getMapCurrCtx();\n-      GenMapRedCtx mapredCtx = mapCurrCtx.get(mapJoin.getParentOperators().get(\n-          pos));\n+      GenMapRedCtx mapredCtx = mapCurrCtx.get(mapJoin.getParentOperators().get(pos));\n       Task<? extends Serializable> currTask = mapredCtx.getCurrTask();\n       MapredWork currPlan = (MapredWork) currTask.getWork();\n-      Operator<? extends OperatorDesc> currTopOp = mapredCtx.getCurrTopOp();\n       String currAliasId = mapredCtx.getCurrAliasId();\n       HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>> opTaskMap =\n           ctx.getOpTaskMap();\n-      Task<? extends Serializable> opMapTask = opTaskMap.get(mapJoin);\n+      Task<? extends Serializable> oldTask = opTaskMap.get(mapJoin);\n \n-      ctx.setCurrTopOp(currTopOp);\n       ctx.setCurrAliasId(currAliasId);\n       ctx.setCurrTask(currTask);\n \n       // If we are seeing this mapjoin for the first time, initialize the plan.\n       // If we are seeing this mapjoin for the second or later time then atleast one of the\n       // branches for this mapjoin have been encounered. Join the plan with the plan created\n       // the first time.\n-      if (opMapTask == null) {\n+      boolean local = pos != mapJoin.getConf().getPosBigTable();\n+      if (oldTask == null) {\n         assert currPlan.getReducer() == null;\n-        initMapJoinPlan(mapJoin, ctx, pos);\n+        initMapJoinPlan(mapJoin, currTask, ctx, local);\n       } else {\n         // The current plan can be thrown away after being merged with the\n         // original plan\n-        joinMapJoinPlan(mapJoin, opMapTask, ctx, pos);\n-        currTask = opMapTask;\n-        ctx.setCurrTask(currTask);\n+        joinMapJoinPlan(mapJoin, oldTask, ctx, local);\n+        ctx.setCurrTask(currTask = oldTask);\n       }\n+      MapredWork plan = (MapredWork) currTask.getWork();\n+      setupBucketMapJoinInfo(plan, mapJoin);\n+\n+      mapCurrCtx.put(mapJoin, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrAliasId()));\n \n-      mapCurrCtx.put(mapJoin, new GenMapRedCtx(ctx.getCurrTask(), ctx\n-          .getCurrTopOp(), ctx.getCurrAliasId()));\n-      return null;\n+      // local aliases need not to hand over context further\n+      return !local;\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java", "sha": "78df02ec37efb2bda0b9969d217852aa59e7d188", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -263,7 +263,8 @@ public static String genLocalWorkForMapJoin(MapredWork newWork, MapJoinOperator\n \n     } catch (Exception e) {\n       e.printStackTrace();\n-      throw new SemanticException(\"Generate New MapJoin Opertor Exeception \" + e.getMessage());\n+      throw new SemanticException(\"Failed to generate new mapJoin operator \" +\n+          \"by exception : \" + e.getMessage());\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/parse/GenMapRedWalker.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/parse/GenMapRedWalker.java", "sha": "9583a1b82cdf01da456b5948ab4f42627d72a15b", "changes": 10, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenMapRedWalker.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -20,7 +20,6 @@\n \n import java.util.List;\n \n-import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;\n import org.apache.hadoop.hive.ql.lib.Dispatcher;\n import org.apache.hadoop.hive.ql.lib.Node;\n@@ -52,12 +51,11 @@ public void walk(Node nd) throws SemanticException {\n \n     // maintain the stack of operators encountered\n     opStack.push(nd);\n-    dispatch(nd, opStack);\n+    Boolean result = dispatchAndReturn(nd, opStack);\n \n-    // kids of reduce sink operator need not be traversed again\n-    if ((children == null)\n-        || ((nd instanceof ReduceSinkOperator) && (getDispatchedList()\n-        .containsAll(children)))) {\n+    // kids of reduce sink operator or mapjoin operators merged into root task\n+    // need not be traversed again\n+    if (children == null || result == Boolean.FALSE) {\n       opStack.pop();\n       return;\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenMapRedWalker.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java", "sha": "24974e705ebe7498a89907e97f58ebde7b50d083", "changes": 6, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -217,8 +217,7 @@ public void compile(final ParseContext pCtx, final List<Task<? extends Serializa\n     GenMRProcContext procCtx = new GenMRProcContext(\n         conf,\n         new HashMap<Operator<? extends OperatorDesc>, Task<? extends Serializable>>(),\n-        new ArrayList<Operator<? extends OperatorDesc>>(), tempParseContext,\n-        mvTask, rootTasks,\n+        tempParseContext, mvTask, rootTasks,\n         new LinkedHashMap<Operator<? extends OperatorDesc>, GenMapRedCtx>(),\n         inputs, outputs);\n \n@@ -599,8 +598,7 @@ public boolean accept(Path file) {\n     boolean hasNonLocalJob = false;\n     for (ExecDriver mrtask : mrtasks) {\n       try {\n-        ContentSummary inputSummary = Utilities.getInputSummary\n-            (ctx, (MapredWork) mrtask.getWork(), p);\n+        ContentSummary inputSummary = Utilities.getInputSummary(ctx, mrtask.getWork(), p);\n         int numReducers = getNumberOfReducers(mrtask.getWork(), conf);\n \n         long estimatedInput;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/MapReduceCompiler.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java", "sha": "2be1cdb3343edc4440b5681361715c4a81a9c4b8", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -95,7 +95,7 @@\n \n   private boolean mapperCannotSpanPartns;\n \n-  // used to indicate the input is sorted, and so a BinarySearchRecordReader shoudl be used\n+  // used to indicate the input is sorted, and so a BinarySearchRecordReader should be used\n   private boolean inputFormatSorted = false;\n \n   private transient boolean useBucketizedHiveInputFormat;\n@@ -630,4 +630,9 @@ public String getSamplingTypeString() {\n     return samplingType == 1 ? \"SAMPLING_ON_PREV_MR\" :\n         samplingType == 2 ? \"SAMPLING_ON_START\" : null;\n   }\n+\n+  public void mergingInto(MapredWork mapred) {\n+    // currently, this is sole field affecting mergee task\n+    mapred.useBucketizedHiveInputFormat |= useBucketizedHiveInputFormat;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/queries/clientpositive/auto_sortmerge_join_6.q", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/queries/clientpositive/auto_sortmerge_join_6.q", "sha": "309987b8f7711ac68c8f56d88d6bce2a9065fbb4", "changes": 19, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/auto_sortmerge_join_6.q?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -19,10 +19,29 @@ set hive.auto.convert.join=true;\n set hive.auto.convert.join.noconditionaltask=true;\n set hive.auto.convert.join.noconditionaltask.size=200;\n set hive.auto.convert.sortmerge.join.to.mapjoin=false;\n+\n -- A SMB join is being followed by a regular join on a non-bucketed table on a different key\n+\n+-- Three tests below are all the same query with different alias, which changes dispatch order of GenMapRedWalker\n+-- This is dependent to iteration order of HashMap, so can be meaningless in non-sun jdk\n+-- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]\n+-- c = TS[1]-RS[7]-JOIN[8]\n+-- a = TS[2]-MAPJOIN[11]\n explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value;\n select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value;\n \n+-- d = TS[0]-RS[7]-JOIN[8]-SEL[9]-FS[10]\n+-- b = TS[1]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]\n+-- a = TS[2]-MAPJOIN[11]\n+explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value;\n+select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value;\n+\n+-- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]\n+-- a = TS[1]-MAPJOIN[11]\n+-- h = TS[2]-RS[7]-JOIN[8]\n+explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value;\n+select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value;\n+\n -- A SMB join is being followed by a regular join on a non-bucketed table on the same key\n explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.key = a.key;\n select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.key = a.key;", "filename": "ql/src/test/queries/clientpositive/auto_sortmerge_join_6.q"}, {"additions": 41, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/queries/clientpositive/smb_mapjoin_25.q", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/queries/clientpositive/smb_mapjoin_25.q", "sha": "8b534e85aee1d7df1349992ce8a5088b7c6d4599", "changes": 41, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/smb_mapjoin_25.q?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -0,0 +1,41 @@\n+set hive.enforce.bucketing=true;\n+set hive.enforce.sorting=true;\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+set hive.exec.max.dynamic.partitions.pernode=1000000;\n+set hive.exec.max.dynamic.partitions=1000000;\n+set hive.exec.max.created.files=1000000;\n+set hive.map.aggr=true;\n+\n+create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; \n+create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; \n+create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;\n+\n+load data local inpath '../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;\n+load data local inpath '../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;\n+load data local inpath '../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;\n+\n+explain \n+select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5;\n+\n+set hive.optimize.bucketmapjoin=true;\n+set hive.optimize.bucketmapjoin.sortedmerge=true;\n+set hive.mapred.reduce.tasks.speculative.execution=false;\n+set hive.auto.convert.join=true;\n+set hive.auto.convert.sortmerge.join=true;\n+set hive.auto.convert.sortmerge.join.noconditionaltask=true;\n+set hive.auto.convert.join.noconditionaltask=true;\n+set hive.auto.convert.join.noconditionaltask.size=10000000000;\n+set hive.optimize.reducededuplication.min.reducer=1;\n+set hive.optimize.mapjoin.mapreduce=true;\n+set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;\n+\n+-- explain\n+-- select * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join src c on a.key=c.value\n+\n+-- select a.key from smb_bucket_1 a\n+\n+explain \n+select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5;\n+\n+select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5;\n+", "filename": "ql/src/test/queries/clientpositive/smb_mapjoin_25.q"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/auto_smb_mapjoin_14.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/auto_smb_mapjoin_14.q.out", "sha": "2bc99fa58c8af703efabfb2e6cd723140691dbee", "changes": 12, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_smb_mapjoin_14.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -162,7 +162,7 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1\n+  Stage-3 depends on stages: Stage-1\n   Stage-0 is a root stage\n \n STAGE PLANS:\n@@ -234,7 +234,7 @@ STAGE PLANS:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -346,8 +346,8 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1, Stage-5\n-  Stage-3 depends on stages: Stage-2\n+  Stage-3 depends on stages: Stage-1, Stage-5\n+  Stage-4 depends on stages: Stage-3\n   Stage-5 is a root stage\n   Stage-0 is a root stage\n \n@@ -420,7 +420,7 @@ STAGE PLANS:\n                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n         $INTNAME \n@@ -476,7 +476,7 @@ STAGE PLANS:\n                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/auto_smb_mapjoin_14.q.out"}, {"additions": 314, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/auto_sortmerge_join_6.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/auto_sortmerge_join_6.q.out", "sha": "1274b76c8e225c0ea3f246d3d0ee1f2c909e5783", "changes": 330, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_sortmerge_join_6.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -71,9 +71,21 @@ POSTHOOK: Lineage: tbl3.value SIMPLE [(src)src.FieldSchema(name:value, type:stri\n POSTHOOK: Lineage: tbl4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: tbl4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n PREHOOK: query: -- A SMB join is being followed by a regular join on a non-bucketed table on a different key\n+\n+-- Three tests below are all the same query with different alias, which changes dispatch order of GenMapRedWalker\n+-- This is dependent to iteration order of HashMap, so can be meaningless in non-sun jdk\n+-- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]\n+-- c = TS[1]-RS[7]-JOIN[8]\n+-- a = TS[2]-MAPJOIN[11]\n explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value\n PREHOOK: type: QUERY\n POSTHOOK: query: -- A SMB join is being followed by a regular join on a non-bucketed table on a different key\n+\n+-- Three tests below are all the same query with different alias, which changes dispatch order of GenMapRedWalker\n+-- This is dependent to iteration order of HashMap, so can be meaningless in non-sun jdk\n+-- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]\n+-- c = TS[1]-RS[7]-JOIN[8]\n+-- a = TS[2]-MAPJOIN[11]\n explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value\n POSTHOOK: type: QUERY\n POSTHOOK: Lineage: tbl1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n@@ -88,12 +100,12 @@ ABSTRACT SYNTAX TREE:\n   (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME tbl1) a) (TOK_TABREF (TOK_TABNAME tbl2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key))) (TOK_TABREF (TOK_TABNAME src) c) (= (. (TOK_TABLE_OR_COL c) value) (. (TOK_TABLE_OR_COL a) value)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count)))))\n \n STAGE DEPENDENCIES:\n-  Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1\n+  Stage-2 is a root stage\n+  Stage-3 depends on stages: Stage-2\n   Stage-0 is a root stage\n \n STAGE PLANS:\n-  Stage: Stage-1\n+  Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n         a \n@@ -154,7 +166,7 @@ STAGE PLANS:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -209,6 +221,292 @@ POSTHOOK: Lineage: tbl3.value SIMPLE [(src)src.FieldSchema(name:value, type:stri\n POSTHOOK: Lineage: tbl4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: tbl4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n 2654\n+PREHOOK: query: -- d = TS[0]-RS[7]-JOIN[8]-SEL[9]-FS[10]\n+-- b = TS[1]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]\n+-- a = TS[2]-MAPJOIN[11]\n+explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- d = TS[0]-RS[7]-JOIN[8]-SEL[9]-FS[10]\n+-- b = TS[1]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]\n+-- a = TS[2]-MAPJOIN[11]\n+explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: tbl1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME tbl1) a) (TOK_TABREF (TOK_TABNAME tbl2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key))) (TOK_TABREF (TOK_TABNAME src) d) (= (. (TOK_TABLE_OR_COL d) value) (. (TOK_TABLE_OR_COL a) value)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        a \n+          TableScan\n+            alias: a\n+            Sorted Merge Bucket Map Join Operator\n+              condition map:\n+                   Inner Join 0 to 1\n+              condition expressions:\n+                0 {value}\n+                1 \n+              handleSkewJoin: false\n+              keys:\n+                0 [Column[key]]\n+                1 [Column[key]]\n+              outputColumnNames: _col1\n+              Position of Big Table: 0\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: _col1\n+                      type: string\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: _col1\n+                      type: string\n+                tag: 0\n+        d \n+          TableScan\n+            alias: d\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: value\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: value\n+                    type: string\n+              tag: 1\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          condition expressions:\n+            0 \n+            1 \n+          handleSkewJoin: false\n+          Select Operator\n+            Group By Operator\n+              aggregations:\n+                    expr: count()\n+              bucketGroup: false\n+              mode: hash\n+              outputColumnNames: _col0\n+              File Output Operator\n+                compressed: false\n+                GlobalTableId: 0\n+                table:\n+                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+#### A masked pattern was here ####\n+            Reduce Output Operator\n+              sort order: \n+              tag: -1\n+              value expressions:\n+                    expr: _col0\n+                    type: bigint\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: count(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: bigint\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Input: default@tbl1\n+PREHOOK: Input: default@tbl2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Input: default@tbl1\n+POSTHOOK: Input: default@tbl2\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: tbl1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+2654\n+PREHOOK: query: -- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]\n+-- a = TS[1]-MAPJOIN[11]\n+-- h = TS[2]-RS[7]-JOIN[8]\n+explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]\n+-- a = TS[1]-MAPJOIN[11]\n+-- h = TS[2]-RS[7]-JOIN[8]\n+explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: tbl1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME tbl1) a) (TOK_TABREF (TOK_TABNAME tbl2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key))) (TOK_TABREF (TOK_TABNAME src) h) (= (. (TOK_TABLE_OR_COL h) value) (. (TOK_TABLE_OR_COL a) value)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-3 depends on stages: Stage-1\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        a \n+          TableScan\n+            alias: a\n+            Sorted Merge Bucket Map Join Operator\n+              condition map:\n+                   Inner Join 0 to 1\n+              condition expressions:\n+                0 {value}\n+                1 \n+              handleSkewJoin: false\n+              keys:\n+                0 [Column[key]]\n+                1 [Column[key]]\n+              outputColumnNames: _col1\n+              Position of Big Table: 0\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: _col1\n+                      type: string\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: _col1\n+                      type: string\n+                tag: 0\n+        h \n+          TableScan\n+            alias: h\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: value\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: value\n+                    type: string\n+              tag: 1\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          condition expressions:\n+            0 \n+            1 \n+          handleSkewJoin: false\n+          Select Operator\n+            Group By Operator\n+              aggregations:\n+                    expr: count()\n+              bucketGroup: false\n+              mode: hash\n+              outputColumnNames: _col0\n+              File Output Operator\n+                compressed: false\n+                GlobalTableId: 0\n+                table:\n+                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-3\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+#### A masked pattern was here ####\n+            Reduce Output Operator\n+              sort order: \n+              tag: -1\n+              value expressions:\n+                    expr: _col0\n+                    type: bigint\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: count(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: bigint\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Input: default@tbl1\n+PREHOOK: Input: default@tbl2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Input: default@tbl1\n+POSTHOOK: Input: default@tbl2\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: tbl1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: tbl4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+2654\n PREHOOK: query: -- A SMB join is being followed by a regular join on a non-bucketed table on the same key\n explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.key = a.key\n PREHOOK: type: QUERY\n@@ -703,12 +1001,12 @@ ABSTRACT SYNTAX TREE:\n   (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME tbl1) a) (TOK_TABREF (TOK_TABNAME tbl2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key))) (TOK_TABREF (TOK_TABNAME tbl4) c) (= (. (TOK_TABLE_OR_COL c) value) (. (TOK_TABLE_OR_COL a) value)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count)))))\n \n STAGE DEPENDENCIES:\n-  Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1\n+  Stage-2 is a root stage\n+  Stage-3 depends on stages: Stage-2\n   Stage-0 is a root stage\n \n STAGE PLANS:\n-  Stage: Stage-1\n+  Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n         a \n@@ -769,7 +1067,7 @@ STAGE PLANS:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -842,12 +1140,12 @@ ABSTRACT SYNTAX TREE:\n   (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME tbl1) a) (TOK_TABREF (TOK_TABNAME tbl2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key))) (TOK_TABREF (TOK_TABNAME src) c) (= (. (TOK_TABLE_OR_COL c) value) (. (TOK_TABLE_OR_COL a) value)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count)))))\n \n STAGE DEPENDENCIES:\n-  Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1\n+  Stage-2 is a root stage\n+  Stage-3 depends on stages: Stage-2\n   Stage-0 is a root stage\n \n STAGE PLANS:\n-  Stage: Stage-1\n+  Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n         a \n@@ -908,7 +1206,7 @@ STAGE PLANS:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -1746,12 +2044,12 @@ ABSTRACT SYNTAX TREE:\n   (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME tbl1) a) (TOK_TABREF (TOK_TABNAME tbl2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key))) (TOK_TABREF (TOK_TABNAME tbl4) c) (= (. (TOK_TABLE_OR_COL c) value) (. (TOK_TABLE_OR_COL a) value)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTIONSTAR count)))))\n \n STAGE DEPENDENCIES:\n-  Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1\n+  Stage-2 is a root stage\n+  Stage-3 depends on stages: Stage-2\n   Stage-0 is a root stage\n \n STAGE PLANS:\n-  Stage: Stage-1\n+  Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n         a \n@@ -1812,7 +2110,7 @@ STAGE PLANS:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/auto_sortmerge_join_6.q.out"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out", "sha": "6add99a3096700ae4a7b7258ee55a8a45f89b215", "changes": 36, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -322,7 +322,7 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1\n+  Stage-3 depends on stages: Stage-1\n   Stage-0 is a root stage\n \n STAGE PLANS:\n@@ -394,7 +394,7 @@ STAGE PLANS:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -506,13 +506,13 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-9 depends on stages: Stage-1, Stage-5 , consists of Stage-10, Stage-11, Stage-2\n-  Stage-10 has a backup stage: Stage-2\n+  Stage-9 depends on stages: Stage-1, Stage-5 , consists of Stage-10, Stage-11, Stage-3\n+  Stage-10 has a backup stage: Stage-3\n   Stage-7 depends on stages: Stage-10\n-  Stage-3 depends on stages: Stage-2, Stage-7, Stage-8\n-  Stage-11 has a backup stage: Stage-2\n+  Stage-4 depends on stages: Stage-3, Stage-7, Stage-8\n+  Stage-11 has a backup stage: Stage-3\n   Stage-8 depends on stages: Stage-11\n-  Stage-2\n+  Stage-3\n   Stage-5 is a root stage\n   Stage-0 is a root stage\n \n@@ -640,7 +640,7 @@ STAGE PLANS:\n       Local Work:\n         Map Reduce Local Work\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -722,7 +722,7 @@ STAGE PLANS:\n       Local Work:\n         Map Reduce Local Work\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n         $INTNAME \n@@ -2695,7 +2695,7 @@ STAGE DEPENDENCIES:\n   Stage-6 is a root stage , consists of Stage-7, Stage-8, Stage-1\n   Stage-7 has a backup stage: Stage-1\n   Stage-4 depends on stages: Stage-7\n-  Stage-2 depends on stages: Stage-1, Stage-4, Stage-5\n+  Stage-3 depends on stages: Stage-1, Stage-4, Stage-5\n   Stage-8 has a backup stage: Stage-1\n   Stage-5 depends on stages: Stage-8\n   Stage-1\n@@ -2795,7 +2795,7 @@ STAGE PLANS:\n                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                     output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -3067,13 +3067,13 @@ STAGE DEPENDENCIES:\n   Stage-12 is a root stage , consists of Stage-18, Stage-19, Stage-1\n   Stage-18 has a backup stage: Stage-1\n   Stage-10 depends on stages: Stage-18\n-  Stage-9 depends on stages: Stage-1, Stage-5, Stage-10, Stage-11, Stage-13, Stage-14 , consists of Stage-16, Stage-17, Stage-2\n-  Stage-16 has a backup stage: Stage-2\n+  Stage-9 depends on stages: Stage-1, Stage-5, Stage-10, Stage-11, Stage-13, Stage-14 , consists of Stage-16, Stage-17, Stage-3\n+  Stage-16 has a backup stage: Stage-3\n   Stage-7 depends on stages: Stage-16\n-  Stage-3 depends on stages: Stage-2, Stage-7, Stage-8\n-  Stage-17 has a backup stage: Stage-2\n+  Stage-4 depends on stages: Stage-3, Stage-7, Stage-8\n+  Stage-17 has a backup stage: Stage-3\n   Stage-8 depends on stages: Stage-17\n-  Stage-2\n+  Stage-3\n   Stage-19 has a backup stage: Stage-1\n   Stage-11 depends on stages: Stage-19\n   Stage-1\n@@ -3234,7 +3234,7 @@ STAGE PLANS:\n       Local Work:\n         Map Reduce Local Work\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -3316,7 +3316,7 @@ STAGE PLANS:\n       Local Work:\n         Map Reduce Local Work\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n         $INTNAME ", "filename": "ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin1.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin1.q.out", "sha": "307132b77f920f3ccaebb87d8f60fb65c7be05cc", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin1.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -338,13 +338,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -481,10 +481,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -512,11 +512,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -582,7 +582,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -648,7 +648,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/bucketmapjoin1.q.out"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin2.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin2.q.out", "sha": "ebbb2baff7770cc9e85cce5d7a1d68e010bb9c17", "changes": 48, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin2.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -79,13 +79,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -225,10 +225,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin_part/ds=2008-04-08 [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -256,11 +256,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -326,7 +326,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -392,7 +392,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -1272,13 +1272,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -1423,10 +1423,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin_part/ds=2008-04-08 [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -1459,11 +1459,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -1544,7 +1544,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -1625,7 +1625,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/bucketmapjoin2.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin3.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin3.q.out", "sha": "66918b6515958136588ae569a217084c40990ed6", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin3.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -96,13 +96,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -242,10 +242,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin_part_2/ds=2008-04-08 [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -273,11 +273,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -343,7 +343,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -409,7 +409,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/bucketmapjoin3.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin4.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin4.q.out", "sha": "960f246b744456930ee1af617b0d16cf08e032ef", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin4.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -96,13 +96,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -239,10 +239,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -270,11 +270,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -340,7 +340,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -406,7 +406,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/bucketmapjoin4.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out", "sha": "2d803db3b466720935143bb77514509a210f3020", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -62,13 +62,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -197,10 +197,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -228,11 +228,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -298,7 +298,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -364,7 +364,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/bucketmapjoin_negative.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out", "sha": "4b8bd14e152debe4dbaa85dde983472b74dcad2d", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -69,13 +69,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -212,10 +212,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -243,11 +243,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -313,7 +313,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -379,7 +379,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/bucketmapjoin_negative2.q.out"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_2.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_2.q.out", "sha": "68962907965ee58142039c58479172f47e26107a", "changes": 12, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketsortoptimize_insert_2.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -111,7 +111,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -160,7 +160,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n \n@@ -290,7 +290,7 @@ STAGE DEPENDENCIES:\n   Stage-7 has a backup stage: Stage-1\n   Stage-4 depends on stages: Stage-7\n   Stage-0 depends on stages: Stage-1, Stage-4, Stage-5\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n   Stage-8 has a backup stage: Stage-1\n   Stage-5 depends on stages: Stage-8\n   Stage-1\n@@ -383,7 +383,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n   Stage: Stage-8\n@@ -665,7 +665,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -714,7 +714,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n ", "filename": "ql/src/test/results/clientpositive/bucketsortoptimize_insert_2.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_4.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_4.q.out", "sha": "305b77944ef32b265f121471efed18bf44f130cd", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketsortoptimize_insert_4.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -73,7 +73,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -124,7 +124,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n \n@@ -271,7 +271,7 @@ STAGE DEPENDENCIES:\n   Stage-7 has a backup stage: Stage-1\n   Stage-4 depends on stages: Stage-7\n   Stage-0 depends on stages: Stage-1, Stage-4, Stage-5\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n   Stage-8 has a backup stage: Stage-1\n   Stage-5 depends on stages: Stage-8\n   Stage-1\n@@ -364,7 +364,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n   Stage: Stage-8", "filename": "ql/src/test/results/clientpositive/bucketsortoptimize_insert_4.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_5.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_5.q.out", "sha": "f8a5c704c24be1cc8abd65ffb07d751deb8ab09b", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketsortoptimize_insert_5.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -75,7 +75,7 @@ STAGE DEPENDENCIES:\n   Stage-7 has a backup stage: Stage-1\n   Stage-4 depends on stages: Stage-7\n   Stage-0 depends on stages: Stage-1, Stage-4, Stage-5\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n   Stage-8 has a backup stage: Stage-1\n   Stage-5 depends on stages: Stage-8\n   Stage-1\n@@ -168,7 +168,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n   Stage: Stage-8", "filename": "ql/src/test/results/clientpositive/bucketsortoptimize_insert_5.q.out"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_6.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_6.q.out", "sha": "f0c6b6448e9b046b5319a0919b313f4522d3a1e3", "changes": 20, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketsortoptimize_insert_6.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -78,7 +78,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -129,7 +129,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n \n@@ -454,7 +454,7 @@ STAGE DEPENDENCIES:\n   Stage-7 has a backup stage: Stage-1\n   Stage-4 depends on stages: Stage-7\n   Stage-0 depends on stages: Stage-1, Stage-4, Stage-5\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n   Stage-8 has a backup stage: Stage-1\n   Stage-5 depends on stages: Stage-8\n   Stage-1\n@@ -555,7 +555,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n   Stage: Stage-8\n@@ -1028,7 +1028,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -1079,7 +1079,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n \n@@ -1251,7 +1251,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -1302,7 +1302,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n \n@@ -1513,7 +1513,7 @@ STAGE DEPENDENCIES:\n   Stage-7 has a backup stage: Stage-1\n   Stage-4 depends on stages: Stage-7\n   Stage-0 depends on stages: Stage-1, Stage-4, Stage-5\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n   Stage-8 has a backup stage: Stage-1\n   Stage-5 depends on stages: Stage-8\n   Stage-1\n@@ -1614,7 +1614,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table4\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n   Stage: Stage-8", "filename": "ql/src/test/results/clientpositive/bucketsortoptimize_insert_6.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_7.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_7.q.out", "sha": "6c2e24363b3cae193292ea3a3a70266c568a4639", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketsortoptimize_insert_7.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -75,7 +75,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -128,7 +128,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n ", "filename": "ql/src/test/results/clientpositive/bucketsortoptimize_insert_7.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_8.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/bucketsortoptimize_insert_8.q.out", "sha": "dca8448dfab52f92415efe7983678a2ba0850664", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketsortoptimize_insert_8.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -73,7 +73,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -124,7 +124,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n \n@@ -240,7 +240,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -291,7 +291,7 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.test_table3\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n ", "filename": "ql/src/test/results/clientpositive/bucketsortoptimize_insert_8.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/correlationoptimizer3.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/correlationoptimizer3.q.out", "sha": "cfa7eff6b02cc5ee691fccd6888ab9efcf7009a6", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/correlationoptimizer3.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -670,7 +670,7 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n-#### A masked pattern was here ####\n+        $INTNAME \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -683,7 +683,7 @@ STAGE PLANS:\n               value expressions:\n                     expr: _col1\n                     type: bigint\n-#### A masked pattern was here ####\n+        $INTNAME1 \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -1547,7 +1547,7 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n-#### A masked pattern was here ####\n+        $INTNAME \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -1560,7 +1560,7 @@ STAGE PLANS:\n               value expressions:\n                     expr: _col1\n                     type: string\n-#### A masked pattern was here ####\n+        $INTNAME1 \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0", "filename": "ql/src/test/results/clientpositive/correlationoptimizer3.q.out"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/correlationoptimizer6.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/correlationoptimizer6.q.out", "sha": "b0438e69c847bd9ae470df149a9808679576a66a", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/correlationoptimizer6.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -733,7 +733,7 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n-#### A masked pattern was here ####\n+        $INTNAME \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -746,7 +746,7 @@ STAGE PLANS:\n               value expressions:\n                     expr: _col1\n                     type: bigint\n-#### A masked pattern was here ####\n+        $INTNAME1 \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -4891,7 +4891,7 @@ STAGE PLANS:\n   Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n-#### A masked pattern was here ####\n+        $INTNAME \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -5236,7 +5236,7 @@ STAGE PLANS:\n   Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n-#### A masked pattern was here ####\n+        $INTNAME \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -5253,7 +5253,7 @@ STAGE PLANS:\n                     type: string\n                     expr: _col2\n                     type: bigint\n-#### A masked pattern was here ####\n+        $INTNAME1 \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0", "filename": "ql/src/test/results/clientpositive/correlationoptimizer6.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/correlationoptimizer7.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/correlationoptimizer7.q.out", "sha": "f8db2bfaa8de49326bed6a185daf64178a99b924", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/correlationoptimizer7.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -349,7 +349,7 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Alias -> Map Operator Tree:\n-#### A masked pattern was here ####\n+        $INTNAME \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -871,7 +871,7 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Alias -> Map Operator Tree:\n-#### A masked pattern was here ####\n+        $INTNAME \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0", "filename": "ql/src/test/results/clientpositive/correlationoptimizer7.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/mapjoin_distinct.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/mapjoin_distinct.q.out", "sha": "f654ca429de7116de26df4dedd7194a1d8e30772", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/mapjoin_distinct.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -16,7 +16,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-4 is a root stage\n   Stage-1 depends on stages: Stage-4\n-  Stage-2 depends on stages: Stage-1\n+  Stage-3 depends on stages: Stage-1\n   Stage-0 is a root stage\n \n STAGE PLANS:\n@@ -96,7 +96,7 @@ STAGE PLANS:\n                 input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                 output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -315,7 +315,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-4 is a root stage\n   Stage-1 depends on stages: Stage-4\n-  Stage-2 depends on stages: Stage-1\n+  Stage-3 depends on stages: Stage-1\n   Stage-0 is a root stage\n \n STAGE PLANS:\n@@ -388,7 +388,7 @@ STAGE PLANS:\n                 input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                 output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/mapjoin_distinct.q.out"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin9.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin9.q.out", "sha": "15fe91fb5f7db92c302fa5627c6f96fb55ebb28a", "changes": 26, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin9.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -247,14 +247,14 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-9 depends on stages: Stage-0\n-  Stage-2 depends on stages: Stage-9\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-9 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-9\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -298,10 +298,10 @@ STAGE PLANS:\n                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                         name: default.smb_mapjoin9_results\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -324,10 +324,10 @@ STAGE PLANS:\n           name: smb_mapjoin9_results\n           isExternal: false\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -339,7 +339,7 @@ STAGE PLANS:\n                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                   name: default.smb_mapjoin9_results\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -351,7 +351,7 @@ STAGE PLANS:\n                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                   name: default.smb_mapjoin9_results\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/smb_mapjoin9.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_11.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_11.q.out", "sha": "50fb7f94fe6ba9d4cd8f0861dd693f2896299c2d", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin_11.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -61,7 +61,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -198,7 +198,7 @@ STAGE PLANS:\n               name: default.test_table3\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n ", "filename": "ql/src/test/results/clientpositive/smb_mapjoin_11.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_12.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_12.q.out", "sha": "d1827348a1c9f07ca59d8bd91139dbe8f8ed7907", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin_12.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -81,7 +81,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -220,7 +220,7 @@ STAGE PLANS:\n               name: default.test_table3\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n@@ -306,7 +306,7 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n   Stage-0 depends on stages: Stage-1\n-  Stage-2 depends on stages: Stage-0\n+  Stage-3 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -455,7 +455,7 @@ STAGE PLANS:\n               name: default.test_table3\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n ", "filename": "ql/src/test/results/clientpositive/smb_mapjoin_12.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_14.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_14.q.out", "sha": "b5f010fc94bbcafaadef837d6a8cb4296d149009", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin_14.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -160,7 +160,7 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-2 depends on stages: Stage-1\n+  Stage-3 depends on stages: Stage-1\n   Stage-0 is a root stage\n \n STAGE PLANS:\n@@ -232,7 +232,7 @@ STAGE PLANS:\n                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/smb_mapjoin_14.q.out"}, {"additions": 371, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_25.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_25.q.out", "sha": "43002d1d4d5351d7006cf3aebfb2e38a5f7a7ed3", "changes": 371, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin_25.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -0,0 +1,371 @@\n+PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@smb_bucket_1\n+PREHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@smb_bucket_2\n+PREHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@smb_bucket_3\n+PREHOOK: query: load data local inpath '../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1\n+PREHOOK: type: LOAD\n+PREHOOK: Output: default@smb_bucket_1\n+POSTHOOK: query: load data local inpath '../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1\n+POSTHOOK: type: LOAD\n+POSTHOOK: Output: default@smb_bucket_1\n+PREHOOK: query: load data local inpath '../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2\n+PREHOOK: type: LOAD\n+PREHOOK: Output: default@smb_bucket_2\n+POSTHOOK: query: load data local inpath '../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2\n+POSTHOOK: type: LOAD\n+POSTHOOK: Output: default@smb_bucket_2\n+PREHOOK: query: load data local inpath '../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3\n+PREHOOK: type: LOAD\n+PREHOOK: Output: default@smb_bucket_3\n+POSTHOOK: query: load data local inpath '../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3\n+POSTHOOK: type: LOAD\n+POSTHOOK: Output: default@smb_bucket_3\n+PREHOOK: query: explain \n+select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain \n+select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME smb_bucket_1) a) (TOK_TABREF (TOK_TABNAME smb_bucket_2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) key))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL a) key) 5)))) t1) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME smb_bucket_2) c) (TOK_TABREF (TOK_TABNAME smb_bucket_3) d) (= (. (TOK_TABLE_OR_COL c) key) (. (TOK_TABLE_OR_COL d) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL c) key))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL c) key) 5)))) t2) (= (. (TOK_TABLE_OR_COL t1) key) (. (TOK_TABLE_OR_COL t2) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (. (TOK_TABLE_OR_COL t2) key) 5))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1, Stage-4\n+  Stage-4 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        t1:a \n+          TableScan\n+            alias: a\n+            Filter Operator\n+              predicate:\n+                  expr: (key = 5)\n+                  type: boolean\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: key\n+                      type: int\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: key\n+                      type: int\n+                tag: 0\n+                value expressions:\n+                      expr: key\n+                      type: int\n+        t1:b \n+          TableScan\n+            alias: b\n+            Filter Operator\n+              predicate:\n+                  expr: (key = 5)\n+                  type: boolean\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: key\n+                      type: int\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: key\n+                      type: int\n+                tag: 1\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          condition expressions:\n+            0 {VALUE._col0}\n+            1 \n+          handleSkewJoin: false\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: int\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        $INTNAME \n+            Reduce Output Operator\n+              key expressions:\n+                    expr: _col0\n+                    type: int\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: _col0\n+                    type: int\n+              tag: 0\n+              value expressions:\n+                    expr: _col0\n+                    type: int\n+        $INTNAME1 \n+            Reduce Output Operator\n+              key expressions:\n+                    expr: _col0\n+                    type: int\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: _col0\n+                    type: int\n+              tag: 1\n+              value expressions:\n+                    expr: _col0\n+                    type: int\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Left Outer Join0 to 1\n+          condition expressions:\n+            0 {VALUE._col0}\n+            1 {VALUE._col0}\n+          handleSkewJoin: false\n+          outputColumnNames: _col0, _col1\n+          Filter Operator\n+            predicate:\n+                expr: (_col1 = 5)\n+                type: boolean\n+            Select Operator\n+              expressions:\n+                    expr: _col0\n+                    type: int\n+                    expr: _col1\n+                    type: int\n+              outputColumnNames: _col0, _col1\n+              File Output Operator\n+                compressed: false\n+                GlobalTableId: 0\n+                table:\n+                    input format: org.apache.hadoop.mapred.TextInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-4\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        t2:c \n+          TableScan\n+            alias: c\n+            Filter Operator\n+              predicate:\n+                  expr: (key = 5)\n+                  type: boolean\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: key\n+                      type: int\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: key\n+                      type: int\n+                tag: 0\n+                value expressions:\n+                      expr: key\n+                      type: int\n+        t2:d \n+          TableScan\n+            alias: d\n+            Filter Operator\n+              predicate:\n+                  expr: (key = 5)\n+                  type: boolean\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: key\n+                      type: int\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: key\n+                      type: int\n+                tag: 1\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          condition expressions:\n+            0 {VALUE._col0}\n+            1 \n+          handleSkewJoin: false\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: int\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: -- explain\n+-- select * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join src c on a.key=c.value\n+\n+-- select a.key from smb_bucket_1 a\n+\n+explain \n+select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- explain\n+-- select * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join src c on a.key=c.value\n+\n+-- select a.key from smb_bucket_1 a\n+\n+explain \n+select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME smb_bucket_1) a) (TOK_TABREF (TOK_TABNAME smb_bucket_2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) key))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL a) key) 5)))) t1) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME smb_bucket_2) c) (TOK_TABREF (TOK_TABNAME smb_bucket_3) d) (= (. (TOK_TABLE_OR_COL c) key) (. (TOK_TABLE_OR_COL d) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL c) key))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL c) key) 5)))) t2) (= (. (TOK_TABLE_OR_COL t1) key) (. (TOK_TABLE_OR_COL t2) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (. (TOK_TABLE_OR_COL t2) key) 5))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        t1:a \n+          TableScan\n+            alias: a\n+            Filter Operator\n+              predicate:\n+                  expr: (key = 5)\n+                  type: boolean\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {key}\n+                  1 \n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[key]]\n+                  1 [Column[key]]\n+                outputColumnNames: _col0\n+                Position of Big Table: 0\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: int\n+                  outputColumnNames: _col0\n+                  Reduce Output Operator\n+                    key expressions:\n+                          expr: _col0\n+                          type: int\n+                    sort order: +\n+                    Map-reduce partition columns:\n+                          expr: _col0\n+                          type: int\n+                    tag: 0\n+                    value expressions:\n+                          expr: _col0\n+                          type: int\n+        t2:c \n+          TableScan\n+            alias: c\n+            Filter Operator\n+              predicate:\n+                  expr: (key = 5)\n+                  type: boolean\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {key}\n+                  1 \n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[key]]\n+                  1 [Column[key]]\n+                outputColumnNames: _col0\n+                Position of Big Table: 0\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: int\n+                  outputColumnNames: _col0\n+                  Reduce Output Operator\n+                    key expressions:\n+                          expr: _col0\n+                          type: int\n+                    sort order: +\n+                    Map-reduce partition columns:\n+                          expr: _col0\n+                          type: int\n+                    tag: 1\n+                    value expressions:\n+                          expr: _col0\n+                          type: int\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Left Outer Join0 to 1\n+          condition expressions:\n+            0 {VALUE._col0}\n+            1 {VALUE._col0}\n+          handleSkewJoin: false\n+          outputColumnNames: _col0, _col1\n+          Filter Operator\n+            predicate:\n+                expr: (_col1 = 5)\n+                type: boolean\n+            Select Operator\n+              expressions:\n+                    expr: _col0\n+                    type: int\n+                    expr: _col1\n+                    type: int\n+              outputColumnNames: _col0, _col1\n+              File Output Operator\n+                compressed: false\n+                GlobalTableId: 0\n+                table:\n+                    input format: org.apache.hadoop.mapred.TextInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@smb_bucket_1\n+PREHOOK: Input: default@smb_bucket_2\n+PREHOOK: Input: default@smb_bucket_3\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@smb_bucket_1\n+POSTHOOK: Input: default@smb_bucket_2\n+POSTHOOK: Input: default@smb_bucket_3\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/smb_mapjoin_25.q.out"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_6.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/smb_mapjoin_6.q.out", "sha": "b597061e4db89a333a51e909014af46a30cc7582", "changes": 48, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin_6.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -1316,13 +1316,13 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -1363,10 +1363,10 @@ STAGE PLANS:\n                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                       name: default.smb_join_results\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -1382,10 +1382,10 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.smb_join_results\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -1398,7 +1398,7 @@ STAGE PLANS:\n                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                   name: default.smb_join_results\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -1411,7 +1411,7 @@ STAGE PLANS:\n                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                   name: default.smb_join_results\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -2858,13 +2858,13 @@ ABSTRACT SYNTAX TREE:\n \n STAGE DEPENDENCIES:\n   Stage-1 is a root stage\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-1\n@@ -2909,10 +2909,10 @@ STAGE PLANS:\n                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                         name: default.smb_join_results\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -2928,10 +2928,10 @@ STAGE PLANS:\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: default.smb_join_results\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -2944,7 +2944,7 @@ STAGE PLANS:\n                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                   name: default.smb_join_results\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -2957,7 +2957,7 @@ STAGE PLANS:\n                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                   name: default.smb_join_results\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/smb_mapjoin_6.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/stats11.q.out", "blob_url": "https://github.com/apache/hive/blob/9a6d96991bab71551aae020d807d940d509b379d/ql/src/test/results/clientpositive/stats11.q.out", "sha": "9a5be332773a223e46839db85ee20593776d0773", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/stats11.q.out?ref=9a6d96991bab71551aae020d807d940d509b379d", "patch": "@@ -283,13 +283,13 @@ ABSTRACT SYNTAX TREE:\n STAGE DEPENDENCIES:\n   Stage-9 is a root stage\n   Stage-1 depends on stages: Stage-9\n-  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n-  Stage-4\n-  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n-  Stage-2 depends on stages: Stage-0\n-  Stage-3\n+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6\n   Stage-5\n-  Stage-6 depends on stages: Stage-5\n+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7\n+  Stage-3 depends on stages: Stage-0\n+  Stage-4\n+  Stage-6\n+  Stage-7 depends on stages: Stage-6\n \n STAGE PLANS:\n   Stage: Stage-9\n@@ -426,10 +426,10 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n         /srcbucket_mapjoin [a]\n \n-  Stage: Stage-7\n+  Stage: Stage-8\n     Conditional Operator\n \n-  Stage: Stage-4\n+  Stage: Stage-5\n     Move Operator\n       files:\n           hdfs directory: true\n@@ -457,11 +457,11 @@ STAGE PLANS:\n               name: default.bucketmapjoin_tmp_result\n #### A masked pattern was here ####\n \n-  Stage: Stage-2\n+  Stage: Stage-3\n     Stats-Aggr Operator\n #### A masked pattern was here ####\n \n-  Stage: Stage-3\n+  Stage: Stage-4\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -527,7 +527,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-5\n+  Stage: Stage-6\n     Map Reduce\n       Alias -> Map Operator Tree:\n #### A masked pattern was here ####\n@@ -593,7 +593,7 @@ STAGE PLANS:\n       Truncated Path -> Alias:\n #### A masked pattern was here ####\n \n-  Stage: Stage-6\n+  Stage: Stage-7\n     Move Operator\n       files:\n           hdfs directory: true", "filename": "ql/src/test/results/clientpositive/stats11.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "parent": "https://github.com/apache/hive/commit/670d3636508f3f486b6f97bd01c6f8a9b51c19c1", "message": "HIVE-4151. HiveProfiler NPE with ScriptOperator. (Pamela Vagata via kevinwilfong)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1465085 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_159", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java", "sha": "b43292826cd1a842947e267fced95747abb000ae", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -525,7 +525,7 @@ public void process(Object row, int tag) throws HiveException {\n     if (fatalError) {\n       return;\n     }\n-    OperatorHookContext opHookContext = new OperatorHookContext(this, row);\n+    OperatorHookContext opHookContext = new OperatorHookContext(this, row, tag);\n     preProcessCounter();\n     enterOperatorHooks(opHookContext);\n     processOp(row, tag);\n@@ -612,7 +612,7 @@ public void close(boolean abort) throws HiveException {\n \n     LOG.info(id + \" forwarded \" + cntr + \" rows\");\n \n-    closeOperatorHooks(new OperatorHookContext(this, null));\n+    closeOperatorHooks(new OperatorHookContext(this));\n     // call the operator specific close routine\n     closeOp(abort);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorHookContext.java", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorHookContext.java", "sha": "a27a653445774fa46d0424c3c09750757f26f671", "changes": 30, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorHookContext.java?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -18,26 +18,44 @@\n \n package org.apache.hadoop.hive.ql.exec;\n \n+import java.util.List;\n+\n public class OperatorHookContext {\n-  private String operatorName;\n-  private String operatorId;\n-  private Object currentRow;\n+  private final String operatorName;\n+  private final String operatorId;\n+  private final Object currentRow;\n+  private final int parentTag;\n   private Operator operator;\n-  public OperatorHookContext(Operator op, Object row) {\n-    this(op.getName(), op.getIdentifier(), row);\n+\n+  public OperatorHookContext(Operator op) {\n+    this(op, null, -1);\n+  }\n+\n+  public OperatorHookContext(Operator op, Object row, int tag) {\n+    this(op.getName(), op.getIdentifier(), row, tag);\n     this.operator = op;\n   }\n \n-  private OperatorHookContext(String opName, String opId, Object row) {\n+  private OperatorHookContext(String opName, String opId, Object row, int tag) {\n     operatorName = opName;\n     operatorId = opId;\n     currentRow = row;\n+    parentTag = tag;\n   }\n \n   public Operator getOperator() {\n     return operator;\n   }\n \n+  public Operator getParentOperator() {\n+    List<Operator> parents = this.operator.getParentOperators();\n+    if (parents == null || parents.isEmpty()) {\n+      return null;\n+    }\n+    return (Operator)(this.operator.getParentOperators().get(this.parentTag));\n+\n+  }\n+\n   public String getOperatorName() {\n     return operatorName;\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorHookContext.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java", "sha": "6c46bacb85d6966104855f5562957bd37bac570d", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -45,7 +45,7 @@ public boolean closeConnection() {\n       }\n       info.getConnection().close();\n       return true;\n-    } catch (SQLException e) {\n+    } catch (Exception e) {\n       LOG.error(\"Error during JDBC termination. \", e);\n       return false;\n     }\n@@ -100,7 +100,7 @@ public Void run(PreparedStatement stmt) throws SQLException {\n         };\n         PreparedStatement insStmt = info.getInsert(stats);\n         Utilities.executeWithRetry(execUpdate, insStmt, info.getWaitWindow(), info.getMaxRetries());\n-      } catch (SQLException e) {\n+      } catch (Exception e) {\n         LOG.error(\"ERROR during publishing profiling data. \", e);\n         return false;\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfiler.java", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfiler.java", "sha": "bdc9e50f4663eb5d8f77e9115ff22b6f5626412c", "changes": 52, "status": "modified", "deletions": 28, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfiler.java?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -17,18 +17,14 @@\n  */\n package org.apache.hadoop.hive.ql.profiler;\n \n-import java.lang.System;\n-import java.util.LinkedList;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.Iterator;\n import java.util.Collection;\n-import java.util.List;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n \n-import org.apache.hadoop.conf.Configuration;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hive.ql.exec.OperatorHook;\n import org.apache.hadoop.hive.ql.exec.OperatorHookContext;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -37,58 +33,58 @@\n   private final Log LOG = LogFactory.getLog(this.getClass().getName());\n   private static final HiveProfilePublisher pub = new HiveProfilePublisher();\n \n-  private LinkedList<HiveProfilerEntry> operatorCallStack =\n-    new LinkedList<HiveProfilerEntry>();\n+  private final Map<String, HiveProfilerEntry> operatorCallStack =\n+    new ConcurrentHashMap<String, HiveProfilerEntry>();\n \n   // Aggregates stats for each operator in memory so that stats are written to DB\n   // all at once - this allows the profiler to be extremely lightweight in\n   // communication with the DB\n-  private Map<String, HiveProfilerStats> aggrStats =\n-    new HashMap<String, HiveProfilerStats>();\n+  private final Map<String, HiveProfilerStats> aggrStats =\n+    new ConcurrentHashMap<String, HiveProfilerStats>();\n \n   public void enter(OperatorHookContext opHookContext) throws HiveException {\n+    String opLevelAnnoName = HiveProfilerUtils.getLevelAnnotatedName(opHookContext);\n     HiveProfilerEntry curEntry = new HiveProfilerEntry(opHookContext);\n-    operatorCallStack.addFirst(curEntry);\n+    assert(operatorCallStack.get(opLevelAnnoName) == null);\n+    operatorCallStack.put(opLevelAnnoName, curEntry);\n   }\n \n-  private void exit(HiveProfilerEntry curEntry, HiveProfilerEntry parentEntry) {\n+  private void exit(HiveProfilerEntry curEntry) {\n     OperatorHookContext opHookContext = curEntry.getOperatorHookContext();\n-\n     // update the metrics we are\n     long exitTime = System.nanoTime();\n     long wallTime = exitTime - curEntry.wallStartTime;\n \n     String opName = opHookContext.getOperatorName();\n \n-    OperatorHookContext parentContext =\n-      parentEntry != null ? parentEntry.getOperatorHookContext() :\n-        null;\n     Configuration conf = opHookContext.getOperator().getConfiguration();\n \n-    String opId = opHookContext.getOperatorId();\n-    if (aggrStats.containsKey(opId)) {\n-      aggrStats.get(opId).updateStats(wallTime, 1);\n+    String opLevelAnnoName = HiveProfilerUtils.getLevelAnnotatedName(opHookContext);\n+\n+    if (aggrStats.containsKey(opLevelAnnoName)) {\n+      aggrStats.get(opLevelAnnoName).updateStats(wallTime, 1);\n     } else {\n       HiveProfilerStats stats =\n-        new HiveProfilerStats(opHookContext, parentContext, 1, wallTime, conf);\n-      aggrStats.put(opId, stats);\n+        new HiveProfilerStats(opHookContext, 1, wallTime, conf);\n+      aggrStats.put(opLevelAnnoName, stats);\n     }\n \n   }\n   public void exit(OperatorHookContext opHookContext) throws HiveException {\n     if (operatorCallStack.isEmpty()) {\n       LOG.error(\"Unexpected state: Operator Call Stack is empty on exit.\");\n     }\n+    String opLevelAnnoName = HiveProfilerUtils.getLevelAnnotatedName(opHookContext);\n+\n+    HiveProfilerEntry curEntry = operatorCallStack.get(opLevelAnnoName);\n \n-    // grab the top item on the call stack since that should be\n-    // the first operator to exit.\n-    HiveProfilerEntry curEntry = operatorCallStack.poll();\n     if (!curEntry.getOperatorHookContext().equals(opHookContext)) {\n       LOG.error(\"Expected to exit from: \" + curEntry.getOperatorHookContext().toString() +\n         \" but exit called on \" + opHookContext.toString());\n     }\n-    HiveProfilerEntry parentEntry = operatorCallStack.peekFirst();\n-    exit(curEntry, parentEntry);\n+\n+    exit(curEntry);\n+    operatorCallStack.remove(opLevelAnnoName);\n   }\n \n   public void close(OperatorHookContext opHookContext) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfiler.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStats.java", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStats.java", "sha": "e4ee89cc259f0e137744adc30f60a69f71c7d31f", "changes": 18, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStats.java?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -21,6 +21,7 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.OperatorHookContext;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n \n@@ -50,24 +51,22 @@\n     Columns.CALL_COUNT\n   };\n \n-  private Map<String, String> stats = new HashMap<String, String>();\n+  private final Map<String, String> stats = new HashMap<String, String>();\n \n   long callCount;\n   long inclTime;\n   String taskId;\n \n   protected HiveProfilerStats(\n     OperatorHookContext opHookContext,\n-    OperatorHookContext parentOpHookContext,\n     long callCount, long wallTime, Configuration conf) {\n     this.callCount = callCount;\n     this.inclTime = wallTime;\n     this.taskId = Utilities.getTaskId(conf);\n-    populateStatsMap(opHookContext, parentOpHookContext, conf);\n+    populateStatsMap(opHookContext, conf);\n   }\n \n   private void populateStatsMap(OperatorHookContext opHookContext,\n-    OperatorHookContext parentOpHookContext,\n     Configuration conf) {\n     String queryId =\n       conf == null ? \"no conf\" : HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID);\n@@ -78,17 +77,14 @@ private void populateStatsMap(OperatorHookContext opHookContext,\n     stats.put(\n       Columns.OPERATOR_ID, opHookContext.getOperatorId());\n \n-    String parentOpName = parentOpHookContext == null ? \"\" : parentOpHookContext.getOperatorName();\n+    Operator parent = opHookContext.getParentOperator();\n+    String parentOpName = parent == null ? \"\" : parent.getName();\n     stats.put(Columns.PARENT_OPERATOR_NAME, parentOpName);\n \n-\n-    String parentOpId = parentOpHookContext == null ? \"-1\" : parentOpHookContext.getOperatorId();\n+    String parentOpId = parent == null ? \"-1\" : parent.getIdentifier();\n     stats.put(Columns.PARENT_OPERATOR_ID, parentOpId);\n \n-    String levelAnnoOpName = opName + \"_\" + opHookContext.getOperatorId();\n-    String levelAnnoName = parentOpHookContext == null ? \"main() ==> \" + levelAnnoOpName :\n-      parentOpName + \"_\" +  parentOpId + \" ==> \" + levelAnnoOpName;\n-    stats.put(Columns.LEVEL_ANNO_NAME, levelAnnoName);\n+    stats.put(Columns.LEVEL_ANNO_NAME, HiveProfilerUtils.getLevelAnnotatedName(opHookContext));\n \n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStats.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStatsAggregator.java", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStatsAggregator.java", "sha": "d640056a54bd1daa2a1e4b3f11750f3748e5f478", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStatsAggregator.java?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -110,7 +110,7 @@ private void populateAggregateStats(ResultSet result) {\n           stats.put(levelAnnoName, curStat);\n         }\n       }\n-    } catch (SQLException e) {\n+    } catch (Exception e) {\n       LOG.error(\"Error Aggregating Stats\", e);\n     }\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerStatsAggregator.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerUtils.java", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerUtils.java", "sha": "d220ed753ccd3a6c43e4aeef08ac599fdd996b90", "changes": 19, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerUtils.java?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -18,14 +18,15 @@\n package org.apache.hadoop.hive.ql.profiler;\n \n import java.sql.Connection;\n-import java.sql.Statement;\n import java.sql.DatabaseMetaData;\n import java.sql.DriverManager;\n import java.sql.ResultSet;\n import java.sql.SQLException;\n+import java.sql.Statement;\n \n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.MapOperator;\n+import org.apache.hadoop.hive.ql.exec.OperatorHookContext;\n \n public class HiveProfilerUtils {\n   public static void createTableIfNonExistent(HiveProfilerConnectionInfo info,\n@@ -57,4 +58,16 @@ public static boolean closeConnection(HiveProfilerConnectionInfo info) throws SQ\n     }\n     return true;\n   }\n+\n+  public static String getLevelAnnotatedName(OperatorHookContext opHookContext) {\n+    Operator parent = opHookContext.getParentOperator();\n+    if (parent != null && parent instanceof MapOperator) {\n+      parent = null;\n+    }\n+    Operator op = opHookContext.getOperator();\n+    String parentOpName = parent == null ? \"\" : parent.getName();\n+    String parentOpId = parent == null ? \"main()\" : parent.getOperatorId();\n+    String levelAnnoName = parentOpId + \" ==> \" + op.getOperatorId();\n+    return levelAnnoName;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilerUtils.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/queries/clientpositive/hiveprofiler0.q", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/queries/clientpositive/hiveprofiler0.q", "sha": "23996ec4d8951902e08c1c74a3c0c25789096f57", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/hiveprofiler0.q?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -1,6 +1,6 @@\n set hive.exec.operator.hooks=org.apache.hadoop.hive.ql.profiler.HiveProfiler;\n set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.HiveProfilerResultsHook;\n-SET hive.exec.mode.local.auto=false;\n SET hive.task.progress=true;\n \n select count(1) from src;\n+explain select count(1) from src;", "filename": "ql/src/test/queries/clientpositive/hiveprofiler0.q"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/queries/clientpositive/hiveprofiler_script0.q", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/queries/clientpositive/hiveprofiler_script0.q", "sha": "3ee30d31c12972561b5880f9c8766a393dd8c352", "changes": 15, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/hiveprofiler_script0.q?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -0,0 +1,15 @@\n+SET hive.task.progress=true;\n+set hive.exec.operator.hooks=org.apache.hadoop.hive.ql.profiler.HiveProfiler;\n+set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.HiveProfilerResultsHook;\n+ADD FILE src/test/scripts/testgrep;\n+\n+\n+-- checking that script operator does not cause NPE\n+-- Derby strangeness is causing the output collector for the Hive Profiler to not get output during DB read \n+\n+SELECT TRANSFORM(src.key, src.value)\n+       USING 'testgrep' AS (tkey, tvalue)\n+FROM src\n+\n+\n+", "filename": "ql/src/test/queries/clientpositive/hiveprofiler_script0.q"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/queries/clientpositive/hiveprofiler_union0.q", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/queries/clientpositive/hiveprofiler_union0.q", "sha": "0a941712f3f888e997b27bdbd69bdd134a451b12", "changes": 19, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/hiveprofiler_union0.q?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -0,0 +1,19 @@\n+set hive.exec.operator.hooks=org.apache.hadoop.hive.ql.profiler.HiveProfiler;\n+set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.HiveProfilerResultsHook;\n+SET hive.task.progress=true;\n+FROM (\n+  FROM src select src.key, src.value WHERE src.key < 100\n+  UNION ALL\n+  FROM src SELECT src.* WHERE src.key > 100\n+) unioninput\n+SELECT unioninput.*;\n+\n+explain\n+  FROM (\n+    FROM src select src.key, src.value WHERE src.key < 100\n+    UNION ALL\n+    FROM src SELECT src.* WHERE src.key > 100\n+  ) unioninput\n+  SELECT unioninput.*;\n+\n+", "filename": "ql/src/test/queries/clientpositive/hiveprofiler_union0.q"}, {"additions": 55, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/results/clientpositive/hiveprofiler0.q.out", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/results/clientpositive/hiveprofiler0.q.out", "sha": "d1777e212f42aa0063f3fe96a0f5917d92c60335", "changes": 57, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/hiveprofiler0.q.out?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -2,11 +2,64 @@ PREHOOK: query: select count(1) from src\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n #### A masked pattern was here ####\n+GBY_2 ==> RS_3: 1\n TS_0 ==> SEL_1: 500\n-main() ==> RS_3: 1\n SEL_1 ==> GBY_2: 500\n-main() ==> SEL_5: 1\n+GBY_4 ==> SEL_5: 1\n main() ==> GBY_4: 1\n main() ==> TS_0: 500\n SEL_5 ==> FS_6: 1\n 500\n+PREHOOK: query: explain select count(1) from src\n+PREHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION count 1)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        src \n+          TableScan\n+            alias: src\n+            Select Operator\n+              Group By Operator\n+                aggregations:\n+                      expr: count(1)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: bigint\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: count(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: bigint\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+", "filename": "ql/src/test/results/clientpositive/hiveprofiler0.q.out"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/results/clientpositive/hiveprofiler_script0.q.out", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/results/clientpositive/hiveprofiler_script0.q.out", "sha": "cc03f5d3292d1551c1e98a7d58a3fc69dc7b8060", "changes": 18, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/hiveprofiler_script0.q.out?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -0,0 +1,18 @@\n+PREHOOK: query: -- checking that script operator does not cause NPE\n+-- Derby strangeness is causing the output collector for the Hive Profiler to not get output during DB read \n+\n+SELECT TRANSFORM(src.key, src.value)\n+       USING 'testgrep' AS (tkey, tvalue)\n+FROM src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+103\tval_103\n+100\tval_100\n+10\tval_10\n+103\tval_103\n+104\tval_104\n+105\tval_105\n+104\tval_104\n+310\tval_310\n+100\tval_100", "filename": "ql/src/test/results/clientpositive/hiveprofiler_script0.q.out"}, {"additions": 598, "raw_url": "https://github.com/apache/hive/raw/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/results/clientpositive/hiveprofiler_union0.q.out", "blob_url": "https://github.com/apache/hive/blob/2f1c4bc3cc350acac32af38f8bcd95a588941c0b/ql/src/test/results/clientpositive/hiveprofiler_union0.q.out", "sha": "ac5c9dc29fd32960de8d7879da3c30a19c1164b0", "changes": 598, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/hiveprofiler_union0.q.out?ref=2f1c4bc3cc350acac32af38f8bcd95a588941c0b", "patch": "@@ -0,0 +1,598 @@\n+PREHOOK: query: FROM (\n+  FROM src select src.key, src.value WHERE src.key < 100\n+  UNION ALL\n+  FROM src SELECT src.* WHERE src.key > 100\n+) unioninput\n+SELECT unioninput.*\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+SEL_5 ==> UNION_6: 414\n+FIL_10 ==> SEL_5: 414\n+TS_0 ==> FIL_9: 500\n+TS_3 ==> FIL_10: 500\n+UNION_6 ==> SEL_7: 498\n+SEL_2 ==> UNION_6: 84\n+FIL_9 ==> SEL_2: 84\n+main() ==> TS_3: 500\n+main() ==> TS_0: 500\n+SEL_7 ==> FS_8: 498\n+238\tval_238\n+86\tval_86\n+311\tval_311\n+27\tval_27\n+165\tval_165\n+409\tval_409\n+255\tval_255\n+278\tval_278\n+98\tval_98\n+484\tval_484\n+265\tval_265\n+193\tval_193\n+401\tval_401\n+150\tval_150\n+273\tval_273\n+224\tval_224\n+369\tval_369\n+66\tval_66\n+128\tval_128\n+213\tval_213\n+146\tval_146\n+406\tval_406\n+429\tval_429\n+374\tval_374\n+152\tval_152\n+469\tval_469\n+145\tval_145\n+495\tval_495\n+37\tval_37\n+327\tval_327\n+281\tval_281\n+277\tval_277\n+209\tval_209\n+15\tval_15\n+82\tval_82\n+403\tval_403\n+166\tval_166\n+417\tval_417\n+430\tval_430\n+252\tval_252\n+292\tval_292\n+219\tval_219\n+287\tval_287\n+153\tval_153\n+193\tval_193\n+338\tval_338\n+446\tval_446\n+459\tval_459\n+394\tval_394\n+237\tval_237\n+482\tval_482\n+174\tval_174\n+413\tval_413\n+494\tval_494\n+207\tval_207\n+199\tval_199\n+466\tval_466\n+208\tval_208\n+174\tval_174\n+399\tval_399\n+396\tval_396\n+247\tval_247\n+417\tval_417\n+489\tval_489\n+162\tval_162\n+377\tval_377\n+397\tval_397\n+309\tval_309\n+365\tval_365\n+266\tval_266\n+439\tval_439\n+342\tval_342\n+367\tval_367\n+325\tval_325\n+167\tval_167\n+195\tval_195\n+475\tval_475\n+17\tval_17\n+113\tval_113\n+155\tval_155\n+203\tval_203\n+339\tval_339\n+0\tval_0\n+455\tval_455\n+128\tval_128\n+311\tval_311\n+316\tval_316\n+57\tval_57\n+302\tval_302\n+205\tval_205\n+149\tval_149\n+438\tval_438\n+345\tval_345\n+129\tval_129\n+170\tval_170\n+20\tval_20\n+489\tval_489\n+157\tval_157\n+378\tval_378\n+221\tval_221\n+92\tval_92\n+111\tval_111\n+47\tval_47\n+72\tval_72\n+4\tval_4\n+280\tval_280\n+35\tval_35\n+427\tval_427\n+277\tval_277\n+208\tval_208\n+356\tval_356\n+399\tval_399\n+169\tval_169\n+382\tval_382\n+498\tval_498\n+125\tval_125\n+386\tval_386\n+437\tval_437\n+469\tval_469\n+192\tval_192\n+286\tval_286\n+187\tval_187\n+176\tval_176\n+54\tval_54\n+459\tval_459\n+51\tval_51\n+138\tval_138\n+103\tval_103\n+239\tval_239\n+213\tval_213\n+216\tval_216\n+430\tval_430\n+278\tval_278\n+176\tval_176\n+289\tval_289\n+221\tval_221\n+65\tval_65\n+318\tval_318\n+332\tval_332\n+311\tval_311\n+275\tval_275\n+137\tval_137\n+241\tval_241\n+83\tval_83\n+333\tval_333\n+180\tval_180\n+284\tval_284\n+12\tval_12\n+230\tval_230\n+181\tval_181\n+67\tval_67\n+260\tval_260\n+404\tval_404\n+384\tval_384\n+489\tval_489\n+353\tval_353\n+373\tval_373\n+272\tval_272\n+138\tval_138\n+217\tval_217\n+84\tval_84\n+348\tval_348\n+466\tval_466\n+58\tval_58\n+8\tval_8\n+411\tval_411\n+230\tval_230\n+208\tval_208\n+348\tval_348\n+24\tval_24\n+463\tval_463\n+431\tval_431\n+179\tval_179\n+172\tval_172\n+42\tval_42\n+129\tval_129\n+158\tval_158\n+119\tval_119\n+496\tval_496\n+0\tval_0\n+322\tval_322\n+197\tval_197\n+468\tval_468\n+393\tval_393\n+454\tval_454\n+298\tval_298\n+199\tval_199\n+191\tval_191\n+418\tval_418\n+96\tval_96\n+26\tval_26\n+165\tval_165\n+327\tval_327\n+230\tval_230\n+205\tval_205\n+120\tval_120\n+131\tval_131\n+51\tval_51\n+404\tval_404\n+43\tval_43\n+436\tval_436\n+156\tval_156\n+469\tval_469\n+468\tval_468\n+308\tval_308\n+95\tval_95\n+196\tval_196\n+288\tval_288\n+481\tval_481\n+457\tval_457\n+98\tval_98\n+282\tval_282\n+197\tval_197\n+187\tval_187\n+318\tval_318\n+318\tval_318\n+409\tval_409\n+470\tval_470\n+137\tval_137\n+369\tval_369\n+316\tval_316\n+169\tval_169\n+413\tval_413\n+85\tval_85\n+77\tval_77\n+0\tval_0\n+490\tval_490\n+87\tval_87\n+364\tval_364\n+179\tval_179\n+118\tval_118\n+134\tval_134\n+395\tval_395\n+282\tval_282\n+138\tval_138\n+238\tval_238\n+419\tval_419\n+15\tval_15\n+118\tval_118\n+72\tval_72\n+90\tval_90\n+307\tval_307\n+19\tval_19\n+435\tval_435\n+10\tval_10\n+277\tval_277\n+273\tval_273\n+306\tval_306\n+224\tval_224\n+309\tval_309\n+389\tval_389\n+327\tval_327\n+242\tval_242\n+369\tval_369\n+392\tval_392\n+272\tval_272\n+331\tval_331\n+401\tval_401\n+242\tval_242\n+452\tval_452\n+177\tval_177\n+226\tval_226\n+5\tval_5\n+497\tval_497\n+402\tval_402\n+396\tval_396\n+317\tval_317\n+395\tval_395\n+58\tval_58\n+35\tval_35\n+336\tval_336\n+95\tval_95\n+11\tval_11\n+168\tval_168\n+34\tval_34\n+229\tval_229\n+233\tval_233\n+143\tval_143\n+472\tval_472\n+322\tval_322\n+498\tval_498\n+160\tval_160\n+195\tval_195\n+42\tval_42\n+321\tval_321\n+430\tval_430\n+119\tval_119\n+489\tval_489\n+458\tval_458\n+78\tval_78\n+76\tval_76\n+41\tval_41\n+223\tval_223\n+492\tval_492\n+149\tval_149\n+449\tval_449\n+218\tval_218\n+228\tval_228\n+138\tval_138\n+453\tval_453\n+30\tval_30\n+209\tval_209\n+64\tval_64\n+468\tval_468\n+76\tval_76\n+74\tval_74\n+342\tval_342\n+69\tval_69\n+230\tval_230\n+33\tval_33\n+368\tval_368\n+103\tval_103\n+296\tval_296\n+113\tval_113\n+216\tval_216\n+367\tval_367\n+344\tval_344\n+167\tval_167\n+274\tval_274\n+219\tval_219\n+239\tval_239\n+485\tval_485\n+116\tval_116\n+223\tval_223\n+256\tval_256\n+263\tval_263\n+70\tval_70\n+487\tval_487\n+480\tval_480\n+401\tval_401\n+288\tval_288\n+191\tval_191\n+5\tval_5\n+244\tval_244\n+438\tval_438\n+128\tval_128\n+467\tval_467\n+432\tval_432\n+202\tval_202\n+316\tval_316\n+229\tval_229\n+469\tval_469\n+463\tval_463\n+280\tval_280\n+2\tval_2\n+35\tval_35\n+283\tval_283\n+331\tval_331\n+235\tval_235\n+80\tval_80\n+44\tval_44\n+193\tval_193\n+321\tval_321\n+335\tval_335\n+104\tval_104\n+466\tval_466\n+366\tval_366\n+175\tval_175\n+403\tval_403\n+483\tval_483\n+53\tval_53\n+105\tval_105\n+257\tval_257\n+406\tval_406\n+409\tval_409\n+190\tval_190\n+406\tval_406\n+401\tval_401\n+114\tval_114\n+258\tval_258\n+90\tval_90\n+203\tval_203\n+262\tval_262\n+348\tval_348\n+424\tval_424\n+12\tval_12\n+396\tval_396\n+201\tval_201\n+217\tval_217\n+164\tval_164\n+431\tval_431\n+454\tval_454\n+478\tval_478\n+298\tval_298\n+125\tval_125\n+431\tval_431\n+164\tval_164\n+424\tval_424\n+187\tval_187\n+382\tval_382\n+5\tval_5\n+70\tval_70\n+397\tval_397\n+480\tval_480\n+291\tval_291\n+24\tval_24\n+351\tval_351\n+255\tval_255\n+104\tval_104\n+70\tval_70\n+163\tval_163\n+438\tval_438\n+119\tval_119\n+414\tval_414\n+200\tval_200\n+491\tval_491\n+237\tval_237\n+439\tval_439\n+360\tval_360\n+248\tval_248\n+479\tval_479\n+305\tval_305\n+417\tval_417\n+199\tval_199\n+444\tval_444\n+120\tval_120\n+429\tval_429\n+169\tval_169\n+443\tval_443\n+323\tval_323\n+325\tval_325\n+277\tval_277\n+230\tval_230\n+478\tval_478\n+178\tval_178\n+468\tval_468\n+310\tval_310\n+317\tval_317\n+333\tval_333\n+493\tval_493\n+460\tval_460\n+207\tval_207\n+249\tval_249\n+265\tval_265\n+480\tval_480\n+83\tval_83\n+136\tval_136\n+353\tval_353\n+172\tval_172\n+214\tval_214\n+462\tval_462\n+233\tval_233\n+406\tval_406\n+133\tval_133\n+175\tval_175\n+189\tval_189\n+454\tval_454\n+375\tval_375\n+401\tval_401\n+421\tval_421\n+407\tval_407\n+384\tval_384\n+256\tval_256\n+26\tval_26\n+134\tval_134\n+67\tval_67\n+384\tval_384\n+379\tval_379\n+18\tval_18\n+462\tval_462\n+492\tval_492\n+298\tval_298\n+9\tval_9\n+341\tval_341\n+498\tval_498\n+146\tval_146\n+458\tval_458\n+362\tval_362\n+186\tval_186\n+285\tval_285\n+348\tval_348\n+167\tval_167\n+18\tval_18\n+273\tval_273\n+183\tval_183\n+281\tval_281\n+344\tval_344\n+97\tval_97\n+469\tval_469\n+315\tval_315\n+84\tval_84\n+28\tval_28\n+37\tval_37\n+448\tval_448\n+152\tval_152\n+348\tval_348\n+307\tval_307\n+194\tval_194\n+414\tval_414\n+477\tval_477\n+222\tval_222\n+126\tval_126\n+90\tval_90\n+169\tval_169\n+403\tval_403\n+400\tval_400\n+200\tval_200\n+97\tval_97\n+PREHOOK: query: explain\n+  FROM (\n+    FROM src select src.key, src.value WHERE src.key < 100\n+    UNION ALL\n+    FROM src SELECT src.* WHERE src.key > 100\n+  ) unioninput\n+  SELECT unioninput.*\n+PREHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL src) value))) (TOK_WHERE (< (. (TOK_TABLE_OR_COL src) key) 100)))) (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME src)))) (TOK_WHERE (> (. (TOK_TABLE_OR_COL src) key) 100))))) unioninput)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF (TOK_TABNAME unioninput))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        null-subquery1:unioninput-subquery1:src \n+          TableScan\n+            alias: src\n+            Filter Operator\n+              predicate:\n+                  expr: (key < 100.0)\n+                  type: boolean\n+              Select Operator\n+                expressions:\n+                      expr: key\n+                      type: string\n+                      expr: value\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                Union\n+                  Select Operator\n+                    expressions:\n+                          expr: _col0\n+                          type: string\n+                          expr: _col1\n+                          type: string\n+                    outputColumnNames: _col0, _col1\n+                    File Output Operator\n+                      compressed: false\n+                      GlobalTableId: 0\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+        null-subquery2:unioninput-subquery2:src \n+          TableScan\n+            alias: src\n+            Filter Operator\n+              predicate:\n+                  expr: (key > 100.0)\n+                  type: boolean\n+              Select Operator\n+                expressions:\n+                      expr: key\n+                      type: string\n+                      expr: value\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                Union\n+                  Select Operator\n+                    expressions:\n+                          expr: _col0\n+                          type: string\n+                          expr: _col1\n+                          type: string\n+                    outputColumnNames: _col0, _col1\n+                    File Output Operator\n+                      compressed: false\n+                      GlobalTableId: 0\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+", "filename": "ql/src/test/results/clientpositive/hiveprofiler_union0.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a525b3e13b2abc6f9b7ee009c6b424cb107405f9", "parent": "https://github.com/apache/hive/commit/f23c3852d30c1bf80c9ff42eb7e8bc8b0b127cd8", "message": "HIVE-4186 : NPE in ReduceSinkDeDuplication (Harish Butani via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1458524 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_160", "file": [{"additions": 8, "raw_url": "https://github.com/apache/hive/raw/a525b3e13b2abc6f9b7ee009c6b424cb107405f9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkDeDuplication.java", "blob_url": "https://github.com/apache/hive/blob/a525b3e13b2abc6f9b7ee009c6b424cb107405f9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkDeDuplication.java", "sha": "11ed9298be177a6c7385c359b16ed8cc0b48391b", "changes": 12, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkDeDuplication.java?ref=a525b3e13b2abc6f9b7ee009c6b424cb107405f9", "patch": "@@ -412,16 +412,20 @@ private boolean backTrackColumnNames(\n         if(partitionCols != null) {\n           for (ExprNodeDesc desc : partitionCols) {\n             List<String> cols = desc.getCols();\n-            for(String col : cols) {\n-              columnMapping.put(col, col);\n+            if ( cols != null ) {\n+              for(String col : cols) {\n+                columnMapping.put(col, col);\n+              }\n             }\n           }\n         }\n         if(reduceKeyCols != null) {\n           for (ExprNodeDesc desc : reduceKeyCols) {\n             List<String> cols = desc.getCols();\n-            for(String col : cols) {\n-              columnMapping.put(col, col);\n+            if ( cols != null ) {\n+              for(String col : cols) {\n+                columnMapping.put(col, col);\n+              }\n             }\n           }\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkDeDuplication.java"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/a525b3e13b2abc6f9b7ee009c6b424cb107405f9/ql/src/test/queries/clientpositive/reducesink_dedup.q", "blob_url": "https://github.com/apache/hive/blob/a525b3e13b2abc6f9b7ee009c6b424cb107405f9/ql/src/test/queries/clientpositive/reducesink_dedup.q", "sha": "d5aa2513c8b3b50590fdf9d24e3096e63771133e", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/reducesink_dedup.q?ref=a525b3e13b2abc6f9b7ee009c6b424cb107405f9", "patch": "@@ -0,0 +1,20 @@\n+DROP TABLE part;\n+\n+-- data setup\n+CREATE TABLE part( \n+    p_partkey INT,\n+    p_name STRING,\n+    p_mfgr STRING,\n+    p_brand STRING,\n+    p_type STRING,\n+    p_size INT,\n+    p_container STRING,\n+    p_retailprice DOUBLE,\n+    p_comment STRING\n+);\n+\n+\n+select p_name \n+from (select p_name from part distribute by 1 sort by 1) p \n+distribute by 1 sort by 1\n+;\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/reducesink_dedup.q"}, {"additions": 43, "raw_url": "https://github.com/apache/hive/raw/a525b3e13b2abc6f9b7ee009c6b424cb107405f9/ql/src/test/results/clientpositive/reducesink_dedup.q.out", "blob_url": "https://github.com/apache/hive/blob/a525b3e13b2abc6f9b7ee009c6b424cb107405f9/ql/src/test/results/clientpositive/reducesink_dedup.q.out", "sha": "32dfdd429346b3c62dc72ca430fb1425122c3615", "changes": 43, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/reducesink_dedup.q.out?ref=a525b3e13b2abc6f9b7ee009c6b424cb107405f9", "patch": "@@ -0,0 +1,43 @@\n+PREHOOK: query: DROP TABLE part\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE part\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: -- data setup\n+CREATE TABLE part( \n+    p_partkey INT,\n+    p_name STRING,\n+    p_mfgr STRING,\n+    p_brand STRING,\n+    p_type STRING,\n+    p_size INT,\n+    p_container STRING,\n+    p_retailprice DOUBLE,\n+    p_comment STRING\n+)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: -- data setup\n+CREATE TABLE part( \n+    p_partkey INT,\n+    p_name STRING,\n+    p_mfgr STRING,\n+    p_brand STRING,\n+    p_type STRING,\n+    p_size INT,\n+    p_container STRING,\n+    p_retailprice DOUBLE,\n+    p_comment STRING\n+)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@part\n+PREHOOK: query: select p_name \n+from (select p_name from part distribute by 1 sort by 1) p \n+distribute by 1 sort by 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@part\n+#### A masked pattern was here ####\n+POSTHOOK: query: select p_name \n+from (select p_name from part distribute by 1 sort by 1) p \n+distribute by 1 sort by 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@part\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/reducesink_dedup.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e521e4db0f22aad59fe558deebdb8c3bfedc7aa3", "parent": "https://github.com/apache/hive/commit/6b4aa70e5260204560d2eb8c5e97834147bb4d37", "message": "HIVE-4029 Hive Profiler dies with NPE\n(Brock Noland via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1447130 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_161", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/e521e4db0f22aad59fe558deebdb8c3bfedc7aa3/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java", "blob_url": "https://github.com/apache/hive/blob/e521e4db0f22aad59fe558deebdb8c3bfedc7aa3/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java", "sha": "d87e8d30f9b95cff80452e411ed9efa3147b6e43", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java?ref=e521e4db0f22aad59fe558deebdb8c3bfedc7aa3", "patch": "@@ -36,7 +36,7 @@\n \n   public boolean closeConnection() {\n \n-    if (info.getConnection() == null) {\n+    if (info == null || info.getConnection() == null) {\n       return true;\n     }\n     try {\n@@ -82,7 +82,7 @@ private String getCreate() {\n \n   public boolean publishStat(String queryId, Map<String, String> stats,\n     Configuration conf) {\n-    if (info.getConnection() == null) {\n+    if (info == null || info.getConnection() == null) {\n       if(!initialize(conf)) {\n         return false;\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/profiler/HiveProfilePublisher.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/af510fa61b2203aca2c210aee9542f05e2692659", "parent": "https://github.com/apache/hive/commit/d59a476e6aee105d80ecc361f8656dd112f6fbc1", "message": "HIVE-3265. HiveHistory.printRowCount() throws NPE (Shreepadma Venugopalan via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1378472 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_162", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/af510fa61b2203aca2c210aee9542f05e2692659/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java", "blob_url": "https://github.com/apache/hive/blob/af510fa61b2203aca2c210aee9542f05e2692659/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java", "sha": "7e80c2d6bd5689a5be4e8ce88be4a64750de7553", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java?ref=af510fa61b2203aca2c210aee9542f05e2692659", "patch": "@@ -409,6 +409,9 @@ public void setTaskCounters(String queryId, String taskId, Counters ctrs) {\n \n   public void printRowCount(String queryId) {\n     QueryInfo ji = queryInfoMap.get(queryId);\n+    if (ji == null) {\n+      return;\n+    }\n     for (String tab : ji.rowCountMap.keySet()) {\n       console.printInfo(ji.rowCountMap.get(tab) + \" Rows loaded to \" + tab);\n     }\n@@ -420,7 +423,6 @@ public void printRowCount(String queryId) {\n    * @param queryId\n    */\n   public void endQuery(String queryId) {\n-\n     QueryInfo ji = queryInfoMap.get(queryId);\n     if (ji == null) {\n       return;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6e21916e15b00143562d22612374299709c0c8a4", "parent": "https://github.com/apache/hive/commit/5aae3d8dba5f8c437e04385a1596e19705b8f5b0", "message": "HIVE-2157 NPE in MapJoinObjectKey\n(Yongqiang He via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1101770 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_163", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/6e21916e15b00143562d22612374299709c0c8a4/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectKey.java", "blob_url": "https://github.com/apache/hive/blob/6e21916e15b00143562d22612374299709c0c8a4/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectKey.java", "sha": "a344daff8be72f7dd0969cca76ef1f5341379c27", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectKey.java?ref=6e21916e15b00143562d22612374299709c0c8a4", "patch": "@@ -60,6 +60,9 @@ public boolean equals(Object o) {\n       if ((obj != null) && (mObjArray != null)) {\n         if (obj.length == mObjArray.length) {\n           for (int i = 0; i < obj.length; i++) {\n+            if (obj[i] == null) {\n+              return mObjArray[i] == null;\n+            }\n             if (!obj[i].equals(mObjArray[i])) {\n               return false;\n             }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinObjectKey.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d71a758ecd622e97d59d52d275661232dcdbabb4", "parent": "https://github.com/apache/hive/commit/4725e62a911b5af88e13b33a17117383f3744dd2", "message": "HIVE-1547 Unarchiving operation throws NPE\n(Paul Yang via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@986481 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_164", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/d71a758ecd622e97d59d52d275661232dcdbabb4/CHANGES.txt", "blob_url": "https://github.com/apache/hive/blob/d71a758ecd622e97d59d52d275661232dcdbabb4/CHANGES.txt", "sha": "1d2f7ab9a50ef17772c890342d98d0c36b7be288", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=d71a758ecd622e97d59d52d275661232dcdbabb4", "patch": "@@ -148,7 +148,7 @@ Trunk -  Unreleased\n     (Ning Zhang via namit)\n \n     HIVE-1547 Unarchiving operation throws NPE\n-    (He Yongqiang via namit)\n+    (Paul Yang via namit)\n \n   TESTS\n ", "filename": "CHANGES.txt"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4725e62a911b5af88e13b33a17117383f3744dd2", "parent": "https://github.com/apache/hive/commit/4b022871902fe99c932ecb9d551db2260886e3e6", "message": "HIVE-1547 Unarchiving operation throws NPE\n(Paul Yang via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@986480 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_165", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/4725e62a911b5af88e13b33a17117383f3744dd2/CHANGES.txt", "blob_url": "https://github.com/apache/hive/blob/4725e62a911b5af88e13b33a17117383f3744dd2/CHANGES.txt", "sha": "1865fb83aeb8f3395f1cf69a0c41b32598379da8", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=4725e62a911b5af88e13b33a17117383f3744dd2", "patch": "@@ -147,6 +147,9 @@ Trunk -  Unreleased\n     HIVE-1543 Abort in ExecMapper when record reader's next gets a exception\n     (Ning Zhang via namit)\n \n+    HIVE-1547 Unarchiving operation throws NPE\n+    (He Yongqiang via namit)\n+\n   TESTS\n \n     HIVE-1464. improve  test query performance", "filename": "CHANGES.txt"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/4725e62a911b5af88e13b33a17117383f3744dd2/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "blob_url": "https://github.com/apache/hive/blob/4725e62a911b5af88e13b33a17117383f3744dd2/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "sha": "56b1c60910bce65bae01e9b386e03a507e545a15", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=4725e62a911b5af88e13b33a17117383f3744dd2", "patch": "@@ -726,7 +726,7 @@ private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)\n       // Verify that there are no files in the tmp dir, because if there are, it\n       // would be copied to the partition\n       FileStatus [] filesInTmpDir = fs.listStatus(tmpDir);\n-      if (filesInTmpDir.length != 0) {\n+      if (filesInTmpDir != null && filesInTmpDir.length != 0) {\n         for (FileStatus file : filesInTmpDir) {\n           console.printInfo(file.getPath().toString());\n         }\n@@ -1584,7 +1584,7 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n     // alter the table\n     Table tbl = db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, alterTbl\n         .getOldName());\n-    \n+\n     Partition part = null;\n     if(alterTbl.getPartSpec() != null) {\n       part = db.getPartition(tbl, alterTbl.getPartSpec(), false);\n@@ -1777,7 +1777,7 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n       if (part != null) {\n         part.setProtectMode(mode);\n       } else {\n-        tbl.setProtectMode(mode);        \n+        tbl.setProtectMode(mode);\n       }\n \n     } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDCLUSTERSORTCOLUMN) {\n@@ -1857,12 +1857,12 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n       part.getParameters().put(\"last_modified_time\", Long.toString(System\n           .currentTimeMillis() / 1000));\n     }\n-    \n+\n     try {\n       if (part == null) {\n         db.alterTable(alterTbl.getOldName(), tbl);\n       } else {\n-        db.alterPartition(tbl.getTableName(), part);        \n+        db.alterPartition(tbl.getTableName(), part);\n       }\n     } catch (InvalidOperationException e) {\n       console.printError(\"Invalid alter operation: \" + e.getMessage());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ae29959d45e3178290a07bcba372e60541dfd4c7", "parent": "https://github.com/apache/hive/commit/138a1d5244e6044467180de225b3a1c80ce7b407", "message": "HIVE-1188. NPE when running TestJdbcDriver/TestHiveServer\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@915215 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_166", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/CHANGES.txt", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/CHANGES.txt", "sha": "125e9810edfc0742fa313d5ee29675149d9edfb1", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -30,6 +30,9 @@ Trunk -  Unreleased\n     HIVE-1178. enforce bucketing for a table.\n     (Namit Jain via He Yongqiang)\n \n+    HIVE-1188. NPE when TestJdbcDriver/TestHiveServer\n+    (Call Steinbach via Ning Zhang)\n+\n   IMPROVEMENTS\n     HIVE-983. Function from_unixtime takes long.\n     (Ning Zhang via zshao)", "filename": "CHANGES.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/negative/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/negative/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/queries/negative/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "contrib/src/test/queries/negative/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/positive/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/queries/positive/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/queries/positive/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "contrib/src/test/queries/positive/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/errors/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/errors/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/results/compiler/errors/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "contrib/src/test/results/compiler/errors/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/parse/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/parse/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/results/compiler/parse/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "contrib/src/test/results/compiler/parse/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/plan/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/contrib/src/test/results/compiler/plan/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/contrib/src/test/results/compiler/plan/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "contrib/src/test/results/compiler/plan/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/data/metadb/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/data/metadb/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/metadb/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "data/metadb/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/data/warehouse/src/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/data/warehouse/src/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/warehouse/src/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "data/warehouse/src/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/metastore/bin/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/metastore/bin/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/bin/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "metastore/bin/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "ql/src/java/org/apache/hadoop/hive/ql/thrift/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "ql/src/test/org/apache/hadoop/hive/ql/negative/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "ql/src/test/org/apache/hadoop/hive/ql/output/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "ql/src/test/org/apache/hadoop/hive/ql/parse/negative/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "ql/src/test/org/apache/hadoop/hive/ql/parse/positive/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "ql/src/test/org/apache/hadoop/hive/ql/positive/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "serde/src/gen-java/org/apache/hadoop/hive/serde/dynamic_type/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "serde/src/gen-java/org/apache/hadoop/hive/serde2/dynamic_type/.gitignore"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-py/serde/.gitignore", "blob_url": "https://github.com/apache/hive/blob/ae29959d45e3178290a07bcba372e60541dfd4c7/serde/src/gen-py/serde/.gitignore", "sha": "0dd98905045cd69b5bff39fd7deacb1eb62d750d", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/gen-py/serde/.gitignore?ref=ae29959d45e3178290a07bcba372e60541dfd4c7", "patch": "@@ -0,0 +1 @@\n+# Dummy file to make Git recognize this empty directory", "filename": "serde/src/gen-py/serde/.gitignore"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "parent": "https://github.com/apache/hive/commit/d6c28bb2b224fd921c97313ea780296496bf692c", "message": "HIVE-496: NPE in UDFs on NULL input\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@776541 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_167", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/CHANGES.txt", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/CHANGES.txt", "sha": "b37a5964b9600f6999ac046be4bdb81bbba7ecdc", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -138,6 +138,8 @@ Trunk - Unreleased\n     HIVE-488. Fix load partition for table with multiple partition columns.\n     (Prasad Chakka via athusoo)\n \n+    HIVE-496. UDFDate fails on null. (Zheng Shao via pchakka)\n+\n Release 0.3.1 - Unreleased\n \n   INCOMPATIBLE CHANGES", "filename": "CHANGES.txt"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFConcat.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFConcat.java", "sha": "c0c64cd33fc77a136bcccebb32d78befe953d996", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFConcat.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -29,6 +29,9 @@ public UDFConcat() {\n \n   Text text = new Text();\n   public Text evaluate(Text a, Text b) {\n+    if (a == null || b == null) {\n+      return null;\n+    }\n     text.clear();\n     text.set(a);\n     text.append(b.getBytes(), 0, b.getLength());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFConcat.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDate.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDate.java", "sha": "9f9cc184f51ec7c259ffcf7366069dc399a698aa", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDate.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -47,6 +47,10 @@ public UDFDate() {\n    */\n   public Text evaluate(Text dateString)  {\n     \n+    if (dateString == null) {\n+      return null;\n+    }\n+    \n     try {\n       Date date = formatter.parse(dateString.toString());\n       t.set(formatter.format(date));", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDate.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateAdd.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateAdd.java", "sha": "55e4daac36c8b6e9552b85b71f77942d1d2d9dfc", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateAdd.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -56,6 +56,10 @@ public UDFDateAdd() {\n    */\n   public Text evaluate(Text dateString1, IntWritable days)  {\n     \n+    if (dateString1 == null || days == null) {\n+      return null;\n+    }\n+    \n     try {\n       calendar.setTime(formatter.parse(dateString1.toString()));\n       calendar.add(Calendar.DAY_OF_MONTH, days.get());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateAdd.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateDiff.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateDiff.java", "sha": "bb792c20899487972da07c24e4e351beac624098", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateDiff.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -51,6 +51,11 @@ public UDFDateDiff() {\n    * @return the difference in days.\n    */\n   public IntWritable evaluate(Text dateString1, Text dateString2)  {\n+    \n+    if (dateString1 == null || dateString2 == null) {\n+      return null;\n+    }\n+    \n     try {\n       // NOTE: This implementation avoids the extra-second problem\n       // by comparing with UTC epoch and integer division.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateDiff.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateSub.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateSub.java", "sha": "12917b9ad220cd87a86094f56f2ec12dfbd9ee01", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateSub.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -55,6 +55,10 @@ public UDFDateSub() {\n    */\n   public Text evaluate(Text dateString1, IntWritable days)  {\n     \n+    if (dateString1 == null || days == null) {\n+      return null;\n+    }\n+    \n     try {\n       calendar.setTime(formatter.parse(dateString1.toString()));\n       calendar.add(Calendar.DAY_OF_MONTH, -days.get());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDateSub.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDayOfMonth.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDayOfMonth.java", "sha": "5088d428786f6fde7fcd82d71600824a31240bfe", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDayOfMonth.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -49,6 +49,10 @@ public UDFDayOfMonth() {\n    */\n   public IntWritable evaluate(Text dateString)  {\n     \n+    if (dateString == null) {\n+      return null;\n+    }\n+    \n     try {\n       Date date = formatter.parse(dateString.toString());\n       calendar.setTime(date);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDayOfMonth.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java", "sha": "8d140bbb4bc15ccf406f531dafee9085a13a6af3", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -33,14 +33,14 @@ public UDFDefaultSampleHashFn() {\n   }\n \n   public IntWritable evaluate(Object o) {\n-    result.set(o.hashCode());\n+    result.set(o == null ? 0 : o.hashCode());\n     return result;\n   }\n   \n   // TODO: For now, only allow up to two columns on which to sample\n   // Going forward we will allow sampling on an arbitrary number of columns\n   public IntWritable evaluate(Object o1, Object o2) {\n-    result.set(o1.hashCode() ^ o2.hashCode());\n+    result.set((o1 == null ? 0 : o1.hashCode()) ^ (o2 == null ? 0 : o2.hashCode()));\n     return result;\n   }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFDefaultSampleHashFn.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMonth.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMonth.java", "sha": "5b36fc6e8026b7f03e441d21e1d427e3d92b2938", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMonth.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -48,7 +48,9 @@ public UDFMonth() {\n    * @return an int from 1 to 12. null if the dateString is not a valid date string.\n    */\n   public IntWritable evaluate(Text dateString)  {\n-    \n+    if (dateString == null) {\n+      return null;\n+    }\n     try {\n       Date date = formatter.parse(dateString.toString());\n       calendar.setTime(date);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMonth.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFYear.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFYear.java", "sha": "7c46fd680156f981f7ca0084efad4d0afdfd5f6a", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFYear.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -49,6 +49,10 @@ public UDFYear() {\n    */\n   public IntWritable evaluate(Text dateString)  {\n     \n+    if (dateString == null) {\n+      return null;\n+    }\n+    \n     try {\n       Date date = formatter.parse(dateString.toString());\n       calendar.setTime(date);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFYear.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFWhen.java", "blob_url": "https://github.com/apache/hive/blob/eb3e40c95cba207661acd7f8c5d79cc40625ec9c/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFWhen.java", "sha": "5ca3b1e4f40ec842db4a44afa03f2127657aa67d", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFWhen.java?ref=eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "patch": "@@ -86,7 +86,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     for (int i=0; i+1<arguments.length; i+=2) {\n       Object caseKey = arguments[i].get();\n-      if (((BooleanObjectInspector)argumentOIs[i]).get(caseKey)) {\n+      if (caseKey != null && ((BooleanObjectInspector)argumentOIs[i]).get(caseKey)) {\n         Object caseValue = arguments[i+1].get();\n         return returnOIResolver.convertIfNecessary(caseValue,\n             argumentOIs[i+1]);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFWhen.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e9860bb80b7a2adeee7732b0dba60cb4d789e249", "parent": "https://github.com/apache/hive/commit/fa57c1733c6f08bc9fc840026dd7b7521f60a226", "message": "HIVE-10408. Fix NPE in scheduler in case of rejected tasks. (Siddharth Seth)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1675176 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_168", "file": [{"additions": 65, "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java", "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java", "sha": "a9033694ae1457775d75d3a4fc438a0b69115cb4", "changes": 110, "status": "modified", "deletions": 45, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249", "patch": "@@ -19,15 +19,16 @@\n import java.net.UnknownHostException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedHashMap;\n import java.util.Map;\n import java.util.Set;\n import java.util.UUID;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeUnit;\n \n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.PathNotFoundException;\n import org.apache.hadoop.hive.llap.configuration.LlapConfiguration;\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstance;\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstanceSet;\n@@ -37,7 +38,6 @@\n import org.apache.hadoop.registry.client.binding.RegistryTypeUtils;\n import org.apache.hadoop.registry.client.binding.RegistryUtils;\n import org.apache.hadoop.registry.client.binding.RegistryUtils.ServiceRecordMarshal;\n-import org.apache.hadoop.registry.client.exceptions.InvalidRecordException;\n import org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService;\n import org.apache.hadoop.registry.client.types.AddressTypes;\n import org.apache.hadoop.registry.client.types.Endpoint;\n@@ -55,9 +55,10 @@\n   private static final Logger LOG = Logger.getLogger(LlapYarnRegistryImpl.class);\n \n   private RegistryOperationsService client;\n-  private String instanceName;\n-  private Configuration conf;\n-  private ServiceRecordMarshal encoder;\n+  private final String instanceName;\n+  private final Configuration conf;\n+  private final ServiceRecordMarshal encoder;\n+  private final String path;\n \n   private final DynamicServiceInstanceSet instances = new DynamicServiceInstanceSet();\n \n@@ -68,7 +69,8 @@\n \n   private final static String SERVICE_CLASS = \"org-apache-hive\";\n \n-  final ScheduledExecutorService refresher = Executors.newScheduledThreadPool(1);\n+  final ScheduledExecutorService refresher = Executors.newScheduledThreadPool(1,\n+      new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"LlapYarnRegistryRefresher\").build());\n   final long refreshDelay;\n \n   static {\n@@ -90,6 +92,8 @@ public LlapYarnRegistryImpl(String instanceName, Configuration conf) {\n     // registry reference\n     client = (RegistryOperationsService) RegistryOperationsFactory.createInstance(conf);\n     encoder = new RegistryUtils.ServiceRecordMarshal();\n+    this.path = RegistryPathUtils.join(RegistryUtils.componentPath(RegistryUtils.currentUser(),\n+        SERVICE_CLASS, instanceName, \"workers\"), \"worker-\");\n     refreshDelay =\n         conf.getInt(LlapConfiguration.LLAP_DAEMON_SERVICE_REFRESH_INTERVAL,\n             LlapConfiguration.LLAP_DAEMON_SERVICE_REFRESH_INTERVAL_DEFAULT);\n@@ -114,8 +118,7 @@ public Endpoint getShuffleEndpoint() {\n   }\n \n   private final String getPath() {\n-    return RegistryPathUtils.join(RegistryUtils.componentPath(RegistryUtils.currentUser(),\n-        SERVICE_CLASS, instanceName, \"workers\"), \"worker-\");\n+    return this.path;\n   }\n \n   @Override\n@@ -199,7 +202,8 @@ public boolean isAlive() {\n     }\n \n     public void kill() {\n-      LOG.info(\"Killing \" + this);\n+      // May be possible to generate a notification back to the scheduler from here.\n+      LOG.info(\"Killing service instance: \" + this);\n       this.alive = false;\n     }\n \n@@ -217,74 +221,90 @@ public Resource getResource() {\n \n     @Override\n     public String toString() {\n-      return \"DynamicServiceInstance [alive=\" + alive + \", host=\" + host + \":\" + rpcPort + \"]\";\n+      return \"DynamicServiceInstance [alive=\" + alive + \", host=\" + host + \":\" + rpcPort + \" with resources=\" + getResource() +\"]\";\n     }\n+\n+    // Relying on the identity hashCode and equality, since refreshing instances retains the old copy\n+    // of an already known instance.\n   }\n \n   private class DynamicServiceInstanceSet implements ServiceInstanceSet {\n \n-    Map<String, ServiceInstance> instances;\n+    // LinkedHashMap to retain iteration order.\n+    private final Map<String, ServiceInstance> instances = new LinkedHashMap<>();\n \n     @Override\n-    public Map<String, ServiceInstance> getAll() {\n-      return instances;\n+    public synchronized Map<String, ServiceInstance> getAll() {\n+      // Return a copy. Instances may be modified during a refresh.\n+      return new LinkedHashMap<>(instances);\n     }\n \n     @Override\n-    public ServiceInstance getInstance(String name) {\n+    public synchronized ServiceInstance getInstance(String name) {\n       return instances.get(name);\n     }\n \n     @Override\n-    public synchronized void refresh() throws IOException {\n+    public  void refresh() throws IOException {\n       /* call this from wherever */\n       Map<String, ServiceInstance> freshInstances = new HashMap<String, ServiceInstance>();\n \n       String path = getPath();\n       Map<String, ServiceRecord> records =\n           RegistryUtils.listServiceRecords(client, RegistryPathUtils.parentOf(path));\n-      Set<String> latestKeys = new HashSet<String>();\n-      LOG.info(\"Starting to refresh ServiceInstanceSet \" + System.identityHashCode(this));\n-      for (ServiceRecord rec : records.values()) {\n-        ServiceInstance instance = new DynamicServiceInstance(rec);\n-        if (instance != null) {\n-          if (instances != null && instances.containsKey(instance.getWorkerIdentity()) == false) {\n-            // add a new object\n-            freshInstances.put(instance.getWorkerIdentity(), instance);\n-            if (LOG.isInfoEnabled()) {\n-              LOG.info(\"Adding new worker \" + instance.getWorkerIdentity() + \" which mapped to \"\n-                  + instance);\n+      // Synchronize after reading the service records from the external service (ZK)\n+      synchronized (this) {\n+        Set<String> latestKeys = new HashSet<String>();\n+        LOG.info(\"Starting to refresh ServiceInstanceSet \" + System.identityHashCode(this));\n+        for (ServiceRecord rec : records.values()) {\n+          ServiceInstance instance = new DynamicServiceInstance(rec);\n+          if (instance != null) {\n+            if (instances != null && instances.containsKey(instance.getWorkerIdentity()) == false) {\n+              // add a new object\n+              freshInstances.put(instance.getWorkerIdentity(), instance);\n+              if (LOG.isInfoEnabled()) {\n+                LOG.info(\"Adding new worker \" + instance.getWorkerIdentity() + \" which mapped to \"\n+                    + instance);\n+              }\n+            } else {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Retaining running worker \" + instance.getWorkerIdentity() +\n+                    \" which mapped to \" + instance);\n+              }\n             }\n-          } else if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Retaining running worker \" + instance.getWorkerIdentity() + \" which mapped to \" + instance);\n           }\n+          latestKeys.add(instance.getWorkerIdentity());\n         }\n-        latestKeys.add(instance.getWorkerIdentity());\n-      }\n \n-      if (instances != null) {\n-        // deep-copy before modifying\n-        Set<String> oldKeys = new HashSet(instances.keySet());\n-        if (oldKeys.removeAll(latestKeys)) {\n-          for (String k : oldKeys) {\n-            // this is so that people can hold onto ServiceInstance references as placeholders for tasks\n-            final DynamicServiceInstance dead = (DynamicServiceInstance) instances.get(k);\n-            dead.kill();\n-            if (LOG.isInfoEnabled()) {\n-              LOG.info(\"Deleting dead worker \" + k + \" which mapped to \" + dead);\n+        if (instances != null) {\n+          // deep-copy before modifying\n+          Set<String> oldKeys = new HashSet(instances.keySet());\n+          if (oldKeys.removeAll(latestKeys)) {\n+            // This is all the records which have not checked in, and are effectively dead.\n+            for (String k : oldKeys) {\n+              // this is so that people can hold onto ServiceInstance references as placeholders for tasks\n+              final DynamicServiceInstance dead = (DynamicServiceInstance) instances.get(k);\n+              dead.kill();\n+              if (LOG.isInfoEnabled()) {\n+                LOG.info(\"Deleting dead worker \" + k + \" which mapped to \" + dead);\n+              }\n             }\n           }\n+          // oldKeys contains the set of dead instances at this point.\n+          this.instances.keySet().removeAll(oldKeys);\n+          this.instances.putAll(freshInstances);\n+        } else {\n+          this.instances.putAll(freshInstances);\n         }\n-        this.instances.keySet().removeAll(oldKeys);\n-        this.instances.putAll(freshInstances);\n-      } else {\n-        this.instances = freshInstances;\n       }\n     }\n \n     @Override\n-    public Set<ServiceInstance> getByHost(String host) {\n+    public synchronized Set<ServiceInstance> getByHost(String host) {\n+      // TODO Maybe store this as a map which is populated during construction, to avoid walking\n+      // the map on each request.\n       Set<ServiceInstance> byHost = new HashSet<ServiceInstance>();\n+\n       for (ServiceInstance i : instances.values()) {\n         if (host.equals(i.getHost())) {\n           // all hosts in instances should be alive in this impl", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java", "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java", "sha": "d35b04ae743c34601dce154b9cd766cdbf9a1f32", "changes": 9, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249", "patch": "@@ -218,7 +218,6 @@ public void setResponse(SubmitWorkResponseProto response) {\n \n           @Override\n           public void indicateError(Throwable t) {\n-            LOG.info(\"Failed to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" + containerId, t);\n             if (t instanceof ServiceException) {\n               ServiceException se = (ServiceException) t;\n               t = se.getCause();\n@@ -228,21 +227,29 @@ public void indicateError(Throwable t) {\n               String message = re.toString();\n               // RejectedExecutions from the remote service treated as KILLED\n               if (message.contains(RejectedExecutionException.class.getName())) {\n+                LOG.info(\n+                    \"Unable to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" +\n+                        containerId + \", Service Busy\");\n                 getTaskCommunicatorContext().taskKilled(taskSpec.getTaskAttemptID(),\n                     TaskAttemptEndReason.SERVICE_BUSY, \"Service Busy\");\n               } else {\n                 // All others from the remote service cause the task to FAIL.\n+                LOG.info(\"Failed to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" + containerId, t);\n                 getTaskCommunicatorContext()\n                     .taskFailed(taskSpec.getTaskAttemptID(), TaskAttemptEndReason.OTHER,\n                         t.toString());\n               }\n             } else {\n               // Exception from the RPC layer - communication failure, consider as KILLED / service down.\n               if (t instanceof IOException) {\n+                LOG.info(\n+                    \"Unable to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" +\n+                        containerId + \", Communication Error\");\n                 getTaskCommunicatorContext().taskKilled(taskSpec.getTaskAttemptID(),\n                     TaskAttemptEndReason.COMMUNICATION_ERROR, \"Communication Error\");\n               } else {\n                 // Anything else is a FAIL.\n+                LOG.info(\"Failed to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" + containerId, t);\n                 getTaskCommunicatorContext()\n                     .taskFailed(taskSpec.getTaskAttemptID(), TaskAttemptEndReason.OTHER,\n                         t.getMessage());", "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java"}, {"additions": 54, "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java", "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java", "sha": "1ab3d152f2c07e044a3e7c53f3fe144b6e9527ba", "changes": 54, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249", "patch": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ *  you may not use this file except in compliance with the License.\n+ *  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.tez.dag.app.rm;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.tez.dag.app.AppContext;\n+\n+class ContainerFactory {\n+  final ApplicationAttemptId customAppAttemptId;\n+  AtomicLong nextId;\n+\n+  public ContainerFactory(AppContext appContext, long appIdLong) {\n+    this.nextId = new AtomicLong(1);\n+    ApplicationId appId =\n+        ApplicationId.newInstance(appIdLong, appContext.getApplicationAttemptId()\n+            .getApplicationId().getId());\n+    this.customAppAttemptId =\n+        ApplicationAttemptId.newInstance(appId, appContext.getApplicationAttemptId()\n+            .getAttemptId());\n+  }\n+\n+  public Container createContainer(Resource capability, Priority priority, String hostname,\n+      int port) {\n+    ContainerId containerId =\n+        ContainerId.newContainerId(customAppAttemptId, nextId.getAndIncrement());\n+    NodeId nodeId = NodeId.newInstance(hostname, port);\n+    String nodeHttpAddress = \"hostname:0\"; // TODO: include UI ports\n+\n+    Container container =\n+        Container.newInstance(containerId, nodeId, nodeHttpAddress, capability, priority, null);\n+\n+    return container;\n+  }\n+}", "filename": "llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java"}, {"additions": 69, "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java", "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java", "sha": "39ad5521cd02a538f6c458e0065f2b0c2081e2ef", "changes": 135, "status": "modified", "deletions": 66, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249", "patch": "@@ -21,6 +21,7 @@\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.LinkedHashMap;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -38,7 +39,6 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicInteger;\n-import java.util.concurrent.atomic.AtomicLong;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n import org.apache.commons.logging.Log;\n@@ -48,8 +48,6 @@\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstance;\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstanceSet;\n import org.apache.hadoop.hive.llap.daemon.registry.impl.LlapRegistryService;\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeId;\n@@ -78,15 +76,11 @@\n   // interface into the registry service\n   private ServiceInstanceSet activeInstances;\n \n+  // Tracks all instances, including ones which have been disabled in the past.\n+  // LinkedHashMap to provide the same iteration order when selecting a random host.\n   @VisibleForTesting\n-  final Map<ServiceInstance, NodeInfo> instanceToNodeMap = new HashMap<>();\n-  \n-  @VisibleForTesting\n-  final Set<ServiceInstance> instanceBlackList = new HashSet<ServiceInstance>();\n-\n-  @VisibleForTesting\n-  // Tracks currently allocated containers.\n-  final Map<ContainerId, String> containerToInstanceMap = new HashMap<>();\n+  final Map<ServiceInstance, NodeInfo> instanceToNodeMap = new LinkedHashMap<>();\n+  // TODO Ideally, remove elements from this once it's known that no tasks are linked to the instance (all deallocated)\n \n   // Tracks tasks which could not be allocated immediately.\n   @VisibleForTesting\n@@ -100,8 +94,9 @@ public int compare(Priority o1, Priority o2) {\n   // Tracks running and queued tasks. Cleared after a task completes.\n   private final ConcurrentMap<Object, TaskInfo> knownTasks = new ConcurrentHashMap<>();\n \n+  // Queue for disabled nodes. Nodes make it out of this queue when their expiration timeout is hit.\n   @VisibleForTesting\n-  final DelayQueue<NodeInfo> disabledNodes = new DelayQueue<>();\n+  final DelayQueue<NodeInfo> disabledNodesQueue = new DelayQueue<>();\n \n   private final ContainerFactory containerFactory;\n   private final Random random = new Random();\n@@ -263,9 +258,9 @@ public Resource getAvailableResources() {\n     int vcores = 0;\n     readLock.lock();\n     try {\n-      for (ServiceInstance inst : instanceToNodeMap.keySet()) {\n-        if (inst.isAlive()) {\n-          Resource r = inst.getResource();\n+      for (Entry<ServiceInstance, NodeInfo> entry : instanceToNodeMap.entrySet()) {\n+        if (entry.getKey().isAlive() && !entry.getValue().isDisabled()) {\n+          Resource r = entry.getKey().getResource();\n           memory += r.getMemory();\n           vcores += r.getVirtualCores();\n         }\n@@ -375,8 +370,6 @@ public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEnd\n         }\n         return false;\n       }\n-      String hostForContainer = containerToInstanceMap.remove(taskInfo.containerId);\n-      assert hostForContainer != null;\n       ServiceInstance assignedInstance = taskInfo.assignedInstance;\n       assert assignedInstance != null;\n \n@@ -410,6 +403,8 @@ public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEnd\n   @Override\n   public Object deallocateContainer(ContainerId containerId) {\n     LOG.info(\"DEBUG: Ignoring deallocateContainer for containerId: \" + containerId);\n+    // Containers are not being tracked for re-use.\n+    // This is safe to ignore since a deallocate task should have come in earlier.\n     return null;\n   }\n \n@@ -435,7 +430,7 @@ TaskSchedulerAppCallback createAppCallbackDelegate(TaskSchedulerAppCallback real\n   }\n \n   /**\n-   * @param requestedHosts the list of preferred hosts. null implies any host\n+   * @param request the list of preferred hosts. null implies any host\n    * @return\n    */\n   private ServiceInstance selectHost(TaskInfo request) {\n@@ -444,6 +439,9 @@ private ServiceInstance selectHost(TaskInfo request) {\n     try {\n       // Check if any hosts are active.\n       if (getAvailableResources().getMemory() <= 0) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Refreshing instances since total memory is 0\");\n+        }\n         refreshInstances();\n       }\n \n@@ -453,49 +451,65 @@ private ServiceInstance selectHost(TaskInfo request) {\n       }\n \n       if (requestedHosts != null) {\n+        int prefHostCount = -1;\n         for (String host : requestedHosts) {\n+          prefHostCount++;\n           // Pick the first host always. Weak attempt at cache affinity.\n           Set<ServiceInstance> instances = activeInstances.getByHost(host);\n           if (!instances.isEmpty()) {\n             for (ServiceInstance inst : instances) {\n-              if (inst.isAlive() && instanceToNodeMap.containsKey(inst)) {\n-                // only allocate from the \"available\" list\n+              NodeInfo nodeInfo = instanceToNodeMap.get(inst);\n+              if (inst.isAlive() && nodeInfo != null && !nodeInfo.isDisabled()) {\n                 // TODO Change this to work off of what we think is remaining capacity for an\n                 // instance\n-                LOG.info(\"Assigning \" + inst + \" when looking for \" + host);\n+                LOG.info(\n+                    \"Assigning \" + inst + \" when looking for \" + host + \". FirstRequestedHost=\" +\n+                        (prefHostCount == 0));\n                 return inst;\n               }\n             }\n           }\n         }\n       }\n       /* fall through - miss in locality (random scheduling) */\n-      ServiceInstance[] all = instanceToNodeMap.keySet().toArray(new ServiceInstance[0]);\n+      Entry<ServiceInstance, NodeInfo> [] all = instanceToNodeMap.entrySet().toArray(new Entry[instanceToNodeMap.size()]);\n       // Check again\n       if (all.length > 0) {\n         int n = random.nextInt(all.length);\n         // start at random offset and iterate whole list\n         for (int i = 0; i < all.length; i++) {\n-          ServiceInstance inst = all[(i + n) % all.length];\n-          if (inst.isAlive()) {\n-            LOG.info(\"Assigning \" + inst + \" when looking for any host\");\n-            return inst;\n+          Entry<ServiceInstance, NodeInfo> inst = all[(i + n) % all.length];\n+          if (inst.getKey().isAlive() && !inst.getValue().isDisabled()) {\n+            LOG.info(\"Assigning \" + inst + \" when looking for any host, from #hosts=\" + all.length);\n+            return inst.getKey();\n           }\n         }\n       }\n     } finally {\n       readLock.unlock();\n     }\n \n+    // TODO Ideally, each refresh operation should addNodes if they don't already exist.\n+    // Even better would be to get notifications from the service impl when a node gets added or removed.\n+    // Instead of having to walk through the entire list. The computation of a node getting added or\n+    // removed already exists in the DynamicRegistry implementation.\n+\n+\n+    // This will only happen if no allocations are possible, which means all other nodes have\n+    // been blacklisted.\n+    // TODO Look for new nodes more often. See comment above.\n+\n     /* check again whether nodes are disabled or just missing */\n     writeLock.lock();\n     try {\n       for (ServiceInstance inst : activeInstances.getAll().values()) {\n-        if (inst.isAlive() && instanceBlackList.contains(inst) == false\n-            && instanceToNodeMap.containsKey(inst) == false) {\n+        if (inst.isAlive() && instanceToNodeMap.containsKey(inst) == false) {\n           /* that's a good node, not added to the allocations yet */\n+          LOG.info(\"Found a new node: \" + inst + \". Adding to node list and disabling to trigger scheduling\");\n           addNode(inst, new NodeInfo(inst, BACKOFF_FACTOR, clock));\n           // mark it as disabled to let the pending tasks go there\n+          // TODO If disabling the instance, have it wake up immediately instead of waiting.\n+          // Ideally get rid of this requirement, by having all tasks allocated via a queue.\n           disableInstance(inst, true);\n         }\n       }\n@@ -515,19 +529,22 @@ private void refreshInstances() {\n   }\n \n   private void addNode(ServiceInstance inst, NodeInfo node) {\n+    LOG.info(\"Adding node: \" + inst);\n     instanceToNodeMap.put(inst, node);\n+    // TODO Trigger a scheduling run each time a new node is added.\n   }\n \n   private void reenableDisabledNode(NodeInfo nodeInfo) {\n     writeLock.lock();\n     try {\n       if (!nodeInfo.isBusy()) {\n+        // If the node being re-enabled was not marked busy previously, then it was disabled due to\n+        // some other failure. Refresh the service list to see if it's been removed permanently.\n         refreshInstances();\n       }\n+      LOG.info(\"Attempting to re-enable node: \" + nodeInfo.host);\n       if (nodeInfo.host.isAlive()) {\n         nodeInfo.enableNode();\n-        instanceBlackList.remove(nodeInfo.host);\n-        instanceToNodeMap.put(nodeInfo.host, nodeInfo);\n       } else {\n         if (LOG.isInfoEnabled()) {\n           LOG.info(\"Removing dead node \" + nodeInfo);\n@@ -541,19 +558,18 @@ private void reenableDisabledNode(NodeInfo nodeInfo) {\n   private void disableInstance(ServiceInstance instance, boolean busy) {\n     writeLock.lock();\n     try {\n-      NodeInfo nodeInfo = instanceToNodeMap.remove(instance);\n-      if (nodeInfo == null) {\n+      NodeInfo nodeInfo = instanceToNodeMap.get(instance);\n+      if (nodeInfo == null || nodeInfo.isDisabled()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Node: \" + instance + \" already disabled, or invalid. Not doing anything.\");\n         }\n       } else {\n-        instanceBlackList.add(instance);\n         nodeInfo.disableNode(nodeReEnableTimeout);\n         nodeInfo.setBusy(busy); // daemon failure vs daemon busy\n         // TODO: handle task to container map events in case of hard failures\n-        disabledNodes.add(nodeInfo);\n+        disabledNodesQueue.add(nodeInfo);\n         if (LOG.isInfoEnabled()) {\n-          LOG.info(\"Disabling instance \" + instance + \" for \" + nodeReEnableTimeout + \" seconds\");\n+          LOG.info(\"Disabling instance \" + instance + \" for \" + nodeReEnableTimeout + \" milli-seconds\");\n         }\n       }\n     } finally {\n@@ -640,7 +656,6 @@ private boolean scheduleTask(TaskInfo taskInfo) {\n             host.getHost());\n         taskInfo.setAssignmentInfo(host, container.getId());\n         knownTasks.putIfAbsent(taskInfo.task, taskInfo);\n-        containerToInstanceMap.put(container.getId(), host.getWorkerIdentity());\n       } finally {\n         writeLock.unlock();\n       }\n@@ -660,7 +675,7 @@ public Void call() {\n       while (!isShutdown.get() && !Thread.currentThread().isInterrupted()) {\n         try {\n           while (true) {\n-            NodeInfo nodeInfo = disabledNodes.take();\n+            NodeInfo nodeInfo = disabledNodesQueue.take();\n             // A node became available. Enable the node and try scheduling.\n             reenableDisabledNode(nodeInfo);\n             schedulePendingTasks();\n@@ -694,8 +709,12 @@ public void shutdown() {\n     private long numSuccessfulTasks = 0;\n     private long numSuccessfulTasksAtLastBlacklist = -1;\n     float cumulativeBackoffFactor = 1.0f;\n+    // A node could be disabled for reasons other than being busy.\n+    private boolean disabled = false;\n+    // If disabled, the node could be marked as busy.\n     private boolean busy;\n \n+\n     NodeInfo(ServiceInstance host, float backoffFactor, Clock clock) {\n       this.host = host;\n       constBackOffFactor = backoffFactor;\n@@ -704,10 +723,12 @@ public void shutdown() {\n \n     void enableNode() {\n       expireTimeMillis = -1;\n+      disabled = false;\n     }\n \n     void disableNode(long duration) {\n       long currentTime = clock.getTime();\n+      disabled = true;\n       if (numSuccessfulTasksAtLastBlacklist == numSuccessfulTasks) {\n         // Blacklisted again, without any progress. Will never kick in for the first run.\n         cumulativeBackoffFactor = cumulativeBackoffFactor * constBackOffFactor;\n@@ -721,7 +742,12 @@ void disableNode(long duration) {\n     }\n \n     void registerTaskSuccess() {\n-      this.busy = false; // if a task exited, we might have free slots\n+      // TODO If a task succeeds, we may have free slots. Mark the node as !busy. Ideally take it out\n+      // of the queue for more allocations.\n+      // For now, not chanigng the busy status,\n+\n+      // this.busy = false;\n+      // this.disabled = false;\n       numSuccessfulTasks++;\n     }\n \n@@ -733,9 +759,13 @@ public boolean isBusy() {\n       return busy;\n     }\n \n+    public boolean isDisabled() {\n+      return disabled;\n+    }\n+\n     @Override\n     public long getDelay(TimeUnit unit) {\n-      return expireTimeMillis - clock.getTime();\n+      return unit.convert(expireTimeMillis - clock.getTime(), TimeUnit.MILLISECONDS);\n     }\n \n     @Override\n@@ -869,31 +899,4 @@ void setAssignmentInfo(ServiceInstance instance, ContainerId containerId) {\n     }\n   }\n \n-  static class ContainerFactory {\n-    final ApplicationAttemptId customAppAttemptId;\n-    AtomicLong nextId;\n-\n-    public ContainerFactory(AppContext appContext, long appIdLong) {\n-      this.nextId = new AtomicLong(1);\n-      ApplicationId appId =\n-          ApplicationId.newInstance(appIdLong, appContext.getApplicationAttemptId()\n-              .getApplicationId().getId());\n-      this.customAppAttemptId =\n-          ApplicationAttemptId.newInstance(appId, appContext.getApplicationAttemptId()\n-              .getAttemptId());\n-    }\n-\n-    public Container createContainer(Resource capability, Priority priority, String hostname,\n-        int port) {\n-      ContainerId containerId =\n-          ContainerId.newContainerId(customAppAttemptId, nextId.getAndIncrement());\n-      NodeId nodeId = NodeId.newInstance(hostname, port);\n-      String nodeHttpAddress = \"hostname:0\"; // TODO: include UI ports\n-\n-      Container container =\n-          Container.newInstance(containerId, nodeId, nodeHttpAddress, capability, priority, null);\n-\n-      return container;\n-    }\n-  }\n }", "filename": "llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java", "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java", "sha": "b116af0357c459ae27b2a3bfe91297e0ed543a36", "changes": 8, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249", "patch": "@@ -22,9 +22,7 @@\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.eq;\n import static org.mockito.Mockito.doReturn;\n-import static org.mockito.Mockito.doAnswer;\n import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.never;\n import static org.mockito.Mockito.reset;\n import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n@@ -38,15 +36,13 @@\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n-import org.apache.hadoop.yarn.util.Clock;\n import org.apache.hadoop.yarn.util.SystemClock;\n import org.apache.tez.dag.api.TaskAttemptEndReason;\n import org.apache.tez.dag.app.AppContext;\n import org.apache.tez.dag.app.ControlledClock;\n import org.apache.tez.dag.app.rm.TaskSchedulerService.TaskSchedulerAppCallback;\n import org.junit.Test;\n import org.mockito.ArgumentCaptor;\n-import org.mortbay.log.Log;\n \n public class TestLlapTaskSchedulerService {\n \n@@ -112,7 +108,7 @@ public void testNodeDisabled() {\n       // Verify that the node is blacklisted\n       assertEquals(1, tsWrapper.ts.dagStats.numRejectedTasks);\n       assertEquals(2, tsWrapper.ts.instanceToNodeMap.size());\n-      LlapTaskSchedulerService.NodeInfo disabledNodeInfo = tsWrapper.ts.disabledNodes.peek();\n+      LlapTaskSchedulerService.NodeInfo disabledNodeInfo = tsWrapper.ts.disabledNodesQueue.peek();\n       assertNotNull(disabledNodeInfo);\n       assertEquals(HOST1, disabledNodeInfo.host.getHost());\n       assertEquals((10000l), disabledNodeInfo.getDelay(TimeUnit.NANOSECONDS));\n@@ -164,7 +160,7 @@ public void testNodeReEnabled() throws InterruptedException {\n       // Verify that the node is blacklisted\n       assertEquals(3, tsWrapper.ts.dagStats.numRejectedTasks);\n       assertEquals(0, tsWrapper.ts.instanceToNodeMap.size());\n-      assertEquals(3, tsWrapper.ts.disabledNodes.size());\n+      assertEquals(3, tsWrapper.ts.disabledNodesQueue.size());\n \n \n       Object task4 = new Object();", "filename": "llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "parent": "https://github.com/apache/hive/commit/b7ed17fe74d8f2572b7b359d1723d5de6a5ac425", "message": "HIVE-10273: Union with partition tables which have no data fails with NPE (Vikram Dixit, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1673937 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_169", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/itests/src/test/resources/testconfiguration.properties", "sha": "0a5d839489e3ea79b2d69156c60a23371aaf2cce", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -313,6 +313,7 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_schema_evolution.q,\\\n   tez_union.q,\\\n   tez_union2.q,\\\n+  tez_union_view.q,\\\n   tez_union_decimal.q,\\\n   tez_union_group_by.q,\\\n   tez_smb_main.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java", "sha": "319aacbafedeeaedaeefe9db7c9f38867530bfc1", "changes": 11, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -365,6 +365,17 @@ private boolean validateMapWork(MapWork mapWork, boolean isTez) throws SemanticE\n       addMapWorkRules(opRules, vnp);\n       Dispatcher disp = new DefaultRuleDispatcher(vnp, opRules, null);\n       GraphWalker ogw = new DefaultGraphWalker(disp);\n+      if ((mapWork.getAliasToWork() == null) || (mapWork.getAliasToWork().size() == 0)) {\n+        return false;\n+      } else {\n+        for (Operator<?> op : mapWork.getAliasToWork().values()) {\n+          if (op == null) {\n+            LOG.warn(\"Map work has invalid aliases to work with. Fail validation!\");\n+            return false;\n+          }\n+        }\n+      }\n+\n       // iterator the mapper operator tree\n       ArrayList<Node> topNodes = new ArrayList<Node>();\n       topNodes.addAll(mapWork.getAliasToWork().values());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java", "sha": "3217df27bb5731a1dcd5db1ae17c5bdff2e3fbfc", "changes": 12, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -334,16 +334,8 @@ public void replaceRoots(Map<Operator<?>, Operator<?>> replacementMap) {\n   public Set<Operator<?>> getAllRootOperators() {\n     Set<Operator<?>> opSet = new LinkedHashSet<Operator<?>>();\n \n-    Map<String, ArrayList<String>> pa = getPathToAliases();\n-    if (pa != null) {\n-      for (List<String> ls : pa.values()) {\n-        for (String a : ls) {\n-          Operator<?> op = getAliasToWork().get(a);\n-          if (op != null ) {\n-            opSet.add(op);\n-          }\n-        }\n-      }\n+    for (Operator<?> op : getAliasToWork().values()) {\n+      opSet.add(op);\n     }\n     return opSet;\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/queries/clientpositive/tez_union.q", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/queries/clientpositive/tez_union.q", "sha": "96f58b21529a46b25f0dd06dba7d61fb6aaf0d7f", "changes": 18, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union.q?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -92,3 +92,21 @@ right outer join src s on u.key = s.key;\n \n select * from ut order by ukey, skey limit 20;\n drop table ut;\n+\n+set hive.vectorized.execution.enabled=true;\n+\n+create table TABLE1(EMP_NAME STRING, EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n+\n+create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n+\n+CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1;\n+\n+explain formatted select count(*) from TABLE3;\n+\n+drop table table2;\n+\n+create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n+\n+CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1 UNION ALL select EMP_NAME,EMP_ID from TABLE2;\n+\n+explain formatted select count(*) from TABLE3;", "filename": "ql/src/test/queries/clientpositive/tez_union.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientnegative/join_nonexistent_part.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientnegative/join_nonexistent_part.q.out", "sha": "391dd0592611d7af8484c52efde3a50fb7dfa44d", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/join_nonexistent_part.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -1,2 +1,2 @@\n-Warning: Shuffle Join JOIN[8][tables = [$hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product\n+Warning: Shuffle Join JOIN[8][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n Authorization failed:No privilege 'Select' found for inputs { database:default, table:srcpart, columnName:key}. Use SHOW GRANT to get more details.", "filename": "ql/src/test/results/clientnegative/join_nonexistent_part.q.out"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/annotate_stats_join.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/annotate_stats_join.q.out", "sha": "66e944b83665fb9c1272907e5e4f404ade46fcea", "changes": 32, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/annotate_stats_join.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -439,22 +439,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: e\n-            Statistics: Num rows: 48 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n-            Filter Operator\n-              predicate: deptid is not null (type: boolean)\n-              Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n-              Select Operator\n-                expressions: lastname (type: string), deptid (type: int), locid (type: int)\n-                outputColumnNames: _col0, _col1, _col2\n-                Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n-                Reduce Output Operator\n-                  key expressions: _col1 (type: int)\n-                  sort order: +\n-                  Map-reduce partition columns: _col1 (type: int)\n-                  Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n-                  value expressions: _col0 (type: string), _col2 (type: int)\n           TableScan\n             alias: e\n             Statistics: Num rows: 48 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n@@ -487,6 +471,22 @@ STAGE PLANS:\n                   Map-reduce partition columns: _col0 (type: int)\n                   Statistics: Num rows: 6 Data size: 570 Basic stats: COMPLETE Column stats: COMPLETE\n                   value expressions: _col1 (type: string)\n+          TableScan\n+            alias: e\n+            Statistics: Num rows: 48 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n+            Filter Operator\n+              predicate: deptid is not null (type: boolean)\n+              Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n+              Select Operator\n+                expressions: lastname (type: string), deptid (type: int), locid (type: int)\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  key expressions: _col1 (type: int)\n+                  sort order: +\n+                  Map-reduce partition columns: _col1 (type: int)\n+                  Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col0 (type: string), _col2 (type: int)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:", "filename": "ql/src/test/results/clientpositive/annotate_stats_join.q.out"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/auto_join32.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/auto_join32.q.out", "sha": "bfc8be8599d73dd3a809e44fe515f8dd65a22557", "changes": 30, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_join32.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -391,6 +391,36 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: v\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((p = 'bar') and name is not null) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: name (type: string), registration (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Sorted Merge Bucket Map Join Operator\n+                  condition map:\n+                       Inner Join 0 to 1\n+                  keys:\n+                    0 _col0 (type: string)\n+                    1 _col0 (type: string)\n+                  outputColumnNames: _col1, _col3\n+                  Select Operator\n+                    expressions: _col3 (type: string), _col1 (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Group By Operator\n+                      aggregations: count(DISTINCT _col1)\n+                      keys: _col0 (type: string), _col1 (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Map-reduce partition columns: _col0 (type: string)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(DISTINCT KEY._col1:0._col0)", "filename": "ql/src/test/results/clientpositive/auto_join32.q.out"}, {"additions": 88, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/bucketmapjoin1.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/bucketmapjoin1.q.out", "sha": "72f2a07524b33317726da843d980e51a23132b98", "changes": 88, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -125,6 +125,50 @@ STAGE PLANS:\n \n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col1, _col7\n+                Position of Big Table: 0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                BucketMapJoin: true\n+                Select Operator\n+                  expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2\n+                          columns.types int:string:string\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n       Local Work:\n         Map Reduce Local Work\n \n@@ -249,6 +293,50 @@ STAGE PLANS:\n \n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: b\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (key is not null and (ds = '2008-04-08')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col1, _col7\n+                Position of Big Table: 1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                BucketMapJoin: true\n+                Select Operator\n+                  expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2\n+                          columns.types int:string:string\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n       Local Work:\n         Map Reduce Local Work\n ", "filename": "ql/src/test/results/clientpositive/bucketmapjoin1.q.out"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer3.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer3.q.out", "sha": "5389647fb276a3949c93ea0f20d64eb6733a91c7", "changes": 60, "status": "modified", "deletions": 30, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/correlationoptimizer3.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -284,21 +284,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: y\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: key is not null (type: boolean)\n-              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-              Select Operator\n-                expressions: key (type: string)\n-                outputColumnNames: _col0\n-                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                Reduce Output Operator\n-                  key expressions: _col0 (type: string)\n-                  sort order: +\n-                  Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: y\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -330,6 +315,21 @@ STAGE PLANS:\n                   Map-reduce partition columns: _col0 (type: string)\n                   Statistics: Num rows: 13 Data size: 99 Basic stats: COMPLETE Column stats: NONE\n                   value expressions: _col1 (type: string)\n+          TableScan\n+            alias: y\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: x\n             Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE\n@@ -988,21 +988,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: y\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: key is not null (type: boolean)\n-              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-              Select Operator\n-                expressions: key (type: string)\n-                outputColumnNames: _col0\n-                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                Reduce Output Operator\n-                  key expressions: _col0 (type: string)\n-                  sort order: +\n-                  Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: y\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -1034,6 +1019,21 @@ STAGE PLANS:\n                   Map-reduce partition columns: _col0 (type: string)\n                   Statistics: Num rows: 13 Data size: 99 Basic stats: COMPLETE Column stats: NONE\n                   value expressions: _col1 (type: string)\n+          TableScan\n+            alias: y\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: x\n             Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE", "filename": "ql/src/test/results/clientpositive/correlationoptimizer3.q.out"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer6.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer6.q.out", "sha": "be518dc6684e52c2b0d388d472b89f5c85cf268f", "changes": 32, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/correlationoptimizer6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -3031,22 +3031,6 @@ STAGE PLANS:\n                     Map-reduce partition columns: _col0 (type: string)\n                     Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n                     value expressions: _col1 (type: bigint)\n-          TableScan\n-            alias: x\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: key is not null (type: boolean)\n-              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-              Select Operator\n-                expressions: key (type: string), value (type: string)\n-                outputColumnNames: _col0, _col1\n-                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                Reduce Output Operator\n-                  key expressions: _col0 (type: string)\n-                  sort order: +\n-                  Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                  value expressions: _col1 (type: string)\n           TableScan\n             alias: y\n             Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE\n@@ -3069,6 +3053,22 @@ STAGE PLANS:\n                     Map-reduce partition columns: _col0 (type: string)\n                     Statistics: Num rows: 13 Data size: 99 Basic stats: COMPLETE Column stats: NONE\n                     value expressions: _col1 (type: bigint)\n+          TableScan\n+            alias: x\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col1 (type: string)\n       Reduce Operator Tree:\n         Demux Operator\n           Statistics: Num rows: 513 Data size: 5411 Basic stats: COMPLETE Column stats: NONE", "filename": "ql/src/test/results/clientpositive/correlationoptimizer6.q.out"}, {"additions": 54, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/groupby_sort_6.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/groupby_sort_6.q.out", "sha": "c5cb8b9908cc25710ec57f21e5849e2069dedcda", "changes": 54, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/groupby_sort_6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -66,6 +66,33 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: t1\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Group By Operator\n+                  aggregations: count(1)\n+                  keys: _col0 (type: string)\n+                  mode: hash\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    tag: -1\n+                    value expressions: _col1 (type: bigint)\n+                    auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator\n@@ -208,6 +235,33 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: t1\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Group By Operator\n+                  aggregations: count(1)\n+                  keys: _col0 (type: string)\n+                  mode: hash\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    tag: -1\n+                    value expressions: _col1 (type: bigint)\n+                    auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator", "filename": "ql/src/test/results/clientpositive/groupby_sort_6.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input23.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input23.q.out", "sha": "73038c34cb24804bc1f3338e8864663f701bcac7", "changes": 22, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/input23.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -1,4 +1,4 @@\n-Warning: Shuffle Join JOIN[9][tables = [$hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product\n+Warning: Shuffle Join JOIN[9][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: explain extended\n  select * from srcpart a join srcpart b where a.ds = '2008-04-08' and a.hr = '11' and b.ds = '2008-04-08' and b.hr = '14' limit 5\n PREHOOK: type: QUERY\n@@ -79,6 +79,24 @@ STAGE PLANS:\n                 tag: 0\n                 value expressions: _col0 (type: string), _col1 (type: string)\n                 auto parallelism: false\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: ((ds = '2008-04-08') and (hr = '14')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: 1\n+                  value expressions: _col0 (type: string), _col1 (type: string)\n+                  auto parallelism: false\n       Path -> Alias:\n #### A masked pattern was here ####\n       Path -> Partition:\n@@ -175,7 +193,7 @@ STAGE PLANS:\n       Processor Tree:\n         ListSink\n \n-Warning: Shuffle Join JOIN[9][tables = [$hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product\n+Warning: Shuffle Join JOIN[9][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: select * from srcpart a join srcpart b where a.ds = '2008-04-08' and a.hr = '11' and b.ds = '2008-04-08' and b.hr = '14' limit 5\n PREHOOK: type: QUERY\n PREHOOK: Input: default@srcpart", "filename": "ql/src/test/results/clientpositive/input23.q.out"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input26.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input26.q.out", "sha": "1b24aa672a3c37bd57dad18abeed827dcc244200", "changes": 18, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/input26.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -87,6 +87,24 @@ STAGE PLANS:\n \n   Stage: Stage-3\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((ds = '2008-04-08') and (hr = '14')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Limit\n+                  Number of rows: 5\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    sort order: \n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    value expressions: _col0 (type: string), _col1 (type: string)\n       Reduce Operator Tree:\n         Select Operator\n           expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)", "filename": "ql/src/test/results/clientpositive/input26.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out", "sha": "678ddb8c0a938832fcaaaace1218e9db8f1e0247", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -80,18 +80,6 @@ STAGE PLANS:\n                 Map-reduce partition columns: p_name (type: string)\n                 Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n                 value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n-          TableScan\n-            alias: p4\n-            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: p_name is not null (type: boolean)\n-              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-              Reduce Output Operator\n-                key expressions: p_name (type: string)\n-                sort order: +\n-                Map-reduce partition columns: p_name (type: string)\n-                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n           TableScan\n             alias: p2\n             Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n@@ -116,6 +104,18 @@ STAGE PLANS:\n                 Map-reduce partition columns: p3_name (type: string)\n                 Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                 value expressions: p3_partkey (type: int), p3_mfgr (type: string), p3_brand (type: string), p3_type (type: string), p3_size (type: int), p3_container (type: string), p3_retailprice (type: double), p3_comment (type: string)\n+          TableScan\n+            alias: p4\n+            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: p_name is not null (type: boolean)\n+              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: p_name (type: string)\n+                sort order: +\n+                Map-reduce partition columns: p_name (type: string)\n+                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:", "filename": "ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out", "sha": "4668eb1d59e1b705c9562d857c3731bbb3fa01b5", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -82,18 +82,6 @@ STAGE PLANS:\n                 Map-reduce partition columns: p_name (type: string)\n                 Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n                 value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n-          TableScan\n-            alias: p4\n-            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: p_name is not null (type: boolean)\n-              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-              Reduce Output Operator\n-                key expressions: p_name (type: string)\n-                sort order: +\n-                Map-reduce partition columns: p_name (type: string)\n-                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n           TableScan\n             alias: p2\n             Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n@@ -118,6 +106,18 @@ STAGE PLANS:\n                 Map-reduce partition columns: p3_name (type: string)\n                 Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                 value expressions: p3_partkey (type: int), p3_mfgr (type: string), p3_brand (type: string), p3_type (type: string), p3_size (type: int), p3_container (type: string), p3_retailprice (type: double), p3_comment (type: string)\n+          TableScan\n+            alias: p4\n+            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: p_name is not null (type: boolean)\n+              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: p_name (type: string)\n+                sort order: +\n+                Map-reduce partition columns: p_name (type: string)\n+                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:", "filename": "ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out"}, {"additions": 25, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_view.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_view.q.out", "sha": "e703e0b33b5961221b32081b0148dc21ad40a1da", "changes": 25, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/join_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -49,6 +49,31 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: invites\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2011-09-01') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: '2011-09-01' (type: string)\n+                sort order: +\n+                Map-reduce partition columns: '2011-09-01' (type: string)\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                value expressions: bar (type: string)\n+          TableScan\n+            alias: invites2\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2011-09-01') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: '2011-09-01' (type: string)\n+                sort order: +\n+                Map-reduce partition columns: '2011-09-01' (type: string)\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                value expressions: foo (type: int)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:", "filename": "ql/src/test/results/clientpositive/join_view.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/metadataonly1.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/metadataonly1.q.out", "sha": "e55efd599a93dee4bf91900d542cdbaa08ae57a0", "changes": 20, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/metadataonly1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -36,6 +36,26 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: test1\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Select Operator\n+              expressions: ds (type: string)\n+              outputColumnNames: _col0\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: max(_col0)\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                  tag: -1\n+                  value expressions: _col0 (type: string)\n+                  auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator", "filename": "ql/src/test/results/clientpositive/metadataonly1.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/nullgroup5.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/nullgroup5.q.out", "sha": "8a94d62f2be4ff0f594c14f5e8342fc8b83c66d9", "changes": 19, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/nullgroup5.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -56,6 +56,25 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n+          TableScan\n+            alias: x\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2009-04-05') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           TableScan\n             alias: y\n             Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE", "filename": "ql/src/test/results/clientpositive/nullgroup5.q.out"}, {"additions": 80, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/optimize_nullscan.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/optimize_nullscan.q.out", "sha": "1f4becf14aa10158a88db1e2c95b76ec068115bd", "changes": 122, "status": "modified", "deletions": 42, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/optimize_nullscan.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -176,6 +176,29 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: count(key)\n+                keys: key (type: string)\n+                mode: hash\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: -1\n+                  value expressions: _col1 (type: bigint)\n+                  auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator\n@@ -534,15 +557,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            GatherStats: false\n-            Reduce Output Operator\n-              key expressions: _col0 (type: string)\n-              sort order: +\n-              Map-reduce partition columns: _col0 (type: string)\n-              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-              tag: 1\n-              auto parallelism: false\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -562,6 +576,15 @@ STAGE PLANS:\n                   Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                   tag: 0\n                   auto parallelism: false\n+          TableScan\n+            GatherStats: false\n+            Reduce Output Operator\n+              key expressions: _col0 (type: string)\n+              sort order: +\n+              Map-reduce partition columns: _col0 (type: string)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              tag: 1\n+              auto parallelism: false\n       Path -> Alias:\n         -mr-10003default.src{} [a:src]\n #### A masked pattern was here ####\n@@ -1510,14 +1533,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            GatherStats: false\n-            Reduce Output Operator\n-              sort order: \n-              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-              tag: 1\n-              value expressions: _col0 (type: string)\n-              auto parallelism: false\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -1536,6 +1551,14 @@ STAGE PLANS:\n                   tag: 0\n                   value expressions: _col0 (type: string)\n                   auto parallelism: false\n+          TableScan\n+            GatherStats: false\n+            Reduce Output Operator\n+              sort order: \n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              tag: 1\n+              value expressions: _col0 (type: string)\n+              auto parallelism: false\n       Path -> Alias:\n         -mr-10003default.src{} [a:src]\n #### A masked pattern was here ####\n@@ -1752,6 +1775,21 @@ STAGE PLANS:\n                 Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                 tag: 0\n                 auto parallelism: false\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: key (type: string)\n+                sort order: +\n+                Map-reduce partition columns: key (type: string)\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                tag: 1\n+                auto parallelism: false\n       Path -> Alias:\n         -mr-10004default.src{} [null-subquery2:a-subquery2:src]\n       Path -> Partition:\n@@ -1831,31 +1869,6 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            GatherStats: false\n-            Union\n-              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-              File Output Operator\n-                compressed: false\n-                GlobalTableId: 0\n-#### A masked pattern was here ####\n-                NumFilesPerFileSink: 1\n-                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-#### A masked pattern was here ####\n-                table:\n-                    input format: org.apache.hadoop.mapred.TextInputFormat\n-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                    properties:\n-                      columns _col0\n-                      columns.types string\n-                      escape.delim \\\n-                      hive.serialization.extend.additional.nesting.levels true\n-                      serialization.format 1\n-                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                TotalFiles: 1\n-                GatherStats: false\n-                MultiFileSpray: false\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -1891,6 +1904,31 @@ STAGE PLANS:\n                     TotalFiles: 1\n                     GatherStats: false\n                     MultiFileSpray: false\n+          TableScan\n+            GatherStats: false\n+            Union\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              File Output Operator\n+                compressed: false\n+                GlobalTableId: 0\n+#### A masked pattern was here ####\n+                NumFilesPerFileSink: 1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                table:\n+                    input format: org.apache.hadoop.mapred.TextInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                    properties:\n+                      columns _col0\n+                      columns.types string\n+                      escape.delim \\\n+                      hive.serialization.extend.additional.nesting.levels true\n+                      serialization.format 1\n+                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                TotalFiles: 1\n+                GatherStats: false\n+                MultiFileSpray: false\n       Path -> Alias:\n         -mr-10003default.src{} [null-subquery1:a-subquery1:src]\n #### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/optimize_nullscan.q.out"}, {"additions": 32, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/partition_boolexpr.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/partition_boolexpr.q.out", "sha": "cfd03e23ce354fdb8dde6f64b67af02e6025fa09", "changes": 32, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/partition_boolexpr.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -84,6 +84,22 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: count(1)\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: bigint)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(VALUE._col0)\n@@ -253,6 +269,22 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: count(1)\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: bigint)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(VALUE._col0)", "filename": "ql/src/test/results/clientpositive/partition_boolexpr.q.out"}, {"additions": 80, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/ppd_union_view.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/ppd_union_view.q.out", "sha": "a13ef7ae6e9412ad17d02ac43e202b8e9a63bd36", "changes": 80, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/ppd_union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -331,6 +331,45 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Map Operator Tree:\n+          TableScan\n+            alias: t1_new\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '2011-10-13') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 1 Data size: 15 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string), _col1 (type: string), '2011-10-13' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 1 Data size: 15 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      GlobalTableId: 0\n+#### A masked pattern was here ####\n+                      NumFilesPerFileSink: 1\n+                      Statistics: Num rows: 1 Data size: 15 Basic stats: COMPLETE Column stats: NONE\n+#### A masked pattern was here ####\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          properties:\n+                            columns _col0,_col1,_col2\n+                            columns.types string:string:string\n+                            escape.delim \\\n+                            hive.serialization.extend.additional.nesting.levels true\n+                            serialization.format 1\n+                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      TotalFiles: 1\n+                      GatherStats: false\n+                      MultiFileSpray: false\n           TableScan\n             GatherStats: false\n             Union\n@@ -465,6 +504,47 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-3\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: t1_old\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: ((ds = '2011-10-15') and keymap is not null) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: keymap (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: 0\n+                  value expressions: _col1 (type: string)\n+                  auto parallelism: false\n+          TableScan\n+            alias: t1_mapping\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (('2011-10-15' = ds) and keymap is not null) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), keymap (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col1 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col1 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: 1\n+                  value expressions: _col0 (type: string)\n+                  auto parallelism: false\n       Needs Tagging: true\n       Reduce Operator Tree:\n         Join Operator", "filename": "ql/src/test/results/clientpositive/ppd_union_view.q.out"}, {"additions": 34, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/reduce_deduplicate.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/reduce_deduplicate.q.out", "sha": "fa714b8a9c09761ce55f512a76e149563f0dfe6c", "changes": 34, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/reduce_deduplicate.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -382,6 +382,40 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: complex_tbl_2\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '2010-03-29') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: aet (type: string), aes (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Transform Operator\n+                  command: cat\n+                  output info:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      properties:\n+                        columns _col0,_col1,_col2,_col3,_col4,_col5,_col6\n+                        columns.types string,string,int,string,bigint,string,string\n+                        field.delim 9\n+                        serialization.format 9\n+                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col1 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col1 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    tag: -1\n+                    value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int), _col3 (type: string), _col4 (type: bigint), _col5 (type: string), _col6 (type: string)\n+                    auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Select Operator", "filename": "ql/src/test/results/clientpositive/reduce_deduplicate.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/sample6.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/sample6.q.out", "sha": "9891cbbeb963be1b3d7a30036464481b138fd155", "changes": 20, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/sample6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -3195,6 +3195,26 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: true\n+              predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)\n+              sampleDesc: BUCKET 1 OUT OF 2\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: int), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: int), _col1 (type: string)\n+                  sort order: ++\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: -1\n+                  auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Select Operator", "filename": "ql/src/test/results/clientpositive/sample6.q.out"}, {"additions": 80, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/smb_mapjoin9.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/smb_mapjoin9.q.out", "sha": "9530be11299c683a19bcce90895adbf8e088370c", "changes": 80, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin9.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -112,6 +112,46 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (key is not null and (ds = '2010-10-15')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col6, _col7\n+                Position of Big Table: 0\n+                Select Operator\n+                  expressions: _col6 (type: int), _col7 (type: string), '2010-10-15' (type: string), _col0 (type: int)\n+                  outputColumnNames: _col0, _col1, _col2, _col3\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2,_col3\n+                          columns.types int:string:string:int\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -231,6 +271,46 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: b\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (key is not null and (ds = '2010-10-15')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col6, _col7\n+                Position of Big Table: 1\n+                Select Operator\n+                  expressions: _col6 (type: int), _col7 (type: string), '2010-10-15' (type: string), _col0 (type: int)\n+                  outputColumnNames: _col0, _col1, _col2, _col3\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2,_col3\n+                          columns.types int:string:string:int\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n \n   Stage: Stage-0\n     Fetch Operator", "filename": "ql/src/test/results/clientpositive/smb_mapjoin9.q.out"}, {"additions": 26, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/auto_join32.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/auto_join32.q.out", "sha": "c537b950d570be6dab5df5aedcb077a6a3451dbf", "changes": 26, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/auto_join32.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -426,6 +426,32 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (name is not null and (p = 'bar')) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Sorted Merge Bucket Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 name (type: string)\n+                        1 name (type: string)\n+                      outputColumnNames: _col0, _col9\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Group By Operator\n+                        aggregations: count(DISTINCT _col9)\n+                        keys: _col0 (type: string), _col9 (type: string)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: string), _col1 (type: string)\n+                          sort order: ++\n+                          Map-reduce partition columns: _col0 (type: string)\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator", "filename": "ql/src/test/results/clientpositive/spark/auto_join32.q.out"}, {"additions": 120, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out", "sha": "44f4d0c045b9623a20964117b9f64214e9cc9231", "changes": 120, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -101,6 +101,20 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: b\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: (key is not null and (ds = '2008-04-08')) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Spark HashTable Sink Operator\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      Position of Big Table: 0\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:\n@@ -112,6 +126,52 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: a\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: key is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      outputColumnNames: _col0, _col1, _col7\n+                      input vertices:\n+                        1 Map 2\n+                      Position of Big Table: 0\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      BucketMapJoin: true\n+                      Select Operator\n+                        expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        File Output Operator\n+                          compressed: false\n+                          GlobalTableId: 0\n+#### A masked pattern was here ####\n+                          NumFilesPerFileSink: 1\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              properties:\n+                                columns _col0,_col1,_col2\n+                                columns.types int:string:string\n+                                escape.delim \\\n+                                hive.serialization.extend.additional.nesting.levels true\n+                                serialization.format 1\n+                                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                          TotalFiles: 1\n+                          GatherStats: false\n+                          MultiFileSpray: false\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:\n@@ -215,6 +275,20 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: a\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: key is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Spark HashTable Sink Operator\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      Position of Big Table: 1\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:\n@@ -226,6 +300,52 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: b\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: (key is not null and (ds = '2008-04-08')) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      outputColumnNames: _col0, _col1, _col7\n+                      input vertices:\n+                        0 Map 1\n+                      Position of Big Table: 1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      BucketMapJoin: true\n+                      Select Operator\n+                        expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        File Output Operator\n+                          compressed: false\n+                          GlobalTableId: 0\n+#### A masked pattern was here ####\n+                          NumFilesPerFileSink: 1\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              properties:\n+                                columns _col0,_col1,_col2\n+                                columns.types int:string:string\n+                                escape.delim \\\n+                                hive.serialization.extend.additional.nesting.levels true\n+                                serialization.format 1\n+                                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                          TotalFiles: 1\n+                          GatherStats: false\n+                          MultiFileSpray: false\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:", "filename": "ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out"}, {"additions": 26, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/join_view.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/join_view.q.out", "sha": "f6e0542390889645edfb1845dc9983e30f23480f", "changes": 26, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/join_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -54,7 +54,33 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: invites\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (ds = '2011-09-01') (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: '2011-09-01' (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: '2011-09-01' (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      value expressions: bar (type: string)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: invites2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (ds = '2011-09-01') (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: '2011-09-01' (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: '2011-09-01' (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      value expressions: foo (type: int)\n         Reducer 2 \n             Reduce Operator Tree:\n               Join Operator", "filename": "ql/src/test/results/clientpositive/spark/join_view.q.out"}, {"additions": 39, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out", "sha": "843570e4db1b89afbd3d1684e6ee50c7d9723bc4", "changes": 39, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -107,6 +107,29 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(key)\n+                      keys: key (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        tag: -1\n+                        value expressions: _col1 (type: bigint)\n+                        auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:\n@@ -1654,6 +1677,22 @@ STAGE PLANS:\n             Truncated Path -> Alias:\n               -mr-10003default.src{} [src]\n         Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: key (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: key (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      tag: 1\n+                      auto parallelism: false\n         Reducer 3 \n             Needs Tagging: true\n             Reduce Operator Tree:", "filename": "ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out"}, {"additions": 34, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out", "sha": "8d3f56b2fb379acd1d887a77d2abdd778efc7cad", "changes": 34, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -393,6 +393,40 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: complex_tbl_2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: (ds = '2010-03-29') (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: aet (type: string), aes (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Transform Operator\n+                        command: cat\n+                        output info:\n+                            input format: org.apache.hadoop.mapred.TextInputFormat\n+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                            properties:\n+                              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6\n+                              columns.types string,string,int,string,bigint,string,string\n+                              field.delim 9\n+                              serialization.format 9\n+                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        Reduce Output Operator\n+                          key expressions: _col1 (type: string)\n+                          sort order: +\n+                          Map-reduce partition columns: _col1 (type: string)\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                          tag: -1\n+                          value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int), _col3 (type: string), _col4 (type: bigint), _col5 (type: string), _col6 (type: string)\n+                          auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:", "filename": "ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/sample6.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/sample6.q.out", "sha": "4117732faa988a5c62647124218a9fe5796410bf", "changes": 20, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/sample6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -3081,6 +3081,26 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: true\n+                    predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)\n+                    sampleDesc: BUCKET 1 OUT OF 2\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: int), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: int), _col1 (type: string)\n+                        sort order: ++\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        tag: -1\n+                        auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:", "filename": "ql/src/test/results/clientpositive/spark/sample6.q.out"}, {"additions": 264, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/union_view.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/union_view.q.out", "sha": "2ca9e1332764e8b826c815aa3b5ce234cb826c29", "changes": 264, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -290,7 +290,43 @@ STAGE PLANS:\n                               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -308,6 +344,24 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n             Map Operator Tree:\n                 TableScan\n@@ -328,6 +382,24 @@ STAGE PLANS:\n                               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -345,7 +417,43 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n             Map Operator Tree:\n                 TableScan\n@@ -490,7 +598,37 @@ STAGE PLANS:\n                           sort order: \n                           value expressions: _col0 (type: bigint)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -524,6 +662,21 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 3 \n             Map Operator Tree:\n                 TableScan\n@@ -539,6 +692,21 @@ STAGE PLANS:\n                           sort order: \n                           value expressions: _col0 (type: bigint)\n         Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -572,7 +740,37 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 4 \n             Map Operator Tree:\n                 TableScan\n@@ -621,7 +819,43 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n             Map Operator Tree:\n                 TableScan\n@@ -661,7 +895,37 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 4 \n             Map Operator Tree:\n                 TableScan", "filename": "ql/src/test/results/clientpositive/spark/union_view.q.out"}, {"additions": 35, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out", "sha": "9a04fa2d2132e025fa7c58389ae2a3ef82d337fb", "changes": 35, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -2588,6 +2588,19 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: ds (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: ds (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 5 \n             Map Operator Tree:\n                 TableScan\n@@ -4595,6 +4608,28 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 ds (type: string)\n+                        1 ds (type: string)\n+                      input vertices:\n+                        1 Map 2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: '13' (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: '13' (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 2 \n             Map Operator Tree:\n                 TableScan", "filename": "ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/metadataonly1.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/metadataonly1.q.out", "sha": "28503140784d331a82502dde2318a60229eca3a9", "changes": 20, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/metadataonly1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -41,6 +41,26 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: test1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Select Operator\n+                    expressions: ds (type: string)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: max(_col0)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                        tag: -1\n+                        value expressions: _col0 (type: string)\n+                        auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:", "filename": "ql/src/test/results/clientpositive/tez/metadataonly1.q.out"}, {"additions": 39, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out", "sha": "cca489e3deaa0d8e633be74a0cb19f3173dae0e7", "changes": 39, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -104,6 +104,29 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(key)\n+                      keys: key (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        tag: -1\n+                        value expressions: _col1 (type: bigint)\n+                        auto parallelism: true\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:\n@@ -1656,6 +1679,22 @@ STAGE PLANS:\n             Truncated Path -> Alias:\n               -mr-10002default.src{} [src]\n         Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: key (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: key (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      tag: 1\n+                      auto parallelism: true\n         Reducer 4 \n             Needs Tagging: false\n             Reduce Operator Tree:", "filename": "ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out"}, {"additions": 65, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union.q.out", "sha": "6f6e8cab6e2e87b904965e28f8325daaccd7d4cc", "changes": 65, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -1253,3 +1253,68 @@ POSTHOOK: query: drop table ut\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@ut\n POSTHOOK: Output: default@ut\n+PREHOOK: query: create table TABLE1(EMP_NAME STRING, EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@TABLE1\n+POSTHOOK: query: create table TABLE1(EMP_NAME STRING, EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@TABLE1\n+PREHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@table2\n+POSTHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@table2\n+PREHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1\n+PREHOOK: type: CREATEVIEW\n+PREHOOK: Input: default@table1\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@TABLE3\n+POSTHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1\n+POSTHOOK: type: CREATEVIEW\n+POSTHOOK: Input: default@table1\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@TABLE3\n+PREHOOK: query: explain formatted select count(*) from TABLE3\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain formatted select count(*) from TABLE3\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####\n+PREHOOK: query: drop table table2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@table2\n+PREHOOK: Output: default@table2\n+POSTHOOK: query: drop table table2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@table2\n+POSTHOOK: Output: default@table2\n+PREHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@table2\n+POSTHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@table2\n+PREHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1 UNION ALL select EMP_NAME,EMP_ID from TABLE2\n+PREHOOK: type: CREATEVIEW\n+PREHOOK: Input: default@table1\n+PREHOOK: Input: default@table2\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@TABLE3\n+POSTHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1 UNION ALL select EMP_NAME,EMP_ID from TABLE2\n+POSTHOOK: type: CREATEVIEW\n+POSTHOOK: Input: default@table1\n+POSTHOOK: Input: default@table2\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@TABLE3\n+POSTHOOK: Output: default@table3\n+PREHOOK: query: explain formatted select count(*) from TABLE3\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain formatted select count(*) from TABLE3\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/tez/tez_union.q.out"}, {"additions": 67, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out", "sha": "654b34bcb0d4152462ec1423fb4dffe2566ecf99", "changes": 67, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -156,6 +156,24 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: x\n+                  Filter Operator\n+                    predicate: ((date < '2014-09-02') and (u <> 0)) (type: boolean)\n+                    Select Operator\n+                      expressions: u (type: bigint), date (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Group By Operator\n+                        aggregations: min(_col1)\n+                        keys: _col0 (type: bigint)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: bigint)\n+                          sort order: +\n+                          Map-reduce partition columns: _col0 (type: bigint)\n+                          value expressions: _col1 (type: string)\n         Map 10 \n             Map Operator Tree:\n                 TableScan\n@@ -170,8 +188,57 @@ STAGE PLANS:\n                       Map-reduce partition columns: t (type: string), st (type: string)\n                       Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: y\n+                  Filter Operator\n+                    predicate: ((date < '2014-09-02') and (u <> 0)) (type: boolean)\n+                    Select Operator\n+                      expressions: u (type: bigint), date (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Group By Operator\n+                        aggregations: min(_col1)\n+                        keys: _col0 (type: bigint)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: bigint)\n+                          sort order: +\n+                          Map-reduce partition columns: _col0 (type: bigint)\n+                          value expressions: _col1 (type: string)\n         Map 6 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: z\n+                  Filter Operator\n+                    predicate: ((date < '2014-09-02') and (u <> 0)) (type: boolean)\n+                    Select Operator\n+                      expressions: u (type: bigint), date (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Group By Operator\n+                        aggregations: min(_col1)\n+                        keys: _col0 (type: bigint)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: bigint)\n+                          sort order: +\n+                          Map-reduce partition columns: _col0 (type: bigint)\n+                          value expressions: _col1 (type: string)\n         Map 7 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: x\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (((t is not null and (date >= '2014-03-04')) and (date < '2014-09-03')) and (u <> 0)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: t (type: string), st (type: string)\n+                      sort order: ++\n+                      Map-reduce partition columns: t (type: string), st (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      value expressions: u (type: bigint)\n         Reducer 3 \n             Reduce Operator Tree:\n               Group By Operator", "filename": "ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out"}, {"additions": 1004, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/union_view.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/union_view.q.out", "sha": "ae6d7c8849269df0729a49f00dad1fc59ff3862c", "changes": 1004, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -0,0 +1,1004 @@\n+PREHOOK: query: CREATE TABLE src_union_1 (key int, value string) PARTITIONED BY (ds string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@src_union_1\n+POSTHOOK: query: CREATE TABLE src_union_1 (key int, value string) PARTITIONED BY (ds string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@src_union_1\n+PREHOOK: query: CREATE INDEX src_union_1_key_idx ON TABLE src_union_1(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+PREHOOK: type: CREATEINDEX\n+PREHOOK: Input: default@src_union_1\n+POSTHOOK: query: CREATE INDEX src_union_1_key_idx ON TABLE src_union_1(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+POSTHOOK: type: CREATEINDEX\n+POSTHOOK: Input: default@src_union_1\n+POSTHOOK: Output: default@default__src_union_1_src_union_1_key_idx__\n+PREHOOK: query: CREATE TABLE src_union_2 (key int, value string) PARTITIONED BY (ds string, part_1 string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@src_union_2\n+POSTHOOK: query: CREATE TABLE src_union_2 (key int, value string) PARTITIONED BY (ds string, part_1 string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@src_union_2\n+PREHOOK: query: CREATE INDEX src_union_2_key_idx ON TABLE src_union_2(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+PREHOOK: type: CREATEINDEX\n+PREHOOK: Input: default@src_union_2\n+POSTHOOK: query: CREATE INDEX src_union_2_key_idx ON TABLE src_union_2(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+POSTHOOK: type: CREATEINDEX\n+POSTHOOK: Input: default@src_union_2\n+POSTHOOK: Output: default@default__src_union_2_src_union_2_key_idx__\n+PREHOOK: query: CREATE TABLE src_union_3(key int, value string) PARTITIONED BY (ds string, part_1 string, part_2 string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@src_union_3\n+POSTHOOK: query: CREATE TABLE src_union_3(key int, value string) PARTITIONED BY (ds string, part_1 string, part_2 string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@src_union_3\n+PREHOOK: query: CREATE INDEX src_union_3_key_idx ON TABLE src_union_3(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+PREHOOK: type: CREATEINDEX\n+PREHOOK: Input: default@src_union_3\n+POSTHOOK: query: CREATE INDEX src_union_3_key_idx ON TABLE src_union_3(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+POSTHOOK: type: CREATEINDEX\n+POSTHOOK: Input: default@src_union_3\n+POSTHOOK: Output: default@default__src_union_3_src_union_3_key_idx__\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: src_union_1\n+          filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+          Filter Operator\n+            predicate: (key = 86) (type: boolean)\n+            Select Operator\n+              expressions: 86 (type: int), value (type: string), '1' (type: string)\n+              outputColumnNames: _col0, _col1, _col2\n+              ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: src_union_2\n+          filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+          Filter Operator\n+            predicate: (key = 86) (type: boolean)\n+            Select Operator\n+              expressions: 86 (type: int), value (type: string), '2' (type: string)\n+              outputColumnNames: _col0, _col1, _col2\n+              ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: src_union_3\n+          filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+          Filter Operator\n+            predicate: (key = 86) (type: boolean)\n+            Select Operator\n+              expressions: 86 (type: int), value (type: string), '3' (type: string)\n+              outputColumnNames: _col0, _col1, _col2\n+              ListSink\n+\n+86\tval_86\t1\n+86\tval_86\t2\n+86\tval_86\t2\n+86\tval_86\t3\n+86\tval_86\t3\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+500\n+1000\n+1000\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and ds is not null) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string), ds (type: string)\n+                      outputColumnNames: _col1, _col2\n+                      Select Operator\n+                        expressions: _col1 (type: string), _col2 (type: string)\n+                        outputColumnNames: _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions: _col2 (type: string)\n+                          sort order: +\n+                          value expressions: _col1 (type: string)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and ds is not null) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string), ds (type: string)\n+                      outputColumnNames: _col1, _col2\n+                      Select Operator\n+                        expressions: _col1 (type: string), _col2 (type: string)\n+                        outputColumnNames: _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions: _col2 (type: string)\n+                          sort order: +\n+                          value expressions: _col1 (type: string)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and ds is not null) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string), ds (type: string)\n+                      outputColumnNames: _col1, _col2\n+                      Select Operator\n+                        expressions: _col1 (type: string), _col2 (type: string)\n+                        outputColumnNames: _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions: _col2 (type: string)\n+                          sort order: +\n+                          value expressions: _col1 (type: string)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Select Operator\n+                expressions: 86 (type: int), VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string)\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 1250 Data size: 13280 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1250 Data size: 13280 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+86\tval_86\t1\n+86\tval_86\t2\n+86\tval_86\t2\n+86\tval_86\t3\n+86\tval_86\t3\n+86\tval_86\t1\n+86\tval_86\t2\n+86\tval_86\t2\n+86\tval_86\t3\n+86\tval_86\t3\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+500\n+1000\n+1000\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+86\tval_86\t4\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+500", "filename": "ql/src/test/results/clientpositive/tez/union_view.q.out"}, {"additions": 35, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out", "sha": "a8b25dba663e90979ffd5f25e57583b30a5f1854", "changes": 35, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -2620,6 +2620,19 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: ds (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: ds (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 5 \n             Map Operator Tree:\n                 TableScan\n@@ -4652,6 +4665,28 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 ds (type: string)\n+                        1 ds (type: string)\n+                      input vertices:\n+                        1 Map 2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: '13' (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: '13' (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 2 \n             Map Operator Tree:\n                 TableScan", "filename": "ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union30.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union30.q.out", "sha": "4529074118aa114fbe50c78d4b65d489a99ec18c", "changes": 42, "status": "modified", "deletions": 21, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union30.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -95,27 +95,6 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: src\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Select Operator\n-              expressions: key (type: string), value (type: string)\n-              outputColumnNames: _col0, _col1\n-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-              Union\n-                Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n-                Select Operator\n-                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)\n-                  outputColumnNames: _col0, _col1\n-                  Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n-                  File Output Operator\n-                    compressed: false\n-                    Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n-                    table:\n-                        input format: org.apache.hadoop.mapred.TextInputFormat\n-                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                        name: default.union_subq_union\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -167,6 +146,27 @@ STAGE PLANS:\n                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                       name: default.union_subq_union\n+          TableScan\n+            alias: src\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Select Operator\n+              expressions: key (type: string), value (type: string)\n+              outputColumnNames: _col0, _col1\n+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+              Union\n+                Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n+                Select Operator\n+                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        name: default.union_subq_union\n \n   Stage: Stage-8\n     Conditional Operator", "filename": "ql/src/test/results/clientpositive/union30.q.out"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_lateralview.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_lateralview.q.out", "sha": "734c1f42cca6d13d31e1ee28b036fece10ddd065", "changes": 16, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union_lateralview.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -101,14 +101,6 @@ STAGE PLANS:\n                             Map-reduce partition columns: _col1 (type: string)\n                             Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE\n                             value expressions: _col0 (type: int), _col2 (type: string)\n-          TableScan\n-            alias: b\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Reduce Output Operator\n-              key expressions: key (type: string)\n-              sort order: +\n-              Map-reduce partition columns: key (type: string)\n-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: srcpart\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -157,6 +149,14 @@ STAGE PLANS:\n                             Map-reduce partition columns: _col1 (type: string)\n                             Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE\n                             value expressions: _col0 (type: int), _col2 (type: string)\n+          TableScan\n+            alias: b\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Reduce Output Operator\n+              key expressions: key (type: string)\n+              sort order: +\n+              Map-reduce partition columns: key (type: string)\n+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n       Reduce Operator Tree:\n         Join Operator\n           condition map:", "filename": "ql/src/test/results/clientpositive/union_lateralview.q.out"}, {"additions": 210, "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_view.q.out", "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_view.q.out", "sha": "68854b3cad1a6cd711ccaa0aa862b325f9561cae", "changes": 210, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9", "patch": "@@ -503,6 +503,54 @@ STAGE PLANS:\n                           input format: org.apache.hadoop.mapred.TextInputFormat\n                           output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_2\n+            filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -570,6 +618,30 @@ STAGE PLANS:\n                       sort order: +\n                       Statistics: Num rows: 1250 Data size: 13280 Basic stats: COMPLETE Column stats: NONE\n                       value expressions: _col1 (type: string)\n+          TableScan\n+            alias: src_union_1\n+            filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           TableScan\n             alias: src_union_2\n             filterExpr: ((key = 86) and ds is not null) (type: boolean)\n@@ -965,6 +1037,30 @@ STAGE PLANS:\n                           input format: org.apache.hadoop.mapred.TextInputFormat\n                           output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -1005,6 +1101,54 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n+          TableScan\n+            alias: src_union_1\n+            filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_2\n+            filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           TableScan\n             alias: src_union_3\n             filterExpr: (ds = '4') (type: boolean)\n@@ -1024,6 +1168,72 @@ STAGE PLANS:\n                       sort order: \n                       Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                       value expressions: _col0 (type: bigint)\n+          TableScan\n+            alias: src_union_2\n+            filterExpr: (ds = '1') (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: (ds = '1') (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: (ds = '2') (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(VALUE._col0)", "filename": "ql/src/test/results/clientpositive/union_view.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1ae7c0e9045057a48c9debc829e4a0afa9aefd1a", "parent": "https://github.com/apache/hive/commit/d533b3935dc973a6e14d444ae1492f043a5353c0", "message": "HIVE-10333 : LLAP: NPE due to failure to find position (Sergey Shelukhin)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1673608 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_170", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/1ae7c0e9045057a48c9debc829e4a0afa9aefd1a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java", "blob_url": "https://github.com/apache/hive/blob/1ae7c0e9045057a48c9debc829e4a0afa9aefd1a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java", "sha": "db65259a718219530558ac9b0ff362ca7edf8012", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java?ref=1ae7c0e9045057a48c9debc829e4a0afa9aefd1a", "patch": "@@ -697,9 +697,10 @@ public static DiskRangeList uncompressStream(long fileId, long baseOffset, DiskR\n           toDecompress = new ArrayList<ProcCacheChunk>();\n           toRelease = (zcr == null) ? null : new ArrayList<ByteBuffer>();\n         }\n-        lastCached = addOneCompressionBuffer(bc, zcr, bufferSize,\n+        ProcCacheChunk newCached = addOneCompressionBuffer(bc, zcr, bufferSize,\n             cache, streamBuffer.cacheBuffers, toDecompress, toRelease);\n-        next = (lastCached != null) ? lastCached.next : null;\n+        lastCached = (newCached == null) ? lastCached : newCached;\n+        next = (newCached != null) ? newCached.next : null;\n         currentCOffset = (next != null) ? next.getOffset() : -1;\n       }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/50610ce3757506c5df96025ecaacaf04a566c245", "parent": "https://github.com/apache/hive/commit/3ee63c1402e9efda6afa5a8d52785299a08b9a57", "message": "HIVE-9073: NPE when using custom windowing UDAFs (Jason Dere, reviewed by Ashutosh Chauhan)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1671971 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_171", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/itests/src/test/resources/testconfiguration.properties", "sha": "4ab928056ae6d64a70f7363b00881a83221fabd4", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=50610ce3757506c5df96025ecaacaf04a566c245", "patch": "@@ -29,6 +29,7 @@ minimr.query.files=auto_sortmerge_join_16.q,\\\n   list_bucket_dml_10.q,\\\n   load_fs2.q,\\\n   load_hdfs_file_with_space_in_the_name.q,\\\n+  non_native_window_udf.q, \\\n   optrstat_groupby.q,\\\n   parallel_orderby.q,\\\n   ql_rewrite_gbtoidx.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 77, "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java", "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java", "sha": "d7817d90dce7c851affdf35aff65ce3de259c866", "changes": 81, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java?ref=50610ce3757506c5df96025ecaacaf04a566c245", "patch": "@@ -20,10 +20,14 @@\n \n import java.util.AbstractList;\n import java.util.ArrayList;\n+import java.util.HashMap;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n import org.apache.commons.lang.ArrayUtils;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n@@ -60,10 +64,42 @@\n \n @SuppressWarnings(\"deprecation\")\n public class WindowingTableFunction extends TableFunctionEvaluator {\n+  public static final Log LOG =LogFactory.getLog(WindowingTableFunction.class.getName());\n+  static class WindowingFunctionInfoHelper {\n+    private boolean supportsWindow;\n+\n+    WindowingFunctionInfoHelper() {\n+    }\n+\n+    public WindowingFunctionInfoHelper(boolean supportsWindow) {\n+      this.supportsWindow = supportsWindow;\n+    }\n+\n+    public boolean isSupportsWindow() {\n+      return supportsWindow;\n+    }\n+    public void setSupportsWindow(boolean supportsWindow) {\n+      this.supportsWindow = supportsWindow;\n+    }\n+  }\n \n   StreamingState streamingState;\n   RankLimit rnkLimitDef;\n+\n+  // There is some information about the windowing functions that needs to be initialized\n+  // during query compilation time, and made available to during the map/reduce tasks via\n+  // plan serialization.\n+  Map<String, WindowingFunctionInfoHelper> windowingFunctionHelpers = null;\n   \n+  public Map<String, WindowingFunctionInfoHelper> getWindowingFunctionHelpers() {\n+    return windowingFunctionHelpers;\n+  }\n+\n+  public void setWindowingFunctionHelpers(\n+      Map<String, WindowingFunctionInfoHelper> windowingFunctionHelpers) {\n+    this.windowingFunctionHelpers = windowingFunctionHelpers;\n+  }\n+\n   @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n   @Override\n   public void execute(PTFPartitionIterator<Object> pItr, PTFPartition outP) throws HiveException {\n@@ -147,9 +183,8 @@ private boolean processWindow(WindowFunctionDef wFn) {\n   private boolean streamingPossible(Configuration cfg, WindowFunctionDef wFnDef)\n       throws HiveException {\n     WindowFrameDef wdwFrame = wFnDef.getWindowFrame();\n-    WindowFunctionInfo wFnInfo = FunctionRegistry.getWindowFunctionInfo(wFnDef\n-        .getName());\n \n+    WindowingFunctionInfoHelper wFnInfo = getWindowingFunctionInfoHelper(wFnDef.getName());\n     if (!wFnInfo.isSupportsWindow()) {\n       return true;\n     }\n@@ -259,6 +294,45 @@ private boolean streamingPossible(Configuration cfg, WindowFunctionDef wFnDef)\n     return new int[] {precedingSpan, followingSpan};\n   }\n \n+  private void initializeWindowingFunctionInfoHelpers() throws SemanticException {\n+    // getWindowFunctionInfo() cannot be called during map/reduce tasks. So cache necessary\n+    // values during query compilation, and rely on plan serialization to bring this info\n+    // to the object during the map/reduce tasks.\n+    if (windowingFunctionHelpers != null) {\n+      return;\n+    }\n+\n+    windowingFunctionHelpers = new HashMap<String, WindowingFunctionInfoHelper>();\n+    WindowTableFunctionDef tabDef = (WindowTableFunctionDef) getTableDef();\n+    for (int i = 0; i < tabDef.getWindowFunctions().size(); i++) {\n+      WindowFunctionDef wFn = tabDef.getWindowFunctions().get(i);\n+      GenericUDAFEvaluator fnEval = wFn.getWFnEval();\n+      WindowFunctionInfo wFnInfo = FunctionRegistry.getWindowFunctionInfo(wFn.getName());\n+      boolean supportsWindow = wFnInfo.isSupportsWindow();\n+      windowingFunctionHelpers.put(wFn.getName(), new WindowingFunctionInfoHelper(supportsWindow));\n+    }\n+  }\n+\n+  @Override\n+  protected void setOutputOI(StructObjectInspector outputOI) {\n+    super.setOutputOI(outputOI);\n+    // Call here because at this point the WindowTableFunctionDef has been set\n+    try {\n+      initializeWindowingFunctionInfoHelpers();\n+    } catch (SemanticException err) {\n+      throw new RuntimeException(\"Unexpected error while setting up windowing function\", err);\n+    }\n+  }\n+\n+  private WindowingFunctionInfoHelper getWindowingFunctionInfoHelper(String fnName) {\n+    WindowingFunctionInfoHelper wFnInfoHelper = windowingFunctionHelpers.get(fnName);\n+    if (wFnInfoHelper == null) {\n+      // Should not happen\n+      throw new RuntimeException(\"No cached WindowingFunctionInfoHelper for \" + fnName);\n+    }\n+    return wFnInfoHelper;\n+  }\n+\n   @Override\n   public void initializeStreaming(Configuration cfg,\n       StructObjectInspector inputOI, boolean isMapSide) throws HiveException {\n@@ -412,8 +486,7 @@ public void startPartition() throws HiveException {\n       if (fnEval instanceof ISupportStreamingModeForWindowing) {\n         fnEval.terminate(streamingState.aggBuffers[i]);\n \n-        WindowFunctionInfo wFnInfo = FunctionRegistry.getWindowFunctionInfo(wFn\n-            .getName());\n+        WindowingFunctionInfoHelper wFnInfo = getWindowingFunctionInfoHelper(wFn.getName());\n         if (!wFnInfo.isSupportsWindow()) {\n           numRowsRemaining = ((ISupportStreamingModeForWindowing) fnEval)\n               .getRowsRemainingAfterTerminate();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/queries/clientpositive/non_native_window_udf.q", "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/queries/clientpositive/non_native_window_udf.q", "sha": "c3b88453d87b1b76c455026e4a1079f13342f52c", "changes": 11, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/non_native_window_udf.q?ref=50610ce3757506c5df96025ecaacaf04a566c245", "patch": "@@ -0,0 +1,11 @@\n+\n+create temporary function mylastval as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue';\n+\n+select  p_mfgr,p_name, p_size, \n+sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2, \n+first_value(p_size) over w1  as f, \n+last_value(p_size, false) over w1  as l,\n+mylastval(p_size, false) over w1  as m \n+from part \n+window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);\n+", "filename": "ql/src/test/queries/clientpositive/non_native_window_udf.q"}, {"additions": 52, "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/results/clientpositive/non_native_window_udf.q.out", "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/results/clientpositive/non_native_window_udf.q.out", "sha": "605e5b2456efc87282455e0d8d1a3ac3c66ecced", "changes": 52, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/non_native_window_udf.q.out?ref=50610ce3757506c5df96025ecaacaf04a566c245", "patch": "@@ -0,0 +1,52 @@\n+PREHOOK: query: create temporary function mylastval as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue'\n+PREHOOK: type: CREATEFUNCTION\n+PREHOOK: Output: mylastval\n+POSTHOOK: query: create temporary function mylastval as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue'\n+POSTHOOK: type: CREATEFUNCTION\n+POSTHOOK: Output: mylastval\n+PREHOOK: query: select  p_mfgr,p_name, p_size, \n+sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2, \n+first_value(p_size) over w1  as f, \n+last_value(p_size, false) over w1  as l,\n+mylastval(p_size, false) over w1  as m \n+from part \n+window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@part\n+#### A masked pattern was here ####\n+POSTHOOK: query: select  p_mfgr,p_name, p_size, \n+sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2, \n+first_value(p_size) over w1  as f, \n+last_value(p_size, false) over w1  as l,\n+mylastval(p_size, false) over w1  as m \n+from part \n+window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@part\n+#### A masked pattern was here ####\n+Manufacturer#1\talmond antique burnished rose metallic\t2\t2\t2\t34\t34\n+Manufacturer#1\talmond antique burnished rose metallic\t2\t2\t2\t6\t6\n+Manufacturer#1\talmond antique chartreuse lavender yellow\t34\t34\t2\t28\t28\n+Manufacturer#1\talmond antique salmon chartreuse burlywood\t6\t6\t2\t42\t42\n+Manufacturer#1\talmond aquamarine burnished black steel\t28\t28\t34\t42\t42\n+Manufacturer#1\talmond aquamarine pink moccasin thistle\t42\t42\t6\t42\t42\n+Manufacturer#2\talmond antique violet chocolate turquoise\t14\t14\t14\t2\t2\n+Manufacturer#2\talmond antique violet turquoise frosted\t40\t40\t14\t25\t25\n+Manufacturer#2\talmond aquamarine midnight light salmon\t2\t2\t14\t18\t18\n+Manufacturer#2\talmond aquamarine rose maroon antique\t25\t25\t40\t18\t18\n+Manufacturer#2\talmond aquamarine sandy cyan gainsboro\t18\t18\t2\t18\t18\n+Manufacturer#3\talmond antique chartreuse khaki white\t17\t17\t17\t19\t19\n+Manufacturer#3\talmond antique forest lavender goldenrod\t14\t14\t17\t1\t1\n+Manufacturer#3\talmond antique metallic orange dim\t19\t19\t17\t45\t45\n+Manufacturer#3\talmond antique misty red olive\t1\t1\t14\t45\t45\n+Manufacturer#3\talmond antique olive coral navajo\t45\t45\t19\t45\t45\n+Manufacturer#4\talmond antique gainsboro frosted violet\t10\t10\t10\t27\t27\n+Manufacturer#4\talmond antique violet mint lemon\t39\t39\t10\t7\t7\n+Manufacturer#4\talmond aquamarine floral ivory bisque\t27\t27\t10\t12\t12\n+Manufacturer#4\talmond aquamarine yellow dodger mint\t7\t7\t39\t12\t12\n+Manufacturer#4\talmond azure aquamarine papaya violet\t12\t12\t27\t12\t12\n+Manufacturer#5\talmond antique blue firebrick mint\t31\t31\t31\t2\t2\n+Manufacturer#5\talmond antique medium spring khaki\t6\t6\t31\t46\t46\n+Manufacturer#5\talmond antique sky peru orange\t2\t2\t31\t23\t23\n+Manufacturer#5\talmond aquamarine dodger light gainsboro\t46\t46\t6\t23\t23\n+Manufacturer#5\talmond azure blanched chiffon midnight\t23\t23\t2\t23\t23", "filename": "ql/src/test/results/clientpositive/non_native_window_udf.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/24449abd0dcca45d68c8ebb98509859b97490725", "parent": "https://github.com/apache/hive/commit/900753fd910fb097cdcb70811370d2d4fbe8b65c", "message": "HIVE-10095: format_number udf throws NPE (Alex Pivovarov via Jason Dere)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1670405 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_172", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/24449abd0dcca45d68c8ebb98509859b97490725/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFormatNumber.java", "blob_url": "https://github.com/apache/hive/blob/24449abd0dcca45d68c8ebb98509859b97490725/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFormatNumber.java", "sha": "71ca8f2241beb339efdfdadc2efa14a58ea1f024", "changes": 18, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFormatNumber.java?ref=24449abd0dcca45d68c8ebb98509859b97490725", "patch": "@@ -147,7 +147,13 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n \n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n-    int dValue = ((IntObjectInspector) argumentOIs[1]).get(arguments[1].get());\n+    Object arg0;\n+    Object arg1;\n+    if ((arg0 = arguments[0].get()) == null || (arg1 = arguments[1].get()) == null) {\n+      return null;\n+    }\n+\n+    int dValue = ((IntObjectInspector) argumentOIs[1]).get(arg1);\n \n     if (dValue < 0) {\n       throw new HiveException(\"Argument 2 of function FORMAT_NUMBER must be >= 0, but \\\"\"\n@@ -181,26 +187,26 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     switch (xObjectInspector.getPrimitiveCategory()) {\n       case VOID:\n       case DOUBLE:\n-        xDoubleValue = ((DoubleObjectInspector) argumentOIs[0]).get(arguments[0].get());\n+        xDoubleValue = ((DoubleObjectInspector) argumentOIs[0]).get(arg0);\n         resultText.set(numberFormat.format(xDoubleValue));\n         break;\n       case FLOAT:\n-        xFloatValue = ((FloatObjectInspector) argumentOIs[0]).get(arguments[0].get());\n+        xFloatValue = ((FloatObjectInspector) argumentOIs[0]).get(arg0);\n         resultText.set(numberFormat.format(xFloatValue));\n         break;\n       case DECIMAL:\n         xDecimalValue = ((HiveDecimalObjectInspector) argumentOIs[0])\n-            .getPrimitiveJavaObject(arguments[0].get());\n+            .getPrimitiveJavaObject(arg0);\n         resultText.set(numberFormat.format(xDecimalValue.bigDecimalValue()));\n         break;\n       case BYTE:\n       case SHORT:\n       case INT:\n-        xIntValue = ((IntObjectInspector) argumentOIs[0]).get(arguments[0].get());\n+        xIntValue = ((IntObjectInspector) argumentOIs[0]).get(arg0);\n         resultText.set(numberFormat.format(xIntValue));\n         break;\n       case LONG:\n-        xLongValue = ((LongObjectInspector) argumentOIs[0]).get(arguments[0].get());\n+        xLongValue = ((LongObjectInspector) argumentOIs[0]).get(arg0);\n         resultText.set(numberFormat.format(xLongValue));\n         break;\n       default:", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFormatNumber.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/24449abd0dcca45d68c8ebb98509859b97490725/ql/src/test/queries/clientpositive/udf_format_number.q", "blob_url": "https://github.com/apache/hive/blob/24449abd0dcca45d68c8ebb98509859b97490725/ql/src/test/queries/clientpositive/udf_format_number.q", "sha": "28f087d02da02174ad33a6fbb819ca3b79ccc8c5", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_format_number.q?ref=24449abd0dcca45d68c8ebb98509859b97490725", "patch": "@@ -71,3 +71,9 @@ SELECT format_number(12332.123456BD, 4),\n     format_number(-12332.2BD, 0),\n     format_number(CAST(12332.567 AS DECIMAL(8, 1)), 4)\n FROM src tablesample (1 rows);\n+\n+-- nulls\n+SELECT\n+  format_number(cast(null as int), 0),\n+  format_number(12332.123456BD, cast(null as int)),\n+  format_number(cast(null as int), cast(null as int));", "filename": "ql/src/test/queries/clientpositive/udf_format_number.q"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/24449abd0dcca45d68c8ebb98509859b97490725/ql/src/test/results/clientpositive/udf_format_number.q.out", "blob_url": "https://github.com/apache/hive/blob/24449abd0dcca45d68c8ebb98509859b97490725/ql/src/test/results/clientpositive/udf_format_number.q.out", "sha": "4a2c80d39ae2ad235d4644bb1aff86e05f9960b5", "changes": 17, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_format_number.q.out?ref=24449abd0dcca45d68c8ebb98509859b97490725", "patch": "@@ -211,3 +211,20 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n #### A masked pattern was here ####\n 12,332.1235\t12,332.12\t12,332.1000\t-12,332\t12,332.6000\n+PREHOOK: query: -- nulls\n+SELECT\n+  format_number(cast(null as int), 0),\n+  format_number(12332.123456BD, cast(null as int)),\n+  format_number(cast(null as int), cast(null as int))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- nulls\n+SELECT\n+  format_number(cast(null as int), 0),\n+  format_number(12332.123456BD, cast(null as int)),\n+  format_number(cast(null as int), cast(null as int))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+NULL\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_format_number.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/63080f82c02c2b7756be09209bf07f03280e4f9b", "parent": "https://github.com/apache/hive/commit/10cfdd96b69084bf0da7b864113d073006147e77", "message": "HIVE-10018 Activating SQLStandardAuth results in NPE [hbase-metastore branch] (Alan Gates)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/hbase-metastore@1668746 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_173", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java", "blob_url": "https://github.com/apache/hive/blob/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java", "sha": "c30ac34cb65d10ac9b0eb4a504fe9a385f6879b9", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java?ref=63080f82c02c2b7756be09209bf07f03280e4f9b", "patch": "@@ -25,6 +25,8 @@\n import org.apache.hadoop.hive.cli.CliSessionState;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.mockito.Mock;\n import org.mockito.Mockito;\n@@ -107,6 +109,14 @@ protected void setupDriver() {\n         \"org.apache.hadoop.hive.metastore.hbase.HBaseStore\");\n     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);\n     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);\n+    // Setup so we can test SQL standard auth\n+    conf.setBoolVar(HiveConf.ConfVars.HIVE_TEST_AUTHORIZATION_SQLSTD_HS2_MODE, true);\n+    conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,\n+        SQLStdHiveAuthorizerFactoryForTest.class.getName());\n+    conf.setVar(HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER,\n+        SessionStateConfigUserAuthenticator.class.getName());\n+    conf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED, true);\n+    conf.setVar(HiveConf.ConfVars.USERS_IN_ADMIN_ROLE, System.getProperty(\"user.name\"));\n     HBaseReadWrite.setTestConnection(hconn);\n \n     SessionState.start(new CliSessionState(conf));", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java"}, {"additions": 57, "raw_url": "https://github.com/apache/hive/raw/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java", "blob_url": "https://github.com/apache/hive/blob/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java", "sha": "fe5e8e2789af748e91ff5d95832e5c23d79088b5", "changes": 58, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java?ref=63080f82c02c2b7756be09209bf07f03280e4f9b", "patch": "@@ -73,7 +73,10 @@ public void insertIntoPartitionTable() throws Exception {\n   public void database() throws Exception {\n     CommandProcessorResponse rsp = driver.run(\"create database db\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n-    rsp = driver.run(\"alter database db set owner user me\");\n+    rsp = driver.run(\"set role admin\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    // security doesn't let me change the properties\n+    rsp = driver.run(\"alter database db set dbproperties ('key' = 'value')\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n     rsp = driver.run(\"drop database db\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n@@ -124,5 +127,58 @@ public void partitionedTable() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n   }\n \n+  @Test\n+  public void role() throws Exception {\n+    CommandProcessorResponse rsp = driver.run(\"set role admin\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"create role role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"grant role1 to user fred with admin option\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"create role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"grant role1 to role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show principals role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show role grant role role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show role grant user \" + System.getProperty(\"user.name\"));\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show roles\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"revoke admin option for role1 from user fred\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"revoke role1 from user fred\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"revoke role1 from role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show current roles\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+\n+    rsp = driver.run(\"drop role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"drop role role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+  }\n+\n+  @Test\n+  public void grant() throws Exception {\n+    CommandProcessorResponse rsp = driver.run(\"set role admin\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"create role role3\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"create table granttbl (c int)\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"grant select on granttbl to \" + System.getProperty(\"user.name\"));\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"grant select on granttbl to role3 with grant option\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"revoke select on granttbl from \" + System.getProperty(\"user.name\"));\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"revoke grant option for select on granttbl from role3\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+  }\n+\n \n }", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java"}, {"additions": 57, "raw_url": "https://github.com/apache/hive/raw/63080f82c02c2b7756be09209bf07f03280e4f9b/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java", "blob_url": "https://github.com/apache/hive/blob/63080f82c02c2b7756be09209bf07f03280e4f9b/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java", "sha": "159b9e729876d0da7c68cbc76866b9e2920fd2cb", "changes": 94, "status": "modified", "deletions": 37, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java?ref=63080f82c02c2b7756be09209bf07f03280e4f9b", "patch": "@@ -617,17 +617,22 @@ public PrincipalPrivilegeSet getUserPrivilegeSet(String userName, List<String> g\n       PrincipalPrivilegeSet pps = new PrincipalPrivilegeSet();\n       PrincipalPrivilegeSet global = getHBase().getGlobalPrivs();\n       if (global == null) return null;\n-      List<PrivilegeGrantInfo> pgi = global.getUserPrivileges().get(userName);\n-      if (pgi != null) {\n-        pps.putToUserPrivileges(userName, pgi);\n+      List<PrivilegeGrantInfo> pgi;\n+      if (global.getUserPrivileges() != null) {\n+        pgi = global.getUserPrivileges().get(userName);\n+        if (pgi != null) {\n+          pps.putToUserPrivileges(userName, pgi);\n+        }\n       }\n \n-      List<String> roles = getHBase().getUserRoles(userName);\n-      if (roles != null) {\n-        for (String role : roles) {\n-          pgi = global.getRolePrivileges().get(role);\n-          if (pgi != null) {\n-            pps.putToRolePrivileges(role, pgi);\n+      if (global.getRolePrivileges() != null) {\n+        List<String> roles = getHBase().getUserRoles(userName);\n+        if (roles != null) {\n+          for (String role : roles) {\n+            pgi = global.getRolePrivileges().get(role);\n+            if (pgi != null) {\n+              pps.putToRolePrivileges(role, pgi);\n+            }\n           }\n         }\n       }\n@@ -645,18 +650,25 @@ public PrincipalPrivilegeSet getDBPrivilegeSet(String dbName, String userName,\n     try {\n       PrincipalPrivilegeSet pps = new PrincipalPrivilegeSet();\n       Database db = getHBase().getDb(dbName);\n-      // Find the user privileges for this db\n-      List<PrivilegeGrantInfo> pgi = db.getPrivileges().getUserPrivileges().get(userName);\n-      if (pgi != null) {\n-        pps.putToUserPrivileges(userName, pgi);\n-      }\n-\n-      List<String> roles = getHBase().getUserRoles(userName);\n-      if (roles != null) {\n-        for (String role : roles) {\n-          pgi = db.getPrivileges().getRolePrivileges().get(role);\n+      if (db.getPrivileges() != null) {\n+        List<PrivilegeGrantInfo> pgi;\n+        // Find the user privileges for this db\n+        if (db.getPrivileges().getUserPrivileges() != null) {\n+          pgi = db.getPrivileges().getUserPrivileges().get(userName);\n           if (pgi != null) {\n-            pps.putToRolePrivileges(role, pgi);\n+            pps.putToUserPrivileges(userName, pgi);\n+          }\n+        }\n+\n+        if (db.getPrivileges().getRolePrivileges() != null) {\n+          List<String> roles = getHBase().getUserRoles(userName);\n+          if (roles != null) {\n+            for (String role : roles) {\n+              pgi = db.getPrivileges().getRolePrivileges().get(role);\n+              if (pgi != null) {\n+                pps.putToRolePrivileges(role, pgi);\n+              }\n+            }\n           }\n         }\n       }\n@@ -674,18 +686,24 @@ public PrincipalPrivilegeSet getTablePrivilegeSet(String dbName, String tableNam\n     try {\n       PrincipalPrivilegeSet pps = new PrincipalPrivilegeSet();\n       Table table = getHBase().getTable(dbName, tableName);\n-      // Find the user privileges for this db\n-      List<PrivilegeGrantInfo> pgi = table.getPrivileges().getUserPrivileges().get(userName);\n-      if (pgi != null) {\n-        pps.putToUserPrivileges(userName, pgi);\n-      }\n-\n-      List<String> roles = getHBase().getUserRoles(userName);\n-      if (roles != null) {\n-        for (String role : roles) {\n-          pgi = table.getPrivileges().getRolePrivileges().get(role);\n+      List<PrivilegeGrantInfo> pgi;\n+      if (table.getPrivileges() != null) {\n+        if (table.getPrivileges().getUserPrivileges() != null) {\n+          pgi = table.getPrivileges().getUserPrivileges().get(userName);\n           if (pgi != null) {\n-            pps.putToRolePrivileges(role, pgi);\n+            pps.putToUserPrivileges(userName, pgi);\n+          }\n+        }\n+\n+        if (table.getPrivileges().getRolePrivileges() != null) {\n+          List<String> roles = getHBase().getUserRoles(userName);\n+          if (roles != null) {\n+            for (String role : roles) {\n+              pgi = table.getPrivileges().getRolePrivileges().get(role);\n+              if (pgi != null) {\n+                pps.putToRolePrivileges(role, pgi);\n+              }\n+            }\n           }\n         }\n       }\n@@ -1068,12 +1086,14 @@ public Role getRole(String roleName) throws NoSuchObjectException {\n       List<RolePrincipalGrant> rpgs = new ArrayList<RolePrincipalGrant>(roles.size());\n       for (Role role : roles) {\n         HbaseMetastoreProto.RoleGrantInfoList grants = getHBase().getRolePrincipals(role.getRoleName());\n-        for (HbaseMetastoreProto.RoleGrantInfo grant : grants.getGrantInfoList()) {\n-          if (grant.getPrincipalType().equals(principalType) &&\n-              grant.getPrincipalName().equals(principalName)) {\n-            rpgs.add(new RolePrincipalGrant(role.getRoleName(), principalName, principalType,\n-                grant.getGrantOption(), (int)grant.getAddTime(), grant.getGrantor(),\n-                HBaseUtils.convertPrincipalTypes(grant.getGrantorType())));\n+        if (grants != null) {\n+          for (HbaseMetastoreProto.RoleGrantInfo grant : grants.getGrantInfoList()) {\n+            if (grant.getPrincipalType() == HBaseUtils.convertPrincipalTypes(principalType) &&\n+                grant.getPrincipalName().equals(principalName)) {\n+              rpgs.add(new RolePrincipalGrant(role.getRoleName(), principalName, principalType,\n+                  grant.getGrantOption(), (int) grant.getAddTime(), grant.getGrantor(),\n+                  HBaseUtils.convertPrincipalTypes(grant.getGrantorType())));\n+            }\n           }\n         }\n       }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/73fb1953c1a5328dbfcf32f545fe74dc47dcc889", "parent": "https://github.com/apache/hive/commit/d9a6435e9cc348c6e85230cc1a290497926f76b4", "message": "HIVE-10045 : LLAP : NPE in DiskRangeList helper init - initial fix (Sergey Shelukhin)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1668186 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_174", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/73fb1953c1a5328dbfcf32f545fe74dc47dcc889/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/73fb1953c1a5328dbfcf32f545fe74dc47dcc889/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java", "sha": "e5e76a0e7a980b453c6606a6f25002d146ae4d0d", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java?ref=73fb1953c1a5328dbfcf32f545fe74dc47dcc889", "patch": "@@ -216,6 +216,11 @@ public void readEncodedColumns(int stripeIx, StripeInformation stripe,\n       offset += length;\n     }\n \n+    if (listToRead.get() == null) {\n+      LOG.warn(\"Nothing to read for stripe [\" + stripe + \"]\");\n+      return;\n+    }\n+\n     // 2. Now, read all of the ranges from cache or disk.\n     DiskRangeListMutateHelper toRead = new DiskRangeListMutateHelper(listToRead.get());\n     if (DebugUtils.isTraceOrcEnabled()) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/EncodedReaderImpl.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/10cfdd96b69084bf0da7b864113d073006147e77", "parent": "https://github.com/apache/hive/commit/35ac8b0257614523064c1b3647a91e123b78e999", "message": "HIVE-10010 Alter table results in NPE [hbase-metastore branch] (Alan Gates)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/hbase-metastore@1668071 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_175", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/10cfdd96b69084bf0da7b864113d073006147e77/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java", "blob_url": "https://github.com/apache/hive/blob/10cfdd96b69084bf0da7b864113d073006147e77/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java", "sha": "676c3896da53129e6d813ae0dcfe61f278563c35", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java?ref=10cfdd96b69084bf0da7b864113d073006147e77", "patch": "@@ -79,7 +79,7 @@ public void database() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n   }\n \n-  @Ignore\n+  @Test\n   public void table() throws Exception {\n     driver.run(\"create table tbl (c int)\");\n     CommandProcessorResponse rsp = driver.run(\"insert into table tbl values (3)\");\n@@ -92,7 +92,7 @@ public void table() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n   }\n \n-  @Ignore\n+  @Test\n   public void partitionedTable() throws Exception {\n     driver.run(\"create table parttbl (c int) partitioned by (ds string)\");\n     CommandProcessorResponse rsp =\n@@ -113,8 +113,11 @@ public void partitionedTable() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n     rsp = driver.run(\"alter table parttbl touch partition (ds = 'whenever')\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n+    // TODO - Can't do this until getPartitionsByExpr implemented\n+    /*\n     rsp = driver.run(\"alter table parttbl drop partition (ds = 'whenever')\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n+    */\n     rsp = driver.run(\"select * from parttbl\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n     rsp = driver.run(\"select * from parttbl where ds = 'today'\");", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java"}, {"additions": 122, "raw_url": "https://github.com/apache/hive/raw/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java", "blob_url": "https://github.com/apache/hive/blob/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java", "sha": "d772dca50db704313c70c53d42237bfe785430bd", "changes": 770, "status": "modified", "deletions": 648, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java?ref=10cfdd96b69084bf0da7b864113d073006147e77", "patch": "@@ -27,751 +27,225 @@\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n \n import java.util.ArrayList;\n-import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n-import java.util.Map;\n \n /**\n  * A {@link org.apache.hadoop.hive.metastore.api.StorageDescriptor} with most of it's content\n- * shared.  Location and parameters are left alone, everything else is redirected to a shared\n- * reference in the cache.\n+ * shallow copied from the underlying storage descriptor.  Location and parameters are left alone.\n+ * To avoid issues when users change the contents, all lists and nested structures (cols, serde,\n+ * buckets, sortCols, and skewed) are deep copied when they are accessed for reading or writing.\n+ * (It has to be done on read as well because there's no way to guarantee the user won't change the\n+ * nested structure or list, which would result in changing every storage descriptor sharing that\n+ * structure.)  Users wishing better performance can call setReadyOnly(), which will prevent the\n+ * copies.\n  */\n public class SharedStorageDescriptor extends StorageDescriptor {\n   static final private Log LOG = LogFactory.getLog(SharedStorageDescriptor.class.getName());\n-  private StorageDescriptor shared;\n-  private boolean copied = false;\n-  private CopyOnWriteColList colList = null;\n-  private CopyOnWriteOrderList orderList = null;\n-  private CopyOnWriteBucketList bucketList = null;\n+  private boolean colsCopied = false;\n+  private boolean serdeCopied = false;\n+  private boolean bucketsCopied = false;\n+  private boolean sortCopied = false;\n+  private boolean skewedCopied = false;\n \n   SharedStorageDescriptor() {\n   }\n \n-  public SharedStorageDescriptor(SharedStorageDescriptor that) {\n-    this.setLocation(that.getLocation());\n-    this.setParameters(that.getParameters());\n-    this.shared = that.shared;\n+  void setShared(StorageDescriptor shared) {\n+    if (shared.getCols() != null) super.setCols(shared.getCols());\n+    // Skip location\n+    if (shared.getInputFormat() != null) super.setInputFormat(shared.getInputFormat());\n+    if (shared.getOutputFormat() != null) super.setOutputFormat(shared.getOutputFormat());\n+    super.setCompressed(shared.isCompressed());\n+    super.setNumBuckets(shared.getNumBuckets());\n+    if (shared.getSerdeInfo() != null) super.setSerdeInfo(shared.getSerdeInfo());\n+    if (shared.getBucketCols() != null) super.setBucketCols(shared.getBucketCols());\n+    if (shared.getSortCols() != null) super.setSortCols(shared.getSortCols());\n+    // skip parameters\n+    if (shared.getSkewedInfo() != null) super.setSkewedInfo(shared.getSkewedInfo());\n+    super.setStoredAsSubDirectories(shared.isStoredAsSubDirectories());\n   }\n \n-  @Override\n-  public StorageDescriptor deepCopy() {\n-    return new SharedStorageDescriptor(this);\n+  /**\n+   * Promise that you'll only use this shared storage descriptor in a read only mode.\n+   * This prevents the copies of the nested structures and lists when reading them.  However, the\n+   * caller must not change the structures or lists returned to it, as this will change all\n+   * storage descriptor sharing that list.\n+   */\n+  public void setReadOnly() {\n+    colsCopied = serdeCopied = bucketsCopied = sortCopied = skewedCopied = true;\n   }\n \n   @Override\n-  public boolean isSetCols() {\n-    return shared.isSetCols();\n+  public void addToCols(FieldSchema fs) {\n+    copyCols();\n+    super.addToCols(fs);\n   }\n \n   @Override\n   public List<FieldSchema> getCols() {\n-    return copied ? shared.getCols() : (\n-        shared.getCols() == null ? null : copyCols(shared.getCols()));\n-  }\n-\n-  @Override\n-  public int getColsSize() {\n-    return shared.getColsSize();\n-  }\n-\n-  @Override\n-  public Iterator<FieldSchema> getColsIterator() {\n-    return shared.getColsIterator();\n+    copyCols();\n+    return super.getCols();\n   }\n \n   @Override\n   public void setCols(List<FieldSchema> cols) {\n-    copyOnWrite();\n-    shared.setCols(cols);\n-  }\n-\n-  @Override\n-  public void addToCols(FieldSchema fs) {\n-    copyOnWrite();\n-    shared.addToCols(fs);\n+    colsCopied = true;\n+    super.setCols(cols);\n   }\n \n   @Override\n   public void unsetCols() {\n-    copyOnWrite();\n-    shared.unsetCols();\n+    colsCopied = true;\n+    super.unsetCols();\n   }\n \n   @Override\n-  public boolean isSetInputFormat() {\n-    return shared.isSetInputFormat();\n-  }\n-\n-  @Override\n-  public String getInputFormat() {\n-    return shared.getInputFormat();\n-  }\n-\n-  @Override\n-  public void setInputFormat(String inputFormat) {\n-    copyOnWrite();\n-    shared.setInputFormat(inputFormat);\n-  }\n-\n-  @Override\n-  public void unsetInputFormat() {\n-    copyOnWrite();\n-    shared.unsetInputFormat();\n-  }\n-\n-  @Override\n-  public boolean isSetOutputFormat() {\n-    return shared.isSetOutputFormat();\n-  }\n-\n-  @Override\n-  public String getOutputFormat() {\n-    return shared.getOutputFormat();\n-  }\n-\n-  @Override\n-  public void setOutputFormat(String outputFormat) {\n-    copyOnWrite();\n-    shared.setOutputFormat(outputFormat);\n-  }\n-\n-  @Override\n-  public void unsetOutputFormat() {\n-    copyOnWrite();\n-    shared.unsetOutputFormat();\n-  }\n-\n-  @Override\n-  public boolean isSetCompressed() {\n-    return shared.isSetCompressed();\n-  }\n-\n-  @Override\n-  public boolean isCompressed() {\n-    return shared.isCompressed();\n-  }\n-\n-  @Override\n-  public void setCompressed(boolean isCompressed) {\n-    copyOnWrite();\n-    shared.setCompressed(isCompressed);\n-  }\n-\n-  @Override\n-  public void unsetCompressed() {\n-    copyOnWrite();\n-    shared.unsetCompressed();\n-  }\n-\n-  @Override\n-  public boolean isSetNumBuckets() {\n-    return shared.isSetNumBuckets();\n-  }\n-\n-  @Override\n-  public int getNumBuckets() {\n-    return shared.getNumBuckets();\n-  }\n-\n-  @Override\n-  public void setNumBuckets(int numBuckets) {\n-    copyOnWrite();\n-    shared.setNumBuckets(numBuckets);\n-  }\n-\n-  @Override\n-  public void unsetNumBuckets() {\n-    copyOnWrite();\n-    shared.unsetNumBuckets();\n+  public Iterator<FieldSchema> getColsIterator() {\n+    copyCols();\n+    return super.getColsIterator();\n   }\n \n-  @Override\n-  public boolean isSetSerdeInfo() {\n-    return shared.isSetSerdeInfo();\n+  private void copyCols() {\n+    if (!colsCopied) {\n+      colsCopied = true;\n+      if (super.getCols() != null) {\n+        List<FieldSchema> cols = new ArrayList<FieldSchema>(super.getColsSize());\n+        for (FieldSchema fs : super.getCols()) cols.add(new FieldSchema(fs));\n+        super.setCols(cols);\n+      }\n+    }\n   }\n \n   @Override\n   public SerDeInfo getSerdeInfo() {\n-    return copied ? shared.getSerdeInfo() : (\n-        shared.getSerdeInfo() == null ? null : new SerDeInfoWrapper(shared.getSerdeInfo()));\n+    copySerde();\n+    return super.getSerdeInfo();\n   }\n \n   @Override\n   public void setSerdeInfo(SerDeInfo serdeInfo) {\n-    copyOnWrite();\n-    shared.setSerdeInfo(serdeInfo);\n+    serdeCopied = true;\n+    super.setSerdeInfo(serdeInfo);\n   }\n \n   @Override\n   public void unsetSerdeInfo() {\n-    copyOnWrite();\n-    shared.unsetSerdeInfo();\n-  }\n-\n-  @Override\n-  public boolean isSetBucketCols() {\n-    return shared.isSetBucketCols();\n+    serdeCopied = true;\n+    super.unsetSerdeInfo();\n   }\n \n-  @Override\n-  public List<String> getBucketCols() {\n-    return copied ? shared.getBucketCols() : (\n-        shared.getBucketCols() == null ? null : copyBucketCols(shared.getBucketCols()));\n-  }\n-\n-  @Override\n-  public int getBucketColsSize() {\n-    return shared.getBucketColsSize();\n+  private void copySerde() {\n+    if (!serdeCopied) {\n+      serdeCopied = true;\n+      if (super.getSerdeInfo() != null) super.setSerdeInfo(new SerDeInfo(super.getSerdeInfo()));\n+    }\n   }\n \n   @Override\n-  public Iterator<String> getBucketColsIterator() {\n-    return shared.getBucketColsIterator();\n+  public void addToBucketCols(String bucket) {\n+    copyBucketCols();\n+    super.addToBucketCols(bucket);\n   }\n \n   @Override\n-  public void setBucketCols(List<String> bucketCols) {\n-    copyOnWrite();\n-    shared.setBucketCols(bucketCols);\n+  public List<String> getBucketCols() {\n+    copyBucketCols();\n+    return super.getBucketCols();\n   }\n \n   @Override\n-  public void addToBucketCols(String bucketCol) {\n-    copyOnWrite();\n-    shared.addToBucketCols(bucketCol);\n+  public void setBucketCols(List<String> buckets) {\n+    bucketsCopied = true;\n+    super.setBucketCols(buckets);\n   }\n \n   @Override\n   public void unsetBucketCols() {\n-    copyOnWrite();\n-    shared.unsetBucketCols();\n+    bucketsCopied = true;\n+    super.unsetBucketCols();\n   }\n \n   @Override\n-  public boolean isSetSortCols() {\n-    return shared.isSetSortCols();\n+  public Iterator<String> getBucketColsIterator() {\n+    copyBucketCols();\n+    return super.getBucketColsIterator();\n   }\n \n-  @Override\n-  public List<Order> getSortCols() {\n-    return copied ? shared.getSortCols() : (\n-        shared.getSortCols() == null ? null : copySort(shared.getSortCols()));\n+  private void copyBucketCols() {\n+    if (!bucketsCopied) {\n+      bucketsCopied = true;\n+      if (super.getBucketCols() != null) {\n+        List<String> buckets = new ArrayList<String>(super.getBucketColsSize());\n+        for (String bucket : super.getBucketCols()) buckets.add(bucket);\n+        super.setBucketCols(buckets);\n+      }\n+    }\n   }\n \n   @Override\n-  public int getSortColsSize() {\n-    return shared.getSortColsSize();\n+  public void addToSortCols(Order sort) {\n+    copySort();\n+    super.addToSortCols(sort);\n   }\n \n   @Override\n-  public Iterator<Order> getSortColsIterator() {\n-    return shared.getSortColsIterator();\n+  public List<Order> getSortCols() {\n+    copySort();\n+    return super.getSortCols();\n   }\n \n   @Override\n-  public void setSortCols(List<Order> sortCols) {\n-    copyOnWrite();\n-    shared.setSortCols(sortCols);\n+  public void setSortCols(List<Order> sorts) {\n+    sortCopied = true;\n+    super.setSortCols(sorts);\n   }\n \n   @Override\n-  public void addToSortCols(Order sortCol) {\n-    copyOnWrite();\n-    shared.addToSortCols(sortCol);\n+  public void unsetSortCols() {\n+    sortCopied = true;\n+    super.unsetSortCols();\n   }\n \n   @Override\n-  public void unsetSortCols() {\n-    copyOnWrite();\n-    shared.unsetSortCols();\n+  public Iterator<Order> getSortColsIterator() {\n+    copySort();\n+    return super.getSortColsIterator();\n   }\n \n-  @Override\n-  public boolean isSetSkewedInfo() {\n-    return shared.isSetSkewedInfo();\n+  private void copySort() {\n+    if (!sortCopied) {\n+      sortCopied = true;\n+      if (super.getSortCols() != null) {\n+        List<Order> sortCols = new ArrayList<Order>(super.getSortColsSize());\n+        for (Order sortCol : super.getSortCols()) sortCols.add(new Order(sortCol));\n+        super.setSortCols(sortCols);\n+      }\n+    }\n   }\n \n   @Override\n   public SkewedInfo getSkewedInfo() {\n-    return copied ? shared.getSkewedInfo() : (\n-        shared.getSkewedInfo() == null ? null : new SkewWrapper(shared.getSkewedInfo()));\n+    copySkewed();\n+    return super.getSkewedInfo();\n   }\n \n   @Override\n   public void setSkewedInfo(SkewedInfo skewedInfo) {\n-    copyOnWrite();\n-    shared.setSkewedInfo(skewedInfo);\n+    skewedCopied = true;\n+    super.setSkewedInfo(skewedInfo);\n   }\n \n   @Override\n   public void unsetSkewedInfo() {\n-    copyOnWrite();\n-    shared.unsetSkewedInfo();\n-  }\n-\n-  @Override\n-  public boolean isSetStoredAsSubDirectories() {\n-    return shared.isSetStoredAsSubDirectories();\n-  }\n-\n-  @Override\n-  public boolean isStoredAsSubDirectories() {\n-    return shared.isStoredAsSubDirectories();\n-  }\n-\n-  @Override\n-  public void setStoredAsSubDirectories(boolean sasd) {\n-    copyOnWrite();\n-    shared.setStoredAsSubDirectories(sasd);\n-  }\n-\n-  @Override\n-  public void unsetStoredAsSubDirectories() {\n-    copyOnWrite();\n-    shared.unsetStoredAsSubDirectories();\n+    skewedCopied = true;\n+    super.unsetSkewedInfo();\n   }\n \n-  void setShared(StorageDescriptor sd) {\n-    shared = sd;\n-  }\n-\n-  StorageDescriptor getShared() {\n-    return shared;\n-  }\n-\n-  private void copyOnWrite() {\n-    if (!copied) {\n-      shared = new StorageDescriptor(shared);\n-      copied = true;\n-    }\n-  }\n-\n-  private class SerDeInfoWrapper extends SerDeInfo {\n-\n-    SerDeInfoWrapper(SerDeInfo serde) {\n-      super(serde);\n-    }\n-\n-    @Override\n-    public void setName(String name) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().setName(name);\n-    }\n-\n-    @Override\n-    public void unsetName() {\n-      copyOnWrite();\n-      shared.getSerdeInfo().unsetName();\n-    }\n-\n-    @Override\n-    public void setSerializationLib(String lib) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().setSerializationLib(lib);\n-    }\n-\n-    @Override\n-    public void unsetSerializationLib() {\n-      copyOnWrite();\n-      shared.getSerdeInfo().unsetSerializationLib();\n-    }\n-\n-    @Override\n-    public void setParameters(Map<String, String> parameters) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().setParameters(parameters);\n-    }\n-\n-    @Override\n-    public void unsetParameters() {\n-      copyOnWrite();\n-      shared.getSerdeInfo().unsetParameters();\n-    }\n-\n-    @Override\n-    public void putToParameters(String key, String value) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().putToParameters(key, value);\n+  private void copySkewed() {\n+    if (!skewedCopied) {\n+      skewedCopied = true;\n+      if (super.getSkewedInfo() != null) super.setSkewedInfo(new SkewedInfo(super.getSkewedInfo()));\n     }\n   }\n-\n-  private class SkewWrapper extends SkewedInfo {\n-    SkewWrapper(SkewedInfo skew) {\n-      super(skew);\n-    }\n-\n-    @Override\n-    public void setSkewedColNames(List<String> skewedColNames) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().setSkewedColNames(skewedColNames);\n-    }\n-\n-    @Override\n-    public void unsetSkewedColNames() {\n-      copyOnWrite();\n-      shared.getSkewedInfo().unsetSkewedColNames();\n-    }\n-\n-    @Override\n-    public void addToSkewedColNames(String skewCol) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().addToSkewedColNames(skewCol);\n-    }\n-\n-    @Override\n-    public void setSkewedColValues(List<List<String>> skewedColValues) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().setSkewedColValues(skewedColValues);\n-    }\n-\n-    @Override\n-    public void unsetSkewedColValues() {\n-      copyOnWrite();\n-      shared.getSkewedInfo().unsetSkewedColValues();\n-    }\n-\n-    @Override\n-    public void addToSkewedColValues(List<String> skewedColValue) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().addToSkewedColValues(skewedColValue);\n-    }\n-\n-    @Override\n-    public void setSkewedColValueLocationMaps(Map<List<String>, String> maps) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().setSkewedColValueLocationMaps(maps);\n-    }\n-\n-    @Override\n-    public void unsetSkewedColValueLocationMaps() {\n-      copyOnWrite();\n-      shared.getSkewedInfo().unsetSkewedColValueLocationMaps();\n-    }\n-\n-    @Override\n-    public void putToSkewedColValueLocationMaps(List<String> key, String value) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().putToSkewedColValueLocationMaps(key, value);\n-    }\n-  }\n-\n-  private CopyOnWriteOrderList copySort(List<Order> sort) {\n-    if (orderList == null) {\n-      orderList = new CopyOnWriteOrderList(sort.size());\n-      for (int i = 0; i < sort.size(); i++) {\n-        orderList.secretAdd(new OrderWrapper(i, sort.get(i)));\n-      }\n-    }\n-    return orderList;\n-  }\n-\n-  private class CopyOnWriteOrderList extends ArrayList<Order> {\n-\n-    CopyOnWriteOrderList(int size) {\n-      super(size);\n-    }\n-\n-    private void secretAdd(OrderWrapper order) {\n-      super.add(order);\n-    }\n-\n-    @Override\n-    public boolean add(Order t) {\n-      copyOnWrite();\n-      return shared.getSortCols().add(t);\n-    }\n-\n-    @Override\n-    public boolean remove(Object o) {\n-      copyOnWrite();\n-      return shared.getSortCols().remove(o);\n-    }\n-\n-    @Override\n-    public boolean addAll(Collection<? extends Order> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean addAll(int index, Collection<? extends Order> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean removeAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().removeAll(c);\n-    }\n-\n-    @Override\n-    public boolean retainAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().retainAll(c);\n-    }\n-\n-    @Override\n-    public void clear() {\n-      copyOnWrite();\n-      shared.getSortCols().clear();\n-    }\n-\n-    @Override\n-    public Order set(int index, Order element) {\n-      copyOnWrite();\n-      return shared.getSortCols().set(index, element);\n-    }\n-\n-    @Override\n-    public void add(int index, Order element) {\n-      copyOnWrite();\n-      shared.getSortCols().add(index, element);\n-    }\n-\n-    @Override\n-    public Order remove(int index) {\n-      copyOnWrite();\n-      return shared.getSortCols().remove(index);\n-    }\n-  }\n-\n-  private class OrderWrapper extends Order {\n-    final private int pos;\n-\n-    OrderWrapper(int pos, Order order) {\n-      super(order);\n-      this.pos = pos;\n-    }\n-\n-    @Override\n-    public void setCol(String col) {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).setCol(col);\n-    }\n-\n-    @Override\n-    public void unsetCol() {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).unsetCol();\n-    }\n-\n-    @Override\n-    public void setOrder(int order) {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).setOrder(order);\n-    }\n-\n-    @Override\n-    public void unsetOrder() {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).unsetOrder();\n-    }\n-  }\n-\n-  private CopyOnWriteColList copyCols(List<FieldSchema> cols) {\n-    if (colList == null) {\n-      colList = new CopyOnWriteColList(cols.size());\n-      for (int i = 0; i < cols.size(); i++) {\n-        colList.secretAdd(new FieldSchemaWrapper(i, cols.get(i)));\n-      }\n-    }\n-    return colList;\n-  }\n-\n-  private class CopyOnWriteColList extends ArrayList<FieldSchema> {\n-\n-    CopyOnWriteColList(int size) {\n-      super(size);\n-    }\n-\n-    private void secretAdd(FieldSchemaWrapper col) {\n-      super.add(col);\n-    }\n-\n-    @Override\n-    public boolean add(FieldSchema t) {\n-      copyOnWrite();\n-      return shared.getCols().add(t);\n-    }\n-\n-    @Override\n-    public boolean remove(Object o) {\n-      copyOnWrite();\n-      return shared.getCols().remove(o);\n-    }\n-\n-    @Override\n-    public boolean addAll(Collection<? extends FieldSchema> c) {\n-      copyOnWrite();\n-      return shared.getCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean addAll(int index, Collection<? extends FieldSchema> c) {\n-      copyOnWrite();\n-      return shared.getCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean removeAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getCols().removeAll(c);\n-    }\n-\n-    @Override\n-    public boolean retainAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getCols().retainAll(c);\n-    }\n-\n-    @Override\n-    public void clear() {\n-      copyOnWrite();\n-      shared.getCols().clear();\n-    }\n-\n-    @Override\n-    public FieldSchema set(int index, FieldSchema element) {\n-      copyOnWrite();\n-      return shared.getCols().set(index, element);\n-    }\n-\n-    @Override\n-    public void add(int index, FieldSchema element) {\n-      copyOnWrite();\n-      shared.getCols().add(index, element);\n-    }\n-\n-    @Override\n-    public FieldSchema remove(int index) {\n-      copyOnWrite();\n-      return shared.getCols().remove(index);\n-    }\n-  }\n-\n-  private class FieldSchemaWrapper extends FieldSchema {\n-    final private int pos;\n-\n-    FieldSchemaWrapper(int pos, FieldSchema col) {\n-      super(col);\n-      this.pos = pos;\n-    }\n-\n-    @Override\n-    public void setName(String name) {\n-      copyOnWrite();\n-      shared.getCols().get(pos).setName(name);\n-    }\n-\n-    @Override\n-    public void unsetName() {\n-      copyOnWrite();\n-      shared.getCols().get(pos).unsetName();\n-    }\n-\n-    @Override\n-    public void setType(String type) {\n-      copyOnWrite();\n-      shared.getCols().get(pos).setType(type);\n-    }\n-\n-    @Override\n-    public void unsetType() {\n-      copyOnWrite();\n-      shared.getCols().get(pos).unsetType();\n-    }\n-\n-    @Override\n-    public void setComment(String comment) {\n-      copyOnWrite();\n-      shared.getCols().get(pos).setComment(comment);\n-    }\n-\n-    @Override\n-    public void unsetComment() {\n-      copyOnWrite();\n-      shared.getCols().get(pos).unsetComment();\n-    }\n-  }\n-\n-  private CopyOnWriteBucketList copyBucketCols(List<String> cols) {\n-    if (bucketList == null) {\n-      bucketList = new CopyOnWriteBucketList(cols);\n-    }\n-    return bucketList;\n-  }\n-\n-  private class CopyOnWriteBucketList extends ArrayList<String> {\n-\n-    CopyOnWriteBucketList(Collection<String> c) {\n-      super(c);\n-    }\n-\n-    private void secretAdd(String col) {\n-      super.add(col);\n-    }\n-\n-    @Override\n-    public boolean add(String t) {\n-      copyOnWrite();\n-      return shared.getBucketCols().add(t);\n-    }\n-\n-    @Override\n-    public boolean remove(Object o) {\n-      copyOnWrite();\n-      return shared.getBucketCols().remove(o);\n-    }\n-\n-    @Override\n-    public boolean addAll(Collection<? extends String> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean addAll(int index, Collection<? extends String> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean removeAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().removeAll(c);\n-    }\n-\n-    @Override\n-    public boolean retainAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().retainAll(c);\n-    }\n-\n-    @Override\n-    public void clear() {\n-      copyOnWrite();\n-      shared.getBucketCols().clear();\n-    }\n-\n-    @Override\n-    public String set(int index, String element) {\n-      copyOnWrite();\n-      return shared.getBucketCols().set(index, element);\n-    }\n-\n-    @Override\n-    public void add(int index, String element) {\n-      copyOnWrite();\n-      shared.getBucketCols().add(index, element);\n-    }\n-\n-    @Override\n-    public String remove(int index) {\n-      copyOnWrite();\n-      return shared.getBucketCols().remove(index);\n-    }\n-  }\n-\n }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java"}, {"additions": 74, "raw_url": "https://github.com/apache/hive/raw/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java", "blob_url": "https://github.com/apache/hive/blob/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java", "sha": "fdfb6d12cc6018cdf492bb2838d051419612c5ef", "changes": 122, "status": "modified", "deletions": 48, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java?ref=10cfdd96b69084bf0da7b864113d073006147e77", "patch": "@@ -20,13 +20,16 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.metastore.api.Order;\n import org.apache.hadoop.hive.metastore.api.SerDeInfo;\n import org.apache.hadoop.hive.metastore.api.SkewedInfo;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n \n \n@@ -37,30 +40,6 @@\n   private static final Log LOG = LogFactory.getLog(TestHBaseStore.class.getName());\n \n \n-  @Test\n-  public void location() {\n-    StorageDescriptor sd = new StorageDescriptor();\n-    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n-    ssd.setLocation(\"here\");\n-    ssd.setShared(sd);\n-    ssd.setLocation(\"there\");\n-    Assert.assertTrue(sd == ssd.getShared());\n-  }\n-\n-  @Test\n-  public void changeOnInputFormat() {\n-    StorageDescriptor sd = new StorageDescriptor();\n-    sd.setInputFormat(\"input\");\n-    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n-    ssd.setShared(sd);\n-    Assert.assertEquals(\"input\", ssd.getInputFormat());\n-    ssd.setInputFormat(\"different\");\n-    Assert.assertFalse(sd == ssd.getShared());\n-    Assert.assertEquals(\"input\", sd.getInputFormat());\n-    Assert.assertEquals(\"different\", ssd.getInputFormat());\n-    Assert.assertEquals(\"input\", sd.getInputFormat());\n-  }\n-\n   @Test\n   public void changeOnSerde() {\n     StorageDescriptor sd = new StorageDescriptor();\n@@ -69,32 +48,33 @@ public void changeOnSerde() {\n     sd.setSerdeInfo(serde);\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(\"serde\", ssd.getSerdeInfo().getName());\n     ssd.getSerdeInfo().setName(\"different\");\n-    Assert.assertFalse(sd == ssd.getShared());\n+    Assert.assertFalse(sd.getSerdeInfo() == ssd.getSerdeInfo());\n     Assert.assertEquals(\"serde\", serde.getName());\n     Assert.assertEquals(\"different\", ssd.getSerdeInfo().getName());\n     Assert.assertEquals(\"serde\", sd.getSerdeInfo().getName());\n   }\n \n   @Test\n-  public void multipleChangesDontCauseMultipleCopies() {\n+  public void changeOnSkewed() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    SkewedInfo skew = new SkewedInfo();\n+    sd.setSkewedInfo(skew);\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    ssd.setSkewedInfo(new SkewedInfo());\n+    Assert.assertFalse(sd.getSkewedInfo() == ssd.getSkewedInfo());\n+  }\n+\n+  @Test\n+  public void changeOnUnset() {\n     StorageDescriptor sd = new StorageDescriptor();\n-    sd.setInputFormat(\"input\");\n-    sd.setOutputFormat(\"output\");\n+    SkewedInfo skew = new SkewedInfo();\n+    sd.setSkewedInfo(skew);\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(\"input\", ssd.getInputFormat());\n-    ssd.setInputFormat(\"different\");\n-    Assert.assertFalse(sd == ssd.getShared());\n-    Assert.assertEquals(\"input\", sd.getInputFormat());\n-    Assert.assertEquals(\"different\", ssd.getInputFormat());\n-    StorageDescriptor keep = ssd.getShared();\n-    ssd.setOutputFormat(\"different_output\");\n-    Assert.assertEquals(\"different\", ssd.getInputFormat());\n-    Assert.assertEquals(\"different_output\", ssd.getOutputFormat());\n-    Assert.assertEquals(\"output\", sd.getOutputFormat());\n-    Assert.assertTrue(keep == ssd.getShared());\n+    ssd.unsetSkewedInfo();\n+    Assert.assertFalse(sd.getSkewedInfo() == ssd.getSkewedInfo());\n   }\n \n   @Test\n@@ -103,25 +83,71 @@ public void changeOrder() {\n     sd.addToSortCols(new Order(\"fred\", 1));\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(1, ssd.getSortCols().get(0).getOrder());\n     ssd.getSortCols().get(0).setOrder(2);\n-    Assert.assertFalse(sd == ssd.getShared());\n+    Assert.assertFalse(sd.getSortCols() == ssd.getSortCols());\n     Assert.assertEquals(2, ssd.getSortCols().get(0).getOrder());\n     Assert.assertEquals(1, sd.getSortCols().get(0).getOrder());\n   }\n \n   @Test\n-  public void changeOrderList() {\n+  public void unsetOrder() {\n     StorageDescriptor sd = new StorageDescriptor();\n     sd.addToSortCols(new Order(\"fred\", 1));\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(1, ssd.getSortCols().get(0).getOrder());\n-    List<Order> list = ssd.getSortCols();\n-    list.add(new Order(\"bob\", 2));\n-    Assert.assertFalse(sd == ssd.getShared());\n-    Assert.assertEquals(2, ssd.getSortColsSize());\n+    ssd.unsetSortCols();\n+    Assert.assertFalse(sd.getSortCols() == ssd.getSortCols());\n+    Assert.assertEquals(0, ssd.getSortColsSize());\n     Assert.assertEquals(1, sd.getSortColsSize());\n   }\n \n+  @Test\n+  public void changeBucketList() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToBucketCols(new String(\"fred\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    List<String> list = ssd.getBucketCols();\n+    list.add(new String(\"bob\"));\n+    Assert.assertFalse(sd.getBucketCols() == ssd.getBucketCols());\n+    Assert.assertEquals(2, ssd.getBucketColsSize());\n+    Assert.assertEquals(1, sd.getBucketColsSize());\n+  }\n+\n+  @Test\n+  public void addToColList() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToCols(new FieldSchema(\"fred\", \"string\", \"\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    ssd.addToCols(new FieldSchema(\"joe\", \"int\", \"\"));\n+    Assert.assertFalse(sd.getCols() == ssd.getCols());\n+    Assert.assertEquals(2, ssd.getColsSize());\n+    Assert.assertEquals(1, sd.getColsSize());\n+  }\n+\n+  @Test\n+  public void colIterator() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToCols(new FieldSchema(\"fred\", \"string\", \"\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    Iterator<FieldSchema> iter = ssd.getColsIterator();\n+    Assert.assertTrue(iter.hasNext());\n+    Assert.assertEquals(\"fred\", iter.next().getName());\n+    Assert.assertFalse(sd.getCols() == ssd.getCols());\n+  }\n+\n+  @Test\n+  public void setReadOnly() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToCols(new FieldSchema(\"fred\", \"string\", \"\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    ssd.setReadOnly();\n+    List<FieldSchema> cols = ssd.getCols();\n+    Assert.assertEquals(1, cols.size());\n+    Assert.assertTrue(sd.getCols() == ssd.getCols());\n+  }\n+\n }", "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cdf0d588789ba0155d29b402c8a5bbd9be1cdab7", "parent": "https://github.com/apache/hive/commit/0fc9450ee92dbe97a6156e300a918675e9909825", "message": "HIVE-9952 : fix NPE in CorrelationUtilities (Alexander Pivovarov, reviewed by Sergey Shelukhin)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1667612 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_176", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/cdf0d588789ba0155d29b402c8a5bbd9be1cdab7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java", "blob_url": "https://github.com/apache/hive/blob/cdf0d588789ba0155d29b402c8a5bbd9be1cdab7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java", "sha": "64bef2163b66d896bfb96a15ddc4d8d1b53613fb", "changes": 9, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java?ref=cdf0d588789ba0155d29b402c8a5bbd9be1cdab7", "patch": "@@ -446,14 +446,15 @@ protected static void removeReduceSinkForGroupBy(ReduceSinkOperator cRS, GroupBy\n     }\n   }\n \n-  /** throw a exception if the input operator is null\n+  /**\n+   * Throws an exception if the input operator is null\n+   *\n    * @param operator\n-   * @throws HiveException\n+   * @throws SemanticException if the input operator is null\n    */\n   protected static void isNullOperator(Operator<?> operator) throws SemanticException {\n     if (operator == null) {\n-      throw new SemanticException(\"Operator \" + operator.getName() + \" (ID: \" +\n-          operator.getIdentifier() + \") is null.\");\n+      throw new SemanticException(\"Operator is null.\");\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/CorrelationUtilities.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/626a9daecf884d0a925724612e8a41fffc167316", "parent": "https://github.com/apache/hive/commit/2b2ba59fdd805d01146d37d4287b6658caa41a6e", "message": "HIVE-9936: fix potential NPE in DefaultUDAFEvaluatorResolver (Alexander Pivovarov via Jason Dere)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1667082 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_177", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/626a9daecf884d0a925724612e8a41fffc167316/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDAFEvaluatorResolver.java", "blob_url": "https://github.com/apache/hive/blob/626a9daecf884d0a925724612e8a41fffc167316/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDAFEvaluatorResolver.java", "sha": "ec9f37e29f88e667156d2e5c0c348275c0e071f0", "changes": 5, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDAFEvaluatorResolver.java?ref=626a9daecf884d0a925724612e8a41fffc167316", "patch": "@@ -20,7 +20,6 @@\n \n import java.lang.reflect.Method;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n \n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n@@ -92,12 +91,12 @@ public DefaultUDAFEvaluatorResolver(Class<? extends UDAF> udafClass) {\n         if (found == -1) {\n           found = i;\n         } else {\n-          throw new AmbiguousMethodException(udafClass, null, null); \n+          throw new AmbiguousMethodException(udafClass, argClasses, mList);\n         }\n       }\n     }\n     assert (found != -1);\n-    \n+\n     return cList.get(found);\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DefaultUDAFEvaluatorResolver.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "parent": "https://github.com/apache/hive/commit/5461b9bbde3cf960ffbfc399491a55de7585fd79", "message": "HIVE-9886: Hive on tez: NPE when converting join to SMB in sub-query (Vikram Dixit K, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1665380 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_178", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/itests/src/test/resources/testconfiguration.properties", "sha": "91dcc03381d317decaad329031bb31b0b61e8bc4", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "patch": "@@ -305,7 +305,8 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_smb_main.q,\\\n   tez_smb_1.q,\\\n   vectorized_dynamic_partition_pruning.q,\\\n-  tez_multi_union.q\n+  tez_multi_union.q,\\\n+  tez_join.q\n \n encrypted.query.files=encryption_join_unencrypted_tbl.q,\\\n   encryption_insert_partition_static.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 38, "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "sha": "ce4312029ebc0cc01d3cfcca60cc9cc10a8e0f51", "changes": 75, "status": "modified", "deletions": 37, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "patch": "@@ -111,7 +111,7 @@\n         }\n \n         if (parentOp instanceof ReduceSinkOperator) {\n-          ReduceSinkOperator rs = (ReduceSinkOperator)parentOp;\n+          ReduceSinkOperator rs = (ReduceSinkOperator) parentOp;\n           estimatedBuckets = (estimatedBuckets < rs.getConf().getNumReducers()) ?\n               rs.getConf().getNumReducers() : estimatedBuckets;\n         }\n@@ -133,10 +133,10 @@\n       if (retval == null) {\n         return retval;\n       } else {\n-          // only case is full outer join with SMB enabled which is not possible. Convert to regular\n-          // join.\n-          convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n-          return null;\n+        // only case is full outer join with SMB enabled which is not possible. Convert to regular\n+        // join.\n+        convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n+        return null;\n       }\n     }\n \n@@ -160,8 +160,10 @@\n     }\n \n     MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, mapJoinConversionPos);\n-    // map join operator by default has no bucket cols\n-    mapJoinOp.setOpTraits(new OpTraits(null, -1, null));\n+    // map join operator by default has no bucket cols and num of reduce sinks\n+    // reduced by 1\n+    mapJoinOp\n+        .setOpTraits(new OpTraits(null, -1, null, joinOp.getOpTraits().getNumReduceSinks()));\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     // propagate this change till the next RS\n     for (Operator<? extends OperatorDesc> childOp : mapJoinOp.getChildOperators()) {\n@@ -176,7 +178,8 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // we cannot convert to bucket map join, we cannot convert to\n     // map join either based on the size. Check if we can convert to SMB join.\n-    if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {\n+    if ((context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false)\n+        || (joinOp.getOpTraits().getNumReduceSinks() >= 2)) {\n       convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n       return null;\n     }\n@@ -221,7 +224,7 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       convertJoinSMBJoin(joinOp, context, pos, 0, false, false);\n     }\n     return null;\n-}\n+  }\n \n   // replaces the join operator with a new CommonJoinOperator, removes the\n   // parent reduce sinks\n@@ -240,9 +243,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n           new MapJoinDesc(\n                   MapJoinProcessor.getKeys(joinOp.getConf().isLeftInputJoin(),\n                   joinOp.getConf().getBaseSrc(), joinOp).getSecond(),\n-              null, joinDesc.getExprs(), null, null,\n-              joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n-              joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n+                  null, joinDesc.getExprs(), null, null,\n+                  joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n+                  joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n       mapJoinDesc.setNullSafes(joinDesc.getNullSafes());\n       mapJoinDesc.setFilterMap(joinDesc.getFilterMap());\n       mapJoinDesc.resetOrder();\n@@ -251,9 +254,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n     CommonMergeJoinOperator mergeJoinOp =\n         (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n             isSubQuery, mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n-    OpTraits opTraits =\n-        new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n-            .getSortCols());\n+    int numReduceSinks = joinOp.getOpTraits().getNumReduceSinks();\n+    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp\n+        .getOpTraits().getSortCols(), numReduceSinks);\n     mergeJoinOp.setOpTraits(opTraits);\n     mergeJoinOp.setStatistics(joinOp.getStatistics());\n \n@@ -289,8 +292,7 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n \n     if (adjustParentsChildren) {\n       mergeJoinOp.getConf().setGenJoinKeys(true);\n-      List<Operator<? extends OperatorDesc>> newParentOpList =\n-          new ArrayList<Operator<? extends OperatorDesc>>();\n+      List<Operator<? extends OperatorDesc>> newParentOpList = new ArrayList<Operator<? extends OperatorDesc>>();\n       for (Operator<? extends OperatorDesc> parentOp : mergeJoinOp.getParentOperators()) {\n         for (Operator<? extends OperatorDesc> grandParentOp : parentOp.getParentOperators()) {\n           grandParentOp.getChildOperators().remove(parentOp);\n@@ -328,7 +330,8 @@ private void setAllChildrenTraitsToNull(Operator<? extends OperatorDesc> current\n     if (currentOp instanceof ReduceSinkOperator) {\n       return;\n     }\n-    currentOp.setOpTraits(new OpTraits(null, -1, null));\n+    currentOp.setOpTraits(new OpTraits(null, -1, null,\n+        currentOp.getOpTraits().getNumReduceSinks()));\n     for (Operator<? extends OperatorDesc> childOp : currentOp.getChildOperators()) {\n       if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof GroupByOperator)) {\n         break;\n@@ -351,7 +354,7 @@ private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcCon\n \n     // we can set the traits for this join operator\n     OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n-        tezBucketJoinProcCtx.getNumBuckets(), null);\n+        tezBucketJoinProcCtx.getNumBuckets(), null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);\n@@ -377,8 +380,7 @@ private boolean checkConvertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcCont\n \n     ReduceSinkOperator bigTableRS =\n         (ReduceSinkOperator) joinOp.getParentOperators().get(bigTablePosition);\n-    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits()\n-            .getNumBuckets();\n+    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits().getNumBuckets();\n \n     // the sort and bucket cols have to match on both sides for this\n     // transformation of the join operation\n@@ -425,13 +427,12 @@ private void setNumberOfBucketsOnChildren(Operator<? extends OperatorDesc> curre\n   }\n \n   /*\n-   * If the parent reduce sink of the big table side has the same emit key cols\n-   * as its parent, we can create a bucket map join eliminating the reduce sink.\n+   * If the parent reduce sink of the big table side has the same emit key cols as its parent, we\n+   * can create a bucket map join eliminating the reduce sink.\n    */\n   private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n       OptimizeTezProcContext context, int bigTablePosition,\n-      TezBucketJoinProcCtx tezBucketJoinProcCtx)\n-  throws SemanticException {\n+      TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // bail on mux-operator because mux operator masks the emit keys of the\n     // constituent reduce sinks\n     if (!(joinOp.getParentOperators().get(0) instanceof ReduceSinkOperator)) {\n@@ -453,8 +454,8 @@ private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n     }\n \n     /*\n-     * this is the case when the big table is a sub-query and is probably\n-     * already bucketed by the join column in say a group by operation\n+     * this is the case when the big table is a sub-query and is probably already bucketed by the\n+     * join column in say a group by operation\n      */\n     boolean isSubQuery = false;\n     if (numBuckets < 0) {\n@@ -492,7 +493,8 @@ private boolean checkColEquality(List<List<String>> grandParentColNames,\n           // all columns need to be at least a subset of the parentOfParent's bucket cols\n           ExprNodeDesc exprNodeDesc = colExprMap.get(colName);\n           if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n-            if (((ExprNodeColumnDesc)exprNodeDesc).getColumn().equals(listBucketCols.get(colCount))) {\n+            if (((ExprNodeColumnDesc) exprNodeDesc).getColumn()\n+                .equals(listBucketCols.get(colCount))) {\n               colCount++;\n             } else {\n               break;\n@@ -562,14 +564,13 @@ public int getMapJoinConversionPos(JoinOperator joinOp, OptimizeTezProcContext c\n \n       Statistics currInputStat = parentOp.getStatistics();\n       if (currInputStat == null) {\n-        LOG.warn(\"Couldn't get statistics from: \"+parentOp);\n+        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n         return -1;\n       }\n \n       long inputSize = currInputStat.getDataSize();\n-      if ((bigInputStat == null) ||\n-          ((bigInputStat != null) &&\n-          (inputSize > bigInputStat.getDataSize()))) {\n+      if ((bigInputStat == null)\n+          || ((bigInputStat != null) && (inputSize > bigInputStat.getDataSize()))) {\n \n         if (bigTableFound) {\n           // cannot convert to map join; we've already chosen a big table\n@@ -639,11 +640,11 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n       }\n     }\n \n-    //can safely convert the join to a map join.\n+    // can safely convert the join to a map join.\n     MapJoinOperator mapJoinOp =\n         MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n-                joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n-                joinOp.getConf().getMapAliases(), bigTablePosition, true);\n+            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n+            joinOp.getConf().getMapAliases(), bigTablePosition, true);\n \n     Operator<? extends OperatorDesc> parentBigTableOp =\n         mapJoinOp.getParentOperators().get(bigTablePosition);\n@@ -667,7 +668,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n             parentBigTableOp.getParentOperators().get(0));\n       }\n       parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n-      for (Operator<? extends OperatorDesc> op : mapJoinOp.getParentOperators()) {\n+      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n         if (!(op.getChildOperators().contains(mapJoinOp))) {\n           op.getChildOperators().add(mapJoinOp);\n         }\n@@ -681,7 +682,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n   private boolean hasDynamicPartitionBroadcast(Operator<?> parent) {\n     boolean hasDynamicPartitionPruning = false;\n \n-    for (Operator<?> op: parent.getChildOperators()) {\n+    for (Operator<?> op : parent.getChildOperators()) {\n       while (op != null) {\n         if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n           // found dynamic partition pruning operator", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java"}, {"additions": 62, "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java", "sha": "62428dbdd61d28b63d59388f00b3c0c0d328d593", "changes": 96, "status": "modified", "deletions": 34, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "patch": "@@ -82,7 +82,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   * Reduce sink operator is the de-facto operator \n+   * Reduce sink operator is the de-facto operator\n    * for determining keyCols (emit keys of a map phase)\n    */\n   public static class ReduceSinkRule implements NodeProcessor {\n@@ -106,34 +106,37 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       listBucketCols.add(bucketCols);\n       int numBuckets = -1;\n+      int numReduceSinks = 1;\n       OpTraits parentOpTraits = rs.getParentOperators().get(0).getConf().getOpTraits();\n       if (parentOpTraits != null) {\n         numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks += parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols, numReduceSinks);\n       rs.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   /*\n-   * Table scan has the table object and pruned partitions that has information such as\n-   * bucketing, sorting, etc. that is used later for optimization.\n+   * Table scan has the table object and pruned partitions that has information\n+   * such as bucketing, sorting, etc. that is used later for optimization.\n    */\n   public static class TableScanRule implements NodeProcessor {\n \n-    public boolean checkBucketedTable(Table tbl, \n-        ParseContext pGraphContext,\n+    public boolean checkBucketedTable(Table tbl, ParseContext pGraphContext,\n         PrunedPartitionList prunedParts) throws SemanticException {\n \n       if (tbl.isPartitioned()) {\n         List<Partition> partitions = prunedParts.getNotDeniedPartns();\n         // construct a mapping of (Partition->bucket file names) and (Partition -> bucket number)\n         if (!partitions.isEmpty()) {\n           for (Partition p : partitions) {\n-            List<String> fileNames =\n-                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), pGraphContext);\n-            // The number of files for the table should be same as number of buckets.\n+            List<String> fileNames = \n+                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), \n+                    pGraphContext);\n+            // The number of files for the table should be same as number of\n+            // buckets.\n             int bucketCount = p.getBucketCount();\n \n             if (fileNames.size() != 0 && fileNames.size() != bucketCount) {\n@@ -143,8 +146,9 @@ public boolean checkBucketedTable(Table tbl,\n         }\n       } else {\n \n-        List<String> fileNames =\n-            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), pGraphContext);\n+        List<String> fileNames = \n+            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), \n+                pGraphContext);\n         Integer num = new Integer(tbl.getNumBuckets());\n \n         // The number of files for the table should be same as number of buckets.\n@@ -183,7 +187,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         }\n         sortedColsList.add(sortCols);\n       }\n-      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList);\n+      // num reduce sinks hardcoded to 0 because TS has no parents\n+      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList, 0);\n       ts.setOpTraits(opTraits);\n       return null;\n     }\n@@ -208,17 +213,22 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       }\n \n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = gbyOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n+      }\n       listBucketCols.add(gbyKeys);\n-      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols, numReduceSinks);\n       gbyOp.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   public static class SelectRule implements NodeProcessor {\n \n-    public List<List<String>> getConvertedColNames(List<List<String>> parentColNames,\n-        SelectOperator selOp) {\n+    public List<List<String>> getConvertedColNames(\n+        List<List<String>> parentColNames, SelectOperator selOp) {\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       if (selOp.getColumnExprMap() != null) {\n         if (parentColNames != null) {\n@@ -244,8 +254,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      SelectOperator selOp = (SelectOperator)nd;\n-      List<List<String>> parentBucketColNames =\n+      SelectOperator selOp = (SelectOperator) nd;\n+      List<List<String>> parentBucketColNames = \n           selOp.getParentOperators().get(0).getOpTraits().getBucketColNames();\n \n       List<List<String>> listBucketCols = null;\n@@ -254,18 +264,21 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         if (parentBucketColNames != null) {\n           listBucketCols = getConvertedColNames(parentBucketColNames, selOp);\n         }\n-        List<List<String>> parentSortColNames = selOp.getParentOperators().get(0).getOpTraits()\n-            .getSortCols();\n+        List<List<String>> parentSortColNames = \n+            selOp.getParentOperators().get(0).getOpTraits().getSortCols();\n         if (parentSortColNames != null) {\n           listSortCols = getConvertedColNames(parentSortColNames, selOp);\n         }\n       }\n \n       int numBuckets = -1;\n-      if (selOp.getParentOperators().get(0).getOpTraits() != null) {\n-        numBuckets = selOp.getParentOperators().get(0).getOpTraits().getNumBuckets();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = selOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols, numReduceSinks);\n       selOp.setOpTraits(opTraits);\n       return null;\n     }\n@@ -276,26 +289,31 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      JoinOperator joinOp = (JoinOperator)nd;\n+      JoinOperator joinOp = (JoinOperator) nd;\n       List<List<String>> bucketColsList = new ArrayList<List<String>>();\n       List<List<String>> sortColsList = new ArrayList<List<String>>();\n       byte pos = 0;\n+      int numReduceSinks = 0; // will be set to the larger of the parents\n       for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n         if (!(parentOp instanceof ReduceSinkOperator)) {\n           // can be mux operator\n           break;\n         }\n-        ReduceSinkOperator rsOp = (ReduceSinkOperator)parentOp;\n+        ReduceSinkOperator rsOp = (ReduceSinkOperator) parentOp;\n         if (rsOp.getOpTraits() == null) {\n           ReduceSinkRule rsRule = new ReduceSinkRule();\n           rsRule.process(rsOp, stack, procCtx, nodeOutputs);\n         }\n-        bucketColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getBucketColNames(), pos));\n-        sortColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getSortCols(), pos));\n+        OpTraits parentOpTraits = rsOp.getOpTraits();\n+        bucketColsList.add(getOutputColNames(joinOp, parentOpTraits.getBucketColNames(), pos));\n+        sortColsList.add(getOutputColNames(joinOp, parentOpTraits.getSortCols(), pos));\n+        if (parentOpTraits.getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOpTraits.getNumReduceSinks();\n+        }\n         pos++;\n       }\n \n-      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList));\n+      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList, numReduceSinks));\n       return null;\n     }\n \n@@ -311,7 +329,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         for (String colName : colNames) {\n           for (ExprNodeDesc exprNode : joinOp.getConf().getExprs().get(pos)) {\n             if (exprNode instanceof ExprNodeColumnDesc) {\n-              if(((ExprNodeColumnDesc)(exprNode)).getColumn().equals(colName)) {\n+              if (((ExprNodeColumnDesc) (exprNode)).getColumn().equals(colName)) {\n                 for (Entry<String, ExprNodeDesc> entry : joinOp.getColumnExprMap().entrySet()) {\n                   if (entry.getValue().isSame(exprNode)) {\n                     bucketColNames.add(entry.getKey());\n@@ -338,20 +356,30 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   *  When we have operators that have multiple parents, it is not\n-   *  clear which parent's traits we need to propagate forward.\n+   * When we have operators that have multiple parents, it is not clear which\n+   * parent's traits we need to propagate forward.\n    */\n   public static class MultiParentRule implements NodeProcessor {\n \n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      OpTraits opTraits = new OpTraits(null, -1, null);\n       @SuppressWarnings(\"unchecked\")\n-      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>)nd;\n+      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>) nd;\n+\n+      int numReduceSinks = 0;\n+      for (Operator<?> parentOp : operator.getParentOperators()) {\n+        if (parentOp.getOpTraits() == null) {\n+          continue;\n+        }\n+        if (parentOp.getOpTraits().getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOp.getOpTraits().getNumReduceSinks();\n+        }\n+      }\n+      OpTraits opTraits = new OpTraits(null, -1, null, numReduceSinks);\n       operator.setOpTraits(opTraits);\n       return null;\n-    } \n+    }\n   }\n \n   public static NodeProcessor getTableScanRule() {\n@@ -361,7 +389,7 @@ public static NodeProcessor getTableScanRule() {\n   public static NodeProcessor getReduceSinkRule() {\n     return new ReduceSinkRule();\n   }\n-  \n+\n   public static NodeProcessor getSelectRule() {\n     return new SelectRule();\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java", "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java", "sha": "a455175efccf401b2286e32e731bc970de768f90", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "patch": "@@ -103,7 +103,7 @@\n     }\n \n     // we can set the traits for this join operator\n-    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null);\n+    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java", "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java", "sha": "a687a3d0b9ae439066a09550becc75be2bb2327e", "changes": 13, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "patch": "@@ -25,11 +25,14 @@\n   List<List<String>> bucketColNames;\n   List<List<String>> sortColNames;\n   int numBuckets;\n+  int numReduceSinks;\n \n-  public OpTraits(List<List<String>> bucketColNames, int numBuckets, List<List<String>> sortColNames) {\n+  public OpTraits(List<List<String>> bucketColNames, int numBuckets,\n+      List<List<String>> sortColNames, int numReduceSinks) {\n     this.bucketColNames = bucketColNames;\n     this.numBuckets = numBuckets;\n     this.sortColNames = sortColNames;\n+    this.numReduceSinks = numReduceSinks;\n   }\n \n   public List<List<String>> getBucketColNames() {\n@@ -55,4 +58,12 @@ public void setSortColNames(List<List<String>> sortColNames) {\n   public List<List<String>> getSortCols() {\n     return sortColNames;\n   }\n+\n+  public void setNumReduceSinks(int numReduceSinks) {\n+    this.numReduceSinks = numReduceSinks;\n+  }\n+\n+  public int getNumReduceSinks() {\n+    return this.numReduceSinks;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java"}, {"additions": 43, "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/queries/clientpositive/tez_join.q", "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/queries/clientpositive/tez_join.q", "sha": "d35ec835c74dc925148a46ceda908a5e315006db", "changes": 43, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_join.q?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "patch": "@@ -0,0 +1,43 @@\n+set hive.auto.convert.sortmerge.join = true;\n+\n+create table t1(\n+id string,\n+od string);\n+\n+create table t2(\n+id string,\n+od string);\n+\n+explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;\n+\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;\n+\n+explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;\n+\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;", "filename": "ql/src/test/queries/clientpositive/tez_join.q"}, {"additions": 320, "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/results/clientpositive/tez/tez_join.q.out", "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/results/clientpositive/tez/tez_join.q.out", "sha": "a051dc7cbe006aa99204a4a6f755bcd431d4649c", "changes": 320, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_join.q.out?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f", "patch": "@@ -0,0 +1,320 @@\n+PREHOOK: query: create table t1(\n+id string,\n+od string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@t1\n+POSTHOOK: query: create table t1(\n+id string,\n+od string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@t1\n+PREHOOK: query: create table t2(\n+id string,\n+od string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@t2\n+POSTHOOK: query: create table t2(\n+id string,\n+od string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@t2\n+PREHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n+        Reducer 5 <- Map 4 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: id (type: string), od (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: id (type: string), od (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Select Operator\n+                expressions: KEY.reducesinkkey0 (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Merge Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col0 (type: string)\n+                  1 _col0 (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Filter Operator\n+                  predicate: (_col0 = _col1) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 5 \n+            Reduce Operator Tree:\n+              Select Operator\n+                expressions: KEY.reducesinkkey0 (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@t1\n+PREHOOK: Input: default@t2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@t1\n+POSTHOOK: Input: default@t2\n+#### A masked pattern was here ####\n+PREHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n+        Reducer 5 <- Map 4 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count()\n+                      keys: id (type: string), od (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        value expressions: _col2 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count()\n+                      keys: id (type: string), od (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        value expressions: _col2 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                keys: KEY._col0 (type: string), KEY._col1 (type: string)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: string)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Merge Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col0 (type: string)\n+                  1 _col0 (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Filter Operator\n+                  predicate: (_col0 = _col1) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 5 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                keys: KEY._col0 (type: string), KEY._col1 (type: string)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: string)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@t1\n+PREHOOK: Input: default@t2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@t1\n+POSTHOOK: Input: default@t2\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/tez/tez_join.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a", "parent": "https://github.com/apache/hive/commit/b63aeeebbc8f0f5570570b3790045bea75ed4848", "message": "HIVE-9886: Hive on tez: NPE when converting join to SMB in sub-query (Vikram Dixit K, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1665372 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_179", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/itests/src/test/resources/testconfiguration.properties", "sha": "91dcc03381d317decaad329031bb31b0b61e8bc4", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a", "patch": "@@ -305,7 +305,8 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_smb_main.q,\\\n   tez_smb_1.q,\\\n   vectorized_dynamic_partition_pruning.q,\\\n-  tez_multi_union.q\n+  tez_multi_union.q,\\\n+  tez_join.q\n \n encrypted.query.files=encryption_join_unencrypted_tbl.q,\\\n   encryption_insert_partition_static.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 38, "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "sha": "ce4312029ebc0cc01d3cfcca60cc9cc10a8e0f51", "changes": 75, "status": "modified", "deletions": 37, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a", "patch": "@@ -111,7 +111,7 @@\n         }\n \n         if (parentOp instanceof ReduceSinkOperator) {\n-          ReduceSinkOperator rs = (ReduceSinkOperator)parentOp;\n+          ReduceSinkOperator rs = (ReduceSinkOperator) parentOp;\n           estimatedBuckets = (estimatedBuckets < rs.getConf().getNumReducers()) ?\n               rs.getConf().getNumReducers() : estimatedBuckets;\n         }\n@@ -133,10 +133,10 @@\n       if (retval == null) {\n         return retval;\n       } else {\n-          // only case is full outer join with SMB enabled which is not possible. Convert to regular\n-          // join.\n-          convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n-          return null;\n+        // only case is full outer join with SMB enabled which is not possible. Convert to regular\n+        // join.\n+        convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n+        return null;\n       }\n     }\n \n@@ -160,8 +160,10 @@\n     }\n \n     MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, mapJoinConversionPos);\n-    // map join operator by default has no bucket cols\n-    mapJoinOp.setOpTraits(new OpTraits(null, -1, null));\n+    // map join operator by default has no bucket cols and num of reduce sinks\n+    // reduced by 1\n+    mapJoinOp\n+        .setOpTraits(new OpTraits(null, -1, null, joinOp.getOpTraits().getNumReduceSinks()));\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     // propagate this change till the next RS\n     for (Operator<? extends OperatorDesc> childOp : mapJoinOp.getChildOperators()) {\n@@ -176,7 +178,8 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // we cannot convert to bucket map join, we cannot convert to\n     // map join either based on the size. Check if we can convert to SMB join.\n-    if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {\n+    if ((context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false)\n+        || (joinOp.getOpTraits().getNumReduceSinks() >= 2)) {\n       convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n       return null;\n     }\n@@ -221,7 +224,7 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       convertJoinSMBJoin(joinOp, context, pos, 0, false, false);\n     }\n     return null;\n-}\n+  }\n \n   // replaces the join operator with a new CommonJoinOperator, removes the\n   // parent reduce sinks\n@@ -240,9 +243,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n           new MapJoinDesc(\n                   MapJoinProcessor.getKeys(joinOp.getConf().isLeftInputJoin(),\n                   joinOp.getConf().getBaseSrc(), joinOp).getSecond(),\n-              null, joinDesc.getExprs(), null, null,\n-              joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n-              joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n+                  null, joinDesc.getExprs(), null, null,\n+                  joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n+                  joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n       mapJoinDesc.setNullSafes(joinDesc.getNullSafes());\n       mapJoinDesc.setFilterMap(joinDesc.getFilterMap());\n       mapJoinDesc.resetOrder();\n@@ -251,9 +254,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n     CommonMergeJoinOperator mergeJoinOp =\n         (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n             isSubQuery, mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n-    OpTraits opTraits =\n-        new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n-            .getSortCols());\n+    int numReduceSinks = joinOp.getOpTraits().getNumReduceSinks();\n+    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp\n+        .getOpTraits().getSortCols(), numReduceSinks);\n     mergeJoinOp.setOpTraits(opTraits);\n     mergeJoinOp.setStatistics(joinOp.getStatistics());\n \n@@ -289,8 +292,7 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n \n     if (adjustParentsChildren) {\n       mergeJoinOp.getConf().setGenJoinKeys(true);\n-      List<Operator<? extends OperatorDesc>> newParentOpList =\n-          new ArrayList<Operator<? extends OperatorDesc>>();\n+      List<Operator<? extends OperatorDesc>> newParentOpList = new ArrayList<Operator<? extends OperatorDesc>>();\n       for (Operator<? extends OperatorDesc> parentOp : mergeJoinOp.getParentOperators()) {\n         for (Operator<? extends OperatorDesc> grandParentOp : parentOp.getParentOperators()) {\n           grandParentOp.getChildOperators().remove(parentOp);\n@@ -328,7 +330,8 @@ private void setAllChildrenTraitsToNull(Operator<? extends OperatorDesc> current\n     if (currentOp instanceof ReduceSinkOperator) {\n       return;\n     }\n-    currentOp.setOpTraits(new OpTraits(null, -1, null));\n+    currentOp.setOpTraits(new OpTraits(null, -1, null,\n+        currentOp.getOpTraits().getNumReduceSinks()));\n     for (Operator<? extends OperatorDesc> childOp : currentOp.getChildOperators()) {\n       if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof GroupByOperator)) {\n         break;\n@@ -351,7 +354,7 @@ private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcCon\n \n     // we can set the traits for this join operator\n     OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n-        tezBucketJoinProcCtx.getNumBuckets(), null);\n+        tezBucketJoinProcCtx.getNumBuckets(), null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);\n@@ -377,8 +380,7 @@ private boolean checkConvertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcCont\n \n     ReduceSinkOperator bigTableRS =\n         (ReduceSinkOperator) joinOp.getParentOperators().get(bigTablePosition);\n-    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits()\n-            .getNumBuckets();\n+    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits().getNumBuckets();\n \n     // the sort and bucket cols have to match on both sides for this\n     // transformation of the join operation\n@@ -425,13 +427,12 @@ private void setNumberOfBucketsOnChildren(Operator<? extends OperatorDesc> curre\n   }\n \n   /*\n-   * If the parent reduce sink of the big table side has the same emit key cols\n-   * as its parent, we can create a bucket map join eliminating the reduce sink.\n+   * If the parent reduce sink of the big table side has the same emit key cols as its parent, we\n+   * can create a bucket map join eliminating the reduce sink.\n    */\n   private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n       OptimizeTezProcContext context, int bigTablePosition,\n-      TezBucketJoinProcCtx tezBucketJoinProcCtx)\n-  throws SemanticException {\n+      TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // bail on mux-operator because mux operator masks the emit keys of the\n     // constituent reduce sinks\n     if (!(joinOp.getParentOperators().get(0) instanceof ReduceSinkOperator)) {\n@@ -453,8 +454,8 @@ private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n     }\n \n     /*\n-     * this is the case when the big table is a sub-query and is probably\n-     * already bucketed by the join column in say a group by operation\n+     * this is the case when the big table is a sub-query and is probably already bucketed by the\n+     * join column in say a group by operation\n      */\n     boolean isSubQuery = false;\n     if (numBuckets < 0) {\n@@ -492,7 +493,8 @@ private boolean checkColEquality(List<List<String>> grandParentColNames,\n           // all columns need to be at least a subset of the parentOfParent's bucket cols\n           ExprNodeDesc exprNodeDesc = colExprMap.get(colName);\n           if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n-            if (((ExprNodeColumnDesc)exprNodeDesc).getColumn().equals(listBucketCols.get(colCount))) {\n+            if (((ExprNodeColumnDesc) exprNodeDesc).getColumn()\n+                .equals(listBucketCols.get(colCount))) {\n               colCount++;\n             } else {\n               break;\n@@ -562,14 +564,13 @@ public int getMapJoinConversionPos(JoinOperator joinOp, OptimizeTezProcContext c\n \n       Statistics currInputStat = parentOp.getStatistics();\n       if (currInputStat == null) {\n-        LOG.warn(\"Couldn't get statistics from: \"+parentOp);\n+        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n         return -1;\n       }\n \n       long inputSize = currInputStat.getDataSize();\n-      if ((bigInputStat == null) ||\n-          ((bigInputStat != null) &&\n-          (inputSize > bigInputStat.getDataSize()))) {\n+      if ((bigInputStat == null)\n+          || ((bigInputStat != null) && (inputSize > bigInputStat.getDataSize()))) {\n \n         if (bigTableFound) {\n           // cannot convert to map join; we've already chosen a big table\n@@ -639,11 +640,11 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n       }\n     }\n \n-    //can safely convert the join to a map join.\n+    // can safely convert the join to a map join.\n     MapJoinOperator mapJoinOp =\n         MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n-                joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n-                joinOp.getConf().getMapAliases(), bigTablePosition, true);\n+            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n+            joinOp.getConf().getMapAliases(), bigTablePosition, true);\n \n     Operator<? extends OperatorDesc> parentBigTableOp =\n         mapJoinOp.getParentOperators().get(bigTablePosition);\n@@ -667,7 +668,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n             parentBigTableOp.getParentOperators().get(0));\n       }\n       parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n-      for (Operator<? extends OperatorDesc> op : mapJoinOp.getParentOperators()) {\n+      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n         if (!(op.getChildOperators().contains(mapJoinOp))) {\n           op.getChildOperators().add(mapJoinOp);\n         }\n@@ -681,7 +682,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n   private boolean hasDynamicPartitionBroadcast(Operator<?> parent) {\n     boolean hasDynamicPartitionPruning = false;\n \n-    for (Operator<?> op: parent.getChildOperators()) {\n+    for (Operator<?> op : parent.getChildOperators()) {\n       while (op != null) {\n         if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n           // found dynamic partition pruning operator", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java"}, {"additions": 62, "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java", "sha": "62428dbdd61d28b63d59388f00b3c0c0d328d593", "changes": 96, "status": "modified", "deletions": 34, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a", "patch": "@@ -82,7 +82,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   * Reduce sink operator is the de-facto operator \n+   * Reduce sink operator is the de-facto operator\n    * for determining keyCols (emit keys of a map phase)\n    */\n   public static class ReduceSinkRule implements NodeProcessor {\n@@ -106,34 +106,37 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       listBucketCols.add(bucketCols);\n       int numBuckets = -1;\n+      int numReduceSinks = 1;\n       OpTraits parentOpTraits = rs.getParentOperators().get(0).getConf().getOpTraits();\n       if (parentOpTraits != null) {\n         numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks += parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols, numReduceSinks);\n       rs.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   /*\n-   * Table scan has the table object and pruned partitions that has information such as\n-   * bucketing, sorting, etc. that is used later for optimization.\n+   * Table scan has the table object and pruned partitions that has information\n+   * such as bucketing, sorting, etc. that is used later for optimization.\n    */\n   public static class TableScanRule implements NodeProcessor {\n \n-    public boolean checkBucketedTable(Table tbl, \n-        ParseContext pGraphContext,\n+    public boolean checkBucketedTable(Table tbl, ParseContext pGraphContext,\n         PrunedPartitionList prunedParts) throws SemanticException {\n \n       if (tbl.isPartitioned()) {\n         List<Partition> partitions = prunedParts.getNotDeniedPartns();\n         // construct a mapping of (Partition->bucket file names) and (Partition -> bucket number)\n         if (!partitions.isEmpty()) {\n           for (Partition p : partitions) {\n-            List<String> fileNames =\n-                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), pGraphContext);\n-            // The number of files for the table should be same as number of buckets.\n+            List<String> fileNames = \n+                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), \n+                    pGraphContext);\n+            // The number of files for the table should be same as number of\n+            // buckets.\n             int bucketCount = p.getBucketCount();\n \n             if (fileNames.size() != 0 && fileNames.size() != bucketCount) {\n@@ -143,8 +146,9 @@ public boolean checkBucketedTable(Table tbl,\n         }\n       } else {\n \n-        List<String> fileNames =\n-            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), pGraphContext);\n+        List<String> fileNames = \n+            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), \n+                pGraphContext);\n         Integer num = new Integer(tbl.getNumBuckets());\n \n         // The number of files for the table should be same as number of buckets.\n@@ -183,7 +187,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         }\n         sortedColsList.add(sortCols);\n       }\n-      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList);\n+      // num reduce sinks hardcoded to 0 because TS has no parents\n+      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList, 0);\n       ts.setOpTraits(opTraits);\n       return null;\n     }\n@@ -208,17 +213,22 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       }\n \n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = gbyOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n+      }\n       listBucketCols.add(gbyKeys);\n-      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols, numReduceSinks);\n       gbyOp.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   public static class SelectRule implements NodeProcessor {\n \n-    public List<List<String>> getConvertedColNames(List<List<String>> parentColNames,\n-        SelectOperator selOp) {\n+    public List<List<String>> getConvertedColNames(\n+        List<List<String>> parentColNames, SelectOperator selOp) {\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       if (selOp.getColumnExprMap() != null) {\n         if (parentColNames != null) {\n@@ -244,8 +254,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      SelectOperator selOp = (SelectOperator)nd;\n-      List<List<String>> parentBucketColNames =\n+      SelectOperator selOp = (SelectOperator) nd;\n+      List<List<String>> parentBucketColNames = \n           selOp.getParentOperators().get(0).getOpTraits().getBucketColNames();\n \n       List<List<String>> listBucketCols = null;\n@@ -254,18 +264,21 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         if (parentBucketColNames != null) {\n           listBucketCols = getConvertedColNames(parentBucketColNames, selOp);\n         }\n-        List<List<String>> parentSortColNames = selOp.getParentOperators().get(0).getOpTraits()\n-            .getSortCols();\n+        List<List<String>> parentSortColNames = \n+            selOp.getParentOperators().get(0).getOpTraits().getSortCols();\n         if (parentSortColNames != null) {\n           listSortCols = getConvertedColNames(parentSortColNames, selOp);\n         }\n       }\n \n       int numBuckets = -1;\n-      if (selOp.getParentOperators().get(0).getOpTraits() != null) {\n-        numBuckets = selOp.getParentOperators().get(0).getOpTraits().getNumBuckets();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = selOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols, numReduceSinks);\n       selOp.setOpTraits(opTraits);\n       return null;\n     }\n@@ -276,26 +289,31 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      JoinOperator joinOp = (JoinOperator)nd;\n+      JoinOperator joinOp = (JoinOperator) nd;\n       List<List<String>> bucketColsList = new ArrayList<List<String>>();\n       List<List<String>> sortColsList = new ArrayList<List<String>>();\n       byte pos = 0;\n+      int numReduceSinks = 0; // will be set to the larger of the parents\n       for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n         if (!(parentOp instanceof ReduceSinkOperator)) {\n           // can be mux operator\n           break;\n         }\n-        ReduceSinkOperator rsOp = (ReduceSinkOperator)parentOp;\n+        ReduceSinkOperator rsOp = (ReduceSinkOperator) parentOp;\n         if (rsOp.getOpTraits() == null) {\n           ReduceSinkRule rsRule = new ReduceSinkRule();\n           rsRule.process(rsOp, stack, procCtx, nodeOutputs);\n         }\n-        bucketColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getBucketColNames(), pos));\n-        sortColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getSortCols(), pos));\n+        OpTraits parentOpTraits = rsOp.getOpTraits();\n+        bucketColsList.add(getOutputColNames(joinOp, parentOpTraits.getBucketColNames(), pos));\n+        sortColsList.add(getOutputColNames(joinOp, parentOpTraits.getSortCols(), pos));\n+        if (parentOpTraits.getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOpTraits.getNumReduceSinks();\n+        }\n         pos++;\n       }\n \n-      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList));\n+      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList, numReduceSinks));\n       return null;\n     }\n \n@@ -311,7 +329,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         for (String colName : colNames) {\n           for (ExprNodeDesc exprNode : joinOp.getConf().getExprs().get(pos)) {\n             if (exprNode instanceof ExprNodeColumnDesc) {\n-              if(((ExprNodeColumnDesc)(exprNode)).getColumn().equals(colName)) {\n+              if (((ExprNodeColumnDesc) (exprNode)).getColumn().equals(colName)) {\n                 for (Entry<String, ExprNodeDesc> entry : joinOp.getColumnExprMap().entrySet()) {\n                   if (entry.getValue().isSame(exprNode)) {\n                     bucketColNames.add(entry.getKey());\n@@ -338,20 +356,30 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   *  When we have operators that have multiple parents, it is not\n-   *  clear which parent's traits we need to propagate forward.\n+   * When we have operators that have multiple parents, it is not clear which\n+   * parent's traits we need to propagate forward.\n    */\n   public static class MultiParentRule implements NodeProcessor {\n \n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      OpTraits opTraits = new OpTraits(null, -1, null);\n       @SuppressWarnings(\"unchecked\")\n-      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>)nd;\n+      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>) nd;\n+\n+      int numReduceSinks = 0;\n+      for (Operator<?> parentOp : operator.getParentOperators()) {\n+        if (parentOp.getOpTraits() == null) {\n+          continue;\n+        }\n+        if (parentOp.getOpTraits().getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOp.getOpTraits().getNumReduceSinks();\n+        }\n+      }\n+      OpTraits opTraits = new OpTraits(null, -1, null, numReduceSinks);\n       operator.setOpTraits(opTraits);\n       return null;\n-    } \n+    }\n   }\n \n   public static NodeProcessor getTableScanRule() {\n@@ -361,7 +389,7 @@ public static NodeProcessor getTableScanRule() {\n   public static NodeProcessor getReduceSinkRule() {\n     return new ReduceSinkRule();\n   }\n-  \n+\n   public static NodeProcessor getSelectRule() {\n     return new SelectRule();\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java", "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java", "sha": "a455175efccf401b2286e32e731bc970de768f90", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a", "patch": "@@ -103,7 +103,7 @@\n     }\n \n     // we can set the traits for this join operator\n-    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null);\n+    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java", "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java", "sha": "a687a3d0b9ae439066a09550becc75be2bb2327e", "changes": 13, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a", "patch": "@@ -25,11 +25,14 @@\n   List<List<String>> bucketColNames;\n   List<List<String>> sortColNames;\n   int numBuckets;\n+  int numReduceSinks;\n \n-  public OpTraits(List<List<String>> bucketColNames, int numBuckets, List<List<String>> sortColNames) {\n+  public OpTraits(List<List<String>> bucketColNames, int numBuckets,\n+      List<List<String>> sortColNames, int numReduceSinks) {\n     this.bucketColNames = bucketColNames;\n     this.numBuckets = numBuckets;\n     this.sortColNames = sortColNames;\n+    this.numReduceSinks = numReduceSinks;\n   }\n \n   public List<List<String>> getBucketColNames() {\n@@ -55,4 +58,12 @@ public void setSortColNames(List<List<String>> sortColNames) {\n   public List<List<String>> getSortCols() {\n     return sortColNames;\n   }\n+\n+  public void setNumReduceSinks(int numReduceSinks) {\n+    this.numReduceSinks = numReduceSinks;\n+  }\n+\n+  public int getNumReduceSinks() {\n+    return this.numReduceSinks;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287", "parent": "https://github.com/apache/hive/commit/977dba0916030641c6b8ff2decffd5ce2c401dca", "message": "HIVE-9617 UDF from_utc_timestamp throws NPE if the second argument is null (Alexander Pivovarov via Alan Gates)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1660701 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_180", "file": [{"additions": 9, "raw_url": "https://github.com/apache/hive/raw/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java", "blob_url": "https://github.com/apache/hive/blob/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java", "sha": "1343d3beaec0b1013277a226a3638d4f6b57e6b7", "changes": 25, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java?ref=2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287", "patch": "@@ -32,11 +32,10 @@\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.TextConverter;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.TimestampConverter;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n-import org.apache.hadoop.io.Text;\n \n @Description(name = \"from_utc_timestamp\",\n              value = \"from_utc_timestamp(timestamp, string timezone) - \"\n-                     + \"Assumes given timestamp ist UTC and converts to given timezone (as of Hive 0.8.0)\")\n+                     + \"Assumes given timestamp is UTC and converts to given timezone (as of Hive 0.8.0)\")\n public class GenericUDFFromUtcTimestamp extends GenericUDF {\n \n   static final Log LOG = LogFactory.getLog(GenericUDFFromUtcTimestamp.class);\n@@ -48,17 +47,14 @@\n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n       throws UDFArgumentException {\n-    if (arguments.length < 2) {\n-      throw new UDFArgumentLengthException(\n-          \"The function \" + getName() + \" requires at least two \"\n+    if (arguments.length != 2) {\n+      throw new UDFArgumentLengthException(\"The function \" + getName() + \" requires two \"\n           + \"argument, got \" + arguments.length);\n     }\n     try {\n       argumentOIs = new PrimitiveObjectInspector[2];\n       argumentOIs[0] = (PrimitiveObjectInspector) arguments[0];\n-      if (arguments.length > 1) {\n-        argumentOIs[1] = (PrimitiveObjectInspector) arguments[1];\n-      }\n+      argumentOIs[1] = (PrimitiveObjectInspector) arguments[1];\n     } catch (ClassCastException e) {\n       throw new UDFArgumentException(\n           \"The function \" + getName() + \" takes only primitive types\");\n@@ -73,20 +69,17 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     Object o0 = arguments[0].get();\n-    TimeZone timezone = null;\n     if (o0 == null) {\n       return null;\n     }\n-\n-    if (arguments.length > 1 && arguments[1] != null) {\n-      Text text = textConverter.convert(arguments[1].get());\n-      if (text != null) {\n-        timezone = TimeZone.getTimeZone(text.toString());\n-      }\n-    } else {\n+    Object o1 = arguments[1].get();\n+    if (o1 == null) {\n       return null;\n     }\n \n+    String tzStr = textConverter.convert(o1).toString();\n+    TimeZone timezone = TimeZone.getTimeZone(tzStr);\n+\n     Timestamp timestamp = ((TimestampWritable) timestampConverter.convert(o0))\n         .getTimestamp();\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFromUtcTimestamp.java"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q", "blob_url": "https://github.com/apache/hive/blob/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q", "sha": "de985078f962fcd78c8051d54d1036d29b7d4466", "changes": 24, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q?ref=2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287", "patch": "@@ -0,0 +1,24 @@\n+DESCRIBE FUNCTION from_utc_timestamp;\n+DESC FUNCTION EXTENDED from_utc_timestamp;\n+\n+explain select from_utc_timestamp('2015-02-11 10:30:00', 'PST');\n+\n+select\n+from_utc_timestamp('2015-02-11 04:30:00', 'PST'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'Europe/Moscow'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'GMT+8'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'GMT'),\n+from_utc_timestamp('2015-02-11 04:30:00', ''),\n+from_utc_timestamp('2015-02-11 04:30:00', '---'),\n+from_utc_timestamp(cast(null as string), 'PST'),\n+from_utc_timestamp('2015-02-11 04:30:00', cast(null as string));\n+\n+select\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'PST'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'Europe/Moscow'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT+8'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), ''),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), '---'),\n+from_utc_timestamp(cast(null as timestamp), 'PST'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), cast(null as string));\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/udf_from_utc_timestamp.q"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/queries/clientpositive/udf_to_utc_timestamp.q", "blob_url": "https://github.com/apache/hive/blob/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/queries/clientpositive/udf_to_utc_timestamp.q", "sha": "fe0b647698aba3aa82be272b81af9018dcdf4918", "changes": 24, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_to_utc_timestamp.q?ref=2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287", "patch": "@@ -0,0 +1,24 @@\n+DESCRIBE FUNCTION to_utc_timestamp;\n+DESC FUNCTION EXTENDED to_utc_timestamp;\n+\n+explain select to_utc_timestamp('2015-02-11 10:30:00', 'PST');\n+\n+select\n+to_utc_timestamp('2015-02-10 20:30:00', 'PST'),\n+to_utc_timestamp('2015-02-11 08:30:00', 'Europe/Moscow'),\n+to_utc_timestamp('2015-02-11 12:30:00', 'GMT+8'),\n+to_utc_timestamp('2015-02-11 04:30:00', 'GMT'),\n+to_utc_timestamp('2015-02-11 04:30:00', ''),\n+to_utc_timestamp('2015-02-11 04:30:00', '---'),\n+to_utc_timestamp(cast(null as string), 'PST'),\n+to_utc_timestamp('2015-02-11 04:30:00', cast(null as string));\n+\n+select\n+to_utc_timestamp(cast('2015-02-10 20:30:00' as timestamp), 'PST'),\n+to_utc_timestamp(cast('2015-02-11 08:30:00' as timestamp), 'Europe/Moscow'),\n+to_utc_timestamp(cast('2015-02-11 12:30:00' as timestamp), 'GMT+8'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), ''),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), '---'),\n+to_utc_timestamp(cast(null as timestamp), 'PST'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), cast(null as string));\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/udf_to_utc_timestamp.q"}, {"additions": 82, "raw_url": "https://github.com/apache/hive/raw/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out", "blob_url": "https://github.com/apache/hive/blob/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out", "sha": "4df872c0db3a7c8bb88dba604baec7c42de7344a", "changes": 82, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out?ref=2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287", "patch": "@@ -0,0 +1,82 @@\n+PREHOOK: query: DESCRIBE FUNCTION from_utc_timestamp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION from_utc_timestamp\n+POSTHOOK: type: DESCFUNCTION\n+from_utc_timestamp(timestamp, string timezone) - Assumes given timestamp is UTC and converts to given timezone (as of Hive 0.8.0)\n+PREHOOK: query: DESC FUNCTION EXTENDED from_utc_timestamp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESC FUNCTION EXTENDED from_utc_timestamp\n+POSTHOOK: type: DESCFUNCTION\n+from_utc_timestamp(timestamp, string timezone) - Assumes given timestamp is UTC and converts to given timezone (as of Hive 0.8.0)\n+PREHOOK: query: explain select from_utc_timestamp('2015-02-11 10:30:00', 'PST')\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select from_utc_timestamp('2015-02-11 10:30:00', 'PST')\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: _dummy_table\n+          Row Limit Per Split: 1\n+          Statistics: Num rows: 0 Data size: 1 Basic stats: PARTIAL Column stats: COMPLETE\n+          Select Operator\n+            expressions: 2015-02-11 02:30:00.0 (type: timestamp)\n+            outputColumnNames: _col0\n+            Statistics: Num rows: 0 Data size: 1 Basic stats: PARTIAL Column stats: COMPLETE\n+            ListSink\n+\n+PREHOOK: query: select\n+from_utc_timestamp('2015-02-11 04:30:00', 'PST'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'Europe/Moscow'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'GMT+8'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'GMT'),\n+from_utc_timestamp('2015-02-11 04:30:00', ''),\n+from_utc_timestamp('2015-02-11 04:30:00', '---'),\n+from_utc_timestamp(cast(null as string), 'PST'),\n+from_utc_timestamp('2015-02-11 04:30:00', cast(null as string))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: select\n+from_utc_timestamp('2015-02-11 04:30:00', 'PST'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'Europe/Moscow'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'GMT+8'),\n+from_utc_timestamp('2015-02-11 04:30:00', 'GMT'),\n+from_utc_timestamp('2015-02-11 04:30:00', ''),\n+from_utc_timestamp('2015-02-11 04:30:00', '---'),\n+from_utc_timestamp(cast(null as string), 'PST'),\n+from_utc_timestamp('2015-02-11 04:30:00', cast(null as string))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+2015-02-10 20:30:00\t2015-02-11 08:30:00\t2015-02-11 12:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\tNULL\tNULL\n+PREHOOK: query: select\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'PST'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'Europe/Moscow'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT+8'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), ''),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), '---'),\n+from_utc_timestamp(cast(null as timestamp), 'PST'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), cast(null as string))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: select\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'PST'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'Europe/Moscow'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT+8'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), ''),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), '---'),\n+from_utc_timestamp(cast(null as timestamp), 'PST'),\n+from_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), cast(null as string))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+2015-02-10 20:30:00\t2015-02-11 08:30:00\t2015-02-11 12:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_from_utc_timestamp.q.out"}, {"additions": 82, "raw_url": "https://github.com/apache/hive/raw/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/results/clientpositive/udf_to_utc_timestamp.q.out", "blob_url": "https://github.com/apache/hive/blob/2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287/ql/src/test/results/clientpositive/udf_to_utc_timestamp.q.out", "sha": "e70b4014429ab5a5ac52a30aecf948d37dc1e8a9", "changes": 82, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_to_utc_timestamp.q.out?ref=2db4252d9ae90c1b0d7d0c7cd87f0840c90ca287", "patch": "@@ -0,0 +1,82 @@\n+PREHOOK: query: DESCRIBE FUNCTION to_utc_timestamp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION to_utc_timestamp\n+POSTHOOK: type: DESCFUNCTION\n+to_utc_timestamp(timestamp, string timezone) - Assumes given timestamp is in given timezone and converts to UTC (as of Hive 0.8.0)\n+PREHOOK: query: DESC FUNCTION EXTENDED to_utc_timestamp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESC FUNCTION EXTENDED to_utc_timestamp\n+POSTHOOK: type: DESCFUNCTION\n+to_utc_timestamp(timestamp, string timezone) - Assumes given timestamp is in given timezone and converts to UTC (as of Hive 0.8.0)\n+PREHOOK: query: explain select to_utc_timestamp('2015-02-11 10:30:00', 'PST')\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select to_utc_timestamp('2015-02-11 10:30:00', 'PST')\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: _dummy_table\n+          Row Limit Per Split: 1\n+          Statistics: Num rows: 0 Data size: 1 Basic stats: PARTIAL Column stats: COMPLETE\n+          Select Operator\n+            expressions: 2015-02-11 18:30:00.0 (type: timestamp)\n+            outputColumnNames: _col0\n+            Statistics: Num rows: 0 Data size: 1 Basic stats: PARTIAL Column stats: COMPLETE\n+            ListSink\n+\n+PREHOOK: query: select\n+to_utc_timestamp('2015-02-10 20:30:00', 'PST'),\n+to_utc_timestamp('2015-02-11 08:30:00', 'Europe/Moscow'),\n+to_utc_timestamp('2015-02-11 12:30:00', 'GMT+8'),\n+to_utc_timestamp('2015-02-11 04:30:00', 'GMT'),\n+to_utc_timestamp('2015-02-11 04:30:00', ''),\n+to_utc_timestamp('2015-02-11 04:30:00', '---'),\n+to_utc_timestamp(cast(null as string), 'PST'),\n+to_utc_timestamp('2015-02-11 04:30:00', cast(null as string))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: select\n+to_utc_timestamp('2015-02-10 20:30:00', 'PST'),\n+to_utc_timestamp('2015-02-11 08:30:00', 'Europe/Moscow'),\n+to_utc_timestamp('2015-02-11 12:30:00', 'GMT+8'),\n+to_utc_timestamp('2015-02-11 04:30:00', 'GMT'),\n+to_utc_timestamp('2015-02-11 04:30:00', ''),\n+to_utc_timestamp('2015-02-11 04:30:00', '---'),\n+to_utc_timestamp(cast(null as string), 'PST'),\n+to_utc_timestamp('2015-02-11 04:30:00', cast(null as string))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\tNULL\tNULL\n+PREHOOK: query: select\n+to_utc_timestamp(cast('2015-02-10 20:30:00' as timestamp), 'PST'),\n+to_utc_timestamp(cast('2015-02-11 08:30:00' as timestamp), 'Europe/Moscow'),\n+to_utc_timestamp(cast('2015-02-11 12:30:00' as timestamp), 'GMT+8'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), ''),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), '---'),\n+to_utc_timestamp(cast(null as timestamp), 'PST'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), cast(null as string))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+POSTHOOK: query: select\n+to_utc_timestamp(cast('2015-02-10 20:30:00' as timestamp), 'PST'),\n+to_utc_timestamp(cast('2015-02-11 08:30:00' as timestamp), 'Europe/Moscow'),\n+to_utc_timestamp(cast('2015-02-11 12:30:00' as timestamp), 'GMT+8'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), 'GMT'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), ''),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), '---'),\n+to_utc_timestamp(cast(null as timestamp), 'PST'),\n+to_utc_timestamp(cast('2015-02-11 04:30:00' as timestamp), cast(null as string))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+#### A masked pattern was here ####\n+2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\t2015-02-11 04:30:00\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_to_utc_timestamp.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d5a22eec75e07b8b669cd524f0ca3a85db116830", "parent": "https://github.com/apache/hive/commit/ec0eb1b65ff2457537c254d0691bb763677950d6", "message": "HIVE-9404 NPE in org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct() (Eugene Koifman, reviewed by Alan Gates)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1653337 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_181", "file": [{"additions": 49, "raw_url": "https://github.com/apache/hive/raw/d5a22eec75e07b8b669cd524f0ca3a85db116830/metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java", "blob_url": "https://github.com/apache/hive/blob/d5a22eec75e07b8b669cd524f0ca3a85db116830/metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java", "sha": "abceaf321db790508f897537f1f7e5f5ac070576", "changes": 49, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java?ref=d5a22eec75e07b8b669cd524f0ca3a85db116830", "patch": "@@ -0,0 +1,49 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.metastore.txn;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.junit.Test;\n+\n+public class TestTxnHandlerNegative {\n+  static final private Log LOG = LogFactory.getLog(TestTxnHandlerNegative.class);\n+\n+  /**\n+   * this intentionally sets a bad URL for connection to test error handling logic\n+   * in TxnHandler\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testBadConnection() throws Exception {\n+    HiveConf conf = new HiveConf();\n+    conf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY, \"blah\");\n+    TxnHandler txnHandler1 = new TxnHandler(conf);\n+    MetaException e = null;\n+    try {\n+      txnHandler1.getOpenTxns();\n+    }\n+    catch(MetaException ex) {\n+      LOG.info(\"Expected error: \" + ex.getMessage(), ex);\n+      e = ex;\n+    }\n+    assert e != null : \"did not get exception\";\n+  }\n+}", "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ec0eb1b65ff2457537c254d0691bb763677950d6", "parent": "https://github.com/apache/hive/commit/66054149b8c3d3cefe7a1d15708f4de9b63901c2", "message": "HIVE-9404 NPE in org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct() (Eugene Koifman, reviewed by Alan Gates)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1653336 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_182", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ec0eb1b65ff2457537c254d0691bb763677950d6/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "blob_url": "https://github.com/apache/hive/blob/ec0eb1b65ff2457537c254d0691bb763677950d6/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "sha": "ca6464ee52822f63e34f0e14b55ce100f96a6ff0", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=ec0eb1b65ff2457537c254d0691bb763677950d6", "patch": "@@ -902,7 +902,7 @@ protected void checkRetryable(Connection conn,\n     // so I've tried to capture the different error messages (there appear to be fewer different\n     // error messages than SQL states).\n     // Derby and newer MySQL driver use the new SQLTransactionRollbackException\n-    if (dbProduct == null) {\n+    if (dbProduct == null && conn != null) {\n       determineDatabaseProduct(conn);\n     }\n     if (e instanceof SQLTransactionRollbackException ||", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b650b798ffb589800e19320595c54f5793cd1e40", "parent": "https://github.com/apache/hive/commit/ec3c2f86bc2399cab3127463846efcd44d7efea2", "message": "HIVE-9330 : DummyTxnManager will throw NPE if WriteEntity writeType has not been set (Chaoyu Tang via Szehon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1652897 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_183", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/b650b798ffb589800e19320595c54f5793cd1e40/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java", "blob_url": "https://github.com/apache/hive/blob/b650b798ffb589800e19320595c54f5793cd1e40/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java", "sha": "8fdac5e29e723695a00e99751303ed54e6a9e5e5", "changes": 10, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java?ref=b650b798ffb589800e19320595c54f5793cd1e40", "patch": "@@ -254,9 +254,13 @@ static void dedupLockObjects(List<HiveLockObj> lockObjects) {\n \n   private HiveLockMode getWriteEntityLockMode (WriteEntity we) {\n     HiveLockMode lockMode = we.isComplete() ? HiveLockMode.EXCLUSIVE : HiveLockMode.SHARED;\n-    //but the writeEntity is complete in DDL operations, and we need check its writeType to\n-    //to determine the lockMode\n-    switch (we.getWriteType()) {\n+    //but the writeEntity is complete in DDL operations, instead DDL sets the writeType, so\n+    //we use it to determine its lockMode, and first we check if the writeType was set\n+    WriteEntity.WriteType writeType = we.getWriteType();\n+    if (writeType == null) {\n+      return lockMode;\n+    }\n+    switch (writeType) {\n       case DDL_EXCLUSIVE:\n         return HiveLockMode.EXCLUSIVE;\n       case DDL_SHARED:", "filename": "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/865886ce20041fc1b07208004b87413081439e1b", "parent": "https://github.com/apache/hive/commit/668b529c7c0441a7de39b51935321b60f040b91c", "message": "HIVE-9377 : UDF in_file() in WHERE predicate causes NPE. (Mithun Radhakrishnan via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1652458 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_184", "file": [{"additions": 11, "raw_url": "https://github.com/apache/hive/raw/865886ce20041fc1b07208004b87413081439e1b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInFile.java", "blob_url": "https://github.com/apache/hive/blob/865886ce20041fc1b07208004b87413081439e1b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInFile.java", "sha": "cf5869a6c04759f54ed06f668c953a939dc883b9", "changes": 11, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInFile.java?ref=865886ce20041fc1b07208004b87413081439e1b", "patch": "@@ -139,6 +139,17 @@ public void load(InputStream is) throws HiveException {\n     }\n   }\n \n+  @Override\n+  public void copyToNewInstance(Object newInstance) throws UDFArgumentException {\n+    super.copyToNewInstance(newInstance); // Asserts the class invariant. (Same types.)\n+    GenericUDFInFile that = (GenericUDFInFile)newInstance;\n+    if (that != this) {\n+      that.set = (this.set == null ? null : (HashSet<String>)this.set.clone());\n+      that.strObjectInspector = this.strObjectInspector;\n+      that.fileObjectInspector = this.fileObjectInspector;\n+    }\n+  }\n+\n   @Override\n   public String getDisplayString(String[] children) {\n     assert (children.length == 2);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFInFile.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/36fe70494147cb7f2b4de9e308952d97afcb5d9b", "parent": "https://github.com/apache/hive/commit/4bf41587a8194493f531caf25f3c22a16edc799b", "message": "HIVE-9326: BaseProtocol.Error failed to deserialization due to NPE.[Spark Branch] (Chengxiang via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1650637 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_185", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/36fe70494147cb7f2b4de9e308952d97afcb5d9b/spark-client/src/main/java/org/apache/hive/spark/client/BaseProtocol.java", "blob_url": "https://github.com/apache/hive/blob/36fe70494147cb7f2b4de9e308952d97afcb5d9b/spark-client/src/main/java/org/apache/hive/spark/client/BaseProtocol.java", "sha": "f9c10b196ab47b5b4f4c0126ad455869ab68f0ca", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/spark-client/src/main/java/org/apache/hive/spark/client/BaseProtocol.java?ref=36fe70494147cb7f2b4de9e308952d97afcb5d9b", "patch": "@@ -50,7 +50,11 @@\n     final String cause;\n \n     Error(Throwable cause) {\n-      this.cause = Throwables.getStackTraceAsString(cause);\n+      if (cause == null) {\n+        this.cause = \"\";\n+      } else {\n+        this.cause = Throwables.getStackTraceAsString(cause);\n+      }\n     }\n \n     Error() {", "filename": "spark-client/src/main/java/org/apache/hive/spark/client/BaseProtocol.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/358242f905bf0e94282e03c919850f63f4a7a7be", "parent": "https://github.com/apache/hive/commit/08b8e0d960042818f37fd0dc68fa872a98009e17", "message": "HIVE-9113 : Explain on query failed with NPE (Navis reviewed by Szehon Ho)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1646390 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_186", "file": [{"additions": 21, "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java", "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java", "sha": "1b6b33b1545aa11233f08b6418e3329b0cbd0d64", "changes": 30, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java?ref=358242f905bf0e94282e03c919850f63f4a7a7be", "patch": "@@ -382,7 +382,7 @@ protected ASTNode firstDot(ASTNode dot) {\n     /*\n      * row resolver of the SubQuery.\n      * Set by the SemanticAnalyzer after the Plan for the SubQuery is genned.\n-     * This is neede in case the SubQuery select list contains a TOK_ALLCOLREF\n+     * This is needed in case the SubQuery select list contains a TOK_ALLCOLREF\n      */\n     RowResolver sqRR;\n \n@@ -513,7 +513,10 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n       String outerQueryAlias,\n       Set<String> outerQryAliases) throws SemanticException {\n \n-    ASTNode selectClause = (ASTNode) subQueryAST.getChild(1).getChild(1);\n+    ASTNode fromClause = getChildFromSubqueryAST(\"From\", HiveParser.TOK_FROM);\n+    ASTNode insertClause = getChildFromSubqueryAST(\"Insert\", HiveParser.TOK_INSERT);\n+\n+    ASTNode selectClause = (ASTNode) insertClause.getChild(1);\n \n     int selectExprStart = 0;\n     if ( selectClause.getChild(0).getType() == HiveParser.TOK_HINTLIST ) {\n@@ -537,15 +540,15 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n      * Restriction 17.s :: SubQuery cannot use the same table alias as one used in\n      * the Outer Query.\n      */\n-    List<String> sqAliases = SubQueryUtils.getTableAliasesInSubQuery(this);\n+    List<String> sqAliases = SubQueryUtils.getTableAliasesInSubQuery(fromClause);\n     String sharedAlias = null;\n     for(String s : sqAliases ) {\n       if ( outerQryAliases.contains(s) ) {\n         sharedAlias = s;\n       }\n     }\n     if ( sharedAlias != null) {\n-      ASTNode whereClause = SubQueryUtils.subQueryWhere(subQueryAST);\n+      ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n \n       if ( whereClause != null ) {\n         ASTNode u = SubQueryUtils.hasUnQualifiedColumnReferences(whereClause);\n@@ -581,7 +584,7 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n       containsAggregationExprs = containsAggregationExprs | ( r == 1 );\n     }\n \n-    rewrite(outerQueryRR, forHavingClause, outerQueryAlias);\n+    rewrite(outerQueryRR, forHavingClause, outerQueryAlias, insertClause, selectClause);\n \n     SubQueryUtils.setOriginDeep(subQueryAST, originalSQASTOrigin);\n \n@@ -631,6 +634,16 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n \n   }\n \n+  private ASTNode getChildFromSubqueryAST(String errorMsg, int type) throws SemanticException {\n+    ASTNode childAST = (ASTNode) subQueryAST.getFirstChildWithType(type);\n+    if (childAST == null && errorMsg != null) {\n+      subQueryAST.setOrigin(originalSQASTOrigin);\n+      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n+          subQueryAST, errorMsg + \" clause is missing in SubQuery.\"));\n+    }\n+    return childAST;\n+  }\n+\n   private void setJoinType() {\n     if ( operator.getType() == SubQueryType.NOT_IN ||\n         operator.getType() == SubQueryType.NOT_EXISTS ) {\n@@ -744,7 +757,7 @@ String getNextCorrExprAlias() {\n    *         R2.x = min(R1.y)\n    *      Where R1 is an outer table reference, and R2 is a SubQuery table reference.\n    *   b. When hoisting the correlation predicate to a join predicate, we need to\n-   *      rewrite it to be in the form the Join code allows: so the predicte needs\n+   *      rewrite it to be in the form the Join code allows: so the predict needs\n    *      to contain a qualified column references.\n    *      We handle this by generating a new name for the aggregation expression,\n    *      like R1._gby_sq_col_1 and adding this mapping to the Outer Query's\n@@ -753,9 +766,8 @@ String getNextCorrExprAlias() {\n    */\n   private void rewrite(RowResolver parentQueryRR,\n       boolean forHavingClause,\n-      String outerQueryAlias) throws SemanticException {\n-    ASTNode selectClause = (ASTNode) subQueryAST.getChild(1).getChild(1);\n-    ASTNode whereClause = SubQueryUtils.subQueryWhere(subQueryAST);\n+      String outerQueryAlias, ASTNode insertClause, ASTNode selectClause) throws SemanticException {\n+    ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n \n     if ( whereClause == null ) {\n       return;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java", "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java", "sha": "57868b757b8dbbb012cb63a0c3fd7246e15adfe6", "changes": 15, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java?ref=358242f905bf0e94282e03c919850f63f4a7a7be", "patch": "@@ -43,7 +43,7 @@ static void extractConjuncts(ASTNode node, List<ASTNode> conjuncts) {\n   }\n \n   /*\n-   * Remove the SubQuery from the Where CLause Tree.\n+   * Remove the SubQuery from the Where Clause Tree.\n    * return the remaining WhereClause.\n    */\n   static ASTNode rewriteParentQueryWhere(ASTNode whereCond, ASTNode subQuery)\n@@ -271,10 +271,9 @@ static int checkAggOrWindowing(ASTNode expressionTree) throws SemanticException\n     return r;\n   }\n \n-  static List<String> getTableAliasesInSubQuery(QBSubQuery sq) {\n+  static List<String> getTableAliasesInSubQuery(ASTNode fromClause) {\n     List<String> aliases = new ArrayList<String>();\n-    ASTNode joinAST = (ASTNode) sq.getSubQueryAST().getChild(0);\n-    getTableAliasesInSubQuery((ASTNode) joinAST.getChild(0), aliases);\n+    getTableAliasesInSubQuery((ASTNode) fromClause.getChild(0), aliases);\n     return aliases;\n   }\n \n@@ -318,10 +317,10 @@ else if ( type == HiveParser.TOK_TABLE_OR_COL ) {\n     return null;\n   }\n   \n-  static ASTNode subQueryWhere(ASTNode subQueryAST) {\n-    if ( subQueryAST.getChild(1).getChildCount() > 2 &&\n-        subQueryAST.getChild(1).getChild(2).getType() == HiveParser.TOK_WHERE ) {\n-      return (ASTNode) subQueryAST.getChild(1).getChild(2);\n+  static ASTNode subQueryWhere(ASTNode insertClause) {\n+    if (insertClause.getChildCount() > 2 &&\n+        insertClause.getChild(2).getType() == HiveParser.TOK_WHERE ) {\n+      return (ASTNode) insertClause.getChild(2);\n     }\n     return null;\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/queries/clientnegative/subquery_missing_from.q", "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/queries/clientnegative/subquery_missing_from.q", "sha": "3b49ac6a0a8f00dbc26a4071042d6e05574f0767", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/subquery_missing_from.q?ref=358242f905bf0e94282e03c919850f63f4a7a7be", "patch": "@@ -0,0 +1 @@\n+select * from src where src.key in (select key);\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientnegative/subquery_missing_from.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/results/clientnegative/subquery_missing_from.q.out", "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/results/clientnegative/subquery_missing_from.q.out", "sha": "eaf7735adbe8c24db3acab363c9db800b9908956", "changes": 3, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/subquery_missing_from.q.out?ref=358242f905bf0e94282e03c919850f63f4a7a7be", "patch": "@@ -0,0 +1,3 @@\n+FAILED: SemanticException Line 0:-1 Invalid SubQuery expression 'key' in definition of SubQuery sq_1 [\n+src.key in (select key)\n+] used as sq_1 at Line 1:32: From clause is missing in SubQuery.", "filename": "ql/src/test/results/clientnegative/subquery_missing_from.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c81bd2ee50772d2434af04f580d8ddc4d2480746", "parent": "https://github.com/apache/hive/commit/15ea88cf1788de4b920a8477a1e2f87f6ccb1520", "message": "HIVE-9111: Potential NPE in OrcStruct for list and map types (Prasanth Jayachandran reviewed by Vikram Dixit)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1646046 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_187", "file": [{"additions": 26, "raw_url": "https://github.com/apache/hive/raw/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "blob_url": "https://github.com/apache/hive/blob/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "sha": "2fbdf543d8345a4beed6a953c473a9a9f747c569", "changes": 34, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java?ref=c81bd2ee50772d2434af04f580d8ddc4d2480746", "patch": "@@ -17,14 +17,6 @@\n  */\n package org.apache.hadoop.hive.ql.io.orc;\n \n-import java.io.DataInput;\n-import java.io.DataOutput;\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n@@ -43,6 +35,14 @@\n import org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo;\n import org.apache.hadoop.io.Writable;\n \n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n final public class OrcStruct implements Writable {\n \n   private Object[] fields;\n@@ -346,17 +346,26 @@ public ObjectInspector getMapValueObjectInspector() {\n \n     @Override\n     public Object getMapValueElement(Object map, Object key) {\n+      if (map == null) {\n+        return null;\n+      }\n       return ((Map) map).get(key);\n     }\n \n     @Override\n     @SuppressWarnings(\"unchecked\")\n     public Map<Object, Object> getMap(Object map) {\n+      if (map == null) {\n+        return null;\n+      }\n       return (Map) map;\n     }\n \n     @Override\n     public int getMapSize(Object map) {\n+      if (map == null) {\n+        return -1;\n+      }\n       return ((Map) map).size();\n     }\n \n@@ -429,17 +438,26 @@ public ObjectInspector getListElementObjectInspector() {\n \n     @Override\n     public Object getListElement(Object list, int i) {\n+      if (list == null) {\n+        return null;\n+      }\n       return ((List) list).get(i);\n     }\n \n     @Override\n     public int getListLength(Object list) {\n+      if (list == null) {\n+        return -1;\n+      }\n       return ((List) list).size();\n     }\n \n     @Override\n     @SuppressWarnings(\"unchecked\")\n     public List<?> getList(Object list) {\n+      if (list == null) {\n+        return null;\n+      }\n       return (List) list;\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/queries/clientpositive/orc_null_check.q", "blob_url": "https://github.com/apache/hive/blob/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/queries/clientpositive/orc_null_check.q", "sha": "2cb119024ab2e5e1677acd13a5e8f5f0d4dd69e7", "changes": 8, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_null_check.q?ref=c81bd2ee50772d2434af04f580d8ddc4d2480746", "patch": "@@ -0,0 +1,8 @@\n+create table listtable(l array<string>);\n+create table listtable_orc(l array<string>) stored as orc;\n+\n+insert overwrite table listtable select array(null) from src;\n+insert overwrite table listtable_orc select * from listtable;\n+\n+select size(l) from listtable_orc limit 10;\n+", "filename": "ql/src/test/queries/clientpositive/orc_null_check.q"}, {"additions": 52, "raw_url": "https://github.com/apache/hive/raw/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/results/clientpositive/orc_null_check.q.out", "blob_url": "https://github.com/apache/hive/blob/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/results/clientpositive/orc_null_check.q.out", "sha": "093fdff7269fa059b93fbf2dbd5460fb1e2d486c", "changes": 52, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_null_check.q.out?ref=c81bd2ee50772d2434af04f580d8ddc4d2480746", "patch": "@@ -0,0 +1,52 @@\n+PREHOOK: query: create table listtable(l array<string>)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@listtable\n+POSTHOOK: query: create table listtable(l array<string>)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@listtable\n+PREHOOK: query: create table listtable_orc(l array<string>) stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@listtable_orc\n+POSTHOOK: query: create table listtable_orc(l array<string>) stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@listtable_orc\n+PREHOOK: query: insert overwrite table listtable select array(null) from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@listtable\n+POSTHOOK: query: insert overwrite table listtable select array(null) from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@listtable\n+POSTHOOK: Lineage: listtable.l EXPRESSION []\n+PREHOOK: query: insert overwrite table listtable_orc select * from listtable\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@listtable\n+PREHOOK: Output: default@listtable_orc\n+POSTHOOK: query: insert overwrite table listtable_orc select * from listtable\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@listtable\n+POSTHOOK: Output: default@listtable_orc\n+POSTHOOK: Lineage: listtable_orc.l SIMPLE [(listtable)listtable.FieldSchema(name:l, type:array<string>, comment:null), ]\n+PREHOOK: query: select size(l) from listtable_orc limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@listtable_orc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select size(l) from listtable_orc limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@listtable_orc\n+#### A masked pattern was here ####\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1", "filename": "ql/src/test/results/clientpositive/orc_null_check.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c7fef543e7cebe4be1b06ef3a905c32855fcf82c", "parent": "https://github.com/apache/hive/commit/7e026b1760d8ebe7353a2fc288a7e05c96193a4f", "message": "HIVE-9101: bucket_map_join_spark4.q failed due to NPE.[Spark Branch] (Jimmy via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1645844 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_188", "file": [{"additions": 11, "raw_url": "https://github.com/apache/hive/raw/c7fef543e7cebe4be1b06ef3a905c32855fcf82c/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java", "blob_url": "https://github.com/apache/hive/blob/c7fef543e7cebe4be1b06ef3a905c32855fcf82c/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java", "sha": "ab8010fa7fa2d71606663e45eae7bbcc5d3e89e9", "changes": 18, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java?ref=c7fef543e7cebe4be1b06ef3a905c32855fcf82c", "patch": "@@ -97,13 +97,17 @@ public void load(\n         }\n         String bigInputPath = currentInputPath;\n         if (currentInputPath != null && mapJoinCtx != null) {\n-          Set<String> aliases =\n-            ((SparkBucketMapJoinContext)mapJoinCtx).getPosToAliasMap().get(pos);\n-          String alias = aliases.iterator().next();\n-          // Any one small table input path\n-          String smallInputPath =\n-            mapJoinCtx.getAliasBucketFileNameMapping().get(alias).get(bigInputPath).get(0);\n-          bigInputPath = mapJoinCtx.getMappingBigFile(alias, smallInputPath);\n+          if (!desc.isBucketMapJoin()) {\n+            bigInputPath = null;\n+          } else {\n+            Set<String> aliases =\n+              ((SparkBucketMapJoinContext)mapJoinCtx).getPosToAliasMap().get(pos);\n+            String alias = aliases.iterator().next();\n+            // Any one small table input path\n+            String smallInputPath =\n+              mapJoinCtx.getAliasBucketFileNameMapping().get(alias).get(bigInputPath).get(0);\n+            bigInputPath = mapJoinCtx.getMappingBigFile(alias, smallInputPath);\n+          }\n         }\n         String fileName = localWork.getBucketFileName(bigInputPath);\n         Path path = Utilities.generatePath(baseDir, desc.getDumpFilePrefix(), (byte)pos, fileName);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/c7fef543e7cebe4be1b06ef3a905c32855fcf82c/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java", "blob_url": "https://github.com/apache/hive/blob/c7fef543e7cebe4be1b06ef3a905c32855fcf82c/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java", "sha": "03742d436930526ff2db15d6ed159f4f0d7136f0", "changes": 12, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java?ref=c7fef543e7cebe4be1b06ef3a905c32855fcf82c", "patch": "@@ -240,12 +240,14 @@ protected boolean checkConvertBucketMapJoin(\n         for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpEntry : topOps.entrySet()) {\n           if (topOpEntry.getValue() == tso) {\n             String newAlias = topOpEntry.getKey();\n-            joinAliases.set(index, newAlias);\n-            if (baseBigAlias.equals(alias)) {\n-              baseBigAlias = newAlias;\n+            if (!newAlias.equals(alias)) {\n+              joinAliases.set(index, newAlias);\n+              if (baseBigAlias.equals(alias)) {\n+                baseBigAlias = newAlias;\n+              }\n+              aliasToNewAliasMap.put(alias, newAlias);\n+              alias = newAlias;\n             }\n-            aliasToNewAliasMap.put(alias, newAlias);\n-            alias = newAlias;\n             break;\n           }\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/c7fef543e7cebe4be1b06ef3a905c32855fcf82c/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out", "blob_url": "https://github.com/apache/hive/blob/c7fef543e7cebe4be1b06ef3a905c32855fcf82c/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out", "sha": "fae8be42466c220f44a7ec661c27bddecb6471bf", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out?ref=c7fef543e7cebe4be1b06ef3a905c32855fcf82c", "filename": "ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/7e026b1760d8ebe7353a2fc288a7e05c96193a4f", "parent": "https://github.com/apache/hive/commit/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863", "message": "reverse r1645842: HIVE-9101: bucket_map_join_spark4.q failed due to NPE.[Spark Branch] (Jimmy via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1645843 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_189", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/data/conf/spark/hive-site.xml", "blob_url": "https://github.com/apache/hive/blob/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/data/conf/spark/hive-site.xml", "sha": "ee484d182a5483f08c4dd97bdcb8788461d46b76", "changes": 5, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/conf/spark/hive-site.xml?ref=7e026b1760d8ebe7353a2fc288a7e05c96193a4f", "patch": "@@ -220,9 +220,4 @@\n   <value>${maven.local.repository}/org/apache/hive/hive-it-util/${hive.version}/hive-it-util-${hive.version}.jar</value>\n </property>\n \n-<property>\n-  <name>hive.users.in.admin.role</name>\n-  <value>hive_admin_user</value>\n-</property>\n-\n </configuration>", "filename": "data/conf/spark/hive-site.xml"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java", "blob_url": "https://github.com/apache/hive/blob/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java", "sha": "38bb880b48b1e127cd6bc366a1ce2c4f1a2c2708", "changes": 18, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java?ref=7e026b1760d8ebe7353a2fc288a7e05c96193a4f", "patch": "@@ -97,17 +97,13 @@ public void load(\n         }\n         String bigInputPath = currentInputPath;\n         if (currentInputPath != null && mapJoinCtx != null) {\n-          if (!desc.isBucketMapJoin()) {\n-            bigInputPath = null;\n-          } else {\n-            Set<String> aliases =\n-              ((SparkBucketMapJoinContext)mapJoinCtx).getPosToAliasMap().get(pos);\n-            String alias = aliases.iterator().next();\n-            // Any one small table input path\n-            String smallInputPath =\n-              mapJoinCtx.getAliasBucketFileNameMapping().get(alias).get(bigInputPath).get(0);\n-            bigInputPath = mapJoinCtx.getMappingBigFile(alias, smallInputPath);\n-          }\n+          Set<String> aliases =\n+            ((SparkBucketMapJoinContext)mapJoinCtx).getPosToAliasMap().get(pos);\n+          String alias = aliases.iterator().next();\n+          // Any one small table input path\n+          String smallInputPath =\n+            mapJoinCtx.getAliasBucketFileNameMapping().get(alias).get(bigInputPath).get(0);\n+          bigInputPath = mapJoinCtx.getMappingBigFile(alias, smallInputPath);\n         }\n         String fileName = localWork.getBucketFileName(bigInputPath);\n         Path path = Utilities.generatePath(baseDir, desc.getDumpFilePrefix(), (byte)pos, fileName);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java", "blob_url": "https://github.com/apache/hive/blob/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java", "sha": "2ddf73f2a2f80d81b7b8a20145a7713dd208ead5", "changes": 12, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java?ref=7e026b1760d8ebe7353a2fc288a7e05c96193a4f", "patch": "@@ -240,14 +240,12 @@ protected boolean checkConvertBucketMapJoin(\n         for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpEntry : topOps.entrySet()) {\n           if (topOpEntry.getValue() == tso) {\n             String newAlias = topOpEntry.getKey();\n-            if (!newAlias.equals(alias)) {\n-              joinAliases.set(index, newAlias);\n-              if (baseBigAlias.equals(alias)) {\n-                baseBigAlias = newAlias;\n-              }\n-              aliasToNewAliasMap.put(alias, newAlias);\n-              alias = newAlias;\n+            joinAliases.set(index, newAlias);\n+            if (baseBigAlias.equals(alias)) {\n+              baseBigAlias = newAlias;\n             }\n+            aliasToNewAliasMap.put(alias, newAlias);\n+            alias = newAlias;\n             break;\n           }\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out", "blob_url": "https://github.com/apache/hive/blob/7e026b1760d8ebe7353a2fc288a7e05c96193a4f/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out", "sha": "9794b210310338a3cd1d792bd6566f75b8ae0d35", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out?ref=7e026b1760d8ebe7353a2fc288a7e05c96193a4f", "filename": "ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863", "parent": "https://github.com/apache/hive/commit/37b6d8f97c356ab3ec303910bf7ca421cee44861", "message": "HIVE-9101: bucket_map_join_spark4.q failed due to NPE.[Spark Branch] (Jimmy via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1645842 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_190", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/data/conf/spark/hive-site.xml", "blob_url": "https://github.com/apache/hive/blob/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/data/conf/spark/hive-site.xml", "sha": "77dd9c7f1e9c69e9fea0e7855693970c6a502443", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/conf/spark/hive-site.xml?ref=f1b97c7f00c5ee4cd8d6f92b684c97b009a39863", "patch": "@@ -220,4 +220,9 @@\n   <value>${maven.local.repository}/org/apache/hive/hive-it-util/${hive.version}/hive-it-util-${hive.version}.jar</value>\n </property>\n \n+<property>\n+  <name>hive.users.in.admin.role</name>\n+  <value>hive_admin_user</value>\n+</property>\n+\n </configuration>", "filename": "data/conf/spark/hive-site.xml"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java", "blob_url": "https://github.com/apache/hive/blob/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java", "sha": "ab8010fa7fa2d71606663e45eae7bbcc5d3e89e9", "changes": 18, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java?ref=f1b97c7f00c5ee4cd8d6f92b684c97b009a39863", "patch": "@@ -97,13 +97,17 @@ public void load(\n         }\n         String bigInputPath = currentInputPath;\n         if (currentInputPath != null && mapJoinCtx != null) {\n-          Set<String> aliases =\n-            ((SparkBucketMapJoinContext)mapJoinCtx).getPosToAliasMap().get(pos);\n-          String alias = aliases.iterator().next();\n-          // Any one small table input path\n-          String smallInputPath =\n-            mapJoinCtx.getAliasBucketFileNameMapping().get(alias).get(bigInputPath).get(0);\n-          bigInputPath = mapJoinCtx.getMappingBigFile(alias, smallInputPath);\n+          if (!desc.isBucketMapJoin()) {\n+            bigInputPath = null;\n+          } else {\n+            Set<String> aliases =\n+              ((SparkBucketMapJoinContext)mapJoinCtx).getPosToAliasMap().get(pos);\n+            String alias = aliases.iterator().next();\n+            // Any one small table input path\n+            String smallInputPath =\n+              mapJoinCtx.getAliasBucketFileNameMapping().get(alias).get(bigInputPath).get(0);\n+            bigInputPath = mapJoinCtx.getMappingBigFile(alias, smallInputPath);\n+          }\n         }\n         String fileName = localWork.getBucketFileName(bigInputPath);\n         Path path = Utilities.generatePath(baseDir, desc.getDumpFilePrefix(), (byte)pos, fileName);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HashTableLoader.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java", "blob_url": "https://github.com/apache/hive/blob/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java", "sha": "03742d436930526ff2db15d6ed159f4f0d7136f0", "changes": 12, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java?ref=f1b97c7f00c5ee4cd8d6f92b684c97b009a39863", "patch": "@@ -240,12 +240,14 @@ protected boolean checkConvertBucketMapJoin(\n         for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpEntry : topOps.entrySet()) {\n           if (topOpEntry.getValue() == tso) {\n             String newAlias = topOpEntry.getKey();\n-            joinAliases.set(index, newAlias);\n-            if (baseBigAlias.equals(alias)) {\n-              baseBigAlias = newAlias;\n+            if (!newAlias.equals(alias)) {\n+              joinAliases.set(index, newAlias);\n+              if (baseBigAlias.equals(alias)) {\n+                baseBigAlias = newAlias;\n+              }\n+              aliasToNewAliasMap.put(alias, newAlias);\n+              alias = newAlias;\n             }\n-            aliasToNewAliasMap.put(alias, newAlias);\n-            alias = newAlias;\n             break;\n           }\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out", "blob_url": "https://github.com/apache/hive/blob/f1b97c7f00c5ee4cd8d6f92b684c97b009a39863/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out", "sha": "fae8be42466c220f44a7ec661c27bddecb6471bf", "changes": 0, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out?ref=f1b97c7f00c5ee4cd8d6f92b684c97b009a39863", "filename": "ql/src/test/results/clientpositive/spark/bucket_map_join_spark4.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e", "parent": "https://github.com/apache/hive/commit/69a0e049f65805a891787d42cbf75e2d3b075e92", "message": "HIVE-9063: NPE in RemoteSparkJobStatus.getSparkStatistics [Spark Branch] (Rui via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1644518 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_191", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java", "blob_url": "https://github.com/apache/hive/blob/c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java", "sha": "6217de4100a16cee449d126d3d3c9060a9a2dfa7", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java?ref=c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e", "patch": "@@ -107,15 +107,15 @@ public SparkCounters getCounter() {\n \n   @Override\n   public SparkStatistics getSparkStatistics() {\n+    MetricsCollection metricsCollection = jobHandle.getMetrics();\n+    if (metricsCollection == null || getCounter() == null) {\n+      return null;\n+    }\n     SparkStatisticsBuilder sparkStatisticsBuilder = new SparkStatisticsBuilder();\n     // add Hive operator level statistics.\n     sparkStatisticsBuilder.add(getCounter());\n     // add spark job metrics.\n     String jobIdentifier = \"Spark Job[\" + jobHandle.getClientJobId() + \"] Metrics\";\n-    MetricsCollection metricsCollection = jobHandle.getMetrics();\n-    if (metricsCollection == null) {\n-      return null;\n-    }\n \n     Map<String, Long> flatJobMetric = extractMetrics(metricsCollection);\n     for (Map.Entry<String, Long> entry : flatJobMetric.entrySet()) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e/spark-client/src/main/java/org/apache/hive/spark/client/JobHandleImpl.java", "blob_url": "https://github.com/apache/hive/blob/c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e/spark-client/src/main/java/org/apache/hive/spark/client/JobHandleImpl.java", "sha": "6aeb6b79c5c56d3c62f3fecfab66969d9d0df344", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/spark-client/src/main/java/org/apache/hive/spark/client/JobHandleImpl.java?ref=c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e", "patch": "@@ -40,7 +40,7 @@\n   private final MetricsCollection metrics;\n   private final Promise<T> promise;\n   private final List<Integer> sparkJobIds;\n-  private SparkCounters sparkCounters;\n+  private volatile SparkCounters sparkCounters;\n \n   JobHandleImpl(SparkClientImpl client, Promise<T> promise, String jobId) {\n     this.cancelled = new AtomicBoolean();", "filename": "spark-client/src/main/java/org/apache/hive/spark/client/JobHandleImpl.java"}, {"additions": 32, "raw_url": "https://github.com/apache/hive/raw/c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java", "blob_url": "https://github.com/apache/hive/blob/c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java", "sha": "8c07ee196a6ebebdaeeec43840db7a1f4c737de2", "changes": 47, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/hive/contents/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java?ref=c4a9192b9b7eebd4c9fdbdb929a01719e14fcf9e", "patch": "@@ -17,38 +17,54 @@\n \n package org.apache.hive.spark.client;\n \n+import io.netty.channel.ChannelHandlerContext;\n+import io.netty.channel.nio.NioEventLoopGroup;\n+\n import java.io.Serializable;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.Callable;\n import java.util.concurrent.CopyOnWriteArrayList;\n import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.Executors;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n import java.util.concurrent.atomic.AtomicInteger;\n \n-import com.google.common.base.Joiner;\n-import com.google.common.base.Preconditions;\n-import com.google.common.collect.Lists;\n-import com.google.common.collect.Maps;\n-import com.google.common.util.concurrent.ThreadFactoryBuilder;\n-import io.netty.channel.ChannelHandlerContext;\n-import io.netty.channel.nio.NioEventLoopGroup;\n+import org.apache.hadoop.hive.common.classification.InterfaceAudience;\n+import org.apache.hive.spark.client.metrics.Metrics;\n+import org.apache.hive.spark.client.rpc.Rpc;\n+import org.apache.hive.spark.client.rpc.RpcConfiguration;\n+import org.apache.hive.spark.counter.SparkCounters;\n import org.apache.spark.SparkConf;\n-import org.apache.spark.scheduler.*;\n import org.apache.spark.api.java.JavaFutureAction;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.scheduler.SparkListener;\n+import org.apache.spark.scheduler.SparkListenerApplicationEnd;\n+import org.apache.spark.scheduler.SparkListenerApplicationStart;\n+import org.apache.spark.scheduler.SparkListenerBlockManagerAdded;\n+import org.apache.spark.scheduler.SparkListenerBlockManagerRemoved;\n+import org.apache.spark.scheduler.SparkListenerEnvironmentUpdate;\n+import org.apache.spark.scheduler.SparkListenerExecutorMetricsUpdate;\n+import org.apache.spark.scheduler.SparkListenerJobEnd;\n+import org.apache.spark.scheduler.SparkListenerJobStart;\n+import org.apache.spark.scheduler.SparkListenerStageCompleted;\n+import org.apache.spark.scheduler.SparkListenerStageSubmitted;\n+import org.apache.spark.scheduler.SparkListenerTaskEnd;\n+import org.apache.spark.scheduler.SparkListenerTaskGettingResult;\n+import org.apache.spark.scheduler.SparkListenerTaskStart;\n+import org.apache.spark.scheduler.SparkListenerUnpersistRDD;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+\n import scala.Tuple2;\n \n-import org.apache.hadoop.hive.common.classification.InterfaceAudience;\n-import org.apache.hive.spark.client.metrics.Metrics;\n-import org.apache.hive.spark.client.rpc.Rpc;\n-import org.apache.hive.spark.client.rpc.RpcConfiguration;\n-import org.apache.hive.spark.counter.SparkCounters;\n+import com.google.common.base.Joiner;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n \n /**\n  * Driver code for the Spark client library.\n@@ -289,7 +305,8 @@ public void call(JavaFutureAction<?> future, SparkCounters sparkCounters) {\n         // re-thrown so that the executor can destroy the affected thread (or the JVM can\n         // die or whatever would happen if the throwable bubbled up).\n         LOG.info(\"Failed to run job \" + req.id, t);\n-        protocol.jobFinished(req.id, null, t, null);\n+        protocol.jobFinished(req.id, null, t,\n+            sparkCounters != null ? sparkCounters.snapshot() : null);\n         throw new ExecutionException(t);\n       } finally {\n         jc.setMonitorCb(null);", "filename": "spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f583cc60916e7be568995c90c1ce8c813d3a58a6", "parent": "https://github.com/apache/hive/commit/efc8b08672216ca61eee1dfa43187decb094bc81", "message": "HIVE-8811: Dynamic partition pruning can result in NPE during query compilation (Gunther Hagleitner, reviewed by Gopal V)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1638685 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_192", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/f583cc60916e7be568995c90c1ce8c813d3a58a6/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "blob_url": "https://github.com/apache/hive/blob/f583cc60916e7be568995c90c1ce8c813d3a58a6/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "sha": "ea129904f99464f95ac817a45064af2b69edb2da", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java?ref=f583cc60916e7be568995c90c1ce8c813d3a58a6", "patch": "@@ -150,6 +150,7 @@ private void runCycleAnalysisForPartitionPruning(OptimizeTezProcContext procCtx,\n           LOG.info(\"Found cycle in operator plan...\");\n           cycleFree = false;\n           removeEventOperator(component);\n+          break;\n         }\n       }\n       LOG.info(\"Cycle free: \" + cycleFree);\n@@ -227,7 +228,7 @@ private void connect(Operator<?> o, AtomicInteger index, Stack<Operator<?>> node\n     for (Operator<?> child : children) {\n       if (!indexes.containsKey(child)) {\n         connect(child, index, nodes, indexes, lowLinks, components);\n-        lowLinks.put(child, Math.min(lowLinks.get(o), lowLinks.get(child)));\n+        lowLinks.put(o, Math.min(lowLinks.get(o), lowLinks.get(child)));\n       } else if (nodes.contains(child)) {\n         lowLinks.put(o, Math.min(lowLinks.get(o), indexes.get(child)));\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f50f08876a6b84f66d5be52ac1727c62c2900fc5", "parent": "https://github.com/apache/hive/commit/f03ea3976b4f0ad8608128886643a5d47671ec93", "message": "HIVE-8778: ORC split elimination can cause NPE when column statistics is null (Prasanth J reviewed by Gopal V)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1637416 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_193", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "sha": "a6a0ec1a0e022efd23fddadeb44d5dfff8c092d7", "changes": 13, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=f50f08876a6b84f66d5be52ac1727c62c2900fc5", "patch": "@@ -2364,20 +2364,21 @@ static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n                                       PredicateLeaf predicate) {\n     ColumnStatistics cs = ColumnStatisticsImpl.deserialize(index);\n     Object minValue = getMin(cs);\n+    Object maxValue = getMax(cs);\n+    return evaluatePredicateRange(predicate, minValue, maxValue);\n+  }\n+\n+  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,\n+      Object max) {\n     // if we didn't have any values, everything must have been null\n-    if (minValue == null) {\n+    if (min == null) {\n       if (predicate.getOperator() == PredicateLeaf.Operator.IS_NULL) {\n         return TruthValue.YES;\n       } else {\n         return TruthValue.NULL;\n       }\n     }\n-    Object maxValue = getMax(cs);\n-    return evaluatePredicateRange(predicate, minValue, maxValue);\n-  }\n \n-  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,\n-      Object max) {\n     Location loc;\n     try {\n       // Predicate object and stats object can be one of the following base types", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java"}, {"additions": 125, "raw_url": "https://github.com/apache/hive/raw/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java", "blob_url": "https://github.com/apache/hive/blob/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java", "sha": "d1acd88797dd361415eddbaf9b46df10487d1807", "changes": 147, "status": "modified", "deletions": 22, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java?ref=f50f08876a6b84f66d5be52ac1727c62c2900fc5", "patch": "@@ -21,27 +21,6 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n-import java.io.DataInput;\n-import java.io.DataOutput;\n-import java.io.FileNotFoundException;\n-import java.io.IOException;\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.sql.Date;\n-import java.sql.Timestamp;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.Set;\n-import java.util.TimeZone;\n-import java.util.TreeSet;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.BlockLocation;\n import org.apache.hadoop.fs.FSDataInputStream;\n@@ -66,9 +45,9 @@\n import org.apache.hadoop.hive.ql.io.HiveInputFormat;\n import org.apache.hadoop.hive.ql.io.HiveOutputFormat;\n import org.apache.hadoop.hive.ql.io.InputFormatChecker;\n-import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory;\n import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory;\n import org.apache.hadoop.hive.ql.plan.MapWork;\n import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n@@ -104,6 +83,27 @@\n import org.junit.Test;\n import org.junit.rules.TestName;\n \n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.text.SimpleDateFormat;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.TimeZone;\n+import java.util.TreeSet;\n+\n public class TestInputOutputFormat {\n \n   Path workDir = new Path(System.getProperty(\"test.tmp.dir\",\"target/tmp\"));\n@@ -1032,6 +1032,24 @@ public void testInOutFormat() throws Exception {\n     reader.close();\n   }\n \n+  static class SimpleRow implements Writable {\n+    Text z;\n+\n+    public SimpleRow(Text t) {\n+      this.z = t;\n+    }\n+\n+    @Override\n+    public void write(DataOutput dataOutput) throws IOException {\n+      throw new UnsupportedOperationException(\"unsupported\");\n+    }\n+\n+    @Override\n+    public void readFields(DataInput dataInput) throws IOException {\n+      throw new UnsupportedOperationException(\"unsupported\");\n+    }\n+  }\n+\n   static class NestedRow implements Writable {\n     int z;\n     MyRow r;\n@@ -1685,4 +1703,89 @@ public void testSetSearchArgument() throws Exception {\n     assertEquals(\"cost\", leaves.get(0).getColumnName());\n     assertEquals(PredicateLeaf.Operator.IS_NULL, leaves.get(0).getOperator());\n   }\n+\n+  @Test\n+  @SuppressWarnings(\"unchecked,deprecation\")\n+  public void testSplitElimination() throws Exception {\n+    Properties properties = new Properties();\n+    StructObjectInspector inspector;\n+    synchronized (TestOrcFile.class) {\n+      inspector = (StructObjectInspector)\n+          ObjectInspectorFactory.getReflectionObjectInspector(NestedRow.class,\n+              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n+    }\n+    SerDe serde = new OrcSerde();\n+    OutputFormat<?, ?> outFormat = new OrcOutputFormat();\n+    conf.setInt(\"mapred.max.split.size\", 50);\n+    RecordWriter writer =\n+        outFormat.getRecordWriter(fs, conf, testFilePath.toString(),\n+            Reporter.NULL);\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new NestedRow(1,2,3), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new NestedRow(4,5,6), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new NestedRow(7,8,9), inspector));\n+    writer.close(Reporter.NULL);\n+    serde = new OrcSerde();\n+    SearchArgument sarg =\n+        SearchArgumentFactory.newBuilder()\n+            .startAnd()\n+            .lessThan(\"z\", new Integer(0))\n+            .end()\n+            .build();\n+    conf.set(\"sarg.pushdown\", sarg.toKryo());\n+    conf.set(\"hive.io.file.readcolumn.names\", \"z,r\");\n+    properties.setProperty(\"columns\", \"z,r\");\n+    properties.setProperty(\"columns.types\", \"int:struct<x:int,y:int>\");\n+    SerDeUtils.initializeSerDe(serde, conf, properties, null);\n+    inspector = (StructObjectInspector) serde.getObjectInspector();\n+    InputFormat<?,?> in = new OrcInputFormat();\n+    FileInputFormat.setInputPaths(conf, testFilePath.toString());\n+    InputSplit[] splits = in.getSplits(conf, 1);\n+    assertEquals(0, splits.length);\n+  }\n+\n+  @Test\n+  @SuppressWarnings(\"unchecked,deprecation\")\n+  public void testSplitEliminationNullStats() throws Exception {\n+    Properties properties = new Properties();\n+    StructObjectInspector inspector;\n+    synchronized (TestOrcFile.class) {\n+      inspector = (StructObjectInspector)\n+          ObjectInspectorFactory.getReflectionObjectInspector(SimpleRow.class,\n+              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n+    }\n+    SerDe serde = new OrcSerde();\n+    OutputFormat<?, ?> outFormat = new OrcOutputFormat();\n+    conf.setInt(\"mapred.max.split.size\", 50);\n+    RecordWriter writer =\n+        outFormat.getRecordWriter(fs, conf, testFilePath.toString(),\n+            Reporter.NULL);\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new SimpleRow(null), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new SimpleRow(null), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new SimpleRow(null), inspector));\n+    writer.close(Reporter.NULL);\n+    serde = new OrcSerde();\n+    SearchArgument sarg =\n+        SearchArgumentFactory.newBuilder()\n+            .startAnd()\n+            .lessThan(\"z\", new String(\"foo\"))\n+            .end()\n+            .build();\n+    conf.set(\"sarg.pushdown\", sarg.toKryo());\n+    conf.set(\"hive.io.file.readcolumn.names\", \"z\");\n+    properties.setProperty(\"columns\", \"z\");\n+    properties.setProperty(\"columns.types\", \"string\");\n+    SerDeUtils.initializeSerDe(serde, conf, properties, null);\n+    inspector = (StructObjectInspector) serde.getObjectInspector();\n+    InputFormat<?,?> in = new OrcInputFormat();\n+    FileInputFormat.setInputPaths(conf, testFilePath.toString());\n+    InputSplit[] splits = in.getSplits(conf, 1);\n+    assertEquals(0, splits.length);\n+  }\n+\n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/566544d1b780e873132f84660878261935b500df", "parent": "https://github.com/apache/hive/commit/8fb70dca8e0341b17147e9692fb234caecac6407", "message": "HIVE-8579: Guaranteed NPE in DDLSemanticAnalyzer (Jason Dere, reviewed by Vaibhav Gumashta)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1635354 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_194", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java", "blob_url": "https://github.com/apache/hive/blob/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java", "sha": "292c83cf967756ad3056f8a57de9590a0b00babf", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=566544d1b780e873132f84660878261935b500df", "patch": "@@ -422,7 +422,7 @@\n   ACID_NO_SORTED_BUCKETS(10298, \"ACID insert, update, delete not supported on tables that are \" +\n       \"sorted, table {0}\", true),\n   ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED(10299,\n-      \"Alter table partition type {0} does not allow partial partition spec\"),\n+      \"Alter table partition type {0} does not allow partial partition spec\", true),\n \n   //========================== 20000 range starts here ========================//\n   SCRIPT_INIT_ERROR(20000, \"Unable to initialize custom script.\"),", "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "sha": "f3c01a8b4b6ddff2af02f95c091305288e495b27", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java?ref=566544d1b780e873132f84660878261935b500df", "patch": "@@ -1396,8 +1396,9 @@ private void addInputsOutputsAlterTable(String tableName, Map<String, String> pa\n         // Partial partition spec supplied. Make sure this is allowed.\n         if (desc == null\n             || !AlterTableDesc.doesAlterTableTypeSupportPartialPartitionSpec(desc.getOp())) {\n+          String alterTabletype = (desc != null) ? desc.getOp().name() : \"\";\n           throw new SemanticException(\n-              ErrorMsg.ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED, desc.getOp().name());\n+              ErrorMsg.ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED, alterTabletype);\n         } else if (!conf.getBoolVar(HiveConf.ConfVars.DYNAMICPARTITIONING)) {\n           throw new SemanticException(ErrorMsg.DYNAMIC_PARTITION_DISABLED);\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/9051be28c7b51ea7f6e2b3a9faa04824c0772334", "parent": "https://github.com/apache/hive/commit/8aaa49d4f83807e94e66ea58e6a0a07ebf3464af", "message": "HIVE-8628: NPE in case of shuffle join in tez (Vikram Dixit K via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1634922 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_195", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/9051be28c7b51ea7f6e2b3a9faa04824c0772334/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/9051be28c7b51ea7f6e2b3a9faa04824c0772334/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java", "sha": "7487f7e061b07ecf57797c1830695d0e958f207d", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java?ref=9051be28c7b51ea7f6e2b3a9faa04824c0772334", "patch": "@@ -306,6 +306,10 @@ private void fetchNextGroup(Byte t) throws HiveException {\n   public void closeOp(boolean abort) throws HiveException {\n     joinFinalLeftData();\n \n+    if (!((joinKeysObjectInspectors != null) && (joinKeysObjectInspectors[alias] != null))) {\n+      super.closeOp(abort);\n+    }\n+\n     // clean up\n     for (int pos = 0; pos < order.length; pos++) {\n       if (pos != posBigTable) {\n@@ -362,6 +366,9 @@ private void joinFinalLeftData() throws HiveException {\n       joinOneGroup();\n       dataInCache = false;\n       for (byte pos = 0; pos < order.length; pos++) {\n+        if (candidateStorage[pos] == null) {\n+          continue;\n+        }\n         if (this.candidateStorage[pos].rowCount() > 0) {\n           dataInCache = true;\n           break;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/76f780a545465d2ee674c896a0bd50540edd72e0", "parent": "https://github.com/apache/hive/commit/ce7a8b7f53f9cd52ed5e7f0fb427e25ce32fe30f", "message": "HIVE-8563: Running annotate_stats_join_pkfk.q in TestMiniTezCliDriver is causing NPE (Vikram Dixit K via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633986 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_196", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "sha": "f231b0655454085ac447b2df891b1a54e3efed1f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0", "patch": "@@ -228,7 +228,7 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n     @SuppressWarnings(\"unchecked\")\n     CommonMergeJoinOperator mergeJoinOp =\n         (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n-            isSubQuery, mapJoinConversionPos, mapJoinDesc));\n+            isSubQuery, mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n     OpTraits opTraits =\n         new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n             .getSortCols());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java", "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java", "sha": "e3c87277c1c3df47bacdd673210cd961409b78ee", "changes": 21, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0", "patch": "@@ -22,21 +22,6 @@\n import org.apache.hadoop.hive.ql.plan.TezWork.VertexType;\n \n public class MergeJoinProc implements NodeProcessor {\n-\n-  public Operator<? extends OperatorDesc> getLeafOperator(Operator<? extends OperatorDesc> op) {\n-    for (Operator<? extends OperatorDesc> childOp : op.getChildOperators()) {\n-      // FileSink or ReduceSink operators are used to create vertices. See\n-      // TezCompiler.\n-      if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof FileSinkOperator)) {\n-        return childOp;\n-      } else {\n-        return getLeafOperator(childOp);\n-      }\n-    }\n-\n-    return null;\n-  }\n-\n   @Override\n   public Object\n       process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)\n@@ -60,13 +45,13 @@\n     // merge work already exists for this merge join operator, add the dummy store work to the\n     // merge work. Else create a merge work, add above work to the merge work\n     MergeJoinWork mergeWork = null;\n-    if (context.opMergeJoinWorkMap.containsKey(getLeafOperator(mergeJoinOp))) {\n+    if (context.opMergeJoinWorkMap.containsKey(mergeJoinOp)) {\n       // we already have the merge work corresponding to this merge join operator\n-      mergeWork = context.opMergeJoinWorkMap.get(getLeafOperator(mergeJoinOp));\n+      mergeWork = context.opMergeJoinWorkMap.get(mergeJoinOp);\n     } else {\n       mergeWork = new MergeJoinWork();\n       tezWork.add(mergeWork);\n-      context.opMergeJoinWorkMap.put(getLeafOperator(mergeJoinOp), mergeWork);\n+      context.opMergeJoinWorkMap.put(mergeJoinOp, mergeWork);\n     }\n \n     mergeWork.setMergeJoinOperator(mergeJoinOp);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java", "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java", "sha": "59a632776f811dafd2fcd23ec3264b7864535b8f", "changes": 19, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0", "patch": "@@ -137,15 +137,15 @@ public Object process(Node nd, Stack<Node> stack,\n       // we are currently walking the big table side of the merge join. we need to create or hook up\n       // merge join work.\n       MergeJoinWork mergeJoinWork = null;\n-      if (context.opMergeJoinWorkMap.containsKey(operator)) {\n+      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n         // we have found a merge work corresponding to this closing operator. Hook up this work.\n-        mergeJoinWork = context.opMergeJoinWorkMap.get(operator);\n+        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n       } else {\n         // we need to create the merge join work\n         mergeJoinWork = new MergeJoinWork();\n         mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n         tezWork.add(mergeJoinWork);\n-        context.opMergeJoinWorkMap.put(operator, mergeJoinWork);\n+        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n       }\n       // connect the work correctly.\n       mergeJoinWork.addMergedWork(work, null);\n@@ -334,10 +334,15 @@ public Object process(Node nd, Stack<Node> stack,\n           UnionWork unionWork = (UnionWork) followingWork;\n           int index = getMergeIndex(tezWork, unionWork, rs);\n           // guaranteed to be instance of MergeJoinWork if index is valid\n-          MergeJoinWork mergeJoinWork = (MergeJoinWork) tezWork.getChildren(unionWork).get(index);\n-          // disconnect the connection to union work and connect to merge work\n-          followingWork = mergeJoinWork;\n-          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n+          BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n+          if (baseWork instanceof MergeJoinWork) {\n+            MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n+            // disconnect the connection to union work and connect to merge work\n+            followingWork = mergeJoinWork;\n+            rWork = (ReduceWork) mergeJoinWork.getMainWork();\n+          } else {\n+            rWork = (ReduceWork) baseWork;\n+          }\n         } else {\n           rWork = (ReduceWork) followingWork;\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2c16fdbc914cd0b6cf71831b4ced3923858f47c0", "parent": "https://github.com/apache/hive/commit/b4fd2fc3ad7206efd04fb256ec2060531afd3e0b", "message": "HIVE-8549: NPE in PK-FK inference when one side of join is complex tree (Prasanth J via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633736 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_197", "file": [{"additions": 52, "raw_url": "https://github.com/apache/hive/raw/2c16fdbc914cd0b6cf71831b4ced3923858f47c0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/2c16fdbc914cd0b6cf71831b4ced3923858f47c0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "sha": "4b9f2927f7f630d598ece319c2ae394fe0b2ac31", "changes": 56, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java?ref=2c16fdbc914cd0b6cf71831b4ced3923858f47c0", "patch": "@@ -1293,13 +1293,61 @@ private void inferPKFKRelationship() {\n       List<Float> result = Lists.newArrayList();\n       for (Integer idx : opsWithPK) {\n         Operator<? extends OperatorDesc> op = ops.get(idx);\n-        TableScanOperator tsOp = OperatorUtils\n-            .findSingleOperatorUpstream(op, TableScanOperator.class);\n+        float selectivity = getSelectivitySimpleTree(op);\n+        result.add(selectivity);\n+      }\n+      return result;\n+    }\n+\n+    private float getSelectivitySimpleTree(Operator<? extends OperatorDesc> op) {\n+      TableScanOperator tsOp = OperatorUtils\n+          .findSingleOperatorUpstream(op, TableScanOperator.class);\n+      if (tsOp == null) {\n+        // complex tree with multiple parents\n+        return getSelectivityComplexTree(op);\n+      } else {\n+        // simple tree with single parent\n         long inputRow = tsOp.getStatistics().getNumRows();\n         long outputRow = op.getStatistics().getNumRows();\n-        result.add((float) outputRow / (float) inputRow);\n+        return (float) outputRow / (float) inputRow;\n       }\n-      return result;\n+    }\n+\n+    private float getSelectivityComplexTree(Operator<? extends OperatorDesc> op) {\n+      Operator<? extends OperatorDesc> multiParentOp = null;\n+      Operator<? extends OperatorDesc> currentOp = op;\n+\n+      // TS-1      TS-2\n+      //  |          |\n+      // RS-1      RS-2\n+      //    \\      /\n+      //      JOIN\n+      //        |\n+      //       FIL\n+      //        |\n+      //       RS-3\n+      //\n+      // For the above complex operator tree,\n+      // selectivity(JOIN) = selectivity(RS-1) * selectivity(RS-2) and\n+      // selectivity(RS-3) = numRows(RS-3)/numRows(JOIN) * selectivity(JOIN)\n+      while(multiParentOp == null) {\n+        if (op.getParentOperators().size() > 1) {\n+          multiParentOp = op;\n+        } else {\n+          op = op.getParentOperators().get(0);\n+        }\n+      }\n+\n+      float selMultiParent = 1.0f;\n+      for(Operator<? extends OperatorDesc> parent : multiParentOp.getParentOperators()) {\n+        // In the above example, TS-1 -> RS-1 and TS-2 -> RS-2 are simple trees\n+        selMultiParent *= getSelectivitySimpleTree(parent);\n+      }\n+\n+      float selCurrOp = ((float) currentOp.getStatistics().getNumRows() /\n+          (float) multiParentOp.getStatistics().getNumRows()) * selMultiParent;\n+\n+      return selCurrOp;\n     }\n \n     /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/0af2fb0cac270cd6223a4abaf6e410f05450bfef", "parent": "https://github.com/apache/hive/commit/4dd080d286a6db1df33ad643ac618407c1af1a45", "message": "HIVE-8551 : NPE in FunctionRegistry (affects CBO in negative tests) (Sergey Shelukhin, reviewed by Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633692 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_198", "file": [{"additions": 9, "raw_url": "https://github.com/apache/hive/raw/0af2fb0cac270cd6223a4abaf6e410f05450bfef/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java", "blob_url": "https://github.com/apache/hive/blob/0af2fb0cac270cd6223a4abaf6e410f05450bfef/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java", "sha": "451598aa964786e118307643cf92c6f31fcab9aa", "changes": 17, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java?ref=0af2fb0cac270cd6223a4abaf6e410f05450bfef", "patch": "@@ -1992,17 +1992,18 @@ public static void registerTableFunction(String name, Class<? extends TableFunct\n    * @return true if function is a UDAF, has WindowFunctionDescription annotation and the annotations\n    *         confirms a ranking function, false otherwise\n    */\n-  public static boolean isRankingFunction(String name){\n+  public static boolean isRankingFunction(String name) {\n     FunctionInfo info = getFunctionInfo(name);\n+    if (info == null) {\n+      return false;\n+    }\n     GenericUDAFResolver res = info.getGenericUDAFResolver();\n-    if (res != null){\n-      WindowFunctionDescription desc =\n-          AnnotationUtils.getAnnotation(res.getClass(), WindowFunctionDescription.class);\n-      if (desc != null){\n-        return desc.rankingFunction();\n-      }\n+    if (res == null) {\n+      return false;\n     }\n-    return false;\n+    WindowFunctionDescription desc =\n+        AnnotationUtils.getAnnotation(res.getClass(), WindowFunctionDescription.class);\n+    return (desc != null) && desc.rankingFunction();\n   }\n \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ec953bdc45f69118b84e0d4789f58f11e5207f1b", "parent": "https://github.com/apache/hive/commit/8cb8121ae707b1652b1959f23ae9d28f7f51b34c", "message": "HIVE-8332 Reading an ACID table with vectorization on results in NPE (Alan Gates, reviewed by Owen O'Malley)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1631536 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_199", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "sha": "b2b1a4132d3a78222118f686621d7b5d17cc4ecb", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b", "patch": "@@ -1134,7 +1134,7 @@ public float getProgress() throws IOException {\n \n       @Override\n       public ObjectInspector getObjectInspector() {\n-        return ((StructObjectInspector) reader.getObjectInspector())\n+        return ((StructObjectInspector) records.getObjectInspector())\n             .getAllStructFieldRefs().get(OrcRecordUpdater.ROW)\n             .getFieldObjectInspector();\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java", "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java", "sha": "fd97cb93037ab51f4f64f0fce462c437bac698d8", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b", "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hive.ql.io.AcidInputFormat;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n import org.apache.hadoop.hive.ql.io.RecordIdentifier;\n+import org.apache.hadoop.hive.ql.metadata.VirtualColumn;\n import org.apache.hadoop.hive.serde.serdeConstants;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n@@ -37,9 +38,10 @@\n import org.apache.hadoop.io.LongWritable;\n \n import java.io.IOException;\n+import java.util.ArrayDeque;\n import java.util.ArrayList;\n import java.util.Arrays;\n-import java.util.Collections;\n+import java.util.Deque;\n import java.util.List;\n import java.util.Map;\n import java.util.TreeMap;\n@@ -627,8 +629,16 @@ public ObjectInspector getObjectInspector() {\n \n     // Parse the configuration parameters\n     ArrayList<String> columnNames = new ArrayList<String>();\n+    Deque<Integer> virtualColumns = new ArrayDeque<Integer>();\n     if (columnNameProperty != null && columnNameProperty.length() > 0) {\n-      Collections.addAll(columnNames, columnNameProperty.split(\",\"));\n+      String[] colNames = columnNameProperty.split(\",\");\n+      for (int i = 0; i < colNames.length; i++) {\n+        if (VirtualColumn.VIRTUAL_COLUMN_NAMES.contains(colNames[i])) {\n+          virtualColumns.addLast(i);\n+        } else {\n+          columnNames.add(colNames[i]);\n+        }\n+      }\n     }\n     if (columnTypeProperty == null) {\n       // Default type: all string\n@@ -644,6 +654,9 @@ public ObjectInspector getObjectInspector() {\n \n     ArrayList<TypeInfo> fieldTypes =\n         TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n+    while (virtualColumns.size() > 0) {\n+      fieldTypes.remove(virtualColumns.removeLast());\n+    }\n     StructTypeInfo rowType = new StructTypeInfo();\n     rowType.setAllStructFieldNames(columnNames);\n     rowType.setAllStructFieldTypeInfos(fieldTypes);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java", "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java", "sha": "a15a7a7951b5ea35c7d5787ee5357c2f9773658c", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b", "patch": "@@ -73,6 +73,7 @@\n import org.apache.hadoop.hive.ql.plan.MapWork;\n import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n import org.apache.hadoop.hive.serde2.SerDe;\n import org.apache.hadoop.hive.serde2.SerDeUtils;\n@@ -1266,6 +1267,8 @@ JobConf createMockExecutionEnvironment(Path workDir,\n     }\n     conf.set(\"hive.io.file.readcolumn.ids\", columnIds.toString());\n     conf.set(\"partition_columns\", \"p\");\n+    conf.set(serdeConstants.LIST_COLUMNS, columnNames.toString());\n+    conf.set(serdeConstants.LIST_COLUMN_TYPES, columnTypes.toString());\n     MockFileSystem fs = (MockFileSystem) warehouseDir.getFileSystem(conf);\n     fs.clear();\n ", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/queries/clientpositive/acid_vectorization.q", "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/queries/clientpositive/acid_vectorization.q", "sha": "4b114125e77830989dc9fccd70c1afe9ed981408", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/acid_vectorization.q?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b", "patch": "@@ -12,3 +12,5 @@ set hive.vectorized.execution.enabled=true;\n update acid_vectorized set b = 'foo' where b = 'bar';\n set hive.vectorized.execution.enabled=true;\n delete from acid_vectorized where b = 'foo';\n+set hive.vectorized.execution.enabled=true;\n+select a, b from acid_vectorized order by a, b;", "filename": "ql/src/test/queries/clientpositive/acid_vectorization.q"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/acid_vectorization.q.out", "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/acid_vectorization.q.out", "sha": "1792979156ec361c85882ac8b6968e93d42b5f31", "changes": 18, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/acid_vectorization.q.out?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b", "patch": "@@ -42,3 +42,21 @@ POSTHOOK: query: delete from acid_vectorized where b = 'foo'\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@acid_vectorized\n POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: select a, b from acid_vectorized order by a, b\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+POSTHOOK: query: select a, b from acid_vectorized order by a, b\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+-1073279343\toj1YrV5Wa\n+-1073051226\tA34p7oRr2WvUJNf\n+-1072910839\t0iqrc5\n+-1072081801\tdPkN74F7\n+-1072076362\t2uLyD28144vklju213J1mr\n+-1071480828\taw724t8c5558x2xneC624\n+-1071363017\tAnj0oF\n+-1070883071\t0ruyd6Y50JpdGRf6HqD\n+-1070551679\tiUR3Q\n+-1069736047\tk17Am8uPHWk02cEf1jet", "filename": "ql/src/test/results/clientpositive/acid_vectorization.q.out"}, {"additions": 62, "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/tez/acid_vectorization.q.out", "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/tez/acid_vectorization.q.out", "sha": "1792979156ec361c85882ac8b6968e93d42b5f31", "changes": 62, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/acid_vectorization.q.out?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b", "patch": "@@ -0,0 +1,62 @@\n+PREHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+PREHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: delete from acid_vectorized where b = 'foo'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: delete from acid_vectorized where b = 'foo'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: select a, b from acid_vectorized order by a, b\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+POSTHOOK: query: select a, b from acid_vectorized order by a, b\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+-1073279343\toj1YrV5Wa\n+-1073051226\tA34p7oRr2WvUJNf\n+-1072910839\t0iqrc5\n+-1072081801\tdPkN74F7\n+-1072076362\t2uLyD28144vklju213J1mr\n+-1071480828\taw724t8c5558x2xneC624\n+-1071363017\tAnj0oF\n+-1070883071\t0ruyd6Y50JpdGRf6HqD\n+-1070551679\tiUR3Q\n+-1069736047\tk17Am8uPHWk02cEf1jet", "filename": "ql/src/test/results/clientpositive/tez/acid_vectorization.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e412e10f47a6dc16333cb400e0e55ff5bc554a13", "parent": "https://github.com/apache/hive/commit/3c57bdc6626d7a827c3e1f0ac14ed8c510961016", "message": "HIVE-8361 NPE in PTFOperator when there are empty partitions (Harish Butani via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1631483 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_200", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java", "blob_url": "https://github.com/apache/hive/blob/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java", "sha": "2e6a880010c8e932cbc04f37d1dca6bd52a1171a", "changes": 19, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java?ref=e412e10f47a6dc16333cb400e0e55ff5bc554a13", "patch": "@@ -337,17 +337,20 @@ void finishPartition() throws HiveException {\n         handleOutputRows(tabFn.finishPartition());\n       } else {\n         if ( tabFn.canIterateOutput() ) {\n-          outputPartRowsItr = tabFn.iterator(inputPart.iterator());\n+          outputPartRowsItr = inputPart == null ? null :\n+            tabFn.iterator(inputPart.iterator());\n         } else {\n-          outputPart = tabFn.execute(inputPart);\n-          outputPartRowsItr = outputPart.iterator();\n+          outputPart = inputPart == null ? null : tabFn.execute(inputPart);\n+          outputPartRowsItr = outputPart == null ? null : outputPart.iterator();\n         }\n         if ( next != null ) {\n           if (!next.isStreaming() && !isOutputIterator() ) {\n             next.inputPart = outputPart;\n           } else {\n-            while(outputPartRowsItr.hasNext() ) {\n-              next.processRow(outputPartRowsItr.next());\n+            if ( outputPartRowsItr != null ) {\n+              while(outputPartRowsItr.hasNext() ) {\n+                next.processRow(outputPartRowsItr.next());\n+              }\n             }\n           }\n         }\n@@ -357,8 +360,10 @@ void finishPartition() throws HiveException {\n         next.finishPartition();\n       } else {\n         if (!isStreaming() ) {\n-          while(outputPartRowsItr.hasNext() ) {\n-            forward(outputPartRowsItr.next(), outputObjInspector);\n+          if ( outputPartRowsItr != null ) {\n+            while(outputPartRowsItr.hasNext() ) {\n+              forward(outputPartRowsItr.next(), outputObjInspector);\n+            }\n           }\n         }\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/PTFOperator.java"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/queries/clientpositive/ptf_matchpath.q", "blob_url": "https://github.com/apache/hive/blob/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/queries/clientpositive/ptf_matchpath.q", "sha": "2c1d76681b75edd8e003830547525dbcfaababb0", "changes": 12, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/ptf_matchpath.q?ref=e412e10f47a6dc16333cb400e0e55ff5bc554a13", "patch": "@@ -32,5 +32,15 @@ from matchpath(on\n       arg2('LATE'), arg3(arr_delay > 15), \n     arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath') \n    )\n-where fl_num = 1142;       \n+where fl_num = 1142;\n+\n+-- 3. empty partition.\n+select origin_city_name, fl_num, year, month, day_of_month, sz, tpath\n+from matchpath(on\n+        (select * from flights_tiny where fl_num = -1142) flights_tiny\n+        sort by fl_num, year, month, day_of_month\n+      arg1('LATE.LATE+'),\n+      arg2('LATE'), arg3(arr_delay > 15),\n+    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')\n+   );\n    \n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/ptf_matchpath.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/queries/clientpositive/windowing.q", "blob_url": "https://github.com/apache/hive/blob/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/queries/clientpositive/windowing.q", "sha": "8dceafa4f29f697e312b45b1c3d70e755466a6df", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/windowing.q?ref=e412e10f47a6dc16333cb400e0e55ff5bc554a13", "patch": "@@ -444,3 +444,7 @@ select p_retailprice, avg(p_retailprice) over (partition by p_mfgr order by p_na\n sum(p_retailprice) over (partition by p_mfgr order by p_name rows between current row and 6 following) \n from part \n where p_mfgr='Manufacturer#1';\n+\n+-- 47. empty partition\n+select sum(p_size) over (partition by p_mfgr )\n+from part where p_mfgr = 'm1';", "filename": "ql/src/test/queries/clientpositive/windowing.q"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/results/clientpositive/ptf_matchpath.q.out", "blob_url": "https://github.com/apache/hive/blob/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/results/clientpositive/ptf_matchpath.q.out", "sha": "fc0187b513fbd67b2926fd39981f14a1c6a22342", "changes": 24, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/ptf_matchpath.q.out?ref=e412e10f47a6dc16333cb400e0e55ff5bc554a13", "patch": "@@ -107,3 +107,27 @@ Baltimore\t1142\t2010\t10\t21\t5\t21\n Baltimore\t1142\t2010\t10\t22\t4\t22\n Baltimore\t1142\t2010\t10\t25\t3\t25\n Baltimore\t1142\t2010\t10\t26\t2\t26\n+PREHOOK: query: -- 3. empty partition.\n+select origin_city_name, fl_num, year, month, day_of_month, sz, tpath\n+from matchpath(on\n+        (select * from flights_tiny where fl_num = -1142) flights_tiny\n+        sort by fl_num, year, month, day_of_month\n+      arg1('LATE.LATE+'),\n+      arg2('LATE'), arg3(arr_delay > 15),\n+    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')\n+   )\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@flights_tiny\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- 3. empty partition.\n+select origin_city_name, fl_num, year, month, day_of_month, sz, tpath\n+from matchpath(on\n+        (select * from flights_tiny where fl_num = -1142) flights_tiny\n+        sort by fl_num, year, month, day_of_month\n+      arg1('LATE.LATE+'),\n+      arg2('LATE'), arg3(arr_delay > 15),\n+    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')\n+   )\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@flights_tiny\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/ptf_matchpath.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/results/clientpositive/windowing.q.out", "blob_url": "https://github.com/apache/hive/blob/e412e10f47a6dc16333cb400e0e55ff5bc554a13/ql/src/test/results/clientpositive/windowing.q.out", "sha": "a16e73a200402149a28f60146d4077fc9d8e9171", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/windowing.q.out?ref=e412e10f47a6dc16333cb400e0e55ff5bc554a13", "patch": "@@ -2362,3 +2362,15 @@ POSTHOOK: Input: default@part\n 1602.59\t1549.8900000000003\t4649.670000000001\n 1414.42\t1523.5400000000004\t3047.080000000001\n 1632.66\t1632.6600000000008\t1632.6600000000008\n+PREHOOK: query: -- 47. empty partition\n+select sum(p_size) over (partition by p_mfgr )\n+from part where p_mfgr = 'm1'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@part\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- 47. empty partition\n+select sum(p_size) over (partition by p_mfgr )\n+from part where p_mfgr = 'm1'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@part\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/windowing.q.out"}], "repo": "hive"}]
[{"commit": "https://github.com/apache/hive/commit/460d97daa215607674ea7516fec1692c443d7bc8", "parent": "https://github.com/apache/hive/commit/a112a576871e8ee778d0e1c719aef7a467875cfc", "message": "HIVE-8408 : hcat cli throws NPE when authorizer using new api is enabled (Thejas Nair, reviewed by Sushanth Sowmyan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1630996 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_201", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java", "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java", "sha": "8b6922325e3a7096e8e048dc1664a38e16efc039", "changes": 4, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java?ref=460d97daa215607674ea7516fec1692c443d7bc8", "patch": "@@ -26,7 +26,6 @@\n \n import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.ql.exec.DDLTask;\n import org.apache.hadoop.hive.ql.exec.Task;\n@@ -195,8 +194,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,\n \n         //authorize against the table operation so that location permissions can be checked if any\n \n-        if (HiveConf.getBoolVar(context.getConf(),\n-          HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {\n+        if (HCatAuthUtil.isAuthorizationEnabled(context.getConf())) {\n           authorize(table, Privilege.CREATE);\n         }\n       } catch (HiveException ex) {", "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java"}, {"additions": 36, "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java", "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java", "sha": "6dce9c4b1d1218ce1f8b4ebffee0eaacb31d25cd", "changes": 36, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java?ref=460d97daa215607674ea7516fec1692c443d7bc8", "patch": "@@ -0,0 +1,36 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hive.hcatalog.cli.SemanticAnalysis;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+\n+final class HCatAuthUtil {\n+  public static boolean isAuthorizationEnabled(Configuration conf) {\n+    // the session state getAuthorizer can return null even if authorization is\n+    // enabled if the V2 api of authorizer in use.\n+    // The additional authorization checks happening in hcatalog are designed to\n+    // work with  storage based authorization (on client side). It should not try doing\n+    // additional checks if a V2 authorizer is in use. The reccomended configuration is to\n+    // use storage based authorization in metastore server\n+    return HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)\n+        && SessionState.get().getAuthorizer() != null;\n+  }\n+}", "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java", "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java", "sha": "5b3ef946613a0b26b3dab340b44f8eb9338ccaf7", "changes": 4, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java?ref=460d97daa215607674ea7516fec1692c443d7bc8", "patch": "@@ -22,7 +22,6 @@\n import java.io.Serializable;\n import java.util.List;\n \n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.metadata.AuthorizationException;\n@@ -89,8 +88,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,\n   protected void authorizeDDL(HiveSemanticAnalyzerHookContext context,\n                 List<Task<? extends Serializable>> rootTasks) throws SemanticException {\n \n-    if (!HiveConf.getBoolVar(context.getConf(),\n-      HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {\n+    if (!HCatAuthUtil.isAuthorizationEnabled(context.getConf())) {\n       return;\n     }\n ", "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java"}, {"additions": 84, "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java", "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java", "sha": "830dcb8119c255127f64a128a1d7b66f041bfb3e", "changes": 84, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java?ref=460d97daa215607674ea7516fec1692c443d7bc8", "patch": "@@ -0,0 +1,84 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hive.hcatalog.cli.SemanticAnalysis;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactory;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+/**\n+ * Test HCatAuthUtil\n+ */\n+public class TestHCatAuthUtil {\n+\n+  public static class DummyV2AuthorizerFactory implements HiveAuthorizerFactory {\n+\n+    @Override\n+    public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreClientFactory,\n+        HiveConf conf, HiveAuthenticationProvider hiveAuthenticator, HiveAuthzSessionContext ctx)\n+        throws HiveAuthzPluginException {\n+      return Mockito.mock(HiveAuthorizer.class);\n+    }\n+  }\n+\n+  /**\n+   * Test with auth enabled and v1 auth\n+   */\n+  @Test\n+  public void authEnabledV1Auth() throws Exception {\n+    HiveConf hcatConf = new HiveConf(this.getClass());\n+    hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);\n+    SessionState.start(hcatConf);\n+    assertTrue(\"hcat auth should be enabled\", HCatAuthUtil.isAuthorizationEnabled(hcatConf));\n+  }\n+\n+  /**\n+   * Test with auth enabled and v2 auth\n+   */\n+  @Test\n+  public void authEnabledV2Auth() throws Exception {\n+    HiveConf hcatConf = new HiveConf(this.getClass());\n+    hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);\n+    hcatConf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, DummyV2AuthorizerFactory.class.getName());\n+    SessionState.start(hcatConf);\n+    assertFalse(\"hcat auth should be disabled\", HCatAuthUtil.isAuthorizationEnabled(hcatConf));\n+  }\n+\n+  /**\n+   * Test with auth disabled\n+   */\n+  @Test\n+  public void authDisabled() throws Exception {\n+    HiveConf hcatConf = new HiveConf(this.getClass());\n+    hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, false);\n+    SessionState.start(hcatConf);\n+    assertFalse(\"hcat auth should be disabled\", HCatAuthUtil.isAuthorizationEnabled(hcatConf));\n+  }\n+}", "filename": "hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/pom.xml", "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/pom.xml", "sha": "cff3837dd20df14e0950c30a1db1da8b61df6339", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/pom.xml?ref=460d97daa215607674ea7516fec1692c443d7bc8", "patch": "@@ -46,6 +46,15 @@\n     <module>streaming</module>\n   </modules>\n \n+  <dependencies>\n+    <dependency>\n+      <groupId>org.mockito</groupId>\n+      <artifactId>mockito-all</artifactId>\n+      <version>${mockito-all.version}</version>\n+      <scope>test</scope>\n+    </dependency>\n+  </dependencies>\n+\n   <profiles>\n     <profile>\n       <id>hadoop-1</id>", "filename": "hcatalog/pom.xml"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8d8f503affd3441ad9da9b621c8e3875aea29133", "parent": "https://github.com/apache/hive/commit/61464cef0379e63eb1223722a526dfa949fa0626", "message": "HIVE-8372: Potential NPE in Tez MergeFileRecordProcessor (Prasanth J reviewed by Vikram Dixit)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1630011 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_202", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/8d8f503affd3441ad9da9b621c8e3875aea29133/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java", "blob_url": "https://github.com/apache/hive/blob/8d8f503affd3441ad9da9b621c8e3875aea29133/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java", "sha": "04cd4256676586c99c6b6e427a012e52ec4983ca", "changes": 21, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java?ref=8d8f503affd3441ad9da9b621c8e3875aea29133", "patch": "@@ -17,10 +17,13 @@\n  */\n package org.apache.hadoop.hive.ql.exec.tez;\n \n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.ObjectCacheFactory;\n import org.apache.hadoop.hive.ql.exec.Operator;\n@@ -41,10 +44,6 @@\n import org.apache.tez.runtime.api.ProcessorContext;\n import org.apache.tez.runtime.library.api.KeyValueReader;\n \n-import java.io.IOException;\n-import java.util.Map;\n-import java.util.Map.Entry;\n-\n /**\n  * Record processor for fast merging of files.\n  */\n@@ -93,17 +92,17 @@ void init(JobConf jconf, ProcessorContext processorContext,\n       MapWork mapWork = (MapWork) cache.retrieve(MAP_PLAN_KEY);\n       if (mapWork == null) {\n         mapWork = Utilities.getMapWork(jconf);\n-        if (mapWork instanceof MergeFileWork) {\n-          mfWork = (MergeFileWork) mapWork;\n-        } else {\n-          throw new RuntimeException(\"MapWork should be an instance of\" +\n-              \" MergeFileWork.\");\n-        }\n         cache.cache(MAP_PLAN_KEY, mapWork);\n       } else {\n         Utilities.setMapWork(jconf, mapWork);\n       }\n \n+      if (mapWork instanceof MergeFileWork) {\n+        mfWork = (MergeFileWork) mapWork;\n+      } else {\n+        throw new RuntimeException(\"MapWork should be an instance of MergeFileWork.\");\n+      }\n+\n       String alias = mfWork.getAliasToWork().keySet().iterator().next();\n       mergeOp = mfWork.getAliasToWork().get(alias);\n       LOG.info(mergeOp.dump(0));", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/68fa251c4c37440e5f941cfbd144aa9ed25077fd", "parent": "https://github.com/apache/hive/commit/cc93b9cb2eb63a976726761af05d49a040a49afe", "message": "HIVE-8272 : Query with particular decimal expression causes NPE during execution initialization (Jason Dere via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1629914 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_203", "file": [{"additions": 13, "raw_url": "https://github.com/apache/hive/raw/68fa251c4c37440e5f941cfbd144aa9ed25077fd/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java", "blob_url": "https://github.com/apache/hive/blob/68fa251c4c37440e5f941cfbd144aa9ed25077fd/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java", "sha": "686befdd7b0cd791b4bb4de3651664d03aa67fde", "changes": 15, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java?ref=68fa251c4c37440e5f941cfbd144aa9ed25077fd", "patch": "@@ -212,7 +212,14 @@ public static ExprNodeDesc backtrack(ExprNodeDesc source, Operator<?> current,\n     if (source instanceof ExprNodeGenericFuncDesc) {\n       // all children expression should be resolved\n       ExprNodeGenericFuncDesc function = (ExprNodeGenericFuncDesc) source.clone();\n-      function.setChildren(backtrack(function.getChildren(), current, terminal));\n+      List<ExprNodeDesc> children = backtrack(function.getChildren(), current, terminal);\n+      for (ExprNodeDesc child : children) {\n+        if (child == null) {\n+          // Could not resolve all of the function children, fail\n+          return null;\n+        }\n+      }\n+      function.setChildren(children);\n       return function;\n     }\n     if (source instanceof ExprNodeColumnDesc) {\n@@ -222,7 +229,11 @@ public static ExprNodeDesc backtrack(ExprNodeDesc source, Operator<?> current,\n     if (source instanceof ExprNodeFieldDesc) {\n       // field expression should be resolved\n       ExprNodeFieldDesc field = (ExprNodeFieldDesc) source.clone();\n-      field.setDesc(backtrack(field.getDesc(), current, terminal));\n+      ExprNodeDesc fieldDesc = backtrack(field.getDesc(), current, terminal);\n+      if (fieldDesc == null) {\n+        return null;\n+      }\n+      field.setDesc(fieldDesc);\n       return field;\n     }\n     // constant or null expr, just return", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/68fa251c4c37440e5f941cfbd144aa9ed25077fd/ql/src/test/queries/clientpositive/sum_expr_with_order.q", "blob_url": "https://github.com/apache/hive/blob/68fa251c4c37440e5f941cfbd144aa9ed25077fd/ql/src/test/queries/clientpositive/sum_expr_with_order.q", "sha": "4de9837ac333f757b20f1cbc3b61c6512f6a5211", "changes": 5, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/sum_expr_with_order.q?ref=68fa251c4c37440e5f941cfbd144aa9ed25077fd", "patch": "@@ -0,0 +1,5 @@\n+\n+select \n+cast(sum(key)*100 as decimal(15,3)) as c1\n+from src\n+order by c1;", "filename": "ql/src/test/queries/clientpositive/sum_expr_with_order.q"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/68fa251c4c37440e5f941cfbd144aa9ed25077fd/ql/src/test/results/clientpositive/sum_expr_with_order.q.out", "blob_url": "https://github.com/apache/hive/blob/68fa251c4c37440e5f941cfbd144aa9ed25077fd/ql/src/test/results/clientpositive/sum_expr_with_order.q.out", "sha": "00318e875ca345085a9ec298ae781716a54d521f", "changes": 15, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/sum_expr_with_order.q.out?ref=68fa251c4c37440e5f941cfbd144aa9ed25077fd", "patch": "@@ -0,0 +1,15 @@\n+PREHOOK: query: select \n+cast(sum(key)*100 as decimal(15,3)) as c1\n+from src\n+order by c1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select \n+cast(sum(key)*100 as decimal(15,3)) as c1\n+from src\n+order by c1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+13009100", "filename": "ql/src/test/results/clientpositive/sum_expr_with_order.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1be4ee57cd380866f70b22eeea17fc5e896f085d", "parent": "https://github.com/apache/hive/commit/92ab5a0e15a7edbad6468fcd2263c85b04eb91e3", "message": "HIVE-8325: NPE in map join execution (submit via child) (Gunther Hagleitner, reviewed by Szehon Ho)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1628885 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_204", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/1be4ee57cd380866f70b22eeea17fc5e896f085d/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java", "blob_url": "https://github.com/apache/hive/blob/1be4ee57cd380866f70b22eeea17fc5e896f085d/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java", "sha": "79da5a055dae3d2d100d98050d710a3e246324da", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java?ref=1be4ee57cd380866f70b22eeea17fc5e896f085d", "patch": "@@ -302,6 +302,11 @@ public int executeInProcess(DriverContext driverContext) {\n     if (work == null) {\n       return -1;\n     }\n+\n+    if (execContext == null) {\n+      execContext = new ExecMapperContext(job);\n+    }\n+\n     memoryMXBean = ManagementFactory.getMemoryMXBean();\n     long startTime = System.currentTimeMillis();\n     console.printInfo(Utilities.now()", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/662a4e5b26ea3e73aaf9e65df55d56fe9f191fa7", "parent": "https://github.com/apache/hive/commit/51d5a3fcf3ee127747311249445df9396e60a323", "message": "HIVE-8281: NPE with dynamic partition pruning on Tez (Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1628264 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_205", "file": [{"additions": 25, "raw_url": "https://github.com/apache/hive/raw/662a4e5b26ea3e73aaf9e65df55d56fe9f191fa7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "blob_url": "https://github.com/apache/hive/blob/662a4e5b26ea3e73aaf9e65df55d56fe9f191fa7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "sha": "7a3280c79097871cf673cb01ddf56920408b119a", "changes": 33, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=662a4e5b26ea3e73aaf9e65df55d56fe9f191fa7", "patch": "@@ -32,6 +32,7 @@\n import org.apache.hadoop.hive.ql.exec.AppMasterEventOperator;\n import org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator;\n import org.apache.hadoop.hive.ql.exec.DummyStoreOperator;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n import org.apache.hadoop.hive.ql.exec.JoinOperator;\n import org.apache.hadoop.hive.ql.exec.MapJoinOperator;\n@@ -619,15 +620,31 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n     return mapJoinOp;\n   }\n \n-  private boolean hasDynamicPartitionBroadcast(Operator<?> op) {\n-    if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n-      return true;\n-    }\n-    for (Operator<?> c : op.getChildOperators()) {\n-      if (hasDynamicPartitionBroadcast(c)) {\n-        return true;\n+  private boolean hasDynamicPartitionBroadcast(Operator<?> parent) {\n+    boolean hasDynamicPartitionPruning = false;\n+\n+    for (Operator<?> op: parent.getChildOperators()) {\n+      while (op != null) {\n+        if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n+          // found dynamic partition pruning operator\n+          hasDynamicPartitionPruning = true;\n+          break;\n+        }\n+      \n+        if (op instanceof ReduceSinkOperator || op instanceof FileSinkOperator) {\n+          // crossing reduce sink or file sink means the pruning isn't for this parent.\n+          break;\n+        }\n+\n+        if (op.getChildOperators().size() != 1) {\n+          // dynamic partition pruning pipeline doesn't have multiple children\n+          break;\n+        }\n+\n+        op = op.getChildOperators().get(0);\n       }\n     }\n-    return false;\n+\n+    return hasDynamicPartitionPruning;\n   }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/01fe6647430ef5dfcc120f93bdcab40677166c7c", "parent": "https://github.com/apache/hive/commit/b30756c2dac8b3d9adfdb131e3185498d4dd322b", "message": "HIVE-8203 ACID operations result in NPE when run through HS2 (Alan Gates, reviewed by Eugene Koifman)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1627914 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_206", "file": [{"additions": 15, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java", "sha": "7f0a6b3b7daecb9cafb5034877d43ee0aeec882b", "changes": 15, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -75,6 +75,21 @@ public void testTxns() throws Exception {\n     Assert.assertFalse(validTxns.isTxnCommitted(4));\n   }\n \n+  @Test\n+  public void testOpenTxnNotExcluded() throws Exception {\n+    List<Long> tids = client.openTxns(\"me\", 3).getTxn_ids();\n+    Assert.assertEquals(1L, (long) tids.get(0));\n+    Assert.assertEquals(2L, (long) tids.get(1));\n+    Assert.assertEquals(3L, (long) tids.get(2));\n+    client.rollbackTxn(1);\n+    client.commitTxn(2);\n+    ValidTxnList validTxns = client.getValidTxns(3);\n+    Assert.assertFalse(validTxns.isTxnCommitted(1));\n+    Assert.assertTrue(validTxns.isTxnCommitted(2));\n+    Assert.assertTrue(validTxns.isTxnCommitted(3));\n+    Assert.assertFalse(validTxns.isTxnCommitted(4));\n+  }\n+\n   @Test\n   public void testTxnRange() throws Exception {\n     ValidTxnList validTxns = client.getValidTxns();", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java", "sha": "94c22450e417b3ffee293aef5a848ddcfc339c49", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -1689,7 +1689,12 @@ public void cancelDelegationToken(String tokenStrForm) throws MetaException, TEx\n \n   @Override\n   public ValidTxnList getValidTxns() throws TException {\n-    return TxnHandler.createValidTxnList(client.get_open_txns());\n+    return TxnHandler.createValidTxnList(client.get_open_txns(), 0);\n+  }\n+\n+  @Override\n+  public ValidTxnList getValidTxns(long currentTxn) throws TException {\n+    return TxnHandler.createValidTxnList(client.get_open_txns(), currentTxn);\n   }\n \n   @Override", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java", "sha": "066ab68443fdd1bd04e14c8a3b1b2bbac9130d6c", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -1085,6 +1085,15 @@ Function getFunction(String dbName, String funcName)\n    */\n   ValidTxnList getValidTxns() throws TException;\n \n+  /**\n+   * Get a structure that details valid transactions.\n+   * @param currentTxn The current transaction of the caller.  This will be removed from the\n+   *                   exceptions list so that the caller sees records from his own transaction.\n+   * @return list of valid transactions\n+   * @throws TException\n+   */\n+  ValidTxnList getValidTxns(long currentTxn) throws TException;\n+\n   /**\n    * Initiate a transaction.\n    * @param user User who is opening this transaction.  This is the Hive user,", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "sha": "6f44169af90e747838bd7e2ccb5652f59ac26475", "changes": 14, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -233,12 +233,22 @@ public GetOpenTxnsResponse getOpenTxns() throws MetaException {\n     }\n   }\n \n-  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns) {\n+  /**\n+   * Transform a {@link org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse} to a\n+   * {@link org.apache.hadoop.hive.common.ValidTxnList}.\n+   * @param txns txn list from the metastore\n+   * @param currentTxn Current transaction that the user has open.  If this is greater than 0 it\n+   *                   will be removed from the exceptions list so that the user sees his own\n+   *                   transaction as valid.\n+   * @return a valid txn list.\n+   */\n+  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns, long currentTxn) {\n     long highWater = txns.getTxn_high_water_mark();\n     Set<Long> open = txns.getOpen_txns();\n-    long[] exceptions = new long[open.size()];\n+    long[] exceptions = new long[open.size() - (currentTxn > 0 ? 1 : 0)];\n     int i = 0;\n     for(long txn: open) {\n+      if (currentTxn > 0 && currentTxn == txn) continue;\n       exceptions[i++] = txn;\n     }\n     return new ValidTxnListImpl(exceptions, highWater);", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "sha": "4826abcc4fce6102642480c576bd823ec59f9743", "changes": 20, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/Driver.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -390,6 +390,9 @@ public int compile(String command, boolean resetTaskIds) {\n       tree = ParseUtils.findRootNonNullToken(tree);\n       perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.PARSE);\n \n+      // Initialize the transaction manager.  This must be done before analyze is called\n+      SessionState.get().initTxnMgr(conf);\n+\n       perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.ANALYZE);\n       BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, tree);\n       List<HiveSemanticAnalyzerHook> saHooks =\n@@ -889,9 +892,12 @@ private int recordValidTxns() {\n \n   /**\n    * Acquire read and write locks needed by the statement. The list of objects to be locked are\n-   * obtained from he inputs and outputs populated by the compiler. The lock acuisition scheme is\n+   * obtained from the inputs and outputs populated by the compiler. The lock acuisition scheme is\n    * pretty simple. If all the locks cannot be obtained, error out. Deadlock is avoided by making\n    * sure that the locks are lexicographically sorted.\n+   *\n+   * This method also records the list of valid transactions.  This must be done after any\n+   * transactions have been opened and locks acquired.\n    **/\n   private int acquireLocksAndOpenTxn() {\n     PerfLogger perfLogger = PerfLogger.getPerfLogger();\n@@ -931,7 +937,7 @@ private int acquireLocksAndOpenTxn() {\n \n       txnMgr.acquireLocks(plan, ctx, userFromUGI);\n \n-      return 0;\n+      return recordValidTxns();\n     } catch (LockException e) {\n       errorMessage = \"FAILED: Error in acquiring locks: \" + e.getMessage();\n       SQLState = ErrorMsg.findSQLState(e.getMessage());\n@@ -1108,11 +1114,6 @@ private CommandProcessorResponse runInternal(String command, boolean alreadyComp\n     SessionState ss = SessionState.get();\n     try {\n       ckLock = checkConcurrency();\n-      try {\n-        ss.initTxnMgr(conf);\n-      } catch (LockException e) {\n-        throw new SemanticException(e.getMessage(), e);\n-      }\n     } catch (SemanticException e) {\n       errorMessage = \"FAILED: Error in semantic analysis: \" + e.getMessage();\n       SQLState = ErrorMsg.findSQLState(e.getMessage());\n@@ -1121,11 +1122,8 @@ private CommandProcessorResponse runInternal(String command, boolean alreadyComp\n           + org.apache.hadoop.util.StringUtils.stringifyException(e));\n       return createProcessorResponse(10);\n     }\n-    int ret = recordValidTxns();\n-    if (ret != 0) {\n-      return createProcessorResponse(ret);\n-    }\n \n+    int ret;\n     if (!alreadyCompiled) {\n       ret = compileInternal(command);\n       if (ret != 0) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java", "sha": "46b441a13d6d981dd6aea5dfc1c0a2abad884cc1", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -286,7 +286,7 @@ public void heartbeat() throws LockException {\n   public ValidTxnList getValidTxns() throws LockException {\n     init();\n     try {\n-      return client.getValidTxns();\n+      return client.getValidTxns(txnId);\n     } catch (TException e) {\n       throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(),\n           e);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "sha": "c1d0fe11c709c90996ce4d18bd41199d3257d780", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -76,7 +76,7 @@ public void run() {\n         // don't doom the entire thread.\n         try {\n           ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());\n-          ValidTxnList txns = TxnHandler.createValidTxnList(txnHandler.getOpenTxns());\n+          ValidTxnList txns = TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);\n           Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);\n           LOG.debug(\"Found \" + potentials.size() + \" potential compactions, \" +\n               \"checking to see if we should compact any of them\");", "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "sha": "249fece426f05b7c2757d8fd52c6959a80941091", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c", "patch": "@@ -120,7 +120,7 @@ public void run() {\n \n         final boolean isMajor = ci.isMajorCompaction();\n         final ValidTxnList txns =\n-            TxnHandler.createValidTxnList(txnHandler.getOpenTxns());\n+            TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);\n         final StringBuffer jobName = new StringBuffer(name);\n         jobName.append(\"-compactor-\");\n         jobName.append(ci.getFullPartitionName());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6dff2376b68835ba0972ebb89a9c16e443411517", "parent": "https://github.com/apache/hive/commit/796fa713a7fc09f198ecba122232ba3c98264687", "message": "HIVE-8227: NPE w/ hive on tez when doing unions on empty tables (Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1627237 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_207", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/itests/src/test/resources/testconfiguration.properties", "sha": "3ea6bb53cf2b59e46fdf5639858038b625718366", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=6dff2376b68835ba0972ebb89a9c16e443411517", "patch": "@@ -204,6 +204,7 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_schema_evolution.q,\\\n   tez_union.q,\\\n   tez_union_decimal.q,\\\n+  tez_union_group_by.q,\\\n   tez_smb_main.q,\\\n   tez_smb_1.q,\\\n   vectorized_dynamic_partition_pruning.q", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "sha": "2bd40fa6782804804a3b02b312b9e2bfc3cd9e61", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java?ref=6dff2376b68835ba0972ebb89a9c16e443411517", "patch": "@@ -46,6 +46,9 @@\n   public static <T> Set<T> findOperators(Collection<Operator<?>> starts, Class<T> clazz) {\n     Set<T> found = new HashSet<T>();\n     for (Operator<?> start : starts) {\n+      if (start == null) {\n+        continue;\n+      }\n       findOperators(start, clazz, found);\n     }\n     return found;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java"}, {"additions": 87, "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/queries/clientpositive/tez_union_group_by.q", "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/queries/clientpositive/tez_union_group_by.q", "sha": "56e85833f1ce3d6a093d8be2c54d988c58aae071", "changes": 87, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union_group_by.q?ref=6dff2376b68835ba0972ebb89a9c16e443411517", "patch": "@@ -0,0 +1,87 @@\n+CREATE TABLE x\n+(\n+u bigint,\n+t string,\n+st string\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+CREATE TABLE y\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+CREATE TABLE z\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+CREATE TABLE v\n+(\n+t string, \n+st string,\n+id int\n+)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+EXPLAIN \n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02';\n+\n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02';", "filename": "ql/src/test/queries/clientpositive/tez_union_group_by.q"}, {"additions": 327, "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out", "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out", "sha": "b2a61a67d0515f4fbf782fc339d54b7564c4de6d", "changes": 327, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out?ref=6dff2376b68835ba0972ebb89a9c16e443411517", "patch": "@@ -0,0 +1,327 @@\n+PREHOOK: query: CREATE TABLE x\n+(\n+u bigint,\n+t string,\n+st string\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@x\n+POSTHOOK: query: CREATE TABLE x\n+(\n+u bigint,\n+t string,\n+st string\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@x\n+PREHOOK: query: CREATE TABLE y\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@y\n+POSTHOOK: query: CREATE TABLE y\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@y\n+PREHOOK: query: CREATE TABLE z\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@z\n+POSTHOOK: query: CREATE TABLE z\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@z\n+PREHOOK: query: CREATE TABLE v\n+(\n+t string, \n+st string,\n+id int\n+)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@v\n+POSTHOOK: query: CREATE TABLE v\n+(\n+t string, \n+st string,\n+id int\n+)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@v\n+PREHOOK: query: EXPLAIN \n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN \n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 10 <- Union 7 (CONTAINS)\n+        Map 6 <- Union 7 (CONTAINS)\n+        Map 9 <- Union 7 (CONTAINS)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE)\n+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)\n+        Reducer 4 <- Reducer 3 (SIMPLE_EDGE), Reducer 8 (SIMPLE_EDGE)\n+        Reducer 8 <- Union 7 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: v\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: t is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: t (type: string), st (type: string)\n+                      sort order: ++\n+                      Map-reduce partition columns: t (type: string), st (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Map 10 \n+        Map 5 \n+        Map 6 \n+        Map 9 \n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {VALUE._col0}\n+                  1 \n+                nullSafes: [false, true]\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: bigint)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Group By Operator\n+                    keys: _col0 (type: bigint)\n+                    mode: hash\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: bigint)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: bigint)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                keys: KEY._col0 (type: bigint)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: bigint)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: bigint)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: bigint)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 4 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Left Outer Join0 to 1\n+                condition expressions:\n+                  0 {KEY.reducesinkkey0}\n+                  1 {KEY.reducesinkkey0}\n+                outputColumnNames: _col0, _col2\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col2 (type: bigint), _col0 (type: bigint)\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 8 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: min(VALUE._col0)\n+                keys: KEY._col0 (type: bigint)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Filter Operator\n+                  predicate: (_col1 <= '2014-09-02') (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: bigint)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: bigint)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: bigint)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Union 7 \n+            Vertex: Union 7\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@v\n+PREHOOK: Input: default@x\n+PREHOOK: Input: default@y\n+PREHOOK: Input: default@z\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@v\n+POSTHOOK: Input: default@x\n+POSTHOOK: Input: default@y\n+POSTHOOK: Input: default@z\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cd39b40b5bfe4cbf33b97a5498888d99396ca44e", "parent": "https://github.com/apache/hive/commit/cb933c10ea021cfa66922f470ec5530983f2028a", "message": "HIVE-8104 Insert statements against ACID tables NPE when vectorization is on (Alan Gates reviewed by Eugene Koifman)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1625866 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_208", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "db2ad3f3645315906f196ac08284c7ed7c491985", "changes": 17, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=cd39b40b5bfe4cbf33b97a5498888d99396ca44e", "patch": "@@ -47,7 +47,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.common.FileUtils;\n-import org.apache.hadoop.hive.common.JavaUtils;\n import org.apache.hadoop.hive.common.ObjectPair;\n import org.apache.hadoop.hive.common.StatsSetupConst;\n import org.apache.hadoop.hive.common.StatsSetupConst.StatDB;\n@@ -5866,7 +5865,7 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)\n       if (!isNonNativeTable) {\n         AcidUtils.Operation acidOp = getAcidType(table_desc.getOutputFileFormatClass());\n         if (acidOp != AcidUtils.Operation.NOT_ACID) {\n-          checkIfAcidAndOverwriting(qb, table_desc);\n+          checkAcidConstraints(qb, table_desc);\n         }\n         ltd = new LoadTableDesc(queryTmpdir,table_desc, dpCtx, acidOp);\n         ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n@@ -5973,7 +5972,7 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)\n           dest_part.isStoredAsSubDirectories(), conf);\n       AcidUtils.Operation acidOp = getAcidType(table_desc.getOutputFileFormatClass());\n       if (acidOp != AcidUtils.Operation.NOT_ACID) {\n-        checkIfAcidAndOverwriting(qb, table_desc);\n+        checkAcidConstraints(qb, table_desc);\n       }\n       ltd = new LoadTableDesc(queryTmpdir, table_desc, dest_part.getSpec(), acidOp);\n       ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n@@ -6233,15 +6232,19 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)\n     return output;\n   }\n \n-  // Check if we are overwriting any tables.  If so, throw an exception as that is not allowed\n-  // when using an Acid compliant txn manager and operating on an acid table.\n-  private void checkIfAcidAndOverwriting(QB qb, TableDesc tableDesc) throws SemanticException {\n+  // Check constraints on acid tables.  This includes\n+  // * no insert overwrites\n+  // * no use of vectorization\n+  private void checkAcidConstraints(QB qb, TableDesc tableDesc) throws SemanticException {\n     String tableName = tableDesc.getTableName();\n     if (!qb.getParseInfo().isInsertIntoTable(tableName)) {\n       LOG.debug(\"Couldn't find table \" + tableName + \" in insertIntoTable\");\n       throw new SemanticException(ErrorMsg.NO_INSERT_OVERWRITE_WITH_ACID.getMsg());\n     }\n-\n+    if (conf.getBoolVar(ConfVars.HIVE_VECTORIZATION_ENABLED)) {\n+      LOG.info(\"Turning off vectorization for acid write operation\");\n+      conf.setBoolVar(ConfVars.HIVE_VECTORIZATION_ENABLED, false);\n+    }\n   }\n \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/queries/clientpositive/acid_vectorization.q", "blob_url": "https://github.com/apache/hive/blob/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/queries/clientpositive/acid_vectorization.q", "sha": "9d91d880e07e98962fb2f4a0e23a6b02591f2901", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/acid_vectorization.q?ref=cd39b40b5bfe4cbf33b97a5498888d99396ca44e", "patch": "@@ -0,0 +1,16 @@\n+set hive.support.concurrency=true;\n+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n+set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+set hive.enforce.bucketing=true;\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+set hive.vectorized.execution.enabled=true;\n+set hive.mapred.supports.subdirectories=true;\n+\n+CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC;\n+insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;\n+set hive.vectorized.execution.enabled=true;\n+insert into table acid_vectorized values (1, 'bar');\n+set hive.vectorized.execution.enabled=true;\n+update acid_vectorized set b = 'foo' where b = 'bar';\n+set hive.vectorized.execution.enabled=true;\n+delete from acid_vectorized where b = 'foo';", "filename": "ql/src/test/queries/clientpositive/acid_vectorization.q"}, {"additions": 44, "raw_url": "https://github.com/apache/hive/raw/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/results/clientpositive/acid_vectorization.q.out", "blob_url": "https://github.com/apache/hive/blob/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/results/clientpositive/acid_vectorization.q.out", "sha": "4a9d19f5529128303fc748cebcef23b8a560b8fa", "changes": 44, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/acid_vectorization.q.out?ref=cd39b40b5bfe4cbf33b97a5498888d99396ca44e", "patch": "@@ -0,0 +1,44 @@\n+PREHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+PREHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: delete from acid_vectorized where b = 'foo'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: delete from acid_vectorized where b = 'foo'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized", "filename": "ql/src/test/results/clientpositive/acid_vectorization.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d00edc7b5ae5227484f9da610b021e13f60f5812", "parent": "https://github.com/apache/hive/commit/94af26453ece19e170b2a072b38392a305bd5668", "message": "HIVE-7987 : Storage based authorization  - NPE for drop view (Thejas Nair, reviewed by Jason Dere)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1622767 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_209", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/d00edc7b5ae5227484f9da610b021e13f60f5812/common/src/java/org/apache/hadoop/hive/common/FileUtils.java", "blob_url": "https://github.com/apache/hive/blob/d00edc7b5ae5227484f9da610b021e13f60f5812/common/src/java/org/apache/hadoop/hive/common/FileUtils.java", "sha": "79819b89c6821cac9cf7f601daadfa93f87a9045", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/common/FileUtils.java?ref=d00edc7b5ae5227484f9da610b021e13f60f5812", "patch": "@@ -649,6 +649,11 @@ public static void checkDeletePermission(Path path, Configuration conf, String u\n     //   if a user is a super user. Also super users running hive queries is not a common\n     //   use case. super users can also do a chown to be able to drop the file\n \n+    if(path == null) {\n+      // no file/dir to be deleted\n+      return;\n+    }\n+\n     final FileSystem fs = path.getFileSystem(conf);\n     if (!fs.exists(path)) {\n       // no file/dir to be deleted", "filename": "common/src/java/org/apache/hadoop/hive/common/FileUtils.java"}, {"additions": 33, "raw_url": "https://github.com/apache/hive/raw/d00edc7b5ae5227484f9da610b021e13f60f5812/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java", "blob_url": "https://github.com/apache/hive/blob/d00edc7b5ae5227484f9da610b021e13f60f5812/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java", "sha": "6cf8565a3753a6bf92d6ffaf6aeb975c51e9c031", "changes": 34, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java?ref=d00edc7b5ae5227484f9da610b021e13f60f5812", "patch": "@@ -96,6 +96,7 @@ protected void setUp() throws Exception {\n     driver = new Driver(clientHiveConf);\n \n     setupFakeUser();\n+    InjectableDummyAuthenticator.injectMode(false);\n   }\n \n \n@@ -159,6 +160,38 @@ private void dropTableByOtherUser(String perm, int expectedRet) throws Exception\n     assertEquals(expectedRet, resp.getResponseCode());\n   }\n \n+  /**\n+   * Drop view should not be blocked by SBA. View will not have any location to drop.\n+   * @throws Exception\n+   */\n+  public void testDropView() throws Exception {\n+    String dbName = getTestDbName();\n+    String tblName = getTestTableName();\n+    String viewName = \"view\" + tblName;\n+    setPermissions(clientHiveConf.getVar(ConfVars.METASTOREWAREHOUSE), \"-rwxrwxrwx\");\n+\n+    CommandProcessorResponse resp = driver.run(\"create database \" + dbName);\n+    assertEquals(0, resp.getResponseCode());\n+    Database db = msc.getDatabase(dbName);\n+    validateCreateDb(db, dbName);\n+\n+    setPermissions(db.getLocationUri(), \"-rwxrwxrwt\");\n+\n+    String dbDotTable = dbName + \".\" + tblName;\n+    resp = driver.run(\"create table \" + dbDotTable + \"(i int)\");\n+    assertEquals(0, resp.getResponseCode());\n+\n+    String dbDotView = dbName + \".\" + viewName;\n+    resp = driver.run(\"create view \" + dbDotView + \" as select * from \" +  dbDotTable);\n+    assertEquals(0, resp.getResponseCode());\n+\n+    resp = driver.run(\"drop view \" + dbDotView);\n+    assertEquals(0, resp.getResponseCode());\n+\n+    resp = driver.run(\"drop table \" + dbDotTable);\n+    assertEquals(0, resp.getResponseCode());\n+  }\n+\n \n   public void testDropPartition() throws Exception {\n     dropPartitionByOtherUser(\"-rwxrwxrwx\", 0);\n@@ -202,7 +235,6 @@ private void setupFakeUser() {\n \n     InjectableDummyAuthenticator.injectUserName(fakeUser);\n     InjectableDummyAuthenticator.injectGroupNames(fakeGroupNames);\n-    InjectableDummyAuthenticator.injectMode(true);\n   }\n \n   private String setupUser() {", "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/848eda0404126834348338c9c332fc83d568230b", "parent": "https://github.com/apache/hive/commit/58fd22782606241fc1b265ba990c7664cb9fac33", "message": "HIVE-7599 : NPE in MergeTask#main() when -format is absent (DJ Choi via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1621657 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_210", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/848eda0404126834348338c9c332fc83d568230b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeTask.java", "blob_url": "https://github.com/apache/hive/blob/848eda0404126834348338c9c332fc83d568230b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeTask.java", "sha": "c30476b6aac8053f534beb01cbc4dcd7d8c7d7fa", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeTask.java?ref=848eda0404126834348338c9c332fc83d568230b", "patch": "@@ -373,6 +373,10 @@ public static void main(String[] args) {\n       }\n     }\n \n+    if (format == null || format.trim().equals(\"\")) {\n+      printUsage();\n+    }\n+    \n     MergeWork mergeWork = null;\n     if (format.equals(\"rcfile\")) {\n       mergeWork = new MergeWork(inputPaths, new Path(outputDir), RCFileInputFormat.class);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/fb9ac41b4bcb6569e54566b740fb55d72e0046d7", "parent": "https://github.com/apache/hive/commit/491296e431276c5a0b78df71d093801c06728115", "message": "HIVE-7904: Missing null check cause NPE when updating join column stats in statistics annotation (Prasanth J reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1621250 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_211", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/fb9ac41b4bcb6569e54566b740fb55d72e0046d7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/fb9ac41b4bcb6569e54566b740fb55d72e0046d7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "sha": "01c1d30aab93e3ca4710e7727176698e67e0a5a5", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java?ref=fb9ac41b4bcb6569e54566b740fb55d72e0046d7", "patch": "@@ -104,7 +104,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         tsop.setStatistics(stats.clone());\n \n         if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"[0] STATS-\" + tsop.toString() + \": \" + stats.extendedToString());\n+          LOG.debug(\"[0] STATS-\" + tsop.toString() + \" (\" + table.getTableName()\n+              + \"): \" + stats.extendedToString());\n         }\n       } catch (CloneNotSupportedException e) {\n         throw new SemanticException(ErrorMsg.STATISTICS_CLONING_FAILED.getMsg());\n@@ -1092,7 +1093,9 @@ private void updateJoinColumnsNDV(Map<Integer, List<String>> joinKeys,\n             String key = entry.getValue().get(joinColIdx);\n             key = StatsUtils.stripPrefixFromColumnName(key);\n             ColStatistics cs = joinedColStats.get(key);\n-            cs.setCountDistint(minNDV);\n+            if (cs != null) {\n+              cs.setCountDistint(minNDV);\n+            }\n           }\n         }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e9b910cf7e137bf902ebc9dc364dbdc6dd73110e", "parent": "https://github.com/apache/hive/commit/7752c8d3925d6a50d80e3b710e0fbc5ea6afe674", "message": "HIVE-7829 - Entity.getLocation can throw an NPE (Brock reviewed by Szehon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1619931 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_212", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/e9b910cf7e137bf902ebc9dc364dbdc6dd73110e/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java", "blob_url": "https://github.com/apache/hive/blob/e9b910cf7e137bf902ebc9dc364dbdc6dd73110e/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java", "sha": "76b1f01844b1eac4da2b19db1ad2313d7ee64a03", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java?ref=e9b910cf7e137bf902ebc9dc364dbdc6dd73110e", "patch": "@@ -22,6 +22,7 @@\n import java.net.URI;\n import java.util.Map;\n \n+import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.ql.metadata.DummyPartition;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n@@ -276,11 +277,13 @@ public URI getLocation() throws Exception {\n     }\n \n     if (typ == Type.TABLE) {\n-      return t.getDataLocation().toUri();\n+      Path path = t.getDataLocation();\n+      return path == null ? null : path.toUri();\n     }\n \n     if (typ == Type.PARTITION) {\n-      return p.getDataLocation().toUri();\n+      Path path = p.getDataLocation();\n+      return path == null ? null : path.toUri();\n     }\n \n     if (typ == Type.DFS_DIR || typ == Type.LOCAL_DIR) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/hooks/Entity.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/89e51e4eb47c6b505bce1928d89c0e7e2975431c", "parent": "https://github.com/apache/hive/commit/0754c55b488ba2795b61f1778ec617bd5c88aac8", "message": "HIVE-7891: Fix NPE in split generation on Tez 0.5 (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1619739 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_213", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java", "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java", "sha": "78ea21dbf88ef3c5337b97a31b1422bc1202c374", "changes": 25, "status": "modified", "deletions": 22, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c", "patch": "@@ -43,7 +43,6 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Deque;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n@@ -64,19 +63,17 @@\n import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n import org.apache.hadoop.hive.cli.CliDriver;\n import org.apache.hadoop.hive.cli.CliSessionState;\n+import org.apache.hadoop.hive.common.io.CachingPrintStream;\n import org.apache.hadoop.hive.common.io.DigestPrintStream;\n import org.apache.hadoop.hive.common.io.SortAndDigestPrintStream;\n import org.apache.hadoop.hive.common.io.SortPrintStream;\n-import org.apache.hadoop.hive.common.io.CachingPrintStream;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n import org.apache.hadoop.hive.metastore.api.Index;\n import org.apache.hadoop.hive.ql.exec.FunctionRegistry;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n-import org.apache.hadoop.hive.ql.exec.vector.util.AllVectorTypesRecord;\n-import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;\n import org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.Table;\n@@ -87,22 +84,14 @@\n import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.session.SessionState;\n-import org.apache.hadoop.hive.serde.serdeConstants;\n-import org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer;\n-import org.apache.hadoop.hive.serde2.thrift.test.Complex;\n import org.apache.hadoop.hive.shims.HadoopShims;\n import org.apache.hadoop.hive.shims.ShimLoader;\n-import org.apache.hadoop.mapred.SequenceFileInputFormat;\n-import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n-import org.apache.hadoop.mapred.TextInputFormat;\n import org.apache.hadoop.util.Shell;\n import org.apache.hive.common.util.StreamPrinter;\n-import org.apache.thrift.protocol.TBinaryProtocol;\n import org.apache.tools.ant.BuildException;\n import org.apache.zookeeper.WatchedEvent;\n import org.apache.zookeeper.Watcher;\n import org.apache.zookeeper.ZooKeeper;\n-import org.junit.Assume;\n \n import com.google.common.collect.ImmutableList;\n \n@@ -145,8 +134,8 @@\n   private QTestSetup setup = null;\n   private boolean isSessionStateStarted = false;\n \n-  private String initScript;\n-  private String cleanupScript;\n+  private final String initScript;\n+  private final String cleanupScript;\n \n   static {\n     for (String srcTable : System.getProperty(\"test.src.tables\", \"\").trim().split(\",\")) {\n@@ -332,14 +321,6 @@ public QTestUtil(String outDir, String logDir, MiniClusterType clusterType,\n     HadoopShims shims = ShimLoader.getHadoopShims();\n     int numberOfDataNodes = 4;\n \n-    // can run tez tests only on hadoop 2\n-    if (clusterType == MiniClusterType.tez) {\n-      Assume.assumeTrue(ShimLoader.getMajorVersion().equals(\"0.23\"));\n-      // this is necessary temporarily - there's a probem with multi datanodes on MiniTezCluster\n-      // will be fixed in 0.3\n-      numberOfDataNodes = 1;\n-    }\n-\n     if (clusterType != MiniClusterType.none) {\n       dfs = shims.getMiniDfs(conf, numberOfDataNodes, true, null);\n       FileSystem fs = dfs.getFileSystem();", "filename": "itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "sha": "1ef6cc5340096bd7b692941151843c0dd8405f69", "changes": 14, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c", "patch": "@@ -346,7 +346,7 @@ private EdgeProperty createEdgeProperty(TezEdgeProperty edgeProp, Configuration\n \n   /**\n    * Utility method to create a stripped down configuration for the MR partitioner.\n-   * \n+   *\n    * @param partitionerClassName\n    *          the real MR partitioner class name\n    * @param baseConf\n@@ -427,7 +427,7 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n \n     // use tez to combine splits\n     boolean groupSplitsInInputInitializer;\n-    \n+\n     DataSourceDescriptor dataSource;\n \n     int numTasks = -1;\n@@ -462,11 +462,12 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       }\n     }\n \n-    // set up the operator plan. Before setting up Inputs since the config is updated.\n-    Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n-    \n     if (HiveConf.getBoolVar(conf, ConfVars.HIVE_AM_SPLIT_GENERATION)\n         && !mapWork.isUseOneNullRowInputFormat()) {\n+\n+      // set up the operator plan. (before setting up splits on the AM)\n+      Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n+\n       // if we're generating the splits in the AM, we just need to set\n       // the correct plugin.\n       if (groupSplitsInInputInitializer) {\n@@ -484,6 +485,9 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       dataSource = MRInputHelpers.configureMRInputWithLegacySplitGeneration(conf, new Path(tezDir,\n           \"split_\" + mapWork.getName().replaceAll(\" \", \"_\")), true);\n       numTasks = dataSource.getNumberOfShards();\n+\n+      // set up the operator plan. (after generating splits - that changes configs)\n+      Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n     }\n \n     UserPayload serializedConf = TezUtils.createUserPayloadFromConf(conf);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java", "sha": "7556d7bfba81ac67db511e423c634e4125697e10", "changes": 32, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c", "patch": "@@ -64,6 +64,23 @@\n   protected static final String MAP_PLAN_KEY = \"__MAP_PLAN__\";\n   private MapWork mapWork;\n \n+  public MapRecordProcessor(JobConf jconf) {\n+    ObjectCache cache = ObjectCacheFactory.getCache(jconf);\n+    execContext.setJc(jconf);\n+    // create map and fetch operators\n+    mapWork = (MapWork) cache.retrieve(MAP_PLAN_KEY);\n+    if (mapWork == null) {\n+      mapWork = Utilities.getMapWork(jconf);\n+      cache.cache(MAP_PLAN_KEY, mapWork);\n+      l4j.info(\"Plan: \"+mapWork);\n+      for (String s: mapWork.getAliases()) {\n+        l4j.info(\"Alias: \"+s);\n+      }\n+    } else {\n+      Utilities.setMapWork(jconf, mapWork);\n+    }\n+  }\n+\n   @Override\n   void init(JobConf jconf, ProcessorContext processorContext, MRTaskReporter mrReporter,\n       Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs) throws Exception {\n@@ -87,22 +104,7 @@ void init(JobConf jconf, ProcessorContext processorContext, MRTaskReporter mrRep\n       ((TezKVOutputCollector) outMap.get(outputEntry.getKey())).initialize();\n     }\n \n-    ObjectCache cache = ObjectCacheFactory.getCache(jconf);\n     try {\n-\n-      execContext.setJc(jconf);\n-      // create map and fetch operators\n-      mapWork = (MapWork) cache.retrieve(MAP_PLAN_KEY);\n-      if (mapWork == null) {\n-        mapWork = Utilities.getMapWork(jconf);\n-        cache.cache(MAP_PLAN_KEY, mapWork);\n-        l4j.info(\"Plan: \"+mapWork);\n-        for (String s: mapWork.getAliases()) {\n-          l4j.info(\"Alias: \"+s);\n-        }\n-      } else {\n-        Utilities.setMapWork(jconf, mapWork);\n-      }\n       if (mapWork.getVectorMode()) {\n         mapOp = new VectorMapOperator();\n       } else {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java", "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java", "sha": "8b023dcd6a27251dda528b7e82946d850e5e1802", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c", "patch": "@@ -130,7 +130,7 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out\n       LOG.info(\"Running task: \" + getContext().getUniqueIdentifier());\n \n       if (isMap) {\n-        rproc = new MapRecordProcessor();\n+        rproc = new MapRecordProcessor(jobConf);\n         MRInputLegacy mrInput = getMRInput(inputs);\n         try {\n           mrInput.init();\n@@ -201,6 +201,7 @@ void initialize() throws Exception {\n       this.writer = (KeyValueWriter) output.getWriter();\n     }\n \n+    @Override\n     public void collect(Object key, Object value) throws IOException {\n       writer.write(key, value);\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "sha": "f4da332875be221cf668483f4b82bd5ebf32c39e", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c", "patch": "@@ -134,7 +134,7 @@ public int execute(DriverContext driverContext) {\n       }\n \n       List<LocalResource> additionalLr = session.getLocalizedResources();\n-      \n+\n       // log which resources we're adding (apart from the hive exec)\n       if (LOG.isDebugEnabled()) {\n         if (additionalLr == null || additionalLr.size() == 0) {\n@@ -165,7 +165,7 @@ public int execute(DriverContext driverContext) {\n       counters = client.getDAGStatus(statusGetOpts).getDAGCounters();\n       TezSessionPoolManager.getInstance().returnSession(session);\n \n-      if (LOG.isInfoEnabled()) {\n+      if (LOG.isInfoEnabled() && counters != null) {\n         for (CounterGroup group: counters) {\n           LOG.info(group.getDisplayName() +\":\");\n           for (TezCounter counter: group) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/24d63f73899d1dc63b632364b84314e99c844417", "parent": "https://github.com/apache/hive/commit/d5d12a008d659625665c24f43e982fb54289d54e", "message": "HIVE-7738 : tez select sum(decimal) from union all of decimal and null throws NPE (Alexander Pivovarov, reviewed by Gopal V)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1619032 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_214", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/itests/src/test/resources/testconfiguration.properties", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/itests/src/test/resources/testconfiguration.properties", "sha": "861e438960a27880e9232d5d7ca7e7f9db04c65d", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -146,7 +146,8 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_join_tests.q,\\\n   tez_joins_explain.q,\\\n   tez_schema_evolution.q,\\\n-  tez_union.q\n+  tez_union.q,\\\n+  tez_union_decimal.q\n \n beeline.positive.exclude=add_part_exist.q,\\\n   alter1.q,\\", "filename": "itests/src/test/resources/testconfiguration.properties"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/queries/clientpositive/tez_union_decimal.q", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/queries/clientpositive/tez_union_decimal.q", "sha": "0f56e6a12425651f22a718891c4e9da4181b2256", "changes": 37, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union_decimal.q?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -0,0 +1,37 @@\n+select sum(a) from (\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;", "filename": "ql/src/test/queries/clientpositive/tez_union_decimal.q"}, {"additions": 101, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out", "sha": "29332bed6540cc4d785312166f70aaec14a74b17", "changes": 101, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -0,0 +1,101 @@\n+PREHOOK: query: select sum(a) from (\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+1\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2", "filename": "ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java", "sha": "3214e11a7875c8688e74bcc573b700d4c8110a38", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -48,6 +48,9 @@ public ByteWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java", "sha": "b87d1f817aaaba2ec61326e3371ab0ceae8789b5", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -58,11 +58,17 @@ public HiveDecimalWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return value.getHiveDecimal().precision();\n   }\n \n   @Override\n   public int scale() {\n+    if (value == null) {\n+      return super.scale();\n+    }\n     return value.getHiveDecimal().scale();\n   }\n ", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java", "sha": "0a24c2c26229d04320105bce86ee6a51a3ad30b4", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -48,6 +48,9 @@ public IntWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java", "sha": "1973d48d708a24dd3e89e3c10b4254ff84ea4321", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -48,6 +48,9 @@ public LongWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java", "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java", "sha": "2f7479abf6b85f7ae1d86ee0e88930a47e116c3a", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417", "patch": "@@ -48,6 +48,9 @@ public ShortWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/55fc196fcb9ceb787ffac975f94cfe381625d231", "parent": "https://github.com/apache/hive/commit/8882e1e7d7706bd3eababa1e28d7c205b8df0a3e", "message": "HIVE-7498 : NPE on show grant for global privilege (Navis via Thejas Nair)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1614183 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_215", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/55fc196fcb9ceb787ffac975f94cfe381625d231/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java", "blob_url": "https://github.com/apache/hive/blob/55fc196fcb9ceb787ffac975f94cfe381625d231/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java", "sha": "1020622deda2740f5bb9bfd25acc6a760491f1fb", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java?ref=55fc196fcb9ceb787ffac975f94cfe381625d231", "patch": "@@ -35,7 +35,9 @@\n   public int compareTo(HivePrivilegeObject o) {\n     int compare = type.compareTo(o.type);\n     if (compare == 0) {\n-      compare = dbname.compareTo(o.dbname);\n+      compare = dbname != null ?\n+          (o.dbname != null ? dbname.compareTo(o.dbname) : 1) :\n+          (o.dbname != null ? -1 : 0);\n     }\n     if (compare == 0) {\n       compare = objectName != null ?", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeObject.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/55fc196fcb9ceb787ffac975f94cfe381625d231/ql/src/test/queries/clientpositive/authorization_9.q", "blob_url": "https://github.com/apache/hive/blob/55fc196fcb9ceb787ffac975f94cfe381625d231/ql/src/test/queries/clientpositive/authorization_9.q", "sha": "ed62c4586682e1a0159c773de6e8309eb5cd376c", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_9.q?ref=55fc196fcb9ceb787ffac975f94cfe381625d231", "patch": "@@ -2,6 +2,7 @@\n \n create table dummy (key string, value string);\n \n+grant select to user hive_test_user;\n grant select on database default to user hive_test_user;\n grant select on table dummy to user hive_test_user;\n grant select (key, value) on table dummy to user hive_test_user;\n@@ -10,16 +11,21 @@ show grant user hive_test_user on database default;\n show grant user hive_test_user on table dummy;\n show grant user hive_test_user on all;\n \n+grant select to user hive_test_user2;\n grant select on database default to user hive_test_user2;\n grant select on table dummy to user hive_test_user2;\n grant select (key, value) on table dummy to user hive_test_user2;\n \n show grant on all;\n+show grant user hive_test_user on all;\n+show grant user hive_test_user2 on all;\n \n+revoke select from user hive_test_user;\n revoke select on database default from user hive_test_user;\n revoke select on table dummy from user hive_test_user;\n revoke select (key, value) on table dummy from user hive_test_user;\n \n+revoke select from user hive_test_user2;\n revoke select on database default from user hive_test_user2;\n revoke select on table dummy from user hive_test_user2;\n revoke select (key, value) on table dummy from user hive_test_user2;", "filename": "ql/src/test/queries/clientpositive/authorization_9.q"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/55fc196fcb9ceb787ffac975f94cfe381625d231/ql/src/test/results/clientpositive/authorization_9.q.out", "blob_url": "https://github.com/apache/hive/blob/55fc196fcb9ceb787ffac975f94cfe381625d231/ql/src/test/results/clientpositive/authorization_9.q.out", "sha": "2f7282c4b17f1ab7ea78d9599462bdf6e9b9d302", "changes": 37, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_9.q.out?ref=55fc196fcb9ceb787ffac975f94cfe381625d231", "patch": "@@ -9,6 +9,10 @@ create table dummy (key string, value string)\n POSTHOOK: type: CREATETABLE\n POSTHOOK: Output: database:default\n POSTHOOK: Output: default@dummy\n+PREHOOK: query: grant select to user hive_test_user\n+PREHOOK: type: GRANT_PRIVILEGE\n+POSTHOOK: query: grant select to user hive_test_user\n+POSTHOOK: type: GRANT_PRIVILEGE\n PREHOOK: query: grant select on database default to user hive_test_user\n PREHOOK: type: GRANT_PRIVILEGE\n POSTHOOK: query: grant select on database default to user hive_test_user\n@@ -39,10 +43,15 @@ PREHOOK: query: show grant user hive_test_user on all\n PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: show grant user hive_test_user on all\n POSTHOOK: type: SHOW_GRANT\n+\t\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\t\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\tdummy\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\tdummy\t\t[key]\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\tdummy\t\t[value]\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+PREHOOK: query: grant select to user hive_test_user2\n+PREHOOK: type: GRANT_PRIVILEGE\n+POSTHOOK: query: grant select to user hive_test_user2\n+POSTHOOK: type: GRANT_PRIVILEGE\n PREHOOK: query: grant select on database default to user hive_test_user2\n PREHOOK: type: GRANT_PRIVILEGE\n POSTHOOK: query: grant select on database default to user hive_test_user2\n@@ -64,6 +73,8 @@ PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: show grant on all\n POSTHOOK: type: SHOW_GRANT\n \t\t\t\tadmin\tROLE\tALL\ttrue\t-1\tadmin\n+\t\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+\t\t\t\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\t\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\t\t\t\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\tdummy\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n@@ -72,6 +83,28 @@ default\tdummy\t\t[key]\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\tdummy\t\t[key]\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\tdummy\t\t[value]\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n default\tdummy\t\t[value]\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+PREHOOK: query: show grant user hive_test_user on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user hive_test_user on all\n+POSTHOOK: type: SHOW_GRANT\n+\t\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\t\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\tdummy\t\t\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\tdummy\t\t[key]\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\tdummy\t\t[value]\thive_test_user\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+PREHOOK: query: show grant user hive_test_user2 on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user hive_test_user2 on all\n+POSTHOOK: type: SHOW_GRANT\n+\t\t\t\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\t\t\t\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\tdummy\t\t\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\tdummy\t\t[key]\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+default\tdummy\t\t[value]\thive_test_user2\tUSER\tSELECT\tfalse\t-1\thive_test_user\n+PREHOOK: query: revoke select from user hive_test_user\n+PREHOOK: type: REVOKE_PRIVILEGE\n+POSTHOOK: query: revoke select from user hive_test_user\n+POSTHOOK: type: REVOKE_PRIVILEGE\n PREHOOK: query: revoke select on database default from user hive_test_user\n PREHOOK: type: REVOKE_PRIVILEGE\n POSTHOOK: query: revoke select on database default from user hive_test_user\n@@ -88,6 +121,10 @@ PREHOOK: Output: default@dummy\n POSTHOOK: query: revoke select (key, value) on table dummy from user hive_test_user\n POSTHOOK: type: REVOKE_PRIVILEGE\n POSTHOOK: Output: default@dummy\n+PREHOOK: query: revoke select from user hive_test_user2\n+PREHOOK: type: REVOKE_PRIVILEGE\n+POSTHOOK: query: revoke select from user hive_test_user2\n+POSTHOOK: type: REVOKE_PRIVILEGE\n PREHOOK: query: revoke select on database default from user hive_test_user2\n PREHOOK: type: REVOKE_PRIVILEGE\n POSTHOOK: query: revoke select on database default from user hive_test_user2", "filename": "ql/src/test/results/clientpositive/authorization_9.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b872011375bfba1c90abc5b423c46a46dedebdbe", "parent": "https://github.com/apache/hive/commit/e464d0813b9d487b432087dec197bc4a4a6b9d62", "message": "HIVE-7459 - Fix NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat (Matt Martin, Ryan Blue via Brock)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1613830 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_216", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/b872011375bfba1c90abc5b423c46a46dedebdbe/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java", "blob_url": "https://github.com/apache/hive/blob/b872011375bfba1c90abc5b423c46a46dedebdbe/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java", "sha": "f5da46d392d8ac5f5589f66c37d567b1d3bd8843", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java?ref=b872011375bfba1c90abc5b423c46a46dedebdbe", "patch": "@@ -104,9 +104,9 @@ public ParquetRecordReaderWrapper(\n     } else {\n       realReader = null;\n       eof = true;\n-      if (valueObj == null) { // Should initialize the value for createValue\n-        valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);\n-      }\n+    }\n+    if (valueObj == null) { // Should initialize the value for createValue\n+      valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9", "parent": "https://github.com/apache/hive/commit/60be7236a80f57cc23a1a9d0dc8b00062ca5397e", "message": "HIVE-7515 Fix NPE in CBO (Laljo John Pullokkaran via Harish Butani)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1613482 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_217", "file": [{"additions": 14, "raw_url": "https://github.com/apache/hive/raw/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "3da2491f5198f3eed9d119545fec81dc72c7f7dc", "changes": 25, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=27f77f054baf2e7cd1ea55282fbbb80c055f5aa9", "patch": "@@ -11831,6 +11831,7 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n       optiqPreCboPlan = applyPreCBOTransforms(optiqGenPlan, HiveDefaultRelMetadataProvider.INSTANCE);\n       List<RelMetadataProvider> list = Lists.newArrayList();\n       list.add(HiveDefaultRelMetadataProvider.INSTANCE);\n+      RelTraitSet desiredTraits = cluster.traitSetOf(HiveRel.CONVENTION, RelCollationImpl.EMPTY);\n \n       if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_GREEDY_JOIN_ORDER)) {\n         planner.registerMetadataProviders(list);\n@@ -11846,18 +11847,15 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n           planner.addRule(HivePullUpProjectsAboveJoinRule.LEFT_PROJECT);\n           planner.addRule(HivePullUpProjectsAboveJoinRule.RIGHT_PROJECT);\n           planner.addRule(HiveMergeProjectRule.INSTANCE);\n+        }\n \n-          RelTraitSet desiredTraits = cluster\n-              .traitSetOf(HiveRel.CONVENTION, RelCollationImpl.EMPTY);\n-\n-          RelNode rootRel = optiqPreCboPlan;\n-          if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n-            rootRel = planner.changeTraits(optiqPreCboPlan, desiredTraits);\n-          }\n-          planner.setRoot(rootRel);\n-\n-          optiqOptimizedPlan = planner.findBestExp();\n+        RelNode rootRel = optiqPreCboPlan;\n+        if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n+          rootRel = planner.changeTraits(optiqPreCboPlan, desiredTraits);\n         }\n+        planner.setRoot(rootRel);\n+\n+        optiqOptimizedPlan = planner.findBestExp();\n       } else {\n         final HepProgram hepPgm = new HepProgramBuilder().addMatchOrder(HepMatchOrder.BOTTOM_UP)\n             .addRuleInstance(new ConvertMultiJoinRule(HiveJoinRel.class))\n@@ -11869,7 +11867,12 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n         RelMetadataProvider chainedProvider = ChainedRelMetadataProvider.of(list);\n         cluster.setMetadataProvider(new CachingRelMetadataProvider(chainedProvider, hepPlanner));\n \n-        hepPlanner.setRoot(optiqPreCboPlan);\n+        RelNode rootRel = optiqPreCboPlan;\n+        if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n+          rootRel = hepPlanner.changeTraits(optiqPreCboPlan, desiredTraits);\n+        }\n+        hepPlanner.setRoot(rootRel);\n+\n         optiqOptimizedPlan = hepPlanner.findBestExp();\n       }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f3b2c702238f5592a6743adbe5aee42bbec9f4e6", "parent": "https://github.com/apache/hive/commit/e34ca1c02edb3b771c2976c75d57dc5212284637", "message": "HIVE-7310: Turning CBO on results in NPE on some queries (Laljo John Pullokkaran via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1606276 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_218", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/f3b2c702238f5592a6743adbe5aee42bbec9f4e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/f3b2c702238f5592a6743adbe5aee42bbec9f4e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "c7c76e520d8b4ba3edb5cf883b0f9c5b76dd7bd4", "changes": 3, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=f3b2c702238f5592a6743adbe5aee42bbec9f4e6", "patch": "@@ -12125,9 +12125,6 @@ private AggregateCall convertAgg(AggInfo agg, RelNode input,\n       List<Integer> argList = new ArrayList<Integer>();\n       RelDataType type = TypeConverter.convert(agg.m_returnType,\n           this.m_cluster.getTypeFactory());\n-      if (aggregation.equals(SqlStdOperatorTable.AVG)) {\n-        type = type.getField(\"sum\", false).getType();\n-      }\n \n       // TODO: Does HQL allows expressions as aggregate args or can it only be\n       // projections from child?", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46", "parent": "https://github.com/apache/hive/commit/cd05b4c1c4738e8e575ffbe6d542bb93ef626361", "message": "HIVE-7210: NPE with \"No plan file found\" when running Driver instances on multiple threads (Jason Dere, reviewed by Gunther Hagleitner/Vikram Dixit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1603344 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_219", "file": [{"additions": 15, "raw_url": "https://github.com/apache/hive/raw/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "blob_url": "https://github.com/apache/hive/blob/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "sha": "5e5cf97c7b8fefb204e45028af8bf1003437d25a", "changes": 23, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=72c9d3616f9ca836bc4fd6f2addf0edf7799cd46", "patch": "@@ -242,7 +242,7 @@ public static void clearWork(Configuration conf) {\n     Path reducePath = getPlanPath(conf, REDUCE_PLAN_NAME);\n \n     // if the plan path hasn't been initialized just return, nothing to clean.\n-    if (mapPath == null || reducePath == null) {\n+    if (mapPath == null && reducePath == null) {\n       return;\n     }\n \n@@ -260,12 +260,7 @@ public static void clearWork(Configuration conf) {\n     } finally {\n       // where a single process works with multiple plans - we must clear\n       // the cache before working with the next plan.\n-      if (mapPath != null) {\n-        gWorkMap.remove(mapPath);\n-      }\n-      if (reducePath != null) {\n-        gWorkMap.remove(reducePath);\n-      }\n+      clearWorkMapForConf(conf);\n     }\n   }\n \n@@ -3314,7 +3309,19 @@ public static boolean isVectorMode(Configuration conf) {\n     return false;\n   }\n \n-    public static void clearWorkMap() {\n+  public static void clearWorkMapForConf(Configuration conf) {\n+    // Remove cached query plans for the current query only\n+    Path mapPath = getPlanPath(conf, MAP_PLAN_NAME);\n+    Path reducePath = getPlanPath(conf, REDUCE_PLAN_NAME);\n+    if (mapPath != null) {\n+      gWorkMap.remove(mapPath);\n+    }\n+    if (reducePath != null) {\n+      gWorkMap.remove(reducePath);\n+    }\n+  }\n+\n+  public static void clearWorkMap() {\n     gWorkMap.clear();\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java", "sha": "61cc874dfebd8982fd5057371d605e8233b6ca79", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java?ref=72c9d3616f9ca836bc4fd6f2addf0edf7799cd46", "patch": "@@ -393,7 +393,7 @@ private void addSplitsForGroup(List<Path> dirs, TableScanOperator tableScan, Job\n           currentTable, result);\n     }\n \n-    Utilities.clearWorkMap();\n+    Utilities.clearWorkMapForConf(job);\n     LOG.info(\"number of splits \" + result.size());\n     perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);\n     return result.toArray(new HiveInputSplit[result.size()]);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/70877d6cc8553aedbe8c580e4e875296b1e5b77e", "parent": "https://github.com/apache/hive/commit/f24a81bc0b489b857c46875432ea001a8e5b2951", "message": "HIVE-7226 : Windowing Streaming mode causes NPE for empty partitions (Harish Butani via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1602499 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_220", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/70877d6cc8553aedbe8c580e4e875296b1e5b77e/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java", "blob_url": "https://github.com/apache/hive/blob/70877d6cc8553aedbe8c580e4e875296b1e5b77e/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java", "sha": "2290766b110c397f5763224054ccecd3b87d2f76", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java?ref=70877d6cc8553aedbe8c580e4e875296b1e5b77e", "patch": "@@ -1319,6 +1319,7 @@ public void remove() {\n         fnOutputs[i] = new ArrayList<Object>();\n         WindowFunctionDef wFn = tabDef.getWindowFunctions().get(i);\n         funcArgs[i] = new Object[wFn.getArgs() == null ? 0 : wFn.getArgs().size()];\n+        aggBuffers[i] = wFn.getWFnEval().getNewAggregationBuffer();\n       }\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/70877d6cc8553aedbe8c580e4e875296b1e5b77e/ql/src/test/queries/clientpositive/windowing.q", "blob_url": "https://github.com/apache/hive/blob/70877d6cc8553aedbe8c580e4e875296b1e5b77e/ql/src/test/queries/clientpositive/windowing.q", "sha": "a1f4447b7fdfc724fbf75df8f112dea7529ee316", "changes": 8, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/windowing.q?ref=70877d6cc8553aedbe8c580e4e875296b1e5b77e", "patch": "@@ -431,4 +431,10 @@ select p_name, p_retailprice,\n round(avg(p_retailprice) over(),2)\n from part\n order by p_name;\n-        \n+\n+-- 45. empty partition test\n+select p_mfgr, \n+  sum(p_size) over (partition by p_mfgr order by p_size rows between unbounded preceding and current row) \n+from part \n+where p_mfgr = 'Manufacturer#6'\n+;", "filename": "ql/src/test/queries/clientpositive/windowing.q"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/70877d6cc8553aedbe8c580e4e875296b1e5b77e/ql/src/test/results/clientpositive/windowing.q.out", "blob_url": "https://github.com/apache/hive/blob/70877d6cc8553aedbe8c580e4e875296b1e5b77e/ql/src/test/results/clientpositive/windowing.q.out", "sha": "104f4ca049ef0d0af20a031809f63623f8a7633a", "changes": 16, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/windowing.q.out?ref=70877d6cc8553aedbe8c580e4e875296b1e5b77e", "patch": "@@ -2314,3 +2314,19 @@ almond aquamarine sandy cyan gainsboro\t1701.6\t1546.78\n almond aquamarine yellow dodger mint\t1844.92\t1546.78\n almond azure aquamarine papaya violet\t1290.35\t1546.78\n almond azure blanched chiffon midnight\t1464.48\t1546.78\n+PREHOOK: query: -- 45. empty partition test\n+select p_mfgr, \n+  sum(p_size) over (partition by p_mfgr order by p_size rows between unbounded preceding and current row) \n+from part \n+where p_mfgr = 'Manufacturer#6'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@part\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- 45. empty partition test\n+select p_mfgr, \n+  sum(p_size) over (partition by p_mfgr order by p_size rows between unbounded preceding and current row) \n+from part \n+where p_mfgr = 'Manufacturer#6'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@part\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/windowing.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b1f556296405814e6b492660c8f03bcb9d892d4c", "parent": "https://github.com/apache/hive/commit/cc2476929a811b2b5310600b421b52c366223070", "message": "HIVE-6984: Analyzing partitioned table with NULL values for the partition column failed with NPE (reviewed by Sergey)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1591796 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_221", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/data/files/test1.txt", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/data/files/test1.txt", "sha": "0f8801ad3ab78236913eaf63316dd28b744eb08b", "changes": 5, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/test1.txt?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -0,0 +1,5 @@\n+tom\u000115\n+john\u0001\n+mayr\u000140\n+\u000130\n+\u0001", "filename": "data/files/test1.txt"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java", "sha": "58ed550f2af94c2ddcfa94ec08f4ef4bdca984ed", "changes": 9, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.VirtualColumn;\n@@ -65,6 +66,8 @@\n   private transient int rowLimit = -1;\n   private transient int currCount = 0;\n \n+  private String defaultPartitionName;\n+\n   public TableDesc getTableDesc() {\n     return tableDesc;\n   }\n@@ -145,8 +148,9 @@ private void gatherStats(Object row) {\n             (StructObjectInspector) inputObjInspectors[0], ObjectInspectorCopyOption.WRITABLE);\n \n         for (Object o : writable) {\n-          assert (o != null && o.toString().length() > 0);\n-          values.add(o.toString());\n+          // It's possible that a parition column may have NULL value, in which case the row belongs\n+          // to the special partition, __HIVE_DEFAULT_PARTITION__.\n+          values.add(o == null ? defaultPartitionName : o.toString());\n         }\n         partitionSpecs = FileUtils.makePartName(conf.getPartColumns(), values);\n         LOG.info(\"Stats Gathering found a new partition spec = \" + partitionSpecs);\n@@ -205,6 +209,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {\n       jc = new JobConf(hconf);\n     }\n \n+    defaultPartitionName = HiveConf.getVar(hconf, HiveConf.ConfVars.DEFAULTPARTITIONNAME);\n     currentStat = null;\n     stats = new HashMap<String, Stat>();\n     if (conf.getPartColumns() == null || conf.getPartColumns().size() == 0) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/queries/clientpositive/analyze_table_null_partition.q", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/queries/clientpositive/analyze_table_null_partition.q", "sha": "2f7a893d33031cf329767b704707c0c2badcaf2f", "changes": 21, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/analyze_table_null_partition.q?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -0,0 +1,21 @@\n+SET hive.exec.dynamic.partition.mode=nonstrict;\n+\n+DROP TABLE IF EXISTS test1;\n+DROP TABLE IF EXISTS test2;\n+\n+CREATE TABLE test1(name string, age int);\n+CREATE TABLE test2(name string) PARTITIONED by (age int);\n+\n+LOAD DATA LOCAL INPATH '../../data/files/test1.txt' INTO TABLE test1;\n+FROM test1 INSERT OVERWRITE TABLE test2 PARTITION(age) SELECT test1.name, test1.age;\n+\n+ANALYZE TABLE test2 PARTITION(age) COMPUTE STATISTICS;\n+\n+-- To show stats. It doesn't show due to a bug.\n+DESC EXTENDED test2;\n+\n+-- Another way to show stats.\n+EXPLAIN EXTENDED select * from test2;\n+\n+DROP TABLE test1;\n+DROP TABLE test2;", "filename": "ql/src/test/queries/clientpositive/analyze_table_null_partition.q"}, {"additions": 335, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/analyze_table_null_partition.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/analyze_table_null_partition.q.out", "sha": "605f5b44687494b63aced91dfbb5e30e6b6cc754", "changes": 335, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/analyze_table_null_partition.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -0,0 +1,335 @@\n+PREHOOK: query: DROP TABLE IF EXISTS test1\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS test1\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: DROP TABLE IF EXISTS test2\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS test2\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE test1(name string, age int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: CREATE TABLE test1(name string, age int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test1\n+PREHOOK: query: CREATE TABLE test2(name string) PARTITIONED by (age int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: CREATE TABLE test2(name string) PARTITIONED by (age int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test2\n+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/test1.txt' INTO TABLE test1\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@test1\n+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/test1.txt' INTO TABLE test1\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@test1\n+PREHOOK: query: FROM test1 INSERT OVERWRITE TABLE test2 PARTITION(age) SELECT test1.name, test1.age\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test1\n+PREHOOK: Output: default@test2\n+POSTHOOK: query: FROM test1 INSERT OVERWRITE TABLE test2 PARTITION(age) SELECT test1.name, test1.age\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test1\n+POSTHOOK: Output: default@test2@age=15\n+POSTHOOK: Output: default@test2@age=30\n+POSTHOOK: Output: default@test2@age=40\n+POSTHOOK: Output: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: ANALYZE TABLE test2 PARTITION(age) COMPUTE STATISTICS\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test2\n+PREHOOK: Input: default@test2@age=15\n+PREHOOK: Input: default@test2@age=30\n+PREHOOK: Input: default@test2@age=40\n+PREHOOK: Input: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+PREHOOK: Output: default@test2\n+PREHOOK: Output: default@test2@age=15\n+PREHOOK: Output: default@test2@age=30\n+PREHOOK: Output: default@test2@age=40\n+PREHOOK: Output: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: query: ANALYZE TABLE test2 PARTITION(age) COMPUTE STATISTICS\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test2\n+POSTHOOK: Input: default@test2@age=15\n+POSTHOOK: Input: default@test2@age=30\n+POSTHOOK: Input: default@test2@age=40\n+POSTHOOK: Input: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Output: default@test2\n+POSTHOOK: Output: default@test2@age=15\n+POSTHOOK: Output: default@test2@age=30\n+POSTHOOK: Output: default@test2@age=40\n+POSTHOOK: Output: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: -- To show stats. It doesn't show due to a bug.\n+DESC EXTENDED test2\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@test2\n+POSTHOOK: query: -- To show stats. It doesn't show due to a bug.\n+DESC EXTENDED test2\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@test2\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+name                \tstring              \t                    \n+age                 \tint                 \t                    \n+\t \t \n+# Partition Information\t \t \n+# col_name            \tdata_type           \tcomment             \n+\t \t \n+age                 \tint                 \t                    \n+\t \t \n+#### A masked pattern was here ####\n+PREHOOK: query: -- Another way to show stats.\n+EXPLAIN EXTENDED select * from test2\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- Another way to show stats.\n+EXPLAIN EXTENDED select * from test2\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+ABSTRACT SYNTAX TREE:\n+  \n+TOK_QUERY\n+   TOK_FROM\n+      TOK_TABREF\n+         TOK_TABNAME\n+            test2\n+   TOK_INSERT\n+      TOK_DESTINATION\n+         TOK_DIR\n+            TOK_TMP_FILE\n+      TOK_SELECT\n+         TOK_SELEXPR\n+            TOK_ALLCOLREF\n+\n+\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Partition Description:\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age 15\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 1\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 3\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 4\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age 30\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 1\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 0\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 1\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age 40\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 1\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 4\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 5\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age __HIVE_DEFAULT_PARTITION__\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 2\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 4\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 6\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+      Processor Tree:\n+        TableScan\n+          alias: test2\n+          Statistics: Num rows: 5 Data size: 111 Basic stats: COMPLETE Column stats: NONE\n+          GatherStats: false\n+          Select Operator\n+            expressions: name (type: string), age (type: int)\n+            outputColumnNames: _col0, _col1\n+            Statistics: Num rows: 5 Data size: 111 Basic stats: COMPLETE Column stats: NONE\n+            ListSink\n+\n+PREHOOK: query: DROP TABLE test1\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@test1\n+PREHOOK: Output: default@test1\n+POSTHOOK: query: DROP TABLE test1\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@test1\n+POSTHOOK: Output: default@test1\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: DROP TABLE test2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@test2\n+PREHOOK: Output: default@test2\n+POSTHOOK: query: DROP TABLE test2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@test2\n+POSTHOOK: Output: default@test2\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]", "filename": "ql/src/test/results/clientpositive/analyze_table_null_partition.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out", "sha": "b4077354185f9338077eceb435ccad6589f3dad1", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -535,7 +535,7 @@ STAGE PLANS:\n   Stage: Stage-1\n     Tez\n       Edges:\n-        Map 3 <- Map 1 (CUSTOM_EDGE), Map 2 (CUSTOM_EDGE)\n+        Map 3 <- Map 2 (CUSTOM_EDGE), Map 1 (CUSTOM_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 ", "filename": "ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out", "sha": "17d64c71ec1024ba7ee3833c47914e09ad130f70", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -124,7 +124,7 @@ STAGE PLANS:\n   Stage: Stage-1\n     Tez\n       Edges:\n-        Map 3 <- Map 2 (CUSTOM_EDGE), Map 1 (BROADCAST_EDGE)\n+        Map 3 <- Map 1 (BROADCAST_EDGE), Map 2 (CUSTOM_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 ", "filename": "ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/count.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/count.q.out", "sha": "31c12835bfbd0459495ad09658111c9b01cf6622", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/count.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -63,7 +63,7 @@ STAGE PLANS:\n                         sort order: +++\n                         Map-reduce partition columns: _col0 (type: int)\n                         Statistics: Num rows: 4 Data size: 78 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col3 (type: bigint), _col4 (type: bigint), _col5 (type: bigint)\n+                        value expressions: _col5 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator", "filename": "ql/src/test/results/clientpositive/tez/count.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/limit_pushdown.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/limit_pushdown.q.out", "sha": "a8b281aaa2b5f022c975e46289ff1c8b7b28f5dc", "changes": 6, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/limit_pushdown.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -481,7 +481,6 @@ STAGE PLANS:\n                         Map-reduce partition columns: _col0 (type: tinyint)\n                         Statistics: Num rows: 31436 Data size: 377237 Basic stats: COMPLETE Column stats: NONE\n                         TopN Hash Memory Usage: 0.3\n-                        value expressions: _col2 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -577,7 +576,6 @@ STAGE PLANS:\n                         Map-reduce partition columns: _col0 (type: tinyint)\n                         Statistics: Num rows: 1849 Data size: 377237 Basic stats: COMPLETE Column stats: NONE\n                         TopN Hash Memory Usage: 0.3\n-                        value expressions: _col3 (type: bigint), _col4 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -981,8 +979,8 @@ STAGE PLANS:\n                   alias: src\n                   Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n                   Select Operator\n-                    expressions: value (type: string), key (type: string)\n-                    outputColumnNames: value, key\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: key, value\n                     Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n                     Reduce Output Operator\n                       key expressions: value (type: string)", "filename": "ql/src/test/results/clientpositive/tez/limit_pushdown.q.out"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out", "sha": "4638fe7d16bb796e08b58dadf470341e923f6259", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -1,7 +1,9 @@\n PREHOOK: query: show partitions srcpart\n PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@srcpart\n POSTHOOK: query: show partitions srcpart\n POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@srcpart\n ds=2008-04-08/hr=11\n ds=2008-04-08/hr=12\n ds=2008-04-09/hr=11\n@@ -22,8 +24,10 @@ POSTHOOK: Output: database:default\n POSTHOOK: Output: default@nzhang_part2\n PREHOOK: query: describe extended nzhang_part1\n PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@nzhang_part1\n POSTHOOK: query: describe extended nzhang_part1\n POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@nzhang_part1\n key                 \tstring              \tdefault             \n value               \tstring              \tdefault             \n ds                  \tstring              \t                    \n@@ -187,8 +191,10 @@ POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).key SIMPLE [(srcp\n POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]\n PREHOOK: query: show partitions nzhang_part1\n PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@nzhang_part1\n POSTHOOK: query: show partitions nzhang_part1\n POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@nzhang_part1\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]\n@@ -201,8 +207,10 @@ ds=2008-04-08/hr=11\n ds=2008-04-08/hr=12\n PREHOOK: query: show partitions nzhang_part2\n PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@nzhang_part2\n POSTHOOK: query: show partitions nzhang_part2\n POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@nzhang_part2\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]", "filename": "ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/mrr.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/mrr.q.out", "sha": "70c219bb4701bbfac647d0b4a709dad276827302", "changes": 2, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/mrr.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -452,7 +452,6 @@ STAGE PLANS:\n                       sort order: ++\n                       Map-reduce partition columns: _col0 (type: string)\n                       Statistics: Num rows: 63 Data size: 6393 Basic stats: COMPLETE Column stats: NONE\n-                      value expressions: _col2 (type: bigint)\n         Reducer 3 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -864,7 +863,6 @@ STAGE PLANS:\n                           sort order: ++\n                           Map-reduce partition columns: _col0 (type: string)\n                           Statistics: Num rows: 63 Data size: 6393 Basic stats: COMPLETE Column stats: NONE\n-                          value expressions: _col2 (type: bigint)\n         Reducer 3 \n             Reduce Operator Tree:\n               Group By Operator", "filename": "ql/src/test/results/clientpositive/tez/mrr.q.out"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_dml.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_dml.q.out", "sha": "d64c0dadd29ee8f028cdae912a7aa43c63e32264", "changes": 14, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_dml.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -879,10 +879,10 @@ POSTHOOK: Lineage: tmp_src_part PARTITION(d=5).c SIMPLE [(tmp_src)tmp_src.FieldS\n STAGE DEPENDENCIES:\n   Stage-2 is a root stage\n   Stage-3 depends on stages: Stage-2\n-  Stage-1 depends on stages: Stage-3\n-  Stage-4 depends on stages: Stage-1\n   Stage-0 depends on stages: Stage-3\n-  Stage-5 depends on stages: Stage-0\n+  Stage-4 depends on stages: Stage-0\n+  Stage-1 depends on stages: Stage-3\n+  Stage-5 depends on stages: Stage-1\n \n STAGE PLANS:\n   Stage: Stage-2\n@@ -928,28 +928,28 @@ STAGE PLANS:\n   Stage: Stage-3\n     Dependency Collection\n \n-  Stage: Stage-1\n+  Stage: Stage-0\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.odd\n+              name: default.even\n \n   Stage: Stage-4\n     Stats-Aggr Operator\n \n-  Stage: Stage-0\n+  Stage: Stage-1\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.even\n+              name: default.odd\n \n   Stage: Stage-5\n     Stats-Aggr Operator", "filename": "ql/src/test/results/clientpositive/tez/tez_dml.q.out"}, {"additions": 22, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_union.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_union.q.out", "sha": "4dc687cad3e8f13dcbbbc1e02c4e00349f4f14b4", "changes": 44, "status": "modified", "deletions": 22, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -159,8 +159,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -170,8 +170,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -181,8 +181,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -192,8 +192,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -206,9 +206,9 @@ STAGE PLANS:\n                 condition expressions:\n                   0 \n                   1 \n-                Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                 Select Operator\n-                  Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                   Group By Operator\n                     aggregations: count()\n                     mode: hash\n@@ -317,8 +317,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Map Join Operator\n                       condition map:\n                            Inner Join 0 to 1\n@@ -343,8 +343,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Map Join Operator\n                       condition map:\n                            Inner Join 0 to 1\n@@ -446,7 +446,7 @@ STAGE PLANS:\n       Edges:\n         Map 2 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 5 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE)\n         Map 7 <- Map 1 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 8 (BROADCAST_EDGE)\n-        Map 9 <- Map 1 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 10 (BROADCAST_EDGE)\n+        Map 9 <- Map 8 (BROADCAST_EDGE), Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 10 (BROADCAST_EDGE)\n         Reducer 4 <- Union 3 (SIMPLE_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n@@ -1074,8 +1074,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -1096,14 +1096,14 @@ STAGE PLANS:\n                       0 _col0 (type: string)\n                       1 key (type: string)\n                     outputColumnNames: _col0, _col2\n-                    Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                     Select Operator\n                       expressions: _col0 (type: string), _col2 (type: string)\n                       outputColumnNames: _col0, _col1\n-                      Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                       File Output Operator\n                         compressed: false\n-                        Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                        Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                         table:\n                             input format: org.apache.hadoop.mapred.TextInputFormat\n                             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n@@ -1113,8 +1113,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +", "filename": "ql/src/test/results/clientpositive/tez/tez_union.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union2.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union2.q.out", "sha": "a0c6a9d1c5897819b136b8aef83879fd1c72df45", "changes": 10, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union2.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -28,8 +28,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s1\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -43,8 +41,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s2\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -59,14 +55,14 @@ STAGE PLANS:\n                 aggregations: count(VALUE._col0)\n                 mode: mergepartial\n                 outputColumnNames: _col0\n-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                 Select Operator\n                   expressions: _col0 (type: bigint)\n                   outputColumnNames: _col0\n-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                   File Output Operator\n                     compressed: false\n-                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                     table:\n                         input format: org.apache.hadoop.mapred.TextInputFormat\n                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat", "filename": "ql/src/test/results/clientpositive/tez/union2.q.out"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union3.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union3.q.out", "sha": "b5ba13c22f2ddd3695e45e397851dc07373dcd7a", "changes": 60, "status": "modified", "deletions": 36, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union3.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -54,82 +54,70 @@ STAGE PLANS:\n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Map 4 \n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Map 6 \n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Map 9 \n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Reducer 10 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                 Limit\n                   Number of rows: 1\n-                  Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n                     expressions: 2 (type: int)\n                     outputColumnNames: _col0\n-                    Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Reduce Output Operator\n                       key expressions: _col0 (type: int)\n                       sort order: +\n                       Map-reduce partition columns: _col0 (type: int)\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       value expressions: _col0 (type: int)\n         Reducer 11 \n             Reduce Operator Tree:\n@@ -180,19 +168,19 @@ STAGE PLANS:\n         Reducer 7 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                 Limit\n                   Number of rows: 1\n-                  Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n                     expressions: 1 (type: int)\n                     outputColumnNames: _col0\n-                    Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Reduce Output Operator\n                       key expressions: _col0 (type: int)\n                       sort order: +\n                       Map-reduce partition columns: _col0 (type: int)\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       value expressions: _col0 (type: int)\n         Reducer 8 \n             Reduce Operator Tree:", "filename": "ql/src/test/results/clientpositive/tez/union3.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union5.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union5.q.out", "sha": "5c50f8e28e04a6a095e4b75730d81928e1a6fecf", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union5.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -64,8 +64,8 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0\n                 Select Operator\n-                  expressions: 'tst1' (type: string), _col0 (type: bigint)\n-                  outputColumnNames: _col0, _col1\n+                  expressions: 'tst1' (type: string)\n+                  outputColumnNames: _col0\n                   Select Operator\n                     expressions: _col0 (type: string)\n                     outputColumnNames: _col0\n@@ -105,8 +105,8 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0\n                 Select Operator\n-                  expressions: 'tst2' (type: string), _col0 (type: bigint)\n-                  outputColumnNames: _col0, _col1\n+                  expressions: 'tst2' (type: string)\n+                  outputColumnNames: _col0\n                   Select Operator\n                     expressions: _col0 (type: string)\n                     outputColumnNames: _col0", "filename": "ql/src/test/results/clientpositive/tez/union5.q.out"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union7.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union7.q.out", "sha": "38917f19de4b98c583035f21642b72af5b0ee469", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union7.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -46,8 +46,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: s2\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Select Operator\n                       expressions: _col0 (type: string)\n                       outputColumnNames: _col0\n@@ -68,8 +68,8 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0\n                 Select Operator\n-                  expressions: 'tst1' (type: string), UDFToString(_col0) (type: string)\n-                  outputColumnNames: _col0, _col1\n+                  expressions: 'tst1' (type: string)\n+                  outputColumnNames: _col0\n                   Select Operator\n                     expressions: _col0 (type: string)\n                     outputColumnNames: _col0", "filename": "ql/src/test/results/clientpositive/tez/union7.q.out"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union9.q.out", "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union9.q.out", "sha": "04c5acb204d9da2decfc4d50ef6fad487218f518", "changes": 12, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union9.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c", "patch": "@@ -31,8 +31,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s1\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -46,8 +44,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s2\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -61,8 +57,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s3\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -77,14 +71,14 @@ STAGE PLANS:\n                 aggregations: count(VALUE._col0)\n                 mode: mergepartial\n                 outputColumnNames: _col0\n-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                 Select Operator\n                   expressions: _col0 (type: bigint)\n                   outputColumnNames: _col0\n-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                   File Output Operator\n                     compressed: false\n-                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                     table:\n                         input format: org.apache.hadoop.mapred.TextInputFormat\n                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat", "filename": "ql/src/test/results/clientpositive/tez/union9.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/daca6a6dedafd862bcf271a8cf117100378d3f57", "parent": "https://github.com/apache/hive/commit/fffdf11f26b1a5306af4770c581d0267cdbd7334", "message": "HIVE-6841: Vectorized execution throws NPE for partitioning columns with __HIVE_DEFAULT_PARTITION__ (reviewd by Hari, Ashutosh)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1585548 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_222", "file": [{"additions": 9, "raw_url": "https://github.com/apache/hive/raw/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java", "blob_url": "https://github.com/apache/hive/blob/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java", "sha": "4d5ed40eb19158c7883231849466deef3bfe07c9", "changes": 11, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java?ref=daca6a6dedafd862bcf271a8cf117100378d3f57", "patch": "@@ -29,6 +29,8 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hive.common.type.Decimal128;\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n@@ -63,7 +65,9 @@\n  * with the partition column.\n  */\n public class VectorizedRowBatchCtx {\n-  \n+\n+  private static final Log LOG = LogFactory.getLog(VectorizedRowBatchCtx.class.getName());\n+\n   // OI for raw row data (EG without partition cols)\n   private StructObjectInspector rawRowOI;\n \n@@ -223,6 +227,9 @@ public void init(Configuration hiveConf, FileSplit split) throws ClassNotFoundEx\n                   convert(partSpec.get(key));              \n           partitionTypes.put(key, TypeInfoFactory.getPrimitiveTypeInfo(partKeyTypes[i]).getPrimitiveCategory());\n         }\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Partition column: name: \" + key + \", value: \" + objectVal + \", type: \" + partitionTypes.get(key));\n+        }\n         partitionValues.put(key, objectVal);\n         partObjectInspectors.add(objectInspector);\n       }\n@@ -263,7 +270,7 @@ public VectorizedRowBatch createVectorizedRowBatch() throws HiveException\n       // in the included list.\n       if ((colsToInclude == null) || colsToInclude.contains(j)\n           || ((partitionValues != null) &&\n-              (partitionValues.get(fieldRefs.get(j).getFieldName()) != null))) {\n+              partitionValues.containsKey(fieldRefs.get(j).getFieldName()))) {\n         ObjectInspector foi = fieldRefs.get(j).getFieldObjectInspector();\n         switch (foi.getCategory()) {\n         case PRIMITIVE: {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/queries/clientpositive/vector_non_string_partition.q", "blob_url": "https://github.com/apache/hive/blob/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/queries/clientpositive/vector_non_string_partition.q", "sha": "fc1dc6d3b89f31b48131c25df4a04eccd8829ffa", "changes": 17, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/vector_non_string_partition.q?ref=daca6a6dedafd862bcf271a8cf117100378d3f57", "patch": "@@ -0,0 +1,17 @@\n+SET hive.vectorized.execution.enabled=true;\n+CREATE TABLE non_string_part(cint INT, cstring1 STRING, cdouble DOUBLE, ctimestamp1 TIMESTAMP) PARTITIONED BY (ctinyint tinyint) STORED AS ORC;\n+SET hive.exec.dynamic.partition.mode=nonstrict;\n+SET hive.exec.dynamic.partition=true;\n+\n+INSERT OVERWRITE TABLE non_string_part PARTITION(ctinyint) SELECT cint, cstring1, cdouble, ctimestamp1, ctinyint fROM alltypesorc \n+WHERE ctinyint IS NULL AND cdouble IS NOT NULL ORDER BY cdouble;\n+\n+SHOW PARTITIONS non_string_part;\n+\n+EXPLAIN SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10;\n+\n+SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10;\n+\n+EXPLAIN SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10;\n+\n+SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10;", "filename": "ql/src/test/queries/clientpositive/vector_non_string_partition.q"}, {"additions": 180, "raw_url": "https://github.com/apache/hive/raw/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/results/clientpositive/vector_non_string_partition.q.out", "blob_url": "https://github.com/apache/hive/blob/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/results/clientpositive/vector_non_string_partition.q.out", "sha": "7cd26258255a7e3dfa934f74244da4307d4fed7a", "changes": 180, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vector_non_string_partition.q.out?ref=daca6a6dedafd862bcf271a8cf117100378d3f57", "patch": "@@ -0,0 +1,180 @@\n+PREHOOK: query: CREATE TABLE non_string_part(cint INT, cstring1 STRING, cdouble DOUBLE, ctimestamp1 TIMESTAMP) PARTITIONED BY (ctinyint tinyint) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: CREATE TABLE non_string_part(cint INT, cstring1 STRING, cdouble DOUBLE, ctimestamp1 TIMESTAMP) PARTITIONED BY (ctinyint tinyint) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@non_string_part\n+PREHOOK: query: INSERT OVERWRITE TABLE non_string_part PARTITION(ctinyint) SELECT cint, cstring1, cdouble, ctimestamp1, ctinyint fROM alltypesorc \n+WHERE ctinyint IS NULL AND cdouble IS NOT NULL ORDER BY cdouble\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@non_string_part\n+POSTHOOK: query: INSERT OVERWRITE TABLE non_string_part PARTITION(ctinyint) SELECT cint, cstring1, cdouble, ctimestamp1, ctinyint fROM alltypesorc \n+WHERE ctinyint IS NULL AND cdouble IS NOT NULL ORDER BY cdouble\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+PREHOOK: query: SHOW PARTITIONS non_string_part\n+PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@non_string_part\n+POSTHOOK: query: SHOW PARTITIONS non_string_part\n+POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@non_string_part\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+ctinyint=__HIVE_DEFAULT_PARTITION__\n+PREHOOK: query: EXPLAIN SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: non_string_part\n+            Statistics: Num rows: 3073 Data size: 339150 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: (cint > 0) (type: boolean)\n+              Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: cint (type: int), ctinyint (type: tinyint)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: int)\n+                  sort order: +\n+                  Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: int), _col1 (type: tinyint)\n+      Execution mode: vectorized\n+      Reduce Operator Tree:\n+        Extract\n+          Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+          Limit\n+            Number of rows: 10\n+            Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: 10\n+\n+PREHOOK: query: SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@non_string_part\n+PREHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@non_string_part\n+POSTHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+762\tNULL\n+762\tNULL\n+6981\tNULL\n+6981\tNULL\n+6981\tNULL\n+86028\tNULL\n+504142\tNULL\n+799471\tNULL\n+1248059\tNULL\n+1286921\tNULL\n+PREHOOK: query: EXPLAIN SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: non_string_part\n+            Statistics: Num rows: 3073 Data size: 339150 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: (cint > 0) (type: boolean)\n+              Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: cint (type: int), cstring1 (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: int), _col1 (type: string)\n+                  sort order: ++\n+                  Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: int), _col1 (type: string)\n+      Execution mode: vectorized\n+      Reduce Operator Tree:\n+        Extract\n+          Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+          Limit\n+            Number of rows: 10\n+            Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: 10\n+\n+PREHOOK: query: SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@non_string_part\n+PREHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@non_string_part\n+POSTHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+762\t3WsVeqb28VWEEOLI8ail\n+762\t40ks5556SV\n+6981\t1FNNhmiFLGw425NA13g\n+6981\to5mb0QP5Y48Qd4vdB0\n+6981\tsF2CRfgt2K\n+86028\tT2o8XRFAL0HC4ikDQnfoCymw\n+504142\tPlOxor04p5cvVl\n+799471\t2fu24\n+1248059\tUhps6mMh3IfHB3j7yH62K\n+1286921\tODLrXI8882q8LS8", "filename": "ql/src/test/results/clientpositive/vector_non_string_partition.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d375b3977d42936b3b00888e2b1bacf736e8ac3e", "parent": "https://github.com/apache/hive/commit/1569a0e7a1d72e182ababb43f5f2b14d8bf8494d", "message": "HIVE-6753: Unions on Tez NPE when there's a mapjoin the union work (Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1582488 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_223", "file": [{"additions": 16, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java", "sha": "803903256b5986e75cf193c5cb73fd27ccdec701", "changes": 18, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -123,6 +123,9 @@ private void setupMRLegacyConfigs(TezProcessorContext processorContext) {\n   @Override\n   public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs)\n       throws Exception {\n+    \n+    Exception processingException = null;\n+    \n     try{\n       perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_RUN_PROCESSOR);\n       // in case of broadcast-join read the broadcast edge inputs\n@@ -160,9 +163,20 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out\n \n       //done - output does not need to be committed as hive does not use outputcommitter\n       perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_RUN_PROCESSOR);\n+    } catch (Exception e) {\n+      processingException = e;\n     } finally {\n-      if(rproc != null){\n-        rproc.close();\n+      try {\n+        if(rproc != null){\n+          rproc.close();\n+        }\n+      } catch (Exception e) {\n+        if (processingException == null) {\n+          processingException = e;\n+        }\n+      }\n+      if (processingException != null) {\n+        throw processingException;\n       }\n     }\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java"}, {"additions": 78, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java", "sha": "135bb4df5252b3ddc21483f237a68e2ab5ddc515", "changes": 144, "status": "modified", "deletions": 66, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -42,6 +42,7 @@\n import org.apache.hadoop.hive.ql.plan.HashTableDummyDesc;\n import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n import org.apache.hadoop.hive.ql.plan.TezWork;\n import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;\n@@ -63,12 +64,16 @@\n   public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs)\n       throws SemanticException {\n     GenTezProcContext context = (GenTezProcContext) procContext;\n+    MapJoinOperator mapJoinOp = (MapJoinOperator)nd;\n+\n+    if (stack.size() < 2 || !(stack.get(stack.size() - 2) instanceof ReduceSinkOperator)) {\n+      context.currentMapJoinOperators.add(mapJoinOp);\n+      return null;\n+    }\n+\n     context.preceedingWork = null;\n     context.currentRootOperator = null;\n \n-    MapJoinOperator mapJoinOp = (MapJoinOperator)nd;\n-    Operator<? extends OperatorDesc> childOp = mapJoinOp.getChildOperators().get(0);\n-\n     ReduceSinkOperator parentRS = (ReduceSinkOperator)stack.get(stack.size() - 2);\n \n     // remember the original parent list before we start modifying it.\n@@ -77,66 +82,72 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,\n       context.mapJoinParentMap.put(mapJoinOp, parents);\n     }\n \n-    BaseWork myWork = null;\n-\n-    while (childOp != null) {\n-      if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof FileSinkOperator)) {\n-        /*\n-         *  if there was a pre-existing work generated for the big-table mapjoin side,\n-         *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)\n-         *  with the pre-existing work.\n-         *\n-         *  Otherwise, we need to associate that the reduce sink/file sink down the MJ path\n-         *  to be linked to the RS work (associated with the RS-MJ pattern).\n-         *\n-         */\n-\n-        myWork = context.operatorWorkMap.get(childOp);\n-        BaseWork parentWork = context.operatorWorkMap.get(parentRS);\n-\n-        // set the link between mapjoin and parent vertex\n-        int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);\n-        if (pos == -1) {\n-          throw new SemanticException(\"Cannot find position of parent in mapjoin\");\n-        }\n-        LOG.debug(\"Mapjoin \"+mapJoinOp+\", pos: \"+pos+\" --> \"+parentWork.getName());\n-        mapJoinOp.getConf().getParentToInput().put(pos, parentWork.getName());\n-\n-        if (myWork != null) {\n-          // link the work with the work associated with the reduce sink that triggered this rule\n-          TezWork tezWork = context.currentTask.getWork();\n-          tezWork.connect(parentWork, myWork, EdgeType.BROADCAST_EDGE);\n-\n-          // remember the output name of the reduce sink\n-          parentRS.getConf().setOutputName(myWork.getName());\n-          context.connectedReduceSinks.add(parentRS);\n+    List<BaseWork> mapJoinWork = null;\n+\n+    /*\n+     *  if there was a pre-existing work generated for the big-table mapjoin side,\n+     *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)\n+     *  with the pre-existing work.\n+     *\n+     *  Otherwise, we need to associate that the mapjoin op\n+     *  to be linked to the RS work (associated with the RS-MJ pattern).\n+     *\n+     */\n+    mapJoinWork = context.mapJoinWorkMap.get(mapJoinOp);\n+    BaseWork parentWork;\n+    if (context.unionWorkMap.containsKey(parentRS)) {\n+      parentWork = context.unionWorkMap.get(parentRS);\n+    } else {\n+      assert context.childToWorkMap.get(parentRS).size() == 1;\n+      parentWork = context.childToWorkMap.get(parentRS).get(0);\n+    }\n \n+    // set the link between mapjoin and parent vertex\n+    int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);\n+    if (pos == -1) {\n+      throw new SemanticException(\"Cannot find position of parent in mapjoin\");\n+    }\n+    LOG.debug(\"Mapjoin \"+mapJoinOp+\", pos: \"+pos+\" --> \"+parentWork.getName());\n+    mapJoinOp.getConf().getParentToInput().put(pos, parentWork.getName());\n+\n+    if (mapJoinWork != null) {\n+      for (BaseWork myWork: mapJoinWork) {\n+        // link the work with the work associated with the reduce sink that triggered this rule\n+        TezWork tezWork = context.currentTask.getWork();\n+        LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+myWork.getName());\n+        tezWork.connect(parentWork, myWork, EdgeType.BROADCAST_EDGE);\n+        \n+        ReduceSinkOperator r = null;\n+        if (parentRS.getConf().getOutputName() != null) {\n+          LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n+          // we've already set this one up. Need to clone for the next work.\n+          r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n+              (ReduceSinkDesc) parentRS.getConf().clone(), parentRS.getParentOperators());\n+          context.clonedReduceSinks.add(r);\n         } else {\n-          List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(childOp);\n-          if (linkWorkList == null) {\n-            linkWorkList = new ArrayList<BaseWork>();\n-          }\n-          linkWorkList.add(parentWork);\n-          context.linkOpWithWorkMap.put(childOp, linkWorkList);\n-\n-          List<ReduceSinkOperator> reduceSinks \n-            = context.linkWorkWithReduceSinkMap.get(parentWork);\n-          if (reduceSinks == null) {\n-            reduceSinks = new ArrayList<ReduceSinkOperator>();\n-          }\n-          reduceSinks.add(parentRS);\n-          context.linkWorkWithReduceSinkMap.put(parentWork, reduceSinks);\n+          r = parentRS;\n         }\n-\n-        break;\n+        // remember the output name of the reduce sink\n+        r.getConf().setOutputName(myWork.getName());\n+        context.connectedReduceSinks.add(r);\n       }\n+    }\n \n-      if ((childOp.getChildOperators() != null) && (childOp.getChildOperators().size() >= 1)) {\n-        childOp = childOp.getChildOperators().get(0);\n-      } else {\n-        break;\n-      }\n+    // remember in case we need to connect additional work later\n+    List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(mapJoinOp);\n+    if (linkWorkList == null) {\n+      linkWorkList = new ArrayList<BaseWork>();\n+    }\n+    linkWorkList.add(parentWork);\n+    context.linkOpWithWorkMap.put(mapJoinOp, linkWorkList);\n+    \n+    List<ReduceSinkOperator> reduceSinks \n+      = context.linkWorkWithReduceSinkMap.get(parentWork);\n+    if (reduceSinks == null) {\n+      reduceSinks = new ArrayList<ReduceSinkOperator>();\n     }\n+    reduceSinks.add(parentRS);\n+    context.linkWorkWithReduceSinkMap.put(parentWork, reduceSinks);\n \n     // create the dummy operators\n     List<Operator<? extends OperatorDesc>> dummyOperators =\n@@ -178,17 +189,18 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,\n \n     // the \"work\" needs to know about the dummy operators. They have to be separately initialized\n     // at task startup\n-    if (myWork != null) {\n-      myWork.addDummyOp(dummyOp);\n-    } else {\n-      List<Operator<?>> dummyList = dummyOperators;\n-      if (context.linkChildOpWithDummyOp.containsKey(childOp)) {\n-        dummyList = context.linkChildOpWithDummyOp.get(childOp);\n+    if (mapJoinWork != null) {\n+      for (BaseWork myWork: mapJoinWork) {\n+        myWork.addDummyOp(dummyOp);\n+      }\n+    }\n+    if (context.linkChildOpWithDummyOp.containsKey(mapJoinOp)) {\n+      for (Operator<?> op: context.linkChildOpWithDummyOp.get(mapJoinOp)) {\n+        dummyOperators.add(op);\n       }\n-      dummyList.add(dummyOp);\n-      context.linkChildOpWithDummyOp.put(childOp, dummyList);\n     }\n+    context.linkChildOpWithDummyOp.put(mapJoinOp, dummyOperators);\n+\n     return true;\n   }\n-\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java", "sha": "ec21aa8e07dd872d32fe7286b2e7538e8e628a8a", "changes": 16, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -95,12 +95,15 @@\n   // map join work\n   public final Map<BaseWork, List<ReduceSinkOperator>> linkWorkWithReduceSinkMap;\n \n-  // a map that maintains operator (file-sink or reduce-sink) to work mapping\n-  public final Map<Operator<?>, BaseWork> operatorWorkMap;\n+  // map that says which mapjoin belongs to which work item\n+  public final Map<MapJoinOperator, List<BaseWork>> mapJoinWorkMap;\n \n   // a map to keep track of which root generated which work\n   public final Map<Operator<?>, BaseWork> rootToWorkMap;\n \n+  // a map to keep track of which child generated with work\n+  public final Map<Operator<?>, List<BaseWork>> childToWorkMap;\n+\n   // we need to keep the original list of operators in the map join to know\n   // what position in the mapjoin the different parent work items will have.\n   public final Map<MapJoinOperator, List<Operator<?>>> mapJoinParentMap;\n@@ -111,10 +114,14 @@\n   // used to group dependent tasks for multi table inserts\n   public final DependencyCollectionTask dependencyTask;\n \n+  // remember map joins as we encounter them.\n+  public final Set<MapJoinOperator> currentMapJoinOperators;\n+\n   // used to hook up unions\n   public final Map<Operator<?>, BaseWork> unionWorkMap;\n   public final List<UnionOperator> currentUnionOperators;\n   public final Set<BaseWork> workWithUnionOperators;\n+  public final Set<ReduceSinkOperator> clonedReduceSinks;\n \n   // we link filesink that will write to the same final location\n   public final Map<Path, List<FileSinkDesc>> linkedFileSinks;\n@@ -139,15 +146,18 @@ public GenTezProcContext(HiveConf conf, ParseContext parseContext,\n     this.leafOperatorToFollowingWork = new HashMap<Operator<?>, BaseWork>();\n     this.linkOpWithWorkMap = new HashMap<Operator<?>, List<BaseWork>>();\n     this.linkWorkWithReduceSinkMap = new HashMap<BaseWork, List<ReduceSinkOperator>>();\n-    this.operatorWorkMap = new HashMap<Operator<?>, BaseWork>();\n+    this.mapJoinWorkMap = new HashMap<MapJoinOperator, List<BaseWork>>();\n     this.rootToWorkMap = new HashMap<Operator<?>, BaseWork>();\n+    this.childToWorkMap = new HashMap<Operator<?>, List<BaseWork>>();\n     this.mapJoinParentMap = new HashMap<MapJoinOperator, List<Operator<?>>>();\n+    this.currentMapJoinOperators = new HashSet<MapJoinOperator>();\n     this.linkChildOpWithDummyOp = new HashMap<Operator<?>, List<Operator<?>>>();\n     this.dependencyTask = (DependencyCollectionTask)\n         TaskFactory.get(new DependencyCollectionWork(), conf);\n     this.unionWorkMap = new HashMap<Operator<?>, BaseWork>();\n     this.currentUnionOperators = new LinkedList<UnionOperator>();\n     this.workWithUnionOperators = new HashSet<BaseWork>();\n+    this.clonedReduceSinks = new HashSet<ReduceSinkOperator>();\n     this.linkedFileSinks = new HashMap<Path, List<FileSinkDesc>>();\n     this.fileSinkSet = new HashSet<FileSinkOperator>();\n     this.connectedReduceSinks = new HashSet<ReduceSinkOperator>();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java"}, {"additions": 75, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java", "sha": "f0b3fdf612c876db4297839ab195c66c25e485a7", "changes": 108, "status": "modified", "deletions": 33, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -29,7 +29,9 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n import org.apache.hadoop.hive.ql.exec.HashTableDummyOperator;\n+import org.apache.hadoop.hive.ql.exec.MapJoinOperator;\n import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n import org.apache.hadoop.hive.ql.exec.UnionOperator;\n@@ -41,6 +43,7 @@\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.plan.MapWork;\n import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n import org.apache.hadoop.hive.ql.plan.ReduceWork;\n import org.apache.hadoop.hive.ql.plan.TezWork;\n import org.apache.hadoop.hive.ql.plan.UnionWork;\n@@ -87,6 +90,12 @@ public Object process(Node nd, Stack<Node> stack,\n     LOG.debug(\"Root operator: \" + root);\n     LOG.debug(\"Leaf operator: \" + operator);\n \n+    if (context.clonedReduceSinks.contains(operator)) {\n+      // if we're visiting a terminal we've created ourselves,\n+      // just skip and keep going\n+      return null;\n+    }\n+\n     TezWork tezWork = context.currentTask.getWork();\n \n     // Right now the work graph is pretty simple. If there is no\n@@ -112,42 +121,75 @@ public Object process(Node nd, Stack<Node> stack,\n       }\n       context.rootToWorkMap.put(root, work);\n     }\n-    context.operatorWorkMap.put(operator, work);\n-\n-    /*\n-     * this happens in case of map join operations.\n-     * The tree looks like this:\n-     *\n-     *       RS <--- we are here perhaps\n-     *       |\n-     *    MapJoin\n-     *    /     \\\n-     *  RS       TS\n-     *  /\n-     * TS\n-     *\n-     * If we are at the RS pointed above, and we may have already visited the\n-     * RS following the TS, we have already generated work for the TS-RS.\n-     * We need to hook the current work to this generated work.\n-     */\n-    List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(operator);\n-    if (linkWorkList != null) {\n-      if (context.linkChildOpWithDummyOp.containsKey(operator)) {\n-        for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(operator)) {\n-          work.addDummyOp((HashTableDummyOperator) dummy);\n+\n+    if (!context.childToWorkMap.containsKey(operator)) {\n+      List<BaseWork> workItems = new LinkedList<BaseWork>();\n+      workItems.add(work);\n+      context.childToWorkMap.put(operator, workItems);\n+    } else {\n+      context.childToWorkMap.get(operator).add(work);\n+    }\n+\n+    // remember which mapjoin operator links with which work\n+    if (!context.currentMapJoinOperators.isEmpty()) {\n+      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n+        LOG.debug(\"Processing map join: \" + mj);\n+        // remember the mapping in case we scan another branch of the \n+        // mapjoin later\n+        if (!context.mapJoinWorkMap.containsKey(mj)) {\n+          List<BaseWork> workItems = new LinkedList<BaseWork>();\n+          workItems.add(work);\n+          context.mapJoinWorkMap.put(mj, workItems);\n+        } else {\n+          context.mapJoinWorkMap.get(mj).add(work);\n         }\n-      }\n-      for (BaseWork parentWork : linkWorkList) {\n-        tezWork.connect(parentWork, work, EdgeType.BROADCAST_EDGE);\n-\n-        // need to set up output name for reduce sink now that we know the name\n-        // of the downstream work\n-        for (ReduceSinkOperator r:\n-               context.linkWorkWithReduceSinkMap.get(parentWork)) {\n-          r.getConf().setOutputName(work.getName());\n-          context.connectedReduceSinks.add(r);\n+\n+        /*\n+         * this happens in case of map join operations.\n+         * The tree looks like this:\n+         *\n+         *        RS <--- we are here perhaps\n+         *        |\n+         *     MapJoin\n+         *     /     \\\n+         *   RS       TS\n+         *  /\n+         * TS\n+         *\n+         * If we are at the RS pointed above, and we may have already visited the\n+         * RS following the TS, we have already generated work for the TS-RS.\n+         * We need to hook the current work to this generated work.\n+         */\n+        List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(mj);\n+        if (linkWorkList != null) {\n+          if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n+            for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n+              work.addDummyOp((HashTableDummyOperator) dummy);\n+            }\n+          }\n+          for (BaseWork parentWork : linkWorkList) {\n+            LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n+            tezWork.connect(parentWork, work, EdgeType.BROADCAST_EDGE);\n+\n+            // need to set up output name for reduce sink now that we know the name\n+            // of the downstream work\n+            for (ReduceSinkOperator r:\n+                   context.linkWorkWithReduceSinkMap.get(parentWork)) {\n+              if (r.getConf().getOutputName() != null) {\n+                LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n+                // we've already set this one up. Need to clone for the next work.\n+                r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n+                    (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n+                context.clonedReduceSinks.add(r);\n+              }\n+              r.getConf().setOutputName(work.getName());\n+              context.connectedReduceSinks.add(r);\n+            }\n+          }\n         }\n       }\n+      // clear out the set. we don't need it anymore.\n+      context.currentMapJoinOperators.clear();\n     }\n \n     // This is where we cut the tree as described above. We also remember that", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "sha": "e1ce79a41bf475ba2b0fcd98d8e5ab7329db6d0a", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -143,7 +143,6 @@ protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, Pa\n         genTezWork);\n \n     opRules.put(new RuleRegExp(\"No more walking on ReduceSink-MapJoin\",\n-        ReduceSinkOperator.getOperatorName() + \"%\" +\n         MapJoinOperator.getOperatorName() + \"%\"), new ReduceSinkMapJoinProc());\n \n     opRules.put(new RuleRegExp(\"Split Work + Move/Merge - FileSink\",\n@@ -154,7 +153,7 @@ protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, Pa\n         TableScanOperator.getOperatorName() + \"%\"),\n         new ProcessAnalyzeTable(GenTezUtils.getUtils()));\n \n-    opRules.put(new RuleRegExp(\"Handle union\",\n+    opRules.put(new RuleRegExp(\"Remember union\",\n         UnionOperator.getOperatorName() + \"%\"), new NodeProcessor()\n     {\n       @Override", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java", "sha": "157d07283ba9d752108c860f4ba5f04cc7130382", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -132,6 +132,7 @@ public Object clone() {\n     desc.setValueSerializeInfo((TableDesc) getValueSerializeInfo().clone());\n     desc.setNumBuckets(numBuckets);\n     desc.setBucketCols(bucketCols);\n+    desc.setStatistics(this.getStatistics());\n     return desc;\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java"}, {"additions": 74, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/queries/clientpositive/tez_union.q", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/queries/clientpositive/tez_union.q", "sha": "f80d94c4a15fd1288ea6b0845b2896de438e0c23", "changes": 75, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union.q?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -5,17 +5,90 @@ select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2;\n \n+create table ut as\n select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2;\n \n+select * from ut order by key, value limit 20;\n+drop table ut;\n+\n set hive.auto.convert.join=false;\n \n explain\n with u as (select * from src union all select * from src)\n select count(*) from (select u1.key as k1, u2.key as k2 from\n u as u1 join u as u2 on (u1.key = u2.key)) a;\n \n+create table ut as\n with u as (select * from src union all select * from src)\n-select count(*) from (select u1.key as k1, u2.key as k2 from\n+select count(*) as cnt from (select u1.key as k1, u2.key as k2 from\n u as u1 join u as u2 on (u1.key = u2.key)) a;\n+\n+select * from ut order by cnt limit 20;\n+drop table ut;\n+\n+set hive.auto.convert.join=true;\n+\n+explain select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key;\n+\n+create table ut as\n+select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key;\n+\n+select * from ut order by skey, ukey limit 20;\n+drop table ut;\n+\n+explain select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey;\n+\n+create table ut as\n+select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey;\n+\n+select * from ut order by skey, ukey, lkey limit 100;\n+\n+drop table ut;\n+\n+explain\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key;\n+\n+create table ut as\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key;\n+\n+select * from ut order by key limit 30;\n+\n+drop table ut;\n+\n+explain\n+select * from\n+(select * from src union all select * from src) u\n+left outer join src s on u.key = s.key;\n+\n+explain\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key;\n+\n+create table ut as\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key;\n+\n+select * from ut order by ukey, skey limit 20;\n+drop table ut;\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/tez_union.q"}, {"additions": 1882, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out", "sha": "ea0f1b9b4dbecef7b80d5f15396b3991c41ed8ca", "changes": 3741, "status": "modified", "deletions": 1859, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "filename": "ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out"}, {"additions": 942, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out", "sha": "75de746b7edfd2c6fdfc0f20e6ae08d6a5452877", "changes": 1872, "status": "modified", "deletions": 930, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -44,6 +44,8 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n@@ -55,14 +57,24 @@ STAGE PLANS:\n                     expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)\n                     outputColumnNames: _col0, _col1, _col2, _col3\n                     Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n-                    File Output Operator\n-                      compressed: false\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string), _col3 (type: string)\n+                      sort order: ++\n+                      Map-reduce partition columns: _col2 (type: string), _col3 (type: string)\n                       Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n-                      table:\n-                          input format: org.apache.hadoop.mapred.TextInputFormat\n-                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                          name: default.nzhang_part3\n+                      value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Extract\n+                Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      name: default.nzhang_part3\n \n   Stage: Stage-2\n     Dependency Collection\n@@ -634,1006 +646,1006 @@ POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(sr\n 400\tval_400\t2008-04-08\t11\n 200\tval_200\t2008-04-08\t11\n 97\tval_97\t2008-04-08\t11\n-238\tval_238\t2008-04-08\t12\n-86\tval_86\t2008-04-08\t12\n-311\tval_311\t2008-04-08\t12\n-27\tval_27\t2008-04-08\t12\n-165\tval_165\t2008-04-08\t12\n-409\tval_409\t2008-04-08\t12\n-255\tval_255\t2008-04-08\t12\n-278\tval_278\t2008-04-08\t12\n-98\tval_98\t2008-04-08\t12\n-484\tval_484\t2008-04-08\t12\n-265\tval_265\t2008-04-08\t12\n-193\tval_193\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-150\tval_150\t2008-04-08\t12\n-273\tval_273\t2008-04-08\t12\n-224\tval_224\t2008-04-08\t12\n-369\tval_369\t2008-04-08\t12\n-66\tval_66\t2008-04-08\t12\n-128\tval_128\t2008-04-08\t12\n-213\tval_213\t2008-04-08\t12\n-146\tval_146\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-429\tval_429\t2008-04-08\t12\n-374\tval_374\t2008-04-08\t12\n+97\tval_97\t2008-04-08\t12\n+200\tval_200\t2008-04-08\t12\n+400\tval_400\t2008-04-08\t12\n+403\tval_403\t2008-04-08\t12\n+169\tval_169\t2008-04-08\t12\n+90\tval_90\t2008-04-08\t12\n+126\tval_126\t2008-04-08\t12\n+222\tval_222\t2008-04-08\t12\n+477\tval_477\t2008-04-08\t12\n+414\tval_414\t2008-04-08\t12\n+194\tval_194\t2008-04-08\t12\n+307\tval_307\t2008-04-08\t12\n+348\tval_348\t2008-04-08\t12\n 152\tval_152\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n-145\tval_145\t2008-04-08\t12\n-495\tval_495\t2008-04-08\t12\n+448\tval_448\t2008-04-08\t12\n 37\tval_37\t2008-04-08\t12\n-327\tval_327\t2008-04-08\t12\n+28\tval_28\t2008-04-08\t12\n+84\tval_84\t2008-04-08\t12\n+315\tval_315\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+97\tval_97\t2008-04-08\t12\n+344\tval_344\t2008-04-08\t12\n 281\tval_281\t2008-04-08\t12\n-277\tval_277\t2008-04-08\t12\n-209\tval_209\t2008-04-08\t12\n-15\tval_15\t2008-04-08\t12\n-82\tval_82\t2008-04-08\t12\n-403\tval_403\t2008-04-08\t12\n-166\tval_166\t2008-04-08\t12\n-417\tval_417\t2008-04-08\t12\n-430\tval_430\t2008-04-08\t12\n-252\tval_252\t2008-04-08\t12\n-292\tval_292\t2008-04-08\t12\n-219\tval_219\t2008-04-08\t12\n-287\tval_287\t2008-04-08\t12\n-153\tval_153\t2008-04-08\t12\n-193\tval_193\t2008-04-08\t12\n-338\tval_338\t2008-04-08\t12\n-446\tval_446\t2008-04-08\t12\n-459\tval_459\t2008-04-08\t12\n-394\tval_394\t2008-04-08\t12\n-237\tval_237\t2008-04-08\t12\n-482\tval_482\t2008-04-08\t12\n-174\tval_174\t2008-04-08\t12\n-413\tval_413\t2008-04-08\t12\n-494\tval_494\t2008-04-08\t12\n+183\tval_183\t2008-04-08\t12\n+273\tval_273\t2008-04-08\t12\n+18\tval_18\t2008-04-08\t12\n+167\tval_167\t2008-04-08\t12\n+348\tval_348\t2008-04-08\t12\n+285\tval_285\t2008-04-08\t12\n+186\tval_186\t2008-04-08\t12\n+362\tval_362\t2008-04-08\t12\n+458\tval_458\t2008-04-08\t12\n+146\tval_146\t2008-04-08\t12\n+498\tval_498\t2008-04-08\t12\n+341\tval_341\t2008-04-08\t12\n+9\tval_9\t2008-04-08\t12\n+298\tval_298\t2008-04-08\t12\n+100\tval_100\t2008-04-08\t12\n+492\tval_492\t2008-04-08\t12\n+462\tval_462\t2008-04-08\t12\n+18\tval_18\t2008-04-08\t12\n+379\tval_379\t2008-04-08\t12\n+384\tval_384\t2008-04-08\t12\n+67\tval_67\t2008-04-08\t12\n+134\tval_134\t2008-04-08\t12\n+26\tval_26\t2008-04-08\t12\n+256\tval_256\t2008-04-08\t12\n+384\tval_384\t2008-04-08\t12\n+407\tval_407\t2008-04-08\t12\n+421\tval_421\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+375\tval_375\t2008-04-08\t12\n+454\tval_454\t2008-04-08\t12\n+189\tval_189\t2008-04-08\t12\n+175\tval_175\t2008-04-08\t12\n+133\tval_133\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+233\tval_233\t2008-04-08\t12\n+462\tval_462\t2008-04-08\t12\n+214\tval_214\t2008-04-08\t12\n+172\tval_172\t2008-04-08\t12\n+353\tval_353\t2008-04-08\t12\n+136\tval_136\t2008-04-08\t12\n+83\tval_83\t2008-04-08\t12\n+480\tval_480\t2008-04-08\t12\n+265\tval_265\t2008-04-08\t12\n+249\tval_249\t2008-04-08\t12\n 207\tval_207\t2008-04-08\t12\n+460\tval_460\t2008-04-08\t12\n+493\tval_493\t2008-04-08\t12\n+333\tval_333\t2008-04-08\t12\n+317\tval_317\t2008-04-08\t12\n+310\tval_310\t2008-04-08\t12\n+468\tval_468\t2008-04-08\t12\n+178\tval_178\t2008-04-08\t12\n+478\tval_478\t2008-04-08\t12\n+230\tval_230\t2008-04-08\t12\n+277\tval_277\t2008-04-08\t12\n+325\tval_325\t2008-04-08\t12\n+323\tval_323\t2008-04-08\t12\n+443\tval_443\t2008-04-08\t12\n+169\tval_169\t2008-04-08\t12\n+429\tval_429\t2008-04-08\t12\n+120\tval_120\t2008-04-08\t12\n+444\tval_444\t2008-04-08\t12\n 199\tval_199\t2008-04-08\t12\n-466\tval_466\t2008-04-08\t12\n-208\tval_208\t2008-04-08\t12\n-174\tval_174\t2008-04-08\t12\n-399\tval_399\t2008-04-08\t12\n-396\tval_396\t2008-04-08\t12\n-247\tval_247\t2008-04-08\t12\n 417\tval_417\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-162\tval_162\t2008-04-08\t12\n-377\tval_377\t2008-04-08\t12\n-397\tval_397\t2008-04-08\t12\n-309\tval_309\t2008-04-08\t12\n-365\tval_365\t2008-04-08\t12\n-266\tval_266\t2008-04-08\t12\n+305\tval_305\t2008-04-08\t12\n+479\tval_479\t2008-04-08\t12\n+248\tval_248\t2008-04-08\t12\n+360\tval_360\t2008-04-08\t12\n 439\tval_439\t2008-04-08\t12\n-342\tval_342\t2008-04-08\t12\n-367\tval_367\t2008-04-08\t12\n-325\tval_325\t2008-04-08\t12\n-167\tval_167\t2008-04-08\t12\n-195\tval_195\t2008-04-08\t12\n-475\tval_475\t2008-04-08\t12\n-17\tval_17\t2008-04-08\t12\n-113\tval_113\t2008-04-08\t12\n-155\tval_155\t2008-04-08\t12\n-203\tval_203\t2008-04-08\t12\n-339\tval_339\t2008-04-08\t12\n-0\tval_0\t2008-04-08\t12\n-455\tval_455\t2008-04-08\t12\n-128\tval_128\t2008-04-08\t12\n-311\tval_311\t2008-04-08\t12\n-316\tval_316\t2008-04-08\t12\n-57\tval_57\t2008-04-08\t12\n-302\tval_302\t2008-04-08\t12\n-205\tval_205\t2008-04-08\t12\n-149\tval_149\t2008-04-08\t12\n+237\tval_237\t2008-04-08\t12\n+491\tval_491\t2008-04-08\t12\n+200\tval_200\t2008-04-08\t12\n+414\tval_414\t2008-04-08\t12\n+119\tval_119\t2008-04-08\t12\n 438\tval_438\t2008-04-08\t12\n-345\tval_345\t2008-04-08\t12\n-129\tval_129\t2008-04-08\t12\n-170\tval_170\t2008-04-08\t12\n-20\tval_20\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-157\tval_157\t2008-04-08\t12\n-378\tval_378\t2008-04-08\t12\n-221\tval_221\t2008-04-08\t12\n-92\tval_92\t2008-04-08\t12\n-111\tval_111\t2008-04-08\t12\n-47\tval_47\t2008-04-08\t12\n-72\tval_72\t2008-04-08\t12\n-4\tval_4\t2008-04-08\t12\n-280\tval_280\t2008-04-08\t12\n-35\tval_35\t2008-04-08\t12\n-427\tval_427\t2008-04-08\t12\n-277\tval_277\t2008-04-08\t12\n-208\tval_208\t2008-04-08\t12\n-356\tval_356\t2008-04-08\t12\n-399\tval_399\t2008-04-08\t12\n-169\tval_169\t2008-04-08\t12\n+163\tval_163\t2008-04-08\t12\n+70\tval_70\t2008-04-08\t12\n+104\tval_104\t2008-04-08\t12\n+255\tval_255\t2008-04-08\t12\n+351\tval_351\t2008-04-08\t12\n+24\tval_24\t2008-04-08\t12\n+291\tval_291\t2008-04-08\t12\n+480\tval_480\t2008-04-08\t12\n+397\tval_397\t2008-04-08\t12\n+70\tval_70\t2008-04-08\t12\n+5\tval_5\t2008-04-08\t12\n 382\tval_382\t2008-04-08\t12\n-498\tval_498\t2008-04-08\t12\n-125\tval_125\t2008-04-08\t12\n-386\tval_386\t2008-04-08\t12\n-437\tval_437\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n-192\tval_192\t2008-04-08\t12\n-286\tval_286\t2008-04-08\t12\n 187\tval_187\t2008-04-08\t12\n-176\tval_176\t2008-04-08\t12\n-54\tval_54\t2008-04-08\t12\n-459\tval_459\t2008-04-08\t12\n-51\tval_51\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-103\tval_103\t2008-04-08\t12\n-239\tval_239\t2008-04-08\t12\n-213\tval_213\t2008-04-08\t12\n-216\tval_216\t2008-04-08\t12\n-430\tval_430\t2008-04-08\t12\n-278\tval_278\t2008-04-08\t12\n-176\tval_176\t2008-04-08\t12\n-289\tval_289\t2008-04-08\t12\n-221\tval_221\t2008-04-08\t12\n-65\tval_65\t2008-04-08\t12\n-318\tval_318\t2008-04-08\t12\n-332\tval_332\t2008-04-08\t12\n-311\tval_311\t2008-04-08\t12\n-275\tval_275\t2008-04-08\t12\n-137\tval_137\t2008-04-08\t12\n-241\tval_241\t2008-04-08\t12\n-83\tval_83\t2008-04-08\t12\n-333\tval_333\t2008-04-08\t12\n-180\tval_180\t2008-04-08\t12\n-284\tval_284\t2008-04-08\t12\n+424\tval_424\t2008-04-08\t12\n+164\tval_164\t2008-04-08\t12\n+431\tval_431\t2008-04-08\t12\n+125\tval_125\t2008-04-08\t12\n+298\tval_298\t2008-04-08\t12\n+478\tval_478\t2008-04-08\t12\n+454\tval_454\t2008-04-08\t12\n+431\tval_431\t2008-04-08\t12\n+164\tval_164\t2008-04-08\t12\n+217\tval_217\t2008-04-08\t12\n+201\tval_201\t2008-04-08\t12\n+396\tval_396\t2008-04-08\t12\n 12\tval_12\t2008-04-08\t12\n-230\tval_230\t2008-04-08\t12\n-181\tval_181\t2008-04-08\t12\n-67\tval_67\t2008-04-08\t12\n-260\tval_260\t2008-04-08\t12\n-404\tval_404\t2008-04-08\t12\n-384\tval_384\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-353\tval_353\t2008-04-08\t12\n-373\tval_373\t2008-04-08\t12\n-272\tval_272\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-217\tval_217\t2008-04-08\t12\n-84\tval_84\t2008-04-08\t12\n+424\tval_424\t2008-04-08\t12\n 348\tval_348\t2008-04-08\t12\n+262\tval_262\t2008-04-08\t12\n+203\tval_203\t2008-04-08\t12\n+90\tval_90\t2008-04-08\t12\n+258\tval_258\t2008-04-08\t12\n+114\tval_114\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+190\tval_190\t2008-04-08\t12\n+409\tval_409\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+257\tval_257\t2008-04-08\t12\n+105\tval_105\t2008-04-08\t12\n+53\tval_53\t2008-04-08\t12\n+483\tval_483\t2008-04-08\t12\n+403\tval_403\t2008-04-08\t12\n+175\tval_175\t2008-04-08\t12\n+366\tval_366\t2008-04-08\t12\n 466\tval_466\t2008-04-08\t12\n-58\tval_58\t2008-04-08\t12\n-8\tval_8\t2008-04-08\t12\n-411\tval_411\t2008-04-08\t12\n-230\tval_230\t2008-04-08\t12\n-208\tval_208\t2008-04-08\t12\n-348\tval_348\t2008-04-08\t12\n-24\tval_24\t2008-04-08\t12\n+104\tval_104\t2008-04-08\t12\n+335\tval_335\t2008-04-08\t12\n+321\tval_321\t2008-04-08\t12\n+193\tval_193\t2008-04-08\t12\n+44\tval_44\t2008-04-08\t12\n+80\tval_80\t2008-04-08\t12\n+235\tval_235\t2008-04-08\t12\n+331\tval_331\t2008-04-08\t12\n+283\tval_283\t2008-04-08\t12\n+35\tval_35\t2008-04-08\t12\n+2\tval_2\t2008-04-08\t12\n+280\tval_280\t2008-04-08\t12\n 463\tval_463\t2008-04-08\t12\n-431\tval_431\t2008-04-08\t12\n-179\tval_179\t2008-04-08\t12\n-172\tval_172\t2008-04-08\t12\n-42\tval_42\t2008-04-08\t12\n-129\tval_129\t2008-04-08\t12\n-158\tval_158\t2008-04-08\t12\n-119\tval_119\t2008-04-08\t12\n-496\tval_496\t2008-04-08\t12\n-0\tval_0\t2008-04-08\t12\n-322\tval_322\t2008-04-08\t12\n-197\tval_197\t2008-04-08\t12\n-468\tval_468\t2008-04-08\t12\n-393\tval_393\t2008-04-08\t12\n-454\tval_454\t2008-04-08\t12\n-100\tval_100\t2008-04-08\t12\n-298\tval_298\t2008-04-08\t12\n-199\tval_199\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+229\tval_229\t2008-04-08\t12\n+316\tval_316\t2008-04-08\t12\n+202\tval_202\t2008-04-08\t12\n+432\tval_432\t2008-04-08\t12\n+467\tval_467\t2008-04-08\t12\n+128\tval_128\t2008-04-08\t12\n+438\tval_438\t2008-04-08\t12\n+244\tval_244\t2008-04-08\t12\n+5\tval_5\t2008-04-08\t12\n 191\tval_191\t2008-04-08\t12\n-418\tval_418\t2008-04-08\t12\n-96\tval_96\t2008-04-08\t12\n-26\tval_26\t2008-04-08\t12\n-165\tval_165\t2008-04-08\t12\n-327\tval_327\t2008-04-08\t12\n+288\tval_288\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+480\tval_480\t2008-04-08\t12\n+487\tval_487\t2008-04-08\t12\n+70\tval_70\t2008-04-08\t12\n+263\tval_263\t2008-04-08\t12\n+256\tval_256\t2008-04-08\t12\n+223\tval_223\t2008-04-08\t12\n+116\tval_116\t2008-04-08\t12\n+485\tval_485\t2008-04-08\t12\n+239\tval_239\t2008-04-08\t12\n+219\tval_219\t2008-04-08\t12\n+274\tval_274\t2008-04-08\t12\n+167\tval_167\t2008-04-08\t12\n+344\tval_344\t2008-04-08\t12\n+367\tval_367\t2008-04-08\t12\n+216\tval_216\t2008-04-08\t12\n+113\tval_113\t2008-04-08\t12\n+296\tval_296\t2008-04-08\t12\n+103\tval_103\t2008-04-08\t12\n+368\tval_368\t2008-04-08\t12\n+33\tval_33\t2008-04-08\t12\n 230\tval_230\t2008-04-08\t12\n-205\tval_205\t2008-04-08\t12\n-120\tval_120\t2008-04-08\t12\n-131\tval_131\t2008-04-08\t12\n-51\tval_51\t2008-04-08\t12\n-404\tval_404\t2008-04-08\t12\n-43\tval_43\t2008-04-08\t12\n-436\tval_436\t2008-04-08\t12\n-156\tval_156\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n+69\tval_69\t2008-04-08\t12\n+342\tval_342\t2008-04-08\t12\n+74\tval_74\t2008-04-08\t12\n+76\tval_76\t2008-04-08\t12\n 468\tval_468\t2008-04-08\t12\n-308\tval_308\t2008-04-08\t12\n+64\tval_64\t2008-04-08\t12\n+209\tval_209\t2008-04-08\t12\n+30\tval_30\t2008-04-08\t12\n+453\tval_453\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+228\tval_228\t2008-04-08\t12\n+218\tval_218\t2008-04-08\t12\n+449\tval_449\t2008-04-08\t12\n+149\tval_149\t2008-04-08\t12\n+492\tval_492\t2008-04-08\t12\n+223\tval_223\t2008-04-08\t12\n+41\tval_41\t2008-04-08\t12\n+76\tval_76\t2008-04-08\t12\n+78\tval_78\t2008-04-08\t12\n+458\tval_458\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+119\tval_119\t2008-04-08\t12\n+430\tval_430\t2008-04-08\t12\n+321\tval_321\t2008-04-08\t12\n+42\tval_42\t2008-04-08\t12\n+195\tval_195\t2008-04-08\t12\n+160\tval_160\t2008-04-08\t12\n+498\tval_498\t2008-04-08\t12\n+322\tval_322\t2008-04-08\t12\n+472\tval_472\t2008-04-08\t12\n+143\tval_143\t2008-04-08\t12\n+233\tval_233\t2008-04-08\t12\n+229\tval_229\t2008-04-08\t12\n+34\tval_34\t2008-04-08\t12\n+168\tval_168\t2008-04-08\t12\n+11\tval_11\t2008-04-08\t12\n 95\tval_95\t2008-04-08\t12\n-196\tval_196\t2008-04-08\t12\n-288\tval_288\t2008-04-08\t12\n-481\tval_481\t2008-04-08\t12\n-457\tval_457\t2008-04-08\t12\n-98\tval_98\t2008-04-08\t12\n-282\tval_282\t2008-04-08\t12\n-197\tval_197\t2008-04-08\t12\n-187\tval_187\t2008-04-08\t12\n-318\tval_318\t2008-04-08\t12\n-318\tval_318\t2008-04-08\t12\n-409\tval_409\t2008-04-08\t12\n-470\tval_470\t2008-04-08\t12\n-137\tval_137\t2008-04-08\t12\n-369\tval_369\t2008-04-08\t12\n-316\tval_316\t2008-04-08\t12\n-169\tval_169\t2008-04-08\t12\n-413\tval_413\t2008-04-08\t12\n-85\tval_85\t2008-04-08\t12\n-77\tval_77\t2008-04-08\t12\n-0\tval_0\t2008-04-08\t12\n-490\tval_490\t2008-04-08\t12\n-87\tval_87\t2008-04-08\t12\n-364\tval_364\t2008-04-08\t12\n-179\tval_179\t2008-04-08\t12\n-118\tval_118\t2008-04-08\t12\n-134\tval_134\t2008-04-08\t12\n+336\tval_336\t2008-04-08\t12\n+35\tval_35\t2008-04-08\t12\n+58\tval_58\t2008-04-08\t12\n 395\tval_395\t2008-04-08\t12\n-282\tval_282\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-238\tval_238\t2008-04-08\t12\n-419\tval_419\t2008-04-08\t12\n-15\tval_15\t2008-04-08\t12\n-118\tval_118\t2008-04-08\t12\n-72\tval_72\t2008-04-08\t12\n-90\tval_90\t2008-04-08\t12\n-307\tval_307\t2008-04-08\t12\n-19\tval_19\t2008-04-08\t12\n-435\tval_435\t2008-04-08\t12\n-10\tval_10\t2008-04-08\t12\n-277\tval_277\t2008-04-08\t12\n-273\tval_273\t2008-04-08\t12\n-306\tval_306\t2008-04-08\t12\n-224\tval_224\t2008-04-08\t12\n-309\tval_309\t2008-04-08\t12\n-389\tval_389\t2008-04-08\t12\n-327\tval_327\t2008-04-08\t12\n+317\tval_317\t2008-04-08\t12\n+396\tval_396\t2008-04-08\t12\n+402\tval_402\t2008-04-08\t12\n+497\tval_497\t2008-04-08\t12\n+5\tval_5\t2008-04-08\t12\n+226\tval_226\t2008-04-08\t12\n+177\tval_177\t2008-04-08\t12\n+452\tval_452\t2008-04-08\t12\n 242\tval_242\t2008-04-08\t12\n-369\tval_369\t2008-04-08\t12\n-392\tval_392\t2008-04-08\t12\n-272\tval_272\t2008-04-08\t12\n-331\tval_331\t2008-04-08\t12\n 401\tval_401\t2008-04-08\t12\n+331\tval_331\t2008-04-08\t12\n+272\tval_272\t2008-04-08\t12\n+392\tval_392\t2008-04-08\t12\n+369\tval_369\t2008-04-08\t12\n 242\tval_242\t2008-04-08\t12\n-452\tval_452\t2008-04-08\t12\n-177\tval_177\t2008-04-08\t12\n-226\tval_226\t2008-04-08\t12\n-5\tval_5\t2008-04-08\t12\n-497\tval_497\t2008-04-08\t12\n-402\tval_402\t2008-04-08\t12\n-396\tval_396\t2008-04-08\t12\n-317\tval_317\t2008-04-08\t12\n+327\tval_327\t2008-04-08\t12\n+389\tval_389\t2008-04-08\t12\n+309\tval_309\t2008-04-08\t12\n+224\tval_224\t2008-04-08\t12\n+306\tval_306\t2008-04-08\t12\n+273\tval_273\t2008-04-08\t12\n+277\tval_277\t2008-04-08\t12\n+10\tval_10\t2008-04-08\t12\n+435\tval_435\t2008-04-08\t12\n+19\tval_19\t2008-04-08\t12\n+307\tval_307\t2008-04-08\t12\n+90\tval_90\t2008-04-08\t12\n+72\tval_72\t2008-04-08\t12\n+118\tval_118\t2008-04-08\t12\n+15\tval_15\t2008-04-08\t12\n+419\tval_419\t2008-04-08\t12\n+238\tval_238\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+282\tval_282\t2008-04-08\t12\n 395\tval_395\t2008-04-08\t12\n-58\tval_58\t2008-04-08\t12\n-35\tval_35\t2008-04-08\t12\n-336\tval_336\t2008-04-08\t12\n-95\tval_95\t2008-04-08\t12\n-11\tval_11\t2008-04-08\t12\n-168\tval_168\t2008-04-08\t12\n-34\tval_34\t2008-04-08\t12\n-229\tval_229\t2008-04-08\t12\n-233\tval_233\t2008-04-08\t12\n-143\tval_143\t2008-04-08\t12\n-472\tval_472\t2008-04-08\t12\n-322\tval_322\t2008-04-08\t12\n-498\tval_498\t2008-04-08\t12\n-160\tval_160\t2008-04-08\t12\n-195\tval_195\t2008-04-08\t12\n-42\tval_42\t2008-04-08\t12\n-321\tval_321\t2008-04-08\t12\n-430\tval_430\t2008-04-08\t12\n-119\tval_119\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-458\tval_458\t2008-04-08\t12\n-78\tval_78\t2008-04-08\t12\n-76\tval_76\t2008-04-08\t12\n-41\tval_41\t2008-04-08\t12\n-223\tval_223\t2008-04-08\t12\n-492\tval_492\t2008-04-08\t12\n-149\tval_149\t2008-04-08\t12\n-449\tval_449\t2008-04-08\t12\n-218\tval_218\t2008-04-08\t12\n-228\tval_228\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-453\tval_453\t2008-04-08\t12\n-30\tval_30\t2008-04-08\t12\n-209\tval_209\t2008-04-08\t12\n-64\tval_64\t2008-04-08\t12\n+134\tval_134\t2008-04-08\t12\n+118\tval_118\t2008-04-08\t12\n+179\tval_179\t2008-04-08\t12\n+364\tval_364\t2008-04-08\t12\n+87\tval_87\t2008-04-08\t12\n+490\tval_490\t2008-04-08\t12\n+0\tval_0\t2008-04-08\t12\n+77\tval_77\t2008-04-08\t12\n+85\tval_85\t2008-04-08\t12\n+413\tval_413\t2008-04-08\t12\n+169\tval_169\t2008-04-08\t12\n+316\tval_316\t2008-04-08\t12\n+369\tval_369\t2008-04-08\t12\n+137\tval_137\t2008-04-08\t12\n+470\tval_470\t2008-04-08\t12\n+409\tval_409\t2008-04-08\t12\n+318\tval_318\t2008-04-08\t12\n+318\tval_318\t2008-04-08\t12\n+187\tval_187\t2008-04-08\t12\n+197\tval_197\t2008-04-08\t12\n+282\tval_282\t2008-04-08\t12\n+98\tval_98\t2008-04-08\t12\n+457\tval_457\t2008-04-08\t12\n+481\tval_481\t2008-04-08\t12\n+288\tval_288\t2008-04-08\t12\n+196\tval_196\t2008-04-08\t12\n+95\tval_95\t2008-04-08\t12\n+308\tval_308\t2008-04-08\t12\n 468\tval_468\t2008-04-08\t12\n-76\tval_76\t2008-04-08\t12\n-74\tval_74\t2008-04-08\t12\n-342\tval_342\t2008-04-08\t12\n-69\tval_69\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+156\tval_156\t2008-04-08\t12\n+436\tval_436\t2008-04-08\t12\n+43\tval_43\t2008-04-08\t12\n+404\tval_404\t2008-04-08\t12\n+51\tval_51\t2008-04-08\t12\n+131\tval_131\t2008-04-08\t12\n+120\tval_120\t2008-04-08\t12\n+205\tval_205\t2008-04-08\t12\n 230\tval_230\t2008-04-08\t12\n-33\tval_33\t2008-04-08\t12\n-368\tval_368\t2008-04-08\t12\n-103\tval_103\t2008-04-08\t12\n-296\tval_296\t2008-04-08\t12\n-113\tval_113\t2008-04-08\t12\n-216\tval_216\t2008-04-08\t12\n-367\tval_367\t2008-04-08\t12\n-344\tval_344\t2008-04-08\t12\n-167\tval_167\t2008-04-08\t12\n-274\tval_274\t2008-04-08\t12\n-219\tval_219\t2008-04-08\t12\n-239\tval_239\t2008-04-08\t12\n-485\tval_485\t2008-04-08\t12\n-116\tval_116\t2008-04-08\t12\n-223\tval_223\t2008-04-08\t12\n-256\tval_256\t2008-04-08\t12\n-263\tval_263\t2008-04-08\t12\n-70\tval_70\t2008-04-08\t12\n-487\tval_487\t2008-04-08\t12\n-480\tval_480\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-288\tval_288\t2008-04-08\t12\n+327\tval_327\t2008-04-08\t12\n+165\tval_165\t2008-04-08\t12\n+26\tval_26\t2008-04-08\t12\n+96\tval_96\t2008-04-08\t12\n+418\tval_418\t2008-04-08\t12\n 191\tval_191\t2008-04-08\t12\n-5\tval_5\t2008-04-08\t12\n-244\tval_244\t2008-04-08\t12\n-438\tval_438\t2008-04-08\t12\n-128\tval_128\t2008-04-08\t12\n-467\tval_467\t2008-04-08\t12\n-432\tval_432\t2008-04-08\t12\n-202\tval_202\t2008-04-08\t12\n-316\tval_316\t2008-04-08\t12\n-229\tval_229\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n+199\tval_199\t2008-04-08\t12\n+298\tval_298\t2008-04-08\t12\n+100\tval_100\t2008-04-08\t12\n+454\tval_454\t2008-04-08\t12\n+393\tval_393\t2008-04-08\t12\n+468\tval_468\t2008-04-08\t12\n+197\tval_197\t2008-04-08\t12\n+322\tval_322\t2008-04-08\t12\n+0\tval_0\t2008-04-08\t12\n+496\tval_496\t2008-04-08\t12\n+119\tval_119\t2008-04-08\t12\n+158\tval_158\t2008-04-08\t12\n+129\tval_129\t2008-04-08\t12\n+42\tval_42\t2008-04-08\t12\n+172\tval_172\t2008-04-08\t12\n+179\tval_179\t2008-04-08\t12\n+431\tval_431\t2008-04-08\t12\n 463\tval_463\t2008-04-08\t12\n-280\tval_280\t2008-04-08\t12\n-2\tval_2\t2008-04-08\t12\n-35\tval_35\t2008-04-08\t12\n-283\tval_283\t2008-04-08\t12\n-331\tval_331\t2008-04-08\t12\n-235\tval_235\t2008-04-08\t12\n-80\tval_80\t2008-04-08\t12\n-44\tval_44\t2008-04-08\t12\n-193\tval_193\t2008-04-08\t12\n-321\tval_321\t2008-04-08\t12\n-335\tval_335\t2008-04-08\t12\n-104\tval_104\t2008-04-08\t12\n+24\tval_24\t2008-04-08\t12\n+348\tval_348\t2008-04-08\t12\n+208\tval_208\t2008-04-08\t12\n+230\tval_230\t2008-04-08\t12\n+411\tval_411\t2008-04-08\t12\n+8\tval_8\t2008-04-08\t12\n+58\tval_58\t2008-04-08\t12\n 466\tval_466\t2008-04-08\t12\n-366\tval_366\t2008-04-08\t12\n-175\tval_175\t2008-04-08\t12\n-403\tval_403\t2008-04-08\t12\n-483\tval_483\t2008-04-08\t12\n-53\tval_53\t2008-04-08\t12\n-105\tval_105\t2008-04-08\t12\n-257\tval_257\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-409\tval_409\t2008-04-08\t12\n-190\tval_190\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-114\tval_114\t2008-04-08\t12\n-258\tval_258\t2008-04-08\t12\n-90\tval_90\t2008-04-08\t12\n-203\tval_203\t2008-04-08\t12\n-262\tval_262\t2008-04-08\t12\n 348\tval_348\t2008-04-08\t12\n-424\tval_424\t2008-04-08\t12\n-12\tval_12\t2008-04-08\t12\n-396\tval_396\t2008-04-08\t12\n-201\tval_201\t2008-04-08\t12\n+84\tval_84\t2008-04-08\t12\n 217\tval_217\t2008-04-08\t12\n-164\tval_164\t2008-04-08\t12\n-431\tval_431\t2008-04-08\t12\n-454\tval_454\t2008-04-08\t12\n-478\tval_478\t2008-04-08\t12\n-298\tval_298\t2008-04-08\t12\n-125\tval_125\t2008-04-08\t12\n-431\tval_431\t2008-04-08\t12\n-164\tval_164\t2008-04-08\t12\n-424\tval_424\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+272\tval_272\t2008-04-08\t12\n+373\tval_373\t2008-04-08\t12\n+353\tval_353\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+384\tval_384\t2008-04-08\t12\n+404\tval_404\t2008-04-08\t12\n+260\tval_260\t2008-04-08\t12\n+67\tval_67\t2008-04-08\t12\n+181\tval_181\t2008-04-08\t12\n+230\tval_230\t2008-04-08\t12\n+12\tval_12\t2008-04-08\t12\n+284\tval_284\t2008-04-08\t12\n+180\tval_180\t2008-04-08\t12\n+333\tval_333\t2008-04-08\t12\n+83\tval_83\t2008-04-08\t12\n+241\tval_241\t2008-04-08\t12\n+137\tval_137\t2008-04-08\t12\n+275\tval_275\t2008-04-08\t12\n+311\tval_311\t2008-04-08\t12\n+332\tval_332\t2008-04-08\t12\n+318\tval_318\t2008-04-08\t12\n+65\tval_65\t2008-04-08\t12\n+221\tval_221\t2008-04-08\t12\n+289\tval_289\t2008-04-08\t12\n+176\tval_176\t2008-04-08\t12\n+278\tval_278\t2008-04-08\t12\n+430\tval_430\t2008-04-08\t12\n+216\tval_216\t2008-04-08\t12\n+213\tval_213\t2008-04-08\t12\n+239\tval_239\t2008-04-08\t12\n+103\tval_103\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+51\tval_51\t2008-04-08\t12\n+459\tval_459\t2008-04-08\t12\n+54\tval_54\t2008-04-08\t12\n+176\tval_176\t2008-04-08\t12\n 187\tval_187\t2008-04-08\t12\n+286\tval_286\t2008-04-08\t12\n+192\tval_192\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+437\tval_437\t2008-04-08\t12\n+386\tval_386\t2008-04-08\t12\n+125\tval_125\t2008-04-08\t12\n+498\tval_498\t2008-04-08\t12\n 382\tval_382\t2008-04-08\t12\n-5\tval_5\t2008-04-08\t12\n-70\tval_70\t2008-04-08\t12\n-397\tval_397\t2008-04-08\t12\n-480\tval_480\t2008-04-08\t12\n-291\tval_291\t2008-04-08\t12\n-24\tval_24\t2008-04-08\t12\n-351\tval_351\t2008-04-08\t12\n-255\tval_255\t2008-04-08\t12\n-104\tval_104\t2008-04-08\t12\n-70\tval_70\t2008-04-08\t12\n-163\tval_163\t2008-04-08\t12\n-438\tval_438\t2008-04-08\t12\n-119\tval_119\t2008-04-08\t12\n-414\tval_414\t2008-04-08\t12\n-200\tval_200\t2008-04-08\t12\n-491\tval_491\t2008-04-08\t12\n-237\tval_237\t2008-04-08\t12\n-439\tval_439\t2008-04-08\t12\n-360\tval_360\t2008-04-08\t12\n-248\tval_248\t2008-04-08\t12\n-479\tval_479\t2008-04-08\t12\n-305\tval_305\t2008-04-08\t12\n-417\tval_417\t2008-04-08\t12\n-199\tval_199\t2008-04-08\t12\n-444\tval_444\t2008-04-08\t12\n-120\tval_120\t2008-04-08\t12\n-429\tval_429\t2008-04-08\t12\n 169\tval_169\t2008-04-08\t12\n-443\tval_443\t2008-04-08\t12\n-323\tval_323\t2008-04-08\t12\n-325\tval_325\t2008-04-08\t12\n+399\tval_399\t2008-04-08\t12\n+356\tval_356\t2008-04-08\t12\n+208\tval_208\t2008-04-08\t12\n 277\tval_277\t2008-04-08\t12\n-230\tval_230\t2008-04-08\t12\n-478\tval_478\t2008-04-08\t12\n-178\tval_178\t2008-04-08\t12\n-468\tval_468\t2008-04-08\t12\n-310\tval_310\t2008-04-08\t12\n-317\tval_317\t2008-04-08\t12\n-333\tval_333\t2008-04-08\t12\n-493\tval_493\t2008-04-08\t12\n-460\tval_460\t2008-04-08\t12\n-207\tval_207\t2008-04-08\t12\n-249\tval_249\t2008-04-08\t12\n-265\tval_265\t2008-04-08\t12\n-480\tval_480\t2008-04-08\t12\n-83\tval_83\t2008-04-08\t12\n-136\tval_136\t2008-04-08\t12\n-353\tval_353\t2008-04-08\t12\n-172\tval_172\t2008-04-08\t12\n-214\tval_214\t2008-04-08\t12\n-462\tval_462\t2008-04-08\t12\n-233\tval_233\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-133\tval_133\t2008-04-08\t12\n-175\tval_175\t2008-04-08\t12\n-189\tval_189\t2008-04-08\t12\n-454\tval_454\t2008-04-08\t12\n-375\tval_375\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-421\tval_421\t2008-04-08\t12\n-407\tval_407\t2008-04-08\t12\n-384\tval_384\t2008-04-08\t12\n-256\tval_256\t2008-04-08\t12\n-26\tval_26\t2008-04-08\t12\n-134\tval_134\t2008-04-08\t12\n-67\tval_67\t2008-04-08\t12\n-384\tval_384\t2008-04-08\t12\n-379\tval_379\t2008-04-08\t12\n-18\tval_18\t2008-04-08\t12\n-462\tval_462\t2008-04-08\t12\n-492\tval_492\t2008-04-08\t12\n-100\tval_100\t2008-04-08\t12\n-298\tval_298\t2008-04-08\t12\n-9\tval_9\t2008-04-08\t12\n-341\tval_341\t2008-04-08\t12\n-498\tval_498\t2008-04-08\t12\n-146\tval_146\t2008-04-08\t12\n-458\tval_458\t2008-04-08\t12\n-362\tval_362\t2008-04-08\t12\n-186\tval_186\t2008-04-08\t12\n-285\tval_285\t2008-04-08\t12\n-348\tval_348\t2008-04-08\t12\n+427\tval_427\t2008-04-08\t12\n+35\tval_35\t2008-04-08\t12\n+280\tval_280\t2008-04-08\t12\n+4\tval_4\t2008-04-08\t12\n+72\tval_72\t2008-04-08\t12\n+47\tval_47\t2008-04-08\t12\n+111\tval_111\t2008-04-08\t12\n+92\tval_92\t2008-04-08\t12\n+221\tval_221\t2008-04-08\t12\n+378\tval_378\t2008-04-08\t12\n+157\tval_157\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+20\tval_20\t2008-04-08\t12\n+170\tval_170\t2008-04-08\t12\n+129\tval_129\t2008-04-08\t12\n+345\tval_345\t2008-04-08\t12\n+438\tval_438\t2008-04-08\t12\n+149\tval_149\t2008-04-08\t12\n+205\tval_205\t2008-04-08\t12\n+302\tval_302\t2008-04-08\t12\n+57\tval_57\t2008-04-08\t12\n+316\tval_316\t2008-04-08\t12\n+311\tval_311\t2008-04-08\t12\n+128\tval_128\t2008-04-08\t12\n+455\tval_455\t2008-04-08\t12\n+0\tval_0\t2008-04-08\t12\n+339\tval_339\t2008-04-08\t12\n+203\tval_203\t2008-04-08\t12\n+155\tval_155\t2008-04-08\t12\n+113\tval_113\t2008-04-08\t12\n+17\tval_17\t2008-04-08\t12\n+475\tval_475\t2008-04-08\t12\n+195\tval_195\t2008-04-08\t12\n 167\tval_167\t2008-04-08\t12\n-18\tval_18\t2008-04-08\t12\n-273\tval_273\t2008-04-08\t12\n-183\tval_183\t2008-04-08\t12\n+325\tval_325\t2008-04-08\t12\n+367\tval_367\t2008-04-08\t12\n+342\tval_342\t2008-04-08\t12\n+439\tval_439\t2008-04-08\t12\n+266\tval_266\t2008-04-08\t12\n+365\tval_365\t2008-04-08\t12\n+309\tval_309\t2008-04-08\t12\n+397\tval_397\t2008-04-08\t12\n+377\tval_377\t2008-04-08\t12\n+162\tval_162\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+417\tval_417\t2008-04-08\t12\n+247\tval_247\t2008-04-08\t12\n+396\tval_396\t2008-04-08\t12\n+399\tval_399\t2008-04-08\t12\n+174\tval_174\t2008-04-08\t12\n+208\tval_208\t2008-04-08\t12\n+466\tval_466\t2008-04-08\t12\n+199\tval_199\t2008-04-08\t12\n+207\tval_207\t2008-04-08\t12\n+494\tval_494\t2008-04-08\t12\n+413\tval_413\t2008-04-08\t12\n+174\tval_174\t2008-04-08\t12\n+482\tval_482\t2008-04-08\t12\n+237\tval_237\t2008-04-08\t12\n+394\tval_394\t2008-04-08\t12\n+459\tval_459\t2008-04-08\t12\n+446\tval_446\t2008-04-08\t12\n+338\tval_338\t2008-04-08\t12\n+193\tval_193\t2008-04-08\t12\n+153\tval_153\t2008-04-08\t12\n+287\tval_287\t2008-04-08\t12\n+219\tval_219\t2008-04-08\t12\n+292\tval_292\t2008-04-08\t12\n+252\tval_252\t2008-04-08\t12\n+430\tval_430\t2008-04-08\t12\n+417\tval_417\t2008-04-08\t12\n+166\tval_166\t2008-04-08\t12\n+403\tval_403\t2008-04-08\t12\n+82\tval_82\t2008-04-08\t12\n+15\tval_15\t2008-04-08\t12\n+209\tval_209\t2008-04-08\t12\n+277\tval_277\t2008-04-08\t12\n 281\tval_281\t2008-04-08\t12\n-344\tval_344\t2008-04-08\t12\n-97\tval_97\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n-315\tval_315\t2008-04-08\t12\n-84\tval_84\t2008-04-08\t12\n-28\tval_28\t2008-04-08\t12\n+327\tval_327\t2008-04-08\t12\n 37\tval_37\t2008-04-08\t12\n-448\tval_448\t2008-04-08\t12\n+495\tval_495\t2008-04-08\t12\n+145\tval_145\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n 152\tval_152\t2008-04-08\t12\n-348\tval_348\t2008-04-08\t12\n-307\tval_307\t2008-04-08\t12\n-194\tval_194\t2008-04-08\t12\n-414\tval_414\t2008-04-08\t12\n-477\tval_477\t2008-04-08\t12\n-222\tval_222\t2008-04-08\t12\n-126\tval_126\t2008-04-08\t12\n-90\tval_90\t2008-04-08\t12\n-169\tval_169\t2008-04-08\t12\n-403\tval_403\t2008-04-08\t12\n-400\tval_400\t2008-04-08\t12\n-200\tval_200\t2008-04-08\t12\n-97\tval_97\t2008-04-08\t12\n+374\tval_374\t2008-04-08\t12\n+429\tval_429\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+146\tval_146\t2008-04-08\t12\n+213\tval_213\t2008-04-08\t12\n+128\tval_128\t2008-04-08\t12\n+66\tval_66\t2008-04-08\t12\n+369\tval_369\t2008-04-08\t12\n+224\tval_224\t2008-04-08\t12\n+273\tval_273\t2008-04-08\t12\n+150\tval_150\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+193\tval_193\t2008-04-08\t12\n+265\tval_265\t2008-04-08\t12\n+484\tval_484\t2008-04-08\t12\n+98\tval_98\t2008-04-08\t12\n+278\tval_278\t2008-04-08\t12\n+255\tval_255\t2008-04-08\t12\n+409\tval_409\t2008-04-08\t12\n+165\tval_165\t2008-04-08\t12\n+27\tval_27\t2008-04-08\t12\n+311\tval_311\t2008-04-08\t12\n+86\tval_86\t2008-04-08\t12\n+238\tval_238\t2008-04-08\t12\n 238\tval_238\t2008-04-09\t11\n-86\tval_86\t2008-04-09\t11\n-311\tval_311\t2008-04-09\t11\n-27\tval_27\t2008-04-09\t11\n-165\tval_165\t2008-04-09\t11\n-409\tval_409\t2008-04-09\t11\n-255\tval_255\t2008-04-09\t11\n-278\tval_278\t2008-04-09\t11\n-98\tval_98\t2008-04-09\t11\n-484\tval_484\t2008-04-09\t11\n-265\tval_265\t2008-04-09\t11\n-193\tval_193\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-150\tval_150\t2008-04-09\t11\n-273\tval_273\t2008-04-09\t11\n-224\tval_224\t2008-04-09\t11\n-369\tval_369\t2008-04-09\t11\n-66\tval_66\t2008-04-09\t11\n-128\tval_128\t2008-04-09\t11\n-213\tval_213\t2008-04-09\t11\n-146\tval_146\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-429\tval_429\t2008-04-09\t11\n-374\tval_374\t2008-04-09\t11\n+97\tval_97\t2008-04-09\t11\n+200\tval_200\t2008-04-09\t11\n+400\tval_400\t2008-04-09\t11\n+403\tval_403\t2008-04-09\t11\n+169\tval_169\t2008-04-09\t11\n+90\tval_90\t2008-04-09\t11\n+126\tval_126\t2008-04-09\t11\n+222\tval_222\t2008-04-09\t11\n+477\tval_477\t2008-04-09\t11\n+414\tval_414\t2008-04-09\t11\n+194\tval_194\t2008-04-09\t11\n+307\tval_307\t2008-04-09\t11\n+348\tval_348\t2008-04-09\t11\n 152\tval_152\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n-145\tval_145\t2008-04-09\t11\n-495\tval_495\t2008-04-09\t11\n+448\tval_448\t2008-04-09\t11\n 37\tval_37\t2008-04-09\t11\n-327\tval_327\t2008-04-09\t11\n+28\tval_28\t2008-04-09\t11\n+84\tval_84\t2008-04-09\t11\n+315\tval_315\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+97\tval_97\t2008-04-09\t11\n+344\tval_344\t2008-04-09\t11\n 281\tval_281\t2008-04-09\t11\n-277\tval_277\t2008-04-09\t11\n-209\tval_209\t2008-04-09\t11\n-15\tval_15\t2008-04-09\t11\n-82\tval_82\t2008-04-09\t11\n-403\tval_403\t2008-04-09\t11\n-166\tval_166\t2008-04-09\t11\n-417\tval_417\t2008-04-09\t11\n-430\tval_430\t2008-04-09\t11\n-252\tval_252\t2008-04-09\t11\n-292\tval_292\t2008-04-09\t11\n-219\tval_219\t2008-04-09\t11\n-287\tval_287\t2008-04-09\t11\n-153\tval_153\t2008-04-09\t11\n-193\tval_193\t2008-04-09\t11\n-338\tval_338\t2008-04-09\t11\n-446\tval_446\t2008-04-09\t11\n-459\tval_459\t2008-04-09\t11\n-394\tval_394\t2008-04-09\t11\n-237\tval_237\t2008-04-09\t11\n-482\tval_482\t2008-04-09\t11\n-174\tval_174\t2008-04-09\t11\n-413\tval_413\t2008-04-09\t11\n-494\tval_494\t2008-04-09\t11\n-207\tval_207\t2008-04-09\t11\n-199\tval_199\t2008-04-09\t11\n-466\tval_466\t2008-04-09\t11\n-208\tval_208\t2008-04-09\t11\n-174\tval_174\t2008-04-09\t11\n-399\tval_399\t2008-04-09\t11\n-396\tval_396\t2008-04-09\t11\n-247\tval_247\t2008-04-09\t11\n-417\tval_417\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-162\tval_162\t2008-04-09\t11\n-377\tval_377\t2008-04-09\t11\n-397\tval_397\t2008-04-09\t11\n-309\tval_309\t2008-04-09\t11\n-365\tval_365\t2008-04-09\t11\n-266\tval_266\t2008-04-09\t11\n-439\tval_439\t2008-04-09\t11\n-342\tval_342\t2008-04-09\t11\n-367\tval_367\t2008-04-09\t11\n-325\tval_325\t2008-04-09\t11\n+183\tval_183\t2008-04-09\t11\n+273\tval_273\t2008-04-09\t11\n+18\tval_18\t2008-04-09\t11\n 167\tval_167\t2008-04-09\t11\n-195\tval_195\t2008-04-09\t11\n-475\tval_475\t2008-04-09\t11\n-17\tval_17\t2008-04-09\t11\n-113\tval_113\t2008-04-09\t11\n-155\tval_155\t2008-04-09\t11\n-203\tval_203\t2008-04-09\t11\n-339\tval_339\t2008-04-09\t11\n-0\tval_0\t2008-04-09\t11\n-455\tval_455\t2008-04-09\t11\n-128\tval_128\t2008-04-09\t11\n-311\tval_311\t2008-04-09\t11\n-316\tval_316\t2008-04-09\t11\n-57\tval_57\t2008-04-09\t11\n-302\tval_302\t2008-04-09\t11\n-205\tval_205\t2008-04-09\t11\n-149\tval_149\t2008-04-09\t11\n-438\tval_438\t2008-04-09\t11\n-345\tval_345\t2008-04-09\t11\n-129\tval_129\t2008-04-09\t11\n-170\tval_170\t2008-04-09\t11\n-20\tval_20\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-157\tval_157\t2008-04-09\t11\n-378\tval_378\t2008-04-09\t11\n-221\tval_221\t2008-04-09\t11\n-92\tval_92\t2008-04-09\t11\n-111\tval_111\t2008-04-09\t11\n-47\tval_47\t2008-04-09\t11\n-72\tval_72\t2008-04-09\t11\n-4\tval_4\t2008-04-09\t11\n-280\tval_280\t2008-04-09\t11\n-35\tval_35\t2008-04-09\t11\n-427\tval_427\t2008-04-09\t11\n+348\tval_348\t2008-04-09\t11\n+285\tval_285\t2008-04-09\t11\n+186\tval_186\t2008-04-09\t11\n+362\tval_362\t2008-04-09\t11\n+458\tval_458\t2008-04-09\t11\n+146\tval_146\t2008-04-09\t11\n+498\tval_498\t2008-04-09\t11\n+341\tval_341\t2008-04-09\t11\n+9\tval_9\t2008-04-09\t11\n+298\tval_298\t2008-04-09\t11\n+100\tval_100\t2008-04-09\t11\n+492\tval_492\t2008-04-09\t11\n+462\tval_462\t2008-04-09\t11\n+18\tval_18\t2008-04-09\t11\n+379\tval_379\t2008-04-09\t11\n+384\tval_384\t2008-04-09\t11\n+67\tval_67\t2008-04-09\t11\n+134\tval_134\t2008-04-09\t11\n+26\tval_26\t2008-04-09\t11\n+256\tval_256\t2008-04-09\t11\n+384\tval_384\t2008-04-09\t11\n+407\tval_407\t2008-04-09\t11\n+421\tval_421\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+375\tval_375\t2008-04-09\t11\n+454\tval_454\t2008-04-09\t11\n+189\tval_189\t2008-04-09\t11\n+175\tval_175\t2008-04-09\t11\n+133\tval_133\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+233\tval_233\t2008-04-09\t11\n+462\tval_462\t2008-04-09\t11\n+214\tval_214\t2008-04-09\t11\n+172\tval_172\t2008-04-09\t11\n+353\tval_353\t2008-04-09\t11\n+136\tval_136\t2008-04-09\t11\n+83\tval_83\t2008-04-09\t11\n+480\tval_480\t2008-04-09\t11\n+265\tval_265\t2008-04-09\t11\n+249\tval_249\t2008-04-09\t11\n+207\tval_207\t2008-04-09\t11\n+460\tval_460\t2008-04-09\t11\n+493\tval_493\t2008-04-09\t11\n+333\tval_333\t2008-04-09\t11\n+317\tval_317\t2008-04-09\t11\n+310\tval_310\t2008-04-09\t11\n+468\tval_468\t2008-04-09\t11\n+178\tval_178\t2008-04-09\t11\n+478\tval_478\t2008-04-09\t11\n+230\tval_230\t2008-04-09\t11\n 277\tval_277\t2008-04-09\t11\n-208\tval_208\t2008-04-09\t11\n-356\tval_356\t2008-04-09\t11\n-399\tval_399\t2008-04-09\t11\n+325\tval_325\t2008-04-09\t11\n+323\tval_323\t2008-04-09\t11\n+443\tval_443\t2008-04-09\t11\n 169\tval_169\t2008-04-09\t11\n+429\tval_429\t2008-04-09\t11\n+120\tval_120\t2008-04-09\t11\n+444\tval_444\t2008-04-09\t11\n+199\tval_199\t2008-04-09\t11\n+417\tval_417\t2008-04-09\t11\n+305\tval_305\t2008-04-09\t11\n+479\tval_479\t2008-04-09\t11\n+248\tval_248\t2008-04-09\t11\n+360\tval_360\t2008-04-09\t11\n+439\tval_439\t2008-04-09\t11\n+237\tval_237\t2008-04-09\t11\n+491\tval_491\t2008-04-09\t11\n+200\tval_200\t2008-04-09\t11\n+414\tval_414\t2008-04-09\t11\n+119\tval_119\t2008-04-09\t11\n+438\tval_438\t2008-04-09\t11\n+163\tval_163\t2008-04-09\t11\n+70\tval_70\t2008-04-09\t11\n+104\tval_104\t2008-04-09\t11\n+255\tval_255\t2008-04-09\t11\n+351\tval_351\t2008-04-09\t11\n+24\tval_24\t2008-04-09\t11\n+291\tval_291\t2008-04-09\t11\n+480\tval_480\t2008-04-09\t11\n+397\tval_397\t2008-04-09\t11\n+70\tval_70\t2008-04-09\t11\n+5\tval_5\t2008-04-09\t11\n 382\tval_382\t2008-04-09\t11\n-498\tval_498\t2008-04-09\t11\n-125\tval_125\t2008-04-09\t11\n-386\tval_386\t2008-04-09\t11\n-437\tval_437\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n-192\tval_192\t2008-04-09\t11\n-286\tval_286\t2008-04-09\t11\n 187\tval_187\t2008-04-09\t11\n-176\tval_176\t2008-04-09\t11\n-54\tval_54\t2008-04-09\t11\n-459\tval_459\t2008-04-09\t11\n-51\tval_51\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n-103\tval_103\t2008-04-09\t11\n-239\tval_239\t2008-04-09\t11\n-213\tval_213\t2008-04-09\t11\n-216\tval_216\t2008-04-09\t11\n-430\tval_430\t2008-04-09\t11\n-278\tval_278\t2008-04-09\t11\n-176\tval_176\t2008-04-09\t11\n-289\tval_289\t2008-04-09\t11\n-221\tval_221\t2008-04-09\t11\n-65\tval_65\t2008-04-09\t11\n-318\tval_318\t2008-04-09\t11\n-332\tval_332\t2008-04-09\t11\n-311\tval_311\t2008-04-09\t11\n-275\tval_275\t2008-04-09\t11\n-137\tval_137\t2008-04-09\t11\n-241\tval_241\t2008-04-09\t11\n-83\tval_83\t2008-04-09\t11\n-333\tval_333\t2008-04-09\t11\n-180\tval_180\t2008-04-09\t11\n-284\tval_284\t2008-04-09\t11\n-12\tval_12\t2008-04-09\t11\n-230\tval_230\t2008-04-09\t11\n-181\tval_181\t2008-04-09\t11\n-67\tval_67\t2008-04-09\t11\n-260\tval_260\t2008-04-09\t11\n-404\tval_404\t2008-04-09\t11\n-384\tval_384\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-353\tval_353\t2008-04-09\t11\n-373\tval_373\t2008-04-09\t11\n-272\tval_272\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n+424\tval_424\t2008-04-09\t11\n+164\tval_164\t2008-04-09\t11\n+431\tval_431\t2008-04-09\t11\n+125\tval_125\t2008-04-09\t11\n+298\tval_298\t2008-04-09\t11\n+478\tval_478\t2008-04-09\t11\n+454\tval_454\t2008-04-09\t11\n+431\tval_431\t2008-04-09\t11\n+164\tval_164\t2008-04-09\t11\n 217\tval_217\t2008-04-09\t11\n-84\tval_84\t2008-04-09\t11\n+201\tval_201\t2008-04-09\t11\n+396\tval_396\t2008-04-09\t11\n+12\tval_12\t2008-04-09\t11\n+424\tval_424\t2008-04-09\t11\n 348\tval_348\t2008-04-09\t11\n+262\tval_262\t2008-04-09\t11\n+203\tval_203\t2008-04-09\t11\n+90\tval_90\t2008-04-09\t11\n+258\tval_258\t2008-04-09\t11\n+114\tval_114\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+190\tval_190\t2008-04-09\t11\n+409\tval_409\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+257\tval_257\t2008-04-09\t11\n+105\tval_105\t2008-04-09\t11\n+53\tval_53\t2008-04-09\t11\n+483\tval_483\t2008-04-09\t11\n+403\tval_403\t2008-04-09\t11\n+175\tval_175\t2008-04-09\t11\n+366\tval_366\t2008-04-09\t11\n 466\tval_466\t2008-04-09\t11\n-58\tval_58\t2008-04-09\t11\n-8\tval_8\t2008-04-09\t11\n-411\tval_411\t2008-04-09\t11\n-230\tval_230\t2008-04-09\t11\n-208\tval_208\t2008-04-09\t11\n-348\tval_348\t2008-04-09\t11\n-24\tval_24\t2008-04-09\t11\n+104\tval_104\t2008-04-09\t11\n+335\tval_335\t2008-04-09\t11\n+321\tval_321\t2008-04-09\t11\n+193\tval_193\t2008-04-09\t11\n+44\tval_44\t2008-04-09\t11\n+80\tval_80\t2008-04-09\t11\n+235\tval_235\t2008-04-09\t11\n+331\tval_331\t2008-04-09\t11\n+283\tval_283\t2008-04-09\t11\n+35\tval_35\t2008-04-09\t11\n+2\tval_2\t2008-04-09\t11\n+280\tval_280\t2008-04-09\t11\n 463\tval_463\t2008-04-09\t11\n-431\tval_431\t2008-04-09\t11\n-179\tval_179\t2008-04-09\t11\n-172\tval_172\t2008-04-09\t11\n-42\tval_42\t2008-04-09\t11\n-129\tval_129\t2008-04-09\t11\n-158\tval_158\t2008-04-09\t11\n-119\tval_119\t2008-04-09\t11\n-496\tval_496\t2008-04-09\t11\n-0\tval_0\t2008-04-09\t11\n-322\tval_322\t2008-04-09\t11\n-197\tval_197\t2008-04-09\t11\n-468\tval_468\t2008-04-09\t11\n-393\tval_393\t2008-04-09\t11\n-454\tval_454\t2008-04-09\t11\n-100\tval_100\t2008-04-09\t11\n-298\tval_298\t2008-04-09\t11\n-199\tval_199\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+229\tval_229\t2008-04-09\t11\n+316\tval_316\t2008-04-09\t11\n+202\tval_202\t2008-04-09\t11\n+432\tval_432\t2008-04-09\t11\n+467\tval_467\t2008-04-09\t11\n+128\tval_128\t2008-04-09\t11\n+438\tval_438\t2008-04-09\t11\n+244\tval_244\t2008-04-09\t11\n+5\tval_5\t2008-04-09\t11\n 191\tval_191\t2008-04-09\t11\n-418\tval_418\t2008-04-09\t11\n-96\tval_96\t2008-04-09\t11\n-26\tval_26\t2008-04-09\t11\n-165\tval_165\t2008-04-09\t11\n-327\tval_327\t2008-04-09\t11\n+288\tval_288\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+480\tval_480\t2008-04-09\t11\n+487\tval_487\t2008-04-09\t11\n+70\tval_70\t2008-04-09\t11\n+263\tval_263\t2008-04-09\t11\n+256\tval_256\t2008-04-09\t11\n+223\tval_223\t2008-04-09\t11\n+116\tval_116\t2008-04-09\t11\n+485\tval_485\t2008-04-09\t11\n+239\tval_239\t2008-04-09\t11\n+219\tval_219\t2008-04-09\t11\n+274\tval_274\t2008-04-09\t11\n+167\tval_167\t2008-04-09\t11\n+344\tval_344\t2008-04-09\t11\n+367\tval_367\t2008-04-09\t11\n+216\tval_216\t2008-04-09\t11\n+113\tval_113\t2008-04-09\t11\n+296\tval_296\t2008-04-09\t11\n+103\tval_103\t2008-04-09\t11\n+368\tval_368\t2008-04-09\t11\n+33\tval_33\t2008-04-09\t11\n 230\tval_230\t2008-04-09\t11\n-205\tval_205\t2008-04-09\t11\n-120\tval_120\t2008-04-09\t11\n-131\tval_131\t2008-04-09\t11\n-51\tval_51\t2008-04-09\t11\n-404\tval_404\t2008-04-09\t11\n-43\tval_43\t2008-04-09\t11\n-436\tval_436\t2008-04-09\t11\n-156\tval_156\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n+69\tval_69\t2008-04-09\t11\n+342\tval_342\t2008-04-09\t11\n+74\tval_74\t2008-04-09\t11\n+76\tval_76\t2008-04-09\t11\n 468\tval_468\t2008-04-09\t11\n-308\tval_308\t2008-04-09\t11\n-95\tval_95\t2008-04-09\t11\n-196\tval_196\t2008-04-09\t11\n-288\tval_288\t2008-04-09\t11\n-481\tval_481\t2008-04-09\t11\n-457\tval_457\t2008-04-09\t11\n-98\tval_98\t2008-04-09\t11\n-282\tval_282\t2008-04-09\t11\n-197\tval_197\t2008-04-09\t11\n-187\tval_187\t2008-04-09\t11\n-318\tval_318\t2008-04-09\t11\n-318\tval_318\t2008-04-09\t11\n-409\tval_409\t2008-04-09\t11\n-470\tval_470\t2008-04-09\t11\n-137\tval_137\t2008-04-09\t11\n-369\tval_369\t2008-04-09\t11\n-316\tval_316\t2008-04-09\t11\n-169\tval_169\t2008-04-09\t11\n-413\tval_413\t2008-04-09\t11\n-85\tval_85\t2008-04-09\t11\n-77\tval_77\t2008-04-09\t11\n-0\tval_0\t2008-04-09\t11\n-490\tval_490\t2008-04-09\t11\n-87\tval_87\t2008-04-09\t11\n-364\tval_364\t2008-04-09\t11\n-179\tval_179\t2008-04-09\t11\n-118\tval_118\t2008-04-09\t11\n-134\tval_134\t2008-04-09\t11\n+64\tval_64\t2008-04-09\t11\n+209\tval_209\t2008-04-09\t11\n+30\tval_30\t2008-04-09\t11\n+453\tval_453\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+228\tval_228\t2008-04-09\t11\n+218\tval_218\t2008-04-09\t11\n+449\tval_449\t2008-04-09\t11\n+149\tval_149\t2008-04-09\t11\n+492\tval_492\t2008-04-09\t11\n+223\tval_223\t2008-04-09\t11\n+41\tval_41\t2008-04-09\t11\n+76\tval_76\t2008-04-09\t11\n+78\tval_78\t2008-04-09\t11\n+458\tval_458\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n+119\tval_119\t2008-04-09\t11\n+430\tval_430\t2008-04-09\t11\n+321\tval_321\t2008-04-09\t11\n+42\tval_42\t2008-04-09\t11\n+195\tval_195\t2008-04-09\t11\n+160\tval_160\t2008-04-09\t11\n+498\tval_498\t2008-04-09\t11\n+322\tval_322\t2008-04-09\t11\n+472\tval_472\t2008-04-09\t11\n+143\tval_143\t2008-04-09\t11\n+233\tval_233\t2008-04-09\t11\n+229\tval_229\t2008-04-09\t11\n+34\tval_34\t2008-04-09\t11\n+168\tval_168\t2008-04-09\t11\n+11\tval_11\t2008-04-09\t11\n+95\tval_95\t2008-04-09\t11\n+336\tval_336\t2008-04-09\t11\n+35\tval_35\t2008-04-09\t11\n+58\tval_58\t2008-04-09\t11\n 395\tval_395\t2008-04-09\t11\n-282\tval_282\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n-238\tval_238\t2008-04-09\t11\n-419\tval_419\t2008-04-09\t11\n-15\tval_15\t2008-04-09\t11\n-118\tval_118\t2008-04-09\t11\n-72\tval_72\t2008-04-09\t11\n-90\tval_90\t2008-04-09\t11\n-307\tval_307\t2008-04-09\t11\n-19\tval_19\t2008-04-09\t11\n-435\tval_435\t2008-04-09\t11\n-10\tval_10\t2008-04-09\t11\n-277\tval_277\t2008-04-09\t11\n-273\tval_273\t2008-04-09\t11\n-306\tval_306\t2008-04-09\t11\n-224\tval_224\t2008-04-09\t11\n-309\tval_309\t2008-04-09\t11\n-389\tval_389\t2008-04-09\t11\n-327\tval_327\t2008-04-09\t11\n+317\tval_317\t2008-04-09\t11\n+396\tval_396\t2008-04-09\t11\n+402\tval_402\t2008-04-09\t11\n+497\tval_497\t2008-04-09\t11\n+5\tval_5\t2008-04-09\t11\n+226\tval_226\t2008-04-09\t11\n+177\tval_177\t2008-04-09\t11\n+452\tval_452\t2008-04-09\t11\n 242\tval_242\t2008-04-09\t11\n-369\tval_369\t2008-04-09\t11\n-392\tval_392\t2008-04-09\t11\n-272\tval_272\t2008-04-09\t11\n-331\tval_331\t2008-04-09\t11\n 401\tval_401\t2008-04-09\t11\n+331\tval_331\t2008-04-09\t11\n+272\tval_272\t2008-04-09\t11\n+392\tval_392\t2008-04-09\t11\n+369\tval_369\t2008-04-09\t11\n 242\tval_242\t2008-04-09\t11\n-452\tval_452\t2008-04-09\t11\n-177\tval_177\t2008-04-09\t11\n-226\tval_226\t2008-04-09\t11\n-5\tval_5\t2008-04-09\t11\n-497\tval_497\t2008-04-09\t11\n-402\tval_402\t2008-04-09\t11\n-396\tval_396\t2008-04-09\t11\n-317\tval_317\t2008-04-09\t11\n+327\tval_327\t2008-04-09\t11\n+389\tval_389\t2008-04-09\t11\n+309\tval_309\t2008-04-09\t11\n+224\tval_224\t2008-04-09\t11\n+306\tval_306\t2008-04-09\t11\n+273\tval_273\t2008-04-09\t11\n+277\tval_277\t2008-04-09\t11\n+10\tval_10\t2008-04-09\t11\n+435\tval_435\t2008-04-09\t11\n+19\tval_19\t2008-04-09\t11\n+307\tval_307\t2008-04-09\t11\n+90\tval_90\t2008-04-09\t11\n+72\tval_72\t2008-04-09\t11\n+118\tval_118\t2008-04-09\t11\n+15\tval_15\t2008-04-09\t11\n+419\tval_419\t2008-04-09\t11\n+238\tval_238\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+282\tval_282\t2008-04-09\t11\n 395\tval_395\t2008-04-09\t11\n-58\tval_58\t2008-04-09\t11\n-35\tval_35\t2008-04-09\t11\n-336\tval_336\t2008-04-09\t11\n+134\tval_134\t2008-04-09\t11\n+118\tval_118\t2008-04-09\t11\n+179\tval_179\t2008-04-09\t11\n+364\tval_364\t2008-04-09\t11\n+87\tval_87\t2008-04-09\t11\n+490\tval_490\t2008-04-09\t11\n+0\tval_0\t2008-04-09\t11\n+77\tval_77\t2008-04-09\t11\n+85\tval_85\t2008-04-09\t11\n+413\tval_413\t2008-04-09\t11\n+169\tval_169\t2008-04-09\t11\n+316\tval_316\t2008-04-09\t11\n+369\tval_369\t2008-04-09\t11\n+137\tval_137\t2008-04-09\t11\n+470\tval_470\t2008-04-09\t11\n+409\tval_409\t2008-04-09\t11\n+318\tval_318\t2008-04-09\t11\n+318\tval_318\t2008-04-09\t11\n+187\tval_187\t2008-04-09\t11\n+197\tval_197\t2008-04-09\t11\n+282\tval_282\t2008-04-09\t11\n+98\tval_98\t2008-04-09\t11\n+457\tval_457\t2008-04-09\t11\n+481\tval_481\t2008-04-09\t11\n+288\tval_288\t2008-04-09\t11\n+196\tval_196\t2008-04-09\t11\n 95\tval_95\t2008-04-09\t11\n-11\tval_11\t2008-04-09\t11\n-168\tval_168\t2008-04-09\t11\n-34\tval_34\t2008-04-09\t11\n-229\tval_229\t2008-04-09\t11\n-233\tval_233\t2008-04-09\t11\n-143\tval_143\t2008-04-09\t11\n-472\tval_472\t2008-04-09\t11\n-322\tval_322\t2008-04-09\t11\n-498\tval_498\t2008-04-09\t11\n-160\tval_160\t2008-04-09\t11\n-195\tval_195\t2008-04-09\t11\n-42\tval_42\t2008-04-09\t11\n-321\tval_321\t2008-04-09\t11\n-430\tval_430\t2008-04-09\t11\n-119\tval_119\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-458\tval_458\t2008-04-09\t11\n-78\tval_78\t2008-04-09\t11\n-76\tval_76\t2008-04-09\t11\n-41\tval_41\t2008-04-09\t11\n-223\tval_223\t2008-04-09\t11\n-492\tval_492\t2008-04-09\t11\n-149\tval_149\t2008-04-09\t11\n-449\tval_449\t2008-04-09\t11\n-218\tval_218\t2008-04-09\t11\n-228\tval_228\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n-453\tval_453\t2008-04-09\t11\n-30\tval_30\t2008-04-09\t11\n-209\tval_209\t2008-04-09\t11\n-64\tval_64\t2008-04-09\t11\n+308\tval_308\t2008-04-09\t11\n 468\tval_468\t2008-04-09\t11\n-76\tval_76\t2008-04-09\t11\n-74\tval_74\t2008-04-09\t11\n-342\tval_342\t2008-04-09\t11\n-69\tval_69\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+156\tval_156\t2008-04-09\t11\n+436\tval_436\t2008-04-09\t11\n+43\tval_43\t2008-04-09\t11\n+404\tval_404\t2008-04-09\t11\n+51\tval_51\t2008-04-09\t11\n+131\tval_131\t2008-04-09\t11\n+120\tval_120\t2008-04-09\t11\n+205\tval_205\t2008-04-09\t11\n 230\tval_230\t2008-04-09\t11\n-33\tval_33\t2008-04-09\t11\n-368\tval_368\t2008-04-09\t11\n-103\tval_103\t2008-04-09\t11\n-296\tval_296\t2008-04-09\t11\n-113\tval_113\t2008-04-09\t11\n-216\tval_216\t2008-04-09\t11\n-367\tval_367\t2008-04-09\t11\n-344\tval_344\t2008-04-09\t11\n-167\tval_167\t2008-04-09\t11\n-274\tval_274\t2008-04-09\t11\n-219\tval_219\t2008-04-09\t11\n-239\tval_239\t2008-04-09\t11\n-485\tval_485\t2008-04-09\t11\n-116\tval_116\t2008-04-09\t11\n-223\tval_223\t2008-04-09\t11\n-256\tval_256\t2008-04-09\t11\n-263\tval_263\t2008-04-09\t11\n-70\tval_70\t2008-04-09\t11\n-487\tval_487\t2008-04-09\t11\n-480\tval_480\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-288\tval_288\t2008-04-09\t11\n+327\tval_327\t2008-04-09\t11\n+165\tval_165\t2008-04-09\t11\n+26\tval_26\t2008-04-09\t11\n+96\tval_96\t2008-04-09\t11\n+418\tval_418\t2008-04-09\t11\n 191\tval_191\t2008-04-09\t11\n-5\tval_5\t2008-04-09\t11\n-244\tval_244\t2008-04-09\t11\n-438\tval_438\t2008-04-09\t11\n-128\tval_128\t2008-04-09\t11\n-467\tval_467\t2008-04-09\t11\n-432\tval_432\t2008-04-09\t11\n-202\tval_202\t2008-04-09\t11\n-316\tval_316\t2008-04-09\t11\n-229\tval_229\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n+199\tval_199\t2008-04-09\t11\n+298\tval_298\t2008-04-09\t11\n+100\tval_100\t2008-04-09\t11\n+454\tval_454\t2008-04-09\t11\n+393\tval_393\t2008-04-09\t11\n+468\tval_468\t2008-04-09\t11\n+197\tval_197\t2008-04-09\t11\n+322\tval_322\t2008-04-09\t11\n+0\tval_0\t2008-04-09\t11\n+496\tval_496\t2008-04-09\t11\n+119\tval_119\t2008-04-09\t11\n+158\tval_158\t2008-04-09\t11\n+129\tval_129\t2008-04-09\t11\n+42\tval_42\t2008-04-09\t11\n+172\tval_172\t2008-04-09\t11\n+179\tval_179\t2008-04-09\t11\n+431\tval_431\t2008-04-09\t11\n 463\tval_463\t2008-04-09\t11\n-280\tval_280\t2008-04-09\t11\n-2\tval_2\t2008-04-09\t11\n-35\tval_35\t2008-04-09\t11\n-283\tval_283\t2008-04-09\t11\n-331\tval_331\t2008-04-09\t11\n-235\tval_235\t2008-04-09\t11\n-80\tval_80\t2008-04-09\t11\n-44\tval_44\t2008-04-09\t11\n-193\tval_193\t2008-04-09\t11\n-321\tval_321\t2008-04-09\t11\n-335\tval_335\t2008-04-09\t11\n-104\tval_104\t2008-04-09\t11\n+24\tval_24\t2008-04-09\t11\n+348\tval_348\t2008-04-09\t11\n+208\tval_208\t2008-04-09\t11\n+230\tval_230\t2008-04-09\t11\n+411\tval_411\t2008-04-09\t11\n+8\tval_8\t2008-04-09\t11\n+58\tval_58\t2008-04-09\t11\n 466\tval_466\t2008-04-09\t11\n-366\tval_366\t2008-04-09\t11\n-175\tval_175\t2008-04-09\t11\n-403\tval_403\t2008-04-09\t11\n-483\tval_483\t2008-04-09\t11\n-53\tval_53\t2008-04-09\t11\n-105\tval_105\t2008-04-09\t11\n-257\tval_257\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-409\tval_409\t2008-04-09\t11\n-190\tval_190\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-114\tval_114\t2008-04-09\t11\n-258\tval_258\t2008-04-09\t11\n-90\tval_90\t2008-04-09\t11\n-203\tval_203\t2008-04-09\t11\n-262\tval_262\t2008-04-09\t11\n 348\tval_348\t2008-04-09\t11\n-424\tval_424\t2008-04-09\t11\n+84\tval_84\t2008-04-09\t11\n+217\tval_217\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+272\tval_272\t2008-04-09\t11\n+373\tval_373\t2008-04-09\t11\n+353\tval_353\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n+384\tval_384\t2008-04-09\t11\n+404\tval_404\t2008-04-09\t11\n+260\tval_260\t2008-04-09\t11\n+67\tval_67\t2008-04-09\t11\n+181\tval_181\t2008-04-09\t11\n+230\tval_230\t2008-04-09\t11\n 12\tval_12\t2008-04-09\t11\n-396\tval_396\t2008-04-09\t11\n-201\tval_201\t2008-04-09\t11\n-217\tval_217\t2008-04-09\t11\n-164\tval_164\t2008-04-09\t11\n-431\tval_431\t2008-04-09\t11\n-454\tval_454\t2008-04-09\t11\n-478\tval_478\t2008-04-09\t11\n-298\tval_298\t2008-04-09\t11\n-125\tval_125\t2008-04-09\t11\n-431\tval_431\t2008-04-09\t11\n-164\tval_164\t2008-04-09\t11\n-424\tval_424\t2008-04-09\t11\n+284\tval_284\t2008-04-09\t11\n+180\tval_180\t2008-04-09\t11\n+333\tval_333\t2008-04-09\t11\n+83\tval_83\t2008-04-09\t11\n+241\tval_241\t2008-04-09\t11\n+137\tval_137\t2008-04-09\t11\n+275\tval_275\t2008-04-09\t11\n+311\tval_311\t2008-04-09\t11\n+332\tval_332\t2008-04-09\t11\n+318\tval_318\t2008-04-09\t11\n+65\tval_65\t2008-04-09\t11\n+221\tval_221\t2008-04-09\t11\n+289\tval_289\t2008-04-09\t11\n+176\tval_176\t2008-04-09\t11\n+278\tval_278\t2008-04-09\t11\n+430\tval_430\t2008-04-09\t11\n+216\tval_216\t2008-04-09\t11\n+213\tval_213\t2008-04-09\t11\n+239\tval_239\t2008-04-09\t11\n+103\tval_103\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+51\tval_51\t2008-04-09\t11\n+459\tval_459\t2008-04-09\t11\n+54\tval_54\t2008-04-09\t11\n+176\tval_176\t2008-04-09\t11\n 187\tval_187\t2008-04-09\t11\n+286\tval_286\t2008-04-09\t11\n+192\tval_192\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+437\tval_437\t2008-04-09\t11\n+386\tval_386\t2008-04-09\t11\n+125\tval_125\t2008-04-09\t11\n+498\tval_498\t2008-04-09\t11\n 382\tval_382\t2008-04-09\t11\n-5\tval_5\t2008-04-09\t11\n-70\tval_70\t2008-04-09\t11\n-397\tval_397\t2008-04-09\t11\n-480\tval_480\t2008-04-09\t11\n-291\tval_291\t2008-04-09\t11\n-24\tval_24\t2008-04-09\t11\n-351\tval_351\t2008-04-09\t11\n-255\tval_255\t2008-04-09\t11\n-104\tval_104\t2008-04-09\t11\n-70\tval_70\t2008-04-09\t11\n-163\tval_163\t2008-04-09\t11\n+169\tval_169\t2008-04-09\t11\n+399\tval_399\t2008-04-09\t11\n+356\tval_356\t2008-04-09\t11\n+208\tval_208\t2008-04-09\t11\n+277\tval_277\t2008-04-09\t11\n+427\tval_427\t2008-04-09\t11\n+35\tval_35\t2008-04-09\t11\n+280\tval_280\t2008-04-09\t11\n+4\tval_4\t2008-04-09\t11\n+72\tval_72\t2008-04-09\t11\n+47\tval_47\t2008-04-09\t11\n+111\tval_111\t2008-04-09\t11\n+92\tval_92\t2008-04-09\t11\n+221\tval_221\t2008-04-09\t11\n+378\tval_378\t2008-04-09\t11\n+157\tval_157\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n+20\tval_20\t2008-04-09\t11\n+170\tval_170\t2008-04-09\t11\n+129\tval_129\t2008-04-09\t11\n+345\tval_345\t2008-04-09\t11\n 438\tval_438\t2008-04-09\t11\n-119\tval_119\t2008-04-09\t11\n-414\tval_414\t2008-04-09\t11\n-200\tval_200\t2008-04-09\t11\n-491\tval_491\t2008-04-09\t11\n-237\tval_237\t2008-04-09\t11\n+149\tval_149\t2008-04-09\t11\n+205\tval_205\t2008-04-09\t11\n+302\tval_302\t2008-04-09\t11\n+57\tval_57\t2008-04-09\t11\n+316\tval_316\t2008-04-09\t11\n+311\tval_311\t2008-04-09\t11\n+128\tval_128\t2008-04-09\t11\n+455\tval_455\t2008-04-09\t11\n+0\tval_0\t2008-04-09\t11\n+339\tval_339\t2008-04-09\t11\n+203\tval_203\t2008-04-09\t11\n+155\tval_155\t2008-04-09\t11\n+113\tval_113\t2008-04-09\t11\n+17\tval_17\t2008-04-09\t11\n+475\tval_475\t2008-04-09\t11\n+195\tval_195\t2008-04-09\t11\n+167\tval_167\t2008-04-09\t11\n+325\tval_325\t2008-04-09\t11\n+367\tval_367\t2008-04-09\t11\n+342\tval_342\t2008-04-09\t11\n 439\tval_439\t2008-04-09\t11\n-360\tval_360\t2008-04-09\t11\n-248\tval_248\t2008-04-09\t11\n-479\tval_479\t2008-04-09\t11\n-305\tval_305\t2008-04-09\t11\n+266\tval_266\t2008-04-09\t11\n+365\tval_365\t2008-04-09\t11\n+309\tval_309\t2008-04-09\t11\n+397\tval_397\t2008-04-09\t11\n+377\tval_377\t2008-04-09\t11\n+162\tval_162\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n 417\tval_417\t2008-04-09\t11\n+247\tval_247\t2008-04-09\t11\n+396\tval_396\t2008-04-09\t11\n+399\tval_399\t2008-04-09\t11\n+174\tval_174\t2008-04-09\t11\n+208\tval_208\t2008-04-09\t11\n+466\tval_466\t2008-04-09\t11\n 199\tval_199\t2008-04-09\t11\n-444\tval_444\t2008-04-09\t11\n-120\tval_120\t2008-04-09\t11\n-429\tval_429\t2008-04-09\t11\n-169\tval_169\t2008-04-09\t11\n-443\tval_443\t2008-04-09\t11\n-323\tval_323\t2008-04-09\t11\n-325\tval_325\t2008-04-09\t11\n-277\tval_277\t2008-04-09\t11\n-230\tval_230\t2008-04-09\t11\n-478\tval_478\t2008-04-09\t11\n-178\tval_178\t2008-04-09\t11\n-468\tval_468\t2008-04-09\t11\n-310\tval_310\t2008-04-09\t11\n-317\tval_317\t2008-04-09\t11\n-333\tval_333\t2008-04-09\t11\n-493\tval_493\t2008-04-09\t11\n-460\tval_460\t2008-04-09\t11\n 207\tval_207\t2008-04-09\t11\n-249\tval_249\t2008-04-09\t11\n-265\tval_265\t2008-04-09\t11\n-480\tval_480\t2008-04-09\t11\n-83\tval_83\t2008-04-09\t11\n-136\tval_136\t2008-04-09\t11\n-353\tval_353\t2008-04-09\t11\n-172\tval_172\t2008-04-09\t11\n-214\tval_214\t2008-04-09\t11\n-462\tval_462\t2008-04-09\t11\n-233\tval_233\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-133\tval_133\t2008-04-09\t11\n-175\tval_175\t2008-04-09\t11\n-189\tval_189\t2008-04-09\t11\n-454\tval_454\t2008-04-09\t11\n-375\tval_375\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-421\tval_421\t2008-04-09\t11\n-407\tval_407\t2008-04-09\t11\n-384\tval_384\t2008-04-09\t11\n-256\tval_256\t2008-04-09\t11\n-26\tval_26\t2008-04-09\t11\n-134\tval_134\t2008-04-09\t11\n-67\tval_67\t2008-04-09\t11\n-384\tval_384\t2008-04-09\t11\n-379\tval_379\t2008-04-09\t11\n-18\tval_18\t2008-04-09\t11\n-462\tval_462\t2008-04-09\t11\n-492\tval_492\t2008-04-09\t11\n-100\tval_100\t2008-04-09\t11\n-298\tval_298\t2008-04-09\t11\n-9\tval_9\t2008-04-09\t11\n-341\tval_341\t2008-04-09\t11\n-498\tval_498\t2008-04-09\t11\n-146\tval_146\t2008-04-09\t11\n-458\tval_458\t2008-04-09\t11\n-362\tval_362\t2008-04-09\t11\n-186\tval_186\t2008-04-09\t11\n-285\tval_285\t2008-04-09\t11\n-348\tval_348\t2008-04-09\t11\n-167\tval_167\t2008-04-09\t11\n-18\tval_18\t2008-04-09\t11\n-273\tval_273\t2008-04-09\t11\n-183\tval_183\t2008-04-09\t11\n+494\tval_494\t2008-04-09\t11\n+413\tval_413\t2008-04-09\t11\n+174\tval_174\t2008-04-09\t11\n+482\tval_482\t2008-04-09\t11\n+237\tval_237\t2008-04-09\t11\n+394\tval_394\t2008-04-09\t11\n+459\tval_459\t2008-04-09\t11\n+446\tval_446\t2008-04-09\t11\n+338\tval_338\t2008-04-09\t11\n+193\tval_193\t2008-04-09\t11\n+153\tval_153\t2008-04-09\t11\n+287\tval_287\t2008-04-09\t11\n+219\tval_219\t2008-04-09\t11\n+292\tval_292\t2008-04-09\t11\n+252\tval_252\t2008-04-09\t11\n+430\tval_430\t2008-04-09\t11\n+417\tval_417\t2008-04-09\t11\n+166\tval_166\t2008-04-09\t11\n+403\tval_403\t2008-04-09\t11\n+82\tval_82\t2008-04-09\t11\n+15\tval_15\t2008-04-09\t11\n+209\tval_209\t2008-04-09\t11\n+277\tval_277\t2008-04-09\t11\n 281\tval_281\t2008-04-09\t11\n-344\tval_344\t2008-04-09\t11\n-97\tval_97\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n-315\tval_315\t2008-04-09\t11\n-84\tval_84\t2008-04-09\t11\n-28\tval_28\t2008-04-09\t11\n+327\tval_327\t2008-04-09\t11\n 37\tval_37\t2008-04-09\t11\n-448\tval_448\t2008-04-09\t11\n+495\tval_495\t2008-04-09\t11\n+145\tval_145\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n 152\tval_152\t2008-04-09\t11\n-348\tval_348\t2008-04-09\t11\n-307\tval_307\t2008-04-09\t11\n-194\tval_194\t2008-04-09\t11\n-414\tval_414\t2008-04-09\t11\n-477\tval_477\t2008-04-09\t11\n-222\tval_222\t2008-04-09\t11\n-126\tval_126\t2008-04-09\t11\n-90\tval_90\t2008-04-09\t11\n-169\tval_169\t2008-04-09\t11\n-403\tval_403\t2008-04-09\t11\n-400\tval_400\t2008-04-09\t11\n-200\tval_200\t2008-04-09\t11\n-97\tval_97\t2008-04-09\t11\n+374\tval_374\t2008-04-09\t11\n+429\tval_429\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+146\tval_146\t2008-04-09\t11\n+213\tval_213\t2008-04-09\t11\n+128\tval_128\t2008-04-09\t11\n+66\tval_66\t2008-04-09\t11\n+369\tval_369\t2008-04-09\t11\n+224\tval_224\t2008-04-09\t11\n+273\tval_273\t2008-04-09\t11\n+150\tval_150\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+193\tval_193\t2008-04-09\t11\n+265\tval_265\t2008-04-09\t11\n+484\tval_484\t2008-04-09\t11\n+98\tval_98\t2008-04-09\t11\n+278\tval_278\t2008-04-09\t11\n+255\tval_255\t2008-04-09\t11\n+409\tval_409\t2008-04-09\t11\n+165\tval_165\t2008-04-09\t11\n+27\tval_27\t2008-04-09\t11\n+311\tval_311\t2008-04-09\t11\n+86\tval_86\t2008-04-09\t11\n 238\tval_238\t2008-04-09\t12\n 86\tval_86\t2008-04-09\t12\n 311\tval_311\t2008-04-09\t12", "filename": "ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out"}, {"additions": 25, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/mrr.q.out", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/mrr.q.out", "sha": "29aab50e65eeefad1d823549e3ea734ff9afb02a", "changes": 50, "status": "modified", "deletions": 25, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/mrr.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -1338,24 +1338,27 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0, _col1\n                 Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n-                Select Operator\n-                  expressions: _col0 (type: string), _col1 (type: bigint)\n-                  outputColumnNames: _col0, _col1\n-                  Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n-                  Reduce Output Operator\n-                    key expressions: _col1 (type: bigint)\n-                    sort order: +\n-                    Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n-                    value expressions: _col0 (type: string), _col1 (type: bigint)\n+                Filter Operator\n+                  predicate: (_col1 > 1) (type: boolean)\n+                  Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string), _col1 (type: bigint)\n+                    outputColumnNames: _col0, _col1\n+                    Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col1 (type: bigint)\n+                      sort order: +\n+                      Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: bigint)\n         Reducer 11 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n                 Reduce Output Operator\n                   key expressions: _col0 (type: string)\n                   sort order: +\n                   Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n                   value expressions: _col0 (type: string), _col1 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n@@ -1396,25 +1399,22 @@ STAGE PLANS:\n                   2 {VALUE._col0} {VALUE._col1}\n                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                 Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n-                Filter Operator\n-                  predicate: (_col1 > 1) (type: boolean)\n-                  Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n-                  Select Operator\n-                    expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n-                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n-                    Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n-                    Reduce Output Operator\n-                      key expressions: _col0 (type: string)\n-                      sort order: +\n-                      Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n-                      value expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n+                Select Operator\n+                  expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n+                  Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n         Reducer 5 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n                 File Output Operator\n                   compressed: false\n-                  Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n                   table:\n                       input format: org.apache.hadoop.mapred.TextInputFormat\n                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat", "filename": "ql/src/test/results/clientpositive/tez/mrr.q.out"}, {"additions": 327, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_dml.q.out", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_dml.q.out", "sha": "6b13942472c7a5d8690038a8f85e49c372401c99", "changes": 642, "status": "modified", "deletions": 315, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_dml.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -436,6 +436,8 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n@@ -447,14 +449,24 @@ STAGE PLANS:\n                     expressions: value (type: string), cnt (type: bigint)\n                     outputColumnNames: _col0, _col1\n                     Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n-                    File Output Operator\n-                      compressed: false\n+                    Reduce Output Operator\n+                      key expressions: _col1 (type: bigint)\n+                      sort order: +\n+                      Map-reduce partition columns: _col1 (type: bigint)\n                       Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n-                      table:\n-                          input format: org.apache.hadoop.mapred.TextInputFormat\n-                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                          name: default.tmp_src_part\n+                      value expressions: _col0 (type: string), _col1 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Extract\n+                Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      name: default.tmp_src_part\n \n   Stage: Stage-2\n     Dependency Collection\n@@ -515,314 +527,314 @@ POSTHOOK: Lineage: tmp_src_part PARTITION(d=3).c SIMPLE [(tmp_src)tmp_src.FieldS\n POSTHOOK: Lineage: tmp_src_part PARTITION(d=4).c SIMPLE [(tmp_src)tmp_src.FieldSchema(name:value, type:string, comment:null), ]\n POSTHOOK: Lineage: tmp_src_part PARTITION(d=5).c SIMPLE [(tmp_src)tmp_src.FieldSchema(name:value, type:string, comment:null), ]\n val_490\t1\n-val_287\t1\n-val_286\t1\n-val_285\t1\n-val_284\t1\n-val_283\t1\n-val_114\t1\n-val_487\t1\n-val_485\t1\n-val_28\t1\n-val_484\t1\n-val_181\t1\n-val_275\t1\n-val_274\t1\n-val_183\t1\n-val_483\t1\n-val_27\t1\n-val_266\t1\n-val_482\t1\n-val_263\t1\n-val_262\t1\n-val_260\t1\n-val_481\t1\n-val_258\t1\n-val_257\t1\n-val_116\t1\n-val_479\t1\n-val_252\t1\n-val_249\t1\n-val_248\t1\n-val_247\t1\n-val_244\t1\n-val_92\t1\n-val_241\t1\n-val_477\t1\n-val_475\t1\n-val_472\t1\n-val_470\t1\n-val_235\t1\n-val_47\t1\n-val_186\t1\n-val_126\t1\n-val_228\t1\n-val_226\t1\n-val_131\t1\n-val_467\t1\n-val_222\t1\n-val_133\t1\n-val_82\t1\n-val_218\t1\n-val_80\t1\n-val_460\t1\n-val_214\t1\n-val_8\t1\n-val_78\t1\n-val_189\t1\n-val_457\t1\n-val_455\t1\n-val_136\t1\n-val_202\t1\n-val_201\t1\n-val_453\t1\n-val_20\t1\n-val_2\t1\n-val_19\t1\n-val_452\t1\n-val_196\t1\n-val_449\t1\n-val_194\t1\n-val_190\t1\n-val_192\t1\n-val_448\t1\n-val_446\t1\n-val_444\t1\n-val_443\t1\n-val_44\t1\n-val_77\t1\n-val_143\t1\n-val_437\t1\n-val_436\t1\n-val_435\t1\n-val_432\t1\n-val_145\t1\n-val_150\t1\n-val_43\t1\n-val_10\t1\n-val_427\t1\n-val_74\t1\n-val_421\t1\n-val_9\t1\n-val_419\t1\n-val_418\t1\n-val_153\t1\n-val_105\t1\n-val_69\t1\n-val_411\t1\n-val_41\t1\n-val_155\t1\n-val_407\t1\n-val_156\t1\n-val_87\t1\n-val_157\t1\n-val_402\t1\n-val_158\t1\n-val_400\t1\n-val_4\t1\n-val_66\t1\n-val_65\t1\n-val_160\t1\n-val_64\t1\n-val_394\t1\n-val_393\t1\n-val_392\t1\n-val_389\t1\n-val_386\t1\n-val_162\t1\n-val_86\t1\n-val_379\t1\n-val_378\t1\n-val_377\t1\n-val_375\t1\n-val_374\t1\n-val_373\t1\n-val_57\t1\n-val_163\t1\n-val_368\t1\n-val_54\t1\n-val_366\t1\n-val_365\t1\n-val_364\t1\n-val_362\t1\n-val_360\t1\n-val_356\t1\n-val_53\t1\n-val_351\t1\n-val_166\t1\n-val_168\t1\n-val_345\t1\n-val_85\t1\n-val_11\t1\n-val_341\t1\n-val_34\t1\n-val_339\t1\n-val_338\t1\n-val_336\t1\n-val_335\t1\n-val_111\t1\n-val_332\t1\n-val_497\t1\n-val_33\t1\n-val_17\t1\n-val_496\t1\n-val_323\t1\n-val_495\t1\n-val_494\t1\n-val_170\t1\n-val_493\t1\n-val_177\t1\n-val_315\t1\n-val_178\t1\n-val_310\t1\n-val_96\t1\n-val_308\t1\n-val_491\t1\n-val_306\t1\n-val_305\t1\n-val_302\t1\n-val_30\t1\n-val_180\t1\n-val_296\t1\n-val_292\t1\n-val_291\t1\n val_289\t1\n-val_98\t2\n-val_97\t2\n-val_95\t2\n-val_84\t2\n-val_83\t2\n-val_76\t2\n-val_72\t2\n-val_67\t2\n-val_58\t2\n-val_51\t2\n-val_492\t2\n-val_478\t2\n-val_463\t2\n-val_462\t2\n-val_459\t2\n-val_458\t2\n-val_439\t2\n-val_429\t2\n-val_424\t2\n-val_42\t2\n-val_414\t2\n-val_413\t2\n-val_404\t2\n-val_399\t2\n-val_397\t2\n-val_395\t2\n-val_382\t2\n-val_37\t2\n-val_367\t2\n-val_353\t2\n-val_344\t2\n-val_342\t2\n-val_333\t2\n-val_331\t2\n-val_325\t2\n-val_322\t2\n-val_321\t2\n-val_317\t2\n-val_309\t2\n-val_307\t2\n-val_288\t2\n-val_282\t2\n-val_281\t2\n-val_280\t2\n-val_278\t2\n-val_272\t2\n-val_265\t2\n-val_26\t2\n-val_256\t2\n-val_255\t2\n-val_242\t2\n-val_24\t2\n-val_239\t2\n-val_238\t2\n-val_237\t2\n-val_233\t2\n-val_229\t2\n-val_224\t2\n-val_223\t2\n-val_221\t2\n-val_219\t2\n-val_217\t2\n-val_216\t2\n-val_213\t2\n-val_209\t2\n-val_207\t2\n-val_205\t2\n-val_203\t2\n-val_200\t2\n-val_197\t2\n-val_195\t2\n-val_191\t2\n-val_18\t2\n-val_179\t2\n-val_176\t2\n-val_175\t2\n-val_174\t2\n-val_172\t2\n-val_165\t2\n-val_164\t2\n-val_152\t2\n-val_15\t2\n-val_149\t2\n-val_146\t2\n-val_137\t2\n-val_134\t2\n-val_129\t2\n-val_125\t2\n-val_120\t2\n-val_12\t2\n-val_118\t2\n-val_113\t2\n-val_104\t2\n-val_103\t2\n+val_291\t1\n+val_292\t1\n+val_296\t1\n+val_180\t1\n+val_30\t1\n+val_302\t1\n+val_305\t1\n+val_306\t1\n+val_491\t1\n+val_308\t1\n+val_96\t1\n+val_310\t1\n+val_178\t1\n+val_315\t1\n+val_177\t1\n+val_493\t1\n+val_170\t1\n+val_494\t1\n+val_495\t1\n+val_323\t1\n+val_496\t1\n+val_17\t1\n+val_33\t1\n+val_497\t1\n+val_332\t1\n+val_111\t1\n+val_335\t1\n+val_336\t1\n+val_338\t1\n+val_339\t1\n+val_34\t1\n+val_341\t1\n+val_11\t1\n+val_85\t1\n+val_345\t1\n+val_168\t1\n+val_166\t1\n+val_351\t1\n+val_53\t1\n+val_356\t1\n+val_360\t1\n+val_362\t1\n+val_364\t1\n+val_365\t1\n+val_366\t1\n+val_54\t1\n+val_368\t1\n+val_163\t1\n+val_57\t1\n+val_373\t1\n+val_374\t1\n+val_375\t1\n+val_377\t1\n+val_378\t1\n+val_379\t1\n+val_86\t1\n+val_162\t1\n+val_386\t1\n+val_389\t1\n+val_392\t1\n+val_393\t1\n+val_394\t1\n+val_64\t1\n+val_160\t1\n+val_65\t1\n+val_66\t1\n+val_4\t1\n+val_400\t1\n+val_158\t1\n+val_402\t1\n+val_157\t1\n+val_87\t1\n+val_156\t1\n+val_407\t1\n+val_155\t1\n+val_41\t1\n+val_411\t1\n+val_69\t1\n+val_105\t1\n+val_153\t1\n+val_418\t1\n+val_419\t1\n+val_9\t1\n+val_421\t1\n+val_74\t1\n+val_427\t1\n+val_10\t1\n+val_43\t1\n+val_150\t1\n+val_145\t1\n+val_432\t1\n+val_435\t1\n+val_436\t1\n+val_437\t1\n+val_143\t1\n+val_77\t1\n+val_44\t1\n+val_443\t1\n+val_444\t1\n+val_446\t1\n+val_448\t1\n+val_192\t1\n+val_190\t1\n+val_194\t1\n+val_449\t1\n+val_196\t1\n+val_452\t1\n+val_19\t1\n+val_2\t1\n+val_20\t1\n+val_453\t1\n+val_201\t1\n+val_202\t1\n+val_136\t1\n+val_455\t1\n+val_457\t1\n+val_189\t1\n+val_78\t1\n+val_8\t1\n+val_214\t1\n+val_460\t1\n+val_80\t1\n+val_218\t1\n+val_82\t1\n+val_133\t1\n+val_222\t1\n+val_467\t1\n+val_131\t1\n+val_226\t1\n+val_228\t1\n+val_126\t1\n+val_186\t1\n+val_47\t1\n+val_235\t1\n+val_470\t1\n+val_472\t1\n+val_475\t1\n+val_477\t1\n+val_241\t1\n+val_92\t1\n+val_244\t1\n+val_247\t1\n+val_248\t1\n+val_249\t1\n+val_252\t1\n+val_479\t1\n+val_116\t1\n+val_257\t1\n+val_258\t1\n+val_481\t1\n+val_260\t1\n+val_262\t1\n+val_263\t1\n+val_482\t1\n+val_266\t1\n+val_27\t1\n+val_483\t1\n+val_183\t1\n+val_274\t1\n+val_275\t1\n+val_181\t1\n+val_484\t1\n+val_28\t1\n+val_485\t1\n+val_487\t1\n+val_114\t1\n+val_283\t1\n+val_284\t1\n+val_285\t1\n+val_286\t1\n+val_287\t1\n+val_84\t2\n+val_95\t2\n+val_97\t2\n+val_98\t2\n val_100\t2\n-val_498\t3\n-val_369\t3\n-val_384\t3\n+val_103\t2\n+val_104\t2\n+val_113\t2\n+val_118\t2\n+val_12\t2\n+val_120\t2\n+val_125\t2\n+val_129\t2\n+val_134\t2\n+val_137\t2\n+val_146\t2\n+val_149\t2\n+val_15\t2\n+val_152\t2\n+val_164\t2\n+val_165\t2\n+val_172\t2\n+val_174\t2\n+val_175\t2\n+val_176\t2\n+val_179\t2\n+val_18\t2\n+val_191\t2\n+val_195\t2\n+val_197\t2\n+val_200\t2\n+val_203\t2\n+val_205\t2\n+val_207\t2\n+val_209\t2\n+val_213\t2\n+val_216\t2\n+val_217\t2\n+val_219\t2\n+val_221\t2\n+val_223\t2\n+val_224\t2\n+val_229\t2\n+val_233\t2\n+val_237\t2\n+val_238\t2\n+val_239\t2\n+val_24\t2\n+val_242\t2\n+val_255\t2\n+val_256\t2\n+val_26\t2\n+val_265\t2\n+val_272\t2\n+val_278\t2\n+val_280\t2\n+val_281\t2\n+val_282\t2\n+val_288\t2\n+val_307\t2\n+val_309\t2\n+val_317\t2\n+val_321\t2\n+val_322\t2\n+val_325\t2\n+val_331\t2\n+val_333\t2\n+val_342\t2\n+val_344\t2\n+val_353\t2\n+val_367\t2\n+val_37\t2\n+val_382\t2\n+val_395\t2\n+val_397\t2\n+val_399\t2\n+val_404\t2\n+val_413\t2\n+val_414\t2\n+val_42\t2\n+val_424\t2\n+val_429\t2\n+val_439\t2\n+val_458\t2\n+val_459\t2\n+val_462\t2\n+val_463\t2\n+val_478\t2\n+val_492\t2\n+val_51\t2\n+val_58\t2\n+val_67\t2\n+val_72\t2\n+val_76\t2\n+val_83\t2\n val_396\t3\n-val_403\t3\n-val_409\t3\n-val_417\t3\n-val_5\t3\n-val_430\t3\n-val_70\t3\n-val_119\t3\n-val_0\t3\n-val_431\t3\n-val_438\t3\n-val_480\t3\n-val_193\t3\n-val_199\t3\n-val_208\t3\n-val_187\t3\n-val_273\t3\n-val_298\t3\n-val_454\t3\n-val_311\t3\n-val_316\t3\n-val_466\t3\n-val_90\t3\n-val_128\t3\n-val_318\t3\n-val_327\t3\n-val_167\t3\n+val_384\t3\n+val_369\t3\n+val_498\t3\n val_35\t3\n-val_468\t4\n-val_489\t4\n+val_167\t3\n+val_327\t3\n+val_318\t3\n+val_128\t3\n+val_90\t3\n+val_466\t3\n+val_316\t3\n+val_311\t3\n+val_454\t3\n+val_298\t3\n+val_273\t3\n+val_187\t3\n+val_208\t3\n+val_199\t3\n+val_193\t3\n+val_480\t3\n+val_438\t3\n+val_431\t3\n+val_0\t3\n+val_119\t3\n+val_70\t3\n+val_430\t3\n+val_5\t3\n+val_417\t3\n+val_409\t3\n+val_403\t3\n val_406\t4\n-val_169\t4\n-val_138\t4\n+val_489\t4\n+val_468\t4\n val_277\t4\n-val_469\t5\n-val_401\t5\n-val_230\t5\n+val_138\t4\n+val_169\t4\n val_348\t5\n+val_230\t5\n+val_401\t5\n+val_469\t5\n PREHOOK: query: -- multi insert\n CREATE TABLE even (c int, d string)\n PREHOOK: type: CREATETABLE\n@@ -867,10 +879,10 @@ POSTHOOK: Lineage: tmp_src_part PARTITION(d=5).c SIMPLE [(tmp_src)tmp_src.FieldS\n STAGE DEPENDENCIES:\n   Stage-2 is a root stage\n   Stage-3 depends on stages: Stage-2\n-  Stage-0 depends on stages: Stage-3\n-  Stage-4 depends on stages: Stage-0\n   Stage-1 depends on stages: Stage-3\n-  Stage-5 depends on stages: Stage-1\n+  Stage-4 depends on stages: Stage-1\n+  Stage-0 depends on stages: Stage-3\n+  Stage-5 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-2\n@@ -916,28 +928,28 @@ STAGE PLANS:\n   Stage: Stage-3\n     Dependency Collection\n \n-  Stage: Stage-0\n+  Stage: Stage-1\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.even\n+              name: default.odd\n \n   Stage: Stage-4\n     Stats-Aggr Operator\n \n-  Stage: Stage-1\n+  Stage: Stage-0\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.odd\n+              name: default.even\n \n   Stage: Stage-5\n     Stats-Aggr Operator", "filename": "ql/src/test/results/clientpositive/tez/tez_dml.q.out"}, {"additions": 952, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_union.q.out", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_union.q.out", "sha": "4c0d57cc1ab2f93a7f9a6dbb2e7e6f2cd2e133c8", "changes": 2477, "status": "modified", "deletions": 1525, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -79,1546 +79,55 @@ STAGE PLANS:\n     Fetch Operator\n       limit: -1\n \n-PREHOOK: query: select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n+PREHOOK: query: create table ut as\n+select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2\n-PREHOOK: type: QUERY\n+PREHOOK: type: CREATETABLE_AS_SELECT\n PREHOOK: Input: default@src\n-#### A masked pattern was here ####\n-POSTHOOK: query: select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n+POSTHOOK: query: create table ut as\n+select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2\n-POSTHOOK: type: QUERY\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by key, value limit 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by key, value limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n #### A masked pattern was here ####\n-238\tval_238\n-238\tval_238\n-86\tval_86\n-311\tval_311\n-311\tval_311\n-311\tval_311\n-27\tval_27\n-165\tval_165\n-165\tval_165\n-409\tval_409\n-409\tval_409\n-409\tval_409\n-255\tval_255\n-255\tval_255\n-278\tval_278\n-278\tval_278\n-98\tval_98\n-98\tval_98\n-484\tval_484\n-265\tval_265\n-265\tval_265\n-193\tval_193\n-193\tval_193\n-193\tval_193\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-150\tval_150\n-273\tval_273\n-273\tval_273\n-273\tval_273\n-224\tval_224\n-224\tval_224\n-369\tval_369\n-369\tval_369\n-369\tval_369\n-66\tval_66\n-128\tval_128\n-128\tval_128\n-128\tval_128\n-213\tval_213\n-213\tval_213\n-146\tval_146\n-146\tval_146\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-429\tval_429\n-429\tval_429\n-374\tval_374\n-152\tval_152\n-152\tval_152\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-145\tval_145\n-495\tval_495\n-37\tval_37\n-37\tval_37\n-327\tval_327\n-327\tval_327\n-327\tval_327\n-281\tval_281\n-281\tval_281\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-209\tval_209\n-209\tval_209\n-15\tval_15\n-15\tval_15\n-82\tval_82\n-403\tval_403\n-403\tval_403\n-403\tval_403\n-166\tval_166\n-417\tval_417\n-417\tval_417\n-417\tval_417\n-430\tval_430\n-430\tval_430\n-430\tval_430\n-252\tval_252\n-292\tval_292\n-219\tval_219\n-219\tval_219\n-287\tval_287\n-153\tval_153\n-193\tval_193\n-193\tval_193\n-193\tval_193\n-338\tval_338\n-446\tval_446\n-459\tval_459\n-459\tval_459\n-394\tval_394\n-237\tval_237\n-237\tval_237\n-482\tval_482\n-174\tval_174\n-174\tval_174\n-413\tval_413\n-413\tval_413\n-494\tval_494\n-207\tval_207\n-207\tval_207\n-199\tval_199\n-199\tval_199\n-199\tval_199\n-466\tval_466\n-466\tval_466\n-466\tval_466\n-208\tval_208\n-208\tval_208\n-208\tval_208\n-174\tval_174\n-174\tval_174\n-399\tval_399\n-399\tval_399\n-396\tval_396\n-396\tval_396\n-396\tval_396\n-247\tval_247\n-417\tval_417\n-417\tval_417\n-417\tval_417\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-162\tval_162\n-377\tval_377\n-397\tval_397\n-397\tval_397\n-309\tval_309\n-309\tval_309\n-365\tval_365\n-266\tval_266\n-439\tval_439\n-439\tval_439\n-342\tval_342\n-342\tval_342\n-367\tval_367\n-367\tval_367\n-325\tval_325\n-325\tval_325\n-167\tval_167\n-167\tval_167\n-167\tval_167\n-195\tval_195\n-195\tval_195\n-475\tval_475\n-17\tval_17\n-113\tval_113\n-113\tval_113\n-155\tval_155\n-203\tval_203\n-203\tval_203\n-339\tval_339\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-455\tval_455\n-128\tval_128\n-128\tval_128\n-128\tval_128\n-311\tval_311\n-311\tval_311\n-311\tval_311\n-316\tval_316\n-316\tval_316\n-316\tval_316\n-57\tval_57\n-302\tval_302\n-205\tval_205\n-205\tval_205\n-149\tval_149\n-149\tval_149\n-438\tval_438\n-438\tval_438\n-438\tval_438\n-345\tval_345\n-129\tval_129\n-129\tval_129\n-170\tval_170\n-20\tval_20\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-157\tval_157\n-378\tval_378\n-221\tval_221\n-221\tval_221\n-92\tval_92\n-111\tval_111\n-47\tval_47\n-72\tval_72\n-72\tval_72\n-4\tval_4\n-280\tval_280\n-280\tval_280\n-35\tval_35\n-35\tval_35\n-35\tval_35\n-427\tval_427\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-208\tval_208\n-208\tval_208\n-208\tval_208\n-356\tval_356\n-399\tval_399\n-399\tval_399\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-382\tval_382\n-382\tval_382\n-498\tval_498\n-498\tval_498\n-498\tval_498\n-125\tval_125\n-125\tval_125\n-386\tval_386\n-437\tval_437\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-192\tval_192\n-286\tval_286\n-187\tval_187\n-187\tval_187\n-187\tval_187\n-176\tval_176\n-176\tval_176\n-54\tval_54\n-459\tval_459\n-459\tval_459\n-51\tval_51\n-51\tval_51\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-103\tval_103\n-103\tval_103\n-239\tval_239\n-239\tval_239\n-213\tval_213\n-213\tval_213\n-216\tval_216\n-216\tval_216\n-430\tval_430\n-430\tval_430\n-430\tval_430\n-278\tval_278\n-278\tval_278\n-176\tval_176\n-176\tval_176\n-289\tval_289\n-221\tval_221\n-221\tval_221\n-65\tval_65\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-332\tval_332\n-311\tval_311\n-311\tval_311\n-311\tval_311\n-275\tval_275\n-137\tval_137\n-137\tval_137\n-241\tval_241\n-83\tval_83\n-83\tval_83\n-333\tval_333\n-333\tval_333\n-180\tval_180\n-284\tval_284\n-12\tval_12\n-12\tval_12\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-181\tval_181\n-67\tval_67\n-67\tval_67\n-260\tval_260\n-404\tval_404\n-404\tval_404\n-384\tval_384\n-384\tval_384\n-384\tval_384\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-353\tval_353\n-353\tval_353\n-373\tval_373\n-272\tval_272\n-272\tval_272\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-217\tval_217\n-217\tval_217\n-84\tval_84\n-84\tval_84\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-466\tval_466\n-466\tval_466\n-466\tval_466\n-58\tval_58\n-58\tval_58\n-8\tval_8\n-411\tval_411\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-208\tval_208\n-208\tval_208\n-208\tval_208\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-24\tval_24\n-24\tval_24\n-463\tval_463\n-463\tval_463\n-431\tval_431\n-431\tval_431\n-431\tval_431\n-179\tval_179\n-179\tval_179\n-172\tval_172\n-172\tval_172\n-42\tval_42\n-42\tval_42\n-129\tval_129\n-129\tval_129\n-158\tval_158\n-119\tval_119\n-119\tval_119\n-119\tval_119\n-496\tval_496\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-322\tval_322\n-322\tval_322\n-197\tval_197\n-197\tval_197\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-393\tval_393\n-454\tval_454\n-454\tval_454\n-454\tval_454\n-100\tval_100\n-100\tval_100\n-298\tval_298\n-298\tval_298\n-298\tval_298\n-199\tval_199\n-199\tval_199\n-199\tval_199\n-191\tval_191\n-191\tval_191\n-418\tval_418\n-96\tval_96\n-26\tval_26\n-26\tval_26\n-165\tval_165\n-165\tval_165\n-327\tval_327\n-327\tval_327\n-327\tval_327\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-205\tval_205\n-205\tval_205\n-120\tval_120\n-120\tval_120\n-131\tval_131\n-51\tval_51\n-51\tval_51\n-404\tval_404\n-404\tval_404\n-43\tval_43\n-436\tval_436\n-156\tval_156\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-308\tval_308\n-95\tval_95\n-95\tval_95\n-196\tval_196\n-288\tval_288\n-288\tval_288\n-481\tval_481\n-457\tval_457\n-98\tval_98\n-98\tval_98\n-282\tval_282\n-282\tval_282\n-197\tval_197\n-197\tval_197\n-187\tval_187\n-187\tval_187\n-187\tval_187\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-409\tval_409\n-409\tval_409\n-409\tval_409\n-470\tval_470\n-137\tval_137\n-137\tval_137\n-369\tval_369\n-369\tval_369\n-369\tval_369\n-316\tval_316\n-316\tval_316\n-316\tval_316\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-413\tval_413\n-413\tval_413\n-85\tval_85\n-77\tval_77\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-490\tval_490\n-87\tval_87\n-364\tval_364\n-179\tval_179\n-179\tval_179\n-118\tval_118\n-118\tval_118\n-134\tval_134\n-134\tval_134\n-395\tval_395\n-395\tval_395\n-282\tval_282\n-282\tval_282\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-238\tval_238\n-238\tval_238\n-419\tval_419\n-15\tval_15\n-15\tval_15\n-118\tval_118\n-118\tval_118\n-72\tval_72\n-72\tval_72\n-90\tval_90\n-90\tval_90\n-90\tval_90\n-307\tval_307\n-307\tval_307\n-19\tval_19\n-435\tval_435\n-10\tval_10\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-273\tval_273\n-273\tval_273\n-273\tval_273\n-306\tval_306\n-224\tval_224\n-224\tval_224\n-309\tval_309\n-309\tval_309\n-389\tval_389\n-327\tval_327\n-327\tval_327\n-327\tval_327\n-242\tval_242\n-242\tval_242\n-369\tval_369\n-369\tval_369\n-369\tval_369\n-392\tval_392\n-272\tval_272\n-272\tval_272\n-331\tval_331\n-331\tval_331\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-242\tval_242\n-242\tval_242\n-452\tval_452\n-177\tval_177\n-226\tval_226\n-5\tval_5\n-5\tval_5\n-5\tval_5\n-497\tval_497\n-402\tval_402\n-396\tval_396\n-396\tval_396\n-396\tval_396\n-317\tval_317\n-317\tval_317\n-395\tval_395\n-395\tval_395\n-58\tval_58\n-58\tval_58\n-35\tval_35\n-35\tval_35\n-35\tval_35\n-336\tval_336\n-95\tval_95\n-95\tval_95\n-11\tval_11\n-168\tval_168\n-34\tval_34\n-229\tval_229\n-229\tval_229\n-233\tval_233\n-233\tval_233\n-143\tval_143\n-472\tval_472\n-322\tval_322\n-322\tval_322\n-498\tval_498\n-498\tval_498\n-498\tval_498\n-160\tval_160\n-195\tval_195\n-195\tval_195\n-42\tval_42\n-42\tval_42\n-321\tval_321\n-321\tval_321\n-430\tval_430\n-430\tval_430\n-430\tval_430\n-119\tval_119\n-119\tval_119\n-119\tval_119\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-458\tval_458\n-458\tval_458\n-78\tval_78\n-76\tval_76\n-76\tval_76\n-41\tval_41\n-223\tval_223\n-223\tval_223\n-492\tval_492\n-492\tval_492\n-149\tval_149\n-149\tval_149\n-449\tval_449\n-218\tval_218\n-228\tval_228\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-453\tval_453\n-30\tval_30\n-209\tval_209\n-209\tval_209\n-64\tval_64\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-76\tval_76\n-76\tval_76\n-74\tval_74\n-342\tval_342\n-342\tval_342\n-69\tval_69\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-33\tval_33\n-368\tval_368\n-103\tval_103\n-103\tval_103\n-296\tval_296\n-113\tval_113\n-113\tval_113\n-216\tval_216\n-216\tval_216\n-367\tval_367\n-367\tval_367\n-344\tval_344\n-344\tval_344\n-167\tval_167\n-167\tval_167\n-167\tval_167\n-274\tval_274\n-219\tval_219\n-219\tval_219\n-239\tval_239\n-239\tval_239\n-485\tval_485\n-116\tval_116\n-223\tval_223\n-223\tval_223\n-256\tval_256\n-256\tval_256\n-263\tval_263\n-70\tval_70\n-70\tval_70\n-70\tval_70\n-487\tval_487\n-480\tval_480\n-480\tval_480\n-480\tval_480\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-288\tval_288\n-288\tval_288\n-191\tval_191\n-191\tval_191\n-5\tval_5\n-5\tval_5\n-5\tval_5\n-244\tval_244\n-438\tval_438\n-438\tval_438\n-438\tval_438\n-128\tval_128\n-128\tval_128\n-128\tval_128\n-467\tval_467\n-432\tval_432\n-202\tval_202\n-316\tval_316\n-316\tval_316\n-316\tval_316\n-229\tval_229\n-229\tval_229\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-463\tval_463\n-463\tval_463\n-280\tval_280\n-280\tval_280\n-2\tval_2\n-35\tval_35\n-35\tval_35\n-35\tval_35\n-283\tval_283\n-331\tval_331\n-331\tval_331\n-235\tval_235\n-80\tval_80\n-44\tval_44\n-193\tval_193\n-193\tval_193\n-193\tval_193\n-321\tval_321\n-321\tval_321\n-335\tval_335\n-104\tval_104\n-104\tval_104\n-466\tval_466\n-466\tval_466\n-466\tval_466\n-366\tval_366\n-175\tval_175\n-175\tval_175\n-403\tval_403\n-403\tval_403\n-403\tval_403\n-483\tval_483\n-53\tval_53\n-105\tval_105\n-257\tval_257\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-409\tval_409\n-409\tval_409\n-409\tval_409\n-190\tval_190\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-114\tval_114\n-258\tval_258\n-90\tval_90\n-90\tval_90\n-90\tval_90\n-203\tval_203\n-203\tval_203\n-262\tval_262\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-424\tval_424\n-424\tval_424\n-12\tval_12\n-12\tval_12\n-396\tval_396\n-396\tval_396\n-396\tval_396\n-201\tval_201\n-217\tval_217\n-217\tval_217\n-164\tval_164\n-164\tval_164\n-431\tval_431\n-431\tval_431\n-431\tval_431\n-454\tval_454\n-454\tval_454\n-454\tval_454\n-478\tval_478\n-478\tval_478\n-298\tval_298\n-298\tval_298\n-298\tval_298\n-125\tval_125\n-125\tval_125\n-431\tval_431\n-431\tval_431\n-431\tval_431\n-164\tval_164\n-164\tval_164\n-424\tval_424\n-424\tval_424\n-187\tval_187\n-187\tval_187\n-187\tval_187\n-382\tval_382\n-382\tval_382\n-5\tval_5\n-5\tval_5\n-5\tval_5\n-70\tval_70\n-70\tval_70\n-70\tval_70\n-397\tval_397\n-397\tval_397\n-480\tval_480\n-480\tval_480\n-480\tval_480\n-291\tval_291\n-24\tval_24\n-24\tval_24\n-351\tval_351\n-255\tval_255\n-255\tval_255\n-104\tval_104\n-104\tval_104\n-70\tval_70\n-70\tval_70\n-70\tval_70\n-163\tval_163\n-438\tval_438\n-438\tval_438\n-438\tval_438\n-119\tval_119\n-119\tval_119\n-119\tval_119\n-414\tval_414\n-414\tval_414\n-200\tval_200\n-200\tval_200\n-491\tval_491\n-237\tval_237\n-237\tval_237\n-439\tval_439\n-439\tval_439\n-360\tval_360\n-248\tval_248\n-479\tval_479\n-305\tval_305\n-417\tval_417\n-417\tval_417\n-417\tval_417\n-199\tval_199\n-199\tval_199\n-199\tval_199\n-444\tval_444\n-120\tval_120\n-120\tval_120\n-429\tval_429\n-429\tval_429\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-443\tval_443\n-323\tval_323\n-325\tval_325\n-325\tval_325\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-478\tval_478\n-478\tval_478\n-178\tval_178\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-310\tval_310\n-317\tval_317\n-317\tval_317\n-333\tval_333\n-333\tval_333\n-493\tval_493\n-460\tval_460\n-207\tval_207\n-207\tval_207\n-249\tval_249\n-265\tval_265\n-265\tval_265\n-480\tval_480\n-480\tval_480\n-480\tval_480\n-83\tval_83\n-83\tval_83\n-136\tval_136\n-353\tval_353\n-353\tval_353\n-172\tval_172\n-172\tval_172\n-214\tval_214\n-462\tval_462\n-462\tval_462\n-233\tval_233\n-233\tval_233\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-133\tval_133\n-175\tval_175\n-175\tval_175\n-189\tval_189\n-454\tval_454\n-454\tval_454\n-454\tval_454\n-375\tval_375\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-421\tval_421\n-407\tval_407\n-384\tval_384\n-384\tval_384\n-384\tval_384\n-256\tval_256\n-256\tval_256\n-26\tval_26\n-26\tval_26\n-134\tval_134\n-134\tval_134\n-67\tval_67\n-67\tval_67\n-384\tval_384\n-384\tval_384\n-384\tval_384\n-379\tval_379\n-18\tval_18\n-18\tval_18\n-462\tval_462\n-462\tval_462\n-492\tval_492\n-492\tval_492\n-100\tval_100\n-100\tval_100\n-298\tval_298\n-298\tval_298\n-298\tval_298\n-9\tval_9\n-341\tval_341\n-498\tval_498\n-498\tval_498\n-498\tval_498\n-146\tval_146\n-146\tval_146\n-458\tval_458\n-458\tval_458\n-362\tval_362\n-186\tval_186\n-285\tval_285\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-167\tval_167\n-167\tval_167\n-167\tval_167\n-18\tval_18\n-18\tval_18\n-273\tval_273\n-273\tval_273\n-273\tval_273\n-183\tval_183\n-281\tval_281\n-281\tval_281\n-344\tval_344\n-344\tval_344\n-97\tval_97\n-97\tval_97\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-315\tval_315\n-84\tval_84\n-84\tval_84\n-28\tval_28\n-37\tval_37\n-37\tval_37\n-448\tval_448\n-152\tval_152\n-152\tval_152\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-307\tval_307\n-307\tval_307\n-194\tval_194\n-414\tval_414\n-414\tval_414\n-477\tval_477\n-222\tval_222\n-126\tval_126\n-90\tval_90\n-90\tval_90\n-90\tval_90\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-403\tval_403\n-403\tval_403\n-403\tval_403\n-400\tval_400\n-200\tval_200\n-200\tval_200\n-97\tval_97\n-97\tval_97\n-238\tval_238\n-86\tval_86\n-311\tval_311\n-27\tval_27\n-165\tval_165\n-409\tval_409\n-255\tval_255\n-278\tval_278\n-98\tval_98\n-484\tval_484\n-265\tval_265\n-193\tval_193\n-401\tval_401\n-150\tval_150\n-273\tval_273\n-224\tval_224\n-369\tval_369\n-66\tval_66\n-128\tval_128\n-213\tval_213\n-146\tval_146\n-406\tval_406\n-429\tval_429\n-374\tval_374\n-152\tval_152\n-469\tval_469\n-145\tval_145\n-495\tval_495\n-37\tval_37\n-327\tval_327\n-281\tval_281\n-277\tval_277\n-209\tval_209\n-15\tval_15\n-82\tval_82\n-403\tval_403\n-166\tval_166\n-417\tval_417\n-430\tval_430\n-252\tval_252\n-292\tval_292\n-219\tval_219\n-287\tval_287\n-153\tval_153\n-193\tval_193\n-338\tval_338\n-446\tval_446\n-459\tval_459\n-394\tval_394\n-237\tval_237\n-482\tval_482\n-174\tval_174\n-413\tval_413\n-494\tval_494\n-207\tval_207\n-199\tval_199\n-466\tval_466\n-208\tval_208\n-174\tval_174\n-399\tval_399\n-396\tval_396\n-247\tval_247\n-417\tval_417\n-489\tval_489\n-162\tval_162\n-377\tval_377\n-397\tval_397\n-309\tval_309\n-365\tval_365\n-266\tval_266\n-439\tval_439\n-342\tval_342\n-367\tval_367\n-325\tval_325\n-167\tval_167\n-195\tval_195\n-475\tval_475\n-17\tval_17\n-113\tval_113\n-155\tval_155\n-203\tval_203\n-339\tval_339\n 0\tval_0\n-455\tval_455\n-128\tval_128\n-311\tval_311\n-316\tval_316\n-57\tval_57\n-302\tval_302\n-205\tval_205\n-149\tval_149\n-438\tval_438\n-345\tval_345\n-129\tval_129\n-170\tval_170\n-20\tval_20\n-489\tval_489\n-157\tval_157\n-378\tval_378\n-221\tval_221\n-92\tval_92\n-111\tval_111\n-47\tval_47\n-72\tval_72\n-4\tval_4\n-280\tval_280\n-35\tval_35\n-427\tval_427\n-277\tval_277\n-208\tval_208\n-356\tval_356\n-399\tval_399\n-169\tval_169\n-382\tval_382\n-498\tval_498\n-125\tval_125\n-386\tval_386\n-437\tval_437\n-469\tval_469\n-192\tval_192\n-286\tval_286\n-187\tval_187\n-176\tval_176\n-54\tval_54\n-459\tval_459\n-51\tval_51\n-138\tval_138\n-103\tval_103\n-239\tval_239\n-213\tval_213\n-216\tval_216\n-430\tval_430\n-278\tval_278\n-176\tval_176\n-289\tval_289\n-221\tval_221\n-65\tval_65\n-318\tval_318\n-332\tval_332\n-311\tval_311\n-275\tval_275\n-137\tval_137\n-241\tval_241\n-83\tval_83\n-333\tval_333\n-180\tval_180\n-284\tval_284\n-12\tval_12\n-230\tval_230\n-181\tval_181\n-67\tval_67\n-260\tval_260\n-404\tval_404\n-384\tval_384\n-489\tval_489\n-353\tval_353\n-373\tval_373\n-272\tval_272\n-138\tval_138\n-217\tval_217\n-84\tval_84\n-348\tval_348\n-466\tval_466\n-58\tval_58\n-8\tval_8\n-411\tval_411\n-230\tval_230\n-208\tval_208\n-348\tval_348\n-24\tval_24\n-463\tval_463\n-431\tval_431\n-179\tval_179\n-172\tval_172\n-42\tval_42\n-129\tval_129\n-158\tval_158\n-119\tval_119\n-496\tval_496\n 0\tval_0\n-322\tval_322\n-197\tval_197\n-468\tval_468\n-393\tval_393\n-454\tval_454\n-100\tval_100\n-298\tval_298\n-199\tval_199\n-191\tval_191\n-418\tval_418\n-96\tval_96\n-26\tval_26\n-165\tval_165\n-327\tval_327\n-230\tval_230\n-205\tval_205\n-120\tval_120\n-131\tval_131\n-51\tval_51\n-404\tval_404\n-43\tval_43\n-436\tval_436\n-156\tval_156\n-469\tval_469\n-468\tval_468\n-308\tval_308\n-95\tval_95\n-196\tval_196\n-288\tval_288\n-481\tval_481\n-457\tval_457\n-98\tval_98\n-282\tval_282\n-197\tval_197\n-187\tval_187\n-318\tval_318\n-318\tval_318\n-409\tval_409\n-470\tval_470\n-137\tval_137\n-369\tval_369\n-316\tval_316\n-169\tval_169\n-413\tval_413\n-85\tval_85\n-77\tval_77\n 0\tval_0\n-490\tval_490\n-87\tval_87\n-364\tval_364\n-179\tval_179\n-118\tval_118\n-134\tval_134\n-395\tval_395\n-282\tval_282\n-138\tval_138\n-238\tval_238\n-419\tval_419\n-15\tval_15\n-118\tval_118\n-72\tval_72\n-90\tval_90\n-307\tval_307\n-19\tval_19\n-435\tval_435\n 10\tval_10\n-277\tval_277\n-273\tval_273\n-306\tval_306\n-224\tval_224\n-309\tval_309\n-389\tval_389\n-327\tval_327\n-242\tval_242\n-369\tval_369\n-392\tval_392\n-272\tval_272\n-331\tval_331\n-401\tval_401\n-242\tval_242\n-452\tval_452\n-177\tval_177\n-226\tval_226\n-5\tval_5\n-497\tval_497\n-402\tval_402\n-396\tval_396\n-317\tval_317\n-395\tval_395\n-58\tval_58\n-35\tval_35\n-336\tval_336\n-95\tval_95\n-11\tval_11\n-168\tval_168\n-34\tval_34\n-229\tval_229\n-233\tval_233\n-143\tval_143\n-472\tval_472\n-322\tval_322\n-498\tval_498\n-160\tval_160\n-195\tval_195\n-42\tval_42\n-321\tval_321\n-430\tval_430\n-119\tval_119\n-489\tval_489\n-458\tval_458\n-78\tval_78\n-76\tval_76\n-41\tval_41\n-223\tval_223\n-492\tval_492\n-149\tval_149\n-449\tval_449\n-218\tval_218\n-228\tval_228\n-138\tval_138\n-453\tval_453\n-30\tval_30\n-209\tval_209\n-64\tval_64\n-468\tval_468\n-76\tval_76\n-74\tval_74\n-342\tval_342\n-69\tval_69\n-230\tval_230\n-33\tval_33\n-368\tval_368\n-103\tval_103\n-296\tval_296\n-113\tval_113\n-216\tval_216\n-367\tval_367\n-344\tval_344\n-167\tval_167\n-274\tval_274\n-219\tval_219\n-239\tval_239\n-485\tval_485\n-116\tval_116\n-223\tval_223\n-256\tval_256\n-263\tval_263\n-70\tval_70\n-487\tval_487\n-480\tval_480\n-401\tval_401\n-288\tval_288\n-191\tval_191\n-5\tval_5\n-244\tval_244\n-438\tval_438\n-128\tval_128\n-467\tval_467\n-432\tval_432\n-202\tval_202\n-316\tval_316\n-229\tval_229\n-469\tval_469\n-463\tval_463\n-280\tval_280\n-2\tval_2\n-35\tval_35\n-283\tval_283\n-331\tval_331\n-235\tval_235\n-80\tval_80\n-44\tval_44\n-193\tval_193\n-321\tval_321\n-335\tval_335\n-104\tval_104\n-466\tval_466\n-366\tval_366\n-175\tval_175\n-403\tval_403\n-483\tval_483\n-53\tval_53\n-105\tval_105\n-257\tval_257\n-406\tval_406\n-409\tval_409\n-190\tval_190\n-406\tval_406\n-401\tval_401\n-114\tval_114\n-258\tval_258\n-90\tval_90\n-203\tval_203\n-262\tval_262\n-348\tval_348\n-424\tval_424\n-12\tval_12\n-396\tval_396\n-201\tval_201\n-217\tval_217\n-164\tval_164\n-431\tval_431\n-454\tval_454\n-478\tval_478\n-298\tval_298\n-125\tval_125\n-431\tval_431\n-164\tval_164\n-424\tval_424\n-187\tval_187\n-382\tval_382\n-5\tval_5\n-70\tval_70\n-397\tval_397\n-480\tval_480\n-291\tval_291\n-24\tval_24\n-351\tval_351\n-255\tval_255\n-104\tval_104\n-70\tval_70\n-163\tval_163\n-438\tval_438\n-119\tval_119\n-414\tval_414\n-200\tval_200\n-491\tval_491\n-237\tval_237\n-439\tval_439\n-360\tval_360\n-248\tval_248\n-479\tval_479\n-305\tval_305\n-417\tval_417\n-199\tval_199\n-444\tval_444\n-120\tval_120\n-429\tval_429\n-169\tval_169\n-443\tval_443\n-323\tval_323\n-325\tval_325\n-277\tval_277\n-230\tval_230\n-478\tval_478\n-178\tval_178\n-468\tval_468\n-310\tval_310\n-317\tval_317\n-333\tval_333\n-493\tval_493\n-460\tval_460\n-207\tval_207\n-249\tval_249\n-265\tval_265\n-480\tval_480\n-83\tval_83\n-136\tval_136\n-353\tval_353\n-172\tval_172\n-214\tval_214\n-462\tval_462\n-233\tval_233\n-406\tval_406\n-133\tval_133\n-175\tval_175\n-189\tval_189\n-454\tval_454\n-375\tval_375\n-401\tval_401\n-421\tval_421\n-407\tval_407\n-384\tval_384\n-256\tval_256\n-26\tval_26\n-134\tval_134\n-67\tval_67\n-384\tval_384\n-379\tval_379\n-18\tval_18\n-462\tval_462\n-492\tval_492\n+10\tval_10\n+100\tval_100\n+100\tval_100\n+100\tval_100\n 100\tval_100\n-298\tval_298\n-9\tval_9\n-341\tval_341\n-498\tval_498\n-146\tval_146\n-458\tval_458\n-362\tval_362\n-186\tval_186\n-285\tval_285\n-348\tval_348\n-167\tval_167\n-18\tval_18\n-273\tval_273\n-183\tval_183\n-281\tval_281\n-344\tval_344\n-97\tval_97\n-469\tval_469\n-315\tval_315\n-84\tval_84\n-28\tval_28\n-37\tval_37\n-448\tval_448\n-152\tval_152\n-348\tval_348\n-307\tval_307\n-194\tval_194\n-414\tval_414\n-477\tval_477\n-222\tval_222\n-126\tval_126\n-90\tval_90\n-169\tval_169\n-403\tval_403\n-400\tval_400\n-200\tval_200\n-97\tval_97\n+100\tval_100\n+100\tval_100\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n PREHOOK: query: explain\n with u as (select * from src union all select * from src)\n select count(*) from (select u1.key as k1, u2.key as k2 from\n@@ -1736,16 +245,934 @@ STAGE PLANS:\n     Fetch Operator\n       limit: -1\n \n-PREHOOK: query: with u as (select * from src union all select * from src)\n-select count(*) from (select u1.key as k1, u2.key as k2 from\n+PREHOOK: query: create table ut as\n+with u as (select * from src union all select * from src)\n+select count(*) as cnt from (select u1.key as k1, u2.key as k2 from\n+u as u1 join u as u2 on (u1.key = u2.key)) a\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+with u as (select * from src union all select * from src)\n+select count(*) as cnt from (select u1.key as k1, u2.key as k2 from\n u as u1 join u as u2 on (u1.key = u2.key)) a\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by cnt limit 20\n PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by cnt limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+4112\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 2 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS)\n+        Map 4 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s1\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+        Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      condition expressions:\n+                        0 {key}\n+                        1 {_col0}\n+                      keys:\n+                        0 key (type: string)\n+                        1 _col0 (type: string)\n+                      outputColumnNames: _col0, _col4\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col4 (type: string)\n+                        outputColumnNames: _col0, _col1\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      condition expressions:\n+                        0 {key}\n+                        1 {_col0}\n+                      keys:\n+                        0 key (type: string)\n+                        1 _col0 (type: string)\n+                      outputColumnNames: _col0, _col4\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col4 (type: string)\n+                        outputColumnNames: _col0, _col1\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 3 \n+            Vertex: Union 3\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+PREHOOK: type: CREATETABLE_AS_SELECT\n PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by skey, ukey limit 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n #### A masked pattern was here ####\n-POSTHOOK: query: with u as (select * from src union all select * from src)\n-select count(*) from (select u1.key as k1, u2.key as k2 from\n-u as u1 join u as u2 on (u1.key = u2.key)) a\n+POSTHOOK: query: select * from ut order by skey, ukey limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+10\t10\n+10\t10\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 2 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 5 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE)\n+        Map 7 <- Map 1 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 8 (BROADCAST_EDGE)\n+        Map 9 <- Map 1 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 10 (BROADCAST_EDGE)\n+        Reducer 4 <- Union 3 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s1\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+        Map 10 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s5\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s2\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Map Join Operator\n+                        condition map:\n+                             Inner Join 0 to 1\n+                             Inner Join 1 to 2\n+                        condition expressions:\n+                          0 {key}\n+                          1 {_col0}\n+                          2 {key}\n+                        keys:\n+                          0 key (type: string)\n+                          1 _col0 (type: string)\n+                          2 key (type: string)\n+                        outputColumnNames: _col0, _col4, _col5\n+                        Select Operator\n+                          expressions: _col0 (type: string), _col4 (type: string), _col5 (type: string)\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions: _col2 (type: string)\n+                            sort order: +\n+                            value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s3\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 6 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s7\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 7 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s6\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Map Join Operator\n+                        condition map:\n+                             Inner Join 0 to 1\n+                             Inner Join 1 to 2\n+                        condition expressions:\n+                          0 {key}\n+                          1 {_col0}\n+                          2 {key}\n+                        keys:\n+                          0 key (type: string)\n+                          1 _col0 (type: string)\n+                          2 key (type: string)\n+                        outputColumnNames: _col0, _col4, _col5\n+                        Select Operator\n+                          expressions: _col0 (type: string), _col4 (type: string), _col5 (type: string)\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions: _col2 (type: string)\n+                            sort order: +\n+                            value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+        Map 8 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s8\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+        Map 9 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s4\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Map Join Operator\n+                        condition map:\n+                             Inner Join 0 to 1\n+                             Inner Join 1 to 2\n+                        condition expressions:\n+                          0 {key}\n+                          1 {_col0}\n+                          2 {key}\n+                        keys:\n+                          0 key (type: string)\n+                          1 _col0 (type: string)\n+                          2 key (type: string)\n+                        outputColumnNames: _col0, _col4, _col5\n+                        Select Operator\n+                          expressions: _col0 (type: string), _col4 (type: string), _col5 (type: string)\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions: _col2 (type: string)\n+                            sort order: +\n+                            value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+        Reducer 4 \n+            Reduce Operator Tree:\n+              Extract\n+                Statistics: Num rows: 415 Data size: 42193 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 415 Data size: 42193 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 3 \n+            Vertex: Union 3\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by skey, ukey, lkey limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by skey, ukey, lkey limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS), Map 3 (BROADCAST_EDGE)\n+        Map 4 <- Union 2 (CONTAINS), Map 5 (BROADCAST_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s2\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s3\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s4\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s5\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by key limit 30\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n #### A masked pattern was here ####\n-4112\n+POSTHOOK: query: select * from ut order by key limit 30\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+10\n+10\n+100\n+100\n+100\n+100\n+100\n+100\n+100\n+100\n+103\n+103\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain\n+select * from\n+(select * from src union all select * from src) u\n+left outer join src s on u.key = s.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select * from\n+(select * from src union all select * from src) u\n+left outer join src s on u.key = s.key\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS), Map 3 (BROADCAST_EDGE)\n+        Map 4 <- Map 3 (BROADCAST_EDGE), Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Left Outer Join0 to 1\n+                      condition expressions:\n+                        0 {_col0} {_col1}\n+                        1 {key} {value}\n+                      keys:\n+                        0 _col0 (type: string)\n+                        1 key (type: string)\n+                      outputColumnNames: _col0, _col1, _col2, _col3\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2, _col3\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string), value (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string), value (type: string)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Left Outer Join0 to 1\n+                      condition expressions:\n+                        0 {_col0} {_col1}\n+                        1 {key} {value}\n+                      keys:\n+                        0 _col0 (type: string)\n+                        1 key (type: string)\n+                      outputColumnNames: _col0, _col1, _col2, _col3\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2, _col3\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: explain\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (BROADCAST_EDGE)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: string)\n+                      value expressions: _col0 (type: string)\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Map Join Operator\n+                    condition map:\n+                         Right Outer Join0 to 1\n+                    condition expressions:\n+                      0 {_col0}\n+                      1 {key}\n+                    keys:\n+                      0 _col0 (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0, _col2\n+                    Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: _col0 (type: string), _col2 (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                      File Output Operator\n+                        compressed: false\n+                        Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                        table:\n+                            input format: org.apache.hadoop.mapred.TextInputFormat\n+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: string)\n+                      value expressions: _col0 (type: string)\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by ukey, skey limit 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by ukey, skey limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+10\t10\n+10\t10\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut", "filename": "ql/src/test/results/clientpositive/tez/tez_union.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/vectorization_15.q.out", "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/vectorization_15.q.out", "sha": "717a3cb73cb8f6db5734fb14b9293f721fc8a11f", "changes": 38, "status": "modified", "deletions": 19, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/vectorization_15.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e", "patch": "@@ -77,32 +77,32 @@ NULL\ttrue\t10419.0\t10\tNULL\t-721614386\tNULL\tNULL\t7.2161435972E8\t10419.0\t828862.706\n NULL\ttrue\t14519.0\t100xJdkyc\tNULL\t729277608\tNULL\tNULL\t-7.2927763428E8\t14519.0\t1155030.007\tNULL\tNULL\tNULL\t-23.0\tNULL\t0.0\tNULL\tNULL\t7.2927763428E8\t0.0\n -62.0\tNULL\t15601.0\tNULL\t-62\tNULL\t1969-12-31 16:00:09.889\t0.0\tNULL\t15601.0\t1241106.353\t33.0\t0.0\t0.0\t-23.0\t62\tNULL\tNULL\t-23\tNULL\tNULL\n -51.0\tNULL\t-200.0\tNULL\t-51\tNULL\t1969-12-31 15:59:55.423\t0.0\tNULL\t-200.0\t-15910.599999999999\t33.0\t0.0\t0.0\t-23.0\t51\tNULL\tNULL\t-23\tNULL\tNULL\n--51.0\tfalse\tNULL\t10\t-51\t1058319346\t1969-12-31 16:00:08.451\t0.0\t-1.05831937228E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.058319397E9\t-23\t1.05831937228E9\t0.0\n--51.0\tfalse\tNULL\t10TYIE5S35U6dj3N\t-51\t-469581869\t1969-12-31 16:00:08.451\t0.0\t4.6958184272E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-4.69581818E8\t-23\t-4.6958184272E8\t0.0\n--51.0\tfalse\tNULL\t1Lh6Uoq3WhNtOqQHu7WN7U\t-51\t-352637533\t1969-12-31 16:00:08.451\t0.0\t3.5263750672E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.52637482E8\t-23\t-3.5263750672E8\t0.0\n--51.0\ttrue\tNULL\t04Y1mA17\t-51\t-114647521\t1969-12-31 16:00:08.451\t0.0\t1.1464749472E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-1.1464747E8\t-23\t-1.1464749472E8\t0.0\n+-51.0\tfalse\tNULL\t10\t-51\t1058319346\t1969-12-31 16:00:08.451\t0.0\t-1.05831937228E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.05831942E9\t-23\t1.05831937228E9\t0.0\n+-51.0\tfalse\tNULL\t10TYIE5S35U6dj3N\t-51\t-469581869\t1969-12-31 16:00:08.451\t0.0\t4.6958184272E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-4.69581792E8\t-23\t-4.6958184272E8\t0.0\n+-51.0\tfalse\tNULL\t1Lh6Uoq3WhNtOqQHu7WN7U\t-51\t-352637533\t1969-12-31 16:00:08.451\t0.0\t3.5263750672E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.52637472E8\t-23\t-3.5263750672E8\t0.0\n+-51.0\ttrue\tNULL\t04Y1mA17\t-51\t-114647521\t1969-12-31 16:00:08.451\t0.0\t1.1464749472E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-1.14647472E8\t-23\t-1.1464749472E8\t0.0\n -51.0\ttrue\tNULL\t10Wu570aLPO0p02P17FeH\t-51\t405338893\t1969-12-31 16:00:08.451\t0.0\t-4.0533891928E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t4.05338944E8\t-23\t4.0533891928E8\t0.0\n -51.0\ttrue\tNULL\t3cQp060\t-51\t-226923315\t1969-12-31 16:00:08.451\t0.0\t2.2692328872E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-2.26923264E8\t-23\t-2.2692328872E8\t0.0\n--51.0\ttrue\tNULL\t8EPG0Xi307qd\t-51\t-328662044\t1969-12-31 16:00:08.451\t0.0\t3.2866201772E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.28661993E8\t-23\t-3.2866201772E8\t0.0\n--51.0\ttrue\tNULL\t8iHtdkJ6d\t-51\t1006818344\t1969-12-31 16:00:08.451\t0.0\t-1.00681837028E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.006818395E9\t-23\t1.00681837028E9\t0.0\n--51.0\ttrue\tNULL\tQiOcvR0kt6r7f0R7fiPxQTCU\t-51\t266531954\t1969-12-31 16:00:08.451\t0.0\t-2.6653198028E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t2.66532005E8\t-23\t2.6653198028E8\t0.0\n--51.0\ttrue\tNULL\tYbpj38RTTYl7CnJXPNx1g4C\t-51\t-370919370\t1969-12-31 16:00:08.451\t0.0\t3.7091934372E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.70919319E8\t-23\t-3.7091934372E8\t0.0\n+-51.0\ttrue\tNULL\t8EPG0Xi307qd\t-51\t-328662044\t1969-12-31 16:00:08.451\t0.0\t3.2866201772E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.28661984E8\t-23\t-3.2866201772E8\t0.0\n+-51.0\ttrue\tNULL\t8iHtdkJ6d\t-51\t1006818344\t1969-12-31 16:00:08.451\t0.0\t-1.00681837028E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.00681843E9\t-23\t1.00681837028E9\t0.0\n+-51.0\ttrue\tNULL\tQiOcvR0kt6r7f0R7fiPxQTCU\t-51\t266531954\t1969-12-31 16:00:08.451\t0.0\t-2.6653198028E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t2.66532E8\t-23\t2.6653198028E8\t0.0\n+-51.0\ttrue\tNULL\tYbpj38RTTYl7CnJXPNx1g4C\t-51\t-370919370\t1969-12-31 16:00:08.451\t0.0\t3.7091934372E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.70919296E8\t-23\t-3.7091934372E8\t0.0\n -48.0\tNULL\t-7196.0\tNULL\t-48\tNULL\t1969-12-31 16:00:06.337\t0.0\tNULL\t-7196.0\t-572463.388\t33.0\t0.0\t0.0\t-23.0\t48\tNULL\tNULL\t-23\tNULL\tNULL\n -6.0\tNULL\t-200.0\tNULL\t-6\tNULL\t1969-12-31 15:59:56.094\t0.0\tNULL\t-200.0\t-15910.599999999999\t3.0\t0.0\t0.0\t-23.0\t6\tNULL\tNULL\t-5\tNULL\tNULL\n 5.0\tNULL\t15601.0\tNULL\t5\tNULL\t1969-12-31 16:00:00.959\t0.0\tNULL\t15601.0\t1241106.353\t3.0\t0.0\t0.0\t-23.0\t-5\tNULL\tNULL\t-3\tNULL\tNULL\n-8.0\tfalse\tNULL\t10V3pN5r5lI2qWl2lG103\t8\t-362835731\t1969-12-31 16:00:15.892\t0.0\t3.6283570472E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-3.62835739E8\t-7\t-3.6283570472E8\t0.0\n-8.0\tfalse\tNULL\t10c4qt584m5y6uWT\t8\t-183000142\t1969-12-31 16:00:15.892\t0.0\t1.8300011572E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.8300015E8\t-7\t-1.8300011572E8\t0.0\n+8.0\tfalse\tNULL\t10V3pN5r5lI2qWl2lG103\t8\t-362835731\t1969-12-31 16:00:15.892\t0.0\t3.6283570472E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-3.62835744E8\t-7\t-3.6283570472E8\t0.0\n+8.0\tfalse\tNULL\t10c4qt584m5y6uWT\t8\t-183000142\t1969-12-31 16:00:15.892\t0.0\t1.8300011572E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.8300016E8\t-7\t-1.8300011572E8\t0.0\n 8.0\tfalse\tNULL\t8GloEukQ0c68JDmnYL53\t8\t-722873402\t1969-12-31 16:00:15.892\t0.0\t7.2287337572E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-7.2287341E8\t-7\t-7.2287337572E8\t0.0\n 8.0\tfalse\tNULL\tkA0XH5C5\t8\t-503903864\t1969-12-31 16:00:15.892\t0.0\t5.0390383772E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-5.03903872E8\t-7\t-5.0390383772E8\t0.0\n-8.0\ttrue\tNULL\t100VTM7PEW8GH1uE\t8\t88129338\t1969-12-31 16:00:15.892\t0.0\t-8.812936428E7\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t8.812933E7\t-7\t8.812936428E7\t0.0\n-8.0\ttrue\tNULL\t1062158y\t8\t-1005155523\t1969-12-31 16:00:15.892\t0.0\t1.00515549672E9\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.005155531E9\t-7\t-1.00515549672E9\t0.0\n-8.0\ttrue\tNULL\t1063cEnGjSal\t8\t-624769630\t1969-12-31 16:00:15.892\t0.0\t6.2476960372E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-6.24769638E8\t-7\t-6.2476960372E8\t0.0\n-8.0\ttrue\tNULL\t4kMasVoB7lX1wc5i64bNk\t8\t683567667\t1969-12-31 16:00:15.892\t0.0\t-6.8356769328E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t6.83567659E8\t-7\t6.8356769328E8\t0.0\n-8.0\ttrue\tNULL\tXH6I7A417\t8\t436627202\t1969-12-31 16:00:15.892\t0.0\t-4.3662722828E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t4.36627194E8\t-7\t4.3662722828E8\t0.0\n-11.0\tfalse\tNULL\t10pO8p1LNx4Y\t11\t271296824\t1969-12-31 16:00:02.351\t0.0\t-2.7129685028E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t2.71296813E8\t-1\t2.7129685028E8\t0.0\n-11.0\tfalse\tNULL\t1H6wGP\t11\t-560827082\t1969-12-31 16:00:02.351\t0.0\t5.6082705572E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-5.60827093E8\t-1\t-5.6082705572E8\t0.0\n-11.0\tfalse\tNULL\t2a7V63IL7jK3o\t11\t-325931647\t1969-12-31 16:00:02.351\t0.0\t3.2593162072E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-3.25931658E8\t-1\t-3.2593162072E8\t0.0\n-11.0\ttrue\tNULL\t10\t11\t92365813\t1969-12-31 16:00:02.351\t0.0\t-9.236583928E7\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t9.2365802E7\t-1\t9.236583928E7\t0.0\n+8.0\ttrue\tNULL\t100VTM7PEW8GH1uE\t8\t88129338\t1969-12-31 16:00:15.892\t0.0\t-8.812936428E7\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t8.8129328E7\t-7\t8.812936428E7\t0.0\n+8.0\ttrue\tNULL\t1062158y\t8\t-1005155523\t1969-12-31 16:00:15.892\t0.0\t1.00515549672E9\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.00515552E9\t-7\t-1.00515549672E9\t0.0\n+8.0\ttrue\tNULL\t1063cEnGjSal\t8\t-624769630\t1969-12-31 16:00:15.892\t0.0\t6.2476960372E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-6.247696E8\t-7\t-6.2476960372E8\t0.0\n+8.0\ttrue\tNULL\t4kMasVoB7lX1wc5i64bNk\t8\t683567667\t1969-12-31 16:00:15.892\t0.0\t-6.8356769328E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t6.8356768E8\t-7\t6.8356769328E8\t0.0\n+8.0\ttrue\tNULL\tXH6I7A417\t8\t436627202\t1969-12-31 16:00:15.892\t0.0\t-4.3662722828E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t4.366272E8\t-7\t4.3662722828E8\t0.0\n+11.0\tfalse\tNULL\t10pO8p1LNx4Y\t11\t271296824\t1969-12-31 16:00:02.351\t0.0\t-2.7129685028E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t2.71296832E8\t-1\t2.7129685028E8\t0.0\n+11.0\tfalse\tNULL\t1H6wGP\t11\t-560827082\t1969-12-31 16:00:02.351\t0.0\t5.6082705572E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-5.6082707E8\t-1\t-5.6082705572E8\t0.0\n+11.0\tfalse\tNULL\t2a7V63IL7jK3o\t11\t-325931647\t1969-12-31 16:00:02.351\t0.0\t3.2593162072E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-3.25931648E8\t-1\t-3.2593162072E8\t0.0\n+11.0\ttrue\tNULL\t10\t11\t92365813\t1969-12-31 16:00:02.351\t0.0\t-9.236583928E7\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t9.2365808E7\t-1\t9.236583928E7\t0.0\n 21.0\tNULL\t15601.0\tNULL\t21\tNULL\t1969-12-31 16:00:14.256\t0.0\tNULL\t15601.0\t1241106.353\t12.0\t0.0\t0.0\t-23.0\t-21\tNULL\tNULL\t-2\tNULL\tNULL\n 32.0\tNULL\t-200.0\tNULL\t32\tNULL\t1969-12-31 16:00:02.445\t0.0\tNULL\t-200.0\t-15910.599999999999\t1.0\t0.0\t0.0\t-23.0\t-32\tNULL\tNULL\t-23\tNULL\tNULL\n 36.0\tNULL\t-200.0\tNULL\t36\tNULL\t1969-12-31 16:00:00.554\t0.0\tNULL\t-200.0\t-15910.599999999999\t33.0\t0.0\t0.0\t-23.0\t-36\tNULL\tNULL\t-23\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/tez/vectorization_15.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b", "parent": "https://github.com/apache/hive/commit/62cd2fa79ca064312d93831899ffbdf7c6c56b2a", "message": "HIVE-6674 : show grant on all throws NPE (Navis via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1582136 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_224", "file": [{"additions": 60, "raw_url": "https://github.com/apache/hive/raw/abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "blob_url": "https://github.com/apache/hive/blob/abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "sha": "7ae41e557b6333ba548f6616c05cc66f5593b216", "changes": 90, "status": "modified", "deletions": 30, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b", "patch": "@@ -4146,11 +4146,11 @@ public boolean revokePrivileges(PrivilegeBag privileges)\n     try {\n       openTransaction();\n       LOG.debug(\"Executing listPrincipalDBGrants\");\n-      Query query = pm.newQuery(MDBPrivilege.class,\n-          \"principalName == t1 && principalType == t2 && database.name == t3\");\n-      query\n-          .declareParameters(\"java.lang.String t1, java.lang.String t2, java.lang.String t3\");\n-      mSecurityDBList = (List<MDBPrivilege>) query.executeWithArray(principalName, principalType.toString(), dbName);\n+        Query query = pm.newQuery(MDBPrivilege.class,\n+            \"principalName == t1 && principalType == t2 && database.name == t3\");\n+        query\n+            .declareParameters(\"java.lang.String t1, java.lang.String t2, java.lang.String t3\");\n+        mSecurityDBList = (List<MDBPrivilege>) query.executeWithArray(principalName, principalType.toString(), dbName);\n       LOG.debug(\"Done executing query for listPrincipalDBGrants\");\n       pm.retrieveAll(mSecurityDBList);\n       success = commitTransaction();\n@@ -4199,11 +4199,16 @@ public boolean revokePrivileges(PrivilegeBag privileges)\n     try {\n       openTransaction();\n       LOG.debug(\"Executing listPrincipalAllDBGrant\");\n-      Query query = pm.newQuery(MDBPrivilege.class,\n-          \"principalName == t1 && principalType == t2\");\n-      query\n-          .declareParameters(\"java.lang.String t1, java.lang.String t2\");\n-      mSecurityDBList = (List<MDBPrivilege>) query.execute(principalName, principalType.toString());\n+      if (principalName != null && principalType != null) {\n+        Query query = pm.newQuery(MDBPrivilege.class,\n+            \"principalName == t1 && principalType == t2\");\n+        query\n+            .declareParameters(\"java.lang.String t1, java.lang.String t2\");\n+        mSecurityDBList = (List<MDBPrivilege>) query.execute(principalName, principalType.toString());\n+      } else {\n+        Query query = pm.newQuery(MDBPrivilege.class);\n+        mSecurityDBList = (List<MDBPrivilege>) query.execute();\n+      }\n       LOG.debug(\"Done executing query for listPrincipalAllDBGrant\");\n       pm.retrieveAll(mSecurityDBList);\n       success = commitTransaction();\n@@ -4604,11 +4609,17 @@ private void dropPartitionGrantsNoTxn(String dbName, String tableName, List<Stri\n     try {\n       openTransaction();\n       LOG.debug(\"Executing listPrincipalPartitionColumnGrantsAll\");\n-      Query query = pm.newQuery(MPartitionColumnPrivilege.class,\n-          \"principalName == t1 && principalType == t2\");\n-      query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n-      List<MPartitionColumnPrivilege> mSecurityTabPartList = (List<MPartitionColumnPrivilege>)\n-          query.executeWithArray(principalName, principalType.toString());\n+      List<MPartitionColumnPrivilege> mSecurityTabPartList;\n+      if (principalName != null && principalType != null) {\n+        Query query = pm.newQuery(MPartitionColumnPrivilege.class,\n+            \"principalName == t1 && principalType == t2\");\n+        query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n+        mSecurityTabPartList = (List<MPartitionColumnPrivilege>)\n+            query.executeWithArray(principalName, principalType.toString());\n+      } else {\n+        Query query = pm.newQuery(MPartitionColumnPrivilege.class);\n+        mSecurityTabPartList = (List<MPartitionColumnPrivilege>) query.execute();\n+      }\n       LOG.debug(\"Done executing query for listPrincipalPartitionColumnGrantsAll\");\n       pm.retrieveAll(mSecurityTabPartList);\n       List<HiveObjectPrivilege> result = convertPartCols(mSecurityTabPartList);\n@@ -4703,11 +4714,17 @@ private void dropPartitionGrantsNoTxn(String dbName, String tableName, List<Stri\n     try {\n       openTransaction();\n       LOG.debug(\"Executing listPrincipalAllTableGrants\");\n-      Query query = pm.newQuery(MTablePrivilege.class,\n-          \"principalName == t1 && principalType == t2\");\n-      query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n-      List<MTablePrivilege> mSecurityTabPartList = (List<MTablePrivilege>) query.execute(\n-          principalName, principalType.toString());\n+      List<MTablePrivilege> mSecurityTabPartList;\n+      if (principalName != null && principalType != null) {\n+        Query query = pm.newQuery(MTablePrivilege.class,\n+            \"principalName == t1 && principalType == t2\");\n+        query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n+        mSecurityTabPartList = (List<MTablePrivilege>) query.execute(\n+            principalName, principalType.toString());\n+      } else {\n+        Query query = pm.newQuery(MTablePrivilege.class);\n+        mSecurityTabPartList = (List<MTablePrivilege>) query.execute();\n+      }\n       LOG.debug(\"Done executing query for listPrincipalAllTableGrants\");\n       pm.retrieveAll(mSecurityTabPartList);\n       List<HiveObjectPrivilege> result = convertTable(mSecurityTabPartList);\n@@ -4798,11 +4815,17 @@ private void dropPartitionGrantsNoTxn(String dbName, String tableName, List<Stri\n     try {\n       openTransaction();\n       LOG.debug(\"Executing listPrincipalPartitionGrantsAll\");\n-      Query query = pm.newQuery(MPartitionPrivilege.class,\n-          \"principalName == t1 && principalType == t2\");\n-      query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n-      List<MPartitionPrivilege> mSecurityTabPartList = (List<MPartitionPrivilege>)\n-          query.execute(principalName, principalType.toString());\n+      List<MPartitionPrivilege> mSecurityTabPartList;\n+      if (principalName != null && principalType != null) {\n+        Query query = pm.newQuery(MPartitionPrivilege.class,\n+            \"principalName == t1 && principalType == t2\");\n+        query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n+        mSecurityTabPartList = (List<MPartitionPrivilege>)\n+            query.execute(principalName, principalType.toString());\n+      } else {\n+        Query query = pm.newQuery(MPartitionPrivilege.class);\n+        mSecurityTabPartList = (List<MPartitionPrivilege>) query.execute();\n+      }\n       LOG.debug(\"Done executing query for listPrincipalPartitionGrantsAll\");\n       pm.retrieveAll(mSecurityTabPartList);\n       List<HiveObjectPrivilege> result = convertPartition(mSecurityTabPartList);\n@@ -4895,11 +4918,18 @@ private void dropPartitionGrantsNoTxn(String dbName, String tableName, List<Stri\n     try {\n       openTransaction();\n       LOG.debug(\"Executing listPrincipalTableColumnGrantsAll\");\n-      Query query = pm.newQuery(MTableColumnPrivilege.class,\n-          \"principalName == t1 && principalType == t2\");\n-      query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n-      List<MTableColumnPrivilege> mSecurityTabPartList = (List<MTableColumnPrivilege>)\n-          query.execute(principalName, principalType.toString());\n+\n+      List<MTableColumnPrivilege> mSecurityTabPartList;\n+      if (principalName != null && principalType != null) {\n+        Query query = pm.newQuery(MTableColumnPrivilege.class,\n+            \"principalName == t1 && principalType == t2\");\n+        query.declareParameters(\"java.lang.String t1, java.lang.String t2\");\n+        mSecurityTabPartList = (List<MTableColumnPrivilege>)\n+            query.execute(principalName, principalType.toString());\n+      } else {\n+        Query query = pm.newQuery(MTableColumnPrivilege.class);\n+        mSecurityTabPartList = (List<MTableColumnPrivilege>) query.execute();\n+      }\n       LOG.debug(\"Done executing query for listPrincipalTableColumnGrantsAll\");\n       pm.retrieveAll(mSecurityTabPartList);\n       List<HiveObjectPrivilege> result = convertTableCols(mSecurityTabPartList);", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b/ql/src/test/queries/clientpositive/authorization_9.q", "blob_url": "https://github.com/apache/hive/blob/abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b/ql/src/test/queries/clientpositive/authorization_9.q", "sha": "1abe659fa447642a20e5e6ca88a779f3ee3581c1", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_9.q?ref=abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b", "patch": "@@ -9,3 +9,9 @@ grant select (key, value) on table dummy to user hive_test_user;\n show grant user hive_test_user on database default;\n show grant user hive_test_user on table dummy;\n show grant user hive_test_user on all;\n+\n+grant select on database default to user hive_test_user2;\n+grant select on table dummy to user hive_test_user2;\n+grant select (key, value) on table dummy to user hive_test_user2;\n+\n+show grant on all;", "filename": "ql/src/test/queries/clientpositive/authorization_9.q"}, {"additions": 28, "raw_url": "https://github.com/apache/hive/raw/abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b/ql/src/test/results/clientpositive/authorization_9.q.out", "blob_url": "https://github.com/apache/hive/blob/abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b/ql/src/test/results/clientpositive/authorization_9.q.out", "sha": "12631d8390d1219f9fa22ddef8cf2a5164ae8011", "changes": 28, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_9.q.out?ref=abdcfc7b015754cd3aeb2a5b3d7e7204df2bf50b", "patch": "@@ -43,3 +43,31 @@ default\t\t\t\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n default\tdummy\t\t\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n default\tdummy\t\tkey\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n default\tdummy\t\tvalue\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n+PREHOOK: query: grant select on database default to user hive_test_user2\n+PREHOOK: type: GRANT_PRIVILEGE\n+POSTHOOK: query: grant select on database default to user hive_test_user2\n+POSTHOOK: type: GRANT_PRIVILEGE\n+PREHOOK: query: grant select on table dummy to user hive_test_user2\n+PREHOOK: type: GRANT_PRIVILEGE\n+PREHOOK: Output: default@dummy\n+POSTHOOK: query: grant select on table dummy to user hive_test_user2\n+POSTHOOK: type: GRANT_PRIVILEGE\n+POSTHOOK: Output: default@dummy\n+PREHOOK: query: grant select (key, value) on table dummy to user hive_test_user2\n+PREHOOK: type: GRANT_PRIVILEGE\n+PREHOOK: Output: default@dummy\n+POSTHOOK: query: grant select (key, value) on table dummy to user hive_test_user2\n+POSTHOOK: type: GRANT_PRIVILEGE\n+POSTHOOK: Output: default@dummy\n+PREHOOK: query: show grant on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant on all\n+POSTHOOK: type: SHOW_GRANT\n+default\t\t\t\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n+default\t\t\t\thive_test_user2\tUSER\tSelect\tfalse\t-1\thive_test_user\n+default\tdummy\t\t\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n+default\tdummy\t\t\thive_test_user2\tUSER\tSelect\tfalse\t-1\thive_test_user\n+default\tdummy\t\tkey\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n+default\tdummy\t\tkey\thive_test_user2\tUSER\tSelect\tfalse\t-1\thive_test_user\n+default\tdummy\t\tvalue\thive_test_user\tUSER\tSelect\tfalse\t-1\thive_test_user\n+default\tdummy\t\tvalue\thive_test_user2\tUSER\tSelect\tfalse\t-1\thive_test_user", "filename": "ql/src/test/results/clientpositive/authorization_9.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc", "parent": "https://github.com/apache/hive/commit/a39be7254a38c010154e7ebd65e8282db804ba80", "message": "HIVE-6716: ORC struct throws NPE for tables with inner structs having null values (Prasanth J via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1581007 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_225", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "blob_url": "https://github.com/apache/hive/blob/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "sha": "685b386606cec0e85eb3bf5fa0e7cf0e4fd6f88f", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java?ref=e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc", "patch": "@@ -207,11 +207,17 @@ public StructField getStructFieldRef(String s) {\n \n     @Override\n     public Object getStructFieldData(Object object, StructField field) {\n+      if (object == null) {\n+        return null;\n+      }\n       return ((OrcStruct) object).fields[((Field) field).offset];\n     }\n \n     @Override\n     public List<Object> getStructFieldsDataAsList(Object object) {\n+      if (object == null) {\n+        return null;\n+      }\n       OrcStruct struct = (OrcStruct) object;\n       List<Object> result = new ArrayList<Object>(struct.fields.length);\n       for (Object child: struct.fields) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/70d6eefe775453553ec804889c9319c6dd88f4cd", "parent": "https://github.com/apache/hive/commit/6745354ab2db240bd87f9a884814fff21b3146c5", "message": "HIVE-6704 : date_add()/date_sub()/datediff() fail with NPE with null input (Jason Dere via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1580538 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_226", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java", "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java", "sha": "3168385a4939c4a9217c356d5777da8b84a57485", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd", "patch": "@@ -96,6 +96,9 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n     ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n     switch (inputType1) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n+      inputType1 = PrimitiveCategory.STRING;\n       textConverter = ObjectInspectorConverters.getConverter(\n         (PrimitiveObjectInspector) arguments[0],\n         PrimitiveObjectInspectorFactory.writableStringObjectInspector);\n@@ -129,7 +132,14 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n \n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n     IntWritable toBeAdded = (IntWritable) intWritableConverter.convert(arguments[1].get());\n+    if (toBeAdded == null) {\n+      return null;\n+    }\n+\n     switch (inputType1) {\n     case STRING:\n       String dateString = textConverter.convert(arguments[0].get()).toString();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java", "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java", "sha": "5d8bd0d66580ca7d8a3577f7b5f069d7bcf044d2", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd", "patch": "@@ -113,9 +113,14 @@ private Date convertToDate(PrimitiveCategory inputType, Converter converter, Def\n     throws HiveException {\n     assert(converter != null);\n     assert(argument != null);\n+    if (argument.get() == null) {\n+      return null;\n+    }\n     Date date = new Date();\n     switch (inputType) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n       String dateString = converter.convert(argument.get()).toString();\n       try {\n         date = formatter.parse(dateString);\n@@ -149,6 +154,8 @@ private Converter checkArguments(ObjectInspector[] arguments, int i) throws UDFA\n     Converter converter;\n     switch (inputType) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n       converter = ObjectInspectorConverters.getConverter(\n         (PrimitiveObjectInspector) arguments[i],\n         PrimitiveObjectInspectorFactory.writableStringObjectInspector);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java", "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java", "sha": "1685a86dc5053bcfadda0da940d0c338a2faa2c4", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd", "patch": "@@ -96,6 +96,9 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n     ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n     switch (inputType1) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n+      inputType1 = PrimitiveCategory.STRING;\n       textConverter = ObjectInspectorConverters.getConverter(\n         (PrimitiveObjectInspector) arguments[0],\n         PrimitiveObjectInspectorFactory.writableStringObjectInspector);\n@@ -129,7 +132,14 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n \n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n     IntWritable toBeSubed = (IntWritable) intWritableConverter.convert(arguments[1].get());\n+    if (toBeSubed == null) {\n+      return null;\n+    }\n+\n     switch (inputType1) {\n     case STRING:\n       String dateString = textConverter.convert(arguments[0].get()).toString();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java", "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java", "sha": "53b8f939ec260e2f04b92f6aac4d27c6ebd21680", "changes": 30, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd", "patch": "@@ -47,6 +47,16 @@ public void testStringToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_add() test for STRING failed \", \"2009-07-22\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -63,6 +73,16 @@ public void testTimestampToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_add() test for TIMESTAMP failed \", \"2009-07-23\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -79,6 +99,16 @@ public void testDateWritablepToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_add() test for DATEWRITABLE failed \", \"2009-07-24\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java", "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java", "sha": "849e70c17a1e2c839bf857ca22b5b61cda6de9e1", "changes": 30, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd", "patch": "@@ -48,6 +48,16 @@ public void testStringToDate() throws HiveException {\n     IntWritable output = (IntWritable) udf.evaluate(args);\n \n     assertEquals(\"date_iff() test for STRING failed \", \"-2\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -65,6 +75,16 @@ public void testTimestampToDate() throws HiveException {\n     IntWritable output = (IntWritable) udf.evaluate(args);\n \n     assertEquals(\"datediff() test for TIMESTAMP failed \", \"3\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -81,6 +101,16 @@ public void testDateWritablepToDate() throws HiveException {\n     IntWritable output = (IntWritable) udf.evaluate(args);\n \n     assertEquals(\"datediff() test for DATEWRITABLE failed \", \"10\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java", "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java", "sha": "d419ef28c66f2a4f2c95a1a78d0bdc337e5276c6", "changes": 30, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd", "patch": "@@ -47,6 +47,16 @@ public void testStringToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_sub() test for STRING failed \", \"2009-07-18\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -63,6 +73,16 @@ public void testTimestampToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_sub() test for TIMESTAMP failed \", \"2009-07-17\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -79,6 +99,16 @@ public void testDateWritablepToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_sub() test for DATEWRITABLE failed \", \"2009-07-16\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4f41a46adfb84f5681fc8503fb749e1cdf463cd0", "parent": "https://github.com/apache/hive/commit/e0c0839f766e74c306a3122dac7b47265b0820c7", "message": "HIVE-6673 : sql std auth - show grant statement for all principals throws NPE (Thejas Nair via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1580526 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_227", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "blob_url": "https://github.com/apache/hive/blob/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "sha": "7e78cd51bcb216851cd1d43359edfa9c11a34b46", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=4f41a46adfb84f5681fc8503fb749e1cdf463cd0", "patch": "@@ -890,6 +890,9 @@ private HivePrivilegeObjectType getPrivObjectType(PrivilegeObjectDesc privSubjec\n   }\n \n   private HivePrincipal getHivePrincipal(PrincipalDesc principal) throws HiveException {\n+    if (principal == null) {\n+      return null;\n+    }\n     return new HivePrincipal(principal.getName(),\n         AuthorizationUtils.getHivePrincipalType(principal.getType()));\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java", "blob_url": "https://github.com/apache/hive/blob/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java", "sha": "a95d784f4ca8133228eb4737bdfbc10f82087bca", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java?ref=4f41a46adfb84f5681fc8503fb749e1cdf463cd0", "patch": "@@ -153,6 +153,9 @@ public static HiveObjectType getThriftHiveObjType(HivePrivilegeObjectType type)\n    * @throws HiveException\n    */\n   public static HiveObjectRef getThriftHiveObjectRef(HivePrivilegeObject privObj) throws HiveException {\n+    if (privObj == null) {\n+      return null;\n+    }\n     HiveObjectType objType = getThriftHiveObjType(privObj.getType());\n     return new HiveObjectRef(objType, privObj.getDbname(), privObj.getTableViewURI(), null, null);\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java", "blob_url": "https://github.com/apache/hive/blob/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java", "sha": "5b24578cd02ad30cd0e4ead7ddd637335333f037", "changes": 12, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java?ref=4f41a46adfb84f5681fc8503fb749e1cdf463cd0", "patch": "@@ -17,8 +17,6 @@\n  */\n package org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd;\n \n-import com.google.common.collect.ImmutableSet;\n-\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -56,6 +54,8 @@\n import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveRoleGrant;\n import org.apache.thrift.TException;\n \n+import com.google.common.collect.ImmutableSet;\n+\n /**\n  * Implements functionality of access control statements for sql standard based\n  * authorization\n@@ -368,9 +368,13 @@ public void revokeRole(List<HivePrincipal> hivePrincipals, List<String> roleName\n     try {\n       IMetaStoreClient mClient = metastoreClientFactory.getHiveMetastoreClient();\n       List<HivePrivilegeInfo> resPrivInfos = new ArrayList<HivePrivilegeInfo>();\n+      String principalName = principal == null ? null : principal.getName();\n+      PrincipalType principalType = principal == null ? null :\n+          AuthorizationUtils.getThriftPrincipalType(principal.getType());\n+\n       // get metastore/thrift privilege object using metastore api\n-      List<HiveObjectPrivilege> msObjPrivs = mClient.list_privileges(principal.getName(),\n-          AuthorizationUtils.getThriftPrincipalType(principal.getType()),\n+      List<HiveObjectPrivilege> msObjPrivs = mClient.list_privileges(principalName,\n+          principalType,\n           SQLAuthorizationUtils.getThriftHiveObjectRef(privObj));\n \n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q", "blob_url": "https://github.com/apache/hive/blob/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q", "sha": "fdbeed4bf0311b99209f744f18a0a8ad8fc587cb", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q?ref=4f41a46adfb84f5681fc8503fb749e1cdf463cd0", "patch": "@@ -4,7 +4,10 @@ set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.Sessi\n set hive.security.authorization.enabled=true;\n set user.name=user1;\n \n+-- Test view authorization , and 'show grant' variants\n+\n create table t1(i int, j int, k int);\n+show grant on table t1;\n \n -- protecting certain columns\n create view vt1 as select i,k from t1;\n@@ -36,6 +39,9 @@ show grant user user2 on all;\n revoke all on vt2 from user user2;\n show grant user user2 on table vt2;\n \n+show grant on table vt2;\n+\n+\n revoke select on table vt1 from user user2;\n show grant user user2 on table vt1;\n \n@@ -57,3 +63,4 @@ show grant role role_v on table vt2;\n \n revoke delete on table vt2 from role role_v;\n show grant role role_v on table vt2;\n+show grant on table vt2;", "filename": "ql/src/test/queries/clientpositive/authorization_view_sqlstd.q"}, {"additions": 33, "raw_url": "https://github.com/apache/hive/raw/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out", "blob_url": "https://github.com/apache/hive/blob/4f41a46adfb84f5681fc8503fb749e1cdf463cd0/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out", "sha": "0a986e603d654c203c0b3ca509bdd75c78c5a4b4", "changes": 35, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out?ref=4f41a46adfb84f5681fc8503fb749e1cdf463cd0", "patch": "@@ -1,10 +1,22 @@\n-PREHOOK: query: create table t1(i int, j int, k int)\n+PREHOOK: query: -- Test view authorization , and 'show grant' variants\n+\n+create table t1(i int, j int, k int)\n PREHOOK: type: CREATETABLE\n PREHOOK: Output: database:default\n-POSTHOOK: query: create table t1(i int, j int, k int)\n+POSTHOOK: query: -- Test view authorization , and 'show grant' variants\n+\n+create table t1(i int, j int, k int)\n POSTHOOK: type: CREATETABLE\n POSTHOOK: Output: database:default\n POSTHOOK: Output: default@t1\n+PREHOOK: query: show grant on table t1\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant on table t1\n+POSTHOOK: type: SHOW_GRANT\n+default\tt1\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n PREHOOK: query: -- protecting certain columns\n create view vt1 as select i,k from t1\n PREHOOK: type: CREATEVIEW\n@@ -110,6 +122,14 @@ PREHOOK: query: show grant user user2 on table vt2\n PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: show grant user user2 on table vt2\n POSTHOOK: type: SHOW_GRANT\n+PREHOOK: query: show grant on table vt2\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant on table vt2\n+POSTHOOK: type: SHOW_GRANT\n+default\tvt2\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n PREHOOK: query: revoke select on table vt1 from user user2\n PREHOOK: type: REVOKE_PRIVILEGE\n PREHOOK: Output: default@vt1\n@@ -190,3 +210,14 @@ POSTHOOK: type: SHOW_GRANT\n default\tvt2\t\t\trole_v\tROLE\tINSERT\tfalse\t-1\thive_admin_user\n default\tvt2\t\t\trole_v\tROLE\tSELECT\tfalse\t-1\thive_admin_user\n default\tvt2\t\t\trole_v\tROLE\tUPDATE\tfalse\t-1\thive_admin_user\n+PREHOOK: query: show grant on table vt2\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant on table vt2\n+POSTHOOK: type: SHOW_GRANT\n+default\tvt2\t\t\trole_v\tROLE\tINSERT\tfalse\t-1\thive_admin_user\n+default\tvt2\t\t\trole_v\tROLE\tSELECT\tfalse\t-1\thive_admin_user\n+default\tvt2\t\t\trole_v\tROLE\tUPDATE\tfalse\t-1\thive_admin_user\n+default\tvt2\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1", "filename": "ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/01526cfa663c24e903ba1c493deb308efdd5439e", "parent": "https://github.com/apache/hive/commit/bbdded414f1b841e8d40b4d735b565f1a2ffe066", "message": "HIVE-6645 : to_date()/to_unix_timestamp() fail with NPE if input is null (Jason Dere via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1579500 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_228", "file": [{"additions": 13, "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java", "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java", "sha": "9ee948019189b8d38c55f7a82a460e7f9a60e101", "changes": 15, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e", "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;\n import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;\n@@ -63,14 +64,20 @@\n   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length != 1) {\n       throw new UDFArgumentLengthException(\n-          \"to_date() requires 1 argument, got \" + arguments.length);\n+        \"to_date() requires 1 argument, got \" + arguments.length);\n+    }\n+    if (arguments[0].getCategory() != Category.PRIMITIVE) {\n+      throw new UDFArgumentException(\"to_date() only accepts STRING/TIMESTAMP/DATEWRITABLE types, got \"\n+          + arguments[0].getTypeName());\n     }\n     argumentOI = (PrimitiveObjectInspector) arguments[0];\n     inputType = argumentOI.getPrimitiveCategory();\n     ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n     switch (inputType) {\n+    case CHAR:\n+    case VARCHAR:\n     case STRING:\n-      // textConverter = new TextConverter(argumentOI);\n+      inputType = PrimitiveCategory.STRING;\n       textConverter = ObjectInspectorConverters.getConverter(\n         argumentOI, PrimitiveObjectInspectorFactory.writableStringObjectInspector);\n       break;\n@@ -91,6 +98,10 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n \n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n+\n     switch (inputType) {\n     case STRING:\n       Date date;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java"}, {"additions": 55, "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java", "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java", "sha": "65a2297fa12a877ed35b8c8b291fcbf384deb9ea", "changes": 78, "status": "modified", "deletions": 23, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e", "patch": "@@ -31,11 +31,18 @@\n import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping;\n import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n \n /**\n  * deterministic version of UDFUnixTimeStamp. enforces argument\n@@ -46,10 +53,10 @@\n @VectorizedExpressions({VectorUDFUnixTimeStampLong.class, VectorUDFUnixTimeStampString.class})\n public class GenericUDFToUnixTimeStamp extends GenericUDF {\n \n-  private transient StringObjectInspector intputTextOI;\n   private transient DateObjectInspector inputDateOI;\n   private transient TimestampObjectInspector inputTimestampOI;\n-  private transient StringObjectInspector patternOI;\n+  private transient Converter inputTextConverter;\n+  private transient Converter patternConverter;\n \n   private transient String lasPattern = \"yyyy-MM-dd HH:mm:ss\";\n   private transient final SimpleDateFormat formatter = new SimpleDateFormat(lasPattern);\n@@ -62,26 +69,44 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n \n   protected void initializeInput(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length < 1) {\n-      throw new UDFArgumentLengthException(\"The function TO_UNIX_TIMESTAMP \" +\n+      throw new UDFArgumentLengthException(\"The function \" + getName().toUpperCase() +\n           \"requires at least one argument\");\n     }\n+    for (ObjectInspector argument : arguments) {\n+      if (arguments[0].getCategory() != Category.PRIMITIVE) {\n+        throw new UDFArgumentException(getName().toUpperCase() +\n+            \" only takes string/date/timestamp types, got \" + argument.getTypeName());\n+      }\n+    }\n \n-    if (arguments[0] instanceof StringObjectInspector) {\n-      intputTextOI = (StringObjectInspector) arguments[0];\n-      if (arguments.length > 1) {\n-        if (!(arguments[1] instanceof StringObjectInspector)) {\n-          throw new UDFArgumentException(\n-            \"The time pattern for \" + getName().toUpperCase() + \" should be string type\");\n+    PrimitiveObjectInspector arg1OI = (PrimitiveObjectInspector) arguments[0];\n+    switch (arg1OI.getPrimitiveCategory()) {\n+      case CHAR:\n+      case VARCHAR:\n+      case STRING:\n+        inputTextConverter = ObjectInspectorConverters.getConverter(arg1OI,\n+            PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n+        if (arguments.length > 1) {\n+          PrimitiveObjectInspector arg2OI = (PrimitiveObjectInspector) arguments[1];\n+          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(arg2OI.getPrimitiveCategory())\n+              != PrimitiveGrouping.STRING_GROUP) {\n+            throw new UDFArgumentException(\n+              \"The time pattern for \" + getName().toUpperCase() + \" should be string type\");\n+          }\n+          patternConverter = ObjectInspectorConverters.getConverter(arg2OI,\n+              PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n         }\n-        patternOI = (StringObjectInspector) arguments[1];\n-      }\n-    } else if (arguments[0] instanceof DateObjectInspector) {\n-      inputDateOI = (DateObjectInspector) arguments[0];\n-    } else if (arguments[0] instanceof TimestampObjectInspector) {\n-      inputTimestampOI = (TimestampObjectInspector) arguments[0];\n-    } else {\n-      throw new UDFArgumentException(\n-          \"The function \" + getName().toUpperCase() + \" takes only string or timestamp types\");\n+        break;\n+\n+      case DATE:\n+        inputDateOI = (DateObjectInspector) arguments[0];\n+        break;\n+      case TIMESTAMP:\n+        inputTimestampOI = (TimestampObjectInspector) arguments[0];\n+        break;\n+      default:\n+        throw new UDFArgumentException(\n+            \"The function \" + getName().toUpperCase() + \" takes only string/date/timestamp types\");\n     }\n   }\n \n@@ -93,13 +118,20 @@ protected String getName() {\n \n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n-    if (intputTextOI != null) {\n-      String textVal = intputTextOI.getPrimitiveJavaObject(arguments[0].get());\n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n+\n+    if (inputTextConverter != null) {\n+      String textVal = (String) inputTextConverter.convert(arguments[0].get());\n       if (textVal == null) {\n         return null;\n       }\n-      if (patternOI != null) {\n-        String patternVal = patternOI.getPrimitiveJavaObject(arguments[1].get());\n+      if (patternConverter != null) {\n+        if (arguments[1].get() == null) {\n+          return null;\n+        }\n+        String patternVal = (String) patternConverter.convert(arguments[1].get());\n         if (patternVal == null) {\n           return null;\n         }\n@@ -118,7 +150,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {\n       retValue.set(inputDateOI.getPrimitiveWritableObject(arguments[0].get())\n                    .getTimeInSeconds());\n       return retValue;\n-\t}\n+    }\n     Timestamp timestamp = inputTimestampOI.getPrimitiveJavaObject(arguments[0].get());\n     retValue.set(timestamp.getTime() / 1000);\n     return retValue;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java", "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java", "sha": "0d40ff7cb3c47ed4c2a8d2e88d9d8c32eb87d09d", "changes": 15, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e", "patch": "@@ -45,6 +45,11 @@ public void testStringToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"to_date() test for STRING failed \", \"2009-07-30\", output.toString());\n+\n+    // Try with null args\n+    DeferredObject[] nullArgs = { new DeferredJavaObject(null) };\n+    output = (Text) udf.evaluate(nullArgs);\n+    assertNull(\"to_date() with null STRING\", output);\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -59,6 +64,11 @@ public void testTimestampToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"to_date() test for TIMESTAMP failed \", \"2009-07-30\", output.toString());\n+\n+    // Try with null args\n+    DeferredObject[] nullArgs = { new DeferredJavaObject(null) };\n+    output = (Text) udf.evaluate(nullArgs);\n+    assertNull(\"to_date() with null TIMESTAMP\", output);\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -72,6 +82,11 @@ public void testDateWritablepToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"to_date() test for DATEWRITABLE failed \", \"2009-07-30\", output.toString());\n+\n+    // Try with null args\n+    DeferredObject[] nullArgs = { new DeferredJavaObject(null) };\n+    output = (Text) udf.evaluate(nullArgs);\n+    assertNull(\"to_date() with null DATE\", output);\n   }\n \n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java"}, {"additions": 126, "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java", "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java", "sha": "52d30d3007a0ef00ee67c2cecd7df24fb3fd5a32", "changes": 126, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e", "patch": "@@ -0,0 +1,126 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;\n+import org.apache.hadoop.hive.serde2.io.DateWritable;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+\n+import junit.framework.TestCase;\n+\n+public class TestGenericUDFToUnixTimestamp extends TestCase {\n+\n+  public static void runAndVerify(GenericUDFToUnixTimeStamp udf,\n+      Object arg, Object expected) throws HiveException {\n+    DeferredObject[] args = { new DeferredJavaObject(arg) };\n+    Object result = udf.evaluate(args);\n+    if (expected == null) {\n+      assertNull(result);\n+    } else {\n+      assertEquals(expected.toString(), result.toString());\n+    }\n+  }\n+\n+  public static void runAndVerify(GenericUDFToUnixTimeStamp udf,\n+      Object arg1, Object arg2, Object expected) throws HiveException {\n+    DeferredObject[] args = { new DeferredJavaObject(arg1), new DeferredJavaObject(arg2) };\n+    Object result = udf.evaluate(args);\n+    if (expected == null) {\n+      assertNull(result);\n+    } else {\n+      assertEquals(expected.toString(), result.toString());\n+    }\n+  }\n+\n+  public void testTimestamp() throws HiveException {\n+    GenericUDFToUnixTimeStamp udf = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector valueOI = PrimitiveObjectInspectorFactory.writableTimestampObjectInspector;\n+    ObjectInspector[] arguments = {valueOI};\n+    udf.initialize(arguments);\n+\n+    Timestamp ts = Timestamp.valueOf(\"1970-01-01 00:00:00\");\n+    runAndVerify(udf,\n+        new TimestampWritable(ts),\n+        new LongWritable(ts.getTime() / 1000));\n+\n+    ts = Timestamp.valueOf(\"2001-02-03 01:02:03\");\n+    runAndVerify(udf,\n+        new TimestampWritable(ts),\n+        new LongWritable(ts.getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf, null, null);\n+  }\n+\n+  public void testDate() throws HiveException {\n+    GenericUDFToUnixTimeStamp udf = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector valueOI = PrimitiveObjectInspectorFactory.writableDateObjectInspector;\n+    ObjectInspector[] arguments = {valueOI};\n+    udf.initialize(arguments);\n+\n+    Date date = Date.valueOf(\"1970-01-01\");\n+    runAndVerify(udf,\n+        new DateWritable(date),\n+        new LongWritable(date.getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf, null, null);\n+  }\n+\n+  public void testString() throws HiveException {\n+    GenericUDFToUnixTimeStamp udf1 = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector valueOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n+    ObjectInspector[] arguments = {valueOI};\n+    udf1.initialize(arguments);\n+\n+    String val = \"2001-01-01 01:02:03\";\n+    runAndVerify(udf1,\n+        new Text(val),\n+        new LongWritable(Timestamp.valueOf(val).getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf1, null, null);\n+\n+    // Try 2-arg version\n+    GenericUDFToUnixTimeStamp udf2 = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector[] args2 = {valueOI, valueOI};\n+    udf2.initialize(args2);\n+\n+    val = \"2001-01-01\";\n+    String format = \"yyyy-MM-dd\";\n+    runAndVerify(udf2,\n+        new Text(val),\n+        new Text(format),\n+        new LongWritable(Date.valueOf(val).getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf2, null, null, null);\n+    runAndVerify(udf2, null, new Text(format), null);\n+    runAndVerify(udf2, new Text(val), null, null);\n+  }\n+}", "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09", "parent": "https://github.com/apache/hive/commit/4376224ce5f08fff4ddcf32ccee01b41c1c890ad", "message": "HIVE-6690 : NPE in tez session state (Sergey Shelukhin, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1579293 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_229", "file": [{"additions": 19, "raw_url": "https://github.com/apache/hive/raw/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "blob_url": "https://github.com/apache/hive/blob/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "sha": "e20d8bf386c654584999733313536e39e202414f", "changes": 33, "status": "modified", "deletions": 14, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09", "patch": "@@ -599,6 +599,24 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n \n     // we need the directory on hdfs to which we shall put all these files\n     // Use HIVE_JAR_DIRECTORY only if it's set explicitly; otherwise use default directory\n+    String hdfsDirPathStr = getHiveJarDirectory(conf);\n+\n+    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n+    String[] allFilesArr = allFiles.split(\",\");\n+    for (String file : allFilesArr) {\n+      if (!StringUtils.isNotBlank(file)) {\n+        continue;\n+      }\n+      String hdfsFilePathStr = hdfsDirPathStr + \"/\" + getResourceBaseName(file);\n+      LocalResource localResource = localizeResource(new Path(file),\n+          new Path(hdfsFilePathStr), conf);\n+      tmpResources.add(localResource);\n+    }\n+\n+    return tmpResources;\n+  }\n+\n+  public String getHiveJarDirectory(Configuration conf) throws IOException, LoginException {\n     FileStatus fstatus = null;\n     String hdfsDirPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_JAR_DIRECTORY, null);\n     if (hdfsDirPathStr != null) {\n@@ -618,20 +636,7 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n       Path destDir = getDefaultDestDir(conf);\n       hdfsDirPathStr = destDir.toString();\n     }\n-\n-    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n-    String[] allFilesArr = allFiles.split(\",\");\n-    for (String file : allFilesArr) {\n-      if (!StringUtils.isNotBlank(file)) {\n-        continue;\n-      }\n-      String hdfsFilePathStr = hdfsDirPathStr + \"/\" + getResourceBaseName(file);\n-      LocalResource localResource = localizeResource(new Path(file),\n-          new Path(hdfsFilePathStr), conf);\n-      tmpResources.add(localResource);\n-    }\n-\n-    return tmpResources;\n+    return hdfsDirPathStr;\n   }\n \n   // the api that finds the jar being used by this class on disk", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "blob_url": "https://github.com/apache/hive/blob/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "sha": "3262ae3ef527059e29e9f2a4af17b1a921ce97e5", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java?ref=00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09", "patch": "@@ -247,7 +247,7 @@ private Path createTezDir(String sessionId)\n    */\n   private LocalResource createHiveExecLocalResource()\n     throws IOException, LoginException, URISyntaxException {\n-    String hiveJarDir = conf.getVar(HiveConf.ConfVars.HIVE_JAR_DIRECTORY);\n+    String hiveJarDir = utils.getHiveJarDirectory(conf);\n     String currentVersionPathStr = utils.getExecJarPathLocal();\n     String currentJarName = utils.getResourceBaseName(currentVersionPathStr);\n     FileSystem fs = null;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "parent": "https://github.com/apache/hive/commit/871b2765ca7db24c65fd22a52241bd2b4f8ce6ca", "message": "HIVE-6567 : \"show grant ... on all\" fails with NPE (Thejas Nair, reviewed by Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1577428 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_230", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "sha": "42df435f3e0fa8ec2c347c6a598e9c74fe0cffcf", "changes": 18, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -657,7 +657,7 @@ private int showGrantsV2(ShowGrantDesc showGrantDesc) throws HiveException {\n \n         PrivilegeGrantInfo grantInfo =\n             AuthorizationUtils.getThriftPrivilegeGrantInfo(priv, privInfo.getGrantorPrincipal(),\n-                privInfo.isGrantOption());\n+                privInfo.isGrantOption(), privInfo.getGrantTime());\n \n         //only grantInfo is used\n         HiveObjectPrivilege thriftObjectPriv = new HiveObjectPrivilege(new HiveObjectRef(\n@@ -674,18 +674,6 @@ private int showGrantsV2(ShowGrantDesc showGrantDesc) throws HiveException {\n     return 0;\n   }\n \n-  private static void sortPrivileges(List<HiveObjectPrivilege> privileges) {\n-    Collections.sort(privileges, new Comparator<HiveObjectPrivilege>() {\n-\n-      @Override\n-      public int compare(HiveObjectPrivilege one, HiveObjectPrivilege other) {\n-        return one.getGrantInfo().getPrivilege().compareTo(other.getGrantInfo().getPrivilege());\n-      }\n-\n-    });\n-\n-  }\n-\n   private int grantOrRevokePrivileges(List<PrincipalDesc> principals,\n       List<PrivilegeDesc> privileges, PrivilegeObjectDesc privSubjectDesc,\n       String grantor, PrincipalType grantorType, boolean grantOption, boolean isGrant)\n@@ -854,6 +842,7 @@ private int grantOrRevokePrivilegesV2(List<PrincipalDesc> principals,\n \n   private HivePrivilegeObject getHivePrivilegeObject(PrivilegeObjectDesc privSubjectDesc)\n       throws HiveException {\n+\n     String [] dbTable = Utilities.getDbTableName(privSubjectDesc.getObject());\n     return new HivePrivilegeObject(getPrivObjectType(privSubjectDesc), dbTable[0], dbTable[1]);\n   }\n@@ -877,6 +866,9 @@ private HivePrincipalType getHivePrincipalType(PrincipalType type) throws HiveEx\n   }\n \n   private HivePrivilegeObjectType getPrivObjectType(PrivilegeObjectDesc privSubjectDesc) {\n+    if (privSubjectDesc.getObject() == null) {\n+      return null;\n+    }\n     return privSubjectDesc.getTable() ? HivePrivilegeObjectType.TABLE_OR_VIEW : HivePrivilegeObjectType.DATABASE;\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java", "sha": "9a74fa5800841ad7cbf9beed4abca0981bacf161", "changes": 9, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -70,9 +70,6 @@\n import java.util.Random;\n import java.util.Set;\n import java.util.UUID;\n-import java.util.zip.Deflater;\n-import java.util.zip.DeflaterOutputStream;\n-import java.util.zip.InflaterInputStream;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n@@ -81,6 +78,9 @@\n import java.util.concurrent.TimeUnit;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n+import java.util.zip.Deflater;\n+import java.util.zip.DeflaterOutputStream;\n+import java.util.zip.InflaterInputStream;\n \n import org.antlr.runtime.CommonToken;\n import org.apache.commons.codec.binary.Base64;\n@@ -2029,6 +2029,9 @@ public static String formatBinaryString(byte[] array, int start, int length) {\n    * @throws HiveException\n    */\n   public static String[] getDbTableName(String dbtable) throws HiveException{\n+    if(dbtable == null){\n+      return new String[2];\n+    }\n     String[] names =  dbtable.split(\"\\\\.\");\n     switch (names.length) {\n     case 2:", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java", "sha": "5e2d12c57bae003ddac6dc9b5732c1f4df8516ec", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -108,12 +108,13 @@ public static PrincipalType getThriftPrincipalType(HivePrincipalType type) {\n    * @param privilege\n    * @param grantorPrincipal\n    * @param grantOption\n+   * @param grantTime\n    * @return\n    * @throws HiveException\n    */\n   public static PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privilege,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveException {\n-    return new PrivilegeGrantInfo(privilege.getName(), 0 /* time gets added by server */,\n+      HivePrincipal grantorPrincipal, boolean grantOption, int grantTime) throws HiveException {\n+    return new PrivilegeGrantInfo(privilege.getName(), grantTime,\n         grantorPrincipal.getName(), getThriftPrincipalType(grantorPrincipal.getType()), grantOption);\n   }\n \n@@ -125,6 +126,9 @@ public static PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privi\n    * @throws HiveException\n    */\n   public static HiveObjectType getThriftHiveObjType(HivePrivilegeObjectType type) throws HiveException {\n+    if (type == null) {\n+      return null;\n+    }\n     switch(type){\n     case DATABASE:\n       return HiveObjectType.DATABASE;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java", "sha": "0f91ccbc08614f459b4407c6033c673e27c98611", "changes": 9, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -31,14 +31,17 @@\n   private final HivePrivilegeObject object;\n   private final HivePrincipal grantorPrincipal;\n   private final boolean grantOption;\n+  private final int grantTime;\n \n   public HivePrivilegeInfo(HivePrincipal principal, HivePrivilege privilege,\n-      HivePrivilegeObject object, HivePrincipal grantorPrincipal, boolean grantOption){\n+      HivePrivilegeObject object, HivePrincipal grantorPrincipal, boolean grantOption,\n+      int grantTime){\n     this.principal = principal;\n     this.privilege = privilege;\n     this.object = object;\n     this.grantorPrincipal = grantorPrincipal;\n     this.grantOption = grantOption;\n+    this.grantTime = grantTime;\n   }\n \n   public HivePrincipal getPrincipal() {\n@@ -61,5 +64,9 @@ public boolean isGrantOption() {\n     return grantOption;\n   }\n \n+  public int getGrantTime() {\n+    return grantTime;\n+  }\n+\n \n }\n\\ No newline at end of file", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java", "sha": "03d12ca107cbcd3cf3f00f1517df50ad5c2c93ad", "changes": 9, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -35,15 +35,13 @@\n import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.HiveMetaStore;\n import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;\n import org.apache.hadoop.hive.metastore.api.HiveObjectRef;\n import org.apache.hadoop.hive.metastore.api.HiveObjectType;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet;\n import org.apache.hadoop.hive.metastore.api.PrivilegeBag;\n import org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo;\n@@ -91,7 +89,7 @@ static PrivilegeBag getThriftPrivilegesBag(List<HivePrincipal> hivePrincipals,\n             + \" is not supported in sql standard authorization mode\");\n       }\n       PrivilegeGrantInfo grantInfo = getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n+          grantOption, 0 /*real grant time added by metastore*/);\n       for (HivePrincipal principal : hivePrincipals) {\n         HiveObjectPrivilege objPriv = new HiveObjectPrivilege(privObj, principal.getName(),\n             AuthorizationUtils.getThriftPrincipalType(principal.getType()), grantInfo);\n@@ -102,10 +100,11 @@ static PrivilegeBag getThriftPrivilegesBag(List<HivePrincipal> hivePrincipals,\n   }\n \n   static PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privilege,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveAuthzPluginException {\n+      HivePrincipal grantorPrincipal, boolean grantOption, int grantTime)\n+          throws HiveAuthzPluginException {\n     try {\n       return AuthorizationUtils.getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n+          grantOption, grantTime);\n     } catch (HiveException e) {\n       throw new HiveAuthzPluginException(e);\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java", "sha": "fec5eae72c6ff5bcdd4da7f379c86f81edb185dc", "changes": 48, "status": "modified", "deletions": 46, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -39,7 +39,6 @@\n import org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo;\n import org.apache.hadoop.hive.metastore.api.Role;\n import org.apache.hadoop.hive.metastore.api.RolePrincipalGrant;\n-import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;\n import org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils;\n import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException;\n@@ -156,7 +155,7 @@ public void grantPrivileges(List<HivePrincipal> hivePrincipals,\n         metastoreClient, authenticator.getUserName(), getCurrentRoles(), isUserAdmin());\n \n     // grant\n-    PrivilegeBag privBag = getThriftPrivilegesBag(hivePrincipals, hivePrivileges, hivePrivObject,\n+    PrivilegeBag privBag = SQLAuthorizationUtils.getThriftPrivilegesBag(hivePrincipals, hivePrivileges, hivePrivObject,\n         grantorPrincipal, grantOption);\n     try {\n       metastoreClient.grant_privileges(privBag);\n@@ -188,49 +187,6 @@ public void grantPrivileges(List<HivePrincipal> hivePrincipals,\n     return new ArrayList<HivePrivilege>(hivePrivSet);\n   }\n \n-  /**\n-   * Create thrift privileges bag\n-   *\n-   * @param hivePrincipals\n-   * @param hivePrivileges\n-   * @param hivePrivObject\n-   * @param grantorPrincipal\n-   * @param grantOption\n-   * @return\n-   * @throws HiveAuthzPluginException\n-   */\n-  private PrivilegeBag getThriftPrivilegesBag(List<HivePrincipal> hivePrincipals,\n-      List<HivePrivilege> hivePrivileges, HivePrivilegeObject hivePrivObject,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveAuthzPluginException {\n-\n-    HiveObjectRef privObj = SQLAuthorizationUtils.getThriftHiveObjectRef(hivePrivObject);\n-    PrivilegeBag privBag = new PrivilegeBag();\n-    for (HivePrivilege privilege : hivePrivileges) {\n-      if (privilege.getColumns() != null && privilege.getColumns().size() > 0) {\n-        throw new HiveAuthzPluginException(\"Privileges on columns not supported currently\"\n-            + \" in sql standard authorization mode\");\n-      }\n-\n-      PrivilegeGrantInfo grantInfo = getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n-      for (HivePrincipal principal : hivePrincipals) {\n-        HiveObjectPrivilege objPriv = new HiveObjectPrivilege(privObj, principal.getName(),\n-            AuthorizationUtils.getThriftPrincipalType(principal.getType()), grantInfo);\n-        privBag.addToPrivileges(objPriv);\n-      }\n-    }\n-    return privBag;\n-  }\n-\n-  private PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privilege,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveAuthzPluginException {\n-    try {\n-      return AuthorizationUtils.getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n-    } catch (HiveException e) {\n-      throw new HiveAuthzPluginException(e);\n-    }\n-  }\n \n   @Override\n   public void revokePrivileges(List<HivePrincipal> hivePrincipals,\n@@ -430,7 +386,7 @@ public void revokeRole(List<HivePrincipal> hivePrincipals, List<String> roleName\n             AuthorizationUtils.getHivePrincipalType(msGrantInfo.getGrantorType()));\n \n         HivePrivilegeInfo resPrivInfo = new HivePrivilegeInfo(resPrincipal, resPrivilege,\n-            resPrivObj, grantorPrincipal, msGrantInfo.isGrantOption());\n+            resPrivObj, grantorPrincipal, msGrantInfo.isGrantOption(), msGrantInfo.getCreateTime());\n         resPrivInfos.add(resPrivInfo);\n       }\n       return resPrivInfos;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q", "sha": "2e384d7f1227efa111e4552ef27338d3d435c17c", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -9,6 +9,7 @@ CREATE TABLE table_priv_rev(i int);\n -- grant insert privilege to user2\n GRANT INSERT ON table_priv_rev TO USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n+SHOW GRANT USER user2 ON ALL;\n \n -- revoke insert privilege from user2\n REVOKE INSERT ON TABLE table_priv_rev FROM USER user2;\n@@ -18,6 +19,7 @@ SHOW GRANT USER user2 ON TABLE table_priv_rev;\n -- grant insert privilege to user2\n GRANT INSERT ON table_priv_rev TO USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n+SHOW GRANT USER user2 ON ALL;\n \n -- grant select privilege to user2, with grant option\n GRANT SELECT ON table_priv_rev TO USER user2 WITH GRANT OPTION;\n@@ -31,10 +33,12 @@ SHOW GRANT USER user2 ON TABLE table_priv_rev;\n GRANT DELETE ON table_priv_rev TO USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n \n+\n -- start revoking --\n -- revoke update privilege from user2\n REVOKE UPDATE ON TABLE table_priv_rev FROM USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n+SHOW GRANT USER user2 ON ALL;\n \n -- revoke DELETE privilege from user2\n REVOKE DELETE ON TABLE table_priv_rev FROM USER user2;\n@@ -47,7 +51,7 @@ SHOW GRANT USER user2 ON TABLE table_priv_rev;\n -- revoke select privilege from user2\n REVOKE SELECT ON TABLE table_priv_rev FROM USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n-\n+SHOW GRANT USER user2 ON ALL;\n \n -- grant all followed by revoke all\n GRANT ALL ON table_priv_rev TO USER user2;", "filename": "ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q", "sha": "915237acb93957cd0f061f10fca42c8a5e7602ea", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -12,6 +12,8 @@ create view vt1 as select i,k from t1;\n -- protecting certain rows\n create view vt2 as select * from t1 where i > 1;\n \n+show grant user user1 on all;\n+\n --view grant to user\n -- try with and without table keyword\n \n@@ -21,20 +23,24 @@ grant insert on table vt1 to user user3;\n show grant user user2 on table vt1;\n show grant user user3 on table vt1;\n \n+\n set user.name=user2;\n select * from vt1;\n \n set user.name=user1;\n \n grant all on table vt2 to user user2;\n show grant user user2 on table vt2;\n+show grant user user2 on all;\n \n revoke all on vt2 from user user2;\n show grant user user2 on table vt2;\n \n revoke select on table vt1 from user user2;\n show grant user user2 on table vt1;\n \n+show grant user user2 on all;\n+\n -- grant privileges on roles for view, after next statement\n show grant user user3 on table vt1;\n ", "filename": "ql/src/test/queries/clientpositive/authorization_view_sqlstd.q"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out", "sha": "907c8898b662be53bae7f6a64cd1f02c3f43f547", "changes": 21, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -22,6 +22,11 @@ PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n POSTHOOK: type: SHOW_GRANT\n default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n PREHOOK: query: -- revoke insert privilege from user2\n REVOKE INSERT ON TABLE table_priv_rev FROM USER user2\n PREHOOK: type: REVOKE_PRIVILEGE\n@@ -49,6 +54,11 @@ PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n POSTHOOK: type: SHOW_GRANT\n default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n PREHOOK: query: -- grant select privilege to user2, with grant option\n GRANT SELECT ON table_priv_rev TO USER user2 WITH GRANT OPTION\n PREHOOK: type: GRANT_PRIVILEGE\n@@ -111,6 +121,13 @@ POSTHOOK: type: SHOW_GRANT\n default\ttable_priv_rev\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n default\ttable_priv_rev\t\t\tuser2\tUSER\tSELECT\ttrue\t-1\tuser1\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tSELECT\ttrue\t-1\tuser1\n PREHOOK: query: -- revoke DELETE privilege from user2\n REVOKE DELETE ON TABLE table_priv_rev FROM USER user2\n PREHOOK: type: REVOKE_PRIVILEGE\n@@ -150,6 +167,10 @@ PREHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n POSTHOOK: type: SHOW_GRANT\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n PREHOOK: query: -- grant all followed by revoke all\n GRANT ALL ON table_priv_rev TO USER user2\n PREHOOK: type: GRANT_PRIVILEGE", "filename": "ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out"}, {"additions": 29, "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out", "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out", "sha": "89186a5a1d5c43acdadd17007705a5bfd1a07278", "changes": 29, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2", "patch": "@@ -23,6 +23,22 @@ create view vt2 as select * from t1 where i > 1\n POSTHOOK: type: CREATEVIEW\n POSTHOOK: Input: default@t1\n POSTHOOK: Output: default@vt2\n+PREHOOK: query: show grant user user1 on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user user1 on all\n+POSTHOOK: type: SHOW_GRANT\n+default\tt1\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n PREHOOK: query: --view grant to user\n -- try with and without table keyword\n \n@@ -75,6 +91,15 @@ default\tvt2\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n default\tvt2\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n default\tvt2\t\t\tuser2\tUSER\tSELECT\tfalse\t-1\tuser1\n default\tvt2\t\t\tuser2\tUSER\tUPDATE\tfalse\t-1\tuser1\n+PREHOOK: query: show grant user user2 on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user user2 on all\n+POSTHOOK: type: SHOW_GRANT\n+default\tvt1\t\t\tuser2\tUSER\tSELECT\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tSELECT\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tUPDATE\tfalse\t-1\tuser1\n PREHOOK: query: revoke all on vt2 from user user2\n PREHOOK: type: REVOKE_PRIVILEGE\n PREHOOK: Output: default@vt2\n@@ -95,6 +120,10 @@ PREHOOK: query: show grant user user2 on table vt1\n PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: show grant user user2 on table vt1\n POSTHOOK: type: SHOW_GRANT\n+PREHOOK: query: show grant user user2 on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user user2 on all\n+POSTHOOK: type: SHOW_GRANT\n PREHOOK: query: -- grant privileges on roles for view, after next statement\n show grant user user3 on table vt1\n PREHOOK: type: SHOW_GRANT", "filename": "ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ce92a3074416875f076a1e96ecf8c60c310c5c04", "parent": "https://github.com/apache/hive/commit/5bef1b56a0b03988974c599728a40e98a8a08533", "message": "HIVE-6607 : describe extended on a view fails with NPE (Eugene Koifman via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1576985 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_231", "file": [{"additions": 54, "raw_url": "https://github.com/apache/hive/raw/ce92a3074416875f076a1e96ecf8c60c310c5c04/hcatalog/src/test/e2e/templeton/tests/ddl.conf", "blob_url": "https://github.com/apache/hive/blob/ce92a3074416875f076a1e96ecf8c60c310c5c04/hcatalog/src/test/e2e/templeton/tests/ddl.conf", "sha": "a51fe36eed771265dee1e02f7fbc154fae830eae", "changes": 55, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/src/test/e2e/templeton/tests/ddl.conf?ref=ce92a3074416875f076a1e96ecf8c60c310c5c04", "patch": "@@ -1113,7 +1113,60 @@ $cfg =\n   ]\n \n }\n-\n+,\n+  {\n+    'name' => 'TEST_VIEW',\n+    'tests' =>\n+    [\n+      {\n+        'num' => 1,\n+        'method' => 'POST',\n+        'url' => ':TEMPLETON_URL:/templeton/v1/ddl?user.name=:UNAME:',\n+        'status_code' => 200,\n+        'post_options' => ['exec=DROP VIEW url_table_view;'],\n+        'json_field_substr_match' => {'stderr' => 'OK', 'exitcode' => '^0$'}\n+      },\n+      {\n+        'num' => 2,\n+        'method' => 'DELETE',\n+        'url' => ':TEMPLETON_URL:/templeton/v1/ddl/database/default/table/url_table?user.name=:UNAME:&ifExists=true',\n+        'status_code' => 200,\n+        'json_field_substr_match' => {'database' => 'default',  'table' => 'url_table'},\n+      },\n+      {\n+               #create table\n+        'num' => 3,\n+        'method' => 'PUT',\n+        'url' => ':TEMPLETON_URL:/templeton/v1/ddl/database/default/table/url_table?user.name=:UNAME:',\n+        'format_header' => 'Content-Type: application/json',\n+        'post_options' => ['{\n+          \"columns\": [\n+            { \"name\" : \"count\", \"type\" : \"int\" },\n+            { \"name\" : \"page_url\", \"type\" : \"string\"}\n+          ], \n+          \"format\" : {  \"storedAs\" : \"SEQUENCEFILE\"} \n+        }'],\n+        'status_code' => 200,\n+        'json_field_substr_match' => {'database' => 'default',  'table' => 'url_table'},\n+      \n+      },\n+    {\n+                                #create view\n+     'num' => 4,\n+     'method' => 'POST',\n+     'url' => ':TEMPLETON_URL:/templeton/v1/ddl?user.name=:UNAME:',\n+     'status_code' => 200,\n+     'post_options' => ['exec=CREATE VIEW url_table_view(url) as  SELECT DISTINCT page_url from url_table;'],\n+     'json_field_substr_match' => {'stderr' => 'OK', 'exitcode' => '^0$'}\n+    },\n+    {\n+      'num' => 5,\n+      'method' => 'GET',\n+      'url' => ':TEMPLETON_URL:/templeton/v1/ddl/database/default/table/url_table_view?user.name=:UNAME:&format=extended',\n+      'status_code' => 200,\n+    }\n+    ]\n+  }\n \n \n ", "filename": "hcatalog/src/test/e2e/templeton/tests/ddl.conf"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/ce92a3074416875f076a1e96ecf8c60c310c5c04/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java", "blob_url": "https://github.com/apache/hive/blob/ce92a3074416875f076a1e96ecf8c60c310c5c04/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java", "sha": "ee35857dd5fc2dda7845844e4bc16603d7bf2ff0", "changes": 11, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java?ref=ce92a3074416875f076a1e96ecf8c60c310c5c04", "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.TableType;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -189,9 +190,11 @@ public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf,\n     if (tbl.isPartitioned()) {\n       builder.put(\"partitionColumns\", makeColsUnformatted(tbl.getPartCols()));\n     }\n-\n-    putFileSystemsStats(builder, makeTableStatusLocations(tbl, db, par),\n+    if(tbl.getTableType() != TableType.VIRTUAL_VIEW) {\n+      //tbl.getPath() is null for views\n+      putFileSystemsStats(builder, makeTableStatusLocations(tbl, db, par),\n         conf, tbl.getPath());\n+    }\n \n     return builder.build();\n   }\n@@ -222,6 +225,10 @@ public void showTableStatus(DataOutputStream out, Hive db, HiveConf conf,\n     return locations;\n   }\n \n+  /**\n+   * @param tblPath not NULL\n+   * @throws IOException\n+   */\n   // Duplicates logic in TextMetaDataFormatter\n   private void putFileSystemsStats(MapBuilder builder, List<Path> locations,\n       HiveConf conf, Path tblPath)", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4f683fa1d6f756a5533e94ce95300b14582b8f19", "parent": "https://github.com/apache/hive/commit/bda54e30c61b3c8168fdc294ea06695cb8a2771c", "message": "HIVE-6545 : analyze table throws NPE for non-existent tables. (Ashutosh Chauhan via Harish Butani)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1574257 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_232", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "f80dd029bb6b220cd459d06a05de29dabded912a", "changes": 9, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=4f683fa1d6f756a5533e94ce95300b14582b8f19", "patch": "@@ -1236,8 +1236,13 @@ public void getMetaData(QB qb, ReadEntity parentInput) throws SemanticException\n             sqAliasToCTEName.put(alias, cte_name);\n             continue;\n           }\n-          throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(qb\n-              .getParseInfo().getSrcForAlias(alias)));\n+          ASTNode src = qb.getParseInfo().getSrcForAlias(alias);\n+          if (null != src) {\n+            throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(src));\n+          } else {\n+            throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(alias));\n+          }\n+\n         }\n \n         // Disallow INSERT INTO on bucketized tables", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q", "blob_url": "https://github.com/apache/hive/blob/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q", "sha": "78a97019f192e698ab6669c0fa51feb5b3b8c9dd", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q?ref=4f683fa1d6f756a5533e94ce95300b14582b8f19", "patch": "@@ -0,0 +1 @@\n+analyze table nonexistent compute statistics;", "filename": "ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out", "blob_url": "https://github.com/apache/hive/blob/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out", "sha": "ab2ecbed16291746346aaaf5e7e2b55dc064e995", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out?ref=4f683fa1d6f756a5533e94ce95300b14582b8f19", "patch": "@@ -0,0 +1 @@\n+FAILED: SemanticException [Error 10001]: Table not found nonexistent", "filename": "ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/90fabf185f264508afd329d9e0bf847fc9ae19f1", "parent": "https://github.com/apache/hive/commit/c192ecbb3e28610a46a0ecea6ba677ad0e467c3a", "message": "HIVE-5950: ORC SARG creation fails with NPE for predicate conditions with decimal/date/char/varchar datatypes (Prasanth J via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1574237 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_233", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "sha": "a425a01f0831218199fcad008855dd4d149996bc", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -848,10 +848,10 @@ private boolean isStripeSatisfyPredicate(StripeStatistics stripeStatistics,\n \n             // column statistics at index 0 contains only the number of rows\n             ColumnStatistics stats = stripeStatistics.getColumnStatistics()[filterColumns[pred] + 1];\n-            Object minValue = getMin(stats);\n-            Object maxValue = getMax(stats);\n-            truthValues[pred] = RecordReaderImpl.evaluatePredicateRange(predLeaves.get(pred),\n-                minValue, maxValue);\n+            Object minValue = RecordReaderImpl.getMin(stats);\n+            Object maxValue = RecordReaderImpl.getMax(stats);\n+            PredicateLeaf predLeaf = predLeaves.get(pred);\n+            truthValues[pred] = RecordReaderImpl.evaluatePredicateRange(predLeaf, minValue, maxValue);\n           } else {\n \n             // parition column case.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java"}, {"additions": 99, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "sha": "cfa78cb5f45f065e233b13b4fb880bd8f49e8662", "changes": 148, "status": "modified", "deletions": 49, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -31,16 +31,25 @@\n import java.util.TreeMap;\n \n import org.apache.commons.lang.builder.HashCodeBuilder;\n+import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveChar;\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n import org.apache.hadoop.hive.conf.HiveConf;\n import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVE_ORC_ZEROCOPY;\n import org.apache.hadoop.hive.ql.exec.vector.*;\n+import org.apache.hadoop.hive.common.type.HiveVarchar;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;\n@@ -2165,57 +2174,47 @@ private static TreeReader createTreeReader(Path path,\n   }\n \n   /**\n-   * Get the minimum value out of an index entry.\n-   * @param index the index entry\n-   * @return the object for the minimum value or null if there isn't one\n+   * Get the maximum value out of an index entry.\n+   * @param index\n+   *          the index entry\n+   * @return the object for the maximum value or null if there isn't one\n    */\n-  static Object getMin(OrcProto.ColumnStatistics index) {\n-    if (index.hasIntStatistics()) {\n-      OrcProto.IntegerStatistics stat = index.getIntStatistics();\n-      if (stat.hasMinimum()) {\n-        return stat.getMinimum();\n-      }\n-    }\n-    if (index.hasStringStatistics()) {\n-      OrcProto.StringStatistics stat = index.getStringStatistics();\n-      if (stat.hasMinimum()) {\n-        return stat.getMinimum();\n-      }\n-    }\n-    if (index.hasDoubleStatistics()) {\n-      OrcProto.DoubleStatistics stat = index.getDoubleStatistics();\n-      if (stat.hasMinimum()) {\n-        return stat.getMinimum();\n-      }\n+  static Object getMax(ColumnStatistics index) {\n+    if (index instanceof IntegerColumnStatistics) {\n+      return ((IntegerColumnStatistics) index).getMaximum();\n+    } else if (index instanceof DoubleColumnStatistics) {\n+      return ((DoubleColumnStatistics) index).getMaximum();\n+    } else if (index instanceof StringColumnStatistics) {\n+      return ((StringColumnStatistics) index).getMaximum();\n+    } else if (index instanceof DateColumnStatistics) {\n+      return ((DateColumnStatistics) index).getMaximum();\n+    } else if (index instanceof DecimalColumnStatistics) {\n+      return ((DecimalColumnStatistics) index).getMaximum();\n+    } else {\n+      return null;\n     }\n-    return null;\n   }\n \n   /**\n-   * Get the maximum value out of an index entry.\n-   * @param index the index entry\n-   * @return the object for the maximum value or null if there isn't one\n+   * Get the minimum value out of an index entry.\n+   * @param index\n+   *          the index entry\n+   * @return the object for the minimum value or null if there isn't one\n    */\n-  static Object getMax(OrcProto.ColumnStatistics index) {\n-    if (index.hasIntStatistics()) {\n-      OrcProto.IntegerStatistics stat = index.getIntStatistics();\n-      if (stat.hasMaximum()) {\n-        return stat.getMaximum();\n-      }\n-    }\n-    if (index.hasStringStatistics()) {\n-      OrcProto.StringStatistics stat = index.getStringStatistics();\n-      if (stat.hasMaximum()) {\n-        return stat.getMaximum();\n-      }\n-    }\n-    if (index.hasDoubleStatistics()) {\n-      OrcProto.DoubleStatistics stat = index.getDoubleStatistics();\n-      if (stat.hasMaximum()) {\n-        return stat.getMaximum();\n-      }\n+  static Object getMin(ColumnStatistics index) {\n+    if (index instanceof IntegerColumnStatistics) {\n+      return ((IntegerColumnStatistics) index).getMinimum();\n+    } else if (index instanceof DoubleColumnStatistics) {\n+      return ((DoubleColumnStatistics) index).getMinimum();\n+    } else if (index instanceof StringColumnStatistics) {\n+      return ((StringColumnStatistics) index).getMinimum();\n+    } else if (index instanceof DateColumnStatistics) {\n+      return ((DateColumnStatistics) index).getMinimum();\n+    } else if (index instanceof DecimalColumnStatistics) {\n+      return ((DecimalColumnStatistics) index).getMinimum();\n+    } else {\n+      return null;\n     }\n-    return null;\n   }\n \n   /**\n@@ -2228,7 +2227,8 @@ static Object getMax(OrcProto.ColumnStatistics index) {\n    */\n   static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n                                PredicateLeaf predicate) {\n-    Object minValue = getMin(index);\n+    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(index);\n+    Object minValue = getMin(cs);\n     // if we didn't have any values, everything must have been null\n     if (minValue == null) {\n       if (predicate.getOperator() == PredicateLeaf.Operator.IS_NULL) {\n@@ -2237,13 +2237,20 @@ static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n         return TruthValue.NULL;\n       }\n     }\n-    Object maxValue = getMax(index);\n+    Object maxValue = getMax(cs);\n     return evaluatePredicateRange(predicate, minValue, maxValue);\n   }\n \n-  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValue,\n-      Object maxValue) {\n+  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,\n+      Object max) {\n     Location loc;\n+\n+    // column statistics for char/varchar columns are stored as strings, so convert char/varchar\n+    // type predicates to string\n+    Object predObj = predicate.getLiteral();\n+    Object minValue = getPrimitiveObject(predObj, min);\n+    Object maxValue = getPrimitiveObject(predObj, max);\n+\n     switch (predicate.getOperator()) {\n       case NULL_SAFE_EQUALS:\n         loc = compareToRange((Comparable) predicate.getLiteral(),\n@@ -2288,6 +2295,8 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n           // for a single value, look through to see if that value is in the\n           // set\n           for(Object arg: predicate.getLiteralList()) {\n+            minValue = getPrimitiveObject(arg, min);\n+            maxValue = getPrimitiveObject(arg, max);\n             loc = compareToRange((Comparable) arg, minValue, maxValue);\n             if (loc == Location.MIN) {\n               return TruthValue.YES_NULL;\n@@ -2297,6 +2306,8 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n         } else {\n           // are all of the values outside of the range?\n           for(Object arg: predicate.getLiteralList()) {\n+            minValue = getPrimitiveObject(arg, min);\n+            maxValue = getPrimitiveObject(arg, max);\n             loc = compareToRange((Comparable) arg, minValue, maxValue);\n             if (loc == Location.MIN || loc == Location.MIDDLE ||\n                 loc == Location.MAX) {\n@@ -2307,9 +2318,16 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n         }\n       case BETWEEN:\n         List<Object> args = predicate.getLiteralList();\n+        minValue = getPrimitiveObject(args.get(0), min);\n+        maxValue = getPrimitiveObject(args.get(0), max);\n+\n         loc = compareToRange((Comparable) args.get(0), minValue, maxValue);\n         if (loc == Location.BEFORE || loc == Location.MIN) {\n-          Location loc2 = compareToRange((Comparable) args.get(1), minValue,\n+          Object predObj2 = args.get(1);\n+          minValue = getPrimitiveObject(predObj2, min);\n+          maxValue = getPrimitiveObject(predObj2, max);\n+\n+          Location loc2 = compareToRange((Comparable) predObj2, minValue,\n               maxValue);\n           if (loc2 == Location.AFTER || loc2 == Location.MAX) {\n             return TruthValue.YES_NULL;\n@@ -2330,6 +2348,38 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n     }\n   }\n \n+  private static Object getPrimitiveObject(Object predObj, Object obj) {\n+    if (obj instanceof DateWritable) {\n+      DateWritable dobj = (DateWritable) obj;\n+      if (predObj instanceof String || predObj instanceof HiveChar\n+          || predObj instanceof HiveVarchar) {\n+        return dobj.toString();\n+      }\n+    } else if (obj instanceof HiveDecimal) {\n+      HiveDecimal hdObj = (HiveDecimal) obj;\n+      if (predObj instanceof Float) {\n+        return hdObj.floatValue();\n+      } else if (predObj instanceof Double) {\n+        return hdObj.doubleValue();\n+      } else if (predObj instanceof Short) {\n+        return hdObj.shortValue();\n+      } else if (predObj instanceof Integer) {\n+        return hdObj.intValue();\n+      } else if (predObj instanceof Long) {\n+        return hdObj.longValue();\n+      } else if (predObj instanceof String || predObj instanceof HiveChar\n+          || predObj instanceof HiveVarchar) {\n+        // primitive type of char/varchar is Text (i.e trailing white spaces trimmed string)\n+        return StringUtils.stripEnd(hdObj.toString(), null);\n+      }\n+    } else if (obj instanceof String || obj instanceof HiveChar || obj instanceof HiveVarchar) {\n+      // primitive type of char/varchar is Text (i.e trailing white spaces trimmed string)\n+      return StringUtils.stripEnd(obj.toString(), null);\n+    }\n+\n+    return obj;\n+  }\n+\n   /**\n    * Pick the row groups that we need to load from the current stripe.\n    * @return an array with a boolean for each row group or null if all of the", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java", "sha": "922b99f9331aa048c4b9990f428a8ed0aa991eab", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -45,7 +45,11 @@\n   public static enum Type {\n     INTEGER, // all of the integer types\n     FLOAT,   // float and double\n-    STRING\n+    STRING,\n+    DATE,\n+    DECIMAL,\n+    CHAR,\n+    VARCHAR\n   }\n \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java", "sha": "4f26d3fade609d7e906ff54fa842551ce3dcd20b", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -283,11 +283,19 @@ Operator getOperator() {\n           case INT:\n           case LONG:\n             return PredicateLeaf.Type.INTEGER;\n+          case CHAR:\n+            return PredicateLeaf.Type.CHAR;\n+          case VARCHAR:\n+            return PredicateLeaf.Type.VARCHAR;\n           case STRING:\n             return PredicateLeaf.Type.STRING;\n           case FLOAT:\n           case DOUBLE:\n             return PredicateLeaf.Type.FLOAT;\n+          case DATE:\n+            return PredicateLeaf.Type.DATE;\n+          case DECIMAL:\n+            return PredicateLeaf.Type.DECIMAL;\n           default:\n         }\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java"}, {"additions": 253, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java", "sha": "3595e0516f456135dd3e21bbc3a18469bd91cf71", "changes": 287, "status": "modified", "deletions": 34, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -18,21 +18,21 @@\n \n package org.apache.hadoop.hive.ql.io.orc;\n \n-import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;\n-import org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl;\n-import org.junit.Test;\n-\n-import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.Location;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-\n import static junit.framework.Assert.assertEquals;\n import static org.hamcrest.core.Is.is;\n import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.Location;\n+import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;\n+import org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl;\n+import org.junit.Test;\n+\n public class TestRecordReaderImpl {\n \n   @Test\n@@ -75,26 +75,45 @@ public void testCompareToRangeString() throws Exception {\n         RecordReaderImpl.compareToRange(\"c\", \"b\", \"b\"));\n   }\n \n+  @Test\n+  public void testCompareToCharNeedConvert() throws Exception {\n+    assertEquals(Location.BEFORE,\n+        RecordReaderImpl.compareToRange(\"apple\", \"hello\", \"world\"));\n+    assertEquals(Location.AFTER,\n+        RecordReaderImpl.compareToRange(\"zombie\", \"hello\", \"world\"));\n+    assertEquals(Location.MIN,\n+        RecordReaderImpl.compareToRange(\"hello\", \"hello\", \"world\"));\n+    assertEquals(Location.MIDDLE,\n+        RecordReaderImpl.compareToRange(\"pilot\", \"hello\", \"world\"));\n+    assertEquals(Location.MAX,\n+        RecordReaderImpl.compareToRange(\"world\", \"hello\", \"world\"));\n+    assertEquals(Location.BEFORE,\n+        RecordReaderImpl.compareToRange(\"apple\", \"hello\", \"hello\"));\n+    assertEquals(Location.MIN,\n+        RecordReaderImpl.compareToRange(\"hello\", \"hello\", \"hello\"));\n+    assertEquals(Location.AFTER,\n+        RecordReaderImpl.compareToRange(\"zombie\", \"hello\", \"hello\"));\n+  }\n+\n   @Test\n   public void testGetMin() throws Exception {\n-    assertEquals(null, RecordReaderImpl.getMin(createIntStats(null, null)));\n-    assertEquals(10L, RecordReaderImpl.getMin(createIntStats(10L, 100L)));\n-    assertEquals(null, RecordReaderImpl.getMin(\n-        OrcProto.ColumnStatistics.newBuilder()\n-            .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(10.0d, RecordReaderImpl.getMin(\n+    assertEquals(10L, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(createIntStats(10L, 100L))));\n+    assertEquals(10.0d, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder()\n-                .setMinimum(10.0d).setMaximum(100.0d).build()).build()));\n-    assertEquals(null, RecordReaderImpl.getMin(\n+                .setMinimum(10.0d).setMaximum(100.0d).build()).build())));\n+    assertEquals(null, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(\"a\", RecordReaderImpl.getMin(\n+            .build())));\n+    assertEquals(\"a\", RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder()\n-                .setMinimum(\"a\").setMaximum(\"b\").build()).build()));\n+                .setMinimum(\"a\").setMaximum(\"b\").build()).build())));\n+    assertEquals(\"hello\", RecordReaderImpl.getMin(ColumnStatisticsImpl\n+        .deserialize(createStringStats(\"hello\", \"world\"))));\n+    assertEquals(HiveDecimal.create(\"111.1\"), RecordReaderImpl.getMin(ColumnStatisticsImpl\n+        .deserialize(createDecimalStats(\"111.1\", \"112.1\"))));\n   }\n \n   private static OrcProto.ColumnStatistics createIntStats(Long min,\n@@ -111,26 +130,39 @@ public void testGetMin() throws Exception {\n         .setIntStatistics(intStats.build()).build();\n   }\n \n+  private static OrcProto.ColumnStatistics createStringStats(String min, String max) {\n+    OrcProto.StringStatistics.Builder strStats = OrcProto.StringStatistics.newBuilder();\n+    strStats.setMinimum(min);\n+    strStats.setMaximum(max);\n+    return OrcProto.ColumnStatistics.newBuilder().setStringStatistics(strStats.build()).build();\n+  }\n+\n+  private static OrcProto.ColumnStatistics createDecimalStats(String min, String max) {\n+    OrcProto.DecimalStatistics.Builder decStats = OrcProto.DecimalStatistics.newBuilder();\n+    decStats.setMinimum(min);\n+    decStats.setMaximum(max);\n+    return OrcProto.ColumnStatistics.newBuilder().setDecimalStatistics(decStats.build()).build();\n+  }\n+\n   @Test\n   public void testGetMax() throws Exception {\n-    assertEquals(null, RecordReaderImpl.getMax(createIntStats(null, null)));\n-    assertEquals(100L, RecordReaderImpl.getMax(createIntStats(10L, 100L)));\n-    assertEquals(null, RecordReaderImpl.getMax(\n-        OrcProto.ColumnStatistics.newBuilder()\n-            .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(100.0d, RecordReaderImpl.getMax(\n+    assertEquals(100L, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(createIntStats(10L, 100L))));\n+    assertEquals(100.0d, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder()\n-                .setMinimum(10.0d).setMaximum(100.0d).build()).build()));\n-    assertEquals(null, RecordReaderImpl.getMax(\n+                .setMinimum(10.0d).setMaximum(100.0d).build()).build())));\n+    assertEquals(null, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(\"b\", RecordReaderImpl.getMax(\n+            .build())));\n+    assertEquals(\"b\", RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder()\n-                .setMinimum(\"a\").setMaximum(\"b\").build()).build()));\n+                .setMinimum(\"a\").setMaximum(\"b\").build()).build())));\n+    assertEquals(\"world\", RecordReaderImpl.getMax(ColumnStatisticsImpl\n+        .deserialize(createStringStats(\"hello\", \"world\"))));\n+    assertEquals(HiveDecimal.create(\"112.1\"), RecordReaderImpl.getMax(ColumnStatisticsImpl\n+        .deserialize(createDecimalStats(\"111.1\", \"112.1\"))));\n   }\n \n   @Test\n@@ -150,6 +182,37 @@ public void testEquals() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), pred));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(15L, 15L), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.EQUALS,\n+        PredicateLeaf.Type.CHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.EQUALS,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n   }\n \n   @Test\n@@ -169,6 +232,36 @@ public void testNullSafeEquals() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), pred));\n     assertEquals(TruthValue.YES_NO,\n         RecordReaderImpl.evaluatePredicate(createIntStats(15L, 15L), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,\n+        PredicateLeaf.Type.CHAR, \"x\", \"hello\", null);\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"world\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"hello\"), pred));\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"banana\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"hello\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"hello\", null);\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"world\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"hello\"), pred));\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"banana\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"hello\"), pred));\n   }\n \n   @Test\n@@ -186,6 +279,36 @@ public void testLessThan() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 15L), lessThan));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), lessThan));\n+\n+    PredicateLeaf pred = TestSearchArgumentImpl.createPredicateLeaf(\n+        PredicateLeaf.Operator.LESS_THAN, PredicateLeaf.Type.CHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.LESS_THAN,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test\n@@ -203,6 +326,36 @@ public void testLessThanEquals() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 15L), pred));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.LESS_THAN_EQUALS,\n+        PredicateLeaf.Type.CHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.LESS_THAN_EQUALS,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test\n@@ -221,6 +374,39 @@ public void testIn() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 30L), pred));\n     assertEquals(TruthValue.NO_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(12L, 18L), pred));\n+\n+    args.clear();\n+    args.add(\"a\");\n+    args.add(\"b\");\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.IN,\n+        PredicateLeaf.Type.CHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.IN,\n+        PredicateLeaf.Type.VARCHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test\n@@ -245,6 +431,39 @@ public void testBetween() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 20L), pred));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(12L, 18L), pred));\n+\n+    args.clear();\n+    args.add(\"a\");\n+    args.add(\"b\");\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.BETWEEN,\n+        PredicateLeaf.Type.CHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.BETWEEN,\n+        PredicateLeaf.Type.VARCHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java"}, {"additions": 76, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_char.q", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_char.q", "sha": "1f5f54ae19ee8035505d5aaf264fded6c82b7514", "changes": 76, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_char.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,76 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+set hive.optimize.index.filter=false;\n+\n+-- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where c=\"apple\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c=\"apple\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c!=\"apple\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c!=\"apple\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c<\"hello\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c<\"hello\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c<=\"hello\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c<=\"hello\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c=\"apple \";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c=\"apple \";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c in (\"carrot\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c in (\"carrot\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\";\n+", "filename": "ql/src/test/queries/clientpositive/orc_ppd_char.q"}, {"additions": 97, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_date.q", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_date.q", "sha": "c34be867e484f40dc074fd0579fa96bec40f7027", "changes": 97, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_date.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,97 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+-- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where da='1970-02-20';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da='1970-02-20';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da!='1970-02-20';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da!='1970-02-20';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-27';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-27';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-29';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-29';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-15';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-15';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-20';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-20';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-27';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-27';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19';", "filename": "ql/src/test/queries/clientpositive/orc_ppd_date.q"}, {"additions": 151, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_decimal.q", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_decimal.q", "sha": "a93590eacca01b2d752303eda2a1a8df6caacb47", "changes": 151, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_decimal.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,151 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+-- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where d=0.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d=0.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d='0.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d='0.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d!=0.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d!=0.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d!='0.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d!='0.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<11.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<11.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<'11.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<'11.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<cast('11.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<cast('11.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<1;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<1;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<=11.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<=11.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<='11.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<='11.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<=12;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<=12;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0');\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0');\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22');\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22');\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0');\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0');\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1000;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1000;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and '2.0';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and '2.0';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10));", "filename": "ql/src/test/queries/clientpositive/orc_ppd_decimal.q"}, {"additions": 76, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_varchar.q", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_varchar.q", "sha": "0fecc664e46db6bfdb8e1b270e3b016da68877ef", "changes": 76, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_varchar.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,76 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+set hive.optimize.index.filter=false;\n+\n+-- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where v=\"bee\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v=\"bee\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v!=\"bee\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v!=\"bee\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v<\"world\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v<\"world\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v<=\"world\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v<=\"world\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v=\"bee   \";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v=\"bee   \";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v in (\"orange\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v in (\"orange\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\";\n+", "filename": "ql/src/test/queries/clientpositive/orc_ppd_varchar.q"}, {"additions": 307, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_char.q.out", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_char.q.out", "sha": "6d92e09d942bc71f993875df0f84ac25f544a03a", "changes": 307, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_char.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,307 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where c=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where c=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL", "filename": "ql/src/test/results/clientpositive/orc_ppd_char.q.out"}, {"additions": 411, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_date.q.out", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_date.q.out", "sha": "a3e5253d97a4be97876975a75c1e4d7fb87a42a0", "changes": 411, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_date.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,411 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL", "filename": "ql/src/test/results/clientpositive/orc_ppd_date.q.out"}, {"additions": 645, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_decimal.q.out", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_decimal.q.out", "sha": "a1094b6819725da343ea71477af0e467cf77365d", "changes": 645, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_decimal.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,645 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where d=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where d=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500", "filename": "ql/src/test/results/clientpositive/orc_ppd_decimal.q.out"}, {"additions": 307, "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_varchar.q.out", "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_varchar.q.out", "sha": "0bce7605b436e0a43cf9a9e3ee53550d79c0e114", "changes": 307, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_varchar.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1", "patch": "@@ -0,0 +1,307 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where v=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where v=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL", "filename": "ql/src/test/results/clientpositive/orc_ppd_varchar.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/4d62b4621871c79de3210157dc33021d746b5b2b", "parent": "https://github.com/apache/hive/commit/2684a3ff8e8324be6541b35771b0171907ded0d1", "message": "HIVE-6205 : alter <table> partition column throws NPE in authorization (Navis via Thejas Nair)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1561391 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_234", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java", "sha": "8bb40455c48d14e77187a1d2ff8156764e19fd1a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -78,7 +78,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)\n     case HiveParser.TOK_ALTERTABLE_PARTITION:\n       if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n         return ast;\n-      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n+      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n         // unsupported\n         throw new SemanticException(\"Operation not supported.\");\n       } else {", "filename": "hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java", "sha": "75f54e2ed414ea1461bc61a2fa01c224de12cbbf", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -75,7 +75,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)\n     case HiveParser.TOK_ALTERTABLE_PARTITION:\n       if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n         return ast;\n-      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n+      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n         // unsupported\n         throw new SemanticException(\"Operation not supported.\");\n       } else {", "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "sha": "cb0c1a5b720955db0103b7494175d9f42cfd4fd5", "changes": 75, "status": "modified", "deletions": 38, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/Driver.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -527,55 +527,54 @@ private void doAuthorization(BaseSemanticAnalyzer sem)\n     SessionState ss = SessionState.get();\n     HiveOperation op = ss.getHiveOperation();\n     Hive db = sem.getDb();\n-    if(ss.isAuthorizationModeV2()){\n+    if (ss.isAuthorizationModeV2()) {\n       doAuthorizationV2(ss, op, inputs, outputs);\n       return;\n     }\n \n-    if (op != null) {\n-      if (op.equals(HiveOperation.CREATEDATABASE)) {\n-        ss.getAuthorizer().authorize(\n-            op.getInputRequiredPrivileges(), op.getOutputRequiredPrivileges());\n-      } else if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n-          || op.equals(HiveOperation.CREATETABLE)) {\n-        ss.getAuthorizer().authorize(\n-            db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n-            HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n-      } else {\n-        if (op.equals(HiveOperation.IMPORT)) {\n-          ImportSemanticAnalyzer isa = (ImportSemanticAnalyzer) sem;\n-          if (!isa.existsTable()) {\n-            ss.getAuthorizer().authorize(\n-                db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n-                HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n-          }\n+    if (op == null) {\n+      throw new HiveException(\"Operation should not be null\");\n+    }\n+    if (op.equals(HiveOperation.CREATEDATABASE)) {\n+      ss.getAuthorizer().authorize(\n+          op.getInputRequiredPrivileges(), op.getOutputRequiredPrivileges());\n+    } else if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n+        || op.equals(HiveOperation.CREATETABLE)) {\n+      ss.getAuthorizer().authorize(\n+          db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n+          HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n+    } else {\n+      if (op.equals(HiveOperation.IMPORT)) {\n+        ImportSemanticAnalyzer isa = (ImportSemanticAnalyzer) sem;\n+        if (!isa.existsTable()) {\n+          ss.getAuthorizer().authorize(\n+              db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n+              HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n         }\n       }\n-      if (outputs != null && outputs.size() > 0) {\n-        //do authorization for each output\n-        for (WriteEntity write : outputs) {\n-          if (write.getType() == Entity.Type.DATABASE) {\n-            ss.getAuthorizer().authorize(write.getDatabase(),\n-                null, op.getOutputRequiredPrivileges());\n-            continue;\n-          }\n-\n-          if (write.getType() == WriteEntity.Type.PARTITION) {\n-            Partition part = db.getPartition(write.getTable(), write\n-                .getPartition().getSpec(), false);\n-            if (part != null) {\n-              ss.getAuthorizer().authorize(write.getPartition(), null,\n-                      op.getOutputRequiredPrivileges());\n-              continue;\n-            }\n-          }\n+    }\n+    if (outputs != null && outputs.size() > 0) {\n+      for (WriteEntity write : outputs) {\n+        if (write.getType() == Entity.Type.DATABASE) {\n+          ss.getAuthorizer().authorize(write.getDatabase(),\n+              null, op.getOutputRequiredPrivileges());\n+          continue;\n+        }\n \n-          if (write.getTable() != null) {\n-            ss.getAuthorizer().authorize(write.getTable(), null,\n+        if (write.getType() == WriteEntity.Type.PARTITION) {\n+          Partition part = db.getPartition(write.getTable(), write\n+              .getPartition().getSpec(), false);\n+          if (part != null) {\n+            ss.getAuthorizer().authorize(write.getPartition(), null,\n                     op.getOutputRequiredPrivileges());\n+            continue;\n           }\n         }\n \n+        if (write.getTable() != null) {\n+          ss.getAuthorizer().authorize(write.getTable(), null,\n+                  op.getOutputRequiredPrivileges());\n+        }\n       }\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "sha": "f4d9a83bf3c792bb5e7d18fcf3438a06fd1780f7", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -233,11 +233,11 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {\n       ast = (ASTNode) ast.getChild(1);\n       if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n         analyzeAlterTableFileFormat(ast, tableName, partSpec);\n-      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE) {\n+      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_PROTECTMODE) {\n         analyzeAlterTableProtectMode(ast, tableName, partSpec);\n       } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_LOCATION) {\n         analyzeAlterTableLocation(ast, tableName, partSpec);\n-      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n+      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n         analyzeAlterTablePartMergeFiles(tablePart, ast, tableName, partSpec);\n       } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_SERIALIZER) {\n         analyzeAlterTableSerde(ast, tableName, partSpec);\n@@ -365,8 +365,8 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {\n     case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n       analyzeAlterTableDropParts(ast, false);\n       break;\n-    case HiveParser.TOK_ALTERTABLE_ALTERPARTS:\n-      analyzeAlterTableAlterParts(ast);\n+    case HiveParser.TOK_ALTERTABLE_PARTCOLTYPE:\n+      analyzeAlterTablePartColType(ast);\n       break;\n     case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n       analyzeAlterTableProps(ast, false, false);\n@@ -2433,7 +2433,7 @@ private void analyzeAlterTableDropParts(ASTNode ast, boolean expectView)\n         dropTblDesc), conf));\n   }\n \n-  private void analyzeAlterTableAlterParts(ASTNode ast)\n+  private void analyzeAlterTablePartColType(ASTNode ast)\n       throws SemanticException {\n     // get table name\n     String tblName = getUnescapedName((ASTNode)ast.getChild(0));", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g", "sha": "216c361148a695d32fa1eb67e51fb14e54843abf", "changes": 20, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -138,8 +138,9 @@ TOK_ALTERTABLE_RENAMEPART;\n TOK_ALTERTABLE_REPLACECOLS;\n TOK_ALTERTABLE_ADDPARTS;\n TOK_ALTERTABLE_DROPPARTS;\n-TOK_ALTERTABLE_ALTERPARTS;\n-TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE;\n+TOK_ALTERTABLE_PARTCOLTYPE;\n+TOK_ALTERTABLE_PROTECTMODE;\n+TOK_ALTERTABLE_MERGEFILES;\n TOK_ALTERTABLE_TOUCH;\n TOK_ALTERTABLE_ARCHIVE;\n TOK_ALTERTABLE_UNARCHIVE;\n@@ -285,7 +286,6 @@ TOK_DATABASEPROPERTIES;\n TOK_DATABASELOCATION;\n TOK_DBPROPLIST;\n TOK_ALTERDATABASE_PROPERTIES;\n-TOK_ALTERTABLE_ALTERPARTS_MERGEFILES;\n TOK_TABNAME;\n TOK_TABSRC;\n TOK_RESTRICT;\n@@ -900,8 +900,16 @@ alterTableStatementSuffix\n     | alterTblPartitionStatement\n     | alterStatementSuffixSkewedby\n     | alterStatementSuffixExchangePartition\n+    | alterStatementPartitionKeyType\n     ;\n \n+alterStatementPartitionKeyType\n+@init {msgs.push(\"alter partition key type\"); }\n+@after {msgs.pop();}\n+\t: identifier KW_PARTITION KW_COLUMN LPAREN columnNameType RPAREN\n+\t-> ^(TOK_ALTERTABLE_PARTCOLTYPE identifier columnNameType)\n+\t;\n+\n alterViewStatementSuffix\n @init { msgs.push(\"alter view statement\"); }\n @after { msgs.pop(); }\n@@ -1058,8 +1066,6 @@ alterTblPartitionStatement\n @after {msgs.pop();}\n   : tablePartitionPrefix alterTblPartitionStatementSuffix\n   -> ^(TOK_ALTERTABLE_PARTITION tablePartitionPrefix alterTblPartitionStatementSuffix)\n-  |Identifier KW_PARTITION KW_COLUMN LPAREN columnNameType RPAREN\n-  -> ^(TOK_ALTERTABLE_ALTERPARTS Identifier columnNameType)\n   ;\n \n alterTblPartitionStatementSuffix\n@@ -1151,7 +1157,7 @@ alterStatementSuffixProtectMode\n @init { msgs.push(\"alter partition protect mode statement\"); }\n @after { msgs.pop(); }\n     : alterProtectMode\n-    -> ^(TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE alterProtectMode)\n+    -> ^(TOK_ALTERTABLE_PROTECTMODE alterProtectMode)\n     ;\n \n alterStatementSuffixRenamePart\n@@ -1165,7 +1171,7 @@ alterStatementSuffixMergeFiles\n @init { msgs.push(\"\"); }\n @after { msgs.pop(); }\n     : KW_CONCATENATE\n-    -> ^(TOK_ALTERTABLE_ALTERPARTS_MERGEFILES)\n+    -> ^(TOK_ALTERTABLE_MERGEFILES)\n     ;\n \n alterProtectMode", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java", "sha": "2495c40e0c1a0f0f60e8fe12f8265723cdd51c9b", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -102,11 +102,12 @@\n     commandType.put(HiveParser.TOK_ALTERTABLE_SKEWED, HiveOperation.ALTERTABLE_SKEWED);\n     commandType.put(HiveParser.TOK_ANALYZE, HiveOperation.ANALYZE_TABLE);\n     commandType.put(HiveParser.TOK_ALTERVIEW_RENAME, HiveOperation.ALTERVIEW_RENAME);\n+    commandType.put(HiveParser.TOK_ALTERTABLE_PARTCOLTYPE, HiveOperation.ALTERTABLE_PARTCOLTYPE);\n   }\n \n   static {\n     tablePartitionCommandType.put(\n-        HiveParser.TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE,\n+        HiveParser.TOK_ALTERTABLE_PROTECTMODE,\n         new HiveOperation[] { HiveOperation.ALTERTABLE_PROTECTMODE,\n             HiveOperation.ALTERPARTITION_PROTECTMODE });\n     tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_FILEFORMAT,\n@@ -115,7 +116,7 @@\n     tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_LOCATION,\n         new HiveOperation[] { HiveOperation.ALTERTABLE_LOCATION,\n             HiveOperation.ALTERPARTITION_LOCATION });\n-    tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES,\n+    tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_MERGEFILES,\n         new HiveOperation[] {HiveOperation.ALTERTABLE_MERGEFILES,\n             HiveOperation.ALTERPARTITION_MERGEFILES });\n     tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_SERIALIZER,\n@@ -172,6 +173,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)\n       case HiveParser.TOK_DROPTABLE_PROPERTIES:\n       case HiveParser.TOK_ALTERTABLE_SERIALIZER:\n       case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:\n+      case HiveParser.TOK_ALTERTABLE_PARTCOLTYPE:\n       case HiveParser.TOK_ALTERINDEX_REBUILD:\n       case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n       case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n@@ -196,7 +198,6 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)\n       case HiveParser.TOK_ALTERTABLE_TOUCH:\n       case HiveParser.TOK_ALTERTABLE_ARCHIVE:\n       case HiveParser.TOK_ALTERTABLE_UNARCHIVE:\n-      case HiveParser.TOK_ALTERTABLE_ALTERPARTS:\n       case HiveParser.TOK_LOCKTABLE:\n       case HiveParser.TOK_UNLOCKTABLE:\n       case HiveParser.TOK_LOCKDB:", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java", "sha": "e1a3dce303df13165fc064bdb5a92ec57eba8aa6", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -106,6 +106,7 @@\n   ALTERTABLE_SKEWED(\"ALTERTABLE_SKEWED\", new Privilege[] {Privilege.ALTER_METADATA}, null),\n   ALTERTBLPART_SKEWED_LOCATION(\"ALTERTBLPART_SKEWED_LOCATION\",\n       new Privilege[] {Privilege.ALTER_DATA}, null),\n+  ALTERTABLE_PARTCOLTYPE(\"ALTERTABLE_PARTCOLTYPE\", new Privilege[] { Privilege.SELECT }, new Privilege[] { Privilege.ALTER_DATA }),\n   ALTERVIEW_RENAME(\"ALTERVIEW_RENAME\", new Privilege[] {Privilege.ALTER_METADATA}, null),\n   ;\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java", "sha": "0fcfe5252242c570ac37a700b2d0cf2a52e1e8e2", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -49,6 +49,7 @@\n   ALTERTABLE_UNARCHIVE,\n   ALTERTABLE_PROPERTIES,\n   ALTERTABLE_SERIALIZER,\n+  ALTERTABLE_PARTCOLTYPE,\n   ALTERPARTITION_SERIALIZER,\n   ALTERTABLE_SERDEPROPERTIES,\n   ALTERPARTITION_SERDEPROPERTIES,", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out", "sha": "c8f4021d7c47d51a4128b46bba53d6d70cbc52b3", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -33,4 +33,4 @@ ts                  \tstring              \tNone\n \t \t \n dt                  \tstring              \tNone                \n ts                  \tstring              \tNone                \n-FAILED: ParseException line 4:50 mismatched input ',' expecting ) near 'int' in alter table partition statement\n+FAILED: ParseException line 4:50 mismatched input ',' expecting ) near 'int' in alter partition key type", "filename": "ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out", "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out", "sha": "04b9b2ce5c3639d8a5fae29514b396b199c2c8c5", "changes": 12, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/alter_partition_coltype.q.out?ref=4d62b4621871c79de3210157dc33021d746b5b2b", "patch": "@@ -50,11 +50,11 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src\n 25\n PREHOOK: query: -- alter partition key column data type for dt column.\n alter table alter_coltype partition column (dt int)\n-PREHOOK: type: null\n+PREHOOK: type: ALTERTABLE_PARTCOLTYPE\n PREHOOK: Input: default@alter_coltype\n POSTHOOK: query: -- alter partition key column data type for dt column.\n alter table alter_coltype partition column (dt int)\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERTABLE_PARTCOLTYPE\n POSTHOOK: Input: default@alter_coltype\n POSTHOOK: Output: default@alter_coltype\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]\n@@ -240,22 +240,22 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src\n 0\n PREHOOK: query: -- alter partition key column data type for ts column.\n alter table alter_coltype partition column (ts double)\n-PREHOOK: type: null\n+PREHOOK: type: ALTERTABLE_PARTCOLTYPE\n PREHOOK: Input: default@alter_coltype\n POSTHOOK: query: -- alter partition key column data type for ts column.\n alter table alter_coltype partition column (ts double)\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERTABLE_PARTCOLTYPE\n POSTHOOK: Input: default@alter_coltype\n POSTHOOK: Output: default@alter_coltype\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]\n PREHOOK: query: alter table alter_coltype partition column (dt string)\n-PREHOOK: type: null\n+PREHOOK: type: ALTERTABLE_PARTCOLTYPE\n PREHOOK: Input: default@alter_coltype\n POSTHOOK: query: alter table alter_coltype partition column (dt string)\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERTABLE_PARTCOLTYPE\n POSTHOOK: Input: default@alter_coltype\n POSTHOOK: Output: default@alter_coltype\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]", "filename": "ql/src/test/results/clientpositive/alter_partition_coltype.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a1b07ca59db6f92bb574e136a580f0b2b6ac1490", "parent": "https://github.com/apache/hive/commit/45a2616ab18a136b3873fba57c715ece0d09cbbe", "message": "HIVE-6231: NPE when switching to Tez execution mode after session has been initialized (Patch by Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1560268 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_235", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/a1b07ca59db6f92bb574e136a580f0b2b6ac1490/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "blob_url": "https://github.com/apache/hive/blob/a1b07ca59db6f92bb574e136a580f0b2b6ac1490/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "sha": "c6f431c35f37523983a1e6aa40e57cf8abab9cb7", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=a1b07ca59db6f92bb574e136a580f0b2b6ac1490", "patch": "@@ -114,7 +114,8 @@ public int execute(DriverContext driverContext) {\n \n       // if we don't have one yet create it.\n       if (session == null) {\n-        ss.setTezSession(new TezSessionState());\n+        session = new TezSessionState();\n+        ss.setTezSession(session);\n       }\n \n       // if it's not running start it.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/9544297803858557b776822296663c0438cf0723", "parent": "https://github.com/apache/hive/commit/329d7f354a4ff3b669a2a1aa23b19469814307c2", "message": "HIVE-6097: Sessions on Tez NPE when quitting CLI (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1553108 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_236", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/9544297803858557b776822296663c0438cf0723/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "blob_url": "https://github.com/apache/hive/blob/9544297803858557b776822296663c0438cf0723/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "sha": "3e69f01c14cc1c314f9d77cc51061b3a2fd6ada4", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=9544297803858557b776822296663c0438cf0723", "patch": "@@ -831,9 +831,13 @@ public void close() throws IOException {\n     }\n \n     try {\n-      tezSessionState.close(false);\n+      if (tezSessionState != null) {\n+        tezSessionState.close(false);\n+      }\n     } catch (Exception e) {\n       LOG.info(\"Error closing tez session\", e);\n+    } finally {\n+      tezSessionState = null;\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/9544297803858557b776822296663c0438cf0723/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java", "blob_url": "https://github.com/apache/hive/blob/9544297803858557b776822296663c0438cf0723/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java", "sha": "d4e737fe85f6c6b565b1bba3b4eb573f17f5260c", "changes": 9, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java?ref=9544297803858557b776822296663c0438cf0723", "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hive.ql.session;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n@@ -57,5 +58,11 @@ public void testgetDbName() throws Exception {\n \n   }\n \n-\n+  @Test\n+  public void testClose() throws Exception {\n+    SessionState ss = SessionState.get();\n+    assertNull(ss.getTezSession());\n+    ss.close();\n+    assertNull(ss.getTezSession());\n+  }\n }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/e8221ab0e147580441e89a516c2570a3acaa8e17", "parent": "https://github.com/apache/hive/commit/a93fe77129d10cbd4ec37ae320b8ae32e9305f2f", "message": "HIVE-5580. Predicate pushdown predicates with an and-operator between \nnon-SARGable predicates cause a NPE. (omalley)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1550010 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_237", "file": [{"additions": 4, "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java", "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java", "sha": "1663d785d412d4a92252fef29ae68fc30b22f6cc", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java?ref=e8221ab0e147580441e89a516c2570a3acaa8e17", "patch": "@@ -573,7 +573,7 @@ static ExpressionTree pushDownNot(ExpressionTree root) {\n      * @param expr The expression to clean up\n      * @return The cleaned up expression\n      */\n-    ExpressionTree foldMaybe(ExpressionTree expr) {\n+    static ExpressionTree foldMaybe(ExpressionTree expr) {\n       if (expr.children != null) {\n         for(int i=0; i < expr.children.size(); ++i) {\n           ExpressionTree child = foldMaybe(expr.children.get(i));\n@@ -594,6 +594,9 @@ ExpressionTree foldMaybe(ExpressionTree expr) {\n             expr.children.set(i, child);\n           }\n         }\n+        if (expr.children.isEmpty()) {\n+          return new ExpressionTree(TruthValue.YES_NO_NULL);\n+        }\n       }\n       return expr;\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java"}, {"additions": 25, "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java", "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java", "sha": "6a2a56a831cd682b00dc63a904b79643867ba273", "changes": 25, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java?ref=e8221ab0e147580441e89a516c2570a3acaa8e17", "patch": "@@ -146,6 +146,31 @@ public void testFlatten() throws Exception {\n         ).toString());\n   }\n \n+  @Test\n+  public void testFoldMaybe() throws Exception {\n+    assertEquals(\"(and leaf-1)\",\n+        ExpressionBuilder.foldMaybe(and(leaf(1),\n+            constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"(and leaf-1 leaf-2)\",\n+        ExpressionBuilder.foldMaybe(and(leaf(1),\n+            constant(TruthValue.YES_NO_NULL), leaf(2))).toString());\n+    assertEquals(\"(and leaf-1 leaf-2)\",\n+        ExpressionBuilder.foldMaybe(and(constant(TruthValue.YES_NO_NULL),\n+            leaf(1), leaf(2), constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"YES_NO_NULL\",\n+        ExpressionBuilder.foldMaybe(and(constant(TruthValue.YES_NO_NULL),\n+            constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"YES_NO_NULL\",\n+        ExpressionBuilder.foldMaybe(or(leaf(1),\n+            constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"(or leaf-1 (and leaf-2))\",\n+        ExpressionBuilder.foldMaybe(or(leaf(1),\n+            and(leaf(2), constant(TruthValue.YES_NO_NULL)))).toString());\n+    assertEquals(\"(and leaf-1)\",\n+        ExpressionBuilder.foldMaybe(and(or(leaf(2),\n+            constant(TruthValue.YES_NO_NULL)), leaf(1))).toString());\n+  }\n+\n   @Test\n   public void testCNF() throws Exception {\n     assertEquals(\"leaf-1\", ExpressionBuilder.convertToCNF(leaf(1)).toString());", "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/queries/clientpositive/orc_create.q", "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/queries/clientpositive/orc_create.q", "sha": "823727d5edc604f79714c968b4528407acafad84", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_create.q?ref=e8221ab0e147580441e89a516c2570a3acaa8e17", "patch": "@@ -92,6 +92,15 @@ SET hive.optimize.index.filter=true;\n -- test predicate push down with partition pruning\n SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca';\n \n+-- test predicate push down\n+SELECT COUNT(*) FROM orc_create_people where id = 50;\n+SELECT COUNT(*) FROM orc_create_people where id between 10 and 20;\n+SELECT COUNT(*) FROM orc_create_people where id > 10 and id < 100;\n+SELECT COUNT(*) FROM orc_create_people where (id + 1) = 20;\n+SELECT COUNT(*) FROM orc_create_people where (id + 10) < 200;\n+SELECT COUNT(*) FROM orc_create_people where id < 30  or first_name = \"Rafael\";\n+SELECT COUNT(*) FROM orc_create_people where length(substr(first_name, 1, 2)) <= 2 and last_name like '%';\n+\n -- test predicate push down with no column projection\n SELECT id, first_name, last_name, address\n   FROM orc_create_people WHERE id > 90;", "filename": "ql/src/test/queries/clientpositive/orc_create.q"}, {"additions": 177, "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/results/clientpositive/orc_create.q.out", "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/results/clientpositive/orc_create.q.out", "sha": "259520ed458ef8d589ba7fab2a5f4753744fc2d1", "changes": 177, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_create.q.out?ref=e8221ab0e147580441e89a516c2570a3acaa8e17", "patch": "@@ -506,6 +506,183 @@ POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc\n POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n 5\n+PREHOOK: query: -- test predicate push down\n+SELECT COUNT(*) FROM orc_create_people where id = 50\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- test predicate push down\n+SELECT COUNT(*) FROM orc_create_people where id = 50\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+1\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where id between 10 and 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where id between 10 and 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+11\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where id > 10 and id < 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where id > 10 and id < 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+89\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 1) = 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 1) = 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+1\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 10) < 200\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 10) < 200\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+100\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where id < 30  or first_name = \"Rafael\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where id < 30  or first_name = \"Rafael\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+30\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where length(substr(first_name, 1, 2)) <= 2 and last_name like '%'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where length(substr(first_name, 1, 2)) <= 2 and last_name like '%'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+100\n PREHOOK: query: -- test predicate push down with no column projection\n SELECT id, first_name, last_name, address\n   FROM orc_create_people WHERE id > 90", "filename": "ql/src/test/results/clientpositive/orc_create.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/01a69f00bb8a0ee6467e54e21da0d5e645e6798c", "parent": "https://github.com/apache/hive/commit/622add5609c6d1caa9c15e176415c659d6c3724e", "message": "HIVE-5899 NPE during explain extended with char/varchar columns (Jason Dere via Harish Butani)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1548312 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_238", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/data/files/alltypes.txt", "blob_url": "https://github.com/apache/hive/blob/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/data/files/alltypes.txt", "sha": "358cf400ec81575a658653faa1601dce5f6b4f32", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/alltypes.txt?ref=01a69f00bb8a0ee6467e54e21da0d5e645e6798c", "patch": "@@ -1,2 +1,2 @@\n-true|10|100|1000|10000|4.0|20.0|2.2222|1969-12-31 15:59:58.174|1970-01-01 00:00:00|hello|k1:v1,k2:v2|100,200|{10, \"foo\"}\n-true|20|200|2000|20000|8.0|40.0|4.2222|1970-12-31 15:59:58.174|1971-01-01 00:00:00||k3:v3,k4:v4|200,300|{20, \"bar\"}\n+true|10|100|1000|10000|4.0|20.0|2.2222|1969-12-31 15:59:58.174|1970-01-01 00:00:00|hello|hello|k1:v1,k2:v2|100,200|{10, \"foo\"}\n+true|20|200|2000|20000|8.0|40.0|4.2222|1970-12-31 15:59:58.174|1971-01-01 00:00:00|||k3:v3,k4:v4|200,300|{20, \"bar\"}", "filename": "data/files/alltypes.txt"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "blob_url": "https://github.com/apache/hive/blob/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "sha": "04d399f34d23627f93200d3bdc25e3a1a9236271", "changes": 34, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=01a69f00bb8a0ee6467e54e21da0d5e645e6798c", "patch": "@@ -4943,38 +4943,39 @@ private ColumnStatisticsObj getTableColumnStatisticsObj(MTableColumnStatistics m\n     ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n     statsObj.setColType(mStatsObj.getColType());\n     statsObj.setColName(mStatsObj.getColName());\n-    String colType = mStatsObj.getColType();\n+    String colType = mStatsObj.getColType().toLowerCase();\n     ColumnStatisticsData colStatsData = new ColumnStatisticsData();\n \n-    if (colType.equalsIgnoreCase(\"boolean\")) {\n+    if (colType.equals(\"boolean\")) {\n       BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n       boolStats.setNumFalses(mStatsObj.getNumFalses());\n       boolStats.setNumTrues(mStatsObj.getNumTrues());\n       boolStats.setNumNulls(mStatsObj.getNumNulls());\n       colStatsData.setBooleanStats(boolStats);\n-    } else if (colType.equalsIgnoreCase(\"string\")) {\n+    } else if (colType.equals(\"string\") ||\n+        colType.startsWith(\"varchar\") || colType.startsWith(\"char\")) {\n       StringColumnStatsData stringStats = new StringColumnStatsData();\n       stringStats.setNumNulls(mStatsObj.getNumNulls());\n       stringStats.setAvgColLen(mStatsObj.getAvgColLen());\n       stringStats.setMaxColLen(mStatsObj.getMaxColLen());\n       stringStats.setNumDVs(mStatsObj.getNumDVs());\n       colStatsData.setStringStats(stringStats);\n-    } else if (colType.equalsIgnoreCase(\"binary\")) {\n+    } else if (colType.equals(\"binary\")) {\n       BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n       binaryStats.setNumNulls(mStatsObj.getNumNulls());\n       binaryStats.setAvgColLen(mStatsObj.getAvgColLen());\n       binaryStats.setMaxColLen(mStatsObj.getMaxColLen());\n       colStatsData.setBinaryStats(binaryStats);\n-    } else if (colType.equalsIgnoreCase(\"bigint\") || colType.equalsIgnoreCase(\"int\") ||\n-        colType.equalsIgnoreCase(\"smallint\") || colType.equalsIgnoreCase(\"tinyint\") ||\n-        colType.equalsIgnoreCase(\"timestamp\")) {\n+    } else if (colType.equals(\"bigint\") || colType.equals(\"int\") ||\n+        colType.equals(\"smallint\") || colType.equals(\"tinyint\") ||\n+        colType.equals(\"timestamp\")) {\n       LongColumnStatsData longStats = new LongColumnStatsData();\n       longStats.setNumNulls(mStatsObj.getNumNulls());\n       longStats.setHighValue(mStatsObj.getLongHighValue());\n       longStats.setLowValue(mStatsObj.getLongLowValue());\n       longStats.setNumDVs(mStatsObj.getNumDVs());\n       colStatsData.setLongStats(longStats);\n-   } else if (colType.equalsIgnoreCase(\"double\") || colType.equalsIgnoreCase(\"float\")) {\n+   } else if (colType.equals(\"double\") || colType.equals(\"float\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNulls(mStatsObj.getNumNulls());\n      doubleStats.setHighValue(mStatsObj.getDoubleHighValue());\n@@ -5119,38 +5120,39 @@ private ColumnStatisticsObj getPartitionColumnStatisticsObj(MPartitionColumnStat\n     ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n     statsObj.setColType(mStatsObj.getColType());\n     statsObj.setColName(mStatsObj.getColName());\n-    String colType = mStatsObj.getColType();\n+    String colType = mStatsObj.getColType().toLowerCase();\n     ColumnStatisticsData colStatsData = new ColumnStatisticsData();\n \n-    if (colType.equalsIgnoreCase(\"boolean\")) {\n+    if (colType.equals(\"boolean\")) {\n       BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n       boolStats.setNumFalses(mStatsObj.getNumFalses());\n       boolStats.setNumTrues(mStatsObj.getNumTrues());\n       boolStats.setNumNulls(mStatsObj.getNumNulls());\n       colStatsData.setBooleanStats(boolStats);\n-    } else if (colType.equalsIgnoreCase(\"string\")) {\n+    } else if (colType.equals(\"string\") ||\n+        colType.startsWith(\"varchar\") || colType.startsWith(\"char\")) {\n       StringColumnStatsData stringStats = new StringColumnStatsData();\n       stringStats.setNumNulls(mStatsObj.getNumNulls());\n       stringStats.setAvgColLen(mStatsObj.getAvgColLen());\n       stringStats.setMaxColLen(mStatsObj.getMaxColLen());\n       stringStats.setNumDVs(mStatsObj.getNumDVs());\n       colStatsData.setStringStats(stringStats);\n-    } else if (colType.equalsIgnoreCase(\"binary\")) {\n+    } else if (colType.equals(\"binary\")) {\n       BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n       binaryStats.setNumNulls(mStatsObj.getNumNulls());\n       binaryStats.setAvgColLen(mStatsObj.getAvgColLen());\n       binaryStats.setMaxColLen(mStatsObj.getMaxColLen());\n       colStatsData.setBinaryStats(binaryStats);\n-    } else if (colType.equalsIgnoreCase(\"tinyint\") || colType.equalsIgnoreCase(\"smallint\") ||\n-        colType.equalsIgnoreCase(\"int\") || colType.equalsIgnoreCase(\"bigint\") ||\n-        colType.equalsIgnoreCase(\"timestamp\")) {\n+    } else if (colType.equals(\"tinyint\") || colType.equals(\"smallint\") ||\n+        colType.equals(\"int\") || colType.equals(\"bigint\") ||\n+        colType.equals(\"timestamp\")) {\n       LongColumnStatsData longStats = new LongColumnStatsData();\n       longStats.setNumNulls(mStatsObj.getNumNulls());\n       longStats.setHighValue(mStatsObj.getLongHighValue());\n       longStats.setLowValue(mStatsObj.getLongLowValue());\n       longStats.setNumDVs(mStatsObj.getNumDVs());\n       colStatsData.setLongStats(longStats);\n-   } else if (colType.equalsIgnoreCase(\"double\") || colType.equalsIgnoreCase(\"float\")) {\n+   } else if (colType.equals(\"double\") || colType.equals(\"float\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNulls(mStatsObj.getNumNulls());\n      doubleStats.setHighValue(mStatsObj.getDoubleHighValue());", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java", "blob_url": "https://github.com/apache/hive/blob/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java", "sha": "7e9bed62fd9bba8c3c189f28e80aae3f12845bc0", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java?ref=01a69f00bb8a0ee6467e54e21da0d5e645e6798c", "patch": "@@ -1864,6 +1864,8 @@ private long getRawDataSizeFromPrimitives(TreeWriter child, ObjectInspector oi)\n     case DOUBLE:\n       return numVals * JavaDataModel.get().primitive2();\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n       // ORC strings are converted to java Strings. so use JavaDataModel to\n       // compute the overall size of strings\n       child = (StringTreeWriter) child;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/ql/src/test/queries/clientpositive/annotate_stats_select.q", "blob_url": "https://github.com/apache/hive/blob/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/ql/src/test/queries/clientpositive/annotate_stats_select.q", "sha": "5fc3f64b90180e6663aa187435fe5b3d59f80032", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/annotate_stats_select.q?ref=01a69f00bb8a0ee6467e54e21da0d5e645e6798c", "patch": "@@ -12,6 +12,7 @@ create table if not exists alltypes (\n  ts1 timestamp,\n  da1 timestamp,\n  s1 string,\n+ vc1 varchar(5),\n  m1 map<string, string>,\n  l1 array<int>,\n  st1 struct<c1:int, c2:string>\n@@ -30,7 +31,7 @@ insert overwrite table alltypes_orc select * from alltypes;\n explain extended select * from alltypes_orc;\n \n -- statistics for complex types are not supported yet\n-analyze table alltypes_orc compute statistics for columns bo1, ti1, si1, i1, bi1, f1, d1,s1;\n+analyze table alltypes_orc compute statistics for columns bo1, ti1, si1, i1, bi1, f1, d1, s1, vc1;\n \n -- numRows: 2 rawDataSize: 1514\n explain extended select * from alltypes_orc;", "filename": "ql/src/test/queries/clientpositive/annotate_stats_select.q"}, {"additions": 413, "raw_url": "https://github.com/apache/hive/raw/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/ql/src/test/results/clientpositive/annotate_stats_select.q.out", "blob_url": "https://github.com/apache/hive/blob/01a69f00bb8a0ee6467e54e21da0d5e645e6798c/ql/src/test/results/clientpositive/annotate_stats_select.q.out", "sha": "66d11eadd9e5990db5364522d53890feac21fb9d", "changes": 782, "status": "modified", "deletions": 369, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/annotate_stats_select.q.out?ref=01a69f00bb8a0ee6467e54e21da0d5e645e6798c", "filename": "ql/src/test/results/clientpositive/annotate_stats_select.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2", "parent": "https://github.com/apache/hive/commit/934fad2564e7732fa4210176a3ca2d8dd7933bc8", "message": "HIVE-5765 - Beeline throws NPE when -e option is used (Szehon Ho via Brock Noland)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1539694 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_239", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2/beeline/src/java/org/apache/hive/beeline/Commands.java", "blob_url": "https://github.com/apache/hive/blob/cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2/beeline/src/java/org/apache/hive/beeline/Commands.java", "sha": "d2d7fd3bfec9cc279a61323ce695b7e80973d0a6", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/Commands.java?ref=cc07902b2ea3a0dcc72b905c3e33c4afbd8c80b2", "patch": "@@ -674,7 +674,10 @@ private boolean execute(String line, boolean call) {\n \n     // use multiple lines for statements not terminated by \";\"\n     try {\n-      while (!(line.trim().endsWith(\";\")) && beeLine.getOpts().isAllowMultiLineCommand()) {\n+      //When using -e, console reader is not initialized and command is a single line\n+      while (beeLine.getConsoleReader() != null && !(line.trim().endsWith(\";\"))\n+        && beeLine.getOpts().isAllowMultiLineCommand()) {\n+\n         StringBuilder prompt = new StringBuilder(beeLine.getPrompt());\n         for (int i = 0; i < prompt.length() - 1; i++) {\n           if (prompt.charAt(i) != '>') {\n@@ -691,6 +694,7 @@ private boolean execute(String line, boolean call) {\n       beeLine.handleException(e);\n     }\n \n+\n     if (line.endsWith(\";\")) {\n       line = line.substring(0, line.length() - 1);\n     }", "filename": "beeline/src/java/org/apache/hive/beeline/Commands.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/102f62ca99e14e74bb468c113f5fe6c51a89f037", "parent": "https://github.com/apache/hive/commit/3da2281825671e386c18a718387e30902b97e286", "message": "HIVE-5730: Beeline throws non-terminal NPE upon starting, after mavenization (Szehon Ho reviewed by Navis)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1539117 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_240", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/102f62ca99e14e74bb468c113f5fe6c51a89f037/beeline/src/java/org/apache/hive/beeline/SQLCompletor.java", "blob_url": "https://github.com/apache/hive/blob/102f62ca99e14e74bb468c113f5fe6c51a89f037/beeline/src/java/org/apache/hive/beeline/SQLCompletor.java", "sha": "844b9ae313e5d4cb5680d55a90255418b84155f9", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/SQLCompletor.java?ref=102f62ca99e14e74bb468c113f5fe6c51a89f037", "patch": "@@ -45,7 +45,7 @@ public SQLCompletor(BeeLine beeLine, boolean skipmeta)\n     // add the default SQL completions\n     String keywords = new BufferedReader(new InputStreamReader(\n         SQLCompletor.class.getResourceAsStream(\n-            \"sql-keywords.properties\"))).readLine();\n+            \"/sql-keywords.properties\"))).readLine();\n \n     // now add the keywords from the current connection\n     try {", "filename": "beeline/src/java/org/apache/hive/beeline/SQLCompletor.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/812876a26de199520c07d45daa52b90d611d46b5", "parent": "https://github.com/apache/hive/commit/c5618e6b43204d2f57a459c34a07cf923d1d9f46", "message": "HIVE-5597 : Temporary fix for HIVE-5172 (possible NPE in TUGIContainingTransport) (agate via Navis)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1534524 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_241", "file": [{"additions": 9, "raw_url": "https://github.com/apache/hive/raw/812876a26de199520c07d45daa52b90d611d46b5/shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java", "blob_url": "https://github.com/apache/hive/blob/812876a26de199520c07d45daa52b90d611d46b5/shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java", "sha": "46057fa8941ac89b6497d636b617752f17fa097f", "changes": 11, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java?ref=812876a26de199520c07d45daa52b90d611d46b5", "patch": "@@ -82,8 +82,15 @@ public TUGIContainingTransport getTransport(TTransport trans) {\n \n       // UGI information is not available at connection setup time, it will be set later\n       // via set_ugi() rpc.\n-      transMap.putIfAbsent(trans, new TUGIContainingTransport(trans));\n-      return transMap.get(trans);\n+      TUGIContainingTransport tugiTrans = transMap.get(trans);\n+      if (tugiTrans == null) {\n+        tugiTrans = new TUGIContainingTransport(trans);\n+        TUGIContainingTransport prev = transMap.putIfAbsent(trans, tugiTrans);\n+        if (prev != null) {\n+          return prev\n+        }\n+      }\n+      return tugiTrans;\n     }\n   }\n }", "filename": "shims/src/common/java/org/apache/hadoop/hive/thrift/TUGIContainingTransport.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/2d5fb0e9c12790c6f3f5258120cc8230b2cdf517", "parent": "https://github.com/apache/hive/commit/f0203a823bb53cf680f107e0dcb179c75b15783a", "message": "HIVE-5492 - Explain query fails with NPE if a client doesn't call getResultSetSchema() (Xuefu Zhang via Brock Noland)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532472 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_242", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/2d5fb0e9c12790c6f3f5258120cc8230b2cdf517/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java", "blob_url": "https://github.com/apache/hive/blob/2d5fb0e9c12790c6f3f5258120cc8230b2cdf517/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java", "sha": "f6adf92d8b95d614d1c32d56858b1d386c804773", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java?ref=2d5fb0e9c12790c6f3f5258120cc8230b2cdf517", "patch": "@@ -124,6 +124,7 @@ private void runInternal() throws HiveSQLException {\n       // TODO explain should use a FetchTask for reading\n       for (Task<? extends Serializable> task: driver.getPlan().getRootTasks()) {\n         if (task.getClass() == ExplainTask.class) {\n+          resultSchema = new TableSchema(mResultSchema);\n           setHasResultSet(true);\n           break;\n         }", "filename": "service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84", "parent": "https://github.com/apache/hive/commit/7083f4f3cbeecdb9b2ed984b8aa47537c37fe7f6", "message": "HIVE-5526 - NPE in ConstantVectorExpression.evaluate(vrg) (Remus Rusanu via Brock Noland)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532044 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_243", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java", "blob_url": "https://github.com/apache/hive/blob/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java", "sha": "119b4b9d4d6cc0c0d5cf3961009f760dae3558a8", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java?ref=f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84", "patch": "@@ -42,8 +42,8 @@\n   private byte[] bytesValue = null;\n   private String typeString;\n \n-  private transient Type type;\n-  private transient int bytesValueLength = 0;\n+  private Type type;\n+  private int bytesValueLength = 0;\n \n   public ConstantVectorExpression() {\n     super();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/534b447046a0e66272635efb8429daf6f3e0e737", "parent": "https://github.com/apache/hive/commit/4adf1728e69e4f735c041dd551f5a65bde4f07d3", "message": "HIVE-5364 : NPE on some queries from partitioned orc table (Owen O'Malley via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1529097 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_244", "file": [{"additions": 100, "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/data/files/orc_create_people.txt", "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/data/files/orc_create_people.txt", "sha": "884598981a13cfae7a7bfbbccaedf9d4c5b4083f", "changes": 100, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/orc_create_people.txt?ref=534b447046a0e66272635efb8429daf6f3e0e737", "patch": "@@ -0,0 +1,100 @@\n+1\u0001Celeste\u0001Browning\u0001959-3763 Nec, Av.\u0001Ca\n+2\u0001Risa\u0001Yang\u0001P.O. Box 292, 8229 Porttitor Road\u0001Or\n+3\u0001Venus\u0001Sutton\u0001Ap #962-8021 Egestas Rd.\u0001Ca\n+4\u0001Gretchen\u0001Harrison\u0001P.O. Box 636, 8734 Magna Avenue\u0001Or\n+5\u0001Lani\u0001Irwin\u0001Ap #441-5911 Iaculis, Ave\u0001Ca\n+6\u0001Vera\u0001George\u0001409-1555 Vel, Ave\u0001Or\n+7\u0001Jessica\u0001Malone\u0001286-9779 Aliquam Road\u0001Ca\n+8\u0001Ann\u0001Chapman\u0001Ap #504-3915 Placerat Road\u0001Or\n+9\u0001Nigel\u0001Bartlett\u0001Ap #185-385 Diam Street\u0001Ca\n+10\u0001Azalia\u0001Jennings\u00015772 Diam St.\u0001Or\n+11\u0001Preston\u0001Cannon\u0001Ap #527-8769 Nunc Avenue\u0001Ca\n+12\u0001Allistair\u0001Vasquez\u00012562 Odio. St.\u0001Or\n+13\u0001Reed\u0001Hayes\u00015190 Elit Street\u0001Ca\n+14\u0001Elaine\u0001Barron\u0001P.O. Box 840, 8860 Sodales Av.\u0001Or\n+15\u0001Lydia\u0001Hood\u0001P.O. Box 698, 5666 Semper Road\u0001Ca\n+16\u0001Vance\u0001Maxwell\u0001298-3313 Malesuada Road\u0001Or\n+17\u0001Keiko\u0001Deleon\u0001P.O. Box 732, 5921 Massa. Av.\u0001Ca\n+18\u0001Dolan\u0001Kane\u0001Ap #906-3606 Ut Rd.\u0001Or\n+19\u0001Merritt\u0001Perkins\u0001P.O. Box 228, 7090 Egestas Street\u0001Ca\n+20\u0001Casey\u0001Salazar\u0001506-5065 Ut St.\u0001Or\n+21\u0001Samson\u0001Noel\u00011370 Ultrices, Road\u0001Ca\n+22\u0001Byron\u0001Walker\u0001P.O. Box 386, 8324 Tellus Ave\u0001Or\n+23\u0001Piper\u0001Singleton\u0001Ap #500-3561 Primis St.\u0001Ca\n+24\u0001Ria\u0001Mckinney\u00013080 Dui Rd.\u0001Or\n+25\u0001Rahim\u0001Stanley\u0001559-9016 Nascetur Street\u0001Ca\n+26\u0001Chloe\u0001Steele\u0001P.O. Box 766, 1628 Elit Street\u0001Or\n+27\u0001Paloma\u0001Ward\u0001Ap #390-3042 Ipsum Rd.\u0001Ca\n+28\u0001Roary\u0001Sherman\u0001Ap #409-6549 Metus St.\u0001Or\n+29\u0001Calvin\u0001Buckner\u00016378 Diam Avenue\u0001Ca\n+30\u0001Camille\u0001Good\u0001Ap #113-8659 Suspendisse St.\u0001Or\n+31\u0001Steel\u0001Ayala\u00015518 Justo St.\u0001Ca\n+32\u0001Josiah\u0001Gilbert\u0001Ap #149-6651 At, Av.\u0001Or\n+33\u0001Hamilton\u0001Cruz\u00014620 Tellus. Ave\u0001Ca\n+34\u0001Scarlet\u0001Santos\u0001586-1785 Velit. Av.\u0001Or\n+35\u0001Lewis\u0001Mcintyre\u0001629-6419 Ac Rd.\u0001Ca\n+36\u0001Arsenio\u0001Mejia\u0001P.O. Box 767, 8625 Justo Rd.\u0001Or\n+37\u0001Velma\u0001Haley\u00011377 At Rd.\u0001Ca\n+38\u0001Tatum\u0001Jennings\u0001829-7432 Posuere, Road\u0001Or\n+39\u0001Britanni\u0001Eaton\u00018811 Morbi Street\u0001Ca\n+40\u0001Aileen\u0001Jacobson\u0001P.O. Box 469, 2266 Dui, Rd.\u0001Or\n+41\u0001Kareem\u0001Ayala\u00012706 Ridiculus Street\u0001Ca\n+42\u0001Maite\u0001Rush\u00017592 Neque Road\u0001Or\n+43\u0001Signe\u0001Velasquez\u0001Ap #868-3039 Eget St.\u0001Ca\n+44\u0001Zorita\u0001Camacho\u0001P.O. Box 651, 3340 Quis Av.\u0001Or\n+45\u0001Glenna\u0001Curtis\u0001953-7965 Enim Ave\u0001Ca\n+46\u0001Quin\u0001Cortez\u00014898 Ridiculus St.\u0001Or\n+47\u0001Talon\u0001Dalton\u0001P.O. Box 408, 7597 Integer Rd.\u0001Ca\n+48\u0001Darryl\u0001Blankenship\u0001P.O. Box 771, 1471 Non Rd.\u0001Or\n+49\u0001Vernon\u0001Reyes\u0001P.O. Box 971, 7009 Vulputate Street\u0001Ca\n+50\u0001Tallulah\u0001Heath\u0001P.O. Box 865, 3697 Dis Ave\u0001Or\n+51\u0001Ciaran\u0001Olson\u00012721 Et St.\u0001Ca\n+52\u0001Orlando\u0001Witt\u0001P.O. Box 717, 1102 Nulla. Rd.\u0001Or\n+53\u0001Quinn\u0001Rice\u0001Ap #647-6627 Tristique Avenue\u0001Ca\n+54\u0001Wyatt\u0001Pickett\u0001Ap #128-3130 Vel, Rd.\u0001Or\n+55\u0001Emerald\u0001Copeland\u0001857-5119 Turpis Rd.\u0001Ca\n+56\u0001Jonas\u0001Quinn\u0001Ap #441-7183 Ligula. Street\u0001Or\n+57\u0001Willa\u0001Berg\u00016672 Velit Ave\u0001Ca\n+58\u0001Malik\u0001Lee\u0001998-9208 In Street\u0001Or\n+59\u0001Callie\u0001Medina\u00011620 Dui. Rd.\u0001Ca\n+60\u0001Luke\u0001Mason\u0001P.O. Box 143, 2070 Augue Rd.\u0001Or\n+61\u0001Shafira\u0001Estrada\u00018824 Ante Street\u0001Ca\n+62\u0001Elizabeth\u0001Rutledge\u0001315-6510 Sit St.\u0001Or\n+63\u0001Pandora\u0001Levine\u0001357-3596 Nibh. Ave\u0001Ca\n+64\u0001Hilel\u0001Prince\u0001845-1229 Sociosqu Rd.\u0001Or\n+65\u0001Rinah\u0001Torres\u0001Ap #492-9328 At St.\u0001Ca\n+66\u0001Yael\u0001Hobbs\u0001P.O. Box 477, 3896 In Street\u0001Or\n+67\u0001Nevada\u0001Nash\u0001P.O. Box 251, 1914 Tincidunt Road\u0001Ca\n+68\u0001Marny\u0001Huff\u0001P.O. Box 818, 6086 Ultricies St.\u0001Or\n+69\u0001Kimberley\u0001Miles\u0001Ap #893-3685 In Road\u0001Ca\n+70\u0001Duncan\u0001Fuller\u0001Ap #197-5216 Iaculis Street\u0001Or\n+71\u0001Yardley\u0001Leblanc\u0001P.O. Box 938, 1278 Sit Ave\u0001Ca\n+72\u0001Hamish\u0001Brewer\u0001Ap #854-781 Quisque St.\u0001Or\n+73\u0001Petra\u0001Moon\u0001453-6609 Curabitur Street\u0001Ca\n+74\u0001Reese\u0001Estrada\u0001Ap #382-3313 Malesuada St.\u0001Or\n+75\u0001Gage\u0001Higgins\u00017443 Eu Street\u0001Ca\n+76\u0001Zachery\u0001Camacho\u0001Ap #795-4143 Quam. St.\u0001Or\n+77\u0001Kelly\u0001Garner\u0001P.O. Box 895, 2843 Cras Rd.\u0001Ca\n+78\u0001Hanae\u0001Carr\u00019440 Amet St.\u0001Or\n+79\u0001Ann\u0001Alston\u0001884-7948 Dictum Road\u0001Ca\n+80\u0001Chancellor\u0001Cobb\u0001P.O. Box 889, 5978 Ac Avenue\u0001Or\n+81\u0001Dorothy\u0001Harrell\u00016974 Tristique Ave\u0001Ca\n+82\u0001Vaughan\u0001Leon\u00011610 Luctus Av.\u0001Or\n+83\u0001Wynne\u0001Jimenez\u0001321-9171 Felis. Avenue\u0001Ca\n+84\u0001Willa\u0001Mendoza\u0001489-182 Sed Av.\u0001Or\n+85\u0001Camden\u0001Goodwin\u00014579 Ante St.\u0001Ca\n+86\u0001Ifeoma\u0001French\u0001P.O. Box 160, 8769 Integer Road\u0001Or\n+87\u0001Ramona\u0001Strong\u00011666 Ridiculus Avenue\u0001Ca\n+88\u0001Brett\u0001Ramos\u0001Ap #579-9879 Et, Road\u0001Or\n+89\u0001Ulla\u0001Gray\u0001595-7066 Malesuada Road\u0001Ca\n+90\u0001Kevyn\u0001Mccall\u0001P.O. Box 968, 1420 Aenean Avenue\u0001Or\n+91\u0001Genevieve\u0001Wilkins\u0001908 Turpis. Street\u0001Ca\n+92\u0001Thane\u0001Oneil\u00016766 Lectus St.\u0001Or\n+93\u0001Mariko\u0001Cline\u0001P.O. Box 329, 5375 Ac St.\u0001Ca\n+94\u0001Lael\u0001Mclean\u0001500-7010 Sit St.\u0001Or\n+95\u0001Winifred\u0001Hopper\u0001Ap #140-8982 Velit Avenue\u0001Ca\n+96\u0001Rafael\u0001England\u0001P.O. Box 405, 7857 Eget Av.\u0001Or\n+97\u0001Dana\u0001Carter\u0001814-601 Purus. Av.\u0001Ca\n+98\u0001Juliet\u0001Battle\u0001Ap #535-1965 Cursus St.\u0001Or\n+99\u0001Wynter\u0001Vincent\u0001626-8492 Mollis Avenue\u0001Ca\n+100\u0001Wang\u0001Mitchell\u00014023 Lacinia. Ave\u0001Or", "filename": "data/files/orc_create_people.txt"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "sha": "63b54abaf4e131d4a2fb19a0cc4997eb44236209", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java?ref=534b447046a0e66272635efb8429daf6f3e0e737", "patch": "@@ -114,7 +114,7 @@\n         String[] neededColumnNames = columnNamesString.split(\",\");\n         int i = 0;\n         for(int columnId: types.get(0).getSubtypesList()) {\n-          if (includeColumn[columnId]) {\n+          if (includeColumn == null || includeColumn[columnId]) {\n             columnNames[columnId] = neededColumnNames[i++];\n           }\n         }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "sha": "fe1845c519008a5b3e7e29345b29c6513d34704b", "changes": 19, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=534b447046a0e66272635efb8429daf6f3e0e737", "patch": "@@ -2129,13 +2129,18 @@ static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n     TruthValue[] leafValues = new TruthValue[sargLeaves.size()];\n     for(int rowGroup=0; rowGroup < result.length; ++rowGroup) {\n       for(int pred=0; pred < leafValues.length; ++pred) {\n-        OrcProto.ColumnStatistics stats =\n-            indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();\n-        leafValues[pred] = evaluatePredicate(stats, sargLeaves.get(pred));\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Stats = \" + stats);\n-          LOG.debug(\"Setting \" + sargLeaves.get(pred) + \" to \" +\n-              leafValues[pred]);\n+        if (filterColumns[pred] != -1) {\n+          OrcProto.ColumnStatistics stats =\n+              indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();\n+          leafValues[pred] = evaluatePredicate(stats, sargLeaves.get(pred));\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Stats = \" + stats);\n+            LOG.debug(\"Setting \" + sargLeaves.get(pred) + \" to \" +\n+                leafValues[pred]);\n+          }\n+        } else {\n+          // the column is a virtual column\n+          leafValues[pred] = TruthValue.YES_NO_NULL;\n         }\n       }\n       result[rowGroup] = sarg.evaluate(leafValues).isNotNeeded();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/queries/clientpositive/orc_create.q", "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/queries/clientpositive/orc_create.q", "sha": "6aca5486445c809c54857929f5a17cb291f74093", "changes": 37, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_create.q?ref=534b447046a0e66272635efb8429daf6f3e0e737", "patch": "@@ -1,6 +1,8 @@\n DROP TABLE orc_create;\n DROP TABLE orc_create_complex;\n DROP TABLE orc_create_staging;\n+DROP TABLE orc_create_people_staging;\n+DROP TABLE orc_create_people;\n \n CREATE TABLE orc_create_staging (\n   str STRING,\n@@ -38,6 +40,8 @@ set hive.default.fileformat=orc;\n CREATE TABLE orc_create (key INT, value STRING)\n    PARTITIONED BY (ds string);\n \n+set hive.default.fileformat=text;\n+\n DESCRIBE FORMATTED orc_create;\n \n CREATE TABLE orc_create_complex (\n@@ -61,6 +65,39 @@ SELECT mp from orc_create_complex;\n SELECT lst from orc_create_complex;\n SELECT strct from orc_create_complex;\n \n+CREATE TABLE orc_create_people_staging (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string,\n+  state string);\n+\n+LOAD DATA LOCAL INPATH '../data/files/orc_create_people.txt'\n+  OVERWRITE INTO TABLE orc_create_people_staging;\n+\n+CREATE TABLE orc_create_people (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string)\n+PARTITIONED BY (state string)\n+STORED AS orc;\n+\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+\n+INSERT OVERWRITE TABLE orc_create_people PARTITION (state)\n+  SELECT * FROM orc_create_people_staging;\n+\n+SET hive.optimize.index.filter=true;\n+-- test predicate push down with partition pruning\n+SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca';\n+\n+-- test predicate push down with no column projection\n+SELECT id, first_name, last_name, address\n+  FROM orc_create_people WHERE id > 90;\n+\n DROP TABLE orc_create;\n DROP TABLE orc_create_complex;\n DROP TABLE orc_create_staging;\n+DROP TABLE orc_create_people_staging;\n+DROP TABLE orc_create_people;", "filename": "ql/src/test/queries/clientpositive/orc_create.q"}, {"additions": 210, "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/results/clientpositive/orc_create.q.out", "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/results/clientpositive/orc_create.q.out", "sha": "03e1fb3fd2e503be9a79d7afe01d7a7e6fb12008", "changes": 210, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_create.q.out?ref=534b447046a0e66272635efb8429daf6f3e0e737", "patch": "@@ -10,6 +10,14 @@ PREHOOK: query: DROP TABLE orc_create_staging\n PREHOOK: type: DROPTABLE\n POSTHOOK: query: DROP TABLE orc_create_staging\n POSTHOOK: type: DROPTABLE\n+PREHOOK: query: DROP TABLE orc_create_people_staging\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE orc_create_people_staging\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: DROP TABLE orc_create_people\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE orc_create_people\n+POSTHOOK: type: DROPTABLE\n PREHOOK: query: CREATE TABLE orc_create_staging (\n   str STRING,\n   mp  MAP<STRING,STRING>,\n@@ -398,6 +406,144 @@ POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_crea\n {\"a\":\"one\",\"b\":\"two\"}\n {\"a\":\"three\",\"b\":\"four\"}\n {\"a\":\"five\",\"b\":\"six\"}\n+PREHOOK: query: CREATE TABLE orc_create_people_staging (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string,\n+  state string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE orc_create_people_staging (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string,\n+  state string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/orc_create_people.txt'\n+  OVERWRITE INTO TABLE orc_create_people_staging\n+PREHOOK: type: LOAD\n+PREHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/orc_create_people.txt'\n+  OVERWRITE INTO TABLE orc_create_people_staging\n+POSTHOOK: type: LOAD\n+POSTHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+PREHOOK: query: CREATE TABLE orc_create_people (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string)\n+PARTITIONED BY (state string)\n+STORED AS orc\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE orc_create_people (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string)\n+PARTITIONED BY (state string)\n+STORED AS orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@orc_create_people\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+PREHOOK: query: INSERT OVERWRITE TABLE orc_create_people PARTITION (state)\n+  SELECT * FROM orc_create_people_staging\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people_staging\n+PREHOOK: Output: default@orc_create_people\n+POSTHOOK: query: INSERT OVERWRITE TABLE orc_create_people PARTITION (state)\n+  SELECT * FROM orc_create_people_staging\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people_staging\n+POSTHOOK: Output: default@orc_create_people@state=Ca\n+POSTHOOK: Output: default@orc_create_people@state=Or\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+PREHOOK: query: -- test predicate push down with partition pruning\n+SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- test predicate push down with partition pruning\n+SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+5\n+PREHOOK: query: -- test predicate push down with no column projection\n+SELECT id, first_name, last_name, address\n+  FROM orc_create_people WHERE id > 90\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- test predicate push down with no column projection\n+SELECT id, first_name, last_name, address\n+  FROM orc_create_people WHERE id > 90\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+91\tGenevieve\tWilkins\t908 Turpis. Street\n+93\tMariko\tCline\tP.O. Box 329, 5375 Ac St.\n+95\tWinifred\tHopper\tAp #140-8982 Velit Avenue\n+97\tDana\tCarter\t814-601 Purus. Av.\n+99\tWynter\tVincent\t626-8492 Mollis Avenue\n+92\tThane\tOneil\t6766 Lectus St.\n+94\tLael\tMclean\t500-7010 Sit St.\n+96\tRafael\tEngland\tP.O. Box 405, 7857 Eget Av.\n+98\tJuliet\tBattle\tAp #535-1965 Cursus St.\n+100\tWang\tMitchell\t4023 Lacinia. Ave\n PREHOOK: query: DROP TABLE orc_create\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@orc_create\n@@ -410,6 +556,14 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create\n POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n PREHOOK: query: DROP TABLE orc_create_complex\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@orc_create_complex\n@@ -422,6 +576,14 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create\n POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n PREHOOK: query: DROP TABLE orc_create_staging\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@orc_create_staging\n@@ -434,3 +596,51 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create\n POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+PREHOOK: query: DROP TABLE orc_create_people_staging\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@orc_create_people_staging\n+PREHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: query: DROP TABLE orc_create_people_staging\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@orc_create_people_staging\n+POSTHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+PREHOOK: query: DROP TABLE orc_create_people\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Output: default@orc_create_people\n+POSTHOOK: query: DROP TABLE orc_create_people\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Output: default@orc_create_people\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]", "filename": "ql/src/test/results/clientpositive/orc_create.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/990a95b0d8f25b81e9758d9b2f77dbe24740e15e", "parent": "https://github.com/apache/hive/commit/7ee7363b5d49fdb95f256a939e44f33659f12e17", "message": "HIVE-4837 : Union on void type fails with NPE (Navis via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1527354 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_245", "file": [{"additions": 8, "raw_url": "https://github.com/apache/hive/raw/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java", "blob_url": "https://github.com/apache/hive/blob/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java", "sha": "59c07c352e1bf2bbc275ea8638f2334d120fe90c", "changes": 11, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java?ref=990a95b0d8f25b81e9758d9b2f77dbe24740e15e", "patch": "@@ -80,15 +80,20 @@ protected void initializeOp(Configuration hconf) throws HiveException {\n     for (int p = 0; p < parents; p++) {\n       assert (parentFields[p].size() == columns);\n       for (int c = 0; c < columns; c++) {\n-        columnTypeResolvers[c].update(parentFields[p].get(c)\n-            .getFieldObjectInspector());\n+        if (!columnTypeResolvers[c].update(parentFields[p].get(c)\n+            .getFieldObjectInspector())) {\n+          // checked in SemanticAnalyzer. Should not happen\n+          throw new HiveException(\"Incompatible types for union operator\");\n+        }\n       }\n     }\n \n     ArrayList<ObjectInspector> outputFieldOIs = new ArrayList<ObjectInspector>(\n         columns);\n     for (int c = 0; c < columns; c++) {\n-      outputFieldOIs.add(columnTypeResolvers[c].get());\n+      // can be null for void type\n+      ObjectInspector oi = columnTypeResolvers[c].get();\n+      outputFieldOIs.add(oi == null ? parentFields[0].get(c).getFieldObjectInspector() : oi);\n     }\n \n     // create output row ObjectInspector", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/test/queries/clientpositive/union_null.q", "blob_url": "https://github.com/apache/hive/blob/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/test/queries/clientpositive/union_null.q", "sha": "4368b8a5b6c363c090a2ea61feb67793eec26e8d", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/union_null.q?ref=990a95b0d8f25b81e9758d9b2f77dbe24740e15e", "patch": "@@ -1,2 +1,5 @@\n -- HIVE-2901\n select x from (select value as x from src union all select NULL as x from src)a limit 10;\n+\n+-- HIVE-4837\n+select * from (select null as N from src1 group by key UNION ALL select null as N from src1 group by key ) a;", "filename": "ql/src/test/queries/clientpositive/union_null.q"}, {"additions": 37, "raw_url": "https://github.com/apache/hive/raw/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/test/results/beelinepositive/union_null.q.out", "blob_url": "https://github.com/apache/hive/blob/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/test/results/beelinepositive/union_null.q.out", "sha": "dc67e6587bd1002eeb76082fcba53bb2122cca07", "changes": 37, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/beelinepositive/union_null.q.out?ref=990a95b0d8f25b81e9758d9b2f77dbe24740e15e", "patch": "@@ -14,4 +14,41 @@ Saving all output to \"!!{outputDirectory}!!/union_null.q.raw\". Enter \"record\" wi\n 'val_165'\n ''\n 10 rows selected \n+>>>  \n+>>>  -- HIVE-4837\n+>>>  select * from (select null as N from src1 group by key UNION ALL select null as N from src1 group by key ) a;\n+'n'\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+''\n+32 rows selected \n >>>  !record", "filename": "ql/src/test/results/beelinepositive/union_null.q.out"}, {"additions": 42, "raw_url": "https://github.com/apache/hive/raw/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/test/results/clientpositive/union_null.q.out", "blob_url": "https://github.com/apache/hive/blob/990a95b0d8f25b81e9758d9b2f77dbe24740e15e/ql/src/test/results/clientpositive/union_null.q.out", "sha": "dad45ba0e84fdd656f0226ed099cf33a68c28746", "changes": 42, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union_null.q.out?ref=990a95b0d8f25b81e9758d9b2f77dbe24740e15e", "patch": "@@ -18,3 +18,45 @@ val_27\n NULL\n val_165\n NULL\n+PREHOOK: query: -- HIVE-4837\n+select * from (select null as N from src1 group by key UNION ALL select null as N from src1 group by key ) a\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src1\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- HIVE-4837\n+select * from (select null as N from src1 group by key UNION ALL select null as N from src1 group by key ) a\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src1\n+#### A masked pattern was here ####\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL\n+NULL", "filename": "ql/src/test/results/clientpositive/union_null.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/14ead79b9956d71feee633a2b28e874991df0cf2", "parent": "https://github.com/apache/hive/commit/9fd3724479440bb41c46afc0eac5c01a6df7cc85", "message": "HIVE-4375 : Single sourced multi insert consists of native and non-native table mixed throws NPE (Navis via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1517748 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_246", "file": [{"additions": 21, "raw_url": "https://github.com/apache/hive/raw/14ead79b9956d71feee633a2b28e874991df0cf2/hbase-handler/src/test/queries/positive/hbase_single_sourced_multi_insert.q", "blob_url": "https://github.com/apache/hive/blob/14ead79b9956d71feee633a2b28e874991df0cf2/hbase-handler/src/test/queries/positive/hbase_single_sourced_multi_insert.q", "sha": "96fec0ecc66154de1916d02a584b6c7cf7271aee", "changes": 21, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/queries/positive/hbase_single_sourced_multi_insert.q?ref=14ead79b9956d71feee633a2b28e874991df0cf2", "patch": "@@ -0,0 +1,21 @@\n+-- HIVE-4375 Single sourced multi insert consists of native and non-native table mixed throws NPE\n+CREATE TABLE src_x1(key string, value string);\n+CREATE TABLE src_x2(key string, value string)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key, cf:value\");\n+\n+explain\n+from src a\n+insert overwrite table src_x1\n+select key,\"\" where a.key > 0 AND a.key < 50\n+insert overwrite table src_x2\n+select value,\"\" where a.key > 50 AND a.key < 100;\n+\n+from src a\n+insert overwrite table src_x1\n+select key,\"\" where a.key > 0 AND a.key < 50\n+insert overwrite table src_x2\n+select value,\"\" where a.key > 50 AND a.key < 100;\n+\n+select * from src_x1 order by key;\n+select * from src_x2 order by key;", "filename": "hbase-handler/src/test/queries/positive/hbase_single_sourced_multi_insert.q"}, {"additions": 251, "raw_url": "https://github.com/apache/hive/raw/14ead79b9956d71feee633a2b28e874991df0cf2/hbase-handler/src/test/results/positive/hbase_single_sourced_multi_insert.q.out", "blob_url": "https://github.com/apache/hive/blob/14ead79b9956d71feee633a2b28e874991df0cf2/hbase-handler/src/test/results/positive/hbase_single_sourced_multi_insert.q.out", "sha": "92e8175d83b3b9039e795f50663d9f494c6e8e17", "changes": 251, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/hbase-handler/src/test/results/positive/hbase_single_sourced_multi_insert.q.out?ref=14ead79b9956d71feee633a2b28e874991df0cf2", "patch": "@@ -0,0 +1,251 @@\n+PREHOOK: query: -- HIVE-4375 Single sourced multi insert consists of native and non-native table mixed throws NPE\n+CREATE TABLE src_x1(key string, value string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: -- HIVE-4375 Single sourced multi insert consists of native and non-native table mixed throws NPE\n+CREATE TABLE src_x1(key string, value string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@src_x1\n+PREHOOK: query: CREATE TABLE src_x2(key string, value string)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key, cf:value\")\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE src_x2(key string, value string)\n+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n+WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key, cf:value\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@src_x2\n+PREHOOK: query: explain\n+from src a\n+insert overwrite table src_x1\n+select key,\"\" where a.key > 0 AND a.key < 50\n+insert overwrite table src_x2\n+select value,\"\" where a.key > 50 AND a.key < 100\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+from src a\n+insert overwrite table src_x1\n+select key,\"\" where a.key > 0 AND a.key < 50\n+insert overwrite table src_x2\n+select value,\"\" where a.key > 50 AND a.key < 100\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src) a)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME src_x1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR \"\")) (TOK_WHERE (AND (> (. (TOK_TABLE_OR_COL a) key) 0) (< (. (TOK_TABLE_OR_COL a) key) 50)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME src_x2))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR \"\")) (TOK_WHERE (AND (> (. (TOK_TABLE_OR_COL a) key) 50) (< (. (TOK_TABLE_OR_COL a) key) 100)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5\n+  Stage-4\n+  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6\n+  Stage-2 depends on stages: Stage-0\n+  Stage-3\n+  Stage-5\n+  Stage-6 depends on stages: Stage-5\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        a \n+          TableScan\n+            alias: a\n+            Filter Operator\n+              predicate:\n+                  expr: ((key > 0) and (key < 50))\n+                  type: boolean\n+              Select Operator\n+                expressions:\n+                      expr: key\n+                      type: string\n+                      expr: ''\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                File Output Operator\n+                  compressed: false\n+                  GlobalTableId: 1\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      name: default.src_x1\n+            Filter Operator\n+              predicate:\n+                  expr: ((key > 50) and (key < 100))\n+                  type: boolean\n+              Select Operator\n+                expressions:\n+                      expr: value\n+                      type: string\n+                      expr: ''\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                File Output Operator\n+                  compressed: false\n+                  GlobalTableId: 2\n+                  table:\n+                      input format: org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat\n+                      output format: org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat\n+                      serde: org.apache.hadoop.hive.hbase.HBaseSerDe\n+                      name: default.src_x2\n+\n+  Stage: Stage-7\n+    Conditional Operator\n+\n+  Stage: Stage-4\n+    Move Operator\n+      files:\n+          hdfs directory: true\n+#### A masked pattern was here ####\n+\n+  Stage: Stage-0\n+    Move Operator\n+      tables:\n+          replace: true\n+          table:\n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.src_x1\n+\n+  Stage: Stage-2\n+    Stats-Aggr Operator\n+\n+  Stage: Stage-3\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+#### A masked pattern was here ####\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  name: default.src_x1\n+\n+  Stage: Stage-5\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+#### A masked pattern was here ####\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  name: default.src_x1\n+\n+  Stage: Stage-6\n+    Move Operator\n+      files:\n+          hdfs directory: true\n+#### A masked pattern was here ####\n+\n+\n+PREHOOK: query: from src a\n+insert overwrite table src_x1\n+select key,\"\" where a.key > 0 AND a.key < 50\n+insert overwrite table src_x2\n+select value,\"\" where a.key > 50 AND a.key < 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@src_x1\n+PREHOOK: Output: default@src_x2\n+POSTHOOK: query: from src a\n+insert overwrite table src_x1\n+select key,\"\" where a.key > 0 AND a.key < 50\n+insert overwrite table src_x2\n+select value,\"\" where a.key > 50 AND a.key < 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@src_x1\n+POSTHOOK: Output: default@src_x2\n+POSTHOOK: Lineage: src_x1.key SIMPLE [(src)a.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src_x1.value SIMPLE []\n+PREHOOK: query: select * from src_x1 order by key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src_x1\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from src_x1 order by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src_x1\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: src_x1.key SIMPLE [(src)a.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src_x1.value SIMPLE []\n+10\t\n+11\t\n+12\t\n+12\t\n+15\t\n+15\t\n+17\t\n+18\t\n+18\t\n+19\t\n+2\t\n+20\t\n+24\t\n+24\t\n+26\t\n+26\t\n+27\t\n+28\t\n+30\t\n+33\t\n+34\t\n+35\t\n+35\t\n+35\t\n+37\t\n+37\t\n+4\t\n+41\t\n+42\t\n+42\t\n+43\t\n+44\t\n+47\t\n+5\t\n+5\t\n+5\t\n+8\t\n+9\t\n+PREHOOK: query: select * from src_x2 order by key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src_x2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from src_x2 order by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src_x2\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: src_x1.key SIMPLE [(src)a.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src_x1.value SIMPLE []\n+val_51\t\n+val_53\t\n+val_54\t\n+val_57\t\n+val_58\t\n+val_64\t\n+val_65\t\n+val_66\t\n+val_67\t\n+val_69\t\n+val_70\t\n+val_72\t\n+val_74\t\n+val_76\t\n+val_77\t\n+val_78\t\n+val_80\t\n+val_82\t\n+val_83\t\n+val_84\t\n+val_85\t\n+val_86\t\n+val_87\t\n+val_90\t\n+val_92\t\n+val_95\t\n+val_96\t\n+val_97\t\n+val_98\t", "filename": "hbase-handler/src/test/results/positive/hbase_single_sourced_multi_insert.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/14ead79b9956d71feee633a2b28e874991df0cf2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java", "blob_url": "https://github.com/apache/hive/blob/14ead79b9956d71feee633a2b28e874991df0cf2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java", "sha": "1ede6d7050e50eb9b915f8196525e03da3474317", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java?ref=14ead79b9956d71feee633a2b28e874991df0cf2", "patch": "@@ -125,7 +125,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n         // no need of merging if the move is to a local file system\n         MoveTask mvTask = (MoveTask) findMoveTask(mvTasks, fsOp);\n \n-        if (isInsertTable && hconf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER)) {\n+        if (mvTask != null && isInsertTable && hconf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER)) {\n           addStatsTask(fsOp, mvTask, currTask, parseCtx.getConf());\n         }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/836c3970b3058d9c0c7b02c5fa8a89deeb5829c9", "parent": "https://github.com/apache/hive/commit/3f350375f1424d5a1bf302f9ccec680599c0f8e1", "message": "HIVE-5048 : StorageBasedAuthorization provider causes an NPE when asked to authorize from client side. (Sushanth Sowmyan via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1514569 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_247", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/836c3970b3058d9c0c7b02c5fa8a89deeb5829c9/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveAuthorizationProviderBase.java", "blob_url": "https://github.com/apache/hive/blob/836c3970b3058d9c0c7b02c5fa8a89deeb5829c9/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveAuthorizationProviderBase.java", "sha": "18a1b25a2b912fa681ceeed1a3ca392e7a9454c4", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveAuthorizationProviderBase.java?ref=836c3970b3058d9c0c7b02c5fa8a89deeb5829c9", "patch": "@@ -57,10 +57,14 @@ public void setHandler(HMSHandler handler){\n       this.handler = handler;\n     }\n \n+    public boolean isRunFromMetaStore(){\n+      return (this.hiveClient == null);\n+    }\n+\n     public PrincipalPrivilegeSet get_privilege_set(HiveObjectType column, String dbName,\n         String tableName, List<String> partValues, String col, String userName,\n         List<String> groupNames) throws HiveException {\n-      if (hiveClient != null) {\n+      if (!isRunFromMetaStore()) {\n         return hiveClient.get_privilege_set(\n             column, dbName, tableName, partValues, col, userName, groupNames);\n       } else {\n@@ -77,7 +81,7 @@ public PrincipalPrivilegeSet get_privilege_set(HiveObjectType column, String dbN\n     }\n \n     public Database getDatabase(String dbName) throws HiveException {\n-      if (hiveClient != null) {\n+      if (!isRunFromMetaStore()) {\n         return hiveClient.getDatabase(dbName);\n       } else {\n         try {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveAuthorizationProviderBase.java"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/836c3970b3058d9c0c7b02c5fa8a89deeb5829c9/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java", "blob_url": "https://github.com/apache/hive/blob/836c3970b3058d9c0c7b02c5fa8a89deeb5829c9/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java", "sha": "4d005c14152ad6c87e47f8745a19ac4390655e13", "changes": 18, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java?ref=836c3970b3058d9c0c7b02c5fa8a89deeb5829c9", "patch": "@@ -63,6 +63,22 @@\n \n   private Warehouse wh;\n \n+  /**\n+   * Make sure that the warehouse variable is set up properly.\n+   * @throws MetaException if unable to instantiate\n+   */\n+  private void initWh() throws MetaException {\n+    if (wh == null){\n+      if(!hive_db.isRunFromMetaStore()){\n+        this.wh = new Warehouse(getConf());\n+      }else{\n+        // not good if we reach here, this was initialized at setMetaStoreHandler() time.\n+        // this means handler.getWh() is returning null. Error out.\n+        throw new IllegalStateException(\"Unitialized Warehouse from MetastoreHandler\");\n+      }\n+    }\n+  }\n+\n   @Override\n   public void init(Configuration conf) throws HiveException {\n     hive_db = new HiveProxy();\n@@ -100,6 +116,7 @@ public void authorize(Table table, Privilege[] readRequiredPriv, Privilege[] wri\n     // we try to determine what the path would be after the create table is issued.\n     Path path = null;\n     try {\n+      initWh();\n       String location = table.getTTable().getSd().getLocation();\n       if (location == null || location.isEmpty()) {\n         path = wh.getTablePath(hive_db.getDatabase(table.getDbName()), table.getTableName());\n@@ -305,6 +322,7 @@ protected static void checkPermissions(final FileSystem fs, final Path path,\n \n   protected Path getDbLocation(Database db) throws HiveException {\n     try {\n+      initWh();\n       String location = db.getLocationUri();\n       if (location == null) {\n         return wh.getDefaultDatabasePath(db.getName());", "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/02daf070c8b59062110993bf0161acaa02f1436a", "parent": "https://github.com/apache/hive/commit/1e48c11b7faf3d6c782354e2ddf600b271e88d0c", "message": "HIVE-5061 : Row sampling throws NPE when used in sub-query (Navis via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1513956 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_248", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/02daf070c8b59062110993bf0161acaa02f1436a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/02daf070c8b59062110993bf0161acaa02f1436a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "757fbb8f027f3635224f0cf163787e86a317373f", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=02daf070c8b59062110993bf0161acaa02f1436a", "patch": "@@ -7719,10 +7719,10 @@ private Operator genTablePlan(String alias, QB qb) throws SemanticException {\n       TableScanDesc tsDesc = new TableScanDesc(alias, vcList);\n       setupStats(tsDesc, qb.getParseInfo(), tab, alias, rwsch);\n \n-      SplitSample sample = nameToSplitSample.get(alias);\n+      SplitSample sample = nameToSplitSample.get(alias_id);\n       if (sample != null && sample.getRowCount() != null) {\n         tsDesc.setRowLimit(sample.getRowCount());\n-        nameToSplitSample.remove(alias);\n+        nameToSplitSample.remove(alias_id);\n       }\n \n       top = putOpInsertMap(OperatorFactory.get(tsDesc,", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/02daf070c8b59062110993bf0161acaa02f1436a/ql/src/test/queries/clientpositive/split_sample.q", "blob_url": "https://github.com/apache/hive/blob/02daf070c8b59062110993bf0161acaa02f1436a/ql/src/test/queries/clientpositive/split_sample.q", "sha": "952eaf72f10c17cbc524d3c6280016d5d1da90cb", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/split_sample.q?ref=02daf070c8b59062110993bf0161acaa02f1436a", "patch": "@@ -110,3 +110,6 @@ select key from ss_src2 tablesample(10 ROWS);\n set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n -- ROW type works with other input formats (others, don't)\n select count(1) from ss_src2 tablesample(10 ROWS);\n+\n+--HIVE-5061 row sampling in sub-query\n+select * from (select * from src TABLESAMPLE (1 ROWS)) x;", "filename": "ql/src/test/queries/clientpositive/split_sample.q"}, {"additions": 23, "raw_url": "https://github.com/apache/hive/raw/02daf070c8b59062110993bf0161acaa02f1436a/ql/src/test/results/clientpositive/split_sample.q.out", "blob_url": "https://github.com/apache/hive/blob/02daf070c8b59062110993bf0161acaa02f1436a/ql/src/test/results/clientpositive/split_sample.q.out", "sha": "5569033519d3071b64525ec3ceb27a1a113e112b", "changes": 25, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/split_sample.q.out?ref=02daf070c8b59062110993bf0161acaa02f1436a", "patch": "@@ -107,8 +107,6 @@ POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema\n POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n-# col_name            \tdata_type           \tcomment             \n-\t \t \n key                 \tint                 \tNone                \n value               \tstring              \tNone                \n PREHOOK: query: explain select key, value from ss_src2 tablesample(1 percent) limit 10\n@@ -4841,3 +4839,26 @@ POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(n\n POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n 10\n+PREHOOK: query: --HIVE-5061 row sampling in sub-query\n+select * from (select * from src TABLESAMPLE (1 ROWS)) x\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: --HIVE-5061 row sampling in sub-query\n+select * from (select * from src TABLESAMPLE (1 ROWS)) x\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: ss_i_part PARTITION(p=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+238\tval_238", "filename": "ql/src/test/results/clientpositive/split_sample.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/36af62a7210b82196133ca7924cf7ba04a249e9d", "parent": "https://github.com/apache/hive/commit/6b8ea834ed39d7d3e7be3ed390bbcaaa634d8c72", "message": "HIVE-5058: Fix NPE issue with DAG submission in TEZ (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1513299 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_249", "file": [{"additions": 54, "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "sha": "42f63bb53158d13da3baf6a70367823a2058a82b", "changes": 77, "status": "modified", "deletions": 23, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d", "patch": "@@ -82,10 +82,12 @@\n  */\n public class DagUtils {\n \n+  private static final String TEZ_DIR = \"_tez_scratch_dir\";\n+\n   /*\n    * Creates the configuration object necessary to run a specific vertex from\n    * map work. This includes input formats, input processor, etc.\n-=  */\n+   */\n   private static JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {\n     JobConf conf = new JobConf(baseConf);\n \n@@ -124,8 +126,8 @@ private static JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {\n       inpFormat = BucketizedHiveInputFormat.class.getName();\n     }\n \n-    conf.set(MRJobConfig.MAP_CLASS_ATTR, ExecMapper.class.getName());\n-    conf.set(MRJobConfig.INPUT_FORMAT_CLASS_ATTR, inpFormat);\n+    conf.set(\"mapred.mapper.class\", ExecMapper.class.getName());\n+    conf.set(\"mapred.input.format.class\", inpFormat);\n \n     return conf;\n   }\n@@ -141,11 +143,16 @@ private static JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {\n    * @param w The second vertex (sink)\n    * @return\n    */\n-  public static Edge createEdge(JobConf vConf, Vertex v, JobConf wConf, Vertex w) {\n+  public static Edge createEdge(JobConf vConf, Vertex v, JobConf wConf, Vertex w) \n+      throws IOException {\n \n     // Tez needs to setup output subsequent input pairs correctly\n     MultiStageMRConfToTezTranslator.translateVertexConfToTez(wConf, vConf);\n \n+    // update payloads (configuration for the vertices might have changed)\n+    v.getProcessorDescriptor().setUserPayload(MRHelpers.createUserPayloadFromConf(vConf));\n+    w.getProcessorDescriptor().setUserPayload(MRHelpers.createUserPayloadFromConf(wConf));\n+\n     // all edges are of the same type right now\n     EdgeProperty edgeProperty =\n         new EdgeProperty(ConnectionPattern.BIPARTITE, SourceType.STABLE,\n@@ -161,6 +168,8 @@ private static Vertex createVertex(JobConf conf, MapWork mapWork, int seqNo,\n       LocalResource appJarLr, List<LocalResource> additionalLr, FileSystem fs,\n       Path mrScratchDir, Context ctx) throws Exception {\n \n+    Path tezDir = getTezDir(mrScratchDir);\n+\n     // map work can contain localwork, i.e: hashtables for map-side joins\n     Path hashTableArchive = createHashTables(mapWork, conf);\n     LocalResource localWorkLr = null;\n@@ -171,17 +180,24 @@ private static Vertex createVertex(JobConf conf, MapWork mapWork, int seqNo,\n     }\n \n     // write out the operator plan\n-    Path planPath = Utilities.setMapWork(conf, mapWork, mrScratchDir.toUri().toString(), false);\n+    Path planPath = Utilities.setMapWork(conf, mapWork, \n+        mrScratchDir.toUri().toString(), false);\n     LocalResource planLr = createLocalResource(fs,\n         planPath, LocalResourceType.FILE,\n         LocalResourceVisibility.APPLICATION);\n \n     // setup input paths and split info\n-    List<Path> inputPaths = Utilities.getInputPaths(conf, mapWork, mrScratchDir.toUri().toString(), ctx);\n+    List<Path> inputPaths = Utilities.getInputPaths(conf, mapWork, \n+        mrScratchDir.toUri().toString(), ctx);\n     Utilities.setInputPaths(conf, inputPaths);\n \n-    InputSplitInfo inputSplitInfo = MRHelpers.generateInputSplits(conf, mrScratchDir);\n-    MultiStageMRConfToTezTranslator.translateVertexConfToTez(conf, conf);\n+    InputSplitInfo inputSplitInfo = MRHelpers.generateInputSplits(conf, tezDir);\n+\n+    // create the directories FileSinkOperators need\n+    Utilities.createTmpDirs(conf, mapWork);\n+\n+    // Tez ask us to call this even if there's no preceding vertex\n+    MultiStageMRConfToTezTranslator.translateVertexConfToTez(conf, null);\n \n     // finally create the vertex\n     Vertex map = null;\n@@ -229,17 +245,13 @@ private static Path createHashTables(MapWork mapWork, Configuration conf) {\n   private static JobConf initializeVertexConf(JobConf baseConf, ReduceWork reduceWork) {\n     JobConf conf = new JobConf(baseConf);\n \n-    conf.set(MRJobConfig.REDUCE_CLASS_ATTR, ExecReducer.class.getName());\n+    conf.set(\"mapred.reducer.class\", ExecReducer.class.getName());\n \n     boolean useSpeculativeExecReducers = HiveConf.getBoolVar(conf,\n         HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,\n         useSpeculativeExecReducers);\n \n-    // reducers should have been set at planning stage\n-    // job.setNumberOfReducers(rWork.getNumberOfReducers())\n-    conf.set(MRJobConfig.NUM_REDUCES, reduceWork.getNumReduceTasks().toString());\n-\n     return conf;\n   }\n \n@@ -252,10 +264,13 @@ private static Vertex createVertex(JobConf conf, ReduceWork reduceWork, int seqN\n \n     // write out the operator plan\n     Path planPath = Utilities.setReduceWork(conf, reduceWork,\n-        mrScratchDir.getName(), false);\n+        mrScratchDir.toUri().toString(), false);\n     LocalResource planLr = createLocalResource(fs, planPath,\n         LocalResourceType.FILE, LocalResourceVisibility.APPLICATION);\n \n+    // create the directories FileSinkOperators need\n+    Utilities.createTmpDirs(conf, reduceWork);\n+\n     // create the vertex\n     Vertex reducer = new Vertex(\"Reducer \"+seqNo,\n         new ProcessorDescriptor(ReduceProcessor.class.getName(),\n@@ -476,7 +491,7 @@ private static LocalResource localizeResource(Path src, Path dest, Configuration\n    * @throws URISyntaxException when current jar location cannot be determined.\n    */\n   public static LocalResource createHiveExecLocalResource(HiveConf conf)\n-  throws IOException, LoginException, URISyntaxException {\n+      throws IOException, LoginException, URISyntaxException {\n     String hiveJarDir = conf.getVar(HiveConf.ConfVars.HIVE_JAR_DIRECTORY);\n     String currentVersionPathStr = getExecJarPathLocal();\n     String currentJarName = getResourceBaseName(currentVersionPathStr);\n@@ -560,20 +575,17 @@ public static JobConf createConfiguration(HiveConf hiveConf) throws IOException\n       }\n     }\n \n-    conf.set(\"mapreduce.framework.name\",\"yarn-tez\");\n-    conf.set(\"mapreduce.job.output.committer.class\", NullOutputCommitter.class.getName());\n-\n-    conf.setBoolean(MRJobConfig.SETUP_CLEANUP_NEEDED, false);\n-    conf.setBoolean(MRJobConfig.TASK_CLEANUP_NEEDED, false);\n+    conf.set(\"mapred.output.committer.class\", NullOutputCommitter.class.getName());\n \n-    conf.setClass(MRJobConfig.OUTPUT_FORMAT_CLASS_ATTR, HiveOutputFormatImpl.class, OutputFormat.class);\n+    conf.setBoolean(\"mapred.committer.job.setup.cleanup.needed\", false);\n+    conf.setBoolean(\"mapred.committer.job.task.cleanup.needed\", false);\n \n-    conf.set(MRJobConfig.MAP_CLASS_ATTR, ExecMapper.class.getName());\n+    conf.setClass(\"mapred.output.format.class\", HiveOutputFormatImpl.class, OutputFormat.class);\n \n     conf.set(MRJobConfig.OUTPUT_KEY_CLASS, HiveKey.class.getName());\n     conf.set(MRJobConfig.OUTPUT_VALUE_CLASS, BytesWritable.class.getName());\n \n-    conf.set(MRJobConfig.PARTITIONER_CLASS_ATTR, HiveConf.getVar(conf, HiveConf.ConfVars.HIVEPARTITIONER));\n+    conf.set(\"mapred.partitioner.class\", HiveConf.getVar(conf, HiveConf.ConfVars.HIVEPARTITIONER));\n \n     return conf;\n   }\n@@ -631,6 +643,25 @@ public static Vertex createVertex(JobConf conf, BaseWork work,\n     }\n   }\n \n+  /**\n+   * createTezDir creates a temporary directory in the scratchDir folder to\n+   * be used with Tez. Assumes scratchDir exists.\n+   */\n+  public static Path createTezDir(Path scratchDir, Configuration conf) \n+      throws IOException {\n+    Path tezDir = getTezDir(scratchDir);\n+    FileSystem fs = tezDir.getFileSystem(conf);\n+    fs.mkdirs(tezDir);\n+    return tezDir;\n+  }\n+\n+  /**\n+   * Gets the tez dir that belongs to the hive scratch dir\n+   */\n+  public static Path getTezDir(Path scratchDir) {\n+    return new Path(scratchDir, TEZ_DIR);\n+  }\n+\n   private DagUtils() {\n     // don't instantiate\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java"}, {"additions": 65, "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "sha": "f5d136fd47c5b4c67cde01fbb1baf9b3d0d1e410", "changes": 76, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d", "patch": "@@ -24,16 +24,20 @@\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.Context;\n import org.apache.hadoop.hive.ql.DriverContext;\n+import org.apache.hadoop.hive.ql.exec.JobCloseFeedBack;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.plan.TezWork;\n import org.apache.hadoop.hive.ql.plan.api.StageType;\n import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.yarn.api.records.LocalResource;\n import org.apache.tez.client.TezClient;\n import org.apache.tez.dag.api.DAG;\n@@ -60,15 +64,26 @@ public TezTask() {\n   @Override\n   public int execute(DriverContext driverContext) {\n     int rc = 1;\n-\n-    Context ctx = driverContext.getCtx();\n+    boolean cleanContext = false;\n+    Context ctx = null;\n+    DAGClient client = null;\n \n     try {\n+      // Get or create Context object. If we create it we have to clean\n+      // it later as well.\n+      ctx = driverContext.getCtx();\n+      if (ctx == null) {\n+        ctx = new Context(conf);\n+        cleanContext = true;\n+      }\n \n       // we will localize all the files (jars, plans, hashtables) to the\n       // scratch dir. let's create this first.\n       Path scratchDir = new Path(ctx.getMRScratchDir());\n \n+      // create the tez tmp dir\n+      DagUtils.createTezDir(scratchDir, conf);\n+\n       // jobConf will hold all the configuration for hadoop, tez, and hive\n       JobConf jobConf = DagUtils.createConfiguration(conf);\n \n@@ -80,21 +95,29 @@ public int execute(DriverContext driverContext) {\n       DAG dag = build(jobConf, work, scratchDir, appJarLr, ctx);\n \n       // submit will send the job to the cluster and start executing\n-      DAGClient client = submit(jobConf, dag, scratchDir, appJarLr);\n+      client = submit(jobConf, dag, scratchDir, appJarLr);\n \n       // finally monitor will print progress until the job is done\n       TezJobMonitor monitor = new TezJobMonitor();\n       rc = monitor.monitorExecution(client);\n \n     } catch (Exception e) {\n       LOG.error(\"Failed to execute tez graph.\", e);\n+      // rc will be 1 at this point indicating failure.\n     } finally {\n       Utilities.clearWork(conf);\n-      try {\n-        ctx.clear();\n-      } catch (Exception e) {\n-        /*best effort*/\n-        LOG.warn(\"Failed to clean up after tez job\");\n+      if (cleanContext) {\n+        try {\n+          ctx.clear();\n+        } catch (Exception e) {\n+          /*best effort*/\n+          LOG.warn(\"Failed to clean up after tez job\");\n+        }\n+      }\n+      // need to either move tmp files or remove them\n+      if (client != null) {\n+        // rc will only be overwritten if close errors out\n+        rc = close(work, rc);\n       }\n     }\n     return rc;\n@@ -115,6 +138,9 @@ private DAG build(JobConf conf, TezWork work, Path scratchDir,\n     List<BaseWork> ws = work.getAllWork();\n     Collections.reverse(ws);\n \n+    Path tezDir = DagUtils.getTezDir(scratchDir);\n+    FileSystem fs = tezDir.getFileSystem(conf);\n+\n     // the name of the dag is what is displayed in the AM/Job UI\n     DAG dag = new DAG(\n         Utilities.abbreviate(HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYSTRING),\n@@ -125,8 +151,8 @@ private DAG build(JobConf conf, TezWork work, Path scratchDir,\n \n       // translate work to vertex\n       JobConf wxConf = DagUtils.initializeVertexConf(conf, w);\n-      Vertex wx = DagUtils.createVertex(wxConf, w, scratchDir, i--,\n-          appJarLr, additionalLr, scratchDir.getFileSystem(conf), ctx);\n+      Vertex wx = DagUtils.createVertex(wxConf, w, tezDir, \n+          i--, appJarLr, additionalLr, fs, ctx);\n       dag.addVertex(wx);\n       workToVertex.put(w, wx);\n       workToConf.put(w, wxConf);\n@@ -155,14 +181,42 @@ private DAGClient submit(JobConf conf, DAG dag, Path scratchDir, LocalResource a\n     Map<String, LocalResource> amLrs = new HashMap<String, LocalResource>();\n     amLrs.put(DagUtils.getBaseName(appJarLr), appJarLr);\n \n+    Path tezDir = DagUtils.getTezDir(scratchDir);\n+\n     // ready to start execution on the cluster\n-    DAGClient dagClient = tezClient.submitDAGApplication(dag, scratchDir,\n+    DAGClient dagClient = tezClient.submitDAGApplication(dag, tezDir,\n         null, \"default\", Collections.singletonList(\"\"), amEnv, amLrs,\n         new TezConfiguration(conf));\n \n     return dagClient;\n   }\n \n+  /*\n+   * close will move the temp files into the right place for the fetch\n+   * task. If the job has failed it will clean up the files.\n+   */\n+  private int close(TezWork work, int rc) {\n+    try {\n+      JobCloseFeedBack feedBack = new JobCloseFeedBack();\n+      List<BaseWork> ws = work.getAllWork();\n+      for (BaseWork w: ws) {\n+        List<Operator<?>> ops = w.getAllOperators();\n+        for (Operator<?> op: ops) {\n+          op.jobClose(conf, rc == 0, feedBack);\n+        }\n+      }\n+    } catch (Exception e) {\n+      // jobClose needs to execute successfully otherwise fail task\n+      if (rc == 0) {\n+        rc = 3;\n+        String mesg = \"Job Commit failed with exception '\" \n+          + Utilities.getNameMessage(e) + \"'\";\n+        console.printError(mesg, \"\\n\" + StringUtils.stringifyException(e));\n+      }\n+    }\n+    return rc;\n+  }\n+\n   @Override\n   public boolean isMapRedTask() {\n     return true;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "sha": "7433ddcc014742af3fb09942958a18fce7f718f3", "changes": 23, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d", "patch": "@@ -772,6 +772,21 @@ public static void setTaskPlan(String path, String alias,\n     }\n   }\n \n+  /**\n+   * Set key and value descriptor\n+   * @param work RedueWork\n+   * @param rs ReduceSinkOperator\n+   */\n+  public static void setKeyAndValueDesc(ReduceWork work, ReduceSinkOperator rs) {\n+    work.setKeyDesc(rs.getConf().getKeySerializeInfo());\n+    int tag = Math.max(0, rs.getConf().getTag());\n+    List<TableDesc> tagToSchema = work.getTagToValueDesc();\n+    while (tag + 1 > tagToSchema.size()) {\n+      tagToSchema.add(null);\n+    }\n+    tagToSchema.set(tag, rs.getConf().getValueSerializeInfo());\n+  }\n+\n   /**\n    * set key and value descriptor.\n    *\n@@ -788,13 +803,7 @@ public static void setKeyAndValueDesc(ReduceWork plan,\n \n     if (topOp instanceof ReduceSinkOperator) {\n       ReduceSinkOperator rs = (ReduceSinkOperator) topOp;\n-      plan.setKeyDesc(rs.getConf().getKeySerializeInfo());\n-      int tag = Math.max(0, rs.getConf().getTag());\n-      List<TableDesc> tagToSchema = plan.getTagToValueDesc();\n-      while (tag + 1 > tagToSchema.size()) {\n-        tagToSchema.add(null);\n-      }\n-      tagToSchema.set(tag, rs.getConf().getValueSerializeInfo());\n+      setKeyAndValueDesc(plan, rs);\n     } else {\n       List<Operator<? extends OperatorDesc>> children = topOp.getChildOperators();\n       if (children != null) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java", "blob_url": "https://github.com/apache/hive/blob/36af62a7210b82196133ca7924cf7ba04a249e9d/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java", "sha": "48145ad00c66eeddcedcf22b363ee5125c1d97ab", "changes": 20, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java?ref=36af62a7210b82196133ca7924cf7ba04a249e9d", "patch": "@@ -109,6 +109,14 @@ public Object process(Node nd, Stack<Node> stack,\n         = (ReduceSinkOperator)root.getParentOperators().get(0);\n       reduceWork.setNumReduceTasks(reduceSink.getConf().getNumReducers());\n \n+      // need to fill in information about the key and value in the reducer\n+      GenMapRedUtils.setKeyAndValueDesc(reduceWork, reduceSink);\n+\n+      // needs to be fixed in HIVE-5052. This should be driven off of stats\n+      if (reduceWork.getNumReduceTasks() <= 0) {\n+        reduceWork.setNumReduceTasks(1);\n+      }\n+\n       tezWork.add(reduceWork);\n       tezWork.connect(\n           context.preceedingWork,\n@@ -129,13 +137,25 @@ public Object process(Node nd, Stack<Node> stack,\n     // Also note: the concept of leaf and root is reversed in hive for historical\n     // reasons. Roots are data sources, leaves are data sinks. I know.\n     if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n+\n+      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n+\n+      // need to add this branch to the key + value info\n+      assert operator instanceof ReduceSinkOperator \n+        && followingWork instanceof ReduceWork;\n+      ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n+      ReduceWork rWork = (ReduceWork) followingWork;\n+      GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n+\n+      // add dependency between the two work items\n       tezWork.connect(work, context.leafOperatorToFollowingWork.get(operator));\n     }\n \n     // This is where we cut the tree as described above. We also remember that\n     // we might have to connect parent work with this work later.\n     for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {\n       assert !context.leafOperatorToFollowingWork.containsKey(parent);\n+      assert !(work instanceof MapWork);\n       context.leafOperatorToFollowingWork.put(parent, work);\n       LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n       root.removeParent(parent);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/860edcc62a229a41be0a148c2f00b691e5ab6e3d", "parent": "https://github.com/apache/hive/commit/1cc94c9aab978f4cb07e7845f0bd6444052d72b7", "message": "HIVE-4798 : NPE when we call isSame from an instance of ExprNodeConstantDesc with null value (Yin Huai via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1498151 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_250", "file": [{"additions": 5, "raw_url": "https://github.com/apache/hive/raw/860edcc62a229a41be0a148c2f00b691e5ab6e3d/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java", "blob_url": "https://github.com/apache/hive/blob/860edcc62a229a41be0a148c2f00b691e5ab6e3d/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java", "sha": "f4ffde51ba1c6e4379b52343164ad87108de6084", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java?ref=860edcc62a229a41be0a148c2f00b691e5ab6e3d", "patch": "@@ -103,7 +103,11 @@ public boolean isSame(Object o) {\n     if (!typeInfo.equals(dest.getTypeInfo())) {\n       return false;\n     }\n-    if (!value.equals(dest.getValue())) {\n+    if (value == null) {\n+      if (dest.getValue() != null) {\n+        return false;\n+      }\n+    } else if (!value.equals(dest.getValue())) {\n       return false;\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeConstantDesc.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/fb4141cd83cc449e5fe980b43f14cdab981b0c8e", "parent": "https://github.com/apache/hive/commit/10814d9f04d9e500171452f77be50b7cbe8db84d", "message": "HIVE-4688 : NPE in writing null values. (Jitendra Nath Pandey via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/vectorization@1491154 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_251", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java", "blob_url": "https://github.com/apache/hive/blob/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java", "sha": "8379385fc15ff8713c6713208971e2e0a729abbe", "changes": 13, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java?ref=fb4141cd83cc449e5fe980b43f14cdab981b0c8e", "patch": "@@ -43,6 +43,7 @@\n import org.apache.hadoop.io.FloatWritable;\n import org.apache.hadoop.io.IntWritable;\n import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -112,11 +113,11 @@ public Object writeValue(ColumnVector column, int row) throws HiveException {\n       } else if (!lcv.noNulls && !lcv.isRepeating && !lcv.isNull[row]) {\n         return writeValue(lcv.vector[row]);\n       } else if (!lcv.noNulls && !lcv.isRepeating && lcv.isNull[row]) {\n-        return null;\n+        return NullWritable.get();\n       } else if (!lcv.noNulls && lcv.isRepeating && !lcv.isNull[0]) {\n         return writeValue(lcv.vector[0]);\n       } else if (!lcv.noNulls && lcv.isRepeating && lcv.isNull[0]) {\n-        return null;\n+        return NullWritable.get();\n       }\n       throw new HiveException(\n         String.format(\n@@ -140,11 +141,11 @@ public Object writeValue(ColumnVector column, int row) throws HiveException {\n       } else if (!dcv.noNulls && !dcv.isRepeating && !dcv.isNull[row]) {\n         return writeValue(dcv.vector[row]);\n       } else if (!dcv.noNulls && !dcv.isRepeating && dcv.isNull[row]) {\n-        return null;\n+        return NullWritable.get();\n       } else if (!dcv.noNulls && dcv.isRepeating && !dcv.isNull[0]) {\n         return writeValue(dcv.vector[0]);\n       } else if (!dcv.noNulls && dcv.isRepeating && dcv.isNull[0]) {\n-        return null;\n+        return NullWritable.get();\n       }\n       throw new HiveException(\n         String.format(\n@@ -168,11 +169,11 @@ public Object writeValue(ColumnVector column, int row) throws HiveException {\n       } else if (!bcv.noNulls && !bcv.isRepeating && !bcv.isNull[row]) {\n         return writeValue(bcv.vector[row], bcv.start[row], bcv.length[row]);\n       } else if (!bcv.noNulls && !bcv.isRepeating && bcv.isNull[row]) {\n-        return null;\n+        return NullWritable.get();\n       } else if (!bcv.noNulls && bcv.isRepeating && !bcv.isNull[0]) {\n         return writeValue(bcv.vector[0], bcv.start[0], bcv.length[0]);\n       } else if (!bcv.noNulls && bcv.isRepeating && bcv.isNull[0]) {\n-        return null;\n+        return NullWritable.get();\n       }\n       throw new HiveException(\n         String.format(", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java"}, {"additions": 206, "raw_url": "https://github.com/apache/hive/raw/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java", "blob_url": "https://github.com/apache/hive/blob/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java", "sha": "cde5b1f8e6850302b0e54d6e5927841217aad982", "changes": 206, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java?ref=fb4141cd83cc449e5fe980b43f14cdab981b0c8e", "patch": "@@ -0,0 +1,206 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.expressions;\n+\n+\n+import java.sql.Timestamp;\n+import java.util.Random;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampUtils;\n+import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.serde2.io.ByteWritable;\n+import org.apache.hadoop.hive.serde2.io.DoubleWritable;\n+import org.apache.hadoop.hive.serde2.io.ShortWritable;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.io.BooleanWritable;\n+import org.apache.hadoop.io.FloatWritable;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.junit.Test;\n+\n+public class TestVectorExpressionWriters {\n+\n+  private final int vectorSize = 5;\n+\n+  private VectorExpressionWriter getWriter(TypeInfo colTypeInfo) throws HiveException {\n+    ExprNodeDesc columnDesc = new ExprNodeColumnDesc();\n+    columnDesc.setTypeInfo(colTypeInfo);\n+    VectorExpressionWriter vew = VectorExpressionWriterFactory\n+        .genVectorExpressionWritable(columnDesc);\n+    return vew;\n+  }\n+\n+  private Writable getWritableValue(TypeInfo ti, double value) {\n+    if (ti.equals(TypeInfoFactory.floatTypeInfo)) {\n+      return new FloatWritable((float) value);\n+    } else if (ti.equals(TypeInfoFactory.doubleTypeInfo)) {\n+      return new DoubleWritable(value);\n+    }\n+    return null;\n+  }\n+\n+  private Writable getWritableValue(TypeInfo ti, byte[] value) {\n+    if (ti.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return new Text(value);\n+    }\n+    return null;\n+  }\n+\n+  private Writable getWritableValue(TypeInfo ti, long value) {\n+    if (ti.equals(TypeInfoFactory.byteTypeInfo)) {\n+      return new ByteWritable((byte) value);\n+    } else if (ti.equals(TypeInfoFactory.shortTypeInfo)) {\n+      return new ShortWritable((short) value);\n+    } else if (ti.equals(TypeInfoFactory.intTypeInfo)) {\n+      return new IntWritable( (int) value);\n+    } else if (ti.equals(TypeInfoFactory.longTypeInfo)) {\n+      return new LongWritable( (long) value);\n+    } else if (ti.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return new BooleanWritable( value == 0 ? false : true);\n+    } else if (ti.equals(TypeInfoFactory.timestampTypeInfo)) {\n+      Timestamp ts = new Timestamp(value);\n+      TimestampUtils.assignTimeInNanoSec(value, ts);\n+      TimestampWritable tw = new TimestampWritable(ts);\n+      return tw;\n+    }\n+    return null;\n+  }\n+\n+  private void testWriterDouble(TypeInfo type) throws HiveException {\n+    DoubleColumnVector dcv = VectorizedRowGroupGenUtil.generateDoubleColumnVector(true, false,\n+        this.vectorSize, new Random(10));\n+    dcv.isNull[2] = true;\n+    VectorExpressionWriter vew = getWriter(type);\n+    for (int i = 0; i < vectorSize; i++) {\n+      Writable w = (Writable) vew.writeValue(dcv, i);\n+      if (!(w instanceof NullWritable)) {\n+        Writable expected = getWritableValue(type, dcv.vector[i]);\n+        Assert.assertEquals(expected, w);\n+      } else {\n+        Assert.assertTrue(dcv.isNull[i]);\n+      }\n+    }\n+  }\n+\n+  private void testWriterLong(TypeInfo type) throws HiveException {\n+    LongColumnVector lcv = VectorizedRowGroupGenUtil.generateLongColumnVector(true, false,\n+        vectorSize, new Random(10));\n+    lcv.isNull[3] = true;\n+    VectorExpressionWriter vew = getWriter(type);\n+    for (int i = 0; i < vectorSize; i++) {\n+      Writable w = (Writable) vew.writeValue(lcv, i);\n+      if (!(w instanceof NullWritable)) {\n+        Writable expected = getWritableValue(type, lcv.vector[i]);\n+        if (expected instanceof TimestampWritable) {\n+          TimestampWritable t1 = (TimestampWritable) expected;\n+          TimestampWritable t2 = (TimestampWritable) w;\n+          Assert.assertTrue(t1.getNanos() == t2.getNanos());\n+          Assert.assertTrue(t1.getSeconds() == t2.getSeconds());\n+          continue;\n+        }\n+        Assert.assertEquals(expected, w);\n+      } else {\n+        Assert.assertTrue(lcv.isNull[i]);\n+      }\n+    }\n+  }\n+\n+  private void testWriterBytes(TypeInfo type) throws HiveException {\n+    Text t1 = new Text(\"alpha\");\n+    Text t2 = new Text(\"beta\");\n+    BytesColumnVector bcv = new BytesColumnVector(vectorSize);\n+    bcv.noNulls = false;\n+    bcv.initBuffer();\n+    bcv.setVal(0, t1.getBytes(), 0, t1.getLength());\n+    bcv.isNull[1] = true;\n+    bcv.setVal(2, t2.getBytes(), 0, t2.getLength());\n+    bcv.isNull[3] = true;\n+    bcv.setVal(4, t1.getBytes(), 0, t1.getLength());\n+    VectorExpressionWriter vew = getWriter(type);\n+    for (int i = 0; i < vectorSize; i++) {\n+      Writable w = (Writable) vew.writeValue(bcv, i);\n+      if (!(w instanceof NullWritable)) {\n+        byte [] val = new byte[bcv.length[i]];\n+        System.arraycopy(bcv.vector[i], bcv.start[i], val, 0, bcv.length[i]);\n+        Writable expected = getWritableValue(type, val);\n+        Assert.assertEquals(expected, w);\n+      } else {\n+        Assert.assertTrue(bcv.isNull[i]);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterDouble() throws HiveException {\n+    testWriterDouble(TypeInfoFactory.doubleTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterFloat() throws HiveException {\n+    testWriterDouble(TypeInfoFactory.floatTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterLong() throws HiveException {\n+    testWriterLong(TypeInfoFactory.longTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterInt() throws HiveException {\n+    testWriterLong(TypeInfoFactory.intTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterShort() throws HiveException {\n+    testWriterLong(TypeInfoFactory.shortTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterBoolean() throws HiveException {\n+    testWriterLong(TypeInfoFactory.booleanTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterTimestamp() throws HiveException {\n+    testWriterLong(TypeInfoFactory.timestampTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterBye() throws HiveException {\n+    testWriterLong(TypeInfoFactory.byteTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterBytes() throws HiveException {\n+    testWriterBytes(TypeInfoFactory.stringTypeInfo);\n+  }\n+}", "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/6bc2e2878eca83eb153f1b1f528b84662647977d", "parent": "https://github.com/apache/hive/commit/f5514678cb7cd9851c24106fdb25626fb27efb12", "message": "HIVE-4526 : auto_sortmerge_join_9.q throws NPE but test is succeeded (Navis via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1489703 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_252", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/6bc2e2878eca83eb153f1b1f528b84662647977d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SortMergeJoinTaskDispatcher.java", "blob_url": "https://github.com/apache/hive/blob/6bc2e2878eca83eb153f1b1f528b84662647977d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SortMergeJoinTaskDispatcher.java", "sha": "8e1bafe3d1a9511cc688a669a00011ab229b311f", "changes": 13, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SortMergeJoinTaskDispatcher.java?ref=6bc2e2878eca83eb153f1b1f528b84662647977d", "patch": "@@ -118,17 +118,10 @@ private void genSMBJoinWork(MapredWork currWork, SMBMapJoinOperator smbJoinOp) {\n \n       PartitionDesc partitionInfo = currWork.getAliasToPartnInfo().get(alias);\n       if (fetchWork.getTblDir() != null) {\n-        ArrayList<String> aliases = new ArrayList<String>();\n-        aliases.add(alias);\n-        currWork.getPathToAliases().put(fetchWork.getTblDir(), aliases);\n-        currWork.getPathToPartitionInfo().put(fetchWork.getTblDir(), partitionInfo);\n-      }\n-      else {\n+        currWork.mergeAliasedInput(alias, fetchWork.getTblDir(), partitionInfo);\n+      } else {\n         for (String pathDir : fetchWork.getPartDir()) {\n-          ArrayList<String> aliases = new ArrayList<String>();\n-          aliases.add(alias);\n-          currWork.getPathToAliases().put(pathDir, aliases);\n-          currWork.getPathToPartitionInfo().put(pathDir, partitionInfo);\n+          currWork.mergeAliasedInput(alias, pathDir, partitionInfo);\n         }\n       }\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/SortMergeJoinTaskDispatcher.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/6bc2e2878eca83eb153f1b1f528b84662647977d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java", "blob_url": "https://github.com/apache/hive/blob/6bc2e2878eca83eb153f1b1f528b84662647977d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java", "sha": "616994ed5a5a5eb6b51f5ffa99c33a0991766252", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java?ref=6bc2e2878eca83eb153f1b1f528b84662647977d", "patch": "@@ -20,6 +20,7 @@\n \n import java.io.ByteArrayOutputStream;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Collection;\n import java.util.HashMap;\n import java.util.Iterator;\n@@ -223,6 +224,17 @@ public void setAliasToWork(\n     this.aliasToWork = aliasToWork;\n   }\n \n+  public void mergeAliasedInput(String alias, String pathDir, PartitionDesc partitionInfo) {\n+    ArrayList<String> aliases = pathToAliases.get(pathDir);\n+    if (aliases == null) {\n+      aliases = new ArrayList<String>(Arrays.asList(alias));\n+      pathToAliases.put(pathDir, aliases);\n+      pathToPartitionInfo.put(pathDir, partitionInfo);\n+    } else {\n+      aliases.add(alias);\n+    }\n+  }\n+\n   /**\n    * @return the mapredLocalWork\n    */", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/6bc2e2878eca83eb153f1b1f528b84662647977d/ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out", "blob_url": "https://github.com/apache/hive/blob/6bc2e2878eca83eb153f1b1f528b84662647977d/ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out", "sha": "3d51ef61013ef57a70d5f80d1247ce8f6ae7028b", "changes": 24, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out?ref=6bc2e2878eca83eb153f1b1f528b84662647977d", "patch": "@@ -4765,18 +4765,6 @@ PREHOOK: query: select count(*) from\n PREHOOK: type: QUERY\n PREHOOK: Input: default@tbl1\n #### A masked pattern was here ####\n-Execution failed with exit status: 2\n-Obtaining error information\n-\n-Task failed!\n-Task ID:\n-  Stage-7\n-\n-Logs:\n-\n-#### A masked pattern was here ####\n-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask\n-ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask\n POSTHOOK: query: select count(*) from \n   (\n   select * from\n@@ -6137,18 +6125,6 @@ PREHOOK: type: QUERY\n PREHOOK: Input: default@tbl1\n PREHOOK: Input: default@tbl2\n #### A masked pattern was here ####\n-Execution failed with exit status: 2\n-Obtaining error information\n-\n-Task failed!\n-Task ID:\n-  Stage-10\n-\n-Logs:\n-\n-#### A masked pattern was here ####\n-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapredLocalTask\n-ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.MapRedTask\n POSTHOOK: query: select count(*) from \n   (select a.key as key, a.value as value from tbl1 a where key < 6) subq1 \n     join", "filename": "ql/src/test/results/clientpositive/auto_sortmerge_join_9.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/ddd0b8f7a4c2743f28b9bded725ab55ee3000526", "parent": "https://github.com/apache/hive/commit/5cdecad9402e5b48575f6d65f5d69dca62d4029b", "message": "HIVE-3846 : alter view rename NPEs with authorization on. (Teddy Choi via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1489009 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_253", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java", "blob_url": "https://github.com/apache/hive/blob/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java", "sha": "85e5b310c9046e771b6ae6a80b3c365cd7cba945", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java?ref=ddd0b8f7a4c2743f28b9bded725ab55ee3000526", "patch": "@@ -95,6 +95,7 @@\n     commandType.put(HiveParser.TOK_DESCDATABASE, HiveOperation.DESCDATABASE);\n     commandType.put(HiveParser.TOK_ALTERTABLE_SKEWED, HiveOperation.ALTERTABLE_SKEWED);\n     commandType.put(HiveParser.TOK_ANALYZE, HiveOperation.ANALYZE_TABLE);\n+    commandType.put(HiveParser.TOK_ALTERVIEW_RENAME, HiveOperation.ALTERVIEW_RENAME);\n   }\n \n   static {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java", "blob_url": "https://github.com/apache/hive/blob/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java", "sha": "52f422159442034e987da194e4598764a32148de", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java?ref=ddd0b8f7a4c2743f28b9bded725ab55ee3000526", "patch": "@@ -21,7 +21,6 @@\n import org.apache.hadoop.hive.ql.security.authorization.Privilege;\n \n public enum HiveOperation {\n-\n   EXPLAIN(\"EXPLAIN\", null, null),\n   LOAD(\"LOAD\", null, new Privilege[]{Privilege.ALTER_DATA}),\n   EXPORT(\"EXPORT\", new Privilege[]{Privilege.SELECT}, null),\n@@ -102,6 +101,7 @@\n   ALTERTABLE_SKEWED(\"ALTERTABLE_SKEWED\", new Privilege[] {Privilege.ALTER_METADATA}, null),\n   ALTERTBLPART_SKEWED_LOCATION(\"ALTERTBLPART_SKEWED_LOCATION\",\n       new Privilege[] {Privilege.ALTER_DATA}, null),\n+  ALTERVIEW_RENAME(\"ALTERVIEW_RENAME\", new Privilege[] {Privilege.ALTER_METADATA}, null),\n   ;\n \n   private String operationName;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/queries/clientpositive/authorization_8.q", "blob_url": "https://github.com/apache/hive/blob/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/queries/clientpositive/authorization_8.q", "sha": "67fcf3162dd567e11a9f9e8f7ac980c57cd4bfc5", "changes": 8, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_8.q?ref=ddd0b8f7a4c2743f28b9bded725ab55ee3000526", "patch": "@@ -0,0 +1,8 @@\n+set hive.security.authorization.enabled=true;\n+GRANT ALL TO USER hive_test_user;\n+CREATE TABLE tbl_j5jbymsx8e (key INT, value STRING) PARTITIONED BY (ds STRING);\n+CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e;\n+DESCRIBE view_j5jbymsx8e_1;\n+ALTER VIEW view_j5jbymsx8e_1 RENAME TO view_j5jbymsx8e_2;\n+REVOKE ALL FROM USER hive_test_user;\n+set hive.security.authorization.enabled=false;", "filename": "ql/src/test/queries/clientpositive/authorization_8.q"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/results/clientnegative/recursive_view.q.out", "blob_url": "https://github.com/apache/hive/blob/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/results/clientnegative/recursive_view.q.out", "sha": "1171fa8baf9dd1d8457bddf39bdf8ae6095e834f", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/recursive_view.q.out?ref=ddd0b8f7a4c2743f28b9bded725ab55ee3000526", "patch": "@@ -68,11 +68,11 @@ POSTHOOK: type: DROPVIEW\n POSTHOOK: Input: default@r0\n POSTHOOK: Output: default@r0\n PREHOOK: query: alter view r3 rename to r0\n-PREHOOK: type: null\n+PREHOOK: type: ALTERVIEW_RENAME\n PREHOOK: Input: default@r3\n PREHOOK: Output: default@r3\n POSTHOOK: query: alter view r3 rename to r0\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERVIEW_RENAME\n POSTHOOK: Input: default@r3\n POSTHOOK: Output: default@r0\n POSTHOOK: Output: default@r3", "filename": "ql/src/test/results/clientnegative/recursive_view.q.out"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/results/clientpositive/alter_view_rename.q.out", "blob_url": "https://github.com/apache/hive/blob/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/results/clientpositive/alter_view_rename.q.out", "sha": "d6838f54329f63fcffa2c734f4658d626cbb2395", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/alter_view_rename.q.out?ref=ddd0b8f7a4c2743f28b9bded725ab55ee3000526", "patch": "@@ -18,11 +18,11 @@ ds                  \tstring              \tNone\n \t \t \n #### A masked pattern was here ####\n PREHOOK: query: ALTER VIEW view1 RENAME TO view2\n-PREHOOK: type: null\n+PREHOOK: type: ALTERVIEW_RENAME\n PREHOOK: Input: default@view1\n PREHOOK: Output: default@view1\n POSTHOOK: query: ALTER VIEW view1 RENAME TO view2\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERVIEW_RENAME\n POSTHOOK: Input: default@view1\n POSTHOOK: Output: default@view1\n POSTHOOK: Output: default@view2", "filename": "ql/src/test/results/clientpositive/alter_view_rename.q.out"}, {"additions": 34, "raw_url": "https://github.com/apache/hive/raw/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/results/clientpositive/authorization_8.q.out", "blob_url": "https://github.com/apache/hive/blob/ddd0b8f7a4c2743f28b9bded725ab55ee3000526/ql/src/test/results/clientpositive/authorization_8.q.out", "sha": "b66fabcc848e736ad9d52817d1d968bd44ad32a2", "changes": 34, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_8.q.out?ref=ddd0b8f7a4c2743f28b9bded725ab55ee3000526", "patch": "@@ -0,0 +1,34 @@\n+PREHOOK: query: GRANT ALL TO USER hive_test_user\n+PREHOOK: type: GRANT_PRIVILEGE\n+POSTHOOK: query: GRANT ALL TO USER hive_test_user\n+POSTHOOK: type: GRANT_PRIVILEGE\n+PREHOOK: query: CREATE TABLE tbl_j5jbymsx8e (key INT, value STRING) PARTITIONED BY (ds STRING)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE tbl_j5jbymsx8e (key INT, value STRING) PARTITIONED BY (ds STRING)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@tbl_j5jbymsx8e\n+PREHOOK: query: CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e\n+PREHOOK: type: CREATEVIEW\n+POSTHOOK: query: CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e\n+POSTHOOK: type: CREATEVIEW\n+POSTHOOK: Output: default@view_j5jbymsx8e_1\n+PREHOOK: query: DESCRIBE view_j5jbymsx8e_1\n+PREHOOK: type: DESCTABLE\n+POSTHOOK: query: DESCRIBE view_j5jbymsx8e_1\n+POSTHOOK: type: DESCTABLE\n+key                 \tint                 \tNone                \n+value               \tstring              \tNone                \n+ds                  \tstring              \tNone                \n+PREHOOK: query: ALTER VIEW view_j5jbymsx8e_1 RENAME TO view_j5jbymsx8e_2\n+PREHOOK: type: ALTERVIEW_RENAME\n+PREHOOK: Input: default@view_j5jbymsx8e_1\n+PREHOOK: Output: default@view_j5jbymsx8e_1\n+POSTHOOK: query: ALTER VIEW view_j5jbymsx8e_1 RENAME TO view_j5jbymsx8e_2\n+POSTHOOK: type: ALTERVIEW_RENAME\n+POSTHOOK: Input: default@view_j5jbymsx8e_1\n+POSTHOOK: Output: default@view_j5jbymsx8e_1\n+POSTHOOK: Output: default@view_j5jbymsx8e_2\n+PREHOOK: query: REVOKE ALL FROM USER hive_test_user\n+PREHOOK: type: REVOKE_PRIVILEGE\n+POSTHOOK: query: REVOKE ALL FROM USER hive_test_user\n+POSTHOOK: type: REVOKE_PRIVILEGE", "filename": "ql/src/test/results/clientpositive/authorization_8.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/a56e1c45347ad34d413445b8a9f48bcf32d6eb05", "parent": "https://github.com/apache/hive/commit/01a5c3d3828032d47f60417eb7bc4f5f864ec4b7", "message": "HIVE-4540 JOIN-GRP BY-DISTINCT fails with NPE when mapjoin.mapreduce=true (Gunther Hagleitner via Navis)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1486721 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_254", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java", "blob_url": "https://github.com/apache/hive/blob/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java", "sha": "da9423edb560aa34bab702e766e912d6cc0e34a7", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java?ref=a56e1c45347ad34d413445b8a9f48bcf32d6eb05", "patch": "@@ -36,6 +36,7 @@\n import org.apache.hadoop.hive.ql.exec.JoinOperator;\n import org.apache.hadoop.hive.ql.exec.MapRedTask;\n import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n@@ -364,6 +365,17 @@ private void mergeMapJoinTaskWithMapReduceTask(MapRedTask mapJoinTask, Configura\n       return;\n     }\n \n+    // remove the unnecessary TableScan\n+    if (childAliasOp instanceof TableScanOperator) {\n+      TableScanOperator tso = (TableScanOperator)childAliasOp;\n+      if (tso.getNumChild() != 1) {\n+        // shouldn't happen\n+        return;\n+      }\n+      childAliasOp = tso.getChildOperators().get(0);\n+      childAliasOp.getParentOperators().remove(tso);\n+    }\n+\n     // Merge the 2 trees - remove the FileSinkOperator from the first tree pass it to the\n     // top of the second\n     Operator<? extends Serializable> parentFOp = mapJoinTaskFileSinkOperator", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinTaskDispatcher.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/test/queries/clientpositive/auto_join33.q", "blob_url": "https://github.com/apache/hive/blob/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/test/queries/clientpositive/auto_join33.q", "sha": "5c85842e6d5506a8521a360c010aaf683a17db8f", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/auto_join33.q?ref=a56e1c45347ad34d413445b8a9f48bcf32d6eb05", "patch": "@@ -0,0 +1,16 @@\n+set hive.auto.convert.join=true;\n+set hive.optimize.mapjoin.mapreduce=true;\n+\n+-- empty tables\n+create table studenttab10k (name string, age int, gpa double);\n+create table votertab10k (name string, age int, registration string, contributions float);\n+\n+explain select s.name, count(distinct registration)\n+from studenttab10k s join votertab10k v\n+on (s.name = v.name)\n+group by s.name;\n+\n+select s.name, count(distinct registration)\n+from studenttab10k s join votertab10k v\n+on (s.name = v.name)\n+group by s.name;", "filename": "ql/src/test/queries/clientpositive/auto_join33.q"}, {"additions": 148, "raw_url": "https://github.com/apache/hive/raw/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/test/results/clientpositive/auto_join33.q.out", "blob_url": "https://github.com/apache/hive/blob/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/test/results/clientpositive/auto_join33.q.out", "sha": "8fc0e846811bc9181467d0938f1a0aa0f2b6875b", "changes": 148, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_join33.q.out?ref=a56e1c45347ad34d413445b8a9f48bcf32d6eb05", "patch": "@@ -0,0 +1,148 @@\n+PREHOOK: query: -- empty tables\n+create table studenttab10k (name string, age int, gpa double)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: -- empty tables\n+create table studenttab10k (name string, age int, gpa double)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@studenttab10k\n+PREHOOK: query: create table votertab10k (name string, age int, registration string, contributions float)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table votertab10k (name string, age int, registration string, contributions float)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@votertab10k\n+PREHOOK: query: explain select s.name, count(distinct registration)\n+from studenttab10k s join votertab10k v\n+on (s.name = v.name)\n+group by s.name\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select s.name, count(distinct registration)\n+from studenttab10k s join votertab10k v\n+on (s.name = v.name)\n+group by s.name\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME studenttab10k) s) (TOK_TABREF (TOK_TABNAME votertab10k) v) (= (. (TOK_TABLE_OR_COL s) name) (. (TOK_TABLE_OR_COL v) name)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL s) name)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL registration)))) (TOK_GROUPBY (. (TOK_TABLE_OR_COL s) name))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-5 is a root stage\n+  Stage-4 depends on stages: Stage-5\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-5\n+    Map Reduce Local Work\n+      Alias -> Map Local Tables:\n+        s \n+          Fetch Operator\n+            limit: -1\n+      Alias -> Map Local Operator Tree:\n+        s \n+          TableScan\n+            alias: s\n+            HashTable Sink Operator\n+              condition expressions:\n+                0 {name}\n+                1 {registration}\n+              handleSkewJoin: false\n+              keys:\n+                0 [Column[name]]\n+                1 [Column[name]]\n+              Position of Big Table: 1\n+\n+  Stage: Stage-4\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        v \n+          TableScan\n+            alias: v\n+            Map Join Operator\n+              condition map:\n+                   Inner Join 0 to 1\n+              condition expressions:\n+                0 {name}\n+                1 {registration}\n+              handleSkewJoin: false\n+              keys:\n+                0 [Column[name]]\n+                1 [Column[name]]\n+              outputColumnNames: _col0, _col7\n+              Position of Big Table: 1\n+              Select Operator\n+                expressions:\n+                      expr: _col0\n+                      type: string\n+                      expr: _col7\n+                      type: string\n+                outputColumnNames: _col0, _col7\n+                Group By Operator\n+                  aggregations:\n+                        expr: count(DISTINCT _col7)\n+                  bucketGroup: false\n+                  keys:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col7\n+                        type: string\n+                  mode: hash\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Reduce Output Operator\n+                    key expressions:\n+                          expr: _col0\n+                          type: string\n+                          expr: _col1\n+                          type: string\n+                    sort order: ++\n+                    Map-reduce partition columns:\n+                          expr: _col0\n+                          type: string\n+                    tag: -1\n+                    value expressions:\n+                          expr: _col2\n+                          type: bigint\n+      Local Work:\n+        Map Reduce Local Work\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: count(DISTINCT KEY._col1:0._col0)\n+          bucketGroup: false\n+          keys:\n+                expr: KEY._col0\n+                type: string\n+          mode: mergepartial\n+          outputColumnNames: _col0, _col1\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: string\n+                  expr: _col1\n+                  type: bigint\n+            outputColumnNames: _col0, _col1\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: select s.name, count(distinct registration)\n+from studenttab10k s join votertab10k v\n+on (s.name = v.name)\n+group by s.name\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@studenttab10k\n+PREHOOK: Input: default@votertab10k\n+#### A masked pattern was here ####\n+POSTHOOK: query: select s.name, count(distinct registration)\n+from studenttab10k s join votertab10k v\n+on (s.name = v.name)\n+group by s.name\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@studenttab10k\n+POSTHOOK: Input: default@votertab10k\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/auto_join33.q.out"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/test/results/clientpositive/multiMapJoin1.q.out", "blob_url": "https://github.com/apache/hive/blob/a56e1c45347ad34d413445b8a9f48bcf32d6eb05/ql/src/test/results/clientpositive/multiMapJoin1.q.out", "sha": "3b3eb3ff4ef52e853fe0411112063f89a131f5c1", "changes": 36, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/multiMapJoin1.q.out?ref=a56e1c45347ad34d413445b8a9f48bcf32d6eb05", "patch": "@@ -598,18 +598,18 @@ STAGE PLANS:\n                               type: string\n                         mode: hash\n                         outputColumnNames: _col0, _col1\n-                          Reduce Output Operator\n-                            key expressions:\n-                                  expr: _col0\n-                                  type: string\n-                            sort order: +\n-                            Map-reduce partition columns:\n-                                  expr: _col0\n-                                  type: string\n-                            tag: -1\n-                            value expressions:\n-                                  expr: _col1\n-                                  type: bigint\n+                        Reduce Output Operator\n+                          key expressions:\n+                                expr: _col0\n+                                type: string\n+                          sort order: +\n+                          Map-reduce partition columns:\n+                                expr: _col0\n+                                type: string\n+                          tag: -1\n+                          value expressions:\n+                                expr: _col1\n+                                type: bigint\n       Local Work:\n         Map Reduce Local Work\n       Reduce Operator Tree:\n@@ -1836,12 +1836,12 @@ STAGE PLANS:\n                               bucketGroup: false\n                               mode: hash\n                               outputColumnNames: _col0\n-                                Reduce Output Operator\n-                                  sort order: \n-                                  tag: -1\n-                                  value expressions:\n-                                        expr: _col0\n-                                        type: bigint\n+                              Reduce Output Operator\n+                                sort order: \n+                                tag: -1\n+                                value expressions:\n+                                      expr: _col0\n+                                      type: bigint\n       Local Work:\n         Map Reduce Local Work\n       Reduce Operator Tree:", "filename": "ql/src/test/results/clientpositive/multiMapJoin1.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/11cd905767b98d866b9649bc340dff49b650d9b2", "parent": "https://github.com/apache/hive/commit/fc27aeea48a31782f6ed51aa63925e43a6ba01b9", "message": "HIVE-4342 NPE for query involving UNION ALL with nested JOIN and UNION ALL\n(Navis via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1470316 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_255", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java", "blob_url": "https://github.com/apache/hive/blob/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java", "sha": "a719a836dffbaf25c0a589747011a285db8de2e1", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java?ref=11cd905767b98d866b9649bc340dff49b650d9b2", "patch": "@@ -259,6 +259,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     // Copy into the current union task plan if\n     if (uPrsCtx.getMapOnlySubq(pos) && uPrsCtx.getRootTask(pos)) {\n       processSubQueryUnionMerge(ctx, uCtxTask, union, stack);\n+      if (ctx.getRootTasks().contains(currTask)) {\n+        ctx.getRootTasks().remove(currTask);\n+      }\n     }\n     // If it a map-reduce job, create a temporary file\n     else {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRUnion1.java"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcContext.java", "blob_url": "https://github.com/apache/hive/blob/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcContext.java", "sha": "4be6e8712924826c04c338b8ce073d759d1be7f9", "changes": 31, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcContext.java?ref=11cd905767b98d866b9649bc340dff49b650d9b2", "patch": "@@ -40,7 +40,7 @@\n     private final transient boolean[] mapOnlySubqSet;\n     private final transient boolean[] rootTask;\n \n-    private transient int numInputs;\n+    private final transient int numInputs;\n \n     public UnionParseContext(int numInputs) {\n       this.numInputs = numInputs;\n@@ -70,27 +70,22 @@ public int getNumInputs() {\n       return numInputs;\n     }\n \n-    public void setNumInputs(int numInputs) {\n-      this.numInputs = numInputs;\n-    }\n-\n     public boolean allMapOnlySubQ() {\n-      if (mapOnlySubq != null) {\n-        for (boolean mapOnly : mapOnlySubq) {\n-          if (!mapOnly) {\n-            return false;\n-          }\n-        }\n-      }\n-      return true;\n+      return isAllTrue(mapOnlySubq);\n     }\n \n     public boolean allMapOnlySubQSet() {\n-      if (mapOnlySubqSet != null) {\n-        for (boolean mapOnlySet : mapOnlySubqSet) {\n-          if (!mapOnlySet) {\n-            return false;\n-          }\n+      return isAllTrue(mapOnlySubqSet);\n+    }\n+\n+    public boolean allRootTasks() {\n+      return isAllTrue(rootTask);\n+    }\n+\n+    public boolean isAllTrue(boolean[] array) {\n+      for (boolean value : array) {\n+        if (!value) {\n+          return false;\n         }\n       }\n       return true;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcContext.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcFactory.java", "sha": "da13abf04d6a0f4e3cb630b66e62f282747ae582", "changes": 13, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcFactory.java?ref=11cd905767b98d866b9649bc340dff49b650d9b2", "patch": "@@ -138,20 +138,21 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         }\n         start--;\n       }\n+      assert parentUnionOperator != null;\n \n       // default to false\n       boolean mapOnly = false;\n-      if (parentUnionOperator != null) {\n-        UnionParseContext parentUCtx =\n+      boolean rootTask = false;\n+      UnionParseContext parentUCtx =\n           ctx.getUnionParseContext(parentUnionOperator);\n-        if (parentUCtx != null && parentUCtx.allMapOnlySubQSet()) {\n-          mapOnly = parentUCtx.allMapOnlySubQ();\n-        }\n+      if (parentUCtx != null && parentUCtx.allMapOnlySubQSet()) {\n+        mapOnly = parentUCtx.allMapOnlySubQ();\n+        rootTask = parentUCtx.allRootTasks();\n       }\n \n       uCtx.setMapOnlySubq(pos, mapOnly);\n \n-      uCtx.setRootTask(pos, false);\n+      uCtx.setRootTask(pos, rootTask);\n       ctx.setUnionParseContext(union, uCtx);\n       return null;\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcFactory.java"}, {"additions": 48, "raw_url": "https://github.com/apache/hive/raw/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/test/queries/clientpositive/union34.q", "blob_url": "https://github.com/apache/hive/blob/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/test/queries/clientpositive/union34.q", "sha": "a88e3955fa6e19f420bea7933242bd128224ace8", "changes": 48, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/union34.q?ref=11cd905767b98d866b9649bc340dff49b650d9b2", "patch": "@@ -0,0 +1,48 @@\n+-- HIVE-4342\n+-- Maponly union(UNION-13) is merged into non-maponly union(UNION-15)\n+-- In this case, task for UNION-13 should be removed from top-task and merged into task for UNION-15\n+-- TS[2]-SEL[3]-RS[5]-JOIN[6]-SEL[7]-UNION[15]-SEL[16]-RS[17]-EX[18]-FS[19]\n+-- TS[0]-SEL[1]-RS[4]-JOIN[6]\n+-- TS[8]-SEL[9]-UNION[13]-SEL[14]-UNION[15]\n+-- TS[11]-SEL[12]-UNION[13]\n+\n+create table src10_1 (key string, value string);\n+create table src10_2 (key string, value string);\n+create table src10_3 (key string, value string);\n+create table src10_4 (key string, value string);\n+\n+from (select * from src limit 10) a\n+insert overwrite table src10_1 select *\n+insert overwrite table src10_2 select *\n+insert overwrite table src10_3 select *\n+insert overwrite table src10_4 select *;\n+\n+set hive.auto.convert.join=true;\n+\n+explain\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key;\n+\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key;\n+\n+set hive.auto.convert.join=false;\n+\n+explain\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key;\n+\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key;", "filename": "ql/src/test/queries/clientpositive/union34.q"}, {"additions": 606, "raw_url": "https://github.com/apache/hive/raw/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/test/results/clientpositive/union34.q.out", "blob_url": "https://github.com/apache/hive/blob/11cd905767b98d866b9649bc340dff49b650d9b2/ql/src/test/results/clientpositive/union34.q.out", "sha": "166062a48d108bb843b6b003d1104775e7d3b60e", "changes": 606, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union34.q.out?ref=11cd905767b98d866b9649bc340dff49b650d9b2", "patch": "@@ -0,0 +1,606 @@\n+PREHOOK: query: -- HIVE-4342\n+-- Maponly union(UNION-13) is merged into non-maponly union(UNION-15)\n+-- In this case, task for UNION-13 should be removed from top-task and merged into task for UNION-15\n+-- TS[2]-SEL[3]-RS[5]-JOIN[6]-SEL[7]-UNION[15]-SEL[16]-RS[17]-EX[18]-FS[19]\n+-- TS[0]-SEL[1]-RS[4]-JOIN[6]\n+-- TS[8]-SEL[9]-UNION[13]-SEL[14]-UNION[15]\n+-- TS[11]-SEL[12]-UNION[13]\n+\n+create table src10_1 (key string, value string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: -- HIVE-4342\n+-- Maponly union(UNION-13) is merged into non-maponly union(UNION-15)\n+-- In this case, task for UNION-13 should be removed from top-task and merged into task for UNION-15\n+-- TS[2]-SEL[3]-RS[5]-JOIN[6]-SEL[7]-UNION[15]-SEL[16]-RS[17]-EX[18]-FS[19]\n+-- TS[0]-SEL[1]-RS[4]-JOIN[6]\n+-- TS[8]-SEL[9]-UNION[13]-SEL[14]-UNION[15]\n+-- TS[11]-SEL[12]-UNION[13]\n+\n+create table src10_1 (key string, value string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@src10_1\n+PREHOOK: query: create table src10_2 (key string, value string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table src10_2 (key string, value string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@src10_2\n+PREHOOK: query: create table src10_3 (key string, value string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table src10_3 (key string, value string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@src10_3\n+PREHOOK: query: create table src10_4 (key string, value string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table src10_4 (key string, value string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@src10_4\n+PREHOOK: query: from (select * from src limit 10) a\n+insert overwrite table src10_1 select *\n+insert overwrite table src10_2 select *\n+insert overwrite table src10_3 select *\n+insert overwrite table src10_4 select *\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@src10_1\n+PREHOOK: Output: default@src10_2\n+PREHOOK: Output: default@src10_3\n+PREHOOK: Output: default@src10_4\n+POSTHOOK: query: from (select * from src limit 10) a\n+insert overwrite table src10_1 select *\n+insert overwrite table src10_2 select *\n+insert overwrite table src10_3 select *\n+insert overwrite table src10_4 select *\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@src10_1\n+POSTHOOK: Output: default@src10_2\n+POSTHOOK: Output: default@src10_3\n+POSTHOOK: Output: default@src10_4\n+POSTHOOK: Lineage: src10_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: explain\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: src10_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) sub1) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_2))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) sub0) (= (. (TOK_TABLE_OR_COL sub0) key) (. (TOK_TABLE_OR_COL sub1) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL sub1) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL sub1) value))))) (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_3))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) sub2)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_4))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))) alias0)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)))))) alias1)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-7 is a root stage\n+  Stage-6 depends on stages: Stage-7\n+  Stage-2 depends on stages: Stage-6\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-7\n+    Map Reduce Local Work\n+      Alias -> Map Local Tables:\n+        null-subquery1:alias1-subquery1:sub1:src10_1 \n+          Fetch Operator\n+            limit: -1\n+      Alias -> Map Local Operator Tree:\n+        null-subquery1:alias1-subquery1:sub1:src10_1 \n+          TableScan\n+            alias: src10_1\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              HashTable Sink Operator\n+                condition expressions:\n+                  0 {_col0} {_col1}\n+                  1 \n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[_col0]]\n+                  1 [Column[_col0]]\n+                Position of Big Table: 1\n+\n+  Stage: Stage-6\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        null-subquery1:alias1-subquery1:sub0:src10_2 \n+          TableScan\n+            alias: src10_2\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+              outputColumnNames: _col0\n+              Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {_col0} {_col1}\n+                  1 \n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[_col0]]\n+                  1 [Column[_col0]]\n+                outputColumnNames: _col0, _col1\n+                Position of Big Table: 1\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col1\n+                        type: string\n+                  outputColumnNames: _col0, _col1\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+      Local Work:\n+        Map Reduce Local Work\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+#### A masked pattern was here ####\n+          TableScan\n+            Union\n+              Select Operator\n+                expressions:\n+                      expr: _col0\n+                      type: string\n+                      expr: _col1\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                Reduce Output Operator\n+                  key expressions:\n+                        expr: _col0\n+                        type: string\n+                  sort order: +\n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col1\n+                        type: string\n+        null-subquery2:alias1-subquery2-subquery1:alias0-subquery1:sub2:src10_3 \n+          TableScan\n+            alias: src10_3\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              Union\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col1\n+                        type: string\n+                  outputColumnNames: _col0, _col1\n+                  Union\n+                    Select Operator\n+                      expressions:\n+                            expr: _col0\n+                            type: string\n+                            expr: _col1\n+                            type: string\n+                      outputColumnNames: _col0, _col1\n+                      Reduce Output Operator\n+                        key expressions:\n+                              expr: _col0\n+                              type: string\n+                        sort order: +\n+                        tag: -1\n+                        value expressions:\n+                              expr: _col0\n+                              type: string\n+                              expr: _col1\n+                              type: string\n+        null-subquery2:alias1-subquery2-subquery2:alias0-subquery2:src10_4 \n+          TableScan\n+            alias: src10_4\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              Union\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col1\n+                        type: string\n+                  outputColumnNames: _col0, _col1\n+                  Union\n+                    Select Operator\n+                      expressions:\n+                            expr: _col0\n+                            type: string\n+                            expr: _col1\n+                            type: string\n+                      outputColumnNames: _col0, _col1\n+                      Reduce Output Operator\n+                        key expressions:\n+                              expr: _col0\n+                              type: string\n+                        sort order: +\n+                        tag: -1\n+                        value expressions:\n+                              expr: _col0\n+                              type: string\n+                              expr: _col1\n+                              type: string\n+      Reduce Operator Tree:\n+        Extract\n+          File Output Operator\n+            compressed: false\n+            GlobalTableId: 0\n+            table:\n+                input format: org.apache.hadoop.mapred.TextInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src10_1\n+PREHOOK: Input: default@src10_2\n+PREHOOK: Input: default@src10_3\n+PREHOOK: Input: default@src10_4\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src10_1\n+POSTHOOK: Input: default@src10_2\n+POSTHOOK: Input: default@src10_3\n+POSTHOOK: Input: default@src10_4\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: src10_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+165\tval_165\n+165\tval_165\n+165\tval_165\n+238\tval_238\n+238\tval_238\n+238\tval_238\n+255\tval_255\n+255\tval_255\n+255\tval_255\n+27\tval_27\n+27\tval_27\n+27\tval_27\n+278\tval_278\n+278\tval_278\n+278\tval_278\n+311\tval_311\n+311\tval_311\n+311\tval_311\n+409\tval_409\n+409\tval_409\n+409\tval_409\n+484\tval_484\n+484\tval_484\n+484\tval_484\n+86\tval_86\n+86\tval_86\n+86\tval_86\n+98\tval_98\n+98\tval_98\n+98\tval_98\n+PREHOOK: query: explain\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: src10_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_1))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) sub1) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_2))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) sub0) (= (. (TOK_TABLE_OR_COL sub0) key) (. (TOK_TABLE_OR_COL sub1) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL sub1) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL sub1) value))))) (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_3))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) sub2)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)))) (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src10_4))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF))))) alias0)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)))))) alias1)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL key)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        null-subquery1:alias1-subquery1:sub0:src10_2 \n+          TableScan\n+            alias: src10_2\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+              outputColumnNames: _col0\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: _col0\n+                      type: string\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: _col0\n+                      type: string\n+                tag: 1\n+        null-subquery1:alias1-subquery1:sub1:src10_1 \n+          TableScan\n+            alias: src10_1\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              Reduce Output Operator\n+                key expressions:\n+                      expr: _col0\n+                      type: string\n+                sort order: +\n+                Map-reduce partition columns:\n+                      expr: _col0\n+                      type: string\n+                tag: 0\n+                value expressions:\n+                      expr: _col0\n+                      type: string\n+                      expr: _col1\n+                      type: string\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          condition expressions:\n+            0 {VALUE._col0} {VALUE._col1}\n+            1 \n+          handleSkewJoin: false\n+          outputColumnNames: _col0, _col1\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: string\n+                  expr: _col1\n+                  type: string\n+            outputColumnNames: _col0, _col1\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+#### A masked pattern was here ####\n+          TableScan\n+            Union\n+              Select Operator\n+                expressions:\n+                      expr: _col0\n+                      type: string\n+                      expr: _col1\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                Reduce Output Operator\n+                  key expressions:\n+                        expr: _col0\n+                        type: string\n+                  sort order: +\n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col1\n+                        type: string\n+        null-subquery2:alias1-subquery2-subquery1:alias0-subquery1:sub2:src10_3 \n+          TableScan\n+            alias: src10_3\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              Union\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col1\n+                        type: string\n+                  outputColumnNames: _col0, _col1\n+                  Union\n+                    Select Operator\n+                      expressions:\n+                            expr: _col0\n+                            type: string\n+                            expr: _col1\n+                            type: string\n+                      outputColumnNames: _col0, _col1\n+                      Reduce Output Operator\n+                        key expressions:\n+                              expr: _col0\n+                              type: string\n+                        sort order: +\n+                        tag: -1\n+                        value expressions:\n+                              expr: _col0\n+                              type: string\n+                              expr: _col1\n+                              type: string\n+        null-subquery2:alias1-subquery2-subquery2:alias0-subquery2:src10_4 \n+          TableScan\n+            alias: src10_4\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              Union\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: string\n+                        expr: _col1\n+                        type: string\n+                  outputColumnNames: _col0, _col1\n+                  Union\n+                    Select Operator\n+                      expressions:\n+                            expr: _col0\n+                            type: string\n+                            expr: _col1\n+                            type: string\n+                      outputColumnNames: _col0, _col1\n+                      Reduce Output Operator\n+                        key expressions:\n+                              expr: _col0\n+                              type: string\n+                        sort order: +\n+                        tag: -1\n+                        value expressions:\n+                              expr: _col0\n+                              type: string\n+                              expr: _col1\n+                              type: string\n+      Reduce Operator Tree:\n+        Extract\n+          File Output Operator\n+            compressed: false\n+            GlobalTableId: 0\n+            table:\n+                input format: org.apache.hadoop.mapred.TextInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src10_1\n+PREHOOK: Input: default@src10_2\n+PREHOOK: Input: default@src10_3\n+PREHOOK: Input: default@src10_4\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT * FROM (\n+  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)\n+  UNION ALL\n+  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0\n+) alias1 order by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src10_1\n+POSTHOOK: Input: default@src10_2\n+POSTHOOK: Input: default@src10_3\n+POSTHOOK: Input: default@src10_4\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: src10_1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: src10_4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+165\tval_165\n+165\tval_165\n+165\tval_165\n+238\tval_238\n+238\tval_238\n+238\tval_238\n+255\tval_255\n+255\tval_255\n+255\tval_255\n+27\tval_27\n+27\tval_27\n+27\tval_27\n+278\tval_278\n+278\tval_278\n+278\tval_278\n+311\tval_311\n+311\tval_311\n+311\tval_311\n+409\tval_409\n+409\tval_409\n+409\tval_409\n+484\tval_484\n+484\tval_484\n+484\tval_484\n+86\tval_86\n+86\tval_86\n+86\tval_86\n+98\tval_98\n+98\tval_98\n+98\tval_98", "filename": "ql/src/test/results/clientpositive/union34.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "parent": "https://github.com/apache/hive/commit/0763293314df49c5fd5c628eb22b779e8d9dab3e", "message": "HIVE-4327 : NPE in constant folding with decimal (Gunther Hagleitner via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1468423 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_256", "file": [{"additions": 10, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision.q", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision.q", "sha": "f49649837e2142f89c6ad32e9b5ed70c9f0317e0", "changes": 10, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/decimal_precision.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -0,0 +1,10 @@\n+DROP TABLE IF EXISTS DECIMAL_PRECISION;\n+\n+CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE;\n+\n+SELECT dec * 123456789012345678901234567890.123456789bd FROM DECIMAL_PRECISION;\n+\n+DROP TABLE DECIMAL_PRECISION;", "filename": "ql/src/test/queries/clientnegative/decimal_precision.q"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision_1.q", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientnegative/decimal_precision_1.q", "sha": "036ff1facc0a1ca8190b295f97b91e7af716b1b1", "changes": 10, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/decimal_precision_1.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -0,0 +1,10 @@\n+DROP TABLE IF EXISTS DECIMAL_PRECISION;\n+\n+CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE;\n+\n+SELECT * from DECIMAL_PRECISION WHERE dec > 1234567890123456789.0123456789bd;\n+\n+DROP TABLE DECIMAL_PRECISION;", "filename": "ql/src/test/queries/clientnegative/decimal_precision_1.q"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_precision.q", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_precision.q", "sha": "403c2be3fbc10dc6e18444055cea47d061ed4322", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/decimal_precision.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -17,4 +17,11 @@ SELECT dec, dec * dec FROM DECIMAL_PRECISION ORDER BY dec;\n \n SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION;\n \n+SELECT dec * cast('123456789012345678901234567890.123456789' as decimal) FROM DECIMAL_PRECISION LIMIT 1;\n+SELECT * from DECIMAL_PRECISION WHERE dec > cast('123456789012345678901234567890.123456789' as decimal) LIMIT 1;\n+SELECT dec * 123456789012345678901234567890.123456789 FROM DECIMAL_PRECISION LIMIT 1;\n+\n+SELECT MIN(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION;\n+SELECT COUNT(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION;\n+\n DROP TABLE DECIMAL_PRECISION;", "filename": "ql/src/test/queries/clientpositive/decimal_precision.q"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_udf.q", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/queries/clientpositive/decimal_udf.q", "sha": "b5ff088d1613a92b41948e7d48b0d042fdd52a37", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/decimal_udf.q?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -113,4 +113,16 @@ SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value;\n EXPLAIN SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF; \n SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF; \n \n+-- min\n+EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF;\n+SELECT MIN(key) FROM DECIMAL_UDF;\n+\n+-- max\n+EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF;\n+SELECT MAX(key) FROM DECIMAL_UDF;\n+\n+-- count\n+EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF;\n+SELECT COUNT(key) FROM DECIMAL_UDF;\n+\n DROP TABLE IF EXISTS DECIMAL_UDF;", "filename": "ql/src/test/queries/clientpositive/decimal_udf.q"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision.q.out", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision.q.out", "sha": "9eddd38c787ad9f8e9dd99f7831c797c7481db5a", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/decimal_precision.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -0,0 +1,16 @@\n+PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@DECIMAL_PRECISION\n+FAILED: SemanticException [Error 10029]: Line 3:13 Invalid numerical constant '123456789012345678901234567890.123456789bd'", "filename": "ql/src/test/results/clientnegative/decimal_precision.q.out"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision_1.q.out", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientnegative/decimal_precision_1.q.out", "sha": "21ef65fb27e05873e47c76f17f2f4fb77bc40925", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/decimal_precision_1.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -0,0 +1,16 @@\n+PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) \n+ROW FORMAT DELIMITED\n+   FIELDS TERMINATED BY ' '\n+STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@DECIMAL_PRECISION\n+FAILED: SemanticException [Error 10029]: Line 3:44 Invalid numerical constant '1234567890123456789.0123456789bd'", "filename": "ql/src/test/results/clientnegative/decimal_precision_1.q.out"}, {"additions": 44, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_precision.q.out", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_precision.q.out", "sha": "232dace12b8060579fedb025db9841d2569c2b39", "changes": 44, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_precision.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -514,6 +514,50 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@decimal_precision\n #### A masked pattern was here ####\n NULL\tNULL\n+PREHOOK: query: SELECT dec * cast('123456789012345678901234567890.123456789' as decimal) FROM DECIMAL_PRECISION LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT dec * cast('123456789012345678901234567890.123456789' as decimal) FROM DECIMAL_PRECISION LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+NULL\n+PREHOOK: query: SELECT * from DECIMAL_PRECISION WHERE dec > cast('123456789012345678901234567890.123456789' as decimal) LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT * from DECIMAL_PRECISION WHERE dec > cast('123456789012345678901234567890.123456789' as decimal) LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+PREHOOK: query: SELECT dec * 123456789012345678901234567890.123456789 FROM DECIMAL_PRECISION LIMIT 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT dec * 123456789012345678901234567890.123456789 FROM DECIMAL_PRECISION LIMIT 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+NULL\n+PREHOOK: query: SELECT MIN(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT MIN(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+NULL\n+PREHOOK: query: SELECT COUNT(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(cast('123456789012345678901234567890.123456789' as decimal)) FROM DECIMAL_PRECISION\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_precision\n+#### A masked pattern was here ####\n+0\n PREHOOK: query: DROP TABLE DECIMAL_PRECISION\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@decimal_precision", "filename": "ql/src/test/results/clientpositive/decimal_precision.q.out"}, {"additions": 210, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_udf.q.out", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/ql/src/test/results/clientpositive/decimal_udf.q.out", "sha": "52c8a81f7b8195b7de88b054bd0dd508d0eddde0", "changes": 210, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_udf.q.out?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -2482,6 +2482,216 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@decimal_udf\n #### A masked pattern was here ####\n [{\"x\":-1.2345678901234567E9,\"y\":1.0},{\"x\":-148.75058823529412,\"y\":34.0},{\"x\":1.2345678901234567E9,\"y\":1.0}]\n+PREHOOK: query: -- min\n+EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- min\n+EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME DECIMAL_UDF))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION MIN (TOK_TABLE_OR_COL key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        decimal_udf \n+          TableScan\n+            alias: decimal_udf\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: decimal\n+              outputColumnNames: key\n+              Group By Operator\n+                aggregations:\n+                      expr: min(key)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: decimal\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: min(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: decimal\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+-1234567890.123456789\n+PREHOOK: query: -- max\n+EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- max\n+EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME DECIMAL_UDF))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION MAX (TOK_TABLE_OR_COL key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        decimal_udf \n+          TableScan\n+            alias: decimal_udf\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: decimal\n+              outputColumnNames: key\n+              Group By Operator\n+                aggregations:\n+                      expr: max(key)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: decimal\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: max(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: decimal\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+1234567890.12345678\n+PREHOOK: query: -- count\n+EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- count\n+EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME DECIMAL_UDF))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION COUNT (TOK_TABLE_OR_COL key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        decimal_udf \n+          TableScan\n+            alias: decimal_udf\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: decimal\n+              outputColumnNames: key\n+              Group By Operator\n+                aggregations:\n+                      expr: count(key)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: bigint\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: count(VALUE._col0)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: bigint\n+            outputColumnNames: _col0\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@decimal_udf\n+#### A masked pattern was here ####\n+36\n PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@decimal_udf", "filename": "ql/src/test/results/clientpositive/decimal_udf.q.out"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java", "sha": "742493c82b454d0f35909dcfbc075d06ab1c329f", "changes": 10, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -276,10 +276,14 @@ public Object convert(Object input) {\n       if (input == null) {\n         return null;\n       }\n-      return outputOI.set(r, PrimitiveObjectInspectorUtils.getHiveDecimal(input,\n-          inputOI));\n+      \n+      try {\n+        return outputOI.set(r, PrimitiveObjectInspectorUtils.getHiveDecimal(input,\n+            inputOI));\n+      } catch (NumberFormatException e) {\n+        return null;\n+      }\n     }\n-\n   }\n \n   public static class BinaryConverter implements Converter{", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java"}, {"additions": 41, "raw_url": "https://github.com/apache/hive/raw/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java", "blob_url": "https://github.com/apache/hive/blob/c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java", "sha": "94062dd7812036224676767148b68239000af35b", "changes": 86, "status": "modified", "deletions": 45, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java?ref=c5ae5798f3741ecc494ff9d56b6ebcbcf23d6633", "patch": "@@ -778,52 +778,48 @@ public static HiveDecimal getHiveDecimal(Object o, PrimitiveObjectInspector oi)\n     }\n \n     HiveDecimal result = null;\n-    try {\n-      switch (oi.getPrimitiveCategory()) {\n-      case VOID:\n-        result = null;\n-        break;\n-      case BOOLEAN:\n-        result = ((BooleanObjectInspector) oi).get(o) ?\n-            HiveDecimal.ONE : HiveDecimal.ZERO;\n-        break;\n-      case BYTE:\n-        result = new HiveDecimal(((ByteObjectInspector) oi).get(o));\n-        break;\n-      case SHORT:\n-        result = new HiveDecimal(((ShortObjectInspector) oi).get(o));\n-        break;\n-      case INT:\n-        result = new HiveDecimal(((IntObjectInspector) oi).get(o));\n-        break;\n-      case LONG:\n-        result = new HiveDecimal(((LongObjectInspector) oi).get(o));\n-        break;\n-      case FLOAT:\n-        Float f = ((FloatObjectInspector) oi).get(o);\n-        result = new HiveDecimal(f.toString());\n-        break;\n-      case DOUBLE:\n-        Double d = ((DoubleObjectInspector) oi).get(o);\n-        result = new HiveDecimal(d.toString());\n-        break;\n-      case STRING:\n-        result = new HiveDecimal(((StringObjectInspector) oi).getPrimitiveJavaObject(o));\n-        break;\n-      case TIMESTAMP:\n-        Double ts = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o)\n+    switch (oi.getPrimitiveCategory()) {\n+    case VOID:\n+      result = null;\n+      break;\n+    case BOOLEAN:\n+      result = ((BooleanObjectInspector) oi).get(o) ?\n+        HiveDecimal.ONE : HiveDecimal.ZERO;\n+      break;\n+    case BYTE:\n+      result = new HiveDecimal(((ByteObjectInspector) oi).get(o));\n+      break;\n+    case SHORT:\n+      result = new HiveDecimal(((ShortObjectInspector) oi).get(o));\n+      break;\n+    case INT:\n+      result = new HiveDecimal(((IntObjectInspector) oi).get(o));\n+      break;\n+    case LONG:\n+      result = new HiveDecimal(((LongObjectInspector) oi).get(o));\n+      break;\n+    case FLOAT:\n+      Float f = ((FloatObjectInspector) oi).get(o);\n+      result = new HiveDecimal(f.toString());\n+      break;\n+    case DOUBLE:\n+      Double d = ((DoubleObjectInspector) oi).get(o);\n+      result = new HiveDecimal(d.toString());\n+      break;\n+    case STRING:\n+      result = new HiveDecimal(((StringObjectInspector) oi).getPrimitiveJavaObject(o));\n+      break;\n+    case TIMESTAMP:\n+      Double ts = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o)\n         .getDouble();\n-        result = new HiveDecimal(ts.toString());\n-        break;\n-      case DECIMAL:\n-        result = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);\n-        break;\n-      default:\n-        throw new RuntimeException(\"Hive 2 Internal error: unknown type: \"\n-            + oi.getTypeName());\n-      }\n-    } catch(NumberFormatException e) {\n-      // return null\n+      result = new HiveDecimal(ts.toString());\n+      break;\n+    case DECIMAL:\n+      result = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);\n+      break;\n+    default:\n+      throw new RuntimeException(\"Hive 2 Internal error: unknown type: \"\n+                                 + oi.getTypeName());\n     }\n     return result;\n   }", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d74c67e45b312f00770df7cede81b525b08e68c8", "parent": "https://github.com/apache/hive/commit/8ab7cae4aac8933def4b6cf55f3b144f3ec99e84", "message": "HIVE-4119. ANALYZE TABLE ... COMPUTE STATISTICS FOR COLUMNS fails with NPE if the table is empty (Shreepadma Venugopalan via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1464208 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_257", "file": [{"additions": 39, "raw_url": "https://github.com/apache/hive/raw/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java", "blob_url": "https://github.com/apache/hive/blob/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java", "sha": "5199071d5e831cf9df70d73424318d850205291e", "changes": 56, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java?ref=d74c67e45b312f00770df7cede81b525b08e68c8", "patch": "@@ -61,6 +61,8 @@\n public class ColumnStatsTask extends Task<ColumnStatsWork> implements Serializable {\n   private static final long serialVersionUID = 1L;\n   private FetchOperator ftOp;\n+  private int totalRows;\n+  private int numRows = 0;\n   private static transient final Log LOG = LogFactory.getLog(ColumnStatsTask.class);\n \n   public ColumnStatsTask() {\n@@ -70,6 +72,7 @@ public ColumnStatsTask() {\n   @Override\n   public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {\n     super.initialize(conf, queryPlan, ctx);\n+    work.initializeForFetch();\n     try {\n       JobConf job = new JobConf(conf, ExecDriver.class);\n       ftOp = new FetchOperator(work.getfWork(), job);\n@@ -294,14 +297,16 @@ private int persistPartitionStats() throws HiveException {\n       e.printStackTrace();\n     }\n \n-    // Construct a column statistics object from the result\n-    ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);\n+    if (io != null) {\n+      // Construct a column statistics object from the result\n+      ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);\n \n-    // Persist the column statistics object to the metastore\n-    try {\n-      db.updatePartitionColumnStatistics(colStats);\n-    } catch (Exception e) {\n-      e.printStackTrace();\n+      // Persist the column statistics object to the metastore\n+      try {\n+        db.updatePartitionColumnStatistics(colStats);\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }\n     }\n     return 0;\n   }\n@@ -317,14 +322,16 @@ private int persistTableStats() throws HiveException {\n       e.printStackTrace();\n     }\n \n-    // Construct a column statistics object from the result\n-    ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);\n+    if (io != null) {\n+      // Construct a column statistics object from the result\n+      ColumnStatistics colStats = constructColumnStatsFromPackedRow(io.oi, io.o);\n \n-    // Persist the column statistics object to the metastore\n-    try {\n-      db.updateTableColumnStatistics(colStats);\n-    } catch (Exception e) {\n-      e.printStackTrace();\n+      // Persist the column statistics object to the metastore\n+      try {\n+        db.updateTableColumnStatistics(colStats);\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }\n     }\n     return 0;\n   }\n@@ -344,10 +351,25 @@ public int execute(DriverContext driverContext) {\n   }\n \n   private InspectableObject fetchColumnStats() throws IOException, CommandNeedRetryException {\n+    InspectableObject io = null;\n+\n     try {\n-      InspectableObject io = ftOp.getNextRow();\n-      if (io == null) {\n-        throw new CommandNeedRetryException();\n+      int rowsRet = work.getLeastNumRows();\n+      if (rowsRet <= 0) {\n+        rowsRet = ColumnStatsWork.getLimit() >= 0 ?\n+            Math.min(ColumnStatsWork.getLimit() - totalRows, 1) : 1;\n+      }\n+      if (rowsRet <= 0) {\n+        ftOp.clearFetchContext();\n+        return null;\n+      }\n+      while (numRows < rowsRet) {\n+        if ((io = ftOp.getNextRow()) == null) {\n+          if (work.getLeastNumRows() > 0) {\n+            throw new CommandNeedRetryException();\n+          }\n+        }\n+        numRows++;\n       }\n       return io;\n     } catch (CommandNeedRetryException e) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsTask.java"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java", "blob_url": "https://github.com/apache/hive/blob/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java", "sha": "3cae7273a73cfe26f770eb08f8cfca8c81add4e3", "changes": 21, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java?ref=d74c67e45b312f00770df7cede81b525b08e68c8", "patch": "@@ -20,6 +20,8 @@\n \n import java.io.Serializable;\n \n+import org.apache.hadoop.hive.ql.exec.ListSinkOperator;\n+\n /**\n  * ColumnStats Work.\n  *\n@@ -29,6 +31,8 @@\n   private static final long serialVersionUID = 1L;\n   private FetchWork fWork;\n   private ColumnStatsDesc colStats;\n+  private static final int LIMIT = -1;\n+\n \n   public ColumnStatsWork() {\n   }\n@@ -61,4 +65,21 @@ public ColumnStatsDesc getColStats() {\n   public void setColStats(ColumnStatsDesc colStats) {\n     this.colStats = colStats;\n   }\n+\n+  public ListSinkOperator getSink() {\n+    return fWork.getSink();\n+  }\n+\n+  public void initializeForFetch() {\n+    fWork.initializeForFetch();\n+  }\n+\n+  public int getLeastNumRows() {\n+    return fWork.getLeastNumRows();\n+  }\n+\n+  public static int getLimit() {\n+    return LIMIT;\n+  }\n+\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ColumnStatsWork.java"}, {"additions": 179, "raw_url": "https://github.com/apache/hive/raw/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java", "blob_url": "https://github.com/apache/hive/blob/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java", "sha": "f97e460116d74d0b2e4a7d4203c4ab05379a1273", "changes": 285, "status": "modified", "deletions": 106, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java?ref=d74c67e45b312f00770df7cede81b525b08e68c8", "patch": "@@ -229,24 +229,32 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {\n     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n       Object p = parameters[0];\n       BooleanStatsAgg myagg = (BooleanStatsAgg) agg;\n-      if (p == null) {\n-        myagg.countNulls++;\n+      boolean emptyTable = false;\n+\n+      if (parameters[1] == null) {\n+        emptyTable = true;\n       }\n-      else {\n-        try {\n-          boolean v = PrimitiveObjectInspectorUtils.getBoolean(p, inputOI);\n-          if (v == false) {\n-            myagg.countFalses++;\n-          } else if (v == true){\n-            myagg.countTrues++;\n-          }\n-        } catch (NumberFormatException e) {\n-          if (!warned) {\n-            warned = true;\n-            LOG.warn(getClass().getSimpleName() + \" \"\n-                + StringUtils.stringifyException(e));\n-            LOG.warn(getClass().getSimpleName()\n-                + \" ignoring similar exceptions.\");\n+\n+      if (!emptyTable) {\n+        if (p == null) {\n+          myagg.countNulls++;\n+        }\n+        else {\n+          try {\n+            boolean v = PrimitiveObjectInspectorUtils.getBoolean(p, inputOI);\n+            if (v == false) {\n+              myagg.countFalses++;\n+            } else if (v == true){\n+              myagg.countTrues++;\n+            }\n+          } catch (NumberFormatException e) {\n+            if (!warned) {\n+              warned = true;\n+              LOG.warn(getClass().getSimpleName() + \" \"\n+                  + StringUtils.stringifyException(e));\n+              LOG.warn(getClass().getSimpleName()\n+                  + \" ignoring similar exceptions.\");\n+            }\n           }\n         }\n       }\n@@ -472,14 +480,24 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {\n     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n       Object p = parameters[0];\n       LongStatsAgg myagg = (LongStatsAgg) agg;\n+      boolean emptyTable = false;\n+\n+      if (parameters[1] == null) {\n+        emptyTable = true;\n+      }\n \n       if (myagg.firstItem) {\n-        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);\n+        int numVectors = 0;\n+        if (!emptyTable) {\n+          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);\n+        }\n         initNDVEstimator(myagg, numVectors);\n         myagg.firstItem = false;\n         myagg.numBitVectors = numVectors;\n       }\n \n+      if (!emptyTable) {\n+\n       //Update null counter if a null value is seen\n       if (p == null) {\n         myagg.countNulls++;\n@@ -511,6 +529,7 @@ public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveExcep\n           }\n         }\n       }\n+      }\n     }\n \n     @Override\n@@ -572,7 +591,11 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n     @Override\n     public Object terminate(AggregationBuffer agg) throws HiveException {\n       LongStatsAgg myagg = (LongStatsAgg) agg;\n-      long numDV = myagg.numDV.estimateNumDistinctValues();\n+\n+      long numDV = 0;\n+      if (myagg.numBitVectors != 0) {\n+        numDV = myagg.numDV.estimateNumDistinctValues();\n+      }\n \n       // Serialize the result struct\n       ((Text) result[0]).set(myagg.columnType);\n@@ -770,42 +793,54 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {\n     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n       Object p = parameters[0];\n       DoubleStatsAgg myagg = (DoubleStatsAgg) agg;\n+      boolean emptyTable = false;\n+\n+      if (parameters[1] == null) {\n+        emptyTable = true;\n+      }\n \n       if (myagg.firstItem) {\n-        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);\n+        int numVectors = 0;\n+        if (!emptyTable) {\n+          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);\n+        }\n         initNDVEstimator(myagg, numVectors);\n         myagg.firstItem = false;\n         myagg.numBitVectors = numVectors;\n       }\n \n-      //Update null counter if a null value is seen\n-      if (p == null) {\n-        myagg.countNulls++;\n-      }\n-      else {\n-        try {\n-          double v = PrimitiveObjectInspectorUtils.getDouble(p, inputOI);\n+      if (!emptyTable) {\n \n-          //Update min counter if new value is less than min seen so far\n-          if (v < myagg.min) {\n-            myagg.min = v;\n-          }\n-\n-          //Update max counter if new value is greater than max seen so far\n-          if (v > myagg.max) {\n-            myagg.max = v;\n-          }\n-\n-          // Add value to NumDistinctValue Estimator\n-          myagg.numDV.addToEstimator(v);\n-\n-        } catch (NumberFormatException e) {\n-          if (!warned) {\n-            warned = true;\n-            LOG.warn(getClass().getSimpleName() + \" \"\n-                + StringUtils.stringifyException(e));\n-            LOG.warn(getClass().getSimpleName()\n-                + \" ignoring similar exceptions.\");\n+        //Update null counter if a null value is seen\n+        if (p == null) {\n+          myagg.countNulls++;\n+        }\n+        else {\n+          try {\n+\n+            double v = PrimitiveObjectInspectorUtils.getDouble(p, inputOI);\n+\n+            //Update min counter if new value is less than min seen so far\n+            if (v < myagg.min) {\n+              myagg.min = v;\n+            }\n+\n+            //Update max counter if new value is greater than max seen so far\n+            if (v > myagg.max) {\n+              myagg.max = v;\n+            }\n+\n+            // Add value to NumDistinctValue Estimator\n+            myagg.numDV.addToEstimator(v);\n+\n+          } catch (NumberFormatException e) {\n+            if (!warned) {\n+              warned = true;\n+              LOG.warn(getClass().getSimpleName() + \" \"\n+                  + StringUtils.stringifyException(e));\n+              LOG.warn(getClass().getSimpleName()\n+                  + \" ignoring similar exceptions.\");\n+            }\n           }\n         }\n       }\n@@ -870,7 +905,11 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n     @Override\n     public Object terminate(AggregationBuffer agg) throws HiveException {\n       DoubleStatsAgg myagg = (DoubleStatsAgg) agg;\n-      long numDV = myagg.numDV.estimateNumDistinctValues();\n+      long numDV = 0;\n+\n+      if (myagg.numBitVectors != 0) {\n+        numDV = myagg.numDV.estimateNumDistinctValues();\n+      }\n \n       // Serialize the result struct\n       ((Text) result[0]).set(myagg.columnType);\n@@ -1082,44 +1121,56 @@ private void printDebugOutput(String functionName, AggregationBuffer agg) {\n     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n       Object p = parameters[0];\n       StringStatsAgg myagg = (StringStatsAgg) agg;\n+      boolean emptyTable = false;\n+\n+      if (parameters[1] == null) {\n+        emptyTable = true;\n+      }\n \n       if (myagg.firstItem) {\n-        int numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);\n+        int numVectors = 0;\n+        if (!emptyTable) {\n+          numVectors = PrimitiveObjectInspectorUtils.getInt(parameters[1], numVectorsOI);\n+        }\n         initNDVEstimator(myagg, numVectors);\n         myagg.firstItem = false;\n         myagg.numBitVectors = numVectors;\n       }\n \n-      // Update null counter if a null value is seen\n-      if (p == null) {\n-        myagg.countNulls++;\n-      }\n-      else {\n-        try {\n-          String v = PrimitiveObjectInspectorUtils.getString(p, inputOI);\n+      if (!emptyTable) {\n \n-          // Update max length if new length is greater than the ones seen so far\n-          int len = v.length();\n-          if (len > myagg.maxLength) {\n-            myagg.maxLength = len;\n-          }\n-\n-          // Update sum length with the new length\n-          myagg.sumLength += len;\n-\n-          // Increment count of values seen so far\n-          myagg.count++;\n-\n-          // Add string value to NumDistinctValue Estimator\n-          myagg.numDV.addToEstimator(v);\n-\n-        } catch (NumberFormatException e) {\n-          if (!warned) {\n-            warned = true;\n-            LOG.warn(getClass().getSimpleName() + \" \"\n-                + StringUtils.stringifyException(e));\n-            LOG.warn(getClass().getSimpleName()\n-                + \" ignoring similar exceptions.\");\n+        // Update null counter if a null value is seen\n+        if (p == null) {\n+          myagg.countNulls++;\n+        }\n+        else {\n+          try {\n+\n+            String v = PrimitiveObjectInspectorUtils.getString(p, inputOI);\n+\n+            // Update max length if new length is greater than the ones seen so far\n+            int len = v.length();\n+            if (len > myagg.maxLength) {\n+              myagg.maxLength = len;\n+            }\n+\n+            // Update sum length with the new length\n+            myagg.sumLength += len;\n+\n+            // Increment count of values seen so far\n+            myagg.count++;\n+\n+            // Add string value to NumDistinctValue Estimator\n+            myagg.numDV.addToEstimator(v);\n+\n+          } catch (NumberFormatException e) {\n+            if (!warned) {\n+              warned = true;\n+              LOG.warn(getClass().getSimpleName() + \" \"\n+                  + StringUtils.stringifyException(e));\n+              LOG.warn(getClass().getSimpleName()\n+                  + \" ignoring similar exceptions.\");\n+            }\n           }\n         }\n       }\n@@ -1186,8 +1237,18 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n     @Override\n     public Object terminate(AggregationBuffer agg) throws HiveException {\n       StringStatsAgg myagg = (StringStatsAgg) agg;\n-      long numDV = myagg.numDV.estimateNumDistinctValues();\n-      double avgLength = (double)(myagg.sumLength/(1.0 * (myagg.count + myagg.countNulls)));\n+\n+      long numDV = 0;\n+      double avgLength = 0.0;\n+      long total = myagg.count + myagg.countNulls;\n+\n+      if (myagg.numBitVectors != 0) {\n+        numDV = myagg.numDV.estimateNumDistinctValues();\n+      }\n+\n+      if (total != 0) {\n+         avgLength = (double)(myagg.sumLength / (1.0 * total));\n+      }\n \n       // Serialize the result struct\n       ((Text) result[0]).set(myagg.columnType);\n@@ -1347,34 +1408,41 @@ public void reset(AggregationBuffer agg) throws HiveException {\n     public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n       Object p = parameters[0];\n       BinaryStatsAgg myagg = (BinaryStatsAgg) agg;\n+      boolean emptyTable = false;\n \n-      // Update null counter if a null value is seen\n-      if (p == null) {\n-        myagg.countNulls++;\n+      if (parameters[1] == null) {\n+        emptyTable = true;\n       }\n-      else {\n-        try {\n-          BytesWritable v = PrimitiveObjectInspectorUtils.getBinary(p, inputOI);\n-\n-          // Update max length if new length is greater than the ones seen so far\n-          int len = v.getLength();\n-          if (len > myagg.maxLength) {\n-            myagg.maxLength = len;\n-          }\n \n-          // Update sum length with the new length\n-          myagg.sumLength += len;\n-\n-          // Increment count of values seen so far\n-          myagg.count++;\n-\n-        } catch (NumberFormatException e) {\n-          if (!warned) {\n-            warned = true;\n-            LOG.warn(getClass().getSimpleName() + \" \"\n-                + StringUtils.stringifyException(e));\n-            LOG.warn(getClass().getSimpleName()\n-                + \" ignoring similar exceptions.\");\n+      if (!emptyTable) {\n+        // Update null counter if a null value is seen\n+        if (p == null) {\n+          myagg.countNulls++;\n+        }\n+        else {\n+          try {\n+            BytesWritable v = PrimitiveObjectInspectorUtils.getBinary(p, inputOI);\n+\n+            // Update max length if new length is greater than the ones seen so far\n+            int len = v.getLength();\n+            if (len > myagg.maxLength) {\n+              myagg.maxLength = len;\n+            }\n+\n+            // Update sum length with the new length\n+            myagg.sumLength += len;\n+\n+            // Increment count of values seen so far\n+            myagg.count++;\n+\n+          } catch (NumberFormatException e) {\n+            if (!warned) {\n+              warned = true;\n+              LOG.warn(getClass().getSimpleName() + \" \"\n+                  + StringUtils.stringifyException(e));\n+              LOG.warn(getClass().getSimpleName()\n+                  + \" ignoring similar exceptions.\");\n+            }\n           }\n         }\n       }\n@@ -1440,7 +1508,12 @@ public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n     @Override\n     public Object terminate(AggregationBuffer agg) throws HiveException {\n       BinaryStatsAgg myagg = (BinaryStatsAgg) agg;\n-      double avgLength = (double)(myagg.sumLength/(1.0 * (myagg.count + myagg.countNulls)));\n+      double avgLength = 0.0;\n+      long count = myagg.count + myagg.countNulls;\n+\n+      if (count != 0) {\n+        avgLength = (double)(myagg.sumLength / (1.0 * (myagg.count + myagg.countNulls)));\n+      }\n \n       // Serialize the result struct\n       ((Text) result[0]).set(myagg.columnType);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java"}, {"additions": 13, "raw_url": "https://github.com/apache/hive/raw/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/queries/clientpositive/columnstats_tbllvl.q", "blob_url": "https://github.com/apache/hive/blob/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/queries/clientpositive/columnstats_tbllvl.q", "sha": "72d88a67b521233f2268a033221e1842c5c8662e", "changes": 13, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/columnstats_tbllvl.q?ref=d74c67e45b312f00770df7cede81b525b08e68c8", "patch": "@@ -23,3 +23,16 @@ analyze table UserVisits_web_text_none compute statistics for columns sourceIP,\n \n analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;\n \n+CREATE TABLE empty_tab(\n+   a int,\n+   b double,\n+   c string, \n+   d boolean,\n+   e binary)\n+row format delimited fields terminated by '|'  stored as textfile;\n+\n+explain \n+analyze table empty_tab compute statistics for columns a,b,c,d,e;\n+\n+analyze table empty_tab compute statistics for columns a,b,c,d,e;\n+", "filename": "ql/src/test/queries/clientpositive/columnstats_tbllvl.q"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/queries/clientpositive/compute_stats_empty_table.q", "blob_url": "https://github.com/apache/hive/blob/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/queries/clientpositive/compute_stats_empty_table.q", "sha": "d4ed93fa6398706ac524897eaefa8c50fa36aa68", "changes": 12, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/compute_stats_empty_table.q?ref=d74c67e45b312f00770df7cede81b525b08e68c8", "patch": "@@ -0,0 +1,12 @@\n+create table tab_empty(a boolean, b int, c double, d string, e binary);\n+\n+select count(*) from tab_empty;\n+\n+-- compute statistical summary of data\n+select compute_stats(a, 16) from tab_empty;\n+select compute_stats(b, 16) from tab_empty;\n+select compute_stats(c, 16) from tab_empty;\n+select compute_stats(d, 16) from tab_empty;\n+select compute_stats(e, 16) from tab_empty;\n+\n+", "filename": "ql/src/test/queries/clientpositive/compute_stats_empty_table.q"}, {"additions": 121, "raw_url": "https://github.com/apache/hive/raw/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out", "blob_url": "https://github.com/apache/hive/blob/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out", "sha": "007bc31c066e531451d6f6cea3532745d22f6414", "changes": 121, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/columnstats_tbllvl.q.out?ref=d74c67e45b312f00770df7cede81b525b08e68c8", "patch": "@@ -264,3 +264,124 @@ POSTHOOK: query: analyze table UserVisits_web_text_none compute statistics for c\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@uservisits_web_text_none\n #### A masked pattern was here ####\n+PREHOOK: query: CREATE TABLE empty_tab(\n+   a int,\n+   b double,\n+   c string, \n+   d boolean,\n+   e binary)\n+row format delimited fields terminated by '|'  stored as textfile\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE empty_tab(\n+   a int,\n+   b double,\n+   c string, \n+   d boolean,\n+   e binary)\n+row format delimited fields terminated by '|'  stored as textfile\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@empty_tab\n+PREHOOK: query: explain \n+analyze table empty_tab compute statistics for columns a,b,c,d,e\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain \n+analyze table empty_tab compute statistics for columns a,b,c,d,e\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_ANALYZE (TOK_TAB (TOK_TABNAME empty_tab)) (TOK_TABCOLNAME a b c d e))\n+\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+  Stage-1 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        empty_tab \n+          TableScan\n+            alias: empty_tab\n+            Select Operator\n+              expressions:\n+                    expr: a\n+                    type: int\n+                    expr: b\n+                    type: double\n+                    expr: c\n+                    type: string\n+                    expr: d\n+                    type: boolean\n+                    expr: e\n+                    type: binary\n+              outputColumnNames: a, b, c, d, e\n+              Group By Operator\n+                aggregations:\n+                      expr: compute_stats(a, 16)\n+                      expr: compute_stats(b, 16)\n+                      expr: compute_stats(c, 16)\n+                      expr: compute_stats(d, 16)\n+                      expr: compute_stats(e, 16)\n+                bucketGroup: false\n+                mode: hash\n+                outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+                Reduce Output Operator\n+                  sort order: \n+                  tag: -1\n+                  value expressions:\n+                        expr: _col0\n+                        type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>\n+                        expr: _col1\n+                        type: struct<columntype:string,min:double,max:double,countnulls:bigint,bitvector:string,numbitvectors:int>\n+                        expr: _col2\n+                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>\n+                        expr: _col3\n+                        type: struct<columntype:string,counttrues:bigint,countfalses:bigint,countnulls:bigint>\n+                        expr: _col4\n+                        type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint>\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations:\n+                expr: compute_stats(VALUE._col0)\n+                expr: compute_stats(VALUE._col1)\n+                expr: compute_stats(VALUE._col2)\n+                expr: compute_stats(VALUE._col3)\n+                expr: compute_stats(VALUE._col4)\n+          bucketGroup: false\n+          mode: mergepartial\n+          outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint>\n+                  expr: _col1\n+                  type: struct<columntype:string,min:double,max:double,countnulls:bigint,numdistinctvalues:bigint>\n+                  expr: _col2\n+                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint,numdistinctvalues:bigint>\n+                  expr: _col3\n+                  type: struct<columntype:string,counttrues:bigint,countfalses:bigint,countnulls:bigint>\n+                  expr: _col4\n+                  type: struct<columntype:string,maxlength:bigint,avglength:double,countnulls:bigint>\n+            outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-1\n+    Column Stats Work\n+      Column Stats Desc:\n+          Columns: a, b, c, d, e\n+          Column Types: int, double, string, boolean, binary\n+          Table: empty_tab\n+\n+\n+PREHOOK: query: analyze table empty_tab compute statistics for columns a,b,c,d,e\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@empty_tab\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table empty_tab compute statistics for columns a,b,c,d,e\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@empty_tab\n+#### A masked pattern was here ####", "filename": "ql/src/test/results/clientpositive/columnstats_tbllvl.q.out"}, {"additions": 61, "raw_url": "https://github.com/apache/hive/raw/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/results/clientpositive/compute_stats_empty_table.q.out", "blob_url": "https://github.com/apache/hive/blob/d74c67e45b312f00770df7cede81b525b08e68c8/ql/src/test/results/clientpositive/compute_stats_empty_table.q.out", "sha": "8dffaf3db5c70de1c16193418a6d938b5969eb6a", "changes": 61, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/compute_stats_empty_table.q.out?ref=d74c67e45b312f00770df7cede81b525b08e68c8", "patch": "@@ -0,0 +1,61 @@\n+PREHOOK: query: create table tab_empty(a boolean, b int, c double, d string, e binary)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table tab_empty(a boolean, b int, c double, d string, e binary)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@tab_empty\n+PREHOOK: query: select count(*) from tab_empty\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from tab_empty\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: -- compute statistical summary of data\n+select compute_stats(a, 16) from tab_empty\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- compute statistical summary of data\n+select compute_stats(a, 16) from tab_empty\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+{\"columntype\":\"Boolean\",\"counttrues\":0,\"countfalses\":0,\"countnulls\":0}\n+PREHOOK: query: select compute_stats(b, 16) from tab_empty\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select compute_stats(b, 16) from tab_empty\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+{\"columntype\":\"Long\",\"min\":0,\"max\":0,\"countnulls\":0,\"numdistinctvalues\":0}\n+PREHOOK: query: select compute_stats(c, 16) from tab_empty\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select compute_stats(c, 16) from tab_empty\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+{\"columntype\":\"Double\",\"min\":0.0,\"max\":0.0,\"countnulls\":0,\"numdistinctvalues\":0}\n+PREHOOK: query: select compute_stats(d, 16) from tab_empty\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select compute_stats(d, 16) from tab_empty\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+{\"columntype\":\"String\",\"maxlength\":0,\"avglength\":0.0,\"countnulls\":0,\"numdistinctvalues\":0}\n+PREHOOK: query: select compute_stats(e, 16) from tab_empty\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select compute_stats(e, 16) from tab_empty\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@tab_empty\n+#### A masked pattern was here ####\n+{\"columntype\":\"Binary\",\"maxlength\":0,\"avglength\":0.0,\"countnulls\":0}", "filename": "ql/src/test/results/clientpositive/compute_stats_empty_table.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4", "parent": "https://github.com/apache/hive/commit/a525b3e13b2abc6f9b7ee009c6b424cb107405f9", "message": "HIVE-4154 NPE reading column of empty string from ORC file\n(Kevin Wilfong via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1458570 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_258", "file": [{"additions": 16, "raw_url": "https://github.com/apache/hive/raw/7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "sha": "d5ad8f88cd73564dfdaf4d1ab8f53b12cdc51900", "changes": 26, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4", "patch": "@@ -17,6 +17,15 @@\n  */\n package org.apache.hadoop.hive.ql.io.orc;\n \n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.sql.Timestamp;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -30,15 +39,6 @@\n import org.apache.hadoop.io.LongWritable;\n import org.apache.hadoop.io.Text;\n \n-import java.io.EOFException;\n-import java.io.IOException;\n-import java.nio.ByteBuffer;\n-import java.sql.Timestamp;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n class RecordReaderImpl implements RecordReader {\n   private final FSDataInputStream file;\n   private final long firstRow;\n@@ -686,7 +686,13 @@ Object next(Object previous) throws IOException {\n         } else {\n           length = dictionaryBuffer.size() - offset;\n         }\n-        dictionaryBuffer.setText(result, offset, length);\n+        // If the column is just empty strings, the size will be zero, so the buffer will be null,\n+        // in that case just return result as it will default to empty\n+        if (dictionaryBuffer != null) {\n+          dictionaryBuffer.setText(result, offset, length);\n+        } else {\n+          result.clear();\n+        }\n       }\n       return result;\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4/ql/src/test/queries/clientpositive/orc_empty_strings.q", "blob_url": "https://github.com/apache/hive/blob/7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4/ql/src/test/queries/clientpositive/orc_empty_strings.q", "sha": "6489d9421a92765e43e828ff2484b131104bce49", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_empty_strings.q?ref=7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4", "patch": "@@ -0,0 +1,16 @@\n+CREATE TABLE test_orc (key STRING)\n+ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' \n+STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' \n+OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';\n+\n+INSERT OVERWRITE TABLE test_orc SELECT '' FROM src limit 10;\n+\n+-- Test reading a column which is just empty strings\n+\n+SELECT * FROM test_orc; \n+\n+INSERT OVERWRITE TABLE test_orc SELECT IF (key % 3 = 0, key, '') FROM src limit 10;\n+\n+-- Test reading a column which has some empty strings\n+\n+SELECT * FROM test_orc;", "filename": "ql/src/test/queries/clientpositive/orc_empty_strings.q"}, {"additions": 77, "raw_url": "https://github.com/apache/hive/raw/7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4/ql/src/test/results/clientpositive/orc_empty_strings.q.out", "blob_url": "https://github.com/apache/hive/blob/7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4/ql/src/test/results/clientpositive/orc_empty_strings.q.out", "sha": "adc452bbf544f406bcd5730b6f4f88a8319ccd55", "changes": 77, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_empty_strings.q.out?ref=7b1167e6fe9212e30ee0317d6acbe6a8c784d5d4", "patch": "@@ -0,0 +1,77 @@\n+PREHOOK: query: CREATE TABLE test_orc (key STRING)\n+ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' \n+STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' \n+OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE test_orc (key STRING)\n+ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' \n+STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' \n+OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@test_orc\n+PREHOOK: query: INSERT OVERWRITE TABLE test_orc SELECT '' FROM src limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@test_orc\n+POSTHOOK: query: INSERT OVERWRITE TABLE test_orc SELECT '' FROM src limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@test_orc\n+POSTHOOK: Lineage: test_orc.key SIMPLE []\n+PREHOOK: query: -- Test reading a column which is just empty strings\n+\n+SELECT * FROM test_orc\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_orc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- Test reading a column which is just empty strings\n+\n+SELECT * FROM test_orc\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_orc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: test_orc.key SIMPLE []\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+PREHOOK: query: INSERT OVERWRITE TABLE test_orc SELECT IF (key % 3 = 0, key, '') FROM src limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@test_orc\n+POSTHOOK: query: INSERT OVERWRITE TABLE test_orc SELECT IF (key % 3 = 0, key, '') FROM src limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@test_orc\n+POSTHOOK: Lineage: test_orc.key SIMPLE []\n+POSTHOOK: Lineage: test_orc.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+PREHOOK: query: -- Test reading a column which has some empty strings\n+\n+SELECT * FROM test_orc\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_orc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- Test reading a column which has some empty strings\n+\n+SELECT * FROM test_orc\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_orc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: test_orc.key SIMPLE []\n+POSTHOOK: Lineage: test_orc.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+\n+\n+\n+27\n+165\n+\n+255\n+\n+\n+", "filename": "ql/src/test/results/clientpositive/orc_empty_strings.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/650c6365cb9c03b2c48c2efc8185a5a293488d73", "parent": "https://github.com/apache/hive/commit/0be7d2feaee56e4673ab5003fccdcb4bc04f2c21", "message": "HIVE-4079 Altering a view partition fails with NPE\n(Kevin Wilfong via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1451173 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_259", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "blob_url": "https://github.com/apache/hive/blob/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java", "sha": "94a0a7f2291bf16ccee7aae3d281b2dc676749ce", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=650c6365cb9c03b2c48c2efc8185a5a293488d73", "patch": "@@ -2029,7 +2029,9 @@ private void alterPartitionNoTxn(String dbname, String name, List<String> part_v\n     oldp.setValues(newp.getValues());\n     oldp.setPartitionName(newp.getPartitionName());\n     oldp.setParameters(newPart.getParameters());\n-    copyMSD(newp.getSd(), oldp.getSd());\n+    if (!TableType.VIRTUAL_VIEW.name().equals(oldp.getTable().getTableType())) {\n+      copyMSD(newp.getSd(), oldp.getSd());\n+    }\n     if (newp.getCreateTime() != oldp.getCreateTime()) {\n       oldp.setCreateTime(newp.getCreateTime());\n     }", "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java"}, {"additions": 100, "raw_url": "https://github.com/apache/hive/raw/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java", "blob_url": "https://github.com/apache/hive/blob/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java", "sha": "b6bce7686e0d3b74dd43c54904fa1fea1e3b7d7e", "changes": 100, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java?ref=650c6365cb9c03b2c48c2efc8185a5a293488d73", "patch": "@@ -586,6 +586,106 @@ public void testDropTable() throws Throwable {\n \n   }\n \n+  public void testAlterViewParititon() throws Throwable {\n+    String dbName = \"compdb\";\n+    String tblName = \"comptbl\";\n+    String viewName = \"compView\";\n+\n+    client.dropTable(dbName, tblName);\n+    silentDropDatabase(dbName);\n+    Database db = new Database();\n+    db.setName(dbName);\n+    db.setDescription(\"Alter Partition Test database\");\n+    client.createDatabase(db);\n+\n+    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);\n+    cols.add(new FieldSchema(\"name\", serdeConstants.STRING_TYPE_NAME, \"\"));\n+    cols.add(new FieldSchema(\"income\", serdeConstants.INT_TYPE_NAME, \"\"));\n+\n+    Table tbl = new Table();\n+    tbl.setDbName(dbName);\n+    tbl.setTableName(tblName);\n+    StorageDescriptor sd = new StorageDescriptor();\n+    tbl.setSd(sd);\n+    sd.setCols(cols);\n+    sd.setCompressed(false);\n+    sd.setParameters(new HashMap<String, String>());\n+    sd.setSerdeInfo(new SerDeInfo());\n+    sd.getSerdeInfo().setName(tbl.getTableName());\n+    sd.getSerdeInfo().setParameters(new HashMap<String, String>());\n+    sd.getSerdeInfo().getParameters()\n+        .put(serdeConstants.SERIALIZATION_FORMAT, \"1\");\n+    sd.setSortCols(new ArrayList<Order>());\n+\n+    client.createTable(tbl);\n+\n+    if (isThriftClient) {\n+      // the createTable() above does not update the location in the 'tbl'\n+      // object when the client is a thrift client and the code below relies\n+      // on the location being present in the 'tbl' object - so get the table\n+      // from the metastore\n+      tbl = client.getTable(dbName, tblName);\n+    }\n+\n+    ArrayList<FieldSchema> viewCols = new ArrayList<FieldSchema>(1);\n+    viewCols.add(new FieldSchema(\"income\", serdeConstants.INT_TYPE_NAME, \"\"));\n+\n+    ArrayList<FieldSchema> viewPartitionCols = new ArrayList<FieldSchema>(1);\n+    viewPartitionCols.add(new FieldSchema(\"name\", serdeConstants.STRING_TYPE_NAME, \"\"));\n+\n+    Table view = new Table();\n+    view.setDbName(dbName);\n+    view.setTableName(viewName);\n+    view.setTableType(TableType.VIRTUAL_VIEW.name());\n+    view.setPartitionKeys(viewPartitionCols);\n+    view.setViewOriginalText(\"SELECT income, name FROM \" + tblName);\n+    view.setViewExpandedText(\"SELECT `\" + tblName + \"`.`income`, `\" + tblName +\n+        \"`.`name` FROM `\" + dbName + \"`.`\" + tblName + \"`\");\n+    StorageDescriptor viewSd = new StorageDescriptor();\n+    view.setSd(viewSd);\n+    viewSd.setCols(viewCols);\n+    viewSd.setCompressed(false);\n+    viewSd.setParameters(new HashMap<String, String>());\n+    viewSd.setSerdeInfo(new SerDeInfo());\n+    viewSd.getSerdeInfo().setParameters(new HashMap<String, String>());\n+\n+    client.createTable(view);\n+\n+    if (isThriftClient) {\n+      // the createTable() above does not update the location in the 'tbl'\n+      // object when the client is a thrift client and the code below relies\n+      // on the location being present in the 'tbl' object - so get the table\n+      // from the metastore\n+      view = client.getTable(dbName, viewName);\n+    }\n+\n+    List<String> vals = new ArrayList<String>(1);\n+    vals.add(\"abc\");\n+\n+    Partition part = new Partition();\n+    part.setDbName(dbName);\n+    part.setTableName(viewName);\n+    part.setValues(vals);\n+    part.setParameters(new HashMap<String, String>());\n+\n+    client.add_partition(part);\n+\n+    Partition part2 = client.getPartition(dbName, viewName, part.getValues());\n+\n+    part2.getParameters().put(\"a\", \"b\");\n+\n+    client.alter_partition(dbName, viewName, part2);\n+\n+    Partition part3 = client.getPartition(dbName, viewName, part.getValues());\n+    assertEquals(\"couldn't view alter partition\", part3.getParameters().get(\n+        \"a\"), \"b\");\n+\n+    client.dropTable(dbName, viewName);\n+\n+    client.dropTable(dbName, tblName);\n+\n+    client.dropDatabase(dbName);\n+  }\n \n   public void testAlterPartition() throws Throwable {\n ", "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1b46251935d5672bf0b6f781a1048f19d2148524", "parent": "https://github.com/apache/hive/commit/e521e4db0f22aad59fe558deebdb8c3bfedc7aa3", "message": "HIVE-4033 : NPE at runtime while selecting virtual column after joining three tables on different keys (Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1447131 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_260", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/1b46251935d5672bf0b6f781a1048f19d2148524/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java", "blob_url": "https://github.com/apache/hive/blob/1b46251935d5672bf0b6f781a1048f19d2148524/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java", "sha": "60aa6140ac322ae01df5c933b4c61db7babb5acf", "changes": 8, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java?ref=1b46251935d5672bf0b6f781a1048f19d2148524", "patch": "@@ -85,7 +85,6 @@\n   // convert from partition to table schema\n   private transient Converter partTblObjectInspectorConverter;\n   private transient boolean isPartitioned;\n-  private transient boolean hasVC;\n   private Map<MapInputPath, MapOpCtx> opCtxMap;\n   private final Set<MapInputPath> listInputPaths = new HashSet<MapInputPath>();\n \n@@ -353,7 +352,6 @@ private void setInspectorInput(MapInputPath inp) {\n       if (tsDesc != null) {\n         this.vcs = tsDesc.getVirtualCols();\n         if (vcs != null && vcs.size() > 0) {\n-          this.hasVC = true;\n           List<String> vcNames = new ArrayList<String>(vcs.size());\n           this.vcValues = new Writable[vcs.size()];\n           List<ObjectInspector> vcsObjectInspectors = new ArrayList<ObjectInspector>(vcs.size());\n@@ -617,7 +615,7 @@ public void process(Writable value) throws HiveException {\n \n     Object row = null;\n     try {\n-      if (this.hasVC) {\n+      if (null != this.rowWithPartAndVC) {\n         this.rowWithPartAndVC[0] =\n             partTblObjectInspectorConverter.convert(deserializer.deserialize(value));\n         int vcPos = isPartitioned ? 2 : 1;\n@@ -649,7 +647,7 @@ public void process(Writable value) throws HiveException {\n     // The row has been converted to comply with table schema, irrespective of partition schema.\n     // So, use tblOI (and not partOI) for forwarding\n     try {\n-      if (this.hasVC) {\n+      if (null != this.rowWithPartAndVC) {\n         forward(this.rowWithPartAndVC, this.tblRowObjectInspector);\n       } else if (!isPartitioned) {\n         forward(row, tblRowObjectInspector);\n@@ -660,7 +658,7 @@ public void process(Writable value) throws HiveException {\n       // Serialize the row and output the error message.\n       String rowString;\n       try {\n-        if (this.hasVC) {\n+        if (null != rowWithPartAndVC) {\n           rowString = SerDeUtils.getJSONString(rowWithPartAndVC, tblRowObjectInspector);\n         } else if (!isPartitioned) {\n           rowString = SerDeUtils.getJSONString(row, tblRowObjectInspector);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/1b46251935d5672bf0b6f781a1048f19d2148524/ql/src/test/queries/clientpositive/join_vc.q", "blob_url": "https://github.com/apache/hive/blob/1b46251935d5672bf0b6f781a1048f19d2148524/ql/src/test/queries/clientpositive/join_vc.q", "sha": "6d0eb135c02ea60da9ca28b9cd725ca0a015ed2a", "changes": 5, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/join_vc.q?ref=1b46251935d5672bf0b6f781a1048f19d2148524", "patch": "@@ -0,0 +1,5 @@\n+-- see HIVE-4033 earlier a flag named hasVC was not initialized correctly in MapOperator.java, resulting in NPE for following query. order by and limit in the query is not relevant, problem would be evident even without those. They are there to keep .q.out file small and sorted.\n+\n+explain select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.value limit 3;\n+\n+select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.value limit 3;", "filename": "ql/src/test/queries/clientpositive/join_vc.q"}, {"additions": 164, "raw_url": "https://github.com/apache/hive/raw/1b46251935d5672bf0b6f781a1048f19d2148524/ql/src/test/results/clientpositive/join_vc.q.out", "blob_url": "https://github.com/apache/hive/blob/1b46251935d5672bf0b6f781a1048f19d2148524/ql/src/test/results/clientpositive/join_vc.q.out", "sha": "f0f5905ca114f29b3d3b2c584ab307c15bbc48dd", "changes": 164, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/join_vc.q.out?ref=1b46251935d5672bf0b6f781a1048f19d2148524", "patch": "@@ -0,0 +1,164 @@\n+PREHOOK: query: -- see HIVE-4033 earlier a flag named hasVC was not initialized correctly in MapOperator.java, resulting in NPE for following query. order by and limit in the query is not relevant, problem would be evident even without those. They are there to keep .q.out file small and sorted.\n+\n+explain select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.value limit 3\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- see HIVE-4033 earlier a flag named hasVC was not initialized correctly in MapOperator.java, resulting in NPE for following query. order by and limit in the query is not relevant, problem would be evident even without those. They are there to keep .q.out file small and sorted.\n+\n+explain select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.value limit 3\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_JOIN (TOK_TABREF (TOK_TABNAME src) t1) (TOK_TABREF (TOK_TABNAME src) t2) (= (. (TOK_TABLE_OR_COL t1) key) (. (TOK_TABLE_OR_COL t2) key))) (TOK_TABREF (TOK_TABNAME src) t3) (= (. (TOK_TABLE_OR_COL t2) value) (. (TOK_TABLE_OR_COL t3) value)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL t3) BLOCK__OFFSET__INSIDE__FILE)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL t3) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL t3) value))) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (. (TOK_TABLE_OR_COL t3) value))) (TOK_LIMIT 3)))\n+\n+STAGE DEPENDENCIES:\n+  Stage-3 is a root stage\n+  Stage-1 depends on stages: Stage-3\n+  Stage-2 depends on stages: Stage-1\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-3\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        t1 \n+          TableScan\n+            alias: t1\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: key\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: key\n+                    type: string\n+              tag: 0\n+        t2 \n+          TableScan\n+            alias: t2\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: key\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: key\n+                    type: string\n+              tag: 1\n+              value expressions:\n+                    expr: value\n+                    type: string\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          condition expressions:\n+            0 \n+            1 {VALUE._col1}\n+          handleSkewJoin: false\n+          outputColumnNames: _col5\n+          File Output Operator\n+            compressed: false\n+            GlobalTableId: 0\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        $INTNAME \n+            Reduce Output Operator\n+              key expressions:\n+                    expr: _col5\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: _col5\n+                    type: string\n+              tag: 0\n+        t3 \n+          TableScan\n+            alias: t3\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: value\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: value\n+                    type: string\n+              tag: 1\n+              value expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+                    expr: BLOCK__OFFSET__INSIDE__FILE\n+                    type: bigint\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          condition expressions:\n+            0 \n+            1 {VALUE._col0} {VALUE._col1} {VALUE._col2}\n+          handleSkewJoin: false\n+          outputColumnNames: _col8, _col9, _col10\n+          Select Operator\n+            expressions:\n+                  expr: _col10\n+                  type: bigint\n+                  expr: _col8\n+                  type: string\n+                  expr: _col9\n+                  type: string\n+            outputColumnNames: _col0, _col1, _col2\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+#### A masked pattern was here ####\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: _col2\n+                    type: string\n+              sort order: +\n+              tag: -1\n+              value expressions:\n+                    expr: _col0\n+                    type: bigint\n+                    expr: _col1\n+                    type: string\n+                    expr: _col2\n+                    type: string\n+      Reduce Operator Tree:\n+        Extract\n+          Limit\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: 3\n+\n+\n+PREHOOK: query: select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.value limit 3\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.value limit 3\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2088\t0\tval_0\n+2632\t0\tval_0\n+968\t0\tval_0", "filename": "ql/src/test/results/clientpositive/join_vc.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "parent": "https://github.com/apache/hive/commit/0c36784788187978374a000936e429f556a11d43", "message": "NPE in union processing followed by lateral view followed by 2 group bys (Navis via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1436221 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_261", "file": [{"additions": 14, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/lib/Utils.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/lib/Utils.java", "sha": "37f18f6204fbf47920c65e79f0f704cd6a15db85", "changes": 14, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lib/Utils.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -50,4 +50,18 @@ public static Node getNthAncestor(Stack<Node> st, int n) {\n     \n     return ret_nd;\n   }\n+\n+  /**\n+   * Find the first node of a type from ancestor stack, starting from parents.\n+   * Returns null if not found.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public static <T> T findNode(Stack<Node> stack, Class<T> target) {\n+    for (int i = stack.size() - 2; i >= 0; i--) {\n+      if (target.isInstance(stack.get(i))) {\n+        return (T) stack.get(i);\n+      }\n+    }\n+    return null;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/lib/Utils.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java", "sha": "decb3a26eb07d21139c130da7540812c9485f93d", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -862,7 +862,7 @@ private String processFS(Node nd, Stack<Node> stack,\n           if (currUnionOp != null) {\n             opTaskMap.put(null, currTask);\n             ctx.setCurrTopOp(null);\n-            GenMapRedUtils.initUnionPlan(ctx, currTask, false);\n+            GenMapRedUtils.initUnionPlan(ctx, currUnionOp, currTask, false);\n             return dest;\n           }\n         }\n@@ -880,7 +880,7 @@ private String processFS(Node nd, Stack<Node> stack,\n \n     if (currUnionOp != null) {\n       opTaskMap.put(null, currTask);\n-      GenMapRedUtils.initUnionPlan(ctx, currTask, false);\n+      GenMapRedUtils.initUnionPlan(ctx, currUnionOp, currTask, false);\n       return dest;\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRFileSink1.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java", "sha": "d669480cd6f64d7b0d603c47fd765497c163ec76", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -81,7 +81,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     } else {\n       // This will happen in case of joins. The current plan can be thrown away\n       // after being merged with the original plan\n-      GenMapRedUtils.joinPlan(op, null, opMapTask, ctx, -1, false, false, false);\n+      GenMapRedUtils.joinPlan(op, null, opMapTask, ctx, -1, false, false, null);\n       currTask = opMapTask;\n       ctx.setCurrTask(currTask);\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink1.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java", "sha": "55b8222d9a4c06f4d35271727e79929db8802c93", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -71,8 +71,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     if (opMapTask == null) {\n       GenMapRedUtils.splitPlan(op, ctx);\n     } else {\n-      GenMapRedUtils.joinPlan(op, currTask, opMapTask, ctx, -1, true, false,\n-          false);\n+      GenMapRedUtils.joinPlan(op, currTask, opMapTask, ctx, -1, true, false, null);\n       currTask = opMapTask;\n       ctx.setCurrTask(currTask);\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink2.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java", "sha": "e299a564f3f8c37bc163ee59f98d4862598f06bf", "changes": 13, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -26,9 +26,11 @@\n import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.UnionOperator;\n import org.apache.hadoop.hive.ql.lib.Node;\n import org.apache.hadoop.hive.ql.lib.NodeProcessor;\n import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;\n+import org.apache.hadoop.hive.ql.lib.Utils;\n import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.plan.MapredWork;\n@@ -58,9 +60,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     // union consisted on a bunch of map-reduce jobs, and it has been split at\n     // the union\n     Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);\n+    UnionOperator union = Utils.findNode(stack, UnionOperator.class);\n+    assert union != null;\n+\n     Map<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx = ctx\n         .getMapCurrCtx();\n-    GenMapRedCtx mapredCtx = mapCurrCtx.get(ctx.getCurrUnionOp());\n+    GenMapRedCtx mapredCtx = mapCurrCtx.get(union);\n \n     Task<? extends Serializable> unionTask = null;\n     if(mapredCtx != null) {\n@@ -81,7 +86,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     if (reducerTask == null) {\n       // When the reducer is encountered for the first time\n       if (plan.getReducer() == null) {\n-        GenMapRedUtils.initUnionPlan(op, ctx, unionTask);\n+        GenMapRedUtils.initUnionPlan(op, union, ctx, unionTask);\n         // When union is followed by a multi-table insert\n       } else {\n         GenMapRedUtils.splitPlan(op, ctx);\n@@ -90,9 +95,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n       // The union is already initialized. However, the union is walked from\n       // another input\n       // initUnionPlan is idempotent\n-      GenMapRedUtils.initUnionPlan(op, ctx, unionTask);\n+      GenMapRedUtils.initUnionPlan(op, union, ctx, unionTask);\n     } else {\n-      GenMapRedUtils.joinUnionPlan(ctx, unionTask, reducerTask, false);\n+      GenMapRedUtils.joinUnionPlan(ctx, union, unionTask, reducerTask, false);\n       ctx.setCurrTask(reducerTask);\n     }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink4.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink4.java", "sha": "b94131ac3403c24dbda9400608ba4e9695387df9", "changes": 5, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink4.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -75,7 +75,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     if (opMapTask == null) {\n       // When the reducer is encountered for the first time\n       if (plan.getReducer() == null) {\n-        GenMapRedUtils.initMapJoinPlan(op, ctx, true, false, true, -1);\n+        GenMapRedUtils.initMapJoinPlan(op, ctx, true, null, true, -1);\n         // When mapjoin is followed by a multi-table insert\n       } else {\n         GenMapRedUtils.splitPlan(op, ctx);\n@@ -85,8 +85,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n       // been initialized.\n       // Initialize the current branch, and join with the original plan.\n       assert plan.getReducer() != reducer;\n-      GenMapRedUtils.joinPlan(op, currTask, opMapTask, ctx, -1, false, true,\n-          false);\n+      GenMapRedUtils.joinPlan(op, currTask, opMapTask, ctx, -1, false, true, null);\n     }\n \n     mapCurrCtx.put(op, new GenMapRedCtx(ctx.getCurrTask(), ctx.getCurrTopOp(),", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink4.java"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "sha": "eb6d944bf8f7c6c5e1f4826d6d5d0c57572aa08b", "changes": 29, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -139,9 +139,9 @@ public static void initPlan(ReduceSinkOperator op, GenMRProcContext opProcCtx)\n \n   public static void initMapJoinPlan(\n     Operator<? extends OperatorDesc> op, GenMRProcContext ctx,\n-    boolean readInputMapJoin, boolean readInputUnion, boolean setReducer, int pos)\n+    boolean readInputMapJoin, UnionOperator currUnionOp, boolean setReducer, int pos)\n     throws SemanticException {\n-    initMapJoinPlan(op, ctx, readInputMapJoin, readInputUnion, setReducer, pos, false);\n+    initMapJoinPlan(op, ctx, readInputMapJoin, currUnionOp, setReducer, pos, false);\n   }\n \n   /**\n@@ -156,7 +156,7 @@ public static void initMapJoinPlan(\n    */\n   public static void initMapJoinPlan(Operator<? extends OperatorDesc> op,\n       GenMRProcContext opProcCtx, boolean readInputMapJoin,\n-      boolean readInputUnion, boolean setReducer, int pos, boolean createLocalPlan)\n+      UnionOperator currUnionOp, boolean setReducer, int pos, boolean createLocalPlan)\n       throws SemanticException {\n     Map<Operator<? extends OperatorDesc>, GenMapRedCtx> mapCurrCtx =\n       opProcCtx.getMapCurrCtx();\n@@ -191,7 +191,7 @@ public static void initMapJoinPlan(Operator<? extends OperatorDesc> op,\n         opTaskMap.put(op, currTask);\n       }\n \n-      if (!readInputUnion) {\n+      if (currUnionOp == null) {\n         GenMRMapJoinCtx mjCtx = opProcCtx.getMapJoinCtx(currMapJoinOp);\n         String taskTmpDir;\n         TableDesc tt_desc;\n@@ -212,7 +212,7 @@ public static void initMapJoinPlan(Operator<? extends OperatorDesc> op,\n         setTaskPlan(taskTmpDir, taskTmpDir, rootOp, plan, local, tt_desc);\n         setupBucketMapJoinInfo(plan, currMapJoinOp, createLocalPlan);\n       } else {\n-        initUnionPlan(opProcCtx, currTask, false);\n+        initUnionPlan(opProcCtx, currUnionOp, currTask, false);\n       }\n \n       opProcCtx.setCurrMapJoinOp(null);\n@@ -305,7 +305,7 @@ private static void setupBucketMapJoinInfo(MapredWork plan,\n    * @param opProcCtx\n    *          processing context\n    */\n-  public static void initUnionPlan(ReduceSinkOperator op,\n+  public static void initUnionPlan(ReduceSinkOperator op, UnionOperator currUnionOp,\n       GenMRProcContext opProcCtx,\n       Task<? extends Serializable> unionTask) throws SemanticException {\n     Operator<? extends OperatorDesc> reducer = op.getChildOperators().get(0);\n@@ -324,7 +324,7 @@ public static void initUnionPlan(ReduceSinkOperator op,\n       plan.setNeedsTagging(true);\n     }\n \n-    initUnionPlan(opProcCtx, unionTask, false);\n+    initUnionPlan(opProcCtx, currUnionOp, unionTask, false);\n   }\n \n   private static void setUnionPlan(GenMRProcContext opProcCtx,\n@@ -373,11 +373,10 @@ private static void setUnionPlan(GenMRProcContext opProcCtx,\n    * It is a idempotent function to add various intermediate files as the source\n    * for the union. The plan has already been created.\n    */\n-  public static void initUnionPlan(GenMRProcContext opProcCtx,\n+  public static void initUnionPlan(GenMRProcContext opProcCtx, UnionOperator currUnionOp,\n       Task<? extends Serializable> currTask, boolean local)\n       throws SemanticException {\n     MapredWork plan = (MapredWork) currTask.getWork();\n-    UnionOperator currUnionOp = opProcCtx.getCurrUnionOp();\n     // In case of lateral views followed by a join, the same tree\n     // can be traversed more than one\n     if (currUnionOp != null) {\n@@ -391,11 +390,11 @@ public static void initUnionPlan(GenMRProcContext opProcCtx,\n    * join current union task to old task\n    */\n   public static void joinUnionPlan(GenMRProcContext opProcCtx,\n+      UnionOperator currUnionOp,\n       Task<? extends Serializable> currentUnionTask,\n       Task<? extends Serializable> existingTask, boolean local)\n       throws SemanticException {\n     MapredWork plan = (MapredWork) existingTask.getWork();\n-    UnionOperator currUnionOp = opProcCtx.getCurrUnionOp();\n     assert currUnionOp != null;\n     GenMRUnionCtx uCtx = opProcCtx.getUnionTask(currUnionOp);\n     assert uCtx != null;\n@@ -437,8 +436,8 @@ public static void joinUnionPlan(GenMRProcContext opProcCtx,\n   public static void joinPlan(Operator<? extends OperatorDesc> op,\n       Task<? extends Serializable> oldTask, Task<? extends Serializable> task,\n       GenMRProcContext opProcCtx, int pos, boolean split,\n-      boolean readMapJoinData, boolean readUnionData) throws SemanticException {\n-    joinPlan(op, oldTask, task, opProcCtx, pos, split, readMapJoinData, readUnionData, false);\n+      boolean readMapJoinData, UnionOperator currUnionOp) throws SemanticException {\n+    joinPlan(op, oldTask, task, opProcCtx, pos, split, readMapJoinData, currUnionOp, false);\n   }\n \n   /**\n@@ -458,7 +457,7 @@ public static void joinPlan(Operator<? extends OperatorDesc> op,\n   public static void joinPlan(Operator<? extends OperatorDesc> op,\n       Task<? extends Serializable> oldTask, Task<? extends Serializable> task,\n       GenMRProcContext opProcCtx, int pos, boolean split,\n-      boolean readMapJoinData, boolean readUnionData, boolean createLocalWork)\n+      boolean readMapJoinData, UnionOperator currUnionOp, boolean createLocalWork)\n       throws SemanticException {\n     Task<? extends Serializable> currTask = task;\n     MapredWork plan = (MapredWork) currTask.getWork();\n@@ -502,8 +501,8 @@ public static void joinPlan(Operator<? extends OperatorDesc> op,\n       opProcCtx.setCurrTopOp(currTopOp);\n     } else if (opProcCtx.getCurrMapJoinOp() != null) {\n       AbstractMapJoinOperator<? extends MapJoinDesc> mjOp = opProcCtx.getCurrMapJoinOp();\n-      if (readUnionData) {\n-        initUnionPlan(opProcCtx, currTask, false);\n+      if (currUnionOp != null) {\n+        initUnionPlan(opProcCtx, currUnionOp, currTask, false);\n       } else {\n         GenMRMapJoinCtx mjCtx = opProcCtx.getMapJoinCtx(mjOp);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java", "blob_url": "https://github.com/apache/hive/blob/33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java", "sha": "62da47a0636058c8867688afd501c1453538e4ac", "changes": 25, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java?ref=33f40d1a1af6b589e3104fb30cc1a94e3b56e0f1", "patch": "@@ -37,6 +37,7 @@\n import org.apache.hadoop.hive.ql.lib.Node;\n import org.apache.hadoop.hive.ql.lib.NodeProcessor;\n import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;\n+import org.apache.hadoop.hive.ql.lib.Utils;\n import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMRMapJoinCtx;\n import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;\n import org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext;\n@@ -100,12 +101,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       // If the plan for this reducer does not exist, initialize the plan\n       if (opMapTask == null) {\n         assert currPlan.getReducer() == null;\n-        GenMapRedUtils.initMapJoinPlan(mapJoin, ctx, false, false, false, pos);\n+        GenMapRedUtils.initMapJoinPlan(mapJoin, ctx, false, null, false, pos);\n       } else {\n         // The current plan can be thrown away after being merged with the\n         // original plan\n         GenMapRedUtils.joinPlan(mapJoin, null, opMapTask, ctx, pos, false,\n-            false, false);\n+            false, null);\n         currTask = opMapTask;\n         ctx.setCurrTask(currTask);\n       }\n@@ -155,7 +156,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         // The current plan can be thrown away after being merged with the\n         // original plan\n         GenMapRedUtils.joinPlan(mapJoin, currTask, opMapTask, opProcCtx, pos,\n-            false, false, false);\n+            false, false, null);\n         currTask = opMapTask;\n         opProcCtx.setCurrTask(currTask);\n       }\n@@ -302,12 +303,12 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       // If the plan for this reducer does not exist, initialize the plan\n       if (opMapTask == null) {\n         assert currPlan.getReducer() == null;\n-        GenMapRedUtils.initMapJoinPlan(mapJoin, ctx, true, false, false, pos);\n+        GenMapRedUtils.initMapJoinPlan(mapJoin, ctx, true, null, false, pos);\n       } else {\n         // The current plan can be thrown away after being merged with the\n         // original plan\n         GenMapRedUtils.joinPlan(mapJoin, currTask, opMapTask, ctx, pos, false,\n-            true, false);\n+            true, null);\n         currTask = opMapTask;\n         ctx.setCurrTask(currTask);\n       }\n@@ -336,7 +337,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n             .process(nd, stack, procCtx, nodeOutputs);\n       }\n \n-      UnionOperator currUnion = ctx.getCurrUnionOp();\n+      UnionOperator currUnion = Utils.findNode(stack, UnionOperator.class);\n       assert currUnion != null;\n       ctx.getUnionTask(currUnion);\n       AbstractMapJoinOperator<MapJoinDesc> mapJoin = (AbstractMapJoinOperator<MapJoinDesc>) nd;\n@@ -356,8 +357,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       Task<? extends Serializable> opMapTask = opTaskMap.get(reducer);\n \n       // union result cannot be a map table\n-      boolean local = (pos == (mapJoin.getConf()).getPosBigTable()) ? false\n-          : true;\n+      boolean local = (pos != mapJoin.getConf().getPosBigTable());\n       if (local) {\n         throw new SemanticException(ErrorMsg.INVALID_MAPJOIN_TABLE.getMsg());\n       }\n@@ -366,19 +366,18 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       if (opMapTask == null) {\n         assert currPlan.getReducer() == null;\n         ctx.setCurrMapJoinOp(mapJoin);\n-        GenMapRedUtils.initMapJoinPlan(mapJoin, ctx, true, true, false, pos);\n+        GenMapRedUtils.initMapJoinPlan(mapJoin, ctx, true, currUnion, false, pos);\n         ctx.setCurrUnionOp(null);\n       } else {\n         // The current plan can be thrown away after being merged with the\n         // original plan\n-        Task<? extends Serializable> uTask = ctx.getUnionTask(\n-            ctx.getCurrUnionOp()).getUTask();\n+        Task<? extends Serializable> uTask = ctx.getUnionTask(currUnion).getUTask();\n         if (uTask.getId().equals(opMapTask.getId())) {\n           GenMapRedUtils.joinPlan(mapJoin, null, opMapTask, ctx, pos, false,\n-              false, true);\n+              false, currUnion);\n         } else {\n           GenMapRedUtils.joinPlan(mapJoin, uTask, opMapTask, ctx, pos, false,\n-              false, true);\n+              false, currUnion);\n         }\n         currTask = opMapTask;\n         ctx.setCurrTask(currTask);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/9bf3b8d8567354e2a64e39b63128466e7694724a", "parent": "https://github.com/apache/hive/commit/486bacc3ef73aa7ae0b3f35cd030c211ae37c183", "message": "HIVE-3795 NPE in SELECT when WHERE-clause is an and/or/not operation involving null\n(Xiao Jiang via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1423285 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_262", "file": [{"additions": 15, "raw_url": "https://github.com/apache/hive/raw/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java", "sha": "104948dd020107718689d4ad823619e2373f5fb7", "changes": 22, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java?ref=9bf3b8d8567354e2a64e39b63128466e7694724a", "patch": "@@ -125,7 +125,9 @@ private PcrExprProcFactory() {\n   }\n \n   static Boolean opAnd(Boolean op1, Boolean op2) {\n-    if (op1.equals(Boolean.FALSE) || op2.equals(Boolean.FALSE)) {\n+    // When people forget to quote a string, op1/op2 is null.\n+    // For example, select * from some_table where ds > 2012-12-1 and ds < 2012-12-2 .\n+    if (op1 != null && op1.equals(Boolean.FALSE) || op2 != null && op2.equals(Boolean.FALSE)) {\n       return Boolean.FALSE;\n     }\n     if (op1 == null || op2 == null) {\n@@ -135,7 +137,9 @@ static Boolean opAnd(Boolean op1, Boolean op2) {\n   }\n \n   static Boolean opOr(Boolean op1, Boolean op2) {\n-    if (op1.equals(Boolean.TRUE) || op2.equals(Boolean.TRUE)) {\n+    // When people forget to quote a string, op1/op2 is null.\n+    // For example, select * from some_table where ds > 2012-12-1 or ds < 2012-12-2 .\n+    if (op1 != null && op1.equals(Boolean.TRUE) || op2 != null && op2.equals(Boolean.TRUE)) {\n       return Boolean.TRUE;\n     }\n     if (op1 == null || op2 == null) {\n@@ -145,11 +149,15 @@ static Boolean opOr(Boolean op1, Boolean op2) {\n   }\n \n   static Boolean opNot(Boolean op) {\n-    if (op.equals(Boolean.TRUE)) {\n-      return Boolean.FALSE;\n-    }\n-    if (op.equals(Boolean.FALSE)) {\n-      return Boolean.TRUE;\n+    // When people forget to quote a string, op1/op2 is null.\n+    // For example, select * from some_table where not ds > 2012-12-1 .\n+    if (op != null) {\n+      if (op.equals(Boolean.TRUE)) {\n+        return Boolean.FALSE;\n+      }\n+      if (op.equals(Boolean.FALSE)) {\n+        return Boolean.TRUE;\n+      }\n     }\n     return null;\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/queries/clientpositive/select_unquote_and.q", "blob_url": "https://github.com/apache/hive/blob/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/queries/clientpositive/select_unquote_and.q", "sha": "275c774d1d4eb5d16e047ef1bb81512d6e0e4b82", "changes": 15, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/select_unquote_and.q?ref=9bf3b8d8567354e2a64e39b63128466e7694724a", "patch": "@@ -0,0 +1,15 @@\n+CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING);\n+\n+INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200';\n+\n+INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200';\n+\n+SELECT count(*) FROM npe_test;\n+\n+EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15;\n+\n+SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15;\n+\n+DROP TABLE npe_test;", "filename": "ql/src/test/queries/clientpositive/select_unquote_and.q"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/queries/clientpositive/select_unquote_not.q", "blob_url": "https://github.com/apache/hive/blob/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/queries/clientpositive/select_unquote_not.q", "sha": "93d992e69844f064b12fd5863a567f8bdd042866", "changes": 15, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/select_unquote_not.q?ref=9bf3b8d8567354e2a64e39b63128466e7694724a", "patch": "@@ -0,0 +1,15 @@\n+CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING);\n+\n+INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200';\n+\n+INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200';\n+\n+SELECT count(*) FROM npe_test;\n+\n+EXPLAIN SELECT * FROM npe_test WHERE NOT ds < 2012-11-31;\n+\n+SELECT count(*) FROM npe_test WHERE NOT ds < 2012-11-31;\n+\n+DROP TABLE npe_test;", "filename": "ql/src/test/queries/clientpositive/select_unquote_not.q"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/queries/clientpositive/select_unquote_or.q", "blob_url": "https://github.com/apache/hive/blob/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/queries/clientpositive/select_unquote_or.q", "sha": "5dd0776792f5bb9b95ea36da37bb7e560e421c28", "changes": 15, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/select_unquote_or.q?ref=9bf3b8d8567354e2a64e39b63128466e7694724a", "patch": "@@ -0,0 +1,15 @@\n+CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING);\n+\n+INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200';\n+\n+INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200';\n+\n+SELECT count(*) FROM npe_test;\n+\n+EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15;\n+\n+SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15;\n+\n+DROP TABLE npe_test;", "filename": "ql/src/test/queries/clientpositive/select_unquote_or.q"}, {"additions": 120, "raw_url": "https://github.com/apache/hive/raw/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/results/clientpositive/select_unquote_and.q.out", "blob_url": "https://github.com/apache/hive/blob/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/results/clientpositive/select_unquote_and.q.out", "sha": "299808a255b67381df0fa708a65803252530adfd", "changes": 120, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/select_unquote_and.q.out?ref=9bf3b8d8567354e2a64e39b63128466e7694724a", "patch": "@@ -0,0 +1,120 @@\n+PREHOOK: query: CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@npe_test\n+PREHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@npe_test@ds=2012-12-11\n+POSTHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@npe_test@ds=2012-12-11\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@npe_test@ds=2012-12-12\n+POSTHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@npe_test@ds=2012-12-12\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: SELECT count(*) FROM npe_test\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@npe_test@ds=2012-12-11\n+PREHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM npe_test\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@npe_test@ds=2012-12-11\n+POSTHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+498\n+PREHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME npe_test))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (AND (> (TOK_TABLE_OR_COL ds) (- (- 2012 11) 31)) (< (TOK_TABLE_OR_COL ds) (- (- 2012 12) 15))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        npe_test \n+          TableScan\n+            alias: npe_test\n+            Filter Operator\n+              predicate:\n+                  expr: ((ds > ((2012 - 11) - 31)) and (ds < ((2012 - 12) - 15)))\n+                  type: boolean\n+              Select Operator\n+                expressions:\n+                      expr: key\n+                      type: string\n+                      expr: value\n+                      type: string\n+                      expr: ds\n+                      type: string\n+                outputColumnNames: _col0, _col1, _col2\n+                File Output Operator\n+                  compressed: false\n+                  GlobalTableId: 0\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@npe_test@ds=2012-12-11\n+PREHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@npe_test@ds=2012-12-11\n+POSTHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+0\n+PREHOOK: query: DROP TABLE npe_test\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@npe_test\n+PREHOOK: Output: default@npe_test\n+POSTHOOK: query: DROP TABLE npe_test\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@npe_test\n+POSTHOOK: Output: default@npe_test\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]", "filename": "ql/src/test/results/clientpositive/select_unquote_and.q.out"}, {"additions": 120, "raw_url": "https://github.com/apache/hive/raw/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/results/clientpositive/select_unquote_not.q.out", "blob_url": "https://github.com/apache/hive/blob/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/results/clientpositive/select_unquote_not.q.out", "sha": "3ef79f00623a49f6baadd80c35347892a5635758", "changes": 120, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/select_unquote_not.q.out?ref=9bf3b8d8567354e2a64e39b63128466e7694724a", "patch": "@@ -0,0 +1,120 @@\n+PREHOOK: query: CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@npe_test\n+PREHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@npe_test@ds=2012-12-11\n+POSTHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@npe_test@ds=2012-12-11\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@npe_test@ds=2012-12-12\n+POSTHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@npe_test@ds=2012-12-12\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: SELECT count(*) FROM npe_test\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@npe_test@ds=2012-12-11\n+PREHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM npe_test\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@npe_test@ds=2012-12-11\n+POSTHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+498\n+PREHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE NOT ds < 2012-11-31\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE NOT ds < 2012-11-31\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME npe_test))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (NOT (< (TOK_TABLE_OR_COL ds) (- (- 2012 11) 31))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        npe_test \n+          TableScan\n+            alias: npe_test\n+            Filter Operator\n+              predicate:\n+                  expr: (not (ds < ((2012 - 11) - 31)))\n+                  type: boolean\n+              Select Operator\n+                expressions:\n+                      expr: key\n+                      type: string\n+                      expr: value\n+                      type: string\n+                      expr: ds\n+                      type: string\n+                outputColumnNames: _col0, _col1, _col2\n+                File Output Operator\n+                  compressed: false\n+                  GlobalTableId: 0\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT count(*) FROM npe_test WHERE NOT ds < 2012-11-31\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@npe_test@ds=2012-12-11\n+PREHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM npe_test WHERE NOT ds < 2012-11-31\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@npe_test@ds=2012-12-11\n+POSTHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+0\n+PREHOOK: query: DROP TABLE npe_test\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@npe_test\n+PREHOOK: Output: default@npe_test\n+POSTHOOK: query: DROP TABLE npe_test\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@npe_test\n+POSTHOOK: Output: default@npe_test\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]", "filename": "ql/src/test/results/clientpositive/select_unquote_not.q.out"}, {"additions": 120, "raw_url": "https://github.com/apache/hive/raw/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/results/clientpositive/select_unquote_or.q.out", "blob_url": "https://github.com/apache/hive/blob/9bf3b8d8567354e2a64e39b63128466e7694724a/ql/src/test/results/clientpositive/select_unquote_or.q.out", "sha": "b080383c8f372235463d8d08e398b8df81087b76", "changes": 120, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/select_unquote_or.q.out?ref=9bf3b8d8567354e2a64e39b63128466e7694724a", "patch": "@@ -0,0 +1,120 @@\n+PREHOOK: query: CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@npe_test\n+PREHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@npe_test@ds=2012-12-11\n+POSTHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')\n+SELECT src.key, src.value FROM src WHERE key < '200'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@npe_test@ds=2012-12-11\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@npe_test@ds=2012-12-12\n+POSTHOOK: query: INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')\n+SELECT src.key, src.value FROM src WHERE key > '200'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@npe_test@ds=2012-12-12\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: SELECT count(*) FROM npe_test\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@npe_test@ds=2012-12-11\n+PREHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM npe_test\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@npe_test@ds=2012-12-11\n+POSTHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+498\n+PREHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME npe_test))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (OR (> (TOK_TABLE_OR_COL ds) (- (- 2012 11) 31)) (< (TOK_TABLE_OR_COL ds) (- (- 2012 12) 15))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        npe_test \n+          TableScan\n+            alias: npe_test\n+            Filter Operator\n+              predicate:\n+                  expr: ((ds > ((2012 - 11) - 31)) or (ds < ((2012 - 12) - 15)))\n+                  type: boolean\n+              Select Operator\n+                expressions:\n+                      expr: key\n+                      type: string\n+                      expr: value\n+                      type: string\n+                      expr: ds\n+                      type: string\n+                outputColumnNames: _col0, _col1, _col2\n+                File Output Operator\n+                  compressed: false\n+                  GlobalTableId: 0\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@npe_test@ds=2012-12-11\n+PREHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@npe_test@ds=2012-12-11\n+POSTHOOK: Input: default@npe_test@ds=2012-12-12\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+0\n+PREHOOK: query: DROP TABLE npe_test\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@npe_test\n+PREHOOK: Output: default@npe_test\n+POSTHOOK: query: DROP TABLE npe_test\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@npe_test\n+POSTHOOK: Output: default@npe_test\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-11).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]", "filename": "ql/src/test/results/clientpositive/select_unquote_or.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/7676cd91a8612c55aff0cf30dcaba6342a369eaa", "parent": "https://github.com/apache/hive/commit/6b2fcdbdafb769d0015de5a442f955921451b371", "message": "HIVE-3525. Avro Maps with Nullable Values fail with NPE (Sean Busbey via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1399935 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_263", "file": [{"additions": 2, "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java", "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java", "sha": "c85ef15df43c16ff730eed5925ec7ec948782586", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa", "patch": "@@ -43,7 +43,7 @@\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n-import java.util.Hashtable;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n@@ -246,7 +246,7 @@ private Object deserializeMap(Object datum, Schema mapSchema, MapTypeInfo column\n           throws AvroSerdeException {\n     // Avro only allows maps with Strings for keys, so we only have to worry\n     // about deserializing the values\n-    Map<String, Object> map = new Hashtable<String, Object>();\n+    Map<String, Object> map = new HashMap<String, Object>();\n     Map<Utf8, Object> mapDatum = (Map)datum;\n     Schema valueSchema = mapSchema.getValueType();\n     TypeInfo valueTypeInfo = columnType.getMapValueTypeInfo();", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java", "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java", "sha": "03c16f481e5d6008966f83a2e7078ccfab823d9a", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa", "patch": "@@ -39,7 +39,7 @@\n \n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n-import java.util.Hashtable;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n@@ -229,7 +229,7 @@ private Object serializeMap(MapTypeInfo typeInfo, MapObjectInspector fieldOI, Ob\n     Map<?,?> map = fieldOI.getMap(structFieldData);\n     Schema valueType = schema.getValueType();\n \n-    Map<Object, Object> deserialized = new Hashtable<Object, Object>(fieldOI.getMapSize(structFieldData));\n+    Map<Object, Object> deserialized = new HashMap<Object, Object>(fieldOI.getMapSize(structFieldData));\n \n     for (Map.Entry<?, ?> entry : map.entrySet()) {\n       deserialized.put(serialize(mapKeyTypeInfo, mapKeyObjectInspector, entry.getKey(), null), // This works, but is a bit fragile.  Construct a single String schema?", "filename": "serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java"}, {"additions": 52, "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java", "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java", "sha": "5fe448eb2454810e8dfb97259a7adaae94e5b319", "changes": 52, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa", "patch": "@@ -36,6 +36,7 @@\n import java.util.Hashtable;\n import java.util.List;\n import java.util.Map;\n+import java.util.HashMap;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n@@ -404,6 +405,57 @@ public void canDeserializeNullableTypes() throws IOException, SerDeException {\n     verifyNullableType(record, s, null);\n   }\n \n+  @Test\n+  public void canDeserializeMapWithNullablePrimitiveValues() throws SerDeException, IOException {\n+    Schema s = Schema.parse(TestAvroObjectInspectorGenerator.MAP_WITH_NULLABLE_PRIMITIVE_VALUE_TYPE_SCHEMA);\n+    GenericData.Record record = new GenericData.Record(s);\n+\n+    Map<String, Long> m = new HashMap<String, Long>();\n+    m.put(\"one\", 1l);\n+    m.put(\"two\", 2l);\n+    m.put(\"three\", 3l);\n+    m.put(\"mu\", null);\n+\n+    record.put(\"aMap\", m);\n+    assertTrue(GENERIC_DATA.validate(s, record));\n+    System.out.println(\"record = \" + record);\n+\n+    AvroGenericRecordWritable garw = Utils.serializeAndDeserializeRecord(record);\n+\n+    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(s);\n+\n+    AvroDeserializer de = new AvroDeserializer();\n+\n+    ArrayList<Object> row = (ArrayList<Object>)de.deserialize(aoig.getColumnNames(),\n+            aoig.getColumnTypes(), garw, s);\n+    assertEquals(1, row.size());\n+    Object theMapObject = row.get(0);\n+    assertTrue(theMapObject instanceof Map);\n+    Map theMap = (Map)theMapObject;\n+\n+    // Verify the raw object that's been created\n+    assertEquals(1l, theMap.get(\"one\"));\n+    assertEquals(2l, theMap.get(\"two\"));\n+    assertEquals(3l, theMap.get(\"three\"));\n+    assertTrue(theMap.containsKey(\"mu\"));\n+    assertEquals(null, theMap.get(\"mu\"));\n+\n+    // Verify that the provided object inspector can pull out these same values\n+    StandardStructObjectInspector oi =\n+            (StandardStructObjectInspector)aoig.getObjectInspector();\n+\n+    List<Object> z = oi.getStructFieldsDataAsList(row);\n+    assertEquals(1, z.size());\n+    StructField fieldRef = oi.getStructFieldRef(\"amap\");\n+\n+    Map theMap2 = (Map)oi.getStructFieldData(row, fieldRef);\n+    assertEquals(1l, theMap2.get(\"one\"));\n+    assertEquals(2l, theMap2.get(\"two\"));\n+    assertEquals(3l, theMap2.get(\"three\"));\n+    assertTrue(theMap2.containsKey(\"mu\"));\n+    assertEquals(null, theMap2.get(\"mu\"));\n+  }\n+\n   private void verifyNullableType(GenericData.Record record, Schema s,\n                                   String expected) throws SerDeException, IOException {\n     assertTrue(GENERIC_DATA.validate(s, record));", "filename": "serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java", "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java", "sha": "bc1a5c7305d978d745efb7d2f278d29278d5a08a", "changes": 32, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa", "patch": "@@ -142,6 +142,18 @@\n       \"    {\\\"name\\\":\\\"nullableString\\\", \\\"type\\\":[\\\"null\\\", \\\"string\\\"]}\\n\" +\n       \"  ]\\n\" +\n       \"}\";\n+  public static final String MAP_WITH_NULLABLE_PRIMITIVE_VALUE_TYPE_SCHEMA = \"{\\n\" +\n+      \"  \\\"namespace\\\": \\\"testing\\\",\\n\" +\n+      \"  \\\"name\\\": \\\"mapWithNullableUnionTest\\\",\\n\" +\n+      \"  \\\"type\\\": \\\"record\\\",\\n\" +\n+      \"  \\\"fields\\\": [\\n\" +\n+      \"    {\\n\" +\n+      \"      \\\"name\\\":\\\"aMap\\\",\\n\" +\n+      \"      \\\"type\\\":{\\\"type\\\":\\\"map\\\",\\n\" +\n+      \"      \\\"values\\\":[\\\"null\\\",\\\"long\\\"]}\\n\" +\n+      \"\\t}\\n\" +\n+      \"  ]\\n\" +\n+      \"}\";\n   public static final String BYTES_SCHEMA = \"{\\n\" +\n       \"  \\\"type\\\": \\\"record\\\", \\n\" +\n       \"  \\\"name\\\": \\\"bytesTest\\\",\\n\" +\n@@ -325,10 +337,19 @@ private void verifyColumnNames(String[] expectedColumnNames, List<String> column\n   public void canHandleMapsWithPrimitiveValueTypes() throws SerDeException {\n     Schema s = Schema.parse(MAP_WITH_PRIMITIVE_VALUE_TYPE);\n     AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(s);\n-\n+    verifyMap(aoig, \"aMap\");\n+  }\n+ \n+  /**\n+   * Check a given AvroObjectInspectorGenerator to verify that it matches our test\n+   * schema's expected map.\n+   * @param aoig should already have been intitialized, may not be null\n+   * @param fieldName name of the contianed column, will always fail if null.\n+   */\n+  private void verifyMap(final AvroObjectInspectorGenerator aoig, final String fieldName) {\n     // Column names\n     assertEquals(1, aoig.getColumnNames().size());\n-    assertEquals(\"aMap\", aoig.getColumnNames().get(0));\n+    assertEquals(fieldName, aoig.getColumnNames().get(0));\n \n     // Column types\n     assertEquals(1, aoig.getColumnTypes().size());\n@@ -483,6 +504,13 @@ public void convertsNullableTypes() throws SerDeException {\n     assertEquals(PrimitiveObjectInspector.PrimitiveCategory.STRING, pti.getPrimitiveCategory());\n   }\n \n+  @Test // That Union[T, NULL] is converted to just T, within a Map\n+  public void convertsMapsWithNullablePrimitiveTypes() throws SerDeException {\n+    Schema s = Schema.parse(MAP_WITH_NULLABLE_PRIMITIVE_VALUE_TYPE_SCHEMA);\n+    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(s);\n+    verifyMap(aoig, \"aMap\");\n+  }\n+\n   @Test\n   public void objectInspectorsAreCached() throws SerDeException {\n     // Verify that Hive is caching the object inspectors for us.", "filename": "serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java", "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java", "sha": "2ba0e9ff2b6a4e1058f5d2b5d7b477afae8bb768", "changes": 16, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa", "patch": "@@ -31,6 +31,7 @@\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Hashtable;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n@@ -206,6 +207,21 @@ public void canSerializeNullableTypes() throws SerDeException, IOException {\n     assertNull(r.get(\"nullableint\"));\n   }\n \n+  @Test \n+  public void canSerializeMapsWithNullablePrimitiveValues() throws SerDeException, IOException {\n+    String field = \"{ \\\"name\\\":\\\"mapWithNulls\\\", \\\"type\\\": \" +\n+            \"{\\\"type\\\":\\\"map\\\", \\\"values\\\": [\\\"null\\\", \\\"boolean\\\"]} }\";\n+\n+    Map<String, Boolean> m = new HashMap<String, Boolean>();\n+    m.put(\"yes\", true);\n+    m.put(\"no\", false);\n+    m.put(\"maybe\", null);\n+    GenericRecord r = serializeAndDeserialize(field, \"mapWithNulls\", m);\n+\n+    Object result = r.get(\"mapWithNulls\");\n+    assertEquals(m, result);\n+  }\n+\n   @Test\n   public void canSerializeBytes() throws SerDeException, IOException {\n     String field = \"{ \\\"name\\\":\\\"bytes1\\\", \\\"type\\\":\\\"bytes\\\" }\";", "filename": "serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/918fae35962e89481a92d6813443421bc32a258b", "parent": "https://github.com/apache/hive/commit/a845066944218b5e1e162e0621ba98b36a52900e", "message": "HIVE-3497 Avoid NPE in skewed information read\n(Gang Tim Liu via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1389072 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_264", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/918fae35962e89481a92d6813443421bc32a258b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java", "blob_url": "https://github.com/apache/hive/blob/918fae35962e89481a92d6813443421bc32a258b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java", "sha": "a41b18f0a9a7fb48b63947af70bc1ff1fa0e27c4", "changes": 9, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java?ref=918fae35962e89481a92d6813443421bc32a258b", "patch": "@@ -528,23 +528,26 @@ public void setSkewedValueLocationMap(List<String> valList, String dirName)\n   }\n \n   public Map<List<String>,String> getSkewedColValueLocationMaps() {\n-    return tTable.getSd().getSkewedInfo().getSkewedColValueLocationMaps();\n+    return (tTable.getSd().getSkewedInfo() != null) ? tTable.getSd().getSkewedInfo()\n+        .getSkewedColValueLocationMaps() : new HashMap<List<String>, String>();\n   }\n \n   public void setSkewedColValues(List<List<String>> skewedValues) throws HiveException {\n     tTable.getSd().getSkewedInfo().setSkewedColValues(skewedValues);\n   }\n \n   public List<List<String>> getSkewedColValues(){\n-    return tTable.getSd().getSkewedInfo().getSkewedColValues();\n+    return (tTable.getSd().getSkewedInfo() != null) ? tTable.getSd().getSkewedInfo()\n+        .getSkewedColValues() : new ArrayList<List<String>>();\n   }\n \n   public void setSkewedColNames(List<String> skewedColNames) throws HiveException {\n     tTable.getSd().getSkewedInfo().setSkewedColNames(skewedColNames);\n   }\n \n   public List<String> getSkewedColNames() {\n-    return tTable.getSd().getSkewedInfo().getSkewedColNames();\n+    return (tTable.getSd().getSkewedInfo() != null) ? tTable.getSd().getSkewedInfo()\n+        .getSkewedColNames() : new ArrayList<String>();\n   }\n \n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/8e67ca8a3d10572f866795e9a173fff63e712ba9", "parent": "https://github.com/apache/hive/commit/2fd3f421b847ae4ddbca6349bb00323558ed90e3", "message": "HIVE-3225 : NPE on a join query with authorization enabled (Francis Liu via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1364853 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_265", "file": [{"additions": 6, "raw_url": "https://github.com/apache/hive/raw/8e67ca8a3d10572f866795e9a173fff63e712ba9/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "blob_url": "https://github.com/apache/hive/blob/8e67ca8a3d10572f866795e9a173fff63e712ba9/ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "sha": "567aafa36742f602eca59b9b6fab5d7e376ba65f", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/Driver.java?ref=8e67ca8a3d10572f866795e9a173fff63e712ba9", "patch": "@@ -606,7 +606,11 @@ private void doAuthorization(BaseSemanticAnalyzer sem)\n                 cols.add(columns.get(i).getName());\n               }\n             }\n-            if (tbl.isPartitioned() && tableUsePartLevelAuth.get(tbl.getTableName())) {\n+            //map may not contain all sources, since input list may have been optimized out\n+            //or non-existent tho such sources may still be referenced by the TableScanOperator\n+            //if it's null then the partition probably doesn't exist so let's use table permission\n+            if (tbl.isPartitioned() &&\n+                tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n               String alias_id = topOpMap.getKey();\n               PrunedPartitionList partsList = PartitionPruner.prune(parseCtx\n                   .getTopToTable().get(topOp), parseCtx.getOpToPartPruner()\n@@ -643,7 +647,7 @@ private void doAuthorization(BaseSemanticAnalyzer sem)\n         if (read.getPartition() != null) {\n           tbl = read.getPartition().getTable();\n           // use partition level authorization\n-          if (tableUsePartLevelAuth.get(tbl.getTableName())) {\n+          if (tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n             List<String> cols = part2Cols.get(read.getPartition());\n             if (cols != null && cols.size() > 0) {\n               ss.getAuthorizer().authorize(read.getPartition().getTable(),", "filename": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/8e67ca8a3d10572f866795e9a173fff63e712ba9/ql/src/test/queries/clientnegative/join_nonexistent_part.q", "blob_url": "https://github.com/apache/hive/blob/8e67ca8a3d10572f866795e9a173fff63e712ba9/ql/src/test/queries/clientnegative/join_nonexistent_part.q", "sha": "b4a4757d2214774f5b528084c2baff3289b95804", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/join_nonexistent_part.q?ref=8e67ca8a3d10572f866795e9a173fff63e712ba9", "patch": "@@ -0,0 +1,4 @@\n+SET hive.security.authorization.enabled = true;\n+SELECT *\n+FROM srcpart s1 join src s2 on s1.key == s2.key\n+WHERE s1.ds='non-existent';\n\\ No newline at end of file", "filename": "ql/src/test/queries/clientnegative/join_nonexistent_part.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/8e67ca8a3d10572f866795e9a173fff63e712ba9/ql/src/test/results/clientnegative/join_nonexistent_part.q.out", "blob_url": "https://github.com/apache/hive/blob/8e67ca8a3d10572f866795e9a173fff63e712ba9/ql/src/test/results/clientnegative/join_nonexistent_part.q.out", "sha": "8380d754973605c37a411afc8851e40c1409c085", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/join_nonexistent_part.q.out?ref=8e67ca8a3d10572f866795e9a173fff63e712ba9", "patch": "@@ -0,0 +1 @@\n+Authorization failed:No privilege 'Select' found for inputs { database:default, table:src, columnName:key}. Use show grant to get more details.", "filename": "ql/src/test/results/clientnegative/join_nonexistent_part.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/997a75312914db9d7462a699bc7692431a798434", "parent": "https://github.com/apache/hive/commit/03dc68aa7489c3611a1a8be22ab7ecf88a3e848f", "message": "HIVE-3205 Bucketed mapjoin on partitioned table which has no partition throws NPE\n(Navis via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1363639 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_266", "file": [{"additions": 44, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java", "sha": "4af7a131ed982c1faac87b5fcd189f26532425bb", "changes": 78, "status": "modified", "deletions": 34, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -24,7 +24,6 @@\n import java.util.Arrays;\n import java.util.HashMap;\n import java.util.Iterator;\n-import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n \n@@ -42,6 +41,7 @@\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\n import org.apache.hadoop.hive.serde2.Deserializer;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n import org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n@@ -64,7 +64,6 @@\n   static Log LOG = LogFactory.getLog(FetchOperator.class.getName());\n   static LogHelper console = new LogHelper(LOG);\n \n-  private boolean isEmptyTable;\n   private boolean isNativeTable;\n   private FetchWork work;\n   private int splitNum;\n@@ -96,7 +95,7 @@ public void initialize(JobConf job) {\n     this.job = job;\n     tblDataDone = false;\n     rowWithPart = new Object[2];\n-    if (work.getTblDesc() != null) {\n+    if (work.getTblDir() != null) {\n       isNativeTable = !work.getTblDesc().isNonNative();\n     } else {\n       isNativeTable = true;\n@@ -144,11 +143,7 @@ public void setTblDataDone(boolean tblDataDone) {\n   }\n \n   public boolean isEmptyTable() {\n-    return isEmptyTable;\n-  }\n-\n-  public void setEmptyTable(boolean isEmptyTable) {\n-    this.isEmptyTable = isEmptyTable;\n+    return work.getTblDir() == null && (work.getPartDir() == null || work.getPartDir().isEmpty());\n   }\n \n   /**\n@@ -171,28 +166,37 @@ public void setEmptyTable(boolean isEmptyTable) {\n     return inputFormats.get(inputFormatClass);\n   }\n \n-  private void setPrtnDesc() throws Exception {\n-    List<String> partNames = new ArrayList<String>();\n-    List<String> partValues = new ArrayList<String>();\n-\n-    String pcols = currPart.getTableDesc().getProperties().getProperty(\n+  private void setPrtnDesc(TableDesc table, Map<String, String> partSpec) throws Exception {\n+    String pcols = table.getProperties().getProperty(\n         org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_PARTITION_COLUMNS);\n-    LinkedHashMap<String, String> partSpec = currPart.getPartSpec();\n+    String[] partKeys = pcols.trim().split(\"/\");\n+    if (partSpec != null) {\n+      rowWithPart[1] = createPartValue(partKeys, partSpec);\n+    }\n+    rowObjectInspector = createRowInspector(partKeys);\n+  }\n \n+  private StructObjectInspector createRowInspector(String[] partKeys) throws SerDeException {\n+    List<String> partNames = new ArrayList<String>();\n     List<ObjectInspector> partObjectInspectors = new ArrayList<ObjectInspector>();\n-    String[] partKeys = pcols.trim().split(\"/\");\n     for (String key : partKeys) {\n       partNames.add(key);\n-      partValues.add(partSpec.get(key));\n       partObjectInspectors.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n     }\n     StructObjectInspector partObjectInspector = ObjectInspectorFactory\n         .getStandardStructObjectInspector(partNames, partObjectInspectors);\n-    rowObjectInspector = (StructObjectInspector) serde.getObjectInspector();\n+    StructObjectInspector inspector = (StructObjectInspector) serde.getObjectInspector();\n+\n+    return ObjectInspectorFactory.getUnionStructObjectInspector(\n+        Arrays.asList(inspector, partObjectInspector));\n+  }\n \n-    rowWithPart[1] = partValues;\n-    rowObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(Arrays\n-        .asList(new StructObjectInspector[] {rowObjectInspector, partObjectInspector}));\n+  private List<String> createPartValue(String[] partKeys, Map<String, String> partSpec) {\n+    List<String> partValues = new ArrayList<String>();\n+    for (String key : partKeys) {\n+      partValues.add(partSpec.get(key));\n+    }\n+    return partValues;\n   }\n \n   private void getNextPath() throws Exception {\n@@ -290,7 +294,7 @@ private void getNextPath() throws Exception {\n       }\n \n       if (currPart != null) {\n-        setPrtnDesc();\n+        setPrtnDesc(currPart.getTableDesc(), currPart.getPartSpec());\n       }\n     }\n \n@@ -374,32 +378,38 @@ public void setupContext(Iterator<Path> iterPath, Iterator<PartitionDesc> iterPa\n       } else {\n         // hack, get the first.\n         List<PartitionDesc> listParts = work.getPartDesc();\n-        currPart = listParts.get(0);\n+        currPart = listParts.isEmpty() ? null : listParts.get(0);\n       }\n     }\n   }\n \n+  /**\n+   * returns output ObjectInspector, never null\n+   */\n   public ObjectInspector getOutputObjectInspector() throws HiveException {\n     try {\n       if (work.getTblDir() != null) {\n         TableDesc tbl = work.getTblDesc();\n         Deserializer serde = tbl.getDeserializerClass().newInstance();\n         serde.initialize(job, tbl.getProperties());\n         return serde.getObjectInspector();\n-      } else if (work.getPartDesc() != null) {\n-        List<PartitionDesc> listParts = work.getPartDesc();\n-        if(listParts.size() == 0) {\n-          return null;\n-        }\n-        currPart = listParts.get(0);\n-        serde = currPart.getTableDesc().getDeserializerClass().newInstance();\n-        serde.initialize(job, currPart.getTableDesc().getProperties());\n-        setPrtnDesc();\n-        currPart = null;\n-        return rowObjectInspector;\n+      }\n+      TableDesc tbl;\n+      Map<String, String> partSpec;\n+      List<PartitionDesc> listParts = work.getPartDesc();\n+      if (listParts == null || listParts.isEmpty()) {\n+        tbl = work.getTblDesc();\n+        partSpec = null;\n       } else {\n-        return null;\n+        currPart = listParts.get(0);\n+        tbl = currPart.getTableDesc();\n+        partSpec = currPart.getPartSpec();\n       }\n+      serde = tbl.getDeserializerClass().newInstance();\n+      serde.initialize(job, tbl.getProperties());\n+      setPrtnDesc(tbl, partSpec);\n+      currPart = null;\n+      return rowObjectInspector;\n     } catch (Exception e) {\n       throw new HiveException(\"Failed with exception \" + e.getMessage()\n           + org.apache.hadoop.util.StringUtils.stringifyException(e));", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java", "sha": "93125368589ba9a26a4624da5ae5b9cda2553902", "changes": 10, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -308,7 +308,7 @@ private void startForward(boolean inputFileChangeSenstive, String bigTableBucket\n \n       if (fetchOp.isEmptyTable()) {\n         //generate empty hashtable for empty table\n-        this.generateDummyHashTable(alias, bigTableBucket);\n+        this.generateDummyHashTable(alias, getFileName(bigTableBucket));\n         continue;\n       }\n \n@@ -381,12 +381,8 @@ private void initializeOperators(Map<FetchOperator, JobConf> fetchOpJobConfMap)\n       }\n       // initialize the forward operator\n       ObjectInspector objectInspector = fetchOp.getOutputObjectInspector();\n-      if (objectInspector != null) {\n-        forwardOp.initialize(jobConf, new ObjectInspector[] {objectInspector});\n-        l4j.info(\"fetchoperator for \" + entry.getKey() + \" initialized\");\n-      } else {\n-        fetchOp.setEmptyTable(true);\n-      }\n+      forwardOp.initialize(jobConf, new ObjectInspector[] {objectInspector});\n+      l4j.info(\"fetchoperator for \" + entry.getKey() + \" initialized\");\n     }\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java", "sha": "021494cb8411e54a7f388eba6a9d02f83612b30e", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -482,7 +482,10 @@ private void setUpFetchOpContext(FetchOperator fetchOp, String alias) {\n         .getBucketMatcherClass();\n     BucketMatcher bucketMatcher = (BucketMatcher) ReflectionUtils.newInstance(\n         bucketMatcherCls, null);\n-    this.getExecContext().setFileId(bucketMatcherCxt.getBucketFileNameMapping().get(currentInputFile));\n+    Integer bucketNum = bucketMatcherCxt.getBucketFileNameMapping().get(currentInputFile);\n+    if (bucketNum != null) {\n+      this.getExecContext().setFileId(bucketNum);\n+    }\n     LOG.info(\"set task id: \" + this.getExecContext().getFileId());\n \n     bucketMatcher.setAliasBucketFileNameMapping(bucketMatcherCxt", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketMapJoinOptimizer.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketMapJoinOptimizer.java", "sha": "d62e02ef0b5a9bdba2e59b8ab30d583ddb43d19f", "changes": 7, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketMapJoinOptimizer.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -238,6 +238,11 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n               aliasToPartitionBucketNumberMapping.put(alias, buckets);\n               aliasToPartitionBucketFileNamesMapping.put(alias, files);\n             }\n+          } else {\n+            if (!alias.equals(baseBigAlias)) {\n+              aliasToPartitionBucketNumberMapping.put(alias, Arrays.<Integer>asList());\n+              aliasToPartitionBucketFileNamesMapping.put(alias, new ArrayList<List<String>>());\n+            }\n           }\n         } else {\n           if (!checkBucketColumns(tbl.getBucketCols(), mjDecs, index)) {\n@@ -278,7 +283,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       // in the big table to bucket file names in small tables.\n       for (int j = 0; j < joinAliases.size(); j++) {\n         String alias = joinAliases.get(j);\n-        if(alias.equals(baseBigAlias)) {\n+        if (alias.equals(baseBigAlias)) {\n           continue;\n         }\n         for (List<String> names : aliasToPartitionBucketFileNamesMapping.get(alias)) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketMapJoinOptimizer.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java", "sha": "fd686d50b0754ea9aab95e98d630eb36f2bba60a", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -32,6 +32,7 @@\n import org.apache.hadoop.hive.ql.lib.NodeProcessor;\n import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n import org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.GenMapRedCtx;\n import org.apache.hadoop.hive.ql.parse.ParseContext;\n import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;\n@@ -103,7 +104,9 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n             confirmedPartns.addAll(tblSpec.partitions);\n           }\n           if (confirmedPartns.size() > 0) {\n-            PrunedPartitionList partList = new PrunedPartitionList(confirmedPartns, new HashSet<Partition>(), null);\n+            Table source = parseCtx.getQB().getMetaData().getTableForAlias(alias);\n+            PrunedPartitionList partList = new PrunedPartitionList(source, confirmedPartns,\n+                new HashSet<Partition>(), null);\n             GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currWork, false, ctx, partList);\n           } else { // non-partitioned table\n             GenMapRedUtils.setTaskPlan(currAliasId, currTopOp, currWork, false, ctx);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "sha": "25430cb8d901033cf2ae46c416f1f7fc952d6b99", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -815,9 +815,10 @@ public static void setTaskPlan(String alias_id,\n       assert localPlan.getAliasToFetchWork().get(alias_id) == null;\n       localPlan.getAliasToWork().put(alias_id, topOp);\n       if (tblDir == null) {\n+        tblDesc = Utilities.getTableDesc(partsList.getSourceTable());\n         localPlan.getAliasToFetchWork().put(\n             alias_id,\n-            new FetchWork(FetchWork.convertPathToStringArray(partDir), partDesc));\n+            new FetchWork(FetchWork.convertPathToStringArray(partDir), partDesc, tblDesc));\n       } else {\n         localPlan.getAliasToFetchWork().put(alias_id,\n             new FetchWork(tblDir.toString(), tblDesc));", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java", "sha": "43eef079cde87af3daaaf183b3e798e50834fc57", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -195,7 +195,8 @@ private static String genMapJoinLocalWork(MapredWork newWork, MapJoinOperator ma\n       }\n       // create fetchwork for partitioned table\n       if (fetchWork == null) {\n-        fetchWork = new FetchWork(partDir, partDesc);\n+        TableDesc table = newWork.getAliasToPartnInfo().get(alias).getTableDesc();\n+        fetchWork = new FetchWork(partDir, partDesc, table);\n       }\n       // set alias to fetch work\n       newLocalWork.getAliasToFetchWork().put(alias, fetchWork);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java", "sha": "049e3892952335a00d16d82d5f3d4b278be566f3", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -244,7 +244,7 @@ public static PrunedPartitionList prune(Table tab, ExprNodeDesc prunerExpr,\n     }\n \n     // Now return the set of partitions\n-    ret = new PrunedPartitionList(true_parts, unkn_parts, denied_parts);\n+    ret = new PrunedPartitionList(tab, true_parts, unkn_parts, denied_parts);\n     prunedPartitionsMap.put(key, ret);\n     return ret;\n   }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/parse/PrunedPartitionList.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/parse/PrunedPartitionList.java", "sha": "cefc274178224dd1501ebbd686ca91ee588fcad8", "changes": 12, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/PrunedPartitionList.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -23,11 +23,16 @@\n import java.util.Set;\n \n import org.apache.hadoop.hive.ql.metadata.Partition;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n \n /**\n  * The list of pruned partitions.\n  */\n public class PrunedPartitionList {\n+\n+  // source table\n+  private Table source;\n+\n   // confirmed partitions - satisfy the partition criteria\n   private Set<Partition> confirmedPartns;\n \n@@ -43,13 +48,18 @@\n    * @param unknownPartns\n    *          unknown partitions\n    */\n-  public PrunedPartitionList(Set<Partition> confirmedPartns,\n+  public PrunedPartitionList(Table source, Set<Partition> confirmedPartns,\n       Set<Partition> unknownPartns, Set<Partition> deniedPartns) {\n+    this.source = source;\n     this.confirmedPartns = confirmedPartns;\n     this.unknownPartns = unknownPartns;\n     this.deniedPartns = deniedPartns;\n   }\n \n+  public Table getSourceTable() {\n+    return source;\n+  }\n+\n   /**\n    * get confirmed partitions.\n    * ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/PrunedPartitionList.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "bc4e2c9327d4cd7f14f3dc7e0599385b26530dc7", "changes": 16, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -7051,7 +7051,8 @@ private void genMapRedTasks(QB qb) throws SemanticException {\n                 inputs.add(new ReadEntity(part));\n               }\n \n-              fetch = new FetchWork(listP, partP, qb.getParseInfo()\n+              TableDesc table = Utilities.getTableDesc(partsList.getSourceTable());\n+              fetch = new FetchWork(listP, partP, table, qb.getParseInfo()\n                   .getOuterQueryLimit());\n               noMapRed = true;\n             }\n@@ -7060,18 +7061,7 @@ private void genMapRedTasks(QB qb) throws SemanticException {\n       }\n \n       if (noMapRed) {\n-        if (fetch.getTblDesc() != null) {\n-          PlanUtils.configureInputJobPropertiesForStorageHandler(\n-            fetch.getTblDesc());\n-        } else if ( (fetch.getPartDesc() != null) && (!fetch.getPartDesc().isEmpty())){\n-            PartitionDesc pd0 = fetch.getPartDesc().get(0);\n-            TableDesc td = pd0.getTableDesc();\n-            if ((td != null)&&(td.getProperties() != null)\n-                && td.getProperties().containsKey(\n-                    org.apache.hadoop.hive.metastore.api.Constants.META_TABLE_STORAGE)){\n-              PlanUtils.configureInputJobPropertiesForStorageHandler(td);\n-            }\n-        }\n+        PlanUtils.configureInputJobPropertiesForStorageHandler(fetch.getTblDesc());\n         fetchTask = (FetchTask) TaskFactory.get(fetch, conf);\n         setFetchTask(fetchTask);\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java", "sha": "e5139585d8c8216fb64ec522296686aa0eb1159a", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -59,11 +59,13 @@ public FetchWork(String tblDir, TableDesc tblDesc, int limit) {\n     this.limit = limit;\n   }\n \n-  public FetchWork(List<String> partDir, List<PartitionDesc> partDesc) {\n-    this(partDir, partDesc, -1);\n+  public FetchWork(List<String> partDir, List<PartitionDesc> partDesc, TableDesc tblDesc) {\n+    this(partDir, partDesc, tblDesc, -1);\n   }\n \n-  public FetchWork(List<String> partDir, List<PartitionDesc> partDesc, int limit) {\n+  public FetchWork(List<String> partDir, List<PartitionDesc> partDesc,\n+      TableDesc tblDesc, int limit) {\n+    this.tblDesc = tblDesc;\n     this.partDir = new ArrayList<String>(partDir);\n     this.partDesc = new ArrayList<PartitionDesc>(partDesc);\n     this.limit = limit;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/FetchWork.java"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java", "sha": "4d5ac5708918201c592d1335b80beb9efff28474", "changes": 3, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -126,9 +126,6 @@ public void deriveExplainAttributes() {\n       bucketMapjoinContext.deriveBucketMapJoinMapping();\n     }\n     for (FetchWork fetchWork : aliasToFetchWork.values()) {\n-      if (fetchWork.getTblDesc() == null) {\n-        continue;\n-      }\n       PlanUtils.configureInputJobPropertiesForStorageHandler(\n         fetchWork.getTblDesc());\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java"}, {"additions": 26, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/test/queries/clientpositive/bucketmapjoin1.q", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/test/queries/clientpositive/bucketmapjoin1.q", "sha": "2bd8e1e2e5d987457d85a1a5b5f44fc74431077e", "changes": 28, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/bucketmapjoin1.q?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -1,14 +1,38 @@\n CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;\n+\n+CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;\n+\n+CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;\n+\n+set hive.optimize.bucketmapjoin = true;\n+\n+-- empty partitions (HIVE-3205)\n+explain extended\n+select /*+mapjoin(b)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\";\n+\n+select /*+mapjoin(b)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\";\n+\n+explain extended\n+select /*+mapjoin(a)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\";\n+\n+select /*+mapjoin(a)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\";\n+\n load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;\n load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;\n \n-CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;\n load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n load data local inpath '../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n load data local inpath '../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');\n \n-CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;\n load data local inpath '../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');\n load data local inpath '../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');\n ", "filename": "ql/src/test/queries/clientpositive/bucketmapjoin1.q"}, {"additions": 28, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/test/queries/clientpositive/smb_mapjoin9.q", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/test/queries/clientpositive/smb_mapjoin9.q", "sha": "3d47f2b05fcca35193b8bf494b107c453693e7dd", "changes": 33, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/smb_mapjoin9.q?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -1,17 +1,40 @@\n create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;\n create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;\n \n+set hive.optimize.bucketmapjoin = true;\n+set hive.optimize.bucketmapjoin.sortedmerge = true;\n+set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n+\n+-- empty partitions (HIVE-3205)\n+explain extended\n+SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;\n+\n+SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;\n+\n+explain extended\n+SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;\n+\n+SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;\n+\n set hive.enforce.bucketing = true;\n set hive.enforce.sorting = true;\n \n insert overwrite table hive_test_smb_bucket1 partition (ds='2010-10-15') select key, value from src;\n insert overwrite table hive_test_smb_bucket2 partition (ds='2010-10-15') select key, value from src;\n \n-set hive.optimize.bucketmapjoin = true;\n-set hive.optimize.bucketmapjoin.sortedmerge = true;\n-set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n-\n-explain \n+explain\n create table smb_mapjoin9_results as\n SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n FROM hive_test_smb_bucket1 a JOIN ", "filename": "ql/src/test/queries/clientpositive/smb_mapjoin9.q"}, {"additions": 262, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/test/results/clientpositive/bucketmapjoin1.q.out", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/test/results/clientpositive/bucketmapjoin1.q.out", "sha": "2cf27381a2b74583e1e886f853f6451d5a53ef0b", "changes": 272, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin1.q.out?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -3,6 +3,268 @@ PREHOOK: type: CREATETABLE\n POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n POSTHOOK: type: CREATETABLE\n POSTHOOK: Output: default@srcbucket_mapjoin\n+PREHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@srcbucket_mapjoin_part\n+PREHOOK: query: CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@srcbucket_mapjoin_part_2\n+PREHOOK: query: -- empty partitions (HIVE-3205)\n+explain extended\n+select /*+mapjoin(b)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- empty partitions (HIVE-3205)\n+explain extended\n+select /*+mapjoin(b)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME srcbucket_mapjoin_part) a) (TOK_TABREF (TOK_TABNAME srcbucket_mapjoin_part_2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST b))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) value)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) value))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL b) ds) \"2008-04-08\"))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-3 is a root stage\n+  Stage-1 depends on stages: Stage-3\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-3\n+    Map Reduce Local Work\n+      Alias -> Map Local Tables:\n+        b \n+          Fetch Operator\n+            limit: -1\n+      Alias -> Map Local Operator Tree:\n+        b \n+          TableScan\n+            alias: b\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate:\n+                  expr: (ds = '2008-04-08')\n+                  type: boolean\n+              HashTable Sink Operator\n+                condition expressions:\n+                  0 {key} {value}\n+                  1 {value} {ds}\n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[key]]\n+                  1 [Column[key]]\n+                Position of Big Table: 0\n+      Bucket Mapjoin Context:\n+          Alias Bucket Base File Name Mapping:\n+            b {}\n+          Alias Bucket File Name Mapping:\n+            b {}\n+\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        a \n+          TableScan\n+            alias: a\n+            GatherStats: false\n+            Map Join Operator\n+              condition map:\n+                   Inner Join 0 to 1\n+              condition expressions:\n+                0 {key} {value}\n+                1 {value} {ds}\n+              handleSkewJoin: false\n+              keys:\n+                0 [Column[key]]\n+                1 [Column[key]]\n+              outputColumnNames: _col0, _col1, _col6, _col7\n+              Position of Big Table: 0\n+              Select Operator\n+                expressions:\n+                      expr: _col0\n+                      type: int\n+                      expr: _col1\n+                      type: string\n+                      expr: _col6\n+                      type: string\n+                      expr: _col7\n+                      type: string\n+                outputColumnNames: _col0, _col1, _col6, _col7\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: int\n+                        expr: _col1\n+                        type: string\n+                        expr: _col6\n+                        type: string\n+                  outputColumnNames: _col0, _col1, _col2\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2\n+                          columns.types int:string:string\n+                          escape.delim \\\n+                          serialization.format 1\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n+      Local Work:\n+        Map Reduce Local Work\n+      Needs Tagging: false\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: select /*+mapjoin(b)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+PREHOOK: type: QUERY\n+#### A masked pattern was here ####\n+POSTHOOK: query: select /*+mapjoin(b)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####\n+PREHOOK: query: explain extended\n+select /*+mapjoin(a)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain extended\n+select /*+mapjoin(a)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME srcbucket_mapjoin_part) a) (TOK_TABREF (TOK_TABNAME srcbucket_mapjoin_part_2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST a))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) value)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) value))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL b) ds) \"2008-04-08\"))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-3 is a root stage\n+  Stage-1 depends on stages: Stage-3\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-3\n+    Map Reduce Local Work\n+      Alias -> Map Local Tables:\n+        a \n+          Fetch Operator\n+            limit: -1\n+      Alias -> Map Local Operator Tree:\n+        a \n+          TableScan\n+            alias: a\n+            GatherStats: false\n+            HashTable Sink Operator\n+              condition expressions:\n+                0 {key} {value}\n+                1 {value} {ds}\n+              handleSkewJoin: false\n+              keys:\n+                0 [Column[key]]\n+                1 [Column[key]]\n+              Position of Big Table: 1\n+      Bucket Mapjoin Context:\n+          Alias Bucket Base File Name Mapping:\n+            a {}\n+          Alias Bucket File Name Mapping:\n+            a {}\n+\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        b \n+          TableScan\n+            alias: b\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate:\n+                  expr: (ds = '2008-04-08')\n+                  type: boolean\n+              Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {key} {value}\n+                  1 {value} {ds}\n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[key]]\n+                  1 [Column[key]]\n+                outputColumnNames: _col0, _col1, _col6, _col7\n+                Position of Big Table: 1\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: int\n+                        expr: _col1\n+                        type: string\n+                        expr: _col6\n+                        type: string\n+                        expr: _col7\n+                        type: string\n+                  outputColumnNames: _col0, _col1, _col6, _col7\n+                  Select Operator\n+                    expressions:\n+                          expr: _col0\n+                          type: int\n+                          expr: _col1\n+                          type: string\n+                          expr: _col6\n+                          type: string\n+                    outputColumnNames: _col0, _col1, _col2\n+                    File Output Operator\n+                      compressed: false\n+                      GlobalTableId: 0\n+#### A masked pattern was here ####\n+                      NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          properties:\n+                            columns _col0,_col1,_col2\n+                            columns.types int:string:string\n+                            escape.delim \\\n+                            serialization.format 1\n+                      TotalFiles: 1\n+                      GatherStats: false\n+                      MultiFileSpray: false\n+      Local Work:\n+        Map Reduce Local Work\n+      Needs Tagging: false\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: select /*+mapjoin(a)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+PREHOOK: type: QUERY\n+#### A masked pattern was here ####\n+POSTHOOK: query: select /*+mapjoin(a)*/ a.key, a.value, b.value\n+from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b\n+on a.key=b.key where b.ds=\"2008-04-08\"\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####\n PREHOOK: query: load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin\n PREHOOK: type: LOAD\n PREHOOK: Output: default@srcbucket_mapjoin\n@@ -15,11 +277,6 @@ PREHOOK: Output: default@srcbucket_mapjoin\n POSTHOOK: query: load data local inpath '../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin\n POSTHOOK: type: LOAD\n POSTHOOK: Output: default@srcbucket_mapjoin\n-PREHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE\n-PREHOOK: type: CREATETABLE\n-POSTHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE\n-POSTHOOK: type: CREATETABLE\n-POSTHOOK: Output: default@srcbucket_mapjoin_part\n PREHOOK: query: load data local inpath '../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n PREHOOK: type: LOAD\n PREHOOK: Output: default@srcbucket_mapjoin_part\n@@ -45,11 +302,6 @@ PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n POSTHOOK: query: load data local inpath '../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')\n POSTHOOK: type: LOAD\n POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08\n-PREHOOK: query: CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n-PREHOOK: type: CREATETABLE\n-POSTHOOK: query: CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE\n-POSTHOOK: type: CREATETABLE\n-POSTHOOK: Output: default@srcbucket_mapjoin_part_2\n PREHOOK: query: load data local inpath '../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08')\n PREHOOK: type: LOAD\n PREHOOK: Output: default@srcbucket_mapjoin_part_2", "filename": "ql/src/test/results/clientpositive/bucketmapjoin1.q.out"}, {"additions": 212, "raw_url": "https://github.com/apache/hive/raw/997a75312914db9d7462a699bc7692431a798434/ql/src/test/results/clientpositive/smb_mapjoin9.q.out", "blob_url": "https://github.com/apache/hive/blob/997a75312914db9d7462a699bc7692431a798434/ql/src/test/results/clientpositive/smb_mapjoin9.q.out", "sha": "113d5a9245f0d9f6795ff8a3e5792db732d520a4", "changes": 214, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin9.q.out?ref=997a75312914db9d7462a699bc7692431a798434", "patch": "@@ -8,6 +8,216 @@ PREHOOK: type: CREATETABLE\n POSTHOOK: query: create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets\n POSTHOOK: type: CREATETABLE\n POSTHOOK: Output: default@hive_test_smb_bucket2\n+PREHOOK: query: -- empty partitions (HIVE-3205)\n+explain extended\n+SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- empty partitions (HIVE-3205)\n+explain extended\n+SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME hive_test_smb_bucket1) a) (TOK_TABREF (TOK_TABNAME hive_test_smb_bucket2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST b))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) key) k1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) value)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) ds)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) key) k2)) (TOK_WHERE (and (and (= (. (TOK_TABLE_OR_COL a) ds) '2010-10-15') (= (. (TOK_TABLE_OR_COL b) ds) '2010-10-15')) (TOK_FUNCTION TOK_ISNOTNULL (. (TOK_TABLE_OR_COL b) key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        a \n+          TableScan\n+            alias: a\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate:\n+                  expr: ((ds = '2010-10-15') and key is not null)\n+                  type: boolean\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {key} {ds}\n+                  1 {key} {value} {ds}\n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[key]]\n+                  1 [Column[key]]\n+                outputColumnNames: _col0, _col2, _col5, _col6, _col7\n+                Position of Big Table: 0\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: int\n+                        expr: _col2\n+                        type: string\n+                        expr: _col5\n+                        type: int\n+                        expr: _col6\n+                        type: string\n+                        expr: _col7\n+                        type: string\n+                  outputColumnNames: _col0, _col2, _col5, _col6, _col7\n+                  Select Operator\n+                    expressions:\n+                          expr: _col5\n+                          type: int\n+                          expr: _col6\n+                          type: string\n+                          expr: _col7\n+                          type: string\n+                          expr: _col0\n+                          type: int\n+                    outputColumnNames: _col0, _col1, _col2, _col3\n+                    File Output Operator\n+                      compressed: false\n+                      GlobalTableId: 0\n+#### A masked pattern was here ####\n+                      NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          properties:\n+                            columns _col0,_col1,_col2,_col3\n+                            columns.types int:string:string:int\n+                            escape.delim \\\n+                            serialization.format 1\n+                      TotalFiles: 1\n+                      GatherStats: false\n+                      MultiFileSpray: false\n+      Needs Tagging: false\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+PREHOOK: type: QUERY\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####\n+PREHOOK: query: explain extended\n+SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain extended\n+SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME hive_test_smb_bucket1) a) (TOK_TABREF (TOK_TABNAME hive_test_smb_bucket2) b) (= (. (TOK_TABLE_OR_COL a) key) (. (TOK_TABLE_OR_COL b) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_HINTLIST (TOK_HINT TOK_MAPJOIN (TOK_HINTARGLIST a))) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) key) k1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) value)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) ds)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) key) k2)) (TOK_WHERE (and (and (= (. (TOK_TABLE_OR_COL a) ds) '2010-10-15') (= (. (TOK_TABLE_OR_COL b) ds) '2010-10-15')) (TOK_FUNCTION TOK_ISNOTNULL (. (TOK_TABLE_OR_COL b) key))))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        b \n+          TableScan\n+            alias: b\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate:\n+                  expr: ((ds = '2010-10-15') and key is not null)\n+                  type: boolean\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {key} {ds}\n+                  1 {key} {value} {ds}\n+                handleSkewJoin: false\n+                keys:\n+                  0 [Column[key]]\n+                  1 [Column[key]]\n+                outputColumnNames: _col0, _col2, _col5, _col6, _col7\n+                Position of Big Table: 1\n+                Select Operator\n+                  expressions:\n+                        expr: _col0\n+                        type: int\n+                        expr: _col2\n+                        type: string\n+                        expr: _col5\n+                        type: int\n+                        expr: _col6\n+                        type: string\n+                        expr: _col7\n+                        type: string\n+                  outputColumnNames: _col0, _col2, _col5, _col6, _col7\n+                  Select Operator\n+                    expressions:\n+                          expr: _col5\n+                          type: int\n+                          expr: _col6\n+                          type: string\n+                          expr: _col7\n+                          type: string\n+                          expr: _col0\n+                          type: int\n+                    outputColumnNames: _col0, _col1, _col2, _col3\n+                    File Output Operator\n+                      compressed: false\n+                      GlobalTableId: 0\n+#### A masked pattern was here ####\n+                      NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          properties:\n+                            columns _col0,_col1,_col2,_col3\n+                            columns.types int:string:string:int\n+                            escape.delim \\\n+                            serialization.format 1\n+                      TotalFiles: 1\n+                      GatherStats: false\n+                      MultiFileSpray: false\n+      Needs Tagging: false\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+\n+PREHOOK: query: SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+PREHOOK: type: QUERY\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2\n+FROM hive_test_smb_bucket1 a JOIN\n+hive_test_smb_bucket2 b\n+ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####\n PREHOOK: query: insert overwrite table hive_test_smb_bucket1 partition (ds='2010-10-15') select key, value from src\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n@@ -30,14 +240,14 @@ POSTHOOK: Lineage: hive_test_smb_bucket1 PARTITION(ds=2010-10-15).key EXPRESSION\n POSTHOOK: Lineage: hive_test_smb_bucket1 PARTITION(ds=2010-10-15).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: hive_test_smb_bucket2 PARTITION(ds=2010-10-15).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: hive_test_smb_bucket2 PARTITION(ds=2010-10-15).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n-PREHOOK: query: explain \n+PREHOOK: query: explain\n create table smb_mapjoin9_results as\n SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n FROM hive_test_smb_bucket1 a JOIN \n hive_test_smb_bucket2 b\n ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL\n PREHOOK: type: CREATETABLE_AS_SELECT\n-POSTHOOK: query: explain \n+POSTHOOK: query: explain\n create table smb_mapjoin9_results as\n SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2\n FROM hive_test_smb_bucket1 a JOIN ", "filename": "ql/src/test/results/clientpositive/smb_mapjoin9.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/5aee2b055aa6ed70d9a46badce9e3ed1596da09e", "parent": "https://github.com/apache/hive/commit/3e11b3ddecb545df119356557ed10e1228662402", "message": "HIVE-2956 Provide error message when using UDAF in the place of UDF instead of throwing NPE (navis via kevinwilfong)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1336284 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_267", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java", "blob_url": "https://github.com/apache/hive/blob/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java", "sha": "9815b5782d6b6eba2583f4332265751b57e3cc1a", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java?ref=5aee2b055aa6ed70d9a46badce9e3ed1596da09e", "patch": "@@ -130,6 +130,7 @@\n   UDTF_NO_DISTRIBUTE_BY(\"DISTRUBTE BY is not supported with a UDTF in the SELECT clause\"),\n   UDTF_INVALID_LOCATION(\"UDTF's are not supported outside the SELECT clause, nor nested \"\n       + \"in expressions\"),\n+  UDAF_INVALID_LOCATION(\"Not yet supported place for UDAF\"),\n   UDTF_LATERAL_VIEW(\"UDTF's cannot be in a select expression when there is a lateral view\"),\n   UDTF_ALIAS_MISMATCH(\"The number of aliases supplied in the AS clause does not match the \"\n       + \"number of columns output by the UDTF\"),", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java"}, {"additions": 9, "raw_url": "https://github.com/apache/hive/raw/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java", "sha": "81dd80716d814cdd9ce39dd0d738c398f6eb07d2", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java?ref=5aee2b055aa6ed70d9a46badce9e3ed1596da09e", "patch": "@@ -684,6 +684,15 @@ static ExprNodeDesc getXpathOrFuncExprNodeDesc(ASTNode expr,\n         if (fi.getGenericUDTF() != null) {\n           throw new SemanticException(ErrorMsg.UDTF_INVALID_LOCATION.getMsg());\n         }\n+        // UDAF in filter condition, group-by caluse, param of funtion, etc.\n+        if (fi.getGenericUDAFResolver() != null) {\n+          if (isFunction) {\n+            throw new SemanticException(ErrorMsg.UDAF_INVALID_LOCATION.\n+                getMsg((ASTNode) expr.getChild(0)));\n+          } else {\n+            throw new SemanticException(ErrorMsg.UDAF_INVALID_LOCATION.getMsg(expr));\n+          }\n+        }\n         if (!ctx.getAllowStatefulFunctions() && (fi.getGenericUDF() != null)) {\n           if (FunctionRegistry.isStateful(fi.getGenericUDF())) {\n             throw new SemanticException(", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/queries/clientnegative/udaf_invalid_place.q", "blob_url": "https://github.com/apache/hive/blob/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/queries/clientnegative/udaf_invalid_place.q", "sha": "f37ce72ae41970fbc16fcc4bd2aa7b91cfa80119", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/udaf_invalid_place.q?ref=5aee2b055aa6ed70d9a46badce9e3ed1596da09e", "patch": "@@ -0,0 +1 @@\n+select distinct key, sum(key) from src;", "filename": "ql/src/test/queries/clientnegative/udaf_invalid_place.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/queries/clientnegative/udtf_invalid_place.q", "blob_url": "https://github.com/apache/hive/blob/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/queries/clientnegative/udtf_invalid_place.q", "sha": "ab84a801e9ed6cfb7c88f4aab822d8bb190aa1ca", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/udtf_invalid_place.q?ref=5aee2b055aa6ed70d9a46badce9e3ed1596da09e", "patch": "@@ -0,0 +1 @@\n+select distinct key, explode(key) from src;", "filename": "ql/src/test/queries/clientnegative/udtf_invalid_place.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/results/clientnegative/udaf_invalid_place.q.out", "blob_url": "https://github.com/apache/hive/blob/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/results/clientnegative/udaf_invalid_place.q.out", "sha": "f663f4db55a35209a5f49b1755b6305b8308e6aa", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/udaf_invalid_place.q.out?ref=5aee2b055aa6ed70d9a46badce9e3ed1596da09e", "patch": "@@ -0,0 +1 @@\n+FAILED: Error in semantic analysis: Line 1:21 Not yet supported place for UDAF 'sum'", "filename": "ql/src/test/results/clientnegative/udaf_invalid_place.q.out"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/results/clientnegative/udtf_invalid_place.q.out", "blob_url": "https://github.com/apache/hive/blob/5aee2b055aa6ed70d9a46badce9e3ed1596da09e/ql/src/test/results/clientnegative/udtf_invalid_place.q.out", "sha": "90d498b8b2f10a2b2bd5f3242c6f199a73bda7b9", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/udtf_invalid_place.q.out?ref=5aee2b055aa6ed70d9a46badce9e3ed1596da09e", "patch": "@@ -0,0 +1 @@\n+FAILED: Error in semantic analysis: UDTF's are not supported outside the SELECT clause, nor nested in expressions", "filename": "ql/src/test/results/clientnegative/udtf_invalid_place.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/208a217dce6484043bea60cd9274fd6979b801e4", "parent": "https://github.com/apache/hive/commit/d4f2881b31b9457e393b19ea255e9e323b0ab565", "message": "HIVE-2920 : TestStatsPublisherEnhanced throws NPE on JDBC connection failure (Carl Steinbach via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1308731 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_268", "file": [{"additions": 11, "raw_url": "https://github.com/apache/hive/raw/208a217dce6484043bea60cd9274fd6979b801e4/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java", "blob_url": "https://github.com/apache/hive/blob/208a217dce6484043bea60cd9274fd6979b801e4/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java", "sha": "99176683b06cbde08026289702872232307fd11c", "changes": 21, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java?ref=208a217dce6484043bea60cd9274fd6979b801e4", "patch": "@@ -43,24 +43,25 @@\n \n   public TestStatsPublisherEnhanced(String name) {\n     super(name);\n-  }\n-\n-  @Override\n-  protected void setUp() {\n     conf = new JobConf(TestStatsPublisherEnhanced.class);\n-\n     conf.set(\"hive.stats.dbclass\", \"jdbc:derby\");\n-\n     statsImplementationClass = HiveConf.getVar(conf, HiveConf.ConfVars.HIVESTATSDBCLASS);\n     StatsFactory.setImplementation(statsImplementationClass, conf);\n+  }\n \n+  @Override\n+  protected void setUp() {\n     stats = new HashMap<String, String>();\n-    StatsAggregator sa = StatsFactory.getStatsAggregator();\n-    sa.connect(conf);\n-    sa.cleanUp(\"file_0\");\n-    sa.closeConnection();\n   }\n \n+  @Override\n+  protected void tearDown() {\n+    StatsAggregator sa = StatsFactory.getStatsAggregator();\n+    assertNotNull(sa);\n+    assertTrue(sa.connect(conf));\n+    assertTrue(sa.cleanUp(\"file_0\"));\n+    assertTrue(sa.closeConnection());\n+  }\n \n   private void fillStatMap(String numRows, String rawDataSize) {\n     stats.clear();", "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/b25799576ad4b87472b5dcdd96824500c8662c78", "parent": "https://github.com/apache/hive/commit/81f85b9930f3b49433a04a0ffa6b1e3113eb8d40", "message": "HIVE-2754 NPE in union with lateral view\n(Yongqiang He via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1236479 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_269", "file": [{"additions": 11, "raw_url": "https://github.com/apache/hive/raw/b25799576ad4b87472b5dcdd96824500c8662c78/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java", "blob_url": "https://github.com/apache/hive/blob/b25799576ad4b87472b5dcdd96824500c8662c78/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java", "sha": "a7a45c6b5160b97108de7da72c922ef9652195e4", "changes": 12, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java?ref=b25799576ad4b87472b5dcdd96824500c8662c78", "patch": "@@ -63,7 +63,17 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,\n     Map<Operator<? extends Serializable>, GenMapRedCtx> mapCurrCtx = ctx\n         .getMapCurrCtx();\n     GenMapRedCtx mapredCtx = mapCurrCtx.get(ctx.getCurrUnionOp());\n-    Task<? extends Serializable> unionTask = mapredCtx.getCurrTask();\n+    Task<? extends Serializable> unionTask = null;\n+    if(mapredCtx != null) {\n+      unionTask = mapredCtx.getCurrTask();\n+    } else {\n+      Operator<? extends Serializable> topOp = ctx.getCurrTopOp();\n+      if(topOp == null) {\n+        return null;\n+      }\n+      unionTask = ctx.getCurrTask();\n+    }\n+\n     MapredWork plan = (MapredWork) unionTask.getWork();\n     HashMap<Operator<? extends Serializable>, Task<? extends Serializable>> opTaskMap = ctx\n         .getOpTaskMap();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRRedSink3.java"}, {"additions": 43, "raw_url": "https://github.com/apache/hive/raw/b25799576ad4b87472b5dcdd96824500c8662c78/ql/src/test/queries/clientpositive/union_lateralview.q", "blob_url": "https://github.com/apache/hive/blob/b25799576ad4b87472b5dcdd96824500c8662c78/ql/src/test/queries/clientpositive/union_lateralview.q", "sha": "012296d7c3a51d0ca980caaca0e6721f644e861e", "changes": 43, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/union_lateralview.q?ref=b25799576ad4b87472b5dcdd96824500c8662c78", "patch": "@@ -0,0 +1,43 @@\n+\n+create table test_union_lateral_view(key int, arr_ele int, value string);\n+\n+EXPLAIN \n+INSERT OVERWRITE TABLE test_union_lateral_view\n+SELECT b.key, d.arr_ele, d.value\n+FROM (\n+ SELECT c.arr_ele as arr_ele, a.key as key, a.value as value\n+ FROM (\n+   SELECT key, value, array(1,2,3) as arr\n+   FROM src\n+\n+   UNION ALL\n+   \n+   SELECT key, value, array(1,2,3) as arr\n+   FROM srcpart\n+   WHERE ds = '2008-04-08' and hr='12'\n+ ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele\n+) d\n+LEFT OUTER JOIN src b\n+ON d.key = b.key\n+;\n+\n+INSERT OVERWRITE TABLE test_union_lateral_view\n+SELECT b.key, d.arr_ele, d.value\n+FROM (\n+ SELECT c.arr_ele as arr_ele, a.key as key, a.value as value\n+ FROM (\n+   SELECT key, value, array(1,2,3) as arr\n+   FROM src\n+\n+   UNION ALL\n+   \n+   SELECT key, value, array(1,2,3) as arr\n+   FROM srcpart\n+   WHERE ds = '2008-04-08' and hr='12'\n+ ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele\n+) d\n+LEFT OUTER JOIN src b\n+ON d.key = b.key\n+;\n+\n+select key, arr_ele, value from test_union_lateral_view order by key, arr_ele limit 20;", "filename": "ql/src/test/queries/clientpositive/union_lateralview.q"}, {"additions": 342, "raw_url": "https://github.com/apache/hive/raw/b25799576ad4b87472b5dcdd96824500c8662c78/ql/src/test/results/clientpositive/union_lateralview.q.out", "blob_url": "https://github.com/apache/hive/blob/b25799576ad4b87472b5dcdd96824500c8662c78/ql/src/test/results/clientpositive/union_lateralview.q.out", "sha": "54bfeb78bd32eda7db7cde8ad05c3a3e2dd566df", "changes": 342, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union_lateralview.q.out?ref=b25799576ad4b87472b5dcdd96824500c8662c78", "patch": "@@ -0,0 +1,342 @@\n+PREHOOK: query: create table test_union_lateral_view(key int, arr_ele int, value string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: create table test_union_lateral_view(key int, arr_ele int, value string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@test_union_lateral_view\n+PREHOOK: query: EXPLAIN \n+INSERT OVERWRITE TABLE test_union_lateral_view\n+SELECT b.key, d.arr_ele, d.value\n+FROM (\n+ SELECT c.arr_ele as arr_ele, a.key as key, a.value as value\n+ FROM (\n+   SELECT key, value, array(1,2,3) as arr\n+   FROM src\n+\n+   UNION ALL\n+   \n+   SELECT key, value, array(1,2,3) as arr\n+   FROM srcpart\n+   WHERE ds = '2008-04-08' and hr='12'\n+ ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele\n+) d\n+LEFT OUTER JOIN src b\n+ON d.key = b.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN \n+INSERT OVERWRITE TABLE test_union_lateral_view\n+SELECT b.key, d.arr_ele, d.value\n+FROM (\n+ SELECT c.arr_ele as arr_ele, a.key as key, a.value as value\n+ FROM (\n+   SELECT key, value, array(1,2,3) as arr\n+   FROM src\n+\n+   UNION ALL\n+   \n+   SELECT key, value, array(1,2,3) as arr\n+   FROM srcpart\n+   WHERE ds = '2008-04-08' and hr='12'\n+ ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele\n+) d\n+LEFT OUTER JOIN src b\n+ON d.key = b.key\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_LATERAL_VIEW (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION EXPLODE (TOK_TABLE_OR_COL arr)) arr_ele (TOK_TABALIAS c))) (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_FUNCTION array 1 2 3) arr)))) (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME srcpart))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_FUNCTION array 1 2 3) arr)) (TOK_WHERE (and (= (TOK_TABLE_OR_COL ds) '2008-04-08') (= (TOK_TABLE_OR_COL hr) '12')))))) a))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL c) arr_ele) arr_ele) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) key) key) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) value) value)))) d) (TOK_TABREF (TOK_TABNAME src) b) (= (. (TOK_TABLE_OR_COL d) key) (. (TOK_TABLE_OR_COL b) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME test_union_lateral_view))) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL d) arr_ele)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL d) value)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+  Stage-2 depends on stages: Stage-0\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        b \n+          TableScan\n+            alias: b\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: key\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: key\n+                    type: string\n+              tag: 1\n+              value expressions:\n+                    expr: key\n+                    type: string\n+        d-subquery1:a-subquery1:src \n+          TableScan\n+            alias: src\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+                    expr: array(1,2,3)\n+                    type: array<int>\n+              outputColumnNames: _col0, _col1, _col2\n+              Union\n+                Lateral View Forward\n+                  Select Operator\n+                    SELECT * : (no compute)\n+                    Lateral View Join Operator\n+                      outputColumnNames: _col0, _col1, _col2, _col3\n+                      Select Operator\n+                        expressions:\n+                              expr: _col3\n+                              type: int\n+                              expr: _col0\n+                              type: string\n+                              expr: _col1\n+                              type: string\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions:\n+                                expr: _col1\n+                                type: string\n+                          sort order: +\n+                          Map-reduce partition columns:\n+                                expr: _col1\n+                                type: string\n+                          tag: 0\n+                          value expressions:\n+                                expr: _col0\n+                                type: int\n+                                expr: _col2\n+                                type: string\n+                  Select Operator\n+                    expressions:\n+                          expr: _col2\n+                          type: array<int>\n+                    outputColumnNames: _col0\n+                    UDTF Operator\n+                      function name: explode\n+                      Lateral View Join Operator\n+                        outputColumnNames: _col0, _col1, _col2, _col3\n+                        Select Operator\n+                          expressions:\n+                                expr: _col3\n+                                type: int\n+                                expr: _col0\n+                                type: string\n+                                expr: _col1\n+                                type: string\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions:\n+                                  expr: _col1\n+                                  type: string\n+                            sort order: +\n+                            Map-reduce partition columns:\n+                                  expr: _col1\n+                                  type: string\n+                            tag: 0\n+                            value expressions:\n+                                  expr: _col0\n+                                  type: int\n+                                  expr: _col2\n+                                  type: string\n+        d-subquery2:a-subquery2:srcpart \n+          TableScan\n+            alias: srcpart\n+            Select Operator\n+              expressions:\n+                    expr: key\n+                    type: string\n+                    expr: value\n+                    type: string\n+                    expr: array(1,2,3)\n+                    type: array<int>\n+              outputColumnNames: _col0, _col1, _col2\n+              Union\n+                Lateral View Forward\n+                  Select Operator\n+                    SELECT * : (no compute)\n+                    Lateral View Join Operator\n+                      outputColumnNames: _col0, _col1, _col2, _col3\n+                      Select Operator\n+                        expressions:\n+                              expr: _col3\n+                              type: int\n+                              expr: _col0\n+                              type: string\n+                              expr: _col1\n+                              type: string\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions:\n+                                expr: _col1\n+                                type: string\n+                          sort order: +\n+                          Map-reduce partition columns:\n+                                expr: _col1\n+                                type: string\n+                          tag: 0\n+                          value expressions:\n+                                expr: _col0\n+                                type: int\n+                                expr: _col2\n+                                type: string\n+                  Select Operator\n+                    expressions:\n+                          expr: _col2\n+                          type: array<int>\n+                    outputColumnNames: _col0\n+                    UDTF Operator\n+                      function name: explode\n+                      Lateral View Join Operator\n+                        outputColumnNames: _col0, _col1, _col2, _col3\n+                        Select Operator\n+                          expressions:\n+                                expr: _col3\n+                                type: int\n+                                expr: _col0\n+                                type: string\n+                                expr: _col1\n+                                type: string\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions:\n+                                  expr: _col1\n+                                  type: string\n+                            sort order: +\n+                            Map-reduce partition columns:\n+                                  expr: _col1\n+                                  type: string\n+                            tag: 0\n+                            value expressions:\n+                                  expr: _col0\n+                                  type: int\n+                                  expr: _col2\n+                                  type: string\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Left Outer Join0 to 1\n+          condition expressions:\n+            0 {VALUE._col0} {VALUE._col2}\n+            1 {VALUE._col0}\n+          handleSkewJoin: false\n+          outputColumnNames: _col0, _col2, _col3\n+          Select Operator\n+            expressions:\n+                  expr: _col3\n+                  type: string\n+                  expr: _col0\n+                  type: int\n+                  expr: _col2\n+                  type: string\n+            outputColumnNames: _col0, _col1, _col2\n+            Select Operator\n+              expressions:\n+                    expr: UDFToInteger(_col0)\n+                    type: int\n+                    expr: _col1\n+                    type: int\n+                    expr: _col2\n+                    type: string\n+              outputColumnNames: _col0, _col1, _col2\n+              File Output Operator\n+                compressed: false\n+                GlobalTableId: 1\n+                table:\n+                    input format: org.apache.hadoop.mapred.TextInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    name: default.test_union_lateral_view\n+\n+  Stage: Stage-0\n+    Move Operator\n+      tables:\n+          replace: true\n+          table:\n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test_union_lateral_view\n+\n+  Stage: Stage-2\n+    Stats-Aggr Operator\n+\n+\n+PREHOOK: query: INSERT OVERWRITE TABLE test_union_lateral_view\n+SELECT b.key, d.arr_ele, d.value\n+FROM (\n+ SELECT c.arr_ele as arr_ele, a.key as key, a.value as value\n+ FROM (\n+   SELECT key, value, array(1,2,3) as arr\n+   FROM src\n+\n+   UNION ALL\n+   \n+   SELECT key, value, array(1,2,3) as arr\n+   FROM srcpart\n+   WHERE ds = '2008-04-08' and hr='12'\n+ ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele\n+) d\n+LEFT OUTER JOIN src b\n+ON d.key = b.key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12\n+PREHOOK: Output: default@test_union_lateral_view\n+POSTHOOK: query: INSERT OVERWRITE TABLE test_union_lateral_view\n+SELECT b.key, d.arr_ele, d.value\n+FROM (\n+ SELECT c.arr_ele as arr_ele, a.key as key, a.value as value\n+ FROM (\n+   SELECT key, value, array(1,2,3) as arr\n+   FROM src\n+\n+   UNION ALL\n+   \n+   SELECT key, value, array(1,2,3) as arr\n+   FROM srcpart\n+   WHERE ds = '2008-04-08' and hr='12'\n+ ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele\n+) d\n+LEFT OUTER JOIN src b\n+ON d.key = b.key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12\n+POSTHOOK: Output: default@test_union_lateral_view\n+POSTHOOK: Lineage: test_union_lateral_view.arr_ele EXPRESSION []\n+POSTHOOK: Lineage: test_union_lateral_view.key EXPRESSION [(src)b.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: test_union_lateral_view.value EXPRESSION [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]\n+PREHOOK: query: select key, arr_ele, value from test_union_lateral_view order by key, arr_ele limit 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test_union_lateral_view\n+#### A masked pattern was here ####\n+POSTHOOK: query: select key, arr_ele, value from test_union_lateral_view order by key, arr_ele limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test_union_lateral_view\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: test_union_lateral_view.arr_ele EXPRESSION []\n+POSTHOOK: Lineage: test_union_lateral_view.key EXPRESSION [(src)b.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: test_union_lateral_view.value EXPRESSION [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), (src)src.FieldSchema(name:key, type:string, comment:default), ]\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t1\tval_0\n+0\t2\tval_0\n+0\t2\tval_0", "filename": "ql/src/test/results/clientpositive/union_lateralview.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/f4a6f8535e89fd3ba67a973f76202f64a76699f3", "parent": "https://github.com/apache/hive/commit/8740d0f6b70f8c40f6ff0ef36ddd4b3c073c645d", "message": "HIVE-2718 NPE in union followed by join\n(He Yongqiang via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1233127 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_270", "file": [{"additions": 7, "raw_url": "https://github.com/apache/hive/raw/f4a6f8535e89fd3ba67a973f76202f64a76699f3/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "blob_url": "https://github.com/apache/hive/blob/f4a6f8535e89fd3ba67a973f76202f64a76699f3/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "sha": "8f269d8d2e40329e93fa252774ea0a53d4d87d17", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=f4a6f8535e89fd3ba67a973f76202f64a76699f3", "patch": "@@ -388,6 +388,13 @@ public static void joinUnionPlan(GenMRProcContext opProcCtx,\n     setUnionPlan(opProcCtx, local, plan, uCtx, true);\n \n     List<Task<? extends Serializable>> parTasks = null;\n+    if (opProcCtx.getRootTasks().contains(currentUnionTask)) {\n+      opProcCtx.getRootTasks().remove(currentUnionTask);\n+      if (!opProcCtx.getRootTasks().contains(existingTask)) {\n+        opProcCtx.getRootTasks().add(existingTask);\n+      }\n+    }\n+\n     if ((currentUnionTask != null) && (currentUnionTask.getParentTasks() != null)\n         && !currentUnionTask.getParentTasks().isEmpty()) {\n       parTasks = new ArrayList<Task<? extends Serializable>>();", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/f4a6f8535e89fd3ba67a973f76202f64a76699f3/ql/src/test/queries/clientpositive/union27.q", "blob_url": "https://github.com/apache/hive/blob/f4a6f8535e89fd3ba67a973f76202f64a76699f3/ql/src/test/queries/clientpositive/union27.q", "sha": "e0fccfcd728203344c3776614ef568a5e2492528", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/union27.q?ref=f4a6f8535e89fd3ba67a973f76202f64a76699f3", "patch": "@@ -0,0 +1,4 @@\n+create table jackson_sev_same as select * from src;\n+create table dim_pho as select * from src;\n+create table jackson_sev_add as select * from src;\n+select b.* from jackson_sev_same a join (select * from dim_pho union all select * from jackson_sev_add)b on a.key=b.key and b.key=97;", "filename": "ql/src/test/queries/clientpositive/union27.q"}, {"additions": 41, "raw_url": "https://github.com/apache/hive/raw/f4a6f8535e89fd3ba67a973f76202f64a76699f3/ql/src/test/results/clientpositive/union27.q.out", "blob_url": "https://github.com/apache/hive/blob/f4a6f8535e89fd3ba67a973f76202f64a76699f3/ql/src/test/results/clientpositive/union27.q.out", "sha": "af15e87d98755b5568ac259032b4ea7327ef831c", "changes": 41, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union27.q.out?ref=f4a6f8535e89fd3ba67a973f76202f64a76699f3", "patch": "@@ -0,0 +1,41 @@\n+PREHOOK: query: create table jackson_sev_same as select * from src\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table jackson_sev_same as select * from src\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@jackson_sev_same\n+PREHOOK: query: create table dim_pho as select * from src\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table dim_pho as select * from src\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@dim_pho\n+PREHOOK: query: create table jackson_sev_add as select * from src\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table jackson_sev_add as select * from src\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@jackson_sev_add\n+PREHOOK: query: select b.* from jackson_sev_same a join (select * from dim_pho union all select * from jackson_sev_add)b on a.key=b.key and b.key=97\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@dim_pho\n+PREHOOK: Input: default@jackson_sev_add\n+PREHOOK: Input: default@jackson_sev_same\n+#### A masked pattern was here ####\n+POSTHOOK: query: select b.* from jackson_sev_same a join (select * from dim_pho union all select * from jackson_sev_add)b on a.key=b.key and b.key=97\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@dim_pho\n+POSTHOOK: Input: default@jackson_sev_add\n+POSTHOOK: Input: default@jackson_sev_same\n+#### A masked pattern was here ####\n+97\tval_97\n+97\tval_97\n+97\tval_97\n+97\tval_97\n+97\tval_97\n+97\tval_97\n+97\tval_97\n+97\tval_97", "filename": "ql/src/test/results/clientpositive/union27.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/1d61b282c01e0927fe43039e7cba4efbba640bd5", "parent": "https://github.com/apache/hive/commit/380ac4f45f61105dccbfe6356e560b8d818293cb", "message": "HIVE-2581: explain task: getJSONPlan throws a NPE if the ast is null (namit via He Yongqiang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1202820 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_271", "file": [{"additions": 8, "raw_url": "https://github.com/apache/hive/raw/1d61b282c01e0927fe43039e7cba4efbba640bd5/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java", "blob_url": "https://github.com/apache/hive/blob/1d61b282c01e0927fe43039e7cba4efbba640bd5/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java", "sha": "a29efed42ace2816d2747bedb15b0589ea0a5bdb", "changes": 14, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java?ref=1d61b282c01e0927fe43039e7cba4efbba640bd5", "patch": "@@ -65,13 +65,15 @@ static public JSONObject getJSONPlan(PrintStream out, ExplainWork work)\n     }\n \n     // Print out the parse AST\n-    String jsonAST = outputAST(work.getAstStringTree(), out, jsonOutput, 0);\n-    if (out != null) {\n-      out.println();\n-    }\n+    if (work.getAstStringTree() != null) {\n+      String jsonAST = outputAST(work.getAstStringTree(), out, jsonOutput, 0);\n+      if (out != null) {\n+        out.println();\n+      }\n \n-    if (jsonOutput) {\n-      outJSONObject.put(\"ABSTRACT SYNTAX TREE\", jsonAST);\n+      if (jsonOutput) {\n+        outJSONObject.put(\"ABSTRACT SYNTAX TREE\", jsonAST);\n+      }\n     }\n \n     JSONObject jsonDependencies = outputDependencies(out, jsonOutput,", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/fadecedd690232cbcf533257522b6a75683ec4dd", "parent": "https://github.com/apache/hive/commit/1b3463918cb9c137b9a1d844afecfb5a08f43837", "message": "HIVE-2145. NPE during parsing order-by expression (Chinna Rao Lalam via Ning Zhang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1170672 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_272", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java", "blob_url": "https://github.com/apache/hive/blob/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java", "sha": "2798eaa8ea059f13aac22cdaf22622a791474ba0", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java?ref=fadecedd690232cbcf533257522b6a75683ec4dd", "patch": "@@ -186,6 +186,7 @@\n   NO_COMPARE_BIGINT_DOUBLE(\"In strict mode, comparing bigints and doubles is not allowed, \"\n       + \"it may result in a loss of precision. \"\n       + \"If you really want to perform the operation, set hive.mapred.mode=nonstrict\"),\n+      FUNCTIONS_ARE_NOT_SUPPORTED_IN_ORDER_BY(\"functions are not supported in order by\"),\n       ;\n \n   private String mesg;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "e091e86b67d54a52936e13a80bdb1ff8ce5e9c56", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=fadecedd690232cbcf533257522b6a75683ec4dd", "patch": "@@ -4524,6 +4524,9 @@ private Operator genReduceSinkPlan(String dest, QB qb, Operator input,\n           // ClusterBy\n           order.append(\"+\");\n         }\n+        if (cl.getType() == HiveParser.TOK_FUNCTION) {\n+          throw new SemanticException(ErrorMsg.FUNCTIONS_ARE_NOT_SUPPORTED_IN_ORDER_BY.getMsg());\n+        }\n         ExprNodeDesc exprNode = genExprNodeDesc(cl, inputRR);\n         sortCols.add(exprNode);\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/test/queries/clientnegative/orderby_function.q", "blob_url": "https://github.com/apache/hive/blob/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/test/queries/clientnegative/orderby_function.q", "sha": "e63b6a85cc3b7612e3cbb0f12986c88a9157c6db", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/orderby_function.q?ref=fadecedd690232cbcf533257522b6a75683ec4dd", "patch": "@@ -0,0 +1 @@\n+select src.key FROM src ORDER BY count(1) limit 1;", "filename": "ql/src/test/queries/clientnegative/orderby_function.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/test/results/clientnegative/orderby_function.q.out", "blob_url": "https://github.com/apache/hive/blob/fadecedd690232cbcf533257522b6a75683ec4dd/ql/src/test/results/clientnegative/orderby_function.q.out", "sha": "c4f4dbbf23f3e7e7f1c9df4e01e80717e4814664", "changes": 1, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/orderby_function.q.out?ref=fadecedd690232cbcf533257522b6a75683ec4dd", "patch": "@@ -0,0 +1 @@\n+FAILED: Error in semantic analysis: functions are not supported in order by", "filename": "ql/src/test/results/clientnegative/orderby_function.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/53cf27b412535b5d7c95c760971280c6a68e63ff", "parent": "https://github.com/apache/hive/commit/e31c71efec509a367816995f6f99559ec2a9919e", "message": "HIVE-2334. DESCRIBE TABLE causes NPE when hive.cli.print.header=true (Jakob Homan via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1159742 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_273", "file": [{"additions": 0, "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/build.xml", "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/build.xml", "sha": "add53e155f36f1fc46b55dbad76fc178a4440987", "changes": 4, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/build.xml?ref=53cf27b412535b5d7c95c760971280c6a68e63ff", "patch": "@@ -41,8 +41,4 @@ to call at top-level: ant deploy-contrib compile-core-test\n     </javac>\n   </target>\n \n-  <target name=\"test\">\n-    <echo message=\"Nothing to do!\"/>\n-  </target>\n-\n </project>", "filename": "cli/build.xml"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/ivy.xml", "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/ivy.xml", "sha": "abe72329e2c7ea60b637726317aa51643d91ce3e", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/ivy.xml?ref=53cf27b412535b5d7c95c760971280c6a68e63ff", "patch": "@@ -28,5 +28,6 @@\n           <artifact name=\"hadoop\" type=\"source\" ext=\"tar.gz\"/>\n         </dependency>\n         <dependency org=\"commons-cli\" name=\"commons-cli\" rev=\"${commons-cli.version}\"/>\n+        <dependency org=\"org.mockito\" name=\"mockito-all\" rev=\"${mockito-all.version}\" />\n     </dependencies>\n </ivy-module>", "filename": "cli/ivy.xml"}, {"additions": 114, "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java", "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java", "sha": "a689a2b7f92013b3910849014089b2d7be74f9ab", "changes": 188, "status": "modified", "deletions": 74, "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java?ref=53cf27b412535b5d7c95c760971280c6a68e63ff", "patch": "@@ -48,7 +48,6 @@\n import org.apache.hadoop.hive.common.LogUtils.LogInitializationException;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n-import org.apache.hadoop.hive.metastore.api.Schema;\n import org.apache.hadoop.hive.ql.CommandNeedRetryException;\n import org.apache.hadoop.hive.ql.Driver;\n import org.apache.hadoop.hive.ql.exec.FunctionRegistry;\n@@ -83,7 +82,7 @@\n   public static final String HIVERCFILE = \".hiverc\";\n \n   private final LogHelper console;\n-  private final Configuration conf;\n+  private Configuration conf;\n \n   public CliDriver() {\n     SessionState ss = SessionState.get();\n@@ -94,10 +93,8 @@ public CliDriver() {\n \n   public int processCmd(String cmd) {\n     CliSessionState ss = (CliSessionState) SessionState.get();\n-\n     String cmd_trimmed = cmd.trim();\n-    String[] tokens = cmd_trimmed.split(\"\\\\s+\");\n-    String cmd_1 = cmd_trimmed.substring(tokens[0].length()).trim();\n+    String[] tokens = tokenizeCmd(cmd_trimmed);\n     int ret = 0;\n \n     if (cmd_trimmed.toLowerCase().equals(\"quit\") || cmd_trimmed.toLowerCase().equals(\"exit\")) {\n@@ -109,6 +106,8 @@ public int processCmd(String cmd) {\n       System.exit(0);\n \n     } else if (tokens[0].equalsIgnoreCase(\"source\")) {\n+      String cmd_1 = getFirstCmd(cmd_trimmed, tokens[0].length());\n+\n       File sourceFile = new File(cmd_1);\n       if (! sourceFile.isFile()){\n         console.printError(\"File: \"+ cmd_1 + \" is not a file.\");\n@@ -207,91 +206,132 @@ public int processCmd(String cmd) {\n         }\n       }\n     } else { // local mode\n-      CommandProcessor proc = CommandProcessorFactory.get(tokens[0], (HiveConf)conf);\n-      int tryCount = 0;\n-      boolean needRetry;\n+      CommandProcessor proc = CommandProcessorFactory.get(tokens[0], (HiveConf) conf);\n+      ret = processLocalCmd(cmd, proc, ss);\n+    }\n \n-      do {\n-        try {\n-          needRetry = false;\n-          if (proc != null) {\n-            if (proc instanceof Driver) {\n-              Driver qp = (Driver) proc;\n-              PrintStream out = ss.out;\n-              long start = System.currentTimeMillis();\n-              if (ss.getIsVerbose()) {\n-                out.println(cmd);\n-              }\n+    return ret;\n+  }\n \n-              qp.setTryCount(tryCount);\n-              ret = qp.run(cmd).getResponseCode();\n-              if (ret != 0) {\n-                qp.close();\n-                return ret;\n-              }\n+  /**\n+   * For testing purposes to inject Configuration dependency\n+   * @param conf to replace default\n+   */\n+  void setConf(Configuration conf) {\n+    this.conf = conf;\n+  }\n \n-              ArrayList<String> res = new ArrayList<String>();\n-\n-              if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_HEADER)) {\n-                // Print the column names\n-                boolean first_col = true;\n-                Schema sc = qp.getSchema();\n-                for (FieldSchema fs : sc.getFieldSchemas()) {\n-                  if (!first_col) {\n-                    out.print('\\t');\n-                  }\n-                  out.print(fs.getName());\n-                  first_col = false;\n-                }\n-                out.println();\n-              }\n+  /**\n+   * Extract and clean up the first command in the input.\n+   */\n+  private String getFirstCmd(String cmd, int length) {\n+    return cmd.substring(length).trim();\n+  }\n+\n+  private String[] tokenizeCmd(String cmd) {\n+    return cmd.split(\"\\\\s+\");\n+  }\n+\n+  int processLocalCmd(String cmd, CommandProcessor proc, CliSessionState ss) {\n+    int tryCount = 0;\n+    boolean needRetry;\n+    int ret = 0;\n+\n+    do {\n+      try {\n+        needRetry = false;\n+        if (proc != null) {\n+          if (proc instanceof Driver) {\n+            Driver qp = (Driver) proc;\n+            PrintStream out = ss.out;\n+            long start = System.currentTimeMillis();\n+            if (ss.getIsVerbose()) {\n+              out.println(cmd);\n+            }\n \n-              try {\n-                while (qp.getResults(res)) {\n-                  for (String r : res) {\n-                    out.println(r);\n-                  }\n-                  res.clear();\n-                  if (out.checkError()) {\n-                    break;\n-                  }\n+            qp.setTryCount(tryCount);\n+            ret = qp.run(cmd).getResponseCode();\n+            if (ret != 0) {\n+              qp.close();\n+              return ret;\n+            }\n+\n+            ArrayList<String> res = new ArrayList<String>();\n+\n+            printHeader(qp, out);\n+\n+            try {\n+              while (qp.getResults(res)) {\n+                for (String r : res) {\n+                  out.println(r);\n+                }\n+                res.clear();\n+                if (out.checkError()) {\n+                  break;\n                 }\n-              } catch (IOException e) {\n-                console.printError(\"Failed with exception \" + e.getClass().getName() + \":\"\n-                    + e.getMessage(), \"\\n\"\n-                    + org.apache.hadoop.util.StringUtils.stringifyException(e));\n-                ret = 1;\n               }\n+            } catch (IOException e) {\n+              console.printError(\"Failed with exception \" + e.getClass().getName() + \":\"\n+                  + e.getMessage(), \"\\n\"\n+                  + org.apache.hadoop.util.StringUtils.stringifyException(e));\n+              ret = 1;\n+            }\n \n-              int cret = qp.close();\n-              if (ret == 0) {\n-                ret = cret;\n-              }\n+            int cret = qp.close();\n+            if (ret == 0) {\n+              ret = cret;\n+            }\n \n-              long end = System.currentTimeMillis();\n-              if (end > start) {\n-                double timeTaken = (end - start) / 1000.0;\n-                console.printInfo(\"Time taken: \" + timeTaken + \" seconds\", null);\n-              }\n+            long end = System.currentTimeMillis();\n+            if (end > start) {\n+              double timeTaken = (end - start) / 1000.0;\n+              console.printInfo(\"Time taken: \" + timeTaken + \" seconds\", null);\n+            }\n \n-            } else {\n-              if (ss.getIsVerbose()) {\n-                ss.out.println(tokens[0] + \" \" + cmd_1);\n-              }\n-              ret = proc.run(cmd_1).getResponseCode();\n+          } else {\n+            String firstToken = tokenizeCmd(cmd.trim())[0];\n+            String cmd_1 = getFirstCmd(cmd.trim(), firstToken.length());\n+\n+            if (ss.getIsVerbose()) {\n+              ss.out.println(firstToken + \" \" + cmd_1);\n             }\n+            ret = proc.run(cmd_1).getResponseCode();\n           }\n-        } catch (CommandNeedRetryException e) {\n-          console.printInfo(\"Retry query with a different approach...\");\n-          tryCount++;\n-          needRetry = true;\n         }\n-      } while (needRetry);\n-    }\n+      } catch (CommandNeedRetryException e) {\n+        console.printInfo(\"Retry query with a different approach...\");\n+        tryCount++;\n+        needRetry = true;\n+      }\n+    } while (needRetry);\n \n     return ret;\n   }\n \n+  /**\n+   * If enabled and applicable to this command, print the field headers\n+   * for the output.\n+   *\n+   * @param qp Driver that executed the command\n+   * @param out Printstream which to send output to\n+   */\n+  private void printHeader(Driver qp, PrintStream out) {\n+    List<FieldSchema> fieldSchemas = qp.getSchema().getFieldSchemas();\n+    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_HEADER)\n+          && fieldSchemas != null) {\n+      // Print the column names\n+      boolean first_col = true;\n+      for (FieldSchema fs : fieldSchemas) {\n+        if (!first_col) {\n+          out.print('\\t');\n+        }\n+        out.print(fs.getName());\n+        first_col = false;\n+      }\n+      out.println();\n+    }\n+  }\n+\n   public int processLine(String line) {\n     return processLine(line, false);\n   }", "filename": "cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java"}, {"additions": 102, "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java", "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java", "sha": "22a0891607f4af0ff9f0438bfa396036938915a9", "changes": 102, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java?ref=53cf27b412535b5d7c95c760971280c6a68e63ff", "patch": "@@ -0,0 +1,102 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.cli;\n+\n+import junit.framework.TestCase;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.Schema;\n+import org.apache.hadoop.hive.ql.CommandNeedRetryException;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n+\n+import java.io.PrintStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import static org.mockito.Matchers.anyBoolean;\n+import static org.mockito.Matchers.anyString;\n+import static org.mockito.Matchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+// Cannot call class TestCliDriver since that's the name of the generated\n+// code for the script-based testing\n+public class TestCliDriverMethods extends TestCase {\n+\n+  // If the command has an associated schema, make sure it gets printed to use\n+  public void testThatCliDriverPrintsHeaderForCommandsWithSchema() throws CommandNeedRetryException {\n+    Schema mockSchema = mock(Schema.class);\n+    List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>();\n+    String fieldName = \"FlightOfTheConchords\";\n+    fieldSchemas.add(new FieldSchema(fieldName, \"type\", \"comment\"));\n+\n+    when(mockSchema.getFieldSchemas()).thenReturn(fieldSchemas);\n+\n+    PrintStream mockOut = headerPrintingTestDriver(mockSchema);\n+    // Should have printed out the header for the field schema\n+    verify(mockOut, times(1)).print(fieldName);\n+  }\n+\n+  // If the command has no schema, make sure nothing is printed\n+  public void testThatCliDriverPrintsNoHeaderForCommandsWithNoSchema() throws CommandNeedRetryException {\n+    Schema mockSchema = mock(Schema.class);\n+    when(mockSchema.getFieldSchemas()).thenReturn(null);\n+\n+    PrintStream mockOut = headerPrintingTestDriver(mockSchema);\n+    // Should not have tried to print any thing.\n+    verify(mockOut, never()).print(anyString());\n+  }\n+\n+  /**\n+   * Do the actual testing against a mocked CliDriver based on what type of schema\n+   * @param mockSchema Schema to throw against test\n+   * @return Output that would have been sent to the user\n+   * @throws CommandNeedRetryException won't actually be thrown\n+   */\n+  private PrintStream headerPrintingTestDriver(Schema mockSchema) throws CommandNeedRetryException {\n+    CliDriver cliDriver = new CliDriver();\n+\n+    // We want the driver to try to print the header...\n+    Configuration conf = mock(Configuration.class);\n+    when(conf.getBoolean(eq(ConfVars.HIVE_CLI_PRINT_HEADER.varname), anyBoolean())).thenReturn(true);\n+    cliDriver.setConf(conf);\n+\n+    Driver proc = mock(Driver.class);\n+\n+    CommandProcessorResponse cpr = mock(CommandProcessorResponse.class);\n+    when(cpr.getResponseCode()).thenReturn(0);\n+    when(proc.run(anyString())).thenReturn(cpr);\n+\n+    // and then see what happens based on the provided schema\n+    when(proc.getSchema()).thenReturn(mockSchema);\n+\n+    CliSessionState mockSS = mock(CliSessionState.class);\n+    PrintStream mockOut = mock(PrintStream.class);\n+\n+    mockSS.out = mockOut;\n+\n+    cliDriver.processLocalCmd(\"use default;\", proc, mockSS);\n+    return mockOut;\n+  }\n+\n+}", "filename": "cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/queries/clientpositive/print_header.q", "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/queries/clientpositive/print_header.q", "sha": "3ca0340e2d2675d772a108c7faec7be1c4684f14", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/print_header.q?ref=53cf27b412535b5d7c95c760971280c6a68e63ff", "patch": "@@ -11,3 +11,4 @@ SELECT src1.key as k1, src1.value as v1,\n \n SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key LIMIT 10;\n \n+use default;", "filename": "ql/src/test/queries/clientpositive/print_header.q"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/results/clientpositive/print_header.q.out", "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/results/clientpositive/print_header.q.out", "sha": "adcfb8f45b1285791c296463b8151f72ad84e98b", "changes": 12, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/print_header.q.out?ref=53cf27b412535b5d7c95c760971280c6a68e63ff", "patch": "@@ -7,7 +7,7 @@ PREHOOK: query: SELECT src1.key as k1, src1.value as v1,\n   LIMIT 10\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-11_283_258399688654159660/-mr-10000\n+PREHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-14_972_2863485419468463022/-mr-10000\n POSTHOOK: query: SELECT src1.key as k1, src1.value as v1, \n        src2.key as k2, src2.value as v2 FROM \n   (SELECT * FROM src WHERE src.key < 10) src1 \n@@ -17,7 +17,7 @@ POSTHOOK: query: SELECT src1.key as k1, src1.value as v1,\n   LIMIT 10\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-11_283_258399688654159660/-mr-10000\n+POSTHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-14_972_2863485419468463022/-mr-10000\n k1\tv1\tk2\tv2\n 0\tval_0\t0\tval_0\n 0\tval_0\t0\tval_0\n@@ -32,11 +32,11 @@ k1\tv1\tk2\tv2\n PREHOOK: query: SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key LIMIT 10\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-30_510_2595029549749893604/-mr-10000\n+PREHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-28_635_5275683580628567895/-mr-10000\n POSTHOOK: query: SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key LIMIT 10\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-30_510_2595029549749893604/-mr-10000\n+POSTHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-28_635_5275683580628567895/-mr-10000\n key\t_c1\n 0\t0.0\n 10\t10.0\n@@ -48,3 +48,7 @@ key\t_c1\n 111\t111.0\n 113\t226.0\n 114\t114.0\n+PREHOOK: query: use default\n+PREHOOK: type: SWITCHDATABASE\n+POSTHOOK: query: use default\n+POSTHOOK: type: SWITCHDATABASE", "filename": "ql/src/test/results/clientpositive/print_header.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c935c69ae31a634a2e74d401baf4dea95b7d0878", "parent": "https://github.com/apache/hive/commit/a29e17a29b24e86d305c2eee1dfa76df5142c313", "message": "HIVE-2060:CLI local mode hit NPE when exiting by ^D (Ning Zhang via He Yongqiang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1083640 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_274", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java", "blob_url": "https://github.com/apache/hive/blob/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java", "sha": "1fb11e7abdb2ec6426f9379c85019602104b364f", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java?ref=c935c69ae31a634a2e74d401baf4dea95b7d0878", "patch": "@@ -95,6 +95,7 @@ public int processCmd(String cmd) {\n       // if we have come this far - either the previous commands\n       // are all successful or this is command line. in either case\n       // this counts as a successful run\n+      ss.close();\n       System.exit(0);\n \n     } else if (tokens[0].equalsIgnoreCase(\"source\")) {", "filename": "cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java", "blob_url": "https://github.com/apache/hive/blob/c935c69ae31a634a2e74d401baf4dea95b7d0878/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java", "sha": "c693aeb35a88093abd6ee2dd69cd8da460b0091b", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java?ref=c935c69ae31a634a2e74d401baf4dea95b7d0878", "patch": "@@ -103,9 +103,10 @@ public int getPort() {\n \n   public void close() {\n     try {\n-      client.clean();\n-      client.shutdown();\n-      transport.close();\n+      if (remoteMode) {\n+        client.clean();\n+        transport.close();\n+      }\n     } catch (TException e) {\n       e.printStackTrace();\n     }", "filename": "cli/src/java/org/apache/hadoop/hive/cli/CliSessionState.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7", "parent": "https://github.com/apache/hive/commit/3191ff91b8dc7385fea7ba9fa542d6da34d9ed1c", "message": "HIVE-1320. NPE with lineage in a query of union alls on unions (Ashish Thusoo via Ning Zhang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@937123 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_275", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/CHANGES.txt", "blob_url": "https://github.com/apache/hive/blob/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/CHANGES.txt", "sha": "2cb9e08f739586eeb68c47ca31db1dba6f2c9d8e", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=7b7b8fad21b971b10a1874eb298cdcc43d49c7b7", "patch": "@@ -387,6 +387,9 @@ Trunk -  Unreleased\n     HIVE-1316. Increase hive client side memory\n     (Ning Zhang via namit)\n \n+    HIVE-1320. NPE with lineage in a query of union all on joins\n+    (Ashish Thusoo via Ning Zhang)\n+\n Release 0.5.0 -  Unreleased\n \n   INCOMPATIBLE CHANGES", "filename": "CHANGES.txt"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/OpProcFactory.java", "blob_url": "https://github.com/apache/hive/blob/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/OpProcFactory.java", "sha": "bcfb477949d41e1fb8600fe4ff14bfbebbdb3cd8", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/OpProcFactory.java?ref=7b7b8fad21b971b10a1874eb298cdcc43d49c7b7", "patch": "@@ -393,8 +393,10 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       ArrayList<ColumnInfo> inp_cols = inpOp.getSchema().getSignature();\n       int cnt = 0;\n       for(ColumnInfo ci : rs.getSignature()) {\n-        lCtx.getIndex().mergeDependency(op, ci,\n-            lCtx.getIndex().getDependency(inpOp, inp_cols.get(cnt++)));\n+        Dependency inp_dep = lCtx.getIndex().getDependency(inpOp, inp_cols.get(cnt++));\n+        if (inp_dep != null) {\n+          lCtx.getIndex().mergeDependency(op, ci, inp_dep);\n+        }\n       }\n       return null;\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/lineage/OpProcFactory.java"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/ql/src/test/queries/clientpositive/lineage1.q", "blob_url": "https://github.com/apache/hive/blob/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/ql/src/test/queries/clientpositive/lineage1.q", "sha": "7ead0fb53f2f9a1bb552800960065e3e6b28539d", "changes": 30, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/lineage1.q?ref=7b7b8fad21b971b10a1874eb298cdcc43d49c7b7", "patch": "@@ -0,0 +1,30 @@\n+drop table dest_l1;\n+\n+CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE;\n+\n+\n+EXPLAIN\n+INSERT OVERWRITE TABLE dest_l1\n+SELECT j.*\n+FROM (SELECT t1.key, p1.value \n+      FROM src1 t1 \n+      LEFT OUTER JOIN src p1\n+      ON (t1.key = p1.key)\n+      UNION ALL\n+      SELECT t2.key, p2.value\n+      FROM src1 t2\n+      LEFT OUTER JOIN src p2\n+      ON (t2.key = p2.key)) j;\n+\n+INSERT OVERWRITE TABLE dest_l1\n+SELECT j.*\n+FROM (SELECT t1.key, p1.value\n+      FROM src1 t1\n+      LEFT OUTER JOIN src p1\n+      ON (t1.key = p1.key)\n+      UNION ALL\n+      SELECT t2.key, p2.value\n+      FROM src1 t2\n+      LEFT OUTER JOIN src p2\n+      ON (t2.key = p2.key)) j;\n+", "filename": "ql/src/test/queries/clientpositive/lineage1.q"}, {"additions": 290, "raw_url": "https://github.com/apache/hive/raw/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/ql/src/test/results/clientpositive/lineage1.q.out", "blob_url": "https://github.com/apache/hive/blob/7b7b8fad21b971b10a1874eb298cdcc43d49c7b7/ql/src/test/results/clientpositive/lineage1.q.out", "sha": "97ee88746c96074340945b4dcc60e27078c8c67e", "changes": 290, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/lineage1.q.out?ref=7b7b8fad21b971b10a1874eb298cdcc43d49c7b7", "patch": "@@ -0,0 +1,290 @@\n+PREHOOK: query: drop table dest_l1\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table dest_l1\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@dest_l1\n+PREHOOK: query: EXPLAIN\n+INSERT OVERWRITE TABLE dest_l1\n+SELECT j.*\n+FROM (SELECT t1.key, p1.value \n+      FROM src1 t1 \n+      LEFT OUTER JOIN src p1\n+      ON (t1.key = p1.key)\n+      UNION ALL\n+      SELECT t2.key, p2.value\n+      FROM src1 t2\n+      LEFT OUTER JOIN src p2\n+      ON (t2.key = p2.key)) j\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN\n+INSERT OVERWRITE TABLE dest_l1\n+SELECT j.*\n+FROM (SELECT t1.key, p1.value \n+      FROM src1 t1 \n+      LEFT OUTER JOIN src p1\n+      ON (t1.key = p1.key)\n+      UNION ALL\n+      SELECT t2.key, p2.value\n+      FROM src1 t2\n+      LEFT OUTER JOIN src p2\n+      ON (t2.key = p2.key)) j\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_TABREF src1 t1) (TOK_TABREF src p1) (= (. (TOK_TABLE_OR_COL t1) key) (. (TOK_TABLE_OR_COL p1) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL t1) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL p1) value))))) (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_TABREF src1 t2) (TOK_TABREF src p2) (= (. (TOK_TABLE_OR_COL t2) key) (. (TOK_TABLE_OR_COL p2) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL t2) key)) (TOK_SELEXPR (. (TOK_TABLE_OR_COL p2) value)))))) j)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_l1)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF j)))))\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-2 depends on stages: Stage-1, Stage-6\n+  Stage-5 depends on stages: Stage-2 , consists of Stage-4, Stage-3\n+  Stage-4\n+  Stage-0 depends on stages: Stage-4, Stage-3\n+  Stage-3\n+  Stage-6 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        null-subquery1:j-subquery1:p1 \n+          TableScan\n+            alias: p1\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: key\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: key\n+                    type: string\n+              tag: 1\n+              value expressions:\n+                    expr: value\n+                    type: string\n+        null-subquery1:j-subquery1:t1 \n+          TableScan\n+            alias: t1\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: key\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: key\n+                    type: string\n+              tag: 0\n+              value expressions:\n+                    expr: key\n+                    type: string\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Left Outer Join0 to 1\n+          condition expressions:\n+            0 {VALUE._col0}\n+            1 {VALUE._col1}\n+          handleSkewJoin: false\n+          outputColumnNames: _col0, _col3\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: string\n+                  expr: _col3\n+                  type: string\n+            outputColumnNames: _col0, _col1\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+  Stage: Stage-2\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10002 \n+          Union\n+            Select Operator\n+              expressions:\n+                    expr: _col0\n+                    type: string\n+                    expr: _col1\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              Select Operator\n+                expressions:\n+                      expr: UDFToInteger(_col0)\n+                      type: int\n+                      expr: _col1\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                File Output Operator\n+                  compressed: false\n+                  GlobalTableId: 1\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      name: dest_l1\n+        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10004 \n+          Union\n+            Select Operator\n+              expressions:\n+                    expr: _col0\n+                    type: string\n+                    expr: _col1\n+                    type: string\n+              outputColumnNames: _col0, _col1\n+              Select Operator\n+                expressions:\n+                      expr: UDFToInteger(_col0)\n+                      type: int\n+                      expr: _col1\n+                      type: string\n+                outputColumnNames: _col0, _col1\n+                File Output Operator\n+                  compressed: false\n+                  GlobalTableId: 1\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      name: dest_l1\n+\n+  Stage: Stage-5\n+    Conditional Operator\n+\n+  Stage: Stage-4\n+    Move Operator\n+      files:\n+          hdfs directory: true\n+          destination: file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10000\n+\n+  Stage: Stage-0\n+    Move Operator\n+      tables:\n+          replace: true\n+          table:\n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: dest_l1\n+\n+  Stage: Stage-3\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        file:/data/users/athusoo/apache_workspaces/hive_trunk_ws1/build/ql/scratchdir/hive_2010-04-22_15-52-46_260_8748362802646516479/10003 \n+            Reduce Output Operator\n+              sort order: \n+              Map-reduce partition columns:\n+                    expr: rand()\n+                    type: double\n+              tag: -1\n+              value expressions:\n+                    expr: key\n+                    type: int\n+                    expr: value\n+                    type: string\n+      Reduce Operator Tree:\n+        Extract\n+          File Output Operator\n+            compressed: false\n+            GlobalTableId: 0\n+            table:\n+                input format: org.apache.hadoop.mapred.TextInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                name: dest_l1\n+\n+  Stage: Stage-6\n+    Map Reduce\n+      Alias -> Map Operator Tree:\n+        null-subquery2:j-subquery2:p2 \n+          TableScan\n+            alias: p2\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: key\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: key\n+                    type: string\n+              tag: 1\n+              value expressions:\n+                    expr: value\n+                    type: string\n+        null-subquery2:j-subquery2:t2 \n+          TableScan\n+            alias: t2\n+            Reduce Output Operator\n+              key expressions:\n+                    expr: key\n+                    type: string\n+              sort order: +\n+              Map-reduce partition columns:\n+                    expr: key\n+                    type: string\n+              tag: 0\n+              value expressions:\n+                    expr: key\n+                    type: string\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Left Outer Join0 to 1\n+          condition expressions:\n+            0 {VALUE._col0}\n+            1 {VALUE._col1}\n+          handleSkewJoin: false\n+          outputColumnNames: _col0, _col3\n+          Select Operator\n+            expressions:\n+                  expr: _col0\n+                  type: string\n+                  expr: _col3\n+                  type: string\n+            outputColumnNames: _col0, _col1\n+            File Output Operator\n+              compressed: false\n+              GlobalTableId: 0\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+\n+\n+PREHOOK: query: INSERT OVERWRITE TABLE dest_l1\n+SELECT j.*\n+FROM (SELECT t1.key, p1.value\n+      FROM src1 t1\n+      LEFT OUTER JOIN src p1\n+      ON (t1.key = p1.key)\n+      UNION ALL\n+      SELECT t2.key, p2.value\n+      FROM src1 t2\n+      LEFT OUTER JOIN src p2\n+      ON (t2.key = p2.key)) j\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src1\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@dest_l1\n+POSTHOOK: query: INSERT OVERWRITE TABLE dest_l1\n+SELECT j.*\n+FROM (SELECT t1.key, p1.value\n+      FROM src1 t1\n+      LEFT OUTER JOIN src p1\n+      ON (t1.key = p1.key)\n+      UNION ALL\n+      SELECT t2.key, p2.value\n+      FROM src1 t2\n+      LEFT OUTER JOIN src p2\n+      ON (t2.key = p2.key)) j\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src1\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@dest_l1\n+POSTHOOK: Lineage: dest_l1.key EXPRESSION [(src1)t1.FieldSchema(name:key, type:string, comment:default), (src1)t2.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: dest_l1.value EXPRESSION [(src)p2.FieldSchema(name:value, type:string, comment:default), (src)p1.FieldSchema(name:value, type:string, comment:default), ]", "filename": "ql/src/test/results/clientpositive/lineage1.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/d78c284bc1039cdabd3b42a7d27c332e2680c7b2", "parent": "https://github.com/apache/hive/commit/bed52273cc9b1a93b800cec4cff94abeff2b932c", "message": "HIVE-1011. GenericUDTFExplode() throws NPE when given nulls\n(Paul Yang via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@933381 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_276", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/CHANGES.txt", "blob_url": "https://github.com/apache/hive/blob/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/CHANGES.txt", "sha": "5509c392f426ba8b84022f6069ef8fe3ed594f8c", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=d78c284bc1039cdabd3b42a7d27c332e2680c7b2", "patch": "@@ -348,6 +348,9 @@ Trunk -  Unreleased\n     HIVE-1280. Add option to CombineHiveInputFormat for non-splittable inputs.\n     (Namit Jain via zshao)\n \n+    HIVE-1011. GenericUDTFExplode() throws NPE when given nulls\n+    (Paul Yang via namit)\n+\n Release 0.5.0 -  Unreleased\n \n   INCOMPATIBLE CHANGES", "filename": "CHANGES.txt"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java", "blob_url": "https://github.com/apache/hive/blob/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java", "sha": "d1e7def8def7ee7a8d5eb0516b38e2fd50a09efe", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java?ref=d78c284bc1039cdabd3b42a7d27c332e2680c7b2", "patch": "@@ -62,12 +62,14 @@ public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgume\n         fieldOIs);\n   }\n \n-  private Object[] forwardObj = new Object[1];\n+  private final Object[] forwardObj = new Object[1];\n \n   @Override\n   public void process(Object[] o) throws HiveException {\n-\n     List<?> list = listOI.getList(o[0]);\n+    if(list == null) {\n+      return;\n+    }\n     for (Object r : list) {\n       forwardObj[0] = r;\n       forward(forwardObj);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFExplode.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/ql/src/test/queries/clientpositive/explode_null.q", "blob_url": "https://github.com/apache/hive/blob/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/ql/src/test/queries/clientpositive/explode_null.q", "sha": "c1f3045f39219737623db5f9947f1ce509978aff", "changes": 5, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/explode_null.q?ref=d78c284bc1039cdabd3b42a7d27c332e2680c7b2", "patch": "@@ -0,0 +1,5 @@\n+SELECT explode(col) AS myCol FROM \n+    (SELECT array(1,2,3) AS col FROM src LIMIT 1 \n+     UNION ALL\n+     SELECT IF(false, array(1,2,3), NULL) AS col FROM src LIMIT 1) a;\n+     \n\\ No newline at end of file", "filename": "ql/src/test/queries/clientpositive/explode_null.q"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/ql/src/test/results/clientpositive/explode_null.q.out", "blob_url": "https://github.com/apache/hive/blob/d78c284bc1039cdabd3b42a7d27c332e2680c7b2/ql/src/test/results/clientpositive/explode_null.q.out", "sha": "f53b446507e8ea601ec06015fb458e17659fb8c0", "changes": 17, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/explode_null.q.out?ref=d78c284bc1039cdabd3b42a7d27c332e2680c7b2", "patch": "@@ -0,0 +1,17 @@\n+PREHOOK: query: SELECT explode(col) AS myCol FROM \n+    (SELECT array(1,2,3) AS col FROM src LIMIT 1 \n+     UNION ALL\n+     SELECT IF(false, array(1,2,3), NULL) AS col FROM src LIMIT 1) a\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-04-08_15-57-57_440_1997210643218047348/10000\n+POSTHOOK: query: SELECT explode(col) AS myCol FROM \n+    (SELECT array(1,2,3) AS col FROM src LIMIT 1 \n+     UNION ALL\n+     SELECT IF(false, array(1,2,3), NULL) AS col FROM src LIMIT 1) a\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/scratchdir/hive_2010-04-08_15-57-57_440_1997210643218047348/10000\n+1\n+2\n+3", "filename": "ql/src/test/results/clientpositive/explode_null.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c6bc891dae978c8e9bdd1b60538b5096c7ab4798", "parent": "https://github.com/apache/hive/commit/b306976522482371a6cbe52df7a152605f8e3c88", "message": "HIVE-1064: NPE when operating HIVE CLI in distributed mode (Carl Steinbach via Ning Zhang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@901473 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_277", "file": [{"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "sha": "666bd33aa70b71de7c40c111513fe65d8cbeb301", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798", "patch": "@@ -105,6 +105,7 @@ public DDLTask() {\n     super();\n   }\n \n+  @Override\n   public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {\n     super.initialize(conf, queryPlan, ctx);\n     this.conf = conf;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java", "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java", "sha": "315cec491337e375a9ca2ba55b893f4941e8108e", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798", "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;\n import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;\n import org.apache.hadoop.hive.ql.io.*;\n+import org.apache.hadoop.hive.ql.DriverContext;\n import org.apache.hadoop.hive.ql.QueryPlan;\n import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\n import org.apache.hadoop.hive.ql.session.SessionState;\n@@ -107,8 +108,9 @@ private void initializeFiles(String prop, String files) {\n   /**\n    * Initialization when invoked from QL\n    */\n-  public void initialize(HiveConf conf, QueryPlan queryPlan) {\n-    super.initialize(conf, queryPlan, null);\n+  @Override\n+  public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverContext) {\n+    super.initialize(conf, queryPlan, driverContext);\n     job = new JobConf(conf, ExecDriver.class);\n     // NOTE: initialize is only called if it is in non-local mode.\n     // In case it's in non-local mode, we need to move the SessionState files", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java", "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java", "sha": "b56fed061110704d213746e12607822defc12fb7", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798", "patch": "@@ -51,6 +51,7 @@ public FetchTask() {\n  \t  super();\n  \t}\n  \t\n+ \t@Override\n   public void initialize (HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {\n     super.initialize(conf, queryPlan, ctx);\n     ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java", "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java", "sha": "5eb4f205026979a7ab0969001d8f9844a3551b7a", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798", "patch": "@@ -45,6 +45,7 @@ public FunctionTask() {\n     super();\n   }\n   \n+  @Override\n   public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {\n     super.initialize(conf, queryPlan, ctx);\n     this.conf = conf;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java", "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java", "sha": "e767ededd4442fbe28804a04ce5705441ca73c6a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798", "patch": "@@ -68,7 +68,7 @@ public Task() {\n     this.taskCounters = new HashMap<String, Long>();\n   }\n \n-  public void initialize (HiveConf conf, QueryPlan queryPlan, DriverContext driverContext) {\n+  public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverContext) {\n     this.queryPlan = queryPlan;\n     isdone = false;\n     started = false;", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/481f069b159f496ad29f91f093f71b69551e9180", "parent": "https://github.com/apache/hive/commit/72a3ae384d1deb0a8e6e6609ddf3517f50aaa2fc", "message": "HIVE-996 describe function\" throws NPE when when called on UDTF or UDAF\n(Carl Steinbach via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@896744 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_278", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/CHANGES.txt", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/CHANGES.txt", "sha": "f589d6ce1a33883aa5bbdd298bb2b2943efce51f", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -412,6 +412,9 @@ Trunk -  Unreleased\n     HIVE-1030 mapjoin should throw an error if the input is too large\n     (Ning Zhang via namit)\n \n+    HIVE-996 describe function\" throws NPE when when called on UDTF or UDAF\n+    (Carl Steinbach via namit)\n+\n Release 0.4.0 -  Unreleased\n \n   INCOMPATIBLE CHANGES", "filename": "CHANGES.txt"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java", "sha": "38c6b2a2bc97b9d72ea5a916e7578627a4aa951d", "changes": 48, "status": "modified", "deletions": 29, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -26,10 +26,8 @@\n import java.io.Serializable;\n import java.io.Writer;\n import java.util.ArrayList;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n-import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n@@ -75,8 +73,6 @@\n import org.apache.hadoop.hive.ql.plan.showTableStatusDesc;\n import org.apache.hadoop.hive.ql.plan.showTablesDesc;\n import org.apache.hadoop.hive.ql.plan.api.StageType;\n-import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;\n-import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;\n import org.apache.hadoop.hive.serde.Constants;\n import org.apache.hadoop.hive.serde2.Deserializer;\n import org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe;\n@@ -473,8 +469,8 @@ private int showFunctions(showFunctionsDesc showFuncs)\n    * @throws HiveException\n    */\n   private int describeFunction(descFunctionDesc descFunc)\n-      throws HiveException {\n-    String name = descFunc.getName();\n+  throws HiveException {\n+    String funcName = descFunc.getName();\n \n     // write the results in the file\n     try {\n@@ -483,30 +479,25 @@ private int describeFunction(descFunctionDesc descFunc)\n \n       // get the function documentation\n       description desc = null;\n-      FunctionInfo fi = FunctionRegistry.getFunctionInfo(name);\n-\n       Class<?> funcClass = null;\n-      GenericUDF udf = fi.getGenericUDF();\n-      if (udf != null) {\n-        // If it's a GenericUDFBridge, then let's use the\n-        if (udf instanceof GenericUDFBridge) {\n-          funcClass = ((GenericUDFBridge)udf).getUdfClass();\n-        } else {\n-          funcClass = udf.getClass();\n-        }\n+      FunctionInfo functionInfo = FunctionRegistry.getFunctionInfo(funcName);\n+      if (functionInfo != null) {\n+        funcClass = functionInfo.getFunctionClass();\n       }\n-\n       if (funcClass != null) {\n         desc = funcClass.getAnnotation(description.class);\n       }\n       if (desc != null) {\n-        outStream.writeBytes(desc.value().replace(\"_FUNC_\", name));\n-        if(descFunc.isExtended() && desc.extended().length()>0) {\n-          outStream.writeBytes(\"\\n\"+desc.extended().replace(\"_FUNC_\", name));\n+        outStream.writeBytes(desc.value().replace(\"_FUNC_\", funcName));\n+        if(descFunc.isExtended() && desc.extended().length() > 0) {\n+          outStream.writeBytes(\"\\n\"+desc.extended().replace(\"_FUNC_\", funcName));\n         }\n       } else {\n-        outStream.writeBytes(\"Function \" + name + \" does not exist or cannot\" +\n-        \t\t\" find documentation for it.\");\n+        if (funcClass != null) {\n+          outStream.writeBytes(\"There is no documentation for function \" + funcName);\n+        } else {\n+          outStream.writeBytes(\"Function \" + funcName + \" does not exist.\");\n+        }\n       }\n \n       outStream.write(terminator);\n@@ -533,10 +524,10 @@ private int describeFunction(descFunctionDesc descFunc)\n    * @return Return 0 when execution succeeds and above 0 if it fails.\n    */\n   private int showTableStatus(Hive db, showTableStatusDesc showTblStatus)\n-      throws HiveException {\n+  throws HiveException {\n     // get the tables for the desired pattenn - populate the output stream\n     List<Table> tbls = new ArrayList<Table>();\n-    HashMap<String, String> part = showTblStatus.getPartSpec();\n+    Map<String, String> part = showTblStatus.getPartSpec();\n     Partition par = null;\n     if (part != null) {\n       Table tbl = db.getTable(showTblStatus.getDbName(), showTblStatus\n@@ -613,10 +604,9 @@ private int showTableStatus(Hive db, showTableStatusDesc showTblStatus)\n         List<Path> locations = new ArrayList<Path>();\n         if (isPartitioned) {\n           if (par == null) {\n-            List<Partition> parts = db.getPartitions(tbl);\n-            for (Partition curPart : parts)\n-              locations.add(new Path(curPart.getTPartition().getSd()\n-                  .getLocation()));\n+            for (Partition curPart : db.getPartitions(tbl)) {\n+              locations.add(new Path(curPart.getTPartition().getSd().getLocation()));\n+            }\n           } else {\n             locations.add(new Path(par.getTPartition().getSd().getLocation()));\n           }\n@@ -649,7 +639,7 @@ private int showTableStatus(Hive db, showTableStatusDesc showTblStatus)\n    * @throws HiveException Throws this exception if an unexpected error occurs.\n    */\n   private int describeTable(Hive db, descTableDesc descTbl)\n-      throws HiveException {\n+  throws HiveException {\n     String colPath = descTbl.getTableName();\n     String tableName = colPath.substring(0,\n         colPath.indexOf('.') == -1 ? colPath.length() : colPath.indexOf('.'));", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java"}, {"additions": 53, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java", "sha": "116efd76421e42b8a5a274d830c049aa8316886b", "changes": 57, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -18,8 +18,10 @@\n \n package org.apache.hadoop.hive.ql.exec;\n \n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge;\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFResolver;\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;\n \n public class FunctionInfo {\n@@ -28,11 +30,11 @@\n \n   private String displayName;\n \n-  private GenericUDF genericUDF = null;\n+  private GenericUDF genericUDF;\n \n-  private GenericUDTF genericUDTF = null;\n+  private GenericUDTF genericUDTF;\n   \n-  private GenericUDAFResolver genericUDAFResolver = null;\n+  private GenericUDAFResolver genericUDAFResolver;\n   \n   public FunctionInfo(boolean isNative, String displayName, GenericUDF genericUDF) {\n     this.isNative = isNative;\n@@ -57,6 +59,9 @@ public FunctionInfo(boolean isNative, String displayName, GenericUDTF genericUDT\n    */\n   public GenericUDF getGenericUDF() {\n     // GenericUDF is stateful - we have to make a copy here\n+    if (genericUDF == null) {\n+      return null;\n+    }\n     return FunctionRegistry.cloneGenericUDF(genericUDF);\n   }\n   \n@@ -65,8 +70,9 @@ public GenericUDF getGenericUDF() {\n    */\n   public GenericUDTF getGenericUDTF() {\n     // GenericUDTF is stateful too, copy\n-    if (genericUDTF == null)\n+    if (genericUDTF == null) {\n       return null;\n+    }\n     return FunctionRegistry.cloneGenericUDTF(genericUDTF);\n   }\n   \n@@ -77,6 +83,28 @@ public GenericUDAFResolver getGenericUDAFResolver() {\n     return genericUDAFResolver;\n   }\n   \n+  /**\n+   * Get the Class of the UDF\n+   */\n+  public Class<?> getFunctionClass() {\n+    if (isGenericUDF()) {\n+      if (genericUDF instanceof GenericUDFBridge) {\n+        return ((GenericUDFBridge)genericUDF).getUdfClass();\n+      } else {\n+        return genericUDF.getClass();\n+      }\n+    } else if (isGenericUDAF()) {\n+      if (genericUDAFResolver instanceof GenericUDAFBridge) {\n+        return ((GenericUDAFBridge)genericUDAFResolver).getUDAFClass();\n+      } else {\n+        return genericUDAFResolver.getClass();\n+      }\n+    } else if (isGenericUDTF()) {\n+      return genericUDTF.getClass();\n+    }\n+    return null;\n+  }\n+  \n   /**\n    * Get the display name for this function.\n    * This should be transfered into exprNodeGenericUDFDesc, and will be \n@@ -94,4 +122,25 @@ public String getDisplayName() {\n   public boolean isNative() {\n     return isNative;\n   }\n+  \n+  /**\n+   * @return TRUE if the function is a GenericUDF\n+   */\n+  public boolean isGenericUDF() {\n+    return null != genericUDF;\n+  }\n+  \n+  /**\n+   * @return TRUE if the function is a GenericUDAF\n+   */\n+  public boolean isGenericUDAF() {\n+    return null != genericUDAFResolver;\n+  }\n+  \n+  /**\n+   * @return TRUE if the function is a GenericUDTF\n+   */\n+  public boolean isGenericUDTF() {\n+    return null != genericUDTF;\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java"}, {"additions": 11, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java", "sha": "7825b3c6a43339c8d83f20038b8296d869593354", "changes": 16, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -56,9 +56,8 @@\n   /**\n    * The mapping from expression function names to expression classes.\n    */\n-  static LinkedHashMap<String, FunctionInfo> mFunctions;\n+  static Map<String, FunctionInfo> mFunctions = new LinkedHashMap<String, FunctionInfo>();\n   static {\n-    mFunctions = new LinkedHashMap<String, FunctionInfo>();\n     registerUDF(\"concat\", UDFConcat.class, false);\n     registerUDF(\"substr\", UDFSubstr.class, false);\n     registerUDF(\"substring\", UDFSubstr.class, false);\n@@ -641,19 +640,26 @@ public static GenericUDF getGenericUDFForAnd() {\n    * Create a copy of an existing GenericUDF.\n    */\n   public static GenericUDF cloneGenericUDF(GenericUDF genericUDF) {\n+    if (null == genericUDF) {\n+      return null;\n+    }\n+    \n     if (genericUDF instanceof GenericUDFBridge) {\n       GenericUDFBridge bridge = (GenericUDFBridge)genericUDF;\n       return new GenericUDFBridge(bridge.getUdfName(), bridge.isOperator(), bridge.getUdfClass());\n-    } else {\n-      return (GenericUDF)ReflectionUtils.newInstance(genericUDF.getClass(), null);\n     }\n+    \n+    return (GenericUDF)ReflectionUtils.newInstance(genericUDF.getClass(), null);\n   }\n \n   /**\n    * Create a copy of an existing GenericUDTF.\n    */\n   public static GenericUDTF cloneGenericUDTF(GenericUDTF genericUDTF) {\n-      return (GenericUDTF)ReflectionUtils.newInstance(genericUDTF.getClass(), null);\n+    if (null == genericUDTF) {\n+      return null;\n+    }\n+    return (GenericUDTF)ReflectionUtils.newInstance(genericUDTF.getClass(), null);\n   }\n   \n   /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/UDAF.java", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/exec/UDAF.java", "sha": "b913a1c03306ac4a5872c54599b743655b7f83fe", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/UDAF.java?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -17,6 +17,8 @@\n  */\n \n package org.apache.hadoop.hive.ql.exec;\n+\n+import org.apache.hadoop.hive.ql.udf.UDFType;\n //import org.apache.hadoop.hive.serde.ReflectionSerDe;\n \n /**", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/UDAF.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMax.java", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMax.java", "sha": "d376085607d7619ddc209611bb35887bd989e47c", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMax.java?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.hadoop.hive.ql.exec.UDAF;\n import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;\n+import org.apache.hadoop.hive.ql.exec.description;\n import org.apache.hadoop.hive.serde2.io.DoubleWritable;\n import org.apache.hadoop.hive.serde2.io.ShortWritable;\n import org.apache.hadoop.hive.shims.ShimLoader;\n@@ -29,7 +30,10 @@\n import org.apache.hadoop.io.Text;\n \n \n-\n+@description(\n+    name = \"max\",\n+    value = \"_FUNC_(expr) - Returns the maximum value of expr\"\n+    )\n public class UDAFMax extends UDAF {\n \n   static public class MaxShortEvaluator implements UDAFEvaluator {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMax.java"}, {"additions": 5, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMin.java", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMin.java", "sha": "8950c0924ec29a5a0b073c6f3cf252d065ab0272", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMin.java?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.hadoop.hive.ql.exec.UDAF;\n import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;\n+import org.apache.hadoop.hive.ql.exec.description;\n import org.apache.hadoop.hive.serde2.io.DoubleWritable;\n import org.apache.hadoop.hive.serde2.io.ShortWritable;\n import org.apache.hadoop.hive.shims.ShimLoader;\n@@ -28,7 +29,10 @@\n import org.apache.hadoop.io.LongWritable;\n import org.apache.hadoop.io.Text;\n \n-\n+@description(\n+    name = \"min\",\n+    value = \"_FUNC_(expr) - Returns the minimum value of expr\"\n+    )\n public class UDAFMin extends UDAF {\n \n   static public class MinShortEvaluator implements UDAFEvaluator {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFMin.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFBridge.java", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFBridge.java", "sha": "da4ac08f72d6367ca2b7d6c87729115578ea5dd9", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFBridge.java?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -27,16 +27,12 @@\n import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n-import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.Mode;\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.ConversionHelper;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.ObjectInspectorOptions;\n-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.util.ReflectionUtils;\n-import org.apache.hadoop.io.Writable;\n \n /**\n  * This class is a bridge between GenericUDAF and UDAF.\n@@ -51,6 +47,10 @@ public GenericUDAFBridge(UDAF udaf) {\n     this.udaf = udaf;\n   }\n   \n+  public Class<? extends UDAF> getUDAFClass() {\n+    return udaf.getClass();\n+  }\n+  \n   @Override\n   public GenericUDAFEvaluator getEvaluator(\n       TypeInfo[] parameters) throws SemanticException {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFBridge.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_avg.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_avg.q", "sha": "6bdc477a24679800c30444dfd95f7e46ec308c30", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_avg.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,4 @@\n+DESCRIBE FUNCTION avg;\n+DESCRIBE FUNCTION EXTENDED avg;\n+DESCRIBE FUNCTION avg;\n+DESCRIBE FUNCTION EXTENDED avg;", "filename": "ql/src/test/queries/clientpositive/udaf_avg.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_count.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_count.q", "sha": "4d254984486f21250f8ba1a1bfa73cdb82e86bcf", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_count.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,4 @@\n+DESCRIBE FUNCTION count;\n+DESCRIBE FUNCTION EXTENDED count;\n+DESCRIBE FUNCTION count;\n+DESCRIBE FUNCTION EXTENDED count;", "filename": "ql/src/test/queries/clientpositive/udaf_count.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_max.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_max.q", "sha": "7cae1e2b29202e2ceeb00f9463b96b4d2f17f019", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_max.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,4 @@\n+DESCRIBE FUNCTION max;\n+DESCRIBE FUNCTION EXTENDED max;\n+DESCRIBE FUNCTION max;\n+DESCRIBE FUNCTION EXTENDED max;", "filename": "ql/src/test/queries/clientpositive/udaf_max.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_min.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_min.q", "sha": "bf484d54653f512f415724c8058cd87cb086496d", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_min.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,4 @@\n+DESCRIBE FUNCTION min;\n+DESCRIBE FUNCTION EXTENDED min;\n+DESCRIBE FUNCTION min;\n+DESCRIBE FUNCTION EXTENDED min;", "filename": "ql/src/test/queries/clientpositive/udaf_min.q"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_number_format.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_number_format.q", "sha": "4f2ab453dc97641495934771cd357c0ac23a2fa3", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_number_format.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,4 @@\n+\n EXPLAIN SELECT\n   sum('a'),\n   avg('a'),", "filename": "ql/src/test/queries/clientpositive/udaf_number_format.q"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_std.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_std.q", "sha": "d17c107a6ce7eddd478cdc2cdb70c994e219ef6d", "changes": 16, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_std.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,16 @@\n+DESCRIBE FUNCTION std;\n+DESCRIBE FUNCTION EXTENDED std;\n+\n+DESCRIBE FUNCTION stddev;\n+DESCRIBE FUNCTION EXTENDED stddev;\n+\n+DESCRIBE FUNCTION stddev_pop;\n+DESCRIBE FUNCTION EXTENDED stddev_pop;\n+DESCRIBE FUNCTION std;\n+DESCRIBE FUNCTION EXTENDED std;\n+\n+DESCRIBE FUNCTION stddev;\n+DESCRIBE FUNCTION EXTENDED stddev;\n+\n+DESCRIBE FUNCTION stddev_pop;\n+DESCRIBE FUNCTION EXTENDED stddev_pop;", "filename": "ql/src/test/queries/clientpositive/udaf_std.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_stddev_samp.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_stddev_samp.q", "sha": "30d2f24513864918fd39353477a1e7c48cdb1203", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_stddev_samp.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,4 @@\n+DESCRIBE FUNCTION stddev_samp;\n+DESCRIBE FUNCTION EXTENDED stddev_samp;\n+DESCRIBE FUNCTION stddev_samp;\n+DESCRIBE FUNCTION EXTENDED stddev_samp;", "filename": "ql/src/test/queries/clientpositive/udaf_stddev_samp.q"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_sum.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_sum.q", "sha": "325bcabd6c9aaba7b4a5057b70de70e644538d16", "changes": 6, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_sum.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,6 @@\n+DESCRIBE FUNCTION sum;\n+DESCRIBE FUNCTION EXTENDED sum;\n+\n+DESCRIBE FUNCTION sum;\n+DESCRIBE FUNCTION EXTENDED sum;\n+", "filename": "ql/src/test/queries/clientpositive/udaf_sum.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_var_samp.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_var_samp.q", "sha": "de68e499005ef823bfdade3b78ab74eda296e401", "changes": 4, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_var_samp.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,4 @@\n+DESCRIBE FUNCTION var_samp;\n+DESCRIBE FUNCTION EXTENDED var_samp;\n+DESCRIBE FUNCTION var_samp;\n+DESCRIBE FUNCTION EXTENDED var_samp;", "filename": "ql/src/test/queries/clientpositive/udaf_var_samp.q"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_variance.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udaf_variance.q", "sha": "579cc8e68f49c6a49a6d9f42bb35827e50fbcb19", "changes": 10, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udaf_variance.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,10 @@\n+DESCRIBE FUNCTION variance;\n+DESCRIBE FUNCTION EXTENDED variance;\n+\n+DESCRIBE FUNCTION var_pop;\n+DESCRIBE FUNCTION EXTENDED var_pop;\n+DESCRIBE FUNCTION variance;\n+DESCRIBE FUNCTION EXTENDED variance;\n+\n+DESCRIBE FUNCTION var_pop;\n+DESCRIBE FUNCTION EXTENDED var_pop;", "filename": "ql/src/test/queries/clientpositive/udaf_variance.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_abs.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_abs.q", "sha": "f4f227d0dc1688f5ca97201b645d19b35d9a082a", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_abs.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION abs;\n+DESCRIBE FUNCTION EXTENDED abs;\n+\n EXPLAIN SELECT\n   abs(0),\n   abs(-1),", "filename": "ql/src/test/queries/clientpositive/udf_abs.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_acos.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_acos.q", "sha": "625a2aa5c6aa6ebc94d9c272f6c0fc25cc24ca1a", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_acos.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION acos;\n+DESCRIBE FUNCTION EXTENDED acos;\n+\n SELECT acos(null)\n FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_acos.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_array.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_array.q", "sha": "7c3c0a00f7de830e9564d73b7923ba1cea30ff8a", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_array.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,7 @@\n+-- Parsing bug requires us to quote array\n+DESCRIBE FUNCTION 'array';\n+DESCRIBE FUNCTION EXTENDED 'array';\n+\n EXPLAIN SELECT array(), array()[1], array(1, 2, 3), array(1, 2, 3)[2], array(1,\"a\", 2, 3), array(1,\"a\", 2, 3)[2],\n array(array(1), array(2), array(3), array(4))[1][0] FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_array.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_ascii.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_ascii.q", "sha": "53b389fd38fd8ef7f4544f99d7e336e4fc45c9e8", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_ascii.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION ascii;\n+DESCRIBE FUNCTION EXTENDED ascii;\n+\n EXPLAIN SELECT\n   ascii('Facebook'),\n   ascii(''),", "filename": "ql/src/test/queries/clientpositive/udf_ascii.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_asin.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_asin.q", "sha": "f95a5f57df8f813e007b4e2ecece420314b9eec6", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_asin.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION asin;\n+DESCRIBE FUNCTION EXTENDED asin;\n+\n SELECT asin(null)\n FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_asin.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_bin.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_bin.q", "sha": "2b9ad62a39dbeeb65f2f157a4d31b072cf81611f", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_bin.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION bin;\n+DESCRIBE FUNCTION EXTENDED bin;\n+\n SELECT\n   bin(1),\n   bin(0),", "filename": "ql/src/test/queries/clientpositive/udf_bin.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_case.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_case.q", "sha": "811c5b80f6549ab47ab5a114acb6b8822af61633", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_case.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,7 @@\n+-- Parsing bug requires us to quote case\n+DESCRIBE FUNCTION 'case';\n+DESCRIBE FUNCTION EXTENDED 'case';\n+\n EXPLAIN\n SELECT CASE 1\n         WHEN 1 THEN 2", "filename": "ql/src/test/queries/clientpositive/udf_case.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_coalesce.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_coalesce.q", "sha": "48ca29cbc3ba647d2fda7245cc23f2677b24d905", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_coalesce.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION coalesce;\n+DESCRIBE FUNCTION EXTENDED coalesce;\n+\n EXPLAIN\n SELECT COALESCE(1),\n        COALESCE(1, 2),", "filename": "ql/src/test/queries/clientpositive/udf_coalesce.q"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_concat.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_concat.q", "sha": "f642f6a2d00df1859c9610c17f6ee6e901405204", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_concat.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,5 +1,4 @@\n DESCRIBE FUNCTION concat;\n-\n DESCRIBE FUNCTION EXTENDED concat;\n \n SELECT", "filename": "ql/src/test/queries/clientpositive/udf_concat.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_conv.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_conv.q", "sha": "ea53deb47fd82efe26d5ea8bdb6abdceee77ffbe", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_conv.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION conv;\n+DESCRIBE FUNCTION EXTENDED conv;\n+\n -- conv must work on both strings and integers up to 64-bit precision\n \n -- Some simple conversions to test different bases", "filename": "ql/src/test/queries/clientpositive/udf_conv.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_cos.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_cos.q", "sha": "7887c4c2603f19041ab975be81481a9a7226bb81", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_cos.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION cos;\n+DESCRIBE FUNCTION EXTENDED cos;\n+\n SELECT cos(null)\n FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_cos.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_divider.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_divider.q", "sha": "35f63851f271efee5079ad9cea53c000af58422a", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_divider.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION div;\n+DESCRIBE FUNCTION EXTENDED div;\n+\n SELECT 3 / 2 FROM SRC LIMIT 1;\n \n SELECT 3 DIV 2 FROM SRC LIMIT 1;", "filename": "ql/src/test/queries/clientpositive/udf_divider.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_elt.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_elt.q", "sha": "c32340ac89feb0a4c8771b2a5db211eded27b5c5", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_elt.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION elt;\n+DESCRIBE FUNCTION EXTENDED elt;\n+\n EXPLAIN\n SELECT elt(2, 'abc', 'defg'),\n        elt(3, 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg'),", "filename": "ql/src/test/queries/clientpositive/udf_elt.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_hash.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_hash.q", "sha": "faf372218a10767b48bfe2e093cf41b6988fd56c", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_hash.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION hash;\n+DESCRIBE FUNCTION EXTENDED hash;\n+\n EXPLAIN\n SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),\n        hash(3), hash(CAST('123456789012' AS BIGINT)),", "filename": "ql/src/test/queries/clientpositive/udf_hash.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_hex.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_hex.q", "sha": "37e035ad42b00cac06e0efb007e2582ca9f779d0", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_hex.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION hex;\n+DESCRIBE FUNCTION EXTENDED hex;\n+\n -- If the argument is a string, hex should return a string containing two hex\n -- digits for every character in the input.\n SELECT", "filename": "ql/src/test/queries/clientpositive/udf_hex.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_if.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_if.q", "sha": "4f7c8b4a36bad8f4a756a6f857f10da590fd7977", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_if.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION if;\n+DESCRIBE FUNCTION EXTENDED if;\n+\n EXPLAIN\n SELECT IF(TRUE, 1, 2) AS COL1,\n        IF(FALSE, CAST(NULL AS STRING), CAST(1 AS STRING)) AS COL2,", "filename": "ql/src/test/queries/clientpositive/udf_if.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_instr.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_instr.q", "sha": "20ed8e4ea01761bad3f90020a29589a231d7e6b3", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_instr.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION instr;\n+DESCRIBE FUNCTION EXTENDED instr;\n+\n EXPLAIN\n SELECT instr('abcd', 'abc'),\n        instr('abcabc', 'ccc'),", "filename": "ql/src/test/queries/clientpositive/udf_instr.q"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_isnull_isnotnull.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_isnull_isnotnull.q", "sha": "d1569cc7f6dbb4f0e3ae1bf08fddc0d554062a4f", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_isnull_isnotnull.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,10 @@\n+DESCRIBE FUNCTION isnull;\n+DESCRIBE FUNCTION EXTENDED isnull;\n+\n+DESCRIBE FUNCTION isnotnull;\n+DESCRIBE FUNCTION EXTENDED isnotnull;\n+\n+\n EXPLAIN\n SELECT NULL IS NULL,\n        1 IS NOT NULL, ", "filename": "ql/src/test/queries/clientpositive/udf_isnull_isnotnull.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_json.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_json.q", "sha": "7a0a1bd49539585f1dca88b1d58b352dcb11fd50", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_json.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION get_json_object;\n+DESCRIBE FUNCTION EXTENDED get_json_object;\n+\n CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;\n \n FROM src INSERT OVERWRITE TABLE dest1 SELECT '  abc  ' WHERE src.key = 86;", "filename": "ql/src/test/queries/clientpositive/udf_json.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_length.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_length.q", "sha": "b14c0e836fb737a6994f073ef0fd0d0f182ce915", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_length.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION length;\n+DESCRIBE FUNCTION EXTENDED length;\n+\n CREATE TABLE dest1(len INT);\n EXPLAIN FROM src1 INSERT OVERWRITE TABLE dest1 SELECT length(src1.value);\n FROM src1 INSERT OVERWRITE TABLE dest1 SELECT length(src1.value);", "filename": "ql/src/test/queries/clientpositive/udf_length.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_like.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_like.q", "sha": "dbc1889d2d623875e1fd897a053ecf3a8fc2c8c2", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_like.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION like;\n+DESCRIBE FUNCTION EXTENDED like;\n+\n EXPLAIN\n SELECT '_%_' LIKE '%\\_\\%\\_%', '__' LIKE '%\\_\\%\\_%', '%%_%_' LIKE '%\\_\\%\\_%', '%_%_%' LIKE '%\\%\\_\\%',\n   '_%_' LIKE '\\%\\_%', '%__' LIKE '__\\%%', '_%' LIKE '\\_\\%\\_\\%%', '_%' LIKE '\\_\\%_%',", "filename": "ql/src/test/queries/clientpositive/udf_like.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_locate.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_locate.q", "sha": "80148d09233066a41ff747bfdc66f12d246f573f", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_locate.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION locate;\n+DESCRIBE FUNCTION EXTENDED locate;\n+\n EXPLAIN\n SELECT locate('abc', 'abcd'),\n        locate('ccc', 'abcabc'),", "filename": "ql/src/test/queries/clientpositive/udf_locate.q"}, {"additions": 2, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_lower.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_lower.q", "sha": "c07d2db88ea8917f748af1e51d10313e64354c02", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_lower.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,5 @@\n+DESCRIBE FUNCTION lower;\n+DESCRIBE FUNCTION EXTENDED lower;\n \n EXPLAIN\n SELECT lower('AbC 123'), upper('AbC 123') FROM src WHERE key = 86;", "filename": "ql/src/test/queries/clientpositive/udf_lower.q"}, {"additions": 6, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_lpad_rpad.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_lpad_rpad.q", "sha": "d84e750cd046538a4dfcda236d045aefd3e7bb5f", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_lpad_rpad.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,9 @@\n+DESCRIBE FUNCTION lpad;\n+DESCRIBE FUNCTION EXTENDED lpad;\n+\n+DESCRIBE FUNCTION rpad;\n+DESCRIBE FUNCTION EXTENDED rpad;\n+\n EXPLAIN SELECT\n   lpad('hi', 1, '?'),\n   lpad('hi', 5, '.'),", "filename": "ql/src/test/queries/clientpositive/udf_lpad_rpad.q"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_map.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_map.q", "sha": "8dfd537406ac9d8b6bb71523cd58bb45565f6d31", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_map.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,7 @@\n+-- Parsing bug requires us to quote map\n+DESCRIBE FUNCTION 'map';\n+DESCRIBE FUNCTION EXTENDED 'map';\n+\n EXPLAIN SELECT map(), map(1, \"a\", 2, \"b\", 3, \"c\"), map(1, 2, \"a\", \"b\"), \n map(1, \"a\", 2, \"b\", 3, \"c\")[2],  map(1, 2, \"a\", \"b\")[\"a\"], map(1, array(\"a\"))[1][0] FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_map.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_negative.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_negative.q", "sha": "d403cc68f4fa7f96fb0cdc3525416ffd70f9e617", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_negative.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION negative;\n+DESCRIBE FUNCTION EXTENDED negative;\n+\n select - null from src limit 1;\n select - cast(null as int) from src limit 1;\n select - cast(null as smallint) from src limit 1;", "filename": "ql/src/test/queries/clientpositive/udf_negative.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_parse_url.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_parse_url.q", "sha": "62c67db20b1a2a97d17952a7924e086e0c549abf", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_parse_url.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION parse_url;\n+DESCRIBE FUNCTION EXTENDED parse_url;\n+\n EXPLAIN\n SELECT parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST'), \n parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'PATH'), ", "filename": "ql/src/test/queries/clientpositive/udf_parse_url.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_pmod.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_pmod.q", "sha": "c45737d8d01b51dd7447e48e1beb137f0452dc1e", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_pmod.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION pmod;\n+DESCRIBE FUNCTION EXTENDED pmod;\n+\n SELECT pmod(null, null)\n FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_pmod.q"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_regexp.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_regexp.q", "sha": "3aee10963d74796f2c596bb66ce8493fb66c0999", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_regexp.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,5 +1,4 @@\n DESCRIBE FUNCTION regexp;\n-\n DESCRIBE FUNCTION EXTENDED regexp;\n \n SELECT 'fofo' REGEXP '^fo', 'fo\\no' REGEXP '^fo\\no$', 'Bn' REGEXP '^Ba*n', 'afofo' REGEXP 'fo',", "filename": "ql/src/test/queries/clientpositive/udf_regexp.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_repeat.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_repeat.q", "sha": "162085f4c71c8f528baef6c7db371a7472000e62", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_repeat.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION repeat;\n+DESCRIBE FUNCTION EXTENDED repeat;\n+\n EXPLAIN SELECT\n   repeat(\"Facebook\", 3),\n   repeat(\"\", 4),", "filename": "ql/src/test/queries/clientpositive/udf_repeat.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_reverse.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_reverse.q", "sha": "d042a25604b323b341644cad6686c3f8ad9191a0", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_reverse.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION reverse;\n+DESCRIBE FUNCTION EXTENDED reverse;\n+\n CREATE TABLE dest1(len STRING);\n EXPLAIN FROM src1 INSERT OVERWRITE TABLE dest1 SELECT reverse(src1.value);\n FROM src1 INSERT OVERWRITE TABLE dest1 SELECT reverse(src1.value);", "filename": "ql/src/test/queries/clientpositive/udf_reverse.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_round.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_round.q", "sha": "58d31eb08c1e92c6478f85f724e34e085e497b0a", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_round.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION round;\n+DESCRIBE FUNCTION EXTENDED round;\n+\n SELECT round(null), round(null, 0), round(125, null)\n FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_round.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_sin.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_sin.q", "sha": "abb7cac8dac9d7a0cb9af086c48f3d6d3432bdac", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_sin.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION sin;\n+DESCRIBE FUNCTION EXTENDED sin;\n+\n SELECT sin(null)\n FROM src LIMIT 1;\n ", "filename": "ql/src/test/queries/clientpositive/udf_sin.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_size.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_size.q", "sha": "8aaa68a6e318a8c7c58947e5141d5523e572dcca", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_size.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION size;\n+DESCRIBE FUNCTION EXTENDED size;\n+\n EXPLAIN\n FROM src_thrift\n SELECT size(src_thrift.lint), ", "filename": "ql/src/test/queries/clientpositive/udf_size.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_space.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_space.q", "sha": "cf6466fb63bad60f10316b1abc85c5dd384cf3fd", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_space.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION space;\n+DESCRIBE FUNCTION EXTENDED space;\n+\n EXPLAIN SELECT\n   space(10),\n   space(0),", "filename": "ql/src/test/queries/clientpositive/udf_space.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_split.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_split.q", "sha": "f79901736cf7c5089fc6198bc96d41de575e99c2", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_split.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION split;\n+DESCRIBE FUNCTION EXTENDED split;\n+\n EXPLAIN SELECT \n   split('a b c', ' '),\n   split('oneAtwoBthreeC', '[ABC]'),", "filename": "ql/src/test/queries/clientpositive/udf_split.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_substr.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_substr.q", "sha": "05e9e1b4c43cf8a157e8a579fbb223ef7c90f9a4", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_substr.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION substr;\n+DESCRIBE FUNCTION EXTENDED substr;\n+\n SELECT\n   substr(null, 1), substr(null, 1, 1),\n   substr('ABC', null), substr('ABC', null, 1),", "filename": "ql/src/test/queries/clientpositive/udf_substr.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_unhex.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_unhex.q", "sha": "e80021c8b2c51d9269ad9b1bd72daf9d9c913eb4", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_unhex.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION unhex;\n+DESCRIBE FUNCTION EXTENDED unhex;\n+\n -- Good inputs\n \n SELECT", "filename": "ql/src/test/queries/clientpositive/udf_unhex.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_unix_timestamp.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_unix_timestamp.q", "sha": "9c38f53a0ac8fc4fa4c61d81c5e436bf809466a1", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_unix_timestamp.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION unix_timestamp;\n+DESCRIBE FUNCTION EXTENDED unix_timestamp;\n+\n SELECT\n   '2009-03-20 11:30:01',\n   unix_timestamp('2009-03-20 11:30:01')", "filename": "ql/src/test/queries/clientpositive/udf_unix_timestamp.q"}, {"additions": 0, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_weekofyear.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udf_weekofyear.q", "sha": "4b7b4ea55aa8148ac4b53912df1909469a106dbb", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udf_weekofyear.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,5 +1,4 @@\n DESCRIBE FUNCTION weekofyear;\n-\n DESCRIBE FUNCTION EXTENDED weekofyear;\n \n SELECT weekofyear('1980-01-01'), weekofyear('1980-01-06'), weekofyear('1980-01-07'), weekofyear('1980-12-31'),", "filename": "ql/src/test/queries/clientpositive/udf_weekofyear.q"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udtf_explode.q", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/queries/clientpositive/udtf_explode.q", "sha": "f5c03237c062a25522e07f5dc0581d055f9da853", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/udtf_explode.q?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,6 @@\n+DESCRIBE FUNCTION explode;\n+DESCRIBE FUNCTION EXTENDED explode;\n+\n EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3;\n EXPLAIN EXTENDED SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol;\n ", "filename": "ql/src/test/queries/clientpositive/udtf_explode.q"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_avg.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_avg.q.out", "sha": "776f6b919dd4199b7aac50c0bae9f39d1c014fe4", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_avg.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION avg\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION avg\n+POSTHOOK: type: DESCFUNCTION\n+avg(x) - Returns the mean of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED avg\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED avg\n+POSTHOOK: type: DESCFUNCTION\n+avg(x) - Returns the mean of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION avg\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION avg\n+POSTHOOK: type: DESCFUNCTION\n+avg(x) - Returns the mean of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED avg\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED avg\n+POSTHOOK: type: DESCFUNCTION\n+avg(x) - Returns the mean of a set of numbers", "filename": "ql/src/test/results/clientpositive/udaf_avg.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_count.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_count.q.out", "sha": "dc519f352a6e02d0c532198da0f588c1d906efe5", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_count.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION count\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION count\n+POSTHOOK: type: DESCFUNCTION\n+count(x) - Returns the count\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED count\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED count\n+POSTHOOK: type: DESCFUNCTION\n+count(x) - Returns the count\n+PREHOOK: query: DESCRIBE FUNCTION count\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION count\n+POSTHOOK: type: DESCFUNCTION\n+count(x) - Returns the count\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED count\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED count\n+POSTHOOK: type: DESCFUNCTION\n+count(x) - Returns the count", "filename": "ql/src/test/results/clientpositive/udaf_count.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_max.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_max.q.out", "sha": "e1dfb3d6d4aa0d0eae4d9ce3f4a9ec9ad73dfb17", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_max.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION max\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION max\n+POSTHOOK: type: DESCFUNCTION\n+max(expr) - Returns the maximum value of expr\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED max\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED max\n+POSTHOOK: type: DESCFUNCTION\n+max(expr) - Returns the maximum value of expr\n+PREHOOK: query: DESCRIBE FUNCTION max\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION max\n+POSTHOOK: type: DESCFUNCTION\n+max(expr) - Returns the maximum value of expr\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED max\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED max\n+POSTHOOK: type: DESCFUNCTION\n+max(expr) - Returns the maximum value of expr", "filename": "ql/src/test/results/clientpositive/udaf_max.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_min.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_min.q.out", "sha": "d5903ec0718bf053b1c0b027928f117fa697c5c5", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_min.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION min\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION min\n+POSTHOOK: type: DESCFUNCTION\n+min(expr) - Returns the minimum value of expr\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED min\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED min\n+POSTHOOK: type: DESCFUNCTION\n+min(expr) - Returns the minimum value of expr\n+PREHOOK: query: DESCRIBE FUNCTION min\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION min\n+POSTHOOK: type: DESCFUNCTION\n+min(expr) - Returns the minimum value of expr\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED min\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED min\n+POSTHOOK: type: DESCFUNCTION\n+min(expr) - Returns the minimum value of expr", "filename": "ql/src/test/results/clientpositive/udaf_min.q.out"}, {"additions": 60, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_std.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_std.q.out", "sha": "cdb01c066643757224533d4d3ef84ca7e37413f6", "changes": 60, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_std.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,60 @@\n+PREHOOK: query: DESCRIBE FUNCTION std\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION std\n+POSTHOOK: type: DESCFUNCTION\n+std(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED std\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED std\n+POSTHOOK: type: DESCFUNCTION\n+std(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION stddev\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION stddev\n+POSTHOOK: type: DESCFUNCTION\n+stddev(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED stddev\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED stddev\n+POSTHOOK: type: DESCFUNCTION\n+stddev(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION stddev_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION stddev_pop\n+POSTHOOK: type: DESCFUNCTION\n+stddev_pop(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_pop\n+POSTHOOK: type: DESCFUNCTION\n+stddev_pop(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION std\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION std\n+POSTHOOK: type: DESCFUNCTION\n+std(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED std\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED std\n+POSTHOOK: type: DESCFUNCTION\n+std(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION stddev\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION stddev\n+POSTHOOK: type: DESCFUNCTION\n+stddev(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED stddev\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED stddev\n+POSTHOOK: type: DESCFUNCTION\n+stddev(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION stddev_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION stddev_pop\n+POSTHOOK: type: DESCFUNCTION\n+stddev_pop(x) - Returns the standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_pop\n+POSTHOOK: type: DESCFUNCTION\n+stddev_pop(x) - Returns the standard deviation of a set of numbers", "filename": "ql/src/test/results/clientpositive/udaf_std.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_stddev_samp.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_stddev_samp.q.out", "sha": "6619ea1b1db96600c231a1645b6f23ef6c019f97", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_stddev_samp.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION stddev_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION stddev_samp\n+POSTHOOK: type: DESCFUNCTION\n+stddev_samp(x) - Returns the sample standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_samp\n+POSTHOOK: type: DESCFUNCTION\n+stddev_samp(x) - Returns the sample standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION stddev_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION stddev_samp\n+POSTHOOK: type: DESCFUNCTION\n+stddev_samp(x) - Returns the sample standard deviation of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED stddev_samp\n+POSTHOOK: type: DESCFUNCTION\n+stddev_samp(x) - Returns the sample standard deviation of a set of numbers", "filename": "ql/src/test/results/clientpositive/udaf_stddev_samp.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_sum.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_sum.q.out", "sha": "2f4e3da8b8cd930fcd77a8ff2e444c06793138a4", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_sum.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION sum\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION sum\n+POSTHOOK: type: DESCFUNCTION\n+sum(x) - Returns the sum of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED sum\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED sum\n+POSTHOOK: type: DESCFUNCTION\n+sum(x) - Returns the sum of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION sum\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION sum\n+POSTHOOK: type: DESCFUNCTION\n+sum(x) - Returns the sum of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED sum\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED sum\n+POSTHOOK: type: DESCFUNCTION\n+sum(x) - Returns the sum of a set of numbers", "filename": "ql/src/test/results/clientpositive/udaf_sum.q.out"}, {"additions": 20, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_var_samp.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_var_samp.q.out", "sha": "190435d0f2c5b46ae96b458598b6944f82f69a93", "changes": 20, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_var_samp.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION var_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION var_samp\n+POSTHOOK: type: DESCFUNCTION\n+var_samp(x) - Returns the sample variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED var_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED var_samp\n+POSTHOOK: type: DESCFUNCTION\n+var_samp(x) - Returns the sample variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION var_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION var_samp\n+POSTHOOK: type: DESCFUNCTION\n+var_samp(x) - Returns the sample variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED var_samp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED var_samp\n+POSTHOOK: type: DESCFUNCTION\n+var_samp(x) - Returns the sample variance of a set of numbers", "filename": "ql/src/test/results/clientpositive/udaf_var_samp.q.out"}, {"additions": 40, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_variance.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udaf_variance.q.out", "sha": "626e554a18b6409479dbdb4ff6ccc4af5c34a069", "changes": 40, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udaf_variance.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -0,0 +1,40 @@\n+PREHOOK: query: DESCRIBE FUNCTION variance\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION variance\n+POSTHOOK: type: DESCFUNCTION\n+variance(x) - Returns the variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED variance\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED variance\n+POSTHOOK: type: DESCFUNCTION\n+variance(x) - Returns the variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION var_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION var_pop\n+POSTHOOK: type: DESCFUNCTION\n+var_pop(x) - Returns the variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED var_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED var_pop\n+POSTHOOK: type: DESCFUNCTION\n+var_pop(x) - Returns the variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION variance\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION variance\n+POSTHOOK: type: DESCFUNCTION\n+variance(x) - Returns the variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED variance\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED variance\n+POSTHOOK: type: DESCFUNCTION\n+variance(x) - Returns the variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION var_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION var_pop\n+POSTHOOK: type: DESCFUNCTION\n+var_pop(x) - Returns the variance of a set of numbers\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED var_pop\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED var_pop\n+POSTHOOK: type: DESCFUNCTION\n+var_pop(x) - Returns the variance of a set of numbers", "filename": "ql/src/test/results/clientpositive/udaf_variance.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_abs.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_abs.q.out", "sha": "25d7ecffeb51baaede814af437d2e399d09d069a", "changes": 23, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_abs.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,18 @@\n+PREHOOK: query: DESCRIBE FUNCTION abs\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION abs\n+POSTHOOK: type: DESCFUNCTION\n+abs(x) - returns the absolute value of x\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED abs\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED abs\n+POSTHOOK: type: DESCFUNCTION\n+abs(x) - returns the absolute value of x\n+Example:\n+  > SELECT abs(0) FROM src LIMIT 1;\n+  0\n+  > SELECT abs(-5) FROM src LIMIT 1;\n+  5\n PREHOOK: query: EXPLAIN SELECT\n   abs(0),\n   abs(-1),\n@@ -63,7 +78,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/2094182155/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1461910312/10000\n POSTHOOK: query: SELECT\n   abs(0),\n   abs(-1),\n@@ -73,7 +88,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/2094182155/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1461910312/10000\n 0\t1\t123\t9223372036854775807\t9223372036854775807\n PREHOOK: query: EXPLAIN SELECT\n   abs(0.0),\n@@ -130,13 +145,13 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/108724608/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1794760562/10000\n POSTHOOK: query: SELECT\n   abs(0.0),\n   abs(-3.14159265),\n   abs(3.14159265)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/108724608/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1794760562/10000\n 0.0\t3.14159265\t3.14159265", "filename": "ql/src/test/results/clientpositive/udf_abs.q.out"}, {"additions": 23, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_acos.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_acos.q.out", "sha": "005987fe307ee66c942a300a06aa44bc5c89ce9a", "changes": 31, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_acos.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,44 +1,59 @@\n+PREHOOK: query: DESCRIBE FUNCTION acos\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION acos\n+POSTHOOK: type: DESCFUNCTION\n+acos(x) - returns the arc cosine of x if -1<=x<=1 or NULL otherwise\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED acos\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED acos\n+POSTHOOK: type: DESCFUNCTION\n+acos(x) - returns the arc cosine of x if -1<=x<=1 or NULL otherwise\n+Example:\n+  > SELECT acos(1) FROM src LIMIT 1;\n+  0\n+  > SELECT acos(2) FROM src LIMIT 1;\n+  NULL\n PREHOOK: query: SELECT acos(null)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1457812629/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/959577580/10000\n POSTHOOK: query: SELECT acos(null)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1457812629/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/959577580/10000\n NULL\n PREHOOK: query: SELECT acos(0)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1180311283/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/227674626/10000\n POSTHOOK: query: SELECT acos(0)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1180311283/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/227674626/10000\n 1.5707963267948966\n PREHOOK: query: SELECT acos(-0.5), asin(0.66)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/804897720/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1521007936/10000\n POSTHOOK: query: SELECT acos(-0.5), asin(0.66)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/804897720/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1521007936/10000\n 2.0943951023931957\t0.7208187608700897\n PREHOOK: query: SELECT acos(2)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/615514597/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/325299096/10000\n POSTHOOK: query: SELECT acos(2)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/615514597/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/325299096/10000\n NaN", "filename": "ql/src/test/results/clientpositive/udf_acos.q.out"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_array.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_array.q.out", "sha": "557bf4f0cb64a5f6ea446312d674ec2d7f63d982", "changes": 16, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_array.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,15 @@\n+PREHOOK: query: -- Parsing bug requires us to quote array\n+DESCRIBE FUNCTION 'array'\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: -- Parsing bug requires us to quote array\n+DESCRIBE FUNCTION 'array'\n+POSTHOOK: type: DESCFUNCTION\n+array(n0, n1...) - Creates an array with the given elements \n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED 'array'\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED 'array'\n+POSTHOOK: type: DESCFUNCTION\n+array(n0, n1...) - Creates an array with the given elements \n PREHOOK: query: EXPLAIN SELECT array(), array()[1], array(1, 2, 3), array(1, 2, 3)[2], array(1,\"a\", 2, 3), array(1,\"a\", 2, 3)[2],\n array(array(1), array(2), array(3), array(4))[1][0] FROM src LIMIT 1\n PREHOOK: type: QUERY\n@@ -52,10 +64,10 @@ PREHOOK: query: SELECT array(), array()[1], array(1, 2, 3), array(1, 2, 3)[2], a\n array(array(1), array(2), array(3), array(4))[1][0] FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/pyang/trunk-HIVE-554/VENDOR.hive/trunk/build/ql/tmp/1854297096/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1972672782/10000\n POSTHOOK: query: SELECT array(), array()[1], array(1, 2, 3), array(1, 2, 3)[2], array(1,\"a\", 2, 3), array(1,\"a\", 2, 3)[2],\n array(array(1), array(2), array(3), array(4))[1][0] FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/pyang/trunk-HIVE-554/VENDOR.hive/trunk/build/ql/tmp/1854297096/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1972672782/10000\n []\tNULL\t[1,2,3]\t3\t[\"1\",\"a\",\"2\",\"3\"]\t2\t2", "filename": "ql/src/test/results/clientpositive/udf_array.q.out"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_ascii.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_ascii.q.out", "sha": "ba37e39ce426d10f6d751483c46c20356dcbfec2", "changes": 19, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_ascii.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,18 @@\n+PREHOOK: query: DESCRIBE FUNCTION ascii\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION ascii\n+POSTHOOK: type: DESCFUNCTION\n+ascii(str) - returns the numeric value of the first character of str\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED ascii\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED ascii\n+POSTHOOK: type: DESCFUNCTION\n+ascii(str) - returns the numeric value of the first character of str\n+Returns 0 if str is empty or NULL if str is NULL\n+Example:\n+  > SELECT ascii('222') FROM src LIMIT 1;  50\n+  > SELECT ascii(2) FROM src LIMIT 1;\n+  50\n PREHOOK: query: EXPLAIN SELECT\n   ascii('Facebook'),\n   ascii(''),\n@@ -53,13 +68,13 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/772117818/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1375319396/10000\n POSTHOOK: query: SELECT\n   ascii('Facebook'),\n   ascii(''),\n   ascii('!')\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/772117818/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1375319396/10000\n 70\t0\t33", "filename": "ql/src/test/results/clientpositive/udf_ascii.q.out"}, {"additions": 23, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_asin.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_asin.q.out", "sha": "e3ec006074e939ce0934fdcfa4c8d0f5ec3c38b6", "changes": 31, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_asin.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,44 +1,59 @@\n+PREHOOK: query: DESCRIBE FUNCTION asin\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION asin\n+POSTHOOK: type: DESCFUNCTION\n+asin(x) - returns the arc sine of x if -1<=x<=1 or NULL otherwise\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED asin\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED asin\n+POSTHOOK: type: DESCFUNCTION\n+asin(x) - returns the arc sine of x if -1<=x<=1 or NULL otherwise\n+Example:\n+  > SELECT asin(0) FROM src LIMIT 1;\n+  0\n+  > SELECT asin(2) FROM src LIMIT 1;\n+  NULL\n PREHOOK: query: SELECT asin(null)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/335338053/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/267750936/10000\n POSTHOOK: query: SELECT asin(null)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/335338053/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/267750936/10000\n NULL\n PREHOOK: query: SELECT asin(0)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1930989265/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/833606284/10000\n POSTHOOK: query: SELECT asin(0)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1930989265/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/833606284/10000\n 0.0\n PREHOOK: query: SELECT asin(-0.5), asin(0.66)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1036919646/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/604043294/10000\n POSTHOOK: query: SELECT asin(-0.5), asin(0.66)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1036919646/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/604043294/10000\n -0.5235987755982989\t0.7208187608700897\n PREHOOK: query: SELECT asin(2)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1231279693/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1494381105/10000\n POSTHOOK: query: SELECT asin(2)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1231279693/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1494381105/10000\n NaN", "filename": "ql/src/test/results/clientpositive/udf_asin.q.out"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_bin.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_bin.q.out", "sha": "57b2716877e73a80ddeb10a6f4a8b9bf532bad25", "changes": 22, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_bin.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,28 +1,42 @@\n+PREHOOK: query: DESCRIBE FUNCTION bin\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION bin\n+POSTHOOK: type: DESCFUNCTION\n+bin(n) - returns n in binary\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED bin\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED bin\n+POSTHOOK: type: DESCFUNCTION\n+bin(n) - returns n in binary\n+n is a BIGINT. Returns NULL if n is NULL.\n+Example:\n+  > SELECT bin(13) FROM src LIMIT 1\n+  '1101'\n PREHOOK: query: SELECT\n   bin(1),\n   bin(0),\n   bin(99992421)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/359409613/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/888920394/10000\n POSTHOOK: query: SELECT\n   bin(1),\n   bin(0),\n   bin(99992421)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/359409613/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/888920394/10000\n 1\t0\t101111101011100001101100101\n PREHOOK: query: -- Negative numbers should be treated as two's complement (64 bit).\n SELECT bin(-5) FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/991182455/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1468739151/10000\n POSTHOOK: query: -- Negative numbers should be treated as two's complement (64 bit).\n SELECT bin(-5) FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/991182455/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1468739151/10000\n 1111111111111111111111111111111111111111111111111111111111111011", "filename": "ql/src/test/results/clientpositive/udf_bin.q.out"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_case.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_case.q.out", "sha": "400ef314aa40b6e855c853a25cd8dabd2f0ed666", "changes": 16, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_case.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,15 @@\n+PREHOOK: query: -- Parsing bug requires us to quote case\n+DESCRIBE FUNCTION 'case'\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: -- Parsing bug requires us to quote case\n+DESCRIBE FUNCTION 'case'\n+POSTHOOK: type: DESCFUNCTION\n+There is no documentation for function case\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED 'case'\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED 'case'\n+POSTHOOK: type: DESCFUNCTION\n+There is no documentation for function case\n PREHOOK: query: EXPLAIN\n SELECT CASE 1\n         WHEN 1 THEN 2\n@@ -124,7 +136,7 @@ PREHOOK: query: SELECT CASE 1\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/606727511/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/908511377/10000\n POSTHOOK: query: SELECT CASE 1\n         WHEN 1 THEN 2\n         WHEN 3 THEN 4\n@@ -153,5 +165,5 @@ POSTHOOK: query: SELECT CASE 1\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/606727511/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/908511377/10000\n 2\t5\t15\tNULL\t20\t24", "filename": "ql/src/test/results/clientpositive/udf_case.q.out"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_coalesce.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_coalesce.q.out", "sha": "703f276a5a0fcdca816b648ee3ed69942b39007a", "changes": 21, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_coalesce.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION coalesce\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION coalesce\n+POSTHOOK: type: DESCFUNCTION\n+coalesce(a1, a2, ...) - Returns the first non-null argument\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED coalesce\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED coalesce\n+POSTHOOK: type: DESCFUNCTION\n+coalesce(a1, a2, ...) - Returns the first non-null argument\n+Example:\n+  > SELECT coalesce(NULL, 1, NULL) FROM src LIMIT 1;\n+  1\n PREHOOK: query: EXPLAIN\n SELECT COALESCE(1),\n        COALESCE(1, 2),\n@@ -127,7 +140,7 @@ PREHOOK: query: SELECT COALESCE(1),\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1848821474/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1867907378/10000\n POSTHOOK: query: SELECT COALESCE(1),\n        COALESCE(1, 2),\n        COALESCE(NULL, 2),\n@@ -149,7 +162,7 @@ POSTHOOK: query: SELECT COALESCE(1),\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1848821474/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1867907378/10000\n 1\t1\t2\t1\t3\t4\t1\t1\t2\t1\t3\t4\t1.0\t1.0\t2.0\t2.0\t2.0\tNULL\n PREHOOK: query: EXPLAIN\n SELECT COALESCE(src_thrift.lint[1], 999),\n@@ -204,14 +217,14 @@ PREHOOK: query: SELECT COALESCE(src_thrift.lint[1], 999),\n FROM src_thrift\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_thrift\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1800031969/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/791594604/10000\n POSTHOOK: query: SELECT COALESCE(src_thrift.lint[1], 999),\n        COALESCE(src_thrift.lintstring[0].mystring, '999'),\n        COALESCE(src_thrift.mstringstring['key_2'], '999')\n FROM src_thrift\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_thrift\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1800031969/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/791594604/10000\n 0\t0\t999\n 2\t1\t999\n 4\t8\tvalue_2", "filename": "ql/src/test/results/clientpositive/udf_coalesce.q.out"}, {"additions": 32, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_conv.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_conv.q.out", "sha": "924b6bb930b3c34a515948da2141896223f9bdad", "changes": 48, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_conv.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,19 @@\n+PREHOOK: query: DESCRIBE FUNCTION conv\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION conv\n+POSTHOOK: type: DESCFUNCTION\n+conv(num, from_base, to_base) - convert num from from_base to to_base\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED conv\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED conv\n+POSTHOOK: type: DESCFUNCTION\n+conv(num, from_base, to_base) - convert num from from_base to to_base\n+If to_base is negative, treat num as a signed integer,otherwise, treat it as an unsigned integer.\n+Example:\n+  > SELECT conv('100', 2, 10) FROM src LIMIT 1;\n+  '4'\n+  > SELECT conv(-10, 16, -10) FROM src LIMIT 1;\n+  '16'\n PREHOOK: query: -- conv must work on both strings and integers up to 64-bit precision\n \n -- Some simple conversions to test different bases\n@@ -9,7 +25,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1394785532/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/502953018/10000\n POSTHOOK: query: -- conv must work on both strings and integers up to 64-bit precision\n \n -- Some simple conversions to test different bases\n@@ -21,7 +37,7 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1394785532/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/502953018/10000\n 3HL\t22\t33\t116ED2B2FB4\n PREHOOK: query: -- Test negative numbers. If to_base is positive, the number should be handled\n -- as a two's complement (64-bit)\n@@ -33,7 +49,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/297144160/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1045444731/10000\n POSTHOOK: query: -- Test negative numbers. If to_base is positive, the number should be handled\n -- as a two's complement (64-bit)\n SELECT\n@@ -44,7 +60,7 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/297144160/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1045444731/10000\n -641\tB\tFFFFFFFFFFFFFFFF\tFFFFFFFFFFFFFFF1\n PREHOOK: query: -- Test overflow. If a number is two large, the result should be -1 (if signed)\n -- or MAX_LONG (if unsigned)\n@@ -56,7 +72,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/311355603/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/901682273/10000\n POSTHOOK: query: -- Test overflow. If a number is two large, the result should be -1 (if signed)\n -- or MAX_LONG (if unsigned)\n SELECT\n@@ -67,7 +83,7 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/311355603/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/901682273/10000\n FFFFFFFFFFFFFFFF\t-1\tFFFFFFFFFFFFFFFF\t-1\n PREHOOK: query: -- Test with invalid input. If one of the bases is invalid, the result should\n -- be NULL. If there is an invalid digit in the number, the longest valid\n@@ -80,7 +96,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1893103412/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1068643579/10000\n POSTHOOK: query: -- Test with invalid input. If one of the bases is invalid, the result should\n -- be NULL. If there is an invalid digit in the number, the longest valid\n -- prefix should be converted.\n@@ -92,7 +108,7 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1893103412/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1068643579/10000\n 5\tNULL\tNULL\tNULL\n PREHOOK: query: -- Perform the same tests with number arguments.\n \n@@ -103,7 +119,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/374689536/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/877878753/10000\n POSTHOOK: query: -- Perform the same tests with number arguments.\n \n SELECT\n@@ -113,7 +129,7 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/374689536/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/877878753/10000\n 3HL\t22\t33\n PREHOOK: query: SELECT\n   conv(-641, 10, -10),\n@@ -123,7 +139,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/117827629/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1990388663/10000\n POSTHOOK: query: SELECT\n   conv(-641, 10, -10),\n   conv(1011, 2, -16),\n@@ -132,7 +148,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/117827629/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1990388663/10000\n -641\tB\tFFFFFFFFFFFFFFFF\tFFFFFFFFFFFFFFF1\n PREHOOK: query: SELECT\n   conv(9223372036854775807, 36, 16),\n@@ -142,7 +158,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/638334947/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1540368732/10000\n POSTHOOK: query: SELECT\n   conv(9223372036854775807, 36, 16),\n   conv(9223372036854775807, 36, -16),\n@@ -151,7 +167,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/638334947/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1540368732/10000\n FFFFFFFFFFFFFFFF\t-1\tFFFFFFFFFFFFFFFF\t-1\n PREHOOK: query: SELECT\n   conv(123455, 3, 10),\n@@ -161,7 +177,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/315464649/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1788821246/10000\n POSTHOOK: query: SELECT\n   conv(123455, 3, 10),\n   conv(131, 1, 5),\n@@ -170,5 +186,5 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/315464649/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1788821246/10000\n 5\tNULL\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_conv.q.out"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_cos.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_cos.q.out", "sha": "def0238a0d1cd11799f67090d2fd203a09fd5654", "changes": 21, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_cos.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,22 +1,35 @@\n+PREHOOK: query: DESCRIBE FUNCTION cos\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION cos\n+POSTHOOK: type: DESCFUNCTION\n+cos(x) - returns the cosine of x (x is in radians)\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED cos\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED cos\n+POSTHOOK: type: DESCFUNCTION\n+cos(x) - returns the cosine of x (x is in radians)\n+Example:\n+   > SELECT cos(0) FROM src LIMIT 1;\n+  1\n PREHOOK: query: SELECT cos(null)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1521763480/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2017515612/10000\n POSTHOOK: query: SELECT cos(null)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1521763480/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2017515612/10000\n NULL\n PREHOOK: query: SELECT cos(0.98), cos(1.57), cos(-0.5)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1918624552/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1873387849/10000\n POSTHOOK: query: SELECT cos(0.98), cos(1.57), cos(-0.5)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1918624552/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1873387849/10000\n 0.5570225467662173\t7.963267107332633E-4\t0.8775825618903728", "filename": "ql/src/test/results/clientpositive/udf_cos.q.out"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_divider.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_divider.q.out", "sha": "9a8e501747853dfa536119a1ab5f1ce05376886a", "changes": 21, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_divider.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,18 +1,31 @@\n+PREHOOK: query: DESCRIBE FUNCTION div\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION div\n+POSTHOOK: type: DESCFUNCTION\n+a div b - Divide a by b rounded to the long integer\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED div\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED div\n+POSTHOOK: type: DESCFUNCTION\n+a div b - Divide a by b rounded to the long integer\n+Example:\n+  > SELECT 3 div 2 FROM src LIMIT 1;\n+  1\n PREHOOK: query: SELECT 3 / 2 FROM SRC LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/399492793/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/173284244/10000\n POSTHOOK: query: SELECT 3 / 2 FROM SRC LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/399492793/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/173284244/10000\n 1.5\n PREHOOK: query: SELECT 3 DIV 2 FROM SRC LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1110831247/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1585227313/10000\n POSTHOOK: query: SELECT 3 DIV 2 FROM SRC LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1110831247/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1585227313/10000\n 1", "filename": "ql/src/test/results/clientpositive/udf_divider.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_elt.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_elt.q.out", "sha": "3af2c51ae2c7d5a72464cd417f1d22b58a146afe", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_elt.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION elt\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION elt\n+POSTHOOK: type: DESCFUNCTION\n+elt(n, str1, str2, ...) - returns the n-th string\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED elt\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED elt\n+POSTHOOK: type: DESCFUNCTION\n+elt(n, str1, str2, ...) - returns the n-th string\n+Example:\n+  > SELECT elt(1, 'face', 'book') FROM src LIMIT 1;\n+  'face'\n PREHOOK: query: EXPLAIN\n SELECT elt(2, 'abc', 'defg'),\n        elt(3, 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg'),\n@@ -92,7 +105,7 @@ PREHOOK: query: SELECT elt(2, 'abc', 'defg'),\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1564252056/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1020024876/10000\n POSTHOOK: query: SELECT elt(2, 'abc', 'defg'),\n        elt(3, 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg'),\n        elt('1', 'abc', 'defg'),\n@@ -107,5 +120,5 @@ POSTHOOK: query: SELECT elt(2, 'abc', 'defg'),\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1564252056/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1020024876/10000\n defg\tcc\tabc\t2\t12345\t123456789012\t1.25\t16.0\tNULL\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_elt.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_hash.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_hash.q.out", "sha": "7d8ff24284d35c2e216ea3a47d083a71c96ae3b1", "changes": 14, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_hash.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,13 @@\n+PREHOOK: query: DESCRIBE FUNCTION hash\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION hash\n+POSTHOOK: type: DESCFUNCTION\n+hash(a1, a2, ...) - Returns a hash value of the arguments\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED hash\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED hash\n+POSTHOOK: type: DESCFUNCTION\n+hash(a1, a2, ...) - Returns a hash value of the arguments\n PREHOOK: query: EXPLAIN\n SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),\n        hash(3), hash(CAST('123456789012' AS BIGINT)),\n@@ -74,7 +84,7 @@ PREHOOK: query: SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/875530574/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/697748715/10000\n POSTHOOK: query: SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),\n        hash(3), hash(CAST('123456789012' AS BIGINT)),\n        hash(CAST(1.25 AS FLOAT)), hash(CAST(16.0 AS DOUBLE)),\n@@ -83,5 +93,5 @@ POSTHOOK: query: SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/875530574/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/697748715/10000\n 1\t2\t3\t-1097262584\t1067450368\t1076887552\t51508\t96354\t1\t0\t1026", "filename": "ql/src/test/results/clientpositive/udf_hash.q.out"}, {"additions": 23, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_hex.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_hex.q.out", "sha": "767fab690d2adf032564d8810fd851622b1f1e83", "changes": 29, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_hex.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,20 @@\n+PREHOOK: query: DESCRIBE FUNCTION hex\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION hex\n+POSTHOOK: type: DESCFUNCTION\n+hex(n or str) - Convert the argument to hexadecimal \n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED hex\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED hex\n+POSTHOOK: type: DESCFUNCTION\n+hex(n or str) - Convert the argument to hexadecimal \n+If the argument is a string, returns two hex digits for each character in the string.\n+If the argument is a number, returns the hexadecimal representation.\n+Example:\n+  > SELECT hex(17) FROM src LIMIT 1;\n+  'H1'\n+  > SELECT hex('Facebook') FROM src LIMIT 1;\n+  '46616365626F6F6B'\n PREHOOK: query: -- If the argument is a string, hex should return a string containing two hex\n -- digits for every character in the input.\n SELECT\n@@ -7,7 +24,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1894329188/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/871424994/10000\n POSTHOOK: query: -- If the argument is a string, hex should return a string containing two hex\n -- digits for every character in the input.\n SELECT\n@@ -17,7 +34,7 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1894329188/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/871424994/10000\n 46616365626F6F6B\t00\t71776572747975696F706173646667686A6B6C\n PREHOOK: query: -- If the argument is a number, hex should convert it to hexadecimal.\n SELECT\n@@ -27,7 +44,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1277538639/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1308427474/10000\n POSTHOOK: query: -- If the argument is a number, hex should convert it to hexadecimal.\n SELECT\n   hex(1),\n@@ -36,16 +53,16 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1277538639/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1308427474/10000\n 1\t0\tFACEB005\n PREHOOK: query: -- Negative numbers should be treated as two's complement (64 bit).\n SELECT hex(-5) FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/2119469168/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2089947529/10000\n POSTHOOK: query: -- Negative numbers should be treated as two's complement (64 bit).\n SELECT hex(-5) FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/2119469168/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2089947529/10000\n FFFFFFFFFFFFFFFB", "filename": "ql/src/test/results/clientpositive/udf_hex.q.out"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_if.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_if.q.out", "sha": "d0096bc836290cb1bcab2f6b9f399bf6f259fdd5", "changes": 18, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_if.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,13 @@\n+PREHOOK: query: DESCRIBE FUNCTION if\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION if\n+POSTHOOK: type: DESCFUNCTION\n+There is no documentation for function if\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED if\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED if\n+POSTHOOK: type: DESCFUNCTION\n+There is no documentation for function if\n PREHOOK: query: EXPLAIN\n SELECT IF(TRUE, 1, 2) AS COL1,\n        IF(FALSE, CAST(NULL AS STRING), CAST(1 AS STRING)) AS COL2,\n@@ -67,7 +77,7 @@ PREHOOK: query: SELECT IF(TRUE, 1, 2) AS COL1,\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1333420658/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1635207375/10000\n POSTHOOK: query: SELECT IF(TRUE, 1, 2) AS COL1,\n        IF(FALSE, CAST(NULL AS STRING), CAST(1 AS STRING)) AS COL2,\n        IF(1=1, IF(2=2, 1, 2), IF(3=3, 3, 4)) AS COL3,\n@@ -77,7 +87,7 @@ POSTHOOK: query: SELECT IF(TRUE, 1, 2) AS COL1,\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1333420658/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1635207375/10000\n 1\t1\t1\t1\tNULL\t2\n PREHOOK: query: -- Type conversions\n EXPLAIN\n@@ -140,13 +150,13 @@ PREHOOK: query: SELECT IF(TRUE, CAST(128 AS SMALLINT), CAST(1 AS TINYINT)) AS CO\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/2081932428/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1478265880/10000\n POSTHOOK: query: SELECT IF(TRUE, CAST(128 AS SMALLINT), CAST(1 AS TINYINT)) AS COL1,\n        IF(FALSE, 1, 1.1) AS COL2,\n        IF(FALSE, 1, 'ABC') AS COL3,\n        IF(FALSE, 'ABC', 12.3) AS COL4\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/2081932428/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1478265880/10000\n 128\t1.1\tABC\t12.3", "filename": "ql/src/test/results/clientpositive/udf_if.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_instr.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_instr.q.out", "sha": "61247b24dfffcd5328d4f64c36341ed38c51e338", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_instr.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION instr\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION instr\n+POSTHOOK: type: DESCFUNCTION\n+instr(str, substr) - Returns the index of the first occurance of substr in str\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED instr\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED instr\n+POSTHOOK: type: DESCFUNCTION\n+instr(str, substr) - Returns the index of the first occurance of substr in str\n+Example:\n+  > SELECT instr('Facebook', 'boo') FROM src LIMIT 1;\n+  5\n PREHOOK: query: EXPLAIN\n SELECT instr('abcd', 'abc'),\n        instr('abcabc', 'ccc'),\n@@ -102,7 +115,7 @@ PREHOOK: query: SELECT instr('abcd', 'abc'),\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1088179989/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1069925328/10000\n POSTHOOK: query: SELECT instr('abcd', 'abc'),\n        instr('abcabc', 'ccc'),\n        instr(123, '23'),\n@@ -119,5 +132,5 @@ POSTHOOK: query: SELECT instr('abcd', 'abc'),\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1088179989/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1069925328/10000\n 1\t0\t2\t2\t0\t0\t2\t3\t4\t2\t3\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_instr.q.out"}, {"additions": 24, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_isnull_isnotnull.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_isnull_isnotnull.q.out", "sha": "d92861e327d1adefbbe9caa5b5a89c09e15be07e", "changes": 28, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_isnull_isnotnull.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,23 @@\n+PREHOOK: query: DESCRIBE FUNCTION isnull\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION isnull\n+POSTHOOK: type: DESCFUNCTION\n+isnull a - Returns true if a is NULL and false otherwise\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED isnull\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED isnull\n+POSTHOOK: type: DESCFUNCTION\n+isnull a - Returns true if a is NULL and false otherwise\n+PREHOOK: query: DESCRIBE FUNCTION isnotnull\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION isnotnull\n+POSTHOOK: type: DESCFUNCTION\n+isnotnull a - Returns true if a is not NULL and false otherwise\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED isnotnull\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED isnotnull\n+POSTHOOK: type: DESCFUNCTION\n+isnotnull a - Returns true if a is not NULL and false otherwise\n PREHOOK: query: EXPLAIN\n SELECT NULL IS NULL,\n        1 IS NOT NULL, \n@@ -63,15 +83,15 @@ FROM src\n WHERE true IS NOT NULL LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1106726580/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1963364824/10000\n POSTHOOK: query: SELECT NULL IS NULL,\n        1 IS NOT NULL, \n        'my string' IS NOT NULL\n FROM src\n WHERE true IS NOT NULL LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1106726580/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1963364824/10000\n true\ttrue\ttrue\n PREHOOK: query: EXPLAIN\n FROM src_thrift\n@@ -141,7 +161,7 @@ WHERE  src_thrift.lint IS NOT NULL\n        AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_thrift\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1995556677/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/609638755/10000\n POSTHOOK: query: FROM src_thrift\n SELECT src_thrift.lint IS NOT NULL, \n        src_thrift.lintstring IS NOT NULL, \n@@ -150,5 +170,5 @@ WHERE  src_thrift.lint IS NOT NULL\n        AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_thrift\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1995556677/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/609638755/10000\n true\ttrue\ttrue", "filename": "ql/src/test/results/clientpositive/udf_isnull_isnotnull.q.out"}, {"additions": 41, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_json.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_json.q.out", "sha": "d833888091f885207849250ae09f2b2399e7aede", "changes": 57, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_json.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,28 @@\n+PREHOOK: query: DESCRIBE FUNCTION get_json_object\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION get_json_object\n+POSTHOOK: type: DESCFUNCTION\n+get_json_object(json_txt, path) - Extract a json object from path \n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED get_json_object\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED get_json_object\n+POSTHOOK: type: DESCFUNCTION\n+get_json_object(json_txt, path) - Extract a json object from path \n+Extract json object from a json string based on json path specified, and return json string of the extracted json object. It will return null if the input json string is invalid.\n+A limited version of JSONPath supported:\n+  $   : Root object\n+  .   : Child operator\n+  []  : Subscript operator for array\n+  *   : Wildcard for []\n+Syntax not supported that's worth noticing:\n+  ''  : Zero length string as key\n+  ..  : Recursive descent\n+  &amp;#064;   : Current object/element\n+  ()  : Script expression\n+  ?() : Filter (script) expression.\n+  [,] : Union operator\n+  [start:end:step] : array slice operator\n+\n PREHOOK: query: CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE\n PREHOOK: type: CREATETABLE\n POSTHOOK: query: CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE\n@@ -51,72 +76,72 @@ STAGE PLANS:\n PREHOOK: query: SELECT get_json_object(src_json.json, '$') FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1368089221/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/483712443/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$') FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1368089221/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/483712443/10000\n {\"store\":{\"fruit\":[{\"weight\":8,\"type\":\"apple\"},{\"weight\":9,\"type\":\"pear\"}],\"book\":[{\"author\":\"Nigel Rees\",\"category\":\"reference\",\"title\":\"Sayings of the Century\",\"price\":8.95},{\"author\":\"Herman Melville\",\"category\":\"fiction\",\"title\":\"Moby Dick\",\"price\":8.99,\"isbn\":\"0-553-21311-3\"},{\"author\":\"J. R. R. Tolkien\",\"category\":\"fiction\",\"title\":\"The Lord of the Rings\",\"price\":22.99,\"reader\":[{\"name\":\"bob\",\"age\":25},{\"name\":\"jack\",\"age\":26}],\"isbn\":\"0-395-19395-8\"}],\"basket\":[[1,2,{\"b\":\"y\",\"a\":\"x\"}],[3,4],[5,6]],\"bicycle\":{\"price\":19.95,\"color\":\"red\"}},\"email\":\"amy@only_for_json_udf_test.net\",\"owner\":\"amy\"}\n PREHOOK: query: SELECT get_json_object(src_json.json, '$.owner'), get_json_object(src_json.json, '$.store') FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/726130120/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1962032537/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$.owner'), get_json_object(src_json.json, '$.store') FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/726130120/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1962032537/10000\n amy\t{\"fruit\":[{\"weight\":8,\"type\":\"apple\"},{\"weight\":9,\"type\":\"pear\"}],\"book\":[{\"author\":\"Nigel Rees\",\"category\":\"reference\",\"title\":\"Sayings of the Century\",\"price\":8.95},{\"author\":\"Herman Melville\",\"category\":\"fiction\",\"title\":\"Moby Dick\",\"price\":8.99,\"isbn\":\"0-553-21311-3\"},{\"author\":\"J. R. R. Tolkien\",\"category\":\"fiction\",\"title\":\"The Lord of the Rings\",\"price\":22.99,\"reader\":[{\"name\":\"bob\",\"age\":25},{\"name\":\"jack\",\"age\":26}],\"isbn\":\"0-395-19395-8\"}],\"basket\":[[1,2,{\"b\":\"y\",\"a\":\"x\"}],[3,4],[5,6]],\"bicycle\":{\"price\":19.95,\"color\":\"red\"}}\n PREHOOK: query: SELECT get_json_object(src_json.json, '$.store.bicycle'), get_json_object(src_json.json, '$.store.book') FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/794700282/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2019032273/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$.store.bicycle'), get_json_object(src_json.json, '$.store.book') FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/794700282/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2019032273/10000\n {\"price\":19.95,\"color\":\"red\"}\t[{\"author\":\"Nigel Rees\",\"category\":\"reference\",\"title\":\"Sayings of the Century\",\"price\":8.95},{\"author\":\"Herman Melville\",\"category\":\"fiction\",\"title\":\"Moby Dick\",\"price\":8.99,\"isbn\":\"0-553-21311-3\"},{\"author\":\"J. R. R. Tolkien\",\"category\":\"fiction\",\"title\":\"The Lord of the Rings\",\"price\":22.99,\"reader\":[{\"name\":\"bob\",\"age\":25},{\"name\":\"jack\",\"age\":26}],\"isbn\":\"0-395-19395-8\"}]\n PREHOOK: query: SELECT get_json_object(src_json.json, '$.store.book[0]'), get_json_object(src_json.json, '$.store.book[*]') FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1885237550/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2005938063/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$.store.book[0]'), get_json_object(src_json.json, '$.store.book[*]') FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1885237550/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2005938063/10000\n {\"author\":\"Nigel Rees\",\"category\":\"reference\",\"title\":\"Sayings of the Century\",\"price\":8.95}\t[{\"author\":\"Nigel Rees\",\"category\":\"reference\",\"title\":\"Sayings of the Century\",\"price\":8.95},{\"author\":\"Herman Melville\",\"category\":\"fiction\",\"title\":\"Moby Dick\",\"price\":8.99,\"isbn\":\"0-553-21311-3\"},{\"author\":\"J. R. R. Tolkien\",\"category\":\"fiction\",\"title\":\"The Lord of the Rings\",\"price\":22.99,\"reader\":[{\"name\":\"bob\",\"age\":25},{\"name\":\"jack\",\"age\":26}],\"isbn\":\"0-395-19395-8\"}]\n PREHOOK: query: SELECT get_json_object(src_json.json, '$.store.book[0].category'), get_json_object(src_json.json, '$.store.book[*].category'), get_json_object(src_json.json, '$.store.book[*].isbn'), get_json_object(src_json.json, '$.store.book[*].reader') FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/988296584/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/132584872/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$.store.book[0].category'), get_json_object(src_json.json, '$.store.book[*].category'), get_json_object(src_json.json, '$.store.book[*].isbn'), get_json_object(src_json.json, '$.store.book[*].reader') FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/988296584/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/132584872/10000\n reference\t[\"reference\",\"fiction\",\"fiction\"]\t[\"0-553-21311-3\",\"0-395-19395-8\"]\t[{\"name\":\"bob\",\"age\":25},{\"name\":\"jack\",\"age\":26}]\n PREHOOK: query: SELECT get_json_object(src_json.json, '$.store.book[*].reader[0].age'), get_json_object(src_json.json, '$.store.book[*].reader[*].age')  FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1571323043/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1371238792/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$.store.book[*].reader[0].age'), get_json_object(src_json.json, '$.store.book[*].reader[*].age')  FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1571323043/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1371238792/10000\n 25\t[25,26]\n PREHOOK: query: SELECT get_json_object(src_json.json, '$.store.basket[0][1]'), get_json_object(src_json.json, '$.store.basket[*]'), get_json_object(src_json.json, '$.store.basket[*][0]'), get_json_object(src_json.json, '$.store.basket[0][*]'), get_json_object(src_json.json, '$.store.basket[*][*]'), get_json_object(src_json.json, '$.store.basket[0][2].b'), get_json_object(src_json.json, '$.store.basket[0][*].b') FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/86832583/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1801151655/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$.store.basket[0][1]'), get_json_object(src_json.json, '$.store.basket[*]'), get_json_object(src_json.json, '$.store.basket[*][0]'), get_json_object(src_json.json, '$.store.basket[0][*]'), get_json_object(src_json.json, '$.store.basket[*][*]'), get_json_object(src_json.json, '$.store.basket[0][2].b'), get_json_object(src_json.json, '$.store.basket[0][*].b') FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/86832583/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1801151655/10000\n 2\t[[1,2,{\"b\":\"y\",\"a\":\"x\"}],[3,4],[5,6]]\t1\t[1,2,{\"b\":\"y\",\"a\":\"x\"}]\t[1,2,{\"b\":\"y\",\"a\":\"x\"},3,4,5,6]\ty\t[\"y\"]\n PREHOOK: query: SELECT get_json_object(src_json.json, '$.non_exist_key'),  get_json_object(src_json.json, '$..no_recursive'), get_json_object(src_json.json, '$.store.book[10]'), get_json_object(src_json.json, '$.store.book[0].non_exist_key'), get_json_object(src_json.json, '$.store.basket[*].non_exist_key'), get_json_object(src_json.json, '$.store.basket[0][*].non_exist_key') FROM src_json\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_json\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1655279596/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1780114809/10000\n POSTHOOK: query: SELECT get_json_object(src_json.json, '$.non_exist_key'),  get_json_object(src_json.json, '$..no_recursive'), get_json_object(src_json.json, '$.store.book[10]'), get_json_object(src_json.json, '$.store.book[0].non_exist_key'), get_json_object(src_json.json, '$.store.basket[*].non_exist_key'), get_json_object(src_json.json, '$.store.basket[0][*].non_exist_key') FROM src_json\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_json\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1655279596/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1780114809/10000\n NULL\tNULL\tNULL\tNULL\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_json.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_length.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_length.q.out", "sha": "91c938c34eedc641df0484c9d930acf9ff407886", "changes": 25, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_length.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION length\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION length\n+POSTHOOK: type: DESCFUNCTION\n+length(str) - Returns the length of str \n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED length\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED length\n+POSTHOOK: type: DESCFUNCTION\n+length(str) - Returns the length of str \n+Example:\n+  > SELECT length('Facebook') FROM src LIMIT 1;\n+  8\n PREHOOK: query: CREATE TABLE dest1(len INT)\n PREHOOK: type: CREATETABLE\n POSTHOOK: query: CREATE TABLE dest1(len INT)\n@@ -42,10 +55,10 @@ STAGE PLANS:\n           Move Operator\n             files:\n                 hdfs directory: true\n-                destination: file:/data/users/njain/hive5/hive5/build/ql/tmp/777850344/10000\n+                destination: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1622212784/10000\n           Map Reduce\n             Alias -> Map Operator Tree:\n-              file:/data/users/njain/hive5/hive5/build/ql/tmp/1274386623/10002 \n+              file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/92165457/10002 \n                   Reduce Output Operator\n                     sort order: \n                     Map-reduce partition columns:\n@@ -88,11 +101,11 @@ POSTHOOK: Output: default@dest1\n PREHOOK: query: SELECT dest1.* FROM dest1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@dest1\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/775750892/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2077609609/10000\n POSTHOOK: query: SELECT dest1.* FROM dest1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest1\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/775750892/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2077609609/10000\n 7\n 0\n 7\n@@ -173,11 +186,11 @@ STAGE PLANS:\n PREHOOK: query: SELECT length(dest1.name) FROM dest1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@dest1\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1564729330/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/394449121/10000\n POSTHOOK: query: SELECT length(dest1.name) FROM dest1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest1\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1564729330/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/394449121/10000\n 2\n PREHOOK: query: DROP TABLE dest1\n PREHOOK: type: DROPTABLE", "filename": "ql/src/test/results/clientpositive/udf_length.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_like.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_like.q.out", "sha": "e0b7285a1a39d0cfa575e1d4b517001937244d49", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_like.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION like\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION like\n+POSTHOOK: type: DESCFUNCTION\n+like(str, pattern) - Checks if str matches pattern\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED like\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED like\n+POSTHOOK: type: DESCFUNCTION\n+like(str, pattern) - Checks if str matches pattern\n+Example:\n+  > SELECT a.* FROM srcpart a WHERE a.hr like '%2' LIMIT 1;\n+  27      val_27  2008-04-08      12\n PREHOOK: query: EXPLAIN\n SELECT '_%_' LIKE '%\\_\\%\\_%', '__' LIKE '%\\_\\%\\_%', '%%_%_' LIKE '%\\_\\%\\_%', '%_%_%' LIKE '%\\%\\_\\%',\n   '_%_' LIKE '\\%\\_%', '%__' LIKE '__\\%%', '_%' LIKE '\\_\\%\\_\\%%', '_%' LIKE '\\_\\%_%',\n@@ -77,12 +90,12 @@ PREHOOK: query: SELECT '_%_' LIKE '%\\_\\%\\_%', '__' LIKE '%\\_\\%\\_%', '%%_%_' LIKE\n FROM src WHERE src.key = 86\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/148481666/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1557896001/10000\n POSTHOOK: query: SELECT '_%_' LIKE '%\\_\\%\\_%', '__' LIKE '%\\_\\%\\_%', '%%_%_' LIKE '%\\_\\%\\_%', '%_%_%' LIKE '%\\%\\_\\%',\n   '_%_' LIKE '\\%\\_%', '%__' LIKE '__\\%%', '_%' LIKE '\\_\\%\\_\\%%', '_%' LIKE '\\_\\%_%',\n   '%_' LIKE '\\%\\_', 'ab' LIKE '\\%\\_', 'ab' LIKE '_a%', 'ab' LIKE 'a'\n FROM src WHERE src.key = 86\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/148481666/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1557896001/10000\n true\tfalse\ttrue\ttrue\tfalse\tfalse\tfalse\tfalse\ttrue\tfalse\tfalse\tfalse", "filename": "ql/src/test/results/clientpositive/udf_like.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_locate.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_locate.q.out", "sha": "713f11b346a6e5512c6c545e9dc33cbd13d255f3", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_locate.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION locate\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION locate\n+POSTHOOK: type: DESCFUNCTION\n+locate(substr, str[, pos]) - Returns the position of the first occurance of substr in str after position pos\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED locate\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED locate\n+POSTHOOK: type: DESCFUNCTION\n+locate(substr, str[, pos]) - Returns the position of the first occurance of substr in str after position pos\n+Example:\n+  > SELECT locate('bar', 'foobarbar', 5) FROM src LIMIT 1;\n+  7\n PREHOOK: query: EXPLAIN\n SELECT locate('abc', 'abcd'),\n        locate('ccc', 'abcabc'),\n@@ -122,7 +135,7 @@ PREHOOK: query: SELECT locate('abc', 'abcd'),\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/572327680/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/155745966/10000\n POSTHOOK: query: SELECT locate('abc', 'abcd'),\n        locate('ccc', 'abcabc'),\n        locate('23', 123),\n@@ -143,5 +156,5 @@ POSTHOOK: query: SELECT locate('abc', 'abcd'),\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/572327680/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/155745966/10000\n 1\t0\t2\t2\t4\t4\t0\t0\t2\t3\t4\t2\t3\tNULL\tNULL\t0\t0", "filename": "ql/src/test/results/clientpositive/udf_locate.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_lower.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_lower.q.out", "sha": "d8894beeb900b4f313ed5efe82dd878ae073ef75", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_lower.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION lower\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION lower\n+POSTHOOK: type: DESCFUNCTION\n+lower(str) - Returns str with all characters changed to lowercase\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED lower\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED lower\n+POSTHOOK: type: DESCFUNCTION\n+lower(str) - Returns str with all characters changed to lowercase\n+Example:\n+  > SELECT lower('Facebook') FROM src LIMIT 1;\n+  'facebook'\n PREHOOK: query: EXPLAIN\n SELECT lower('AbC 123'), upper('AbC 123') FROM src WHERE key = 86\n PREHOOK: type: QUERY\n@@ -48,9 +61,9 @@ STAGE PLANS:\n PREHOOK: query: SELECT lower('AbC 123'), upper('AbC 123') FROM src WHERE key = 86\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/655858125/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1887044838/10000\n POSTHOOK: query: SELECT lower('AbC 123'), upper('AbC 123') FROM src WHERE key = 86\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/655858125/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1887044838/10000\n abc 123\tABC 123", "filename": "ql/src/test/results/clientpositive/udf_lower.q.out"}, {"additions": 34, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_lpad_rpad.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_lpad_rpad.q.out", "sha": "548e808c49edae8980d832cb5b656e30e04c2648", "changes": 38, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_lpad_rpad.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,33 @@\n+PREHOOK: query: DESCRIBE FUNCTION lpad\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION lpad\n+POSTHOOK: type: DESCFUNCTION\n+lpad(str, len, pad) - Returns str, left-padded with pad to a length of len\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED lpad\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED lpad\n+POSTHOOK: type: DESCFUNCTION\n+lpad(str, len, pad) - Returns str, left-padded with pad to a length of len\n+If str is longer than len, the return value is shortened to len characters.\n+Example:\n+  > SELECT lpad('hi', 5, '??') FROM src LIMIT 1;\n+  '???hi'  > SELECT lpad('hi', 1, '??') FROM src LIMIT 1;\n+  'h'\n+PREHOOK: query: DESCRIBE FUNCTION rpad\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION rpad\n+POSTHOOK: type: DESCFUNCTION\n+rpad(str, len, pad) - Returns str, right-padded with pad to a length of len\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED rpad\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED rpad\n+POSTHOOK: type: DESCFUNCTION\n+rpad(str, len, pad) - Returns str, right-padded with pad to a length of len\n+If str is longer than len, the return value is shortened to len characters.\n+Example:\n+  > SELECT rpad('hi', 5, '??') FROM src LIMIT 1;\n+  'hi???'  > SELECT rpad('hi', 1, '??') FROM src LIMIT 1;\n+  'h'\n PREHOOK: query: EXPLAIN SELECT\n   lpad('hi', 1, '?'),\n   lpad('hi', 5, '.'),\n@@ -53,15 +83,15 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/369345424/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/361452356/10000\n POSTHOOK: query: SELECT\n   lpad('hi', 1, '?'),\n   lpad('hi', 5, '.'),\n   lpad('hi', 6, '123')\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/369345424/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/361452356/10000\n h\t...hi\t1231hi\n PREHOOK: query: EXPLAIN SELECT\n   rpad('hi', 1, '?'),\n@@ -118,13 +148,13 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/975123871/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1704470803/10000\n POSTHOOK: query: SELECT\n   rpad('hi', 1, '?'),\n   rpad('hi', 5, '.'),\n   rpad('hi', 6, '123')\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/975123871/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1704470803/10000\n h\thi...\thi1231", "filename": "ql/src/test/results/clientpositive/udf_lpad_rpad.q.out"}, {"additions": 14, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_map.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_map.q.out", "sha": "06dd614f29558ded998600b9df1ec782388ed9ed", "changes": 16, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_map.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,15 @@\n+PREHOOK: query: -- Parsing bug requires us to quote map\n+DESCRIBE FUNCTION 'map'\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: -- Parsing bug requires us to quote map\n+DESCRIBE FUNCTION 'map'\n+POSTHOOK: type: DESCFUNCTION\n+map(key0, value0, key1, value1...) - Creates a map with the given key/value pairs \n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED 'map'\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED 'map'\n+POSTHOOK: type: DESCFUNCTION\n+map(key0, value0, key1, value1...) - Creates a map with the given key/value pairs \n PREHOOK: query: EXPLAIN SELECT map(), map(1, \"a\", 2, \"b\", 3, \"c\"), map(1, 2, \"a\", \"b\"), \n map(1, \"a\", 2, \"b\", 3, \"c\")[2],  map(1, 2, \"a\", \"b\")[\"a\"], map(1, array(\"a\"))[1][0] FROM src LIMIT 1\n PREHOOK: type: QUERY\n@@ -50,10 +62,10 @@ PREHOOK: query: SELECT map(), map(1, \"a\", 2, \"b\", 3, \"c\"), map(1, 2, \"a\", \"b\"),\n map(1, \"a\", 2, \"b\", 3, \"c\")[2],  map(1, 2, \"a\", \"b\")[\"a\"], map(1, array(\"a\"))[1][0] FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/pyang/trunk-HIVE-554/VENDOR.hive/trunk/build/ql/tmp/802535376/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1878909043/10000\n POSTHOOK: query: SELECT map(), map(1, \"a\", 2, \"b\", 3, \"c\"), map(1, 2, \"a\", \"b\"), \n map(1, \"a\", 2, \"b\", 3, \"c\")[2],  map(1, 2, \"a\", \"b\")[\"a\"], map(1, array(\"a\"))[1][0] FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/pyang/trunk-HIVE-554/VENDOR.hive/trunk/build/ql/tmp/802535376/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1878909043/10000\n {}\t{1:\"a\",2:\"b\",3:\"c\"}\t{\"1\":\"2\",\"a\":\"b\"}\tb\tb\ta", "filename": "ql/src/test/results/clientpositive/udf_map.q.out"}, {"additions": 22, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_negative.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_negative.q.out", "sha": "08d9cf5560ee2c19ebd55ac07f91a60cc3986ca6", "changes": 34, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_negative.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,54 +1,64 @@\n+PREHOOK: query: DESCRIBE FUNCTION negative\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION negative\n+POSTHOOK: type: DESCFUNCTION\n+negative a - Returns -a\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED negative\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED negative\n+POSTHOOK: type: DESCFUNCTION\n+negative a - Returns -a\n PREHOOK: query: select - null from src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/928832343/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1255375616/10000\n POSTHOOK: query: select - null from src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/928832343/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1255375616/10000\n NULL\n PREHOOK: query: select - cast(null as int) from src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/1834056910/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/925422494/10000\n POSTHOOK: query: select - cast(null as int) from src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/1834056910/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/925422494/10000\n NULL\n PREHOOK: query: select - cast(null as smallint) from src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/677639104/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/75605941/10000\n POSTHOOK: query: select - cast(null as smallint) from src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/677639104/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/75605941/10000\n NULL\n PREHOOK: query: select - cast(null as bigint) from src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/1891198169/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/914031656/10000\n POSTHOOK: query: select - cast(null as bigint) from src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/1891198169/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/914031656/10000\n NULL\n PREHOOK: query: select - cast(null as double) from src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/856616351/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/551335924/10000\n POSTHOOK: query: select - cast(null as double) from src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/856616351/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/551335924/10000\n NULL\n PREHOOK: query: select - cast(null as float) from src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/277516197/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/708448995/10000\n POSTHOOK: query: select - cast(null as float) from src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/nzhang/work/876/apache-hive/build/ql/tmp/277516197/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/708448995/10000\n NULL", "filename": "ql/src/test/results/clientpositive/udf_negative.q.out"}, {"additions": 21, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_parse_url.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_parse_url.q.out", "sha": "aafb7663ca45d59f6cab264d410ca763fd5c541a", "changes": 23, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_parse_url.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,22 @@\n+PREHOOK: query: DESCRIBE FUNCTION parse_url\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION parse_url\n+POSTHOOK: type: DESCFUNCTION\n+parse_url(url, partToExtract[, key]) - extracts a part from a URL\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED parse_url\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED parse_url\n+POSTHOOK: type: DESCFUNCTION\n+parse_url(url, partToExtract[, key]) - extracts a part from a URL\n+Parts: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO\n+key specifies which query to extract\n+Example:\n+  > SELECT parse_url('http://facebook.com/path/p1.php?query=1', 'HOST') FROM src LIMIT 1;\n+  'facebook.com'\n+  > SELECT parse_url('http://facebook.com/path/p1.php?query=1', 'QUERY') FROM src LIMIT 1;\n+  'query=1'\n+  > SELECT parse_url('http://facebook.com/path/p1.php?query=1', 'QUERY', 'query') FROM src LIMIT 1;\n+  '1'\n PREHOOK: query: EXPLAIN\n SELECT parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST'), \n parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'PATH'), \n@@ -99,7 +118,7 @@ parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'AUTHORITY')\n   FROM src WHERE key = 86\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1215092102/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/482859701/10000\n POSTHOOK: query: SELECT parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST'), \n parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'PATH'), \n parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY') ,\n@@ -114,5 +133,5 @@ parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'AUTHORITY')\n   FROM src WHERE key = 86\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1215092102/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/482859701/10000\n facebook.com\t/path1/p.php\tk1=v1&k2=v2\tRef1\tv2\tv1\tNULL\t/path1/p.php?k1=v1&k2=v2\thttp\tNULL\tfacebook.com", "filename": "ql/src/test/results/clientpositive/udf_parse_url.q.out"}, {"additions": 16, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_pmod.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_pmod.q.out", "sha": "9d41b6fbab47efab7c0cfe6e23add7f431295be9", "changes": 22, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_pmod.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,33 +1,43 @@\n+PREHOOK: query: DESCRIBE FUNCTION pmod\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION pmod\n+POSTHOOK: type: DESCFUNCTION\n+a pmod b - Compute the positive modulo\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED pmod\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED pmod\n+POSTHOOK: type: DESCFUNCTION\n+a pmod b - Compute the positive modulo\n PREHOOK: query: SELECT pmod(null, null)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1384517913/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1159318893/10000\n POSTHOOK: query: SELECT pmod(null, null)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1384517913/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1159318893/10000\n NULL\n PREHOOK: query: SELECT pmod(-100,9), pmod(-50,101), pmod(-1000,29)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1606180544/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1059038526/10000\n POSTHOOK: query: SELECT pmod(-100,9), pmod(-50,101), pmod(-1000,29)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1606180544/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1059038526/10000\n 8\t51\t15\n PREHOOK: query: SELECT pmod(100,19), pmod(50,125), pmod(300,15)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1250421811/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2064781823/10000\n POSTHOOK: query: SELECT pmod(100,19), pmod(50,125), pmod(300,15)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1250421811/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/2064781823/10000\n 5\t50\t0", "filename": "ql/src/test/results/clientpositive/udf_pmod.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_repeat.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_repeat.q.out", "sha": "e07f93111bb119a89bb97c853e8bf698233f8630", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_repeat.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION repeat\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION repeat\n+POSTHOOK: type: DESCFUNCTION\n+repeat(str, n) - repeat str n times \n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED repeat\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED repeat\n+POSTHOOK: type: DESCFUNCTION\n+repeat(str, n) - repeat str n times \n+Example:\n+  > SELECT repeat('123', 2) FROM src LIMIT 1;\n+  '123123'\n PREHOOK: query: EXPLAIN SELECT\n   repeat(\"Facebook\", 3),\n   repeat(\"\", 4),\n@@ -58,7 +71,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/208533019/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1513627133/10000\n POSTHOOK: query: SELECT\n   repeat(\"Facebook\", 3),\n   repeat(\"\", 4),\n@@ -67,5 +80,5 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/208533019/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1513627133/10000\n FacebookFacebookFacebook\t\t\t", "filename": "ql/src/test/results/clientpositive/udf_repeat.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_reverse.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_reverse.q.out", "sha": "9feb13bbb893bcf39241862b39d1bbe01ca1033a", "changes": 25, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_reverse.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION reverse\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION reverse\n+POSTHOOK: type: DESCFUNCTION\n+reverse(str) - reverse str\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED reverse\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED reverse\n+POSTHOOK: type: DESCFUNCTION\n+reverse(str) - reverse str\n+Example:\n+  > SELECT reverse('Facebook') FROM src LIMIT 1;\n+  'koobecaF'\n PREHOOK: query: CREATE TABLE dest1(len STRING)\n PREHOOK: type: CREATETABLE\n POSTHOOK: query: CREATE TABLE dest1(len STRING)\n@@ -42,10 +55,10 @@ STAGE PLANS:\n           Move Operator\n             files:\n                 hdfs directory: true\n-                destination: file:/data/users/njain/hive5/hive5/build/ql/tmp/1305154928/10000\n+                destination: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/520090506/10000\n           Map Reduce\n             Alias -> Map Operator Tree:\n-              file:/data/users/njain/hive5/hive5/build/ql/tmp/925020569/10002 \n+              file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/468564432/10002 \n                   Reduce Output Operator\n                     sort order: \n                     Map-reduce partition columns:\n@@ -88,11 +101,11 @@ POSTHOOK: Output: default@dest1\n PREHOOK: query: SELECT dest1.* FROM dest1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@dest1\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/516796328/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/255390900/10000\n POSTHOOK: query: SELECT dest1.* FROM dest1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest1\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/516796328/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/255390900/10000\n 832_lav\n \n 113_lav\n@@ -142,11 +155,11 @@ POSTHOOK: Output: default@dest1\n PREHOOK: query: SELECT count(1) FROM dest1 WHERE reverse(dest1.name) = _UTF-8 0xE993AEE982B5\n PREHOOK: type: QUERY\n PREHOOK: Input: default@dest1\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/465925440/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1130378645/10000\n POSTHOOK: query: SELECT count(1) FROM dest1 WHERE reverse(dest1.name) = _UTF-8 0xE993AEE982B5\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest1\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/465925440/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1130378645/10000\n 1\n PREHOOK: query: DROP TABLE dest1\n PREHOOK: type: DROPTABLE", "filename": "ql/src/test/results/clientpositive/udf_reverse.q.out"}, {"additions": 23, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_round.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_round.q.out", "sha": "d85ee2752efd1e4728a3b712b805891fe20c3fa1", "changes": 33, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_round.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,13 +1,26 @@\n+PREHOOK: query: DESCRIBE FUNCTION round\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION round\n+POSTHOOK: type: DESCFUNCTION\n+round(x[, d]) - round x to d decimal places\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED round\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED round\n+POSTHOOK: type: DESCFUNCTION\n+round(x[, d]) - round x to d decimal places\n+Example:\n+  > SELECT round(12.3456, 1) FROM src LIMIT 1;\n+  12.3'\n PREHOOK: query: SELECT round(null), round(null, 0), round(125, null)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1931880581/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1736987523/10000\n POSTHOOK: query: SELECT round(null), round(null, 0), round(125, null)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1931880581/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1736987523/10000\n NULL\tNULL\tNULL\n PREHOOK: query: SELECT\n   round(55555), round(55555, 0), round(55555, 1), round(55555, 2), round(55555, 3),\n@@ -16,15 +29,15 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1954748857/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1549949974/10000\n POSTHOOK: query: SELECT\n   round(55555), round(55555, 0), round(55555, 1), round(55555, 2), round(55555, 3),\n   round(55555, -1), round(55555, -2), round(55555, -3), round(55555, -4),\n   round(55555, -5), round(55555, -6), round(55555, -7), round(55555, -8)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1954748857/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1549949974/10000\n 55555\t55555.0\t55555.0\t55555.0\t55555.0\t55560.0\t55600.0\t56000.0\t60000.0\t100000.0\t0.0\t0.0\t0.0\n PREHOOK: query: SELECT\n   round(125.315), round(125.315, 0),\n@@ -36,7 +49,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/372474214/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/148858111/10000\n POSTHOOK: query: SELECT\n   round(125.315), round(125.315, 0),\n   round(125.315, 1), round(125.315, 2), round(125.315, 3), round(125.315, 4),\n@@ -47,7 +60,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/372474214/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/148858111/10000\n 125\t125.0\t125.3\t125.32\t125.315\t125.315\t130.0\t100.0\t0.0\t0.0\t-125\t-125.0\t-125.3\t-125.32\t-125.315\t-125.315\t-130.0\t-100.0\t0.0\t0.0\n PREHOOK: query: SELECT\n   round(3.141592653589793, -15), round(3.141592653589793, -16),\n@@ -70,7 +83,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/412153419/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/624837434/10000\n POSTHOOK: query: SELECT\n   round(3.141592653589793, -15), round(3.141592653589793, -16),\n   round(3.141592653589793, -13), round(3.141592653589793, -14),\n@@ -92,16 +105,16 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/412153419/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/624837434/10000\n 0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t3.0\t3.1\t3.14\t3.142\t3.1416\t3.14159\t3.141593\t3.1415927\t3.14159265\t3.141592654\t3.1415926536\t3.14159265359\t3.14159265359\t3.1415926535898\t3.1415926535898\t3.14159265358979\t3.141592653589793\t3.141592653589793\n PREHOOK: query: SELECT round(1809242.3151111344, 9), round(-1809242.3151111344, 9)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/667136403/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/599098185/10000\n POSTHOOK: query: SELECT round(1809242.3151111344, 9), round(-1809242.3151111344, 9)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/667136403/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/599098185/10000\n 1809242.315111134\t-1809242.315111134", "filename": "ql/src/test/results/clientpositive/udf_round.q.out"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_sin.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_sin.q.out", "sha": "a2f90520f7e4c3fca8ec2425e7d5b38a47d177bf", "changes": 21, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_sin.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,22 +1,35 @@\n+PREHOOK: query: DESCRIBE FUNCTION sin\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION sin\n+POSTHOOK: type: DESCFUNCTION\n+sin(x) - returns the sine of x (x is in radians)\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED sin\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED sin\n+POSTHOOK: type: DESCFUNCTION\n+sin(x) - returns the sine of x (x is in radians)\n+Example:\n+   > SELECT sin(0) FROM src LIMIT 1;\n+  0\n PREHOOK: query: SELECT sin(null)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1281720904/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1659199524/10000\n POSTHOOK: query: SELECT sin(null)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1281720904/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1659199524/10000\n NULL\n PREHOOK: query: SELECT sin(0.98), sin(1.57), sin(-0.5)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/820458381/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1109863230/10000\n POSTHOOK: query: SELECT sin(0.98), sin(1.57), sin(-0.5)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/820458381/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1109863230/10000\n 0.8304973704919705\t0.9999996829318346\t-0.479425538604203", "filename": "ql/src/test/results/clientpositive/udf_sin.q.out"}, {"additions": 12, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_size.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_size.q.out", "sha": "9287192e082d6a2d5c6626a049d49ef777a7805e", "changes": 14, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_size.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,13 @@\n+PREHOOK: query: DESCRIBE FUNCTION size\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION size\n+POSTHOOK: type: DESCFUNCTION\n+size(a) - Returns the size of a\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED size\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED size\n+POSTHOOK: type: DESCFUNCTION\n+size(a) - Returns the size of a\n PREHOOK: query: EXPLAIN\n FROM src_thrift\n SELECT size(src_thrift.lint), \n@@ -71,7 +81,7 @@ WHERE  src_thrift.lint IS NOT NULL\n        AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src_thrift\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1116336737/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/961043186/10000\n POSTHOOK: query: FROM src_thrift\n SELECT size(src_thrift.lint), \n        size(src_thrift.lintstring), \n@@ -81,5 +91,5 @@ WHERE  src_thrift.lint IS NOT NULL\n        AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src_thrift\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1116336737/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/961043186/10000\n 3\t1\t1\t-1", "filename": "ql/src/test/results/clientpositive/udf_size.q.out"}, {"additions": 17, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_space.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_space.q.out", "sha": "a48c02a36588283c82be1fa6fca5dd3fc3010652", "changes": 21, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_space.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION space\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION space\n+POSTHOOK: type: DESCFUNCTION\n+space(n) - returns n spaces\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED space\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED space\n+POSTHOOK: type: DESCFUNCTION\n+space(n) - returns n spaces\n+Example:\n+   > SELECT space(2) FROM src LIMIT 1;\n+  '  '\n PREHOOK: query: EXPLAIN SELECT\n   space(10),\n   space(0),\n@@ -63,7 +76,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/228561399/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1208254968/10000\n POSTHOOK: query: SELECT\n   length(space(10)),\n   length(space(0)),\n@@ -73,7 +86,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/228561399/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1208254968/10000\n 10\t0\t1\t0\t0\n PREHOOK: query: SELECT\n   space(10),\n@@ -84,7 +97,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/201745127/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/888230107/10000\n POSTHOOK: query: SELECT\n   space(10),\n   space(0),\n@@ -94,5 +107,5 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/201745127/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/888230107/10000\n           \t\t \t\t", "filename": "ql/src/test/results/clientpositive/udf_space.q.out"}, {"additions": 15, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_split.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_split.q.out", "sha": "a9323dbf142cefe66d5e44c1805aee799494abde", "changes": 17, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_split.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,16 @@\n+PREHOOK: query: DESCRIBE FUNCTION split\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION split\n+POSTHOOK: type: DESCFUNCTION\n+split(str, regex) - Splits str around occurances that match regex\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED split\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED split\n+POSTHOOK: type: DESCFUNCTION\n+split(str, regex) - Splits str around occurances that match regex\n+Example:\n+  > SELECT split('oneAtwoBthreeC', '[ABC]') FROM src LIMIT 1;\n+  [\"one\", \"two\", \"three\"]\n PREHOOK: query: EXPLAIN SELECT \n   split('a b c', ' '),\n   split('oneAtwoBthreeC', '[ABC]'),\n@@ -58,7 +71,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/415947738/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1731015690/10000\n POSTHOOK: query: SELECT \n   split('a b c', ' '),\n   split('oneAtwoBthreeC', '[ABC]'),\n@@ -67,5 +80,5 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/415947738/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1731015690/10000\n [\"a\",\"b\",\"c\"]\t[\"one\",\"two\",\"three\"]\t[]\t[\"5\",\"4\",\"1\",\"2\"]", "filename": "ql/src/test/results/clientpositive/udf_split.q.out"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_substr.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_substr.q.out", "sha": "4c72f24061a3009a2616e1ad30f7bcd6673716c3", "changes": 42, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_substr.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,19 +1,37 @@\n+PREHOOK: query: DESCRIBE FUNCTION substr\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION substr\n+POSTHOOK: type: DESCFUNCTION\n+substr(str, pos[, len]) - returns the substring of str that starts at pos and is of length len\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED substr\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED substr\n+POSTHOOK: type: DESCFUNCTION\n+substr(str, pos[, len]) - returns the substring of str that starts at pos and is of length len\n+pos is a 1-based index. If pos<0 the starting position is determined by counting backwards from the end of str.\n+Example:\n+   > SELECT substr('Facebook', 5) FROM src LIMIT 1;\n+  'book'\n+  > SELECT substr('Facebook', -5) FROM src LIMIT 1;\n+  'ebook'\n+  > SELECT substr('Facebook', 5, 1) FROM src LIMIT 1;\n+  'b'\n PREHOOK: query: SELECT\n   substr(null, 1), substr(null, 1, 1),\n   substr('ABC', null), substr('ABC', null, 1),\n   substr('ABC', 1, null)\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1163670295/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/385391222/10000\n POSTHOOK: query: SELECT\n   substr(null, 1), substr(null, 1, 1),\n   substr('ABC', null), substr('ABC', null, 1),\n   substr('ABC', 1, null)\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1163670295/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/385391222/10000\n NULL\tNULL\tNULL\tNULL\tNULL\n PREHOOK: query: SELECT\n   substr('ABC', 1, 0), substr('ABC', 1, -1), substr('ABC', 2, -100),\n@@ -25,7 +43,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1047004193/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1555439693/10000\n POSTHOOK: query: SELECT\n   substr('ABC', 1, 0), substr('ABC', 1, -1), substr('ABC', 2, -100),\n   substr('ABC', 4), substr('ABC', 4, 100),\n@@ -36,7 +54,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1047004193/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1555439693/10000\n \t\t\t\t\t\t\t\t\t\t\t\t\n PREHOOK: query: SELECT\n   substr('ABCDEFG', 3, 4), substr('ABCDEFG', -5, 4),\n@@ -47,7 +65,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/633389211/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1351450825/10000\n POSTHOOK: query: SELECT\n   substr('ABCDEFG', 3, 4), substr('ABCDEFG', -5, 4),\n   substr('ABCDEFG', 3), substr('ABCDEFG', -5),\n@@ -57,7 +75,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/633389211/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1351450825/10000\n CDEF\tCDEF\tCDEFG\tCDEFG\tABC\tABC\tBC\tC\tABC\tBC\tA\tA\tA\n PREHOOK: query: SELECT\n   substr('ABC', 0, 1), substr('ABC', 0, 2), substr('ABC', 0, 3), substr('ABC', 0, 4),\n@@ -68,7 +86,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1247257569/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1909380959/10000\n POSTHOOK: query: SELECT\n   substr('ABC', 0, 1), substr('ABC', 0, 2), substr('ABC', 0, 3), substr('ABC', 0, 4),\n   substr('ABC', 1, 1), substr('ABC', 1, 2), substr('ABC', 1, 3), substr('ABC', 1, 4),\n@@ -78,7 +96,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1247257569/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1909380959/10000\n A\tAB\tABC\tABC\tA\tAB\tABC\tABC\tB\tBC\tBC\tBC\tC\tC\tC\tC\t\n PREHOOK: query: SELECT\n   substr('ABC', -1, 1), substr('ABC', -1, 2), substr('ABC', -1, 3), substr('ABC', -1, 4),\n@@ -88,7 +106,7 @@ PREHOOK: query: SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1569190573/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/606649550/10000\n POSTHOOK: query: SELECT\n   substr('ABC', -1, 1), substr('ABC', -1, 2), substr('ABC', -1, 3), substr('ABC', -1, 4),\n   substr('ABC', -2, 1), substr('ABC', -2, 2), substr('ABC', -2, 3), substr('ABC', -2, 4),\n@@ -97,7 +115,7 @@ POSTHOOK: query: SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1569190573/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/606649550/10000\n C\tC\tC\tC\tB\tBC\tBC\tBC\tA\tAB\tABC\tABC\t\n PREHOOK: query: -- substring() is a synonim of substr(), so just perform some basic tests\n SELECT\n@@ -109,7 +127,7 @@ SELECT\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1899515229/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1053388822/10000\n POSTHOOK: query: -- substring() is a synonim of substr(), so just perform some basic tests\n SELECT\n   substring('ABCDEFG', 3, 4), substring('ABCDEFG', -5, 4),\n@@ -120,5 +138,5 @@ SELECT\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1899515229/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1053388822/10000\n CDEF\tCDEF\tCDEFG\tCDEFG\tABC\tABC\tBC\tC\tABC\tBC\tA\tA\tA", "filename": "ql/src/test/results/clientpositive/udf_substr.q.out"}, {"additions": 31, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_unhex.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_unhex.q.out", "sha": "06b39dff0db1fb05c7b9a44fcbfaf1af0e193335", "changes": 35, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_unhex.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,30 @@\n+PREHOOK: query: DESCRIBE FUNCTION unhex\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION unhex\n+POSTHOOK: type: DESCFUNCTION\n+unhex(str) - Converts hexadecimal argument to string\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED unhex\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED unhex\n+POSTHOOK: type: DESCFUNCTION\n+unhex(str) - Converts hexadecimal argument to string\n+Performs the inverse operation of HEX(str). That is, it interprets\n+each pair of hexadecimal digits in the argument as a number and\n+converts it to the character represented by the number. The\n+resulting characters are returned as a binary string.\n+\n+Example:\n+> SELECT UNHEX('4D7953514C') from src limit 1;\n+'MySQL'\n+> SELECT UNHEX(HEX('string')) from src limit 1;\n+'string'\n+> SELECT HEX(UNHEX('1267')) from src limit 1;\n+'1267'\n+\n+The characters in the argument string must be legal hexadecimal\n+digits: '0' .. '9', 'A' .. 'F', 'a' .. 'f'. If UNHEX() encounters\n+any nonhexadecimal digits in the argument, it returns NULL. Also,\n+if there are an odd number of characters a leading 0 is appended.\n PREHOOK: query: -- Good inputs\n \n SELECT\n@@ -9,7 +36,7 @@ SELECT\n FROM src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/791570897/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1211131149/10000\n POSTHOOK: query: -- Good inputs\n \n SELECT\n@@ -21,7 +48,7 @@ SELECT\n FROM src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/791570897/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1211131149/10000\n MySQL\t1267\ta\t-4\t\n PREHOOK: query: -- Bad inputs\n SELECT\n@@ -31,7 +58,7 @@ SELECT\n FROM src limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1722929824/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1374300168/10000\n POSTHOOK: query: -- Bad inputs\n SELECT\n   unhex('MySQL'),\n@@ -40,5 +67,5 @@ SELECT\n FROM src limit 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1722929824/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1374300168/10000\n NULL\tNULL\tNULL", "filename": "ql/src/test/results/clientpositive/udf_unhex.q.out"}, {"additions": 19, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_unix_timestamp.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udf_unix_timestamp.q.out", "sha": "f6a6c9f096312e097b3e5042c3d127d76a4a3658", "changes": 27, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udf_unix_timestamp.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,60 +1,71 @@\n+PREHOOK: query: DESCRIBE FUNCTION unix_timestamp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION unix_timestamp\n+POSTHOOK: type: DESCFUNCTION\n+unix_timestamp([date[, pattern]]) - Returns the UNIX timestamp\n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED unix_timestamp\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED unix_timestamp\n+POSTHOOK: type: DESCFUNCTION\n+unix_timestamp([date[, pattern]]) - Returns the UNIX timestamp\n+Converts the current or specified time to number of seconds since 1970-01-01.\n PREHOOK: query: SELECT\n   '2009-03-20 11:30:01',\n   unix_timestamp('2009-03-20 11:30:01')\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/490859379/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1659161710/10000\n POSTHOOK: query: SELECT\n   '2009-03-20 11:30:01',\n   unix_timestamp('2009-03-20 11:30:01')\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/490859379/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1659161710/10000\n 2009-03-20 11:30:01\t1237573801\n PREHOOK: query: SELECT\n   '2009-03-20',\n   unix_timestamp('2009-03-20', 'yyyy-MM-dd')\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1674908517/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1103088265/10000\n POSTHOOK: query: SELECT\n   '2009-03-20',\n   unix_timestamp('2009-03-20', 'yyyy-MM-dd')\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/1674908517/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1103088265/10000\n 2009-03-20\t1237532400\n PREHOOK: query: SELECT\n   '2009 Mar 20 11:30:01 am',\n   unix_timestamp('2009 Mar 20 11:30:01 am', 'yyyy MMM dd h:mm:ss a')\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/265278879/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1519095937/10000\n POSTHOOK: query: SELECT\n   '2009 Mar 20 11:30:01 am',\n   unix_timestamp('2009 Mar 20 11:30:01 am', 'yyyy MMM dd h:mm:ss a')\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/265278879/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1519095937/10000\n 2009 Mar 20 11:30:01 am\t1237573801\n PREHOOK: query: SELECT\n   'random_string',\n   unix_timestamp('random_string')\n FROM src LIMIT 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/568226360/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1021450138/10000\n POSTHOOK: query: SELECT\n   'random_string',\n   unix_timestamp('random_string')\n FROM src LIMIT 1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/njain/hive5/hive5/build/ql/tmp/568226360/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1021450138/10000\n random_string\tNULL", "filename": "ql/src/test/results/clientpositive/udf_unix_timestamp.q.out"}, {"additions": 34, "raw_url": "https://github.com/apache/hive/raw/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udtf_explode.q.out", "blob_url": "https://github.com/apache/hive/blob/481f069b159f496ad29f91f093f71b69551e9180/ql/src/test/results/clientpositive/udtf_explode.q.out", "sha": "f5ca9a7180b2a73e47987bd20f19e064f6b92a3f", "changes": 58, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udtf_explode.q.out?ref=481f069b159f496ad29f91f093f71b69551e9180", "patch": "@@ -1,3 +1,13 @@\n+PREHOOK: query: DESCRIBE FUNCTION explode\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION explode\n+POSTHOOK: type: DESCFUNCTION\n+explode(a) - separates the elements of array a into multiple rows \n+PREHOOK: query: DESCRIBE FUNCTION EXTENDED explode\n+PREHOOK: type: DESCFUNCTION\n+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED explode\n+POSTHOOK: type: DESCFUNCTION\n+explode(a) - separates the elements of array a into multiple rows \n PREHOOK: query: EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3\n PREHOOK: type: QUERY\n POSTHOOK: query: EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3\n@@ -27,7 +37,7 @@ STAGE PLANS:\n                   File Output Operator\n                     compressed: false\n                     GlobalTableId: 0\n-                    directory: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/784241868/10001\n+                    directory: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1666222499/10001\n                     table:\n                         input format: org.apache.hadoop.mapred.TextInputFormat\n                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n@@ -37,9 +47,9 @@ STAGE PLANS:\n                           columns.types int\n       Needs Tagging: false\n       Path -> Alias:\n-        file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src [src]\n+        file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src [src]\n       Path -> Partition:\n-        file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src \n+        file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src \n           Partition\n             base file name: src\n             input format: org.apache.hadoop.mapred.TextInputFormat\n@@ -54,8 +64,8 @@ STAGE PLANS:\n               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               file.inputformat org.apache.hadoop.mapred.TextInputFormat\n               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-              location file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src\n-              transient_lastDdlTime 1262138345\n+              location file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src\n+              transient_lastDdlTime 1262809225\n             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           \n               input format: org.apache.hadoop.mapred.TextInputFormat\n@@ -70,8 +80,8 @@ STAGE PLANS:\n                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                 file.inputformat org.apache.hadoop.mapred.TextInputFormat\n                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                location file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src\n-                transient_lastDdlTime 1262138345\n+                location file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src\n+                transient_lastDdlTime 1262809225\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: src\n             name: src\n@@ -116,9 +126,9 @@ STAGE PLANS:\n                           type: int\n       Needs Tagging: false\n       Path -> Alias:\n-        file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src [a:src]\n+        file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src [a:src]\n       Path -> Partition:\n-        file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src \n+        file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src \n           Partition\n             base file name: src\n             input format: org.apache.hadoop.mapred.TextInputFormat\n@@ -133,8 +143,8 @@ STAGE PLANS:\n               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               file.inputformat org.apache.hadoop.mapred.TextInputFormat\n               file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-              location file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src\n-              transient_lastDdlTime 1262138345\n+              location file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src\n+              transient_lastDdlTime 1262809225\n             serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           \n               input format: org.apache.hadoop.mapred.TextInputFormat\n@@ -149,8 +159,8 @@ STAGE PLANS:\n                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                 file.inputformat org.apache.hadoop.mapred.TextInputFormat\n                 file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                location file:/data/users/jsichi/open/hive-trunk/build/ql/test/data/warehouse/src\n-                transient_lastDdlTime 1262138345\n+                location file:/Users/carl/Projects/hd9/hive-trunk/build/ql/test/data/warehouse/src\n+                transient_lastDdlTime 1262809225\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n               name: src\n             name: src\n@@ -174,7 +184,7 @@ STAGE PLANS:\n                 File Output Operator\n                   compressed: false\n                   GlobalTableId: 0\n-                  directory: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/467070148/10002\n+                  directory: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/894782310/10002\n                   table:\n                       input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n                       output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n@@ -186,7 +196,7 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Alias -> Map Operator Tree:\n-        file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/467070148/10002 \n+        file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/894782310/10002 \n             Reduce Output Operator\n               key expressions:\n                     expr: _col0\n@@ -201,9 +211,9 @@ STAGE PLANS:\n                     type: bigint\n       Needs Tagging: false\n       Path -> Alias:\n-        file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/467070148/10002 [file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/467070148/10002]\n+        file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/894782310/10002 [file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/894782310/10002]\n       Path -> Partition:\n-        file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/467070148/10002 \n+        file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/894782310/10002 \n           Partition\n             base file name: 10002\n             input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n@@ -239,7 +249,7 @@ STAGE PLANS:\n             File Output Operator\n               compressed: false\n               GlobalTableId: 0\n-              directory: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/467070148/10001\n+              directory: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/894782310/10001\n               table:\n                   input format: org.apache.hadoop.mapred.TextInputFormat\n                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n@@ -256,33 +266,33 @@ STAGE PLANS:\n PREHOOK: query: SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1534071913/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/882248436/10000\n POSTHOOK: query: SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1534071913/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/882248436/10000\n 1\n 2\n 3\n PREHOOK: query: SELECT explode(array(1,2,3)) AS (myCol) FROM src LIMIT 3\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/730345114/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1127088300/10000\n POSTHOOK: query: SELECT explode(array(1,2,3)) AS (myCol) FROM src LIMIT 3\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/730345114/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1127088300/10000\n 1\n 2\n 3\n PREHOOK: query: SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1456413188/10000\n+PREHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1245396324/10000\n POSTHOOK: query: SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/data/users/jsichi/open/hive-trunk/build/ql/tmp/1456413188/10000\n+POSTHOOK: Output: file:/Users/carl/Projects/hd9/hive-trunk/build/ql/tmp/1245396324/10000\n 1\t1\n 2\t1\n 3\t1", "filename": "ql/src/test/results/clientpositive/udtf_explode.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3", "parent": "https://github.com/apache/hive/commit/c09884e0b662ecb635ccdb162d06ade2ea71d8b6", "message": "HIVE-9622 - Getting NPE when trying to restart HS2 when metastore is configured to use org.apache.hadoop.hive.thrift.DBTokenStore (Aihua via Brock)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1659302 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_279", "file": [{"additions": 66, "raw_url": "https://github.com/apache/hive/raw/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java", "blob_url": "https://github.com/apache/hive/blob/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java", "sha": "a30ec7e9e5e079f7c13456c29082121da9b6c202", "changes": 66, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java?ref=7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3", "patch": "@@ -0,0 +1,66 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.minikdc;\n+\n+import org.junit.Assert;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hive.service.auth.HiveAuthFactory;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+\n+public class TestHiveAuthFactory {\n+  private static HiveConf hiveConf;\n+  private static MiniHiveKdc miniHiveKdc = null;\n+\n+  @BeforeClass\n+  public static void setUp() throws Exception {\n+    hiveConf = new HiveConf();\n+    miniHiveKdc = MiniHiveKdc.getMiniHiveKdc(hiveConf);\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() throws Exception {\n+  }\n+\n+  /**\n+   * Verify that delegation token manager is started with no exception\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testStartTokenManager() throws Exception {\n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, HiveAuthFactory.AuthTypes.KERBEROS.getAuthName());\n+    String principalName = miniHiveKdc.getFullHiveServicePrincipal();\n+    System.out.println(\"Principal: \" + principalName);\n+    \n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL, principalName);\n+    String keyTabFile = miniHiveKdc.getKeyTabFile(miniHiveKdc.getHiveServicePrincipal());\n+    System.out.println(\"keyTabFile: \" + keyTabFile);\n+    Assert.assertNotNull(keyTabFile);\n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB, keyTabFile);\n+\n+    System.out.println(\"rawStoreClassName =\" +  hiveConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL));\n+\n+    HiveAuthFactory authFactory = new HiveAuthFactory(hiveConf);\n+    Assert.assertNotNull(authFactory);\n+    Assert.assertEquals(\"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory\", \n+        authFactory.getAuthTransFactory().getClass().getName());\n+  }\n+}", "filename": "itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java", "blob_url": "https://github.com/apache/hive/blob/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java", "sha": "22c309fbfd941fb517370057ef38fbec903ce088", "changes": 11, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java?ref=7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3", "patch": "@@ -18,7 +18,6 @@\n package org.apache.hive.service.auth;\n \n import java.io.IOException;\n-import java.net.InetAddress;\n import java.net.InetSocketAddress;\n import java.net.UnknownHostException;\n import java.util.ArrayList;\n@@ -33,6 +32,9 @@\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.HiveMetaStore;\n+import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.hive.shims.HadoopShims.KerberosNameShim;\n import org.apache.hadoop.hive.shims.ShimLoader;\n import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;\n@@ -108,8 +110,11 @@ public HiveAuthFactory(HiveConf conf) throws TTransportException {\n                         conf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL));\n         // start delegation token manager\n         try {\n-          saslServer.startDelegationTokenSecretManager(conf, null, ServerMode.HIVESERVER2);\n-        } catch (IOException e) {\n+          HMSHandler baseHandler = new HiveMetaStore.HMSHandler(\n+              \"new db based metaserver\", conf, true);\n+          saslServer.startDelegationTokenSecretManager(conf, baseHandler.getMS(), ServerMode.HIVESERVER2);\n+        }\n+        catch (MetaException|IOException e) {\n           throw new TTransportException(\"Failed to start token manager\", e);\n         }\n       }", "filename": "service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/c7d2643e0025d6fa16744872c7a131f4b5a377e6", "parent": "https://github.com/apache/hive/commit/88e790ad42a829ba0492e4ecfaf05dc750798a0c", "message": "HIVE-8166 : CBO: 1) Bailout in strict mode 2) OB,LIMIT RR table alias is same as that of sub query 3) If RowCount Not found then fall back to non cbo 4)Fix NPE in unique col name check (John Pullokkaran via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1625852 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_280", "file": [{"additions": 3, "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java", "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java", "sha": "ddef37be78d2ba64332d6b5eb963b98df4223290", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6", "patch": "@@ -131,6 +131,9 @@ public double getRowCount() {\n       }\n     }\n \n+    if (rowCount == -1)\n+      noColsMissingStats.getAndIncrement();\n+\n     return rowCount;\n   }\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java"}, {"additions": 1, "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java", "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java", "sha": "c643aa46c484210fe348602ae41bfe2deccca2c5", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6", "patch": "@@ -86,7 +86,7 @@ public static HiveProjectRel create(RelNode child, List<? extends RexNode> exps,\n     RelOptCluster cluster = child.getCluster();\n \n     // 1 Ensure columnNames are unique - OPTIQ-411\n-    if (!Util.isDistinct(fieldNames)) {\n+    if (fieldNames != null && !Util.isDistinct(fieldNames)) {\n       String msg = \"Select list contains multiple expressions with the same name.\" + fieldNames;\n       throw new OptiqSemanticException(msg);\n     }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "sha": "0daad5bfea23f76c846e17f7c7d5f4940a1cce46", "changes": 20, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6", "patch": "@@ -12139,6 +12139,7 @@ private boolean canHandleQuery(QB qbToChk, boolean topLevelQB) {\n     // 1. If top level QB is query then everything below it must also be Query\n     // 2. Nested Subquery will return false for qbToChk.getIsQuery()\n     if ((!topLevelQB || qbToChk.getIsQuery())\n+        && (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || conf.getVar(ConfVars.HIVEMAPREDMODE).equalsIgnoreCase(\"nonstrict\"))\n         && (!topLevelQB || (queryProperties.getJoinCount() > 1) || conf.getBoolVar(ConfVars.HIVE_IN_TEST))\n         && !queryProperties.hasClusterBy() && !queryProperties.hasDistributeBy()\n         && !queryProperties.hasSortBy() && !queryProperties.hasPTF()\n@@ -13870,7 +13871,6 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n         aliasToRel.put(tableAlias, op);\n       }\n \n-\n       if (aliasToRel.isEmpty()) {\n         //// This may happen for queries like select 1; (no source table)\n         // We can do following which is same, as what Hive does.\n@@ -13910,7 +13910,15 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n       selectRel = genSelectLogicalPlan(qb, srcRel);\n       srcRel = (selectRel == null) ? srcRel : selectRel;\n \n-      // 6. Incase this QB corresponds to subquery then modify its RR to point\n+      // 6. Build Rel for OB Clause\n+      obRel = genOBLogicalPlan(qb, srcRel);\n+      srcRel = (obRel == null) ? srcRel : obRel;\n+\n+      // 7. Build Rel for Limit Clause\n+      limitRel = genLimitLogicalPlan(qb, srcRel);\n+      srcRel = (limitRel == null) ? srcRel : limitRel;\n+\n+      // 8. Incase this QB corresponds to subquery then modify its RR to point\n       // to subquery alias\n       // TODO: cleanup this\n       if (qb.getParseInfo().getAlias() != null) {\n@@ -13932,14 +13940,6 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n         relToHiveColNameOptiqPosMap.put(srcRel, buildHiveToOptiqColumnMap(newRR, srcRel));\n       }\n \n-      // 7. Build Rel for OB Clause\n-      obRel = genOBLogicalPlan(qb, srcRel);\n-      srcRel = (obRel == null) ? srcRel : obRel;\n-\n-      // 8. Build Rel for Limit Clause\n-      limitRel = genLimitLogicalPlan(qb, srcRel);\n-      srcRel = (limitRel == null) ? srcRel : limitRel;\n-\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Created Plan for Query Block \" + qb.getId());\n       }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/463546f4b03e29aaf9f55c11e33f13e53be1840d", "parent": "https://github.com/apache/hive/commit/a989afbdb18f8e7da707f0dda5e481befd3d62fc", "message": "HIVE-6739 : Hive HBase query fails on Tez due to missing jars and then due to NPE in getSplits (Sergey Shelukhin, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1585602 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_281", "file": [{"additions": 27, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "sha": "14d188ff54e4d7e98704139cbf9c262f056967f1", "changes": 38, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -604,7 +604,7 @@ public PreWarmContext createPreWarmContext(TezSessionConfiguration sessionConfig\n     combinedResources.putAll(sessionConfig.getSessionResources());\n \n     try {\n-      for(LocalResource lr : localizeTempFiles(conf)) {\n+      for(LocalResource lr : localizeTempFilesFromConf(getHiveJarDirectory(conf), conf)) {\n         combinedResources.put(getBaseName(lr), lr);\n       }\n     } catch(LoginException le) {\n@@ -665,7 +665,8 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n    * @throws IOException when hdfs operation fails\n    * @throws LoginException when getDefaultDestDir fails with the same exception\n    */\n-  public List<LocalResource> localizeTempFiles(Configuration conf) throws IOException, LoginException {\n+  public List<LocalResource> localizeTempFilesFromConf(\n+      String hdfsDirPathStr, Configuration conf) throws IOException, LoginException {\n     List<LocalResource> tmpResources = new ArrayList<LocalResource>();\n \n     String addedFiles = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);\n@@ -683,15 +684,32 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n \n     String auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS);\n \n-    // need to localize the additional jars and files\n+    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n+    addTempFiles(conf, tmpResources, hdfsDirPathStr, allFiles.split(\",\"));\n+    return tmpResources;\n+  }\n \n-    // we need the directory on hdfs to which we shall put all these files\n-    // Use HIVE_JAR_DIRECTORY only if it's set explicitly; otherwise use default directory\n-    String hdfsDirPathStr = getHiveJarDirectory(conf);\n+  /**\n+   * Localizes files, archives and jars from a provided array of names.\n+   * @param hdfsDirPathStr Destination directoty in HDFS.\n+   * @param conf Configuration.\n+   * @param inputOutputJars The file names to localize.\n+   * @return List<LocalResource> local resources to add to execution\n+   * @throws IOException when hdfs operation fails.\n+   * @throws LoginException when getDefaultDestDir fails with the same exception\n+   */\n+  public List<LocalResource> localizeTempFiles(String hdfsDirPathStr, Configuration conf,\n+      String[] inputOutputJars) throws IOException, LoginException {\n+    if (inputOutputJars == null) return null;\n+    List<LocalResource> tmpResources = new ArrayList<LocalResource>();\n+    addTempFiles(conf, tmpResources, hdfsDirPathStr, inputOutputJars);\n+    return tmpResources;\n+  }\n \n-    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n-    String[] allFilesArr = allFiles.split(\",\");\n-    for (String file : allFilesArr) {\n+  private void addTempFiles(Configuration conf,\n+      List<LocalResource> tmpResources, String hdfsDirPathStr,\n+      String[] files) throws IOException {\n+    for (String file : files) {\n       if (!StringUtils.isNotBlank(file)) {\n         continue;\n       }\n@@ -700,8 +718,6 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n           new Path(hdfsFilePathStr), conf);\n       tmpResources.add(localResource);\n     }\n-\n-    return tmpResources;\n   }\n \n   public String getHiveJarDirectory(Configuration conf) throws IOException, LoginException {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java"}, {"additions": 18, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java", "sha": "6cbe8c55d55cc15bb6458f31a1e813cf27fae83e", "changes": 30, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -64,7 +64,7 @@ public void startPool() throws Exception {\n       HiveConf newConf = new HiveConf(initConf);\n       TezSessionState sessionState = defaultQueuePool.take();\n       newConf.set(\"tez.queue.name\", sessionState.getQueueName());\n-      sessionState.open(TezSessionState.makeSessionId(), newConf);\n+      sessionState.open(newConf);\n       defaultQueuePool.put(sessionState);\n     }\n   }\n@@ -91,7 +91,7 @@ public void setupPool(HiveConf conf) throws InterruptedException {\n         if (queue.length() == 0) {\n           continue;\n         }\n-        TezSessionState sessionState = createSession();\n+        TezSessionState sessionState = createSession(TezSessionState.makeSessionId());\n         sessionState.setQueueName(queue);\n         sessionState.setDefault();\n         LOG.info(\"Created new tez session for queue: \" + queue +\n@@ -102,7 +102,7 @@ public void setupPool(HiveConf conf) throws InterruptedException {\n     }\n   }\n \n-  private TezSessionState getSession(HiveConf conf)\n+  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n       throws Exception {\n \n     String queueName = conf.get(\"tez.queue.name\");\n@@ -120,7 +120,7 @@ private TezSessionState getSession(HiveConf conf)\n       LOG.info(\"QueueName: \" + queueName + \" nonDefaultUser: \" + nonDefaultUser +\n           \" defaultQueuePool: \" + defaultQueuePool +\n           \" blockingQueueLength: \" + blockingQueueLength);\n-      return getNewSessionState(conf, queueName);\n+      return getNewSessionState(conf, queueName, doOpen);\n     }\n \n     LOG.info(\"Choosing a session from the defaultQueuePool\");\n@@ -130,16 +130,21 @@ private TezSessionState getSession(HiveConf conf)\n   /**\n    * @param conf HiveConf that is used to initialize the session\n    * @param queueName could be null. Set in the tez session.\n+   * @param doOpen\n    * @return\n    * @throws Exception\n    */\n   private TezSessionState getNewSessionState(HiveConf conf,\n-      String queueName) throws Exception {\n-    TezSessionState retTezSessionState = createSession();\n+      String queueName, boolean doOpen) throws Exception {\n+    TezSessionState retTezSessionState = createSession(TezSessionState.makeSessionId());\n     retTezSessionState.setQueueName(queueName);\n-    retTezSessionState.open(TezSessionState.makeSessionId(), conf);\n+    String what = \"Created\";\n+    if (doOpen) {\n+      retTezSessionState.open(conf);\n+      what = \"Started\";\n+    }\n \n-    LOG.info(\"Started a new session for queue: \" + queueName +\n+    LOG.info(what + \" a new session for queue: \" + queueName +\n         \" session id: \" + retTezSessionState.getSessionId());\n     return retTezSessionState;\n   }\n@@ -179,11 +184,12 @@ public void stop() throws Exception {\n     }\n   }\n \n-  protected TezSessionState createSession() {\n-    return new TezSessionState();\n+  protected TezSessionState createSession(String sessionId) {\n+    return new TezSessionState(sessionId);\n   }\n \n-  public TezSessionState getSession(TezSessionState session, HiveConf conf) throws Exception {\n+  public TezSessionState getSession(\n+      TezSessionState session, HiveConf conf, boolean doOpen) throws Exception {\n     if (canWorkWithSameSession(session, conf)) {\n       return session;\n     }\n@@ -192,7 +198,7 @@ public TezSessionState getSession(TezSessionState session, HiveConf conf) throws\n       session.close(false);\n     }\n \n-    return getSession(conf);\n+    return getSession(conf, doOpen);\n   }\n \n   /*", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java"}, {"additions": 30, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java", "sha": "74940e68846184586a4fc97cc70868ffd5bd1379", "changes": 35, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -23,6 +23,7 @@\n import java.net.URISyntaxException;\n import java.util.Collections;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -68,6 +69,8 @@\n   private String queueName;\n   private boolean defaultQueue = false;\n \n+  private HashSet<String> additionalAmFiles = null;\n+\n   private static List<TezSessionState> openSessions\n     = Collections.synchronizedList(new LinkedList<TezSessionState>());\n \n@@ -83,8 +86,9 @@ public TezSessionState(DagUtils utils) {\n    * Constructor. We do not automatically connect, because we only want to\n    * load tez classes when the user has tez installed.\n    */\n-  public TezSessionState() {\n+  public TezSessionState(String sessionId) {\n     this(DagUtils.getInstance());\n+    this.sessionId = sessionId;\n   }\n \n   /**\n@@ -106,6 +110,11 @@ public static String makeSessionId() {\n     return UUID.randomUUID().toString();\n   }\n \n+  public void open(HiveConf conf)\n+      throws IOException, LoginException, URISyntaxException, TezException {\n+    open(conf, null);\n+  }\n+\n   /**\n    * Creates a tez session. A session is tied to either a cli/hs2 session. You can\n    * submit multiple DAGs against a session (as long as they are executed serially).\n@@ -114,10 +123,8 @@ public static String makeSessionId() {\n    * @throws LoginException\n    * @throws TezException\n    */\n-  public void open(String sessionId, HiveConf conf)\n-    throws IOException, LoginException, URISyntaxException, TezException {\n-\n-    this.sessionId = sessionId;\n+  public void open(HiveConf conf, List<LocalResource> additionalLr)\n+    throws IOException, LoginException, IllegalArgumentException, URISyntaxException, TezException {\n     this.conf = conf;\n \n     // create the tez tmp dir\n@@ -135,6 +142,14 @@ public void open(String sessionId, HiveConf conf)\n     // configuration for the application master\n     Map<String, LocalResource> commonLocalResources = new HashMap<String, LocalResource>();\n     commonLocalResources.put(utils.getBaseName(appJarLr), appJarLr);\n+    if (additionalLr != null) {\n+      additionalAmFiles = new HashSet<String>();\n+      for (LocalResource lr : additionalLr) {\n+        String baseName = utils.getBaseName(lr);\n+        additionalAmFiles.add(baseName);\n+        commonLocalResources.put(baseName, lr);\n+      }\n+    }\n \n     // Create environment for AM.\n     Map<String, String> amEnv = new HashMap<String, String>();\n@@ -174,6 +189,15 @@ public void open(String sessionId, HiveConf conf)\n     openSessions.add(this);\n   }\n \n+  public boolean hasResources(List<LocalResource> lrs) {\n+    if (lrs == null || lrs.isEmpty()) return true;\n+    if (additionalAmFiles == null || additionalAmFiles.isEmpty()) return false;\n+    for (LocalResource lr : lrs) {\n+      if (!additionalAmFiles.contains(utils.getBaseName(lr))) return false;\n+    }\n+    return true;\n+  }\n+\n   /**\n    * Close a tez session. Will cleanup any tez/am related resources. After closing a session\n    * no further DAGs can be executed against it.\n@@ -202,6 +226,7 @@ public void close(boolean keepTmpDir) throws TezException, IOException {\n     tezScratchDir = null;\n     conf = null;\n     appJarLr = null;\n+    additionalAmFiles = null;\n   }\n \n   public String getSessionId() {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java"}, {"additions": 38, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java", "sha": "c355d5aed2d2f9c8550c8ec56132b9fc68a80c30", "changes": 61, "status": "modified", "deletions": 23, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -23,6 +23,7 @@\n import java.util.Collections;\n import java.util.EnumSet;\n import java.util.HashMap;\n+import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -103,45 +104,62 @@ public int execute(DriverContext driverContext) {\n     TezSessionState session = null;\n \n     try {\n-      // Get or create Context object. If we create it we have to clean\n-      // it later as well.\n+      // Get or create Context object. If we create it we have to clean it later as well.\n       ctx = driverContext.getCtx();\n       if (ctx == null) {\n         ctx = new Context(conf);\n         cleanContext = true;\n       }\n \n-      // Need to remove this static hack. But this is the way currently to\n-      // get a session.\n+      // Need to remove this static hack. But this is the way currently to get a session.\n       SessionState ss = SessionState.get();\n       session = ss.getTezSession();\n-      session = TezSessionPoolManager.getInstance().getSession(session, conf);\n+      session = TezSessionPoolManager.getInstance().getSession(session, conf, false);\n       ss.setTezSession(session);\n \n-      // if it's not running start it.\n-      if (!session.isOpen()) {\n-        // can happen if the user sets the tez flag after the session was\n-        // established\n-        LOG.info(\"Tez session hasn't been created yet. Opening session\");\n-        session.open(session.getSessionId(), conf);\n-      }\n+      // jobConf will hold all the configuration for hadoop, tez, and hive\n+      JobConf jobConf = utils.createConfiguration(conf);\n+\n+      // Get all user jars from work (e.g. input format stuff).\n+      String[] inputOutputJars = work.configureJobConfAndExtractJars(jobConf);\n \n       // we will localize all the files (jars, plans, hashtables) to the\n-      // scratch dir. let's create this first.\n+      // scratch dir. let's create this and tmp first.\n       Path scratchDir = ctx.getMRScratchDir();\n-\n-      // create the tez tmp dir\n       utils.createTezDir(scratchDir, conf);\n \n-      // jobConf will hold all the configuration for hadoop, tez, and hive\n-      JobConf jobConf = utils.createConfiguration(conf);\n+      // we need to get the user specified local resources for this dag\n+      String hiveJarDir = utils.getHiveJarDirectory(conf);\n+      List<LocalResource> additionalLr = utils.localizeTempFilesFromConf(hiveJarDir, conf);\n+      List<LocalResource> handlerLr = utils.localizeTempFiles(hiveJarDir, conf, inputOutputJars);\n+      if (handlerLr != null) {\n+        additionalLr.addAll(handlerLr);\n+      }\n+\n+      // If we have any jars from input format, we need to restart the session because\n+      // AM will need them; so, AM has to be restarted. What a mess...\n+      if (!session.hasResources(handlerLr)) {\n+        if (session.isOpen()) {\n+          LOG.info(\"Tez session being reopened to pass custom jars to AM\");\n+          session.close(false);\n+          session = TezSessionPoolManager.getInstance().getSession(null, conf, false);\n+          ss.setTezSession(session);\n+        }\n+        session.open(conf, additionalLr);\n+      }\n+      if (!session.isOpen()) {\n+        // can happen if the user sets the tez flag after the session was\n+        // established\n+        LOG.info(\"Tez session hasn't been created yet. Opening session\");\n+        session.open(conf);\n+      }\n \n       // unless already installed on all the cluster nodes, we'll have to\n       // localize hive-exec.jar as well.\n       LocalResource appJarLr = session.getAppJarLr();\n \n       // next we translate the TezWork to a Tez DAG\n-      DAG dag = build(jobConf, work, scratchDir, appJarLr, ctx);\n+      DAG dag = build(jobConf, work, scratchDir, appJarLr, additionalLr, ctx);\n \n       // submit will send the job to the cluster and start executing\n       client = submit(jobConf, dag, scratchDir, appJarLr, session);\n@@ -186,16 +204,13 @@ public int execute(DriverContext driverContext) {\n   }\n \n   DAG build(JobConf conf, TezWork work, Path scratchDir,\n-      LocalResource appJarLr, Context ctx)\n+      LocalResource appJarLr, List<LocalResource> additionalLr, Context ctx)\n       throws Exception {\n \n     perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_BUILD_DAG);\n     Map<BaseWork, Vertex> workToVertex = new HashMap<BaseWork, Vertex>();\n     Map<BaseWork, JobConf> workToConf = new HashMap<BaseWork, JobConf>();\n \n-    // we need to get the user specified local resources for this dag\n-    List<LocalResource> additionalLr = utils.localizeTempFiles(conf);\n-\n     // getAllWork returns a topologically sorted list, which we use to make\n     // sure that vertices are created before they are used in edges.\n     List<BaseWork> ws = work.getAllWork();\n@@ -299,7 +314,7 @@ DAGClient submit(JobConf conf, DAG dag, Path scratchDir,\n       sessionState.close(true);\n \n       // (re)open the session\n-      sessionState.open(sessionState.getSessionId(), this.conf);\n+      sessionState.open(this.conf);\n \n       console.printInfo(\"Session re-established.\");\n ", "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java"}, {"additions": 7, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "sha": "f285312bab17a6d14be0a5cacc8ae4e830e0691f", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -845,6 +845,13 @@ public static void setKeyAndValueDescForTaskTree(Task<? extends Serializable> ta\n           setKeyAndValueDesc(work.getReduceWork(), op);\n         }\n       }\n+    } else if (task != null && (task.getWork() instanceof TezWork)) {\n+      TezWork work = (TezWork)task.getWork();\n+      for (BaseWork w : work.getAllWorkUnsorted()) {\n+        if (w instanceof MapWork) {\n+          ((MapWork)w).deriveExplainAttributes();\n+        }\n+      }\n     }\n \n     if (task.getChildTasks() == null) {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java", "sha": "4d3658fb0694b63fe05e3926bf63ceb7425908d9", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -28,6 +28,7 @@\n \n import org.apache.hadoop.hive.ql.exec.HashTableDummyOperator;\n import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.mapred.JobConf;\n \n /**\n  * BaseWork. Base class for any \"work\" that's being done on the cluster. Items like stats\n@@ -106,4 +107,6 @@ public void addDummyOp(HashTableDummyOperator dummyOp) {\n \n     return returnSet;\n   }\n+\n+  public abstract void configureJobConf(JobConf job);\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java"}, {"additions": 3, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java", "sha": "9945dea53e714424d27d6c67babe2798ab77be8b", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -172,6 +172,8 @@ public void setPathToPartitionInfo(\n \n   /**\n    * Derive additional attributes to be rendered by EXPLAIN.\n+   * TODO: this method is relied upon by custom input formats to set jobconf properties.\n+   *       This is madness? - This is Hive Storage Handlers!\n    */\n   public void deriveExplainAttributes() {\n     if (pathToPartitionInfo != null) {\n@@ -495,6 +497,7 @@ public String getSamplingTypeString() {\n         samplingType == 2 ? \"SAMPLING_ON_START\" : null;\n   }\n \n+  @Override\n   public void configureJobConf(JobConf job) {\n     for (PartitionDesc partition : aliasToPartnInfo.values()) {\n       PlanUtils.configureJobConf(partition.getTableDesc(), job);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java"}, {"additions": 36, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java", "sha": "7394588130861466cb98b5e53a74a729214397ec", "changes": 36, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -33,6 +33,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;\n+import org.apache.hadoop.mapred.JobConf;\n import org.apache.tez.dag.api.EdgeProperty;\n \n /**\n@@ -93,6 +94,10 @@ public String getName() {\n     return result;\n   }\n \n+  public Collection<BaseWork> getAllWorkUnsorted() {\n+    return workGraph.keySet();\n+  }\n+\n   private void visit(BaseWork child, Set<BaseWork> seen, List<BaseWork> result) {\n \n     if (seen.contains(child)) {\n@@ -271,6 +276,37 @@ public String getType() {\n     }\n     return result;\n   }\n+  \n+  private static final String MR_JAR_PROPERTY = \"tmpjars\";\n+  /**\n+   * Calls configureJobConf on instances of work that are part of this TezWork.\n+   * Uses the passed job configuration to extract \"tmpjars\" added by these, so that Tez\n+   * could add them to the job proper Tez way. This is a very hacky way but currently\n+   * there's no good way to get these JARs - both storage handler interface, and HBase\n+   * code, would have to change to get the list directly (right now it adds to tmpjars).\n+   * This will happen in 0.14 hopefully.\n+   * @param jobConf Job configuration.\n+   * @return List of files added to tmpjars by storage handlers.\n+   */\n+  public String[] configureJobConfAndExtractJars(JobConf jobConf) {\n+    String[] oldTmpJars = jobConf.getStrings(MR_JAR_PROPERTY);\n+    jobConf.setStrings(MR_JAR_PROPERTY, new String[0]);\n+    for (BaseWork work : workGraph.keySet()) {\n+      work.configureJobConf(jobConf);\n+    }\n+    String[] newTmpJars = jobConf.getStrings(MR_JAR_PROPERTY);\n+    if (oldTmpJars != null && (oldTmpJars.length != 0)) {\n+      if (newTmpJars != null && (newTmpJars.length != 0)) {\n+        String[] combinedTmpJars = new String[newTmpJars.length + oldTmpJars.length];\n+        System.arraycopy(oldTmpJars, 0, combinedTmpJars, 0, oldTmpJars.length);\n+        System.arraycopy(newTmpJars, 0, combinedTmpJars, oldTmpJars.length, newTmpJars.length);\n+        jobConf.setStrings(MR_JAR_PROPERTY, combinedTmpJars);\n+      } else {\n+        jobConf.setStrings(MR_JAR_PROPERTY, oldTmpJars);\n+      }\n+    }\n+    return newTmpJars;\n+   }\n \n   /**\n    * connect adds an edge between a and b. Both nodes have", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java", "sha": "5ef0e074b632ec74b4e546ffddba52719bc6b60d", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.UnionOperator;\n+import org.apache.hadoop.mapred.JobConf;\n \n /**\n  * Simple wrapper for union all cases. All contributing work for a union all\n@@ -68,4 +69,7 @@ public void addUnionOperators(Collection<UnionOperator> unions) {\n   public Set<UnionOperator> getUnionOperators() {\n     return unionOperators;\n   }\n+\n+  public void configureJobConf(JobConf job) {\n+  }\n }", "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java", "sha": "4785b7f1576c8deabb12ab4782d8dc53976bc64b", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -347,9 +347,9 @@ public static SessionState start(SessionState startSs) {\n         .equals(\"tez\") && (startSs.isHiveServerQuery == false)) {\n       try {\n         if (startSs.tezSessionState == null) {\n-          startSs.tezSessionState = new TezSessionState();\n+          startSs.tezSessionState = new TezSessionState(startSs.getSessionId());\n         }\n-        startSs.tezSessionState.open(startSs.getSessionId(), startSs.conf);\n+        startSs.tezSessionState.open(startSs.conf);\n       } catch (Exception e) {\n         throw new RuntimeException(e);\n       }\n@@ -815,10 +815,10 @@ public boolean delete_resource(ResourceType t, String value) {\n   }\n \n   public Set<String> list_resource(ResourceType t, List<String> filter) {\n-    if (resource_map.get(t) == null) {\n+    Set<String> orig = resource_map.get(t);\n+    if (orig == null) {\n       return null;\n     }\n-    Set<String> orig = resource_map.get(t);\n     if (filter == null) {\n       return orig;\n     } else {", "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java", "sha": "ad5a6e7ac218a1d16168b3a93e50dbbc6353dc69", "changes": 20, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -40,9 +40,9 @@ public TestTezSessionPoolManager() {\n     }\n \n     @Override\n-      public TezSessionState createSession() {\n-        return new TestTezSessionState();\n-      }\n+    public TezSessionState createSession(String sessionId) {\n+      return new TestTezSessionState(sessionId);\n+    }\n   }\n \n   @Before\n@@ -54,8 +54,8 @@ public void setUp() {\n     public void testGetNonDefaultSession() {\n       poolManager = new TestTezSessionPoolManager();\n       try {\n-        TezSessionState sessionState = poolManager.getSession(null, conf);\n-        TezSessionState sessionState1 = poolManager.getSession(sessionState, conf);\n+        TezSessionState sessionState = poolManager.getSession(null, conf, true);\n+        TezSessionState sessionState1 = poolManager.getSession(sessionState, conf, true);\n         if (sessionState1 != sessionState) {\n           fail();\n         }\n@@ -75,25 +75,25 @@ public void testSessionPoolGetInOrder() {\n         poolManager = new TestTezSessionPoolManager();\n         poolManager.setupPool(conf);\n         poolManager.startPool();\n-        TezSessionState sessionState = poolManager.getSession(null, conf);\n+        TezSessionState sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"a\") != 0) {\n           fail();\n         }\n         poolManager.returnSession(sessionState);\n \n-        sessionState = poolManager.getSession(null, conf);\n+        sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"b\") != 0) {\n           fail();\n         }\n         poolManager.returnSession(sessionState);\n \n-        sessionState = poolManager.getSession(null, conf);\n+        sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"c\") != 0) {\n           fail();\n         }\n         poolManager.returnSession(sessionState);\n \n-        sessionState = poolManager.getSession(null, conf);\n+        sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"a\") != 0) {\n           fail();\n         }\n@@ -118,7 +118,7 @@ public void run() {\n             tmpConf.set(\"tez.queue.name\", \"\");\n           }\n \n-          TezSessionState session = poolManager.getSession(null, tmpConf);\n+          TezSessionState session = poolManager.getSession(null, tmpConf, true);\n           Thread.sleep((random.nextInt(9) % 10) * 1000);\n           poolManager.returnSession(session);\n         } catch (Exception e) {", "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java"}, {"additions": 8, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java", "sha": "6ee6e42a979d58e1693f9f9ea85cec3842910d99", "changes": 13, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -38,6 +38,11 @@\n   private String sessionId;\n   private HiveConf hiveConf;\n \n+  public TestTezSessionState(String sessionId) {\n+    super(sessionId);\n+    this.sessionId = sessionId;\n+  }\n+\n   @Override\n     public boolean isOpen() {\n       return open;\n@@ -48,11 +53,9 @@ public void setOpen(boolean open) {\n   }\n \n   @Override\n-    public void open(String sessionId, HiveConf conf) throws IOException,\n-           LoginException, URISyntaxException, TezException {\n-             this.sessionId = sessionId;\n-             this.hiveConf = conf;\n-    }\n+  public void open(HiveConf conf) {\n+    this.hiveConf = conf;\n+  }\n \n   @Override\n     public void close(boolean keepTmpDir) throws TezException, IOException {", "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java"}, {"additions": 4, "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java", "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java", "sha": "1793b58fc8a218e9d81cdc0b564d1739f3d637b7", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d", "patch": "@@ -33,6 +33,7 @@\n import java.util.ArrayList;\n import java.util.LinkedHashMap;\n import java.util.List;\n+import java.util.Map;\n \n import javax.security.auth.login.LoginException;\n \n@@ -176,7 +177,7 @@ public void tearDown() throws Exception {\n \n   @Test\n   public void testBuildDag() throws IllegalArgumentException, IOException, Exception {\n-    DAG dag = task.build(conf, work, path, appLr, new Context(conf));\n+    DAG dag = task.build(conf, work, path, appLr, null, new Context(conf));\n     for (BaseWork w: work.getAllWork()) {\n       Vertex v = dag.getVertex(w.getName());\n       assertNotNull(v);\n@@ -196,7 +197,7 @@ public void testBuildDag() throws IllegalArgumentException, IOException, Excepti\n \n   @Test\n   public void testEmptyWork() throws IllegalArgumentException, IOException, Exception {\n-    DAG dag = task.build(conf, new TezWork(\"\"), path, appLr, new Context(conf));\n+    DAG dag = task.build(conf, new TezWork(\"\"), path, appLr, null, new Context(conf));\n     assertEquals(dag.getVertices().size(), 0);\n   }\n \n@@ -206,7 +207,7 @@ public void testSubmit() throws LoginException, IllegalArgumentException,\n     DAG dag = new DAG(\"test\");\n     task.submit(conf, dag, path, appLr, sessionState);\n     // validate close/reopen\n-    verify(sessionState, times(1)).open(any(String.class), any(HiveConf.class));\n+    verify(sessionState, times(1)).open(any(HiveConf.class));\n     verify(sessionState, times(1)).close(eq(true));\n     verify(session, times(2)).submitDAG(any(DAG.class));\n   }", "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/fc525e65f9686b57a429c37be2023fbeca9ac3b6", "parent": "https://github.com/apache/hive/commit/c0306764f9cce5628a24fe57eb7886408557f340", "message": "HIVE-5601: NPE in ORC's PPD when using select * from table with where predicate (Prasanth J via Owen O'Malley and Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1540173 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_282", "file": [{"additions": 8, "raw_url": "https://github.com/apache/hive/raw/fc525e65f9686b57a429c37be2023fbeca9ac3b6/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java", "blob_url": "https://github.com/apache/hive/blob/fc525e65f9686b57a429c37be2023fbeca9ac3b6/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java", "sha": "f143c32fc7948d3fcc720b27517cb7de59d2e537", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java?ref=fc525e65f9686b57a429c37be2023fbeca9ac3b6", "patch": "@@ -22,6 +22,7 @@\n import java.io.InputStream;\n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n@@ -362,6 +363,13 @@ public RecordReader rows(long offset, long length, boolean[] include\n   public RecordReader rows(long offset, long length, boolean[] include,\n                            SearchArgument sarg, String[] columnNames\n                            ) throws IOException {\n+\n+    // if included columns is null, then include all columns\n+    if (include == null) {\n+      include = new boolean[footer.getTypesCount()];\n+      Arrays.fill(include, true);\n+    }\n+\n     return new RecordReaderImpl(this.getStripes(), fileSystem,  path, offset,\n         length, footer.getTypesList(), codec, bufferSize,\n         include, footer.getRowIndexStride(), sarg, columnNames);", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java"}, {"additions": 10, "raw_url": "https://github.com/apache/hive/raw/fc525e65f9686b57a429c37be2023fbeca9ac3b6/ql/src/test/queries/clientpositive/orc_predicate_pushdown.q", "blob_url": "https://github.com/apache/hive/blob/fc525e65f9686b57a429c37be2023fbeca9ac3b6/ql/src/test/queries/clientpositive/orc_predicate_pushdown.q", "sha": "a267bfe8e13b91b9383abcaf6353967806a1c391", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_predicate_pushdown.q?ref=fc525e65f9686b57a429c37be2023fbeca9ac3b6", "patch": "@@ -49,6 +49,16 @@ SET hive.optimize.index.filter=false;\n -- hive.optimize.index.filter is set to true. the explain plan should show filter expression\n -- in table scan operator.\n \n+SELECT * FROM orc_pred WHERE t<2 limit 1;\n+SET hive.optimize.index.filter=true;\n+SELECT * FROM orc_pred WHERE t<2 limit 1;\n+SET hive.optimize.index.filter=false;\n+\n+SELECT * FROM orc_pred WHERE t>2 limit 1;\n+SET hive.optimize.index.filter=true;\n+SELECT * FROM orc_pred WHERE t>2 limit 1;\n+SET hive.optimize.index.filter=false;\n+\n SELECT SUM(HASH(t)) FROM orc_pred\n   WHERE t IS NOT NULL\n   AND t < 0", "filename": "ql/src/test/queries/clientpositive/orc_predicate_pushdown.q"}, {"additions": 85, "raw_url": "https://github.com/apache/hive/raw/fc525e65f9686b57a429c37be2023fbeca9ac3b6/ql/src/test/results/clientpositive/orc_predicate_pushdown.q.out", "blob_url": "https://github.com/apache/hive/blob/fc525e65f9686b57a429c37be2023fbeca9ac3b6/ql/src/test/results/clientpositive/orc_predicate_pushdown.q.out", "sha": "c58e01c6e35f9e289838ab384e4cfddc0bdc5b91", "changes": 90, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_predicate_pushdown.q.out?ref=fc525e65f9686b57a429c37be2023fbeca9ac3b6", "patch": "@@ -277,18 +277,98 @@ PREHOOK: query: -- all the following queries have predicates which are pushed do\n -- hive.optimize.index.filter is set to true. the explain plan should show filter expression\n -- in table scan operator.\n \n-SELECT SUM(HASH(t)) FROM orc_pred\n-  WHERE t IS NOT NULL\n-  AND t < 0\n-  AND t > -2\n+SELECT * FROM orc_pred WHERE t<2 limit 1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@orc_pred\n #### A masked pattern was here ####\n POSTHOOK: query: -- all the following queries have predicates which are pushed down to table scan operator if\n -- hive.optimize.index.filter is set to true. the explain plan should show filter expression\n -- in table scan operator.\n \n-SELECT SUM(HASH(t)) FROM orc_pred\n+SELECT * FROM orc_pred WHERE t<2 limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_pred.b SIMPLE [(staging)staging.FieldSchema(name:b, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bin SIMPLE [(staging)staging.FieldSchema(name:bin, type:binary, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bo SIMPLE [(staging)staging.FieldSchema(name:bo, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.d SIMPLE [(staging)staging.FieldSchema(name:d, type:double, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.dec SIMPLE [(staging)staging.FieldSchema(name:dec, type:decimal(4,2), comment:null), ]\n+POSTHOOK: Lineage: orc_pred.f SIMPLE [(staging)staging.FieldSchema(name:f, type:float, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.i SIMPLE [(staging)staging.FieldSchema(name:i, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.s SIMPLE [(staging)staging.FieldSchema(name:s, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.si SIMPLE [(staging)staging.FieldSchema(name:si, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.t SIMPLE [(staging)staging.FieldSchema(name:t, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.ts SIMPLE [(staging)staging.FieldSchema(name:ts, type:timestamp, comment:null), ]\n+-3\t467\t65575\t4294967437\t81.64\t23.53\ttrue\ttom hernandez\t2013-03-01 09:11:58.703188\t32.85\t\u0001study skills\u0002\n+PREHOOK: query: SELECT * FROM orc_pred WHERE t<2 limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT * FROM orc_pred WHERE t<2 limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_pred.b SIMPLE [(staging)staging.FieldSchema(name:b, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bin SIMPLE [(staging)staging.FieldSchema(name:bin, type:binary, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bo SIMPLE [(staging)staging.FieldSchema(name:bo, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.d SIMPLE [(staging)staging.FieldSchema(name:d, type:double, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.dec SIMPLE [(staging)staging.FieldSchema(name:dec, type:decimal(4,2), comment:null), ]\n+POSTHOOK: Lineage: orc_pred.f SIMPLE [(staging)staging.FieldSchema(name:f, type:float, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.i SIMPLE [(staging)staging.FieldSchema(name:i, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.s SIMPLE [(staging)staging.FieldSchema(name:s, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.si SIMPLE [(staging)staging.FieldSchema(name:si, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.t SIMPLE [(staging)staging.FieldSchema(name:t, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.ts SIMPLE [(staging)staging.FieldSchema(name:ts, type:timestamp, comment:null), ]\n+-3\t467\t65575\t4294967437\t81.64\t23.53\ttrue\ttom hernandez\t2013-03-01 09:11:58.703188\t32.85\t\u0001study skills\u0002\n+PREHOOK: query: SELECT * FROM orc_pred WHERE t>2 limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT * FROM orc_pred WHERE t>2 limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_pred.b SIMPLE [(staging)staging.FieldSchema(name:b, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bin SIMPLE [(staging)staging.FieldSchema(name:bin, type:binary, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bo SIMPLE [(staging)staging.FieldSchema(name:bo, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.d SIMPLE [(staging)staging.FieldSchema(name:d, type:double, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.dec SIMPLE [(staging)staging.FieldSchema(name:dec, type:decimal(4,2), comment:null), ]\n+POSTHOOK: Lineage: orc_pred.f SIMPLE [(staging)staging.FieldSchema(name:f, type:float, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.i SIMPLE [(staging)staging.FieldSchema(name:i, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.s SIMPLE [(staging)staging.FieldSchema(name:s, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.si SIMPLE [(staging)staging.FieldSchema(name:si, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.t SIMPLE [(staging)staging.FieldSchema(name:t, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.ts SIMPLE [(staging)staging.FieldSchema(name:ts, type:timestamp, comment:null), ]\n+124\t336\t65664\t4294967435\t74.72\t42.47\ttrue\tbob davidson\t2013-03-01 09:11:58.703302\t45.4\t\u0001yard duty\u0002\n+PREHOOK: query: SELECT * FROM orc_pred WHERE t>2 limit 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT * FROM orc_pred WHERE t>2 limit 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_pred.b SIMPLE [(staging)staging.FieldSchema(name:b, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bin SIMPLE [(staging)staging.FieldSchema(name:bin, type:binary, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.bo SIMPLE [(staging)staging.FieldSchema(name:bo, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.d SIMPLE [(staging)staging.FieldSchema(name:d, type:double, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.dec SIMPLE [(staging)staging.FieldSchema(name:dec, type:decimal(4,2), comment:null), ]\n+POSTHOOK: Lineage: orc_pred.f SIMPLE [(staging)staging.FieldSchema(name:f, type:float, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.i SIMPLE [(staging)staging.FieldSchema(name:i, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.s SIMPLE [(staging)staging.FieldSchema(name:s, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.si SIMPLE [(staging)staging.FieldSchema(name:si, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.t SIMPLE [(staging)staging.FieldSchema(name:t, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: orc_pred.ts SIMPLE [(staging)staging.FieldSchema(name:ts, type:timestamp, comment:null), ]\n+124\t336\t65664\t4294967435\t74.72\t42.47\ttrue\tbob davidson\t2013-03-01 09:11:58.703302\t45.4\t\u0001yard duty\u0002\n+PREHOOK: query: SELECT SUM(HASH(t)) FROM orc_pred\n+  WHERE t IS NOT NULL\n+  AND t < 0\n+  AND t > -2\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_pred\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT SUM(HASH(t)) FROM orc_pred\n   WHERE t IS NOT NULL\n   AND t < 0\n   AND t > -2", "filename": "ql/src/test/results/clientpositive/orc_predicate_pushdown.q.out"}], "repo": "hive"}, {"commit": "https://github.com/apache/hive/commit/cc61020d7f4adff7905b8301b7908f3737f483a8", "parent": "https://github.com/apache/hive/commit/c4f4bc8b032d7e04e89668731e7c7d3e3e3b9f03", "message": "HIVE-2778 [jira] Fail on table sampling\n(Navis Ryu via Carl Steinbach)\n\nSummary:\nHIVE-2778 fix NPE on table sampling\n\nTrying table sampling on any non-empty table throws NPE. This does not occur by\ntest on mini-MR.  <div class=\"preformatted panel\" style=\"border-width:\n1px;\"><div class=\"preformattedContent panelContent\"> <pre>select count(*) from\nemp tablesample (0.1 percent);      Total MapReduce jobs = 1 Launching Job 1 out\nof 1 Number of reduce tasks determined at compile time: 1 In order to change the\naverage load for a reducer (in bytes):   set\nhive.exec.reducers.bytes.per.reducer=<number> In order to limit the maximum\nnumber of reducers:   set hive.exec.reducers.max=<number> In order to set a\nconstant number of reducers:   set mapred.reduce.tasks=<number>\njava.lang.NullPointerException \tat\norg.apache.hadoop.hive.ql.io.CombineHiveInputFormat.sampleSplits(CombineHiveInputFormat.java:450)\n\tat\norg.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:403)\n\tat org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:971) \tat\norg.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:963) \tat\norg.apache.hadoop.mapred.JobClient.access$500(JobClient.java:170) \tat\norg.apache.hadoop.mapred.JobClient$2.run(JobClient.java:880) \tat\norg.apache.hadoop.mapred.JobClient$2.run(JobClient.java:833) \tat\njava.security.AccessController.doPrivileged(Native Method) \tat\njavax.security.auth.Subject.doAs(Subject.java:396) \tat\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:833) \tat\norg.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:807) \tat\norg.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:432) \tat\norg.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:136) \tat\norg.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134) \tat\norg.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57) \tat\norg.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332) \tat\norg.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123) \tat\norg.apache.hadoop.hive.ql.Driver.run(Driver.java:931) \tat\norg.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255) \tat\norg.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212) \tat\norg.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) \tat\norg.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671) \tat\norg.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554) \tat\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597) \tat\norg.apache.hadoop.util.RunJar.main(RunJar.java:186) Job Submission failed with\nexception 'java.lang.NullPointerException(null)' FAILED: Execution Error, return\ncode 1 from org.apache.hadoop.hive.ql.exec.MapRedTask  </pre> </div></div>\n\nTest Plan: EMPTY\n\nReviewers: JIRA, cwsteinbach\n\nReviewed By: cwsteinbach\n\nDifferential Revision: https://reviews.facebook.net/D1593\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1301310 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "hive_283", "file": [{"additions": 12, "raw_url": "https://github.com/apache/hive/raw/cc61020d7f4adff7905b8301b7908f3737f483a8/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java", "blob_url": "https://github.com/apache/hive/blob/cc61020d7f4adff7905b8301b7908f3737f483a8/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java", "sha": "6eb51dd41d31d9a0f04f8f4989eaea3080a21366", "changes": 13, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java?ref=cc61020d7f4adff7905b8301b7908f3737f483a8", "patch": "@@ -435,14 +435,16 @@ private void processPaths(JobConf job, CombineFileInputFormatShim combine,\n     List<InputSplitShim> retLists = new ArrayList<InputSplitShim>();\n     Map<String, ArrayList<InputSplitShim>> aliasToSplitList = new HashMap<String, ArrayList<InputSplitShim>>();\n     Map<String, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();\n+    Map<String, ArrayList<String>> pathToAliasesNoScheme = removeScheme(pathToAliases);\n \n     // Populate list of exclusive splits for every sampled alias\n     //\n     for (InputSplitShim split : splits) {\n       String alias = null;\n       for (Path path : split.getPaths()) {\n+        boolean schemeless = path.toUri().getScheme() == null;\n         List<String> l = HiveFileFormatUtils.doGetAliasesFromPath(\n-            pathToAliases, path);\n+            schemeless ? pathToAliasesNoScheme : pathToAliases, path);\n         // a path for a split unqualified the split from being sampled if:\n         // 1. it serves more than one alias\n         // 2. the alias it serves is not sampled\n@@ -500,6 +502,15 @@ private void processPaths(JobConf job, CombineFileInputFormatShim combine,\n     return retLists;\n   }\n \n+  Map<String, ArrayList<String>> removeScheme(Map<String, ArrayList<String>> pathToAliases) {\n+    Map<String, ArrayList<String>> result = new HashMap<String, ArrayList<String>>();\n+    for (Map.Entry <String, ArrayList<String>> entry : pathToAliases.entrySet()) {\n+      String newKey = new Path(entry.getKey()).toUri().getPath();\n+      result.put(newKey, entry.getValue());\n+    }\n+    return result;\n+  }\n+\n   /**\n    * Create a generic Hive RecordReader than can iterate over all chunks in a\n    * CombinedFileSplit.", "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java"}], "repo": "hive"}]
