{
    "crunch_ace392b": {
        "bug_id": "crunch_ace392b",
        "commit": "https://github.com/apache/crunch/commit/ace392bde724273b1d300f7b01c4648e9e1b9060",
        "file": [
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/crunch/blob/ace392bde724273b1d300f7b01c4648e9e1b9060/crunch-core/src/it/java/org/apache/crunch/io/text/csv/CSVFileSourceIT.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/it/java/org/apache/crunch/io/text/csv/CSVFileSourceIT.java?ref=ace392bde724273b1d300f7b01c4648e9e1b9060",
                "deletions": 27,
                "filename": "crunch-core/src/it/java/org/apache/crunch/io/text/csv/CSVFileSourceIT.java",
                "patch": "@@ -17,19 +17,18 @@\n  */\n package org.apache.crunch.io.text.csv;\n \n-import java.io.File;\n-import java.io.FileInputStream;\n+import static org.junit.Assert.assertTrue;\n+\n import java.io.IOException;\n import java.util.Collection;\n-import static org.junit.Assert.*;\n \n import org.apache.crunch.PCollection;\n+import org.apache.crunch.PTable;\n import org.apache.crunch.Pipeline;\n import org.apache.crunch.impl.mr.MRPipeline;\n import org.apache.crunch.test.TemporaryPath;\n import org.apache.crunch.test.TemporaryPaths;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.io.Text;\n import org.junit.Rule;\n import org.junit.Test;\n \n@@ -39,14 +38,30 @@\n \n   @Test\n   public void testVanillaCSV() throws Exception {\n-    String[] expectedFileContents = { \"1,2,3,4\", \"5,6,7,8\", \"9,10,11\", \"12,13,14\" };\n+    final String[] expectedFileContents = { \"1,2,3,4\", \"5,6,7,8\", \"9,10,11\", \"12,13,14\" };\n+\n+    final String vanillaCSVFile = tmpDir.copyResourceFileName(\"vanilla.csv\");\n+    final Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n+    final PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(vanillaCSVFile)));\n+\n+    final Collection<String> csvLinesList = csvLines.asCollection().getValue();\n+\n+    for (int i = 0; i < expectedFileContents.length; i++) {\n+      assertTrue(csvLinesList.contains(expectedFileContents[i]));\n+    }\n+  }\n+\n+  @Test\n+  public void testVanillaCSVWithAdditionalActions() throws Exception {\n+    final String[] expectedFileContents = { \"1,2,3,4\", \"5,6,7,8\", \"9,10,11\", \"12,13,14\" };\n \n-    String vanillaCSVFile = tmpDir.copyResourceFileName(\"vanilla.csv\");\n-    Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n-    PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(vanillaCSVFile)));\n-    pipeline.run();\n+    final String vanillaCSVFile = tmpDir.copyResourceFileName(\"vanilla.csv\");\n+    final Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n+    final PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(vanillaCSVFile)));\n \n-    Collection<String> csvLinesList = csvLines.asCollection().getValue();\n+    final PTable<String, Long> countTable = csvLines.count();\n+    final PCollection<String> csvLines2 = countTable.keys();\n+    final Collection<String> csvLinesList = csvLines2.asCollection().getValue();\n \n     for (int i = 0; i < expectedFileContents.length; i++) {\n       assertTrue(csvLinesList.contains(expectedFileContents[i]));\n@@ -55,16 +70,15 @@ public void testVanillaCSV() throws Exception {\n \n   @Test\n   public void testCSVWithNewlines() throws Exception {\n-    String[] expectedFileContents = {\n+    final String[] expectedFileContents = {\n         \"\\\"Champion, Mac\\\",\\\"1234 Hoth St.\\n\\tApartment 101\\n\\tAtlanta, GA\\n\\t64086\\\",\\\"30\\\",\\\"M\\\",\\\"5/28/2010 12:00:00 AM\\\",\\\"Just some guy\\\"\",\n         \"\\\"Champion, Mac\\\",\\\"5678 Tatooine Rd. Apt 5, Mobile, AL 36608\\\",\\\"30\\\",\\\"M\\\",\\\"Some other date\\\",\\\"short description\\\"\" };\n \n-    String csvWithNewlines = tmpDir.copyResourceFileName(\"withNewlines.csv\");\n-    Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n-    PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(csvWithNewlines)));\n-    pipeline.run();\n+    final String csvWithNewlines = tmpDir.copyResourceFileName(\"withNewlines.csv\");\n+    final Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n+    final PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(csvWithNewlines)));\n \n-    Collection<String> csvLinesList = csvLines.asCollection().getValue();\n+    final Collection<String> csvLinesList = csvLines.asCollection().getValue();\n \n     for (int i = 0; i < expectedFileContents.length; i++) {\n       assertTrue(csvLinesList.contains(expectedFileContents[i]));\n@@ -77,18 +91,17 @@ public void testCSVWithNewlines() throws Exception {\n    */\n   @Test\n   public void testCSVWithCustomQuoteAndNewlines() throws IOException {\n-    String[] expectedFileContents = {\n+    final String[] expectedFileContents = {\n         \"*Champion, Mac*,*1234 Hoth St.\\n\\tApartment 101\\n\\tAtlanta, GA\\n\\t64086*,*30*,*M*,*5/28/2010 12:00:00 AM*,*Just some guy*\",\n         \"*Mac, Champion*,*5678 Tatooine Rd. Apt 5, Mobile, AL 36608*,*30*,*M*,*Some other date*,*short description*\" };\n \n-    String csvWithNewlines = tmpDir.copyResourceFileName(\"customQuoteCharWithNewlines.csv\");\n-    Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n-    PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(csvWithNewlines),\n+    final String csvWithNewlines = tmpDir.copyResourceFileName(\"customQuoteCharWithNewlines.csv\");\n+    final Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n+    final PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(csvWithNewlines),\n         CSVLineReader.DEFAULT_BUFFER_SIZE, CSVLineReader.DEFAULT_INPUT_FILE_ENCODING, '*', '*',\n         CSVLineReader.DEFAULT_ESCAPE_CHARACTER));\n-    pipeline.run();\n \n-    Collection<String> csvLinesList = csvLines.asCollection().getValue();\n+    final Collection<String> csvLinesList = csvLines.asCollection().getValue();\n \n     for (int i = 0; i < expectedFileContents.length; i++) {\n       assertTrue(csvLinesList.contains(expectedFileContents[i]));\n@@ -105,13 +118,12 @@ public void testCSVWithCustomQuoteAndNewlines() throws IOException {\n   public void testBrokenLineParsingInChinese() throws IOException {\n     final String[] expectedChineseLines = { \"\u60a8\u597d\u6211\u53eb\u9a6c\u514b\uff0c\u6211\u4ece\u4e9a\u62c9\u5df4\u9a6c\u5dde\u6765\uff0c\u6211\u662f\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u6211\u4e8c\u5341\u516b\u5c81\", \"\u6211\u6709\u4e00\u4e2a\u5ba0\u7269\uff0c\u5b83\u662f\u4e00\u4e2a\u5c0f\u732b\uff0c\u5b83\u516d\u5c81\uff0c\u5b83\u5f88\u6f02\u4eae\",\n         \"\u6211\u559c\u6b22\u5403\u996d\uff0c\u201c\u6211\u89c9\u5f97\u8fd9\u4e2a\u996d\u6700\u597d\\n\uff0a\u86cb\u7cd5\\n\uff0a\u5305\u5b50\\n\uff0a\u51b0\u6dc7\u6dcb\\n\uff0a\u5564\u9152\u201c\uff0c\u4ed6\u4eec\u90fd\u5f88\u597d\uff0c\u6211\u4e5f\u5f88\u559c\u6b22\u5976\u916a\u4f46\u5b83\u662f\u4e0d\u5065\u5eb7\u7684\", \"\u6211\u662f\u7537\u7684\uff0c\u6211\u7684\u5934\u53d1\u5f88\u77ed\uff0c\u6211\u7a7f\u84dd\u8272\u7684\u88e4\u5b50\uff0c\u201c\u6211\u7a7f\u9ed1\u8272\u7684\u3001\u201c\u8863\u670d\u201d\" };\n-    String chineseLines = tmpDir.copyResourceFileName(\"brokenChineseLines.csv\");\n+    final String chineseLines = tmpDir.copyResourceFileName(\"brokenChineseLines.csv\");\n \n-    Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n-    PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(chineseLines),\n+    final Pipeline pipeline = new MRPipeline(CSVFileSourceIT.class, tmpDir.getDefaultConfiguration());\n+    final PCollection<String> csvLines = pipeline.read(new CSVFileSource(new Path(chineseLines),\n         CSVLineReader.DEFAULT_BUFFER_SIZE, CSVLineReader.DEFAULT_INPUT_FILE_ENCODING, '\u201c', '\u201d', '\u3001'));\n-    pipeline.run();\n-    Collection<String> csvLinesList = csvLines.asCollection().getValue();\n+    final Collection<String> csvLinesList = csvLines.asCollection().getValue();\n     for (int i = 0; i < expectedChineseLines.length; i++) {\n       assertTrue(csvLinesList.contains(expectedChineseLines[i]));\n     }",
                "raw_url": "https://github.com/apache/crunch/raw/ace392bde724273b1d300f7b01c4648e9e1b9060/crunch-core/src/it/java/org/apache/crunch/io/text/csv/CSVFileSourceIT.java",
                "sha": "ba8e19310b5ce007d8d8775aa496e6fab309df8d",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/crunch/blob/ace392bde724273b1d300f7b01c4648e9e1b9060/crunch-core/src/main/java/org/apache/crunch/io/text/csv/CSVInputFormat.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/main/java/org/apache/crunch/io/text/csv/CSVInputFormat.java?ref=ace392bde724273b1d300f7b01c4648e9e1b9060",
                "deletions": 20,
                "filename": "crunch-core/src/main/java/org/apache/crunch/io/text/csv/CSVInputFormat.java",
                "patch": "@@ -21,14 +21,15 @@\n import java.util.ArrayList;\n import java.util.List;\n \n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.LongWritable;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.InputSplit;\n import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.RecordReader;\n@@ -43,12 +44,13 @@\n  * format deals with the fact that CSV files can potentially have multiple lines\n  * within fields which should all be treated as one record.\n  */\n-public class CSVInputFormat extends FileInputFormat<LongWritable, Text> {\n+public class CSVInputFormat extends FileInputFormat<LongWritable, Text> implements Configurable {\n   private int bufferSize;\n   private String inputFileEncoding;\n   private char openQuoteChar;\n   private char closeQuoteChar;\n   private char escapeChar;\n+  private Configuration configuration;\n \n   /**\n    * This method is used by crunch to get an instance of {@link CSVRecordReader}\n@@ -57,10 +59,7 @@\n    *          the {@link InputSplit} that will be assigned to the record reader\n    * @param context\n    *          the {@TaskAttemptContext} for the job\n-   * @return an instance of {@link CSVRecordReader} created using\n-   *         {@link CSVInputFormat#getSeparatorChar()},\n-   *         {@link CSVInputFormat#getQuoteChar()}, and\n-   *         {@link CSVInputFormat#getEscapeChar()}.\n+   * @return an instance of {@link CSVRecordReader} created using configured separator, quote, and escape characters.\n    */\n   @Override\n   public RecordReader<LongWritable, Text> createRecordReader(final InputSplit split, final TaskAttemptContext context) {\n@@ -83,17 +82,18 @@\n     final long splitSize = job.getConfiguration().getLong(\"csv.input.split.size\", 67108864);\n     final List<InputSplit> splits = new ArrayList<InputSplit>();\n     final Path[] paths = FileUtil.stat2Paths(listStatus(job).toArray(new FileStatus[0]));\n-    FileSystem fileSystem = null;\n+    FileSystem fileSystem = FileSystem.get(job.getConfiguration());\n     FSDataInputStream inputStream = null;\n     try {\n       for (final Path path : paths) {\n-        fileSystem = path.getFileSystem(job.getConfiguration());\n         inputStream = fileSystem.open(path);\n         splits.addAll(getSplitsForFile(splitSize, fileSystem.getFileStatus(path).getLen(), path, inputStream));\n       }\n       return splits;\n     } finally {\n-      inputStream.close();\n+      if(inputStream != null) {\n+        inputStream.close();\n+      }\n     }\n   }\n \n@@ -166,44 +166,51 @@\n     return splitsList;\n   }\n \n+  @Override\n+  public Configuration getConf() {\n+    return configuration;\n+  }\n+\n+  @Override\n+  public void setConf(final Configuration conf) {\n+    configuration = conf;\n+    configure();\n+  }\n+\n   /**\n-   * This method will read the configuration that is set in\n+   * This method will read the configuration options that were set in\n    * {@link CSVFileSource}'s private getBundle() method\n-   * \n-   * @param jobConf\n-   *          The {@code JobConf} instance from which the CSV configuration\n-   *          parameters will be read, if necessary.\n    */\n-  public void configure(JobConf jobConf) {\n-    String bufferValue = jobConf.get(CSVFileSource.CSV_BUFFER_SIZE);\n+  public void configure() {\n+    final String bufferValue = this.configuration.get(CSVFileSource.CSV_BUFFER_SIZE);\n     if (\"\".equals(bufferValue)) {\n       bufferSize = CSVLineReader.DEFAULT_BUFFER_SIZE;\n     } else {\n       bufferSize = Integer.parseInt(bufferValue);\n     }\n \n-    String inputFileEncodingValue = jobConf.get(CSVFileSource.CSV_INPUT_FILE_ENCODING);\n+    final String inputFileEncodingValue = this.configuration.get(CSVFileSource.CSV_INPUT_FILE_ENCODING);\n     if (\"\".equals(inputFileEncodingValue)) {\n       inputFileEncoding = CSVLineReader.DEFAULT_INPUT_FILE_ENCODING;\n     } else {\n       inputFileEncoding = inputFileEncodingValue;\n     }\n \n-    String openQuoteCharValue = jobConf.get(CSVFileSource.CSV_OPEN_QUOTE_CHAR);\n+    final String openQuoteCharValue = this.configuration.get(CSVFileSource.CSV_OPEN_QUOTE_CHAR);\n     if (\"\".equals(openQuoteCharValue)) {\n       openQuoteChar = CSVLineReader.DEFAULT_QUOTE_CHARACTER;\n     } else {\n       openQuoteChar = openQuoteCharValue.charAt(0);\n     }\n \n-    String closeQuoteCharValue = jobConf.get(CSVFileSource.CSV_CLOSE_QUOTE_CHAR);\n+    final String closeQuoteCharValue = this.configuration.get(CSVFileSource.CSV_CLOSE_QUOTE_CHAR);\n     if (\"\".equals(closeQuoteCharValue)) {\n       closeQuoteChar = CSVLineReader.DEFAULT_QUOTE_CHARACTER;\n     } else {\n       closeQuoteChar = closeQuoteCharValue.charAt(0);\n     }\n \n-    String escapeCharValue = jobConf.get(CSVFileSource.CSV_ESCAPE_CHAR);\n+    final String escapeCharValue = this.configuration.get(CSVFileSource.CSV_ESCAPE_CHAR);\n     if (\"\".equals(escapeCharValue)) {\n       escapeChar = CSVLineReader.DEFAULT_ESCAPE_CHARACTER;\n     } else {",
                "raw_url": "https://github.com/apache/crunch/raw/ace392bde724273b1d300f7b01c4648e9e1b9060/crunch-core/src/main/java/org/apache/crunch/io/text/csv/CSVInputFormat.java",
                "sha": "867b7047a6ce0bd1b5798da9de64fd389dfefecd",
                "status": "modified"
            }
        ],
        "message": "CRUNCH-429: Fix CSVInputFormat\n\nSigned-off-by: Micah Whitacre <mkwhit@apache.org>\n\nCRUNCH-429: Fixed spelling error, handled potential NPE, and moved FileSystem retrieval outside of for loop.",
        "parent": "https://github.com/apache/crunch/commit/8027f706ae47b613c6f0e96e0ee9f7ef21d1d6ed",
        "repo": "crunch",
        "unit_tests": [
            "CSVInputFormatTest.java"
        ]
    },
    "crunch_d743ce7": {
        "bug_id": "crunch_d743ce7",
        "commit": "https://github.com/apache/crunch/commit/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094",
        "file": [
            {
                "additions": 101,
                "blob_url": "https://github.com/apache/crunch/blob/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/it/java/org/apache/crunch/PTableUnionTest.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch/src/it/java/org/apache/crunch/PTableUnionTest.java?ref=d743ce7c8aa9107c3c73bac51bdb5ec3c761f094",
                "deletions": 0,
                "filename": "crunch/src/it/java/org/apache/crunch/PTableUnionTest.java",
                "patch": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.crunch;\n+\n+import static org.junit.Assert.assertNotNull;\n+\n+import java.io.IOException;\n+\n+import org.apache.crunch.PCollection;\n+import org.apache.crunch.PTable;\n+import org.apache.crunch.fn.IdentityFn;\n+import org.apache.crunch.impl.mr.MRPipeline;\n+import org.apache.crunch.test.TemporaryPath;\n+import org.apache.crunch.test.TemporaryPaths;\n+import org.apache.crunch.types.avro.Avros;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+\n+\n+public class PTableUnionTest {\n+\n+  public static class FirstLetterKeyFn extends DoFn<String, Pair<String, String>> {\n+\n+    private static final long serialVersionUID = 5517897875971194220L;\n+\n+    @Override\n+    public void process(String input, Emitter<Pair<String, String>> emitter) {\n+      if (input.length() > 0) {\n+        emitter.emit(Pair.of(input.substring(0, 1), input));\n+      }\n+    }\n+  }\n+  \n+  @Rule\n+  public TemporaryPath tmpDir = TemporaryPaths.create();\n+  \n+  protected MRPipeline pipeline;\n+\n+  @Before\n+  public void setUp() {\n+    pipeline = new MRPipeline(this.getClass(), tmpDir.getDefaultConfiguration());\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    pipeline.done();\n+  }\n+\n+  @Test\n+  public void tableUnionMaterializeNPE() throws Exception {\n+    PCollection<String> words = pipeline.readTextFile(tmpDir.copyResourceFileName(\"shakes.txt\"));\n+    PCollection<String> lorum = pipeline.readTextFile(tmpDir.copyResourceFileName(\"maugham.txt\"));\n+    lorum.materialize();\n+\n+    PTable<String, String> wordsByFirstLetter =\n+        words.parallelDo(\"byFirstLetter\", new FirstLetterKeyFn(), Avros.tableOf(Avros.strings(), Avros.strings()));\n+    PTable<String, String> lorumByFirstLetter =\n+        lorum.parallelDo(\"byFirstLetter\", new FirstLetterKeyFn(), Avros.tableOf(Avros.strings(), Avros.strings()));\n+\n+    @SuppressWarnings(\"unchecked\")\n+    PTable<String, String> union = wordsByFirstLetter.union(lorumByFirstLetter);\n+\n+    assertNotNull(union.materialize().iterator().next());\n+  }\n+\n+  @Test\n+  public void collectionUnionMaterializeNPE() throws Exception {\n+    PCollection<String> words = pipeline.readTextFile(tmpDir.copyResourceFileName(\"shakes.txt\"));\n+    PCollection<String> lorum = pipeline.readTextFile(tmpDir.copyResourceFileName(\"maugham.txt\"));\n+    lorum.materialize();\n+\n+    IdentityFn<String> identity = IdentityFn.getInstance();\n+    words = words.parallelDo(identity, Avros.strings());\n+    lorum = lorum.parallelDo(identity, Avros.strings());\n+\n+    @SuppressWarnings(\"unchecked\")\n+    PCollection<String> union = words.union(lorum);\n+\n+    union.materialize().iterator();\n+    \n+    assertNotNull(union.materialize().iterator().next());\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/crunch/raw/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/it/java/org/apache/crunch/PTableUnionTest.java",
                "sha": "1d31096ee3b9694aa05dbbfa051455fd04131eb6",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/crunch/blob/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java?ref=d743ce7c8aa9107c3c73bac51bdb5ec3c761f094",
                "deletions": 1,
                "filename": "crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java",
                "patch": "@@ -285,7 +285,7 @@ private void addOutput(PCollectionImpl<?> impl, Target target) {\n    */\n   private <T> PCollectionImpl<T> toPcollectionImpl(PCollection<T> pcollection) {\n     PCollectionImpl<T> pcollectionImpl = null;\n-    if (pcollection instanceof UnionCollection) {\n+    if (pcollection instanceof UnionCollection || pcollection instanceof UnionTable) {\n       pcollectionImpl = (PCollectionImpl<T>) pcollection.parallelDo(\"UnionCollectionWrapper\",\n           (MapFn) IdentityFn.<Object> getInstance(), pcollection.getPType());\n     } else {",
                "raw_url": "https://github.com/apache/crunch/raw/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java",
                "sha": "6ef7491c0804b0549ee09d530beb34163f5d42f9",
                "status": "modified"
            }
        ],
        "message": "CRUNCH-154: Fix NPE on materialized union of two PTables",
        "parent": "https://github.com/apache/crunch/commit/035b1b91d60c1ed5029135d73706ffd54b184a8c",
        "repo": "crunch",
        "unit_tests": [
            "MRPipelineTest.java"
        ]
    },
    "crunch_f57c8fc": {
        "bug_id": "crunch_f57c8fc",
        "commit": "https://github.com/apache/crunch/commit/f57c8fc0fc110e9effb95a622aa54c3817c81869",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 1,
                "filename": "crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java",
                "patch": "@@ -101,7 +101,7 @@ public void testReadWrite() throws Exception {\n         pipeline.run();\n \n         // assert the output was written correctly\n-        try (FSDataInputStream inputStream = dfsCluster2.open(new Path(\"hdfs://cluster2/output/out0-m-00000\"))) {\n+        try (FSDataInputStream inputStream = dfsCluster2.open(new Path(\"hdfs://cluster2/output/part-m-00000\"))) {\n             String readValue = IOUtils.toString(inputStream).trim();\n             Assert.assertEquals(testString, readValue);\n         }",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java",
                "sha": "a4efc8b58435625af0ef7f3834b068724f97966d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 0,
                "filename": "crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java",
                "patch": "@@ -51,6 +51,8 @@\n \n   public static final String FILE_TARGET_MAX_DISTCP_TASKS = \"crunch.file.target.max.distcp.tasks\";\n \n+  public static final String FILE_TARGET_MAX_DISTCP_TASK_BANDWIDTH_MB = \"crunch.file.target.max.distcp.task.bandwidth.mb\";\n+\n   // Not instantiated\n   private RuntimeParameters() {\n   }",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java",
                "sha": "bc15169088c8263081ba6ae794e7830c1a613c5d",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 22,
                "filename": "crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java",
                "patch": "@@ -51,13 +51,16 @@\n import org.apache.crunch.io.SourceTargetHelper;\n import org.apache.crunch.types.Converter;\n import org.apache.crunch.types.PType;\n+import org.apache.crunch.util.CrunchRenameCopyListing;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.mapreduce.Job;\n import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n+import org.apache.hadoop.tools.CopyListing;\n import org.apache.hadoop.tools.DistCp;\n+import org.apache.hadoop.tools.DistCpConstants;\n import org.apache.hadoop.tools.DistCpOptions;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -194,14 +197,17 @@ public void handleOutputs(Configuration conf, Path workingPath, int index) throw\n     Path srcPattern = getSourcePattern(workingPath, index);\n     boolean sameFs = isCompatible(srcFs, path);\n     boolean useDistributedCopy = conf.getBoolean(RuntimeParameters.FILE_TARGET_USE_DISTCP, true);\n-    int maxDistributedCopyTasks = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASKS, 1000);\n+    int maxDistributedCopyTasks = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASKS, 100);\n+    int maxDistributedCopyTaskBandwidthMB = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASK_BANDWIDTH_MB,\n+        DistCpConstants.DEFAULT_BANDWIDTH_MB);\n     int maxThreads = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_THREADS, 1);\n \n     if (!sameFs) {\n       if (useDistributedCopy) {\n         LOG.info(\"Source and destination are in different file systems, performing distributed copy from {} to {}\", srcPattern,\n             path);\n-        handleOutputsDistributedCopy(dstFsConf, srcPattern, srcFs, dstFs, maxDistributedCopyTasks);\n+        handleOutputsDistributedCopy(conf, srcPattern, srcFs, dstFs, maxDistributedCopyTasks,\n+            maxDistributedCopyTaskBandwidthMB);\n       } else {\n         LOG.info(\"Source and destination are in different file systems, performing asynch copies from {} to {}\", srcPattern, path);\n         handleOutputsAsynchronously(conf, srcPattern, srcFs, dstFs, sameFs, maxThreads);\n@@ -210,18 +216,17 @@ public void handleOutputs(Configuration conf, Path workingPath, int index) throw\n       LOG.info(\"Source and destination are in the same file system, performing asynch renames from {} to {}\", srcPattern, path);\n       handleOutputsAsynchronously(conf, srcPattern, srcFs, dstFs, sameFs, maxThreads);\n     }\n-\n   }\n \n   private void handleOutputsAsynchronously(Configuration conf, Path srcPattern, FileSystem srcFs, FileSystem dstFs,\n           boolean sameFs, int maxThreads) throws IOException {\n+    Configuration dstFsConf = getEffectiveBundleConfig(conf);\n     Path[] srcs = FileUtil.stat2Paths(srcFs.globStatus(srcPattern), srcPattern);\n     List<ListenableFuture<Boolean>> renameFutures = Lists.newArrayList();\n     ListeningExecutorService executorService =\n         MoreExecutors.listeningDecorator(\n             Executors.newFixedThreadPool(\n                 maxThreads));\n-    Configuration dstFsConf = getEffectiveBundleConfig(conf);\n     for (Path s : srcs) {\n       Path d = getDestFile(dstFsConf, s, path, s.getName().contains(\"-m-\"));\n       renameFutures.add(\n@@ -255,26 +260,12 @@ private void handleOutputsAsynchronously(Configuration conf, Path srcPattern, Fi\n   }\n \n   private void handleOutputsDistributedCopy(Configuration conf, Path srcPattern, FileSystem srcFs, FileSystem dstFs,\n-          int maxDistributedCopyTasks) throws IOException {\n+          int maxTasks, int maxBandwidthMB) throws IOException {\n+    Configuration dstFsConf = getEffectiveBundleConfig(conf);\n     Path[] srcs = FileUtil.stat2Paths(srcFs.globStatus(srcPattern), srcPattern);\n     if (srcs.length > 0) {\n-      LOG.info(\"Distributed copying {} files using at most {} tasks\", srcs.length, maxDistributedCopyTasks);\n-      // Once https://issues.apache.org/jira/browse/HADOOP-15281 is available, we can use the direct write\n-      // distcp optimization if the target path is in S3\n-      DistCpOptions options = new DistCpOptions(Arrays.asList(srcs), path);\n-      options.setMaxMaps(maxDistributedCopyTasks);\n-      options.setOverwrite(true);\n-      options.setBlocking(true);\n-\n-      Configuration distCpConf = new Configuration(conf);\n-      // Remove unnecessary and problematic properties from the DistCp configuration. This is necessary since\n-      // files referenced by these properties may have already been deleted when the DistCp is being started.\n-      distCpConf.unset(\"mapreduce.job.cache.files\");\n-      distCpConf.unset(\"mapreduce.job.classpath.files\");\n-      distCpConf.unset(\"tmpjars\");\n-\n       try {\n-        DistCp distCp = new DistCp(distCpConf, options);\n+        DistCp distCp = createDistCp(srcs, maxTasks, maxBandwidthMB, dstFsConf);\n         if (!distCp.execute().isSuccessful()) {\n           throw new CrunchRuntimeException(\"Distributed copy failed from \" + srcPattern + \" to \" + path);\n         }\n@@ -329,7 +320,38 @@ protected Path getDestFile(Configuration conf, Path src, Path dir, boolean mapOn\n     }\n     return new Path(dir, outputFilename);\n   }\n-  \n+\n+  protected DistCp createDistCp(Path[] srcs, int maxTasks, int maxBandwidthMB, Configuration conf) throws Exception {\n+    LOG.info(\"Distributed copying {} files using at most {} tasks and bandwidth limit of {} MB/s per task\",\n+        new Object[]{srcs.length, maxTasks, maxBandwidthMB});\n+\n+    Configuration distCpConf = new Configuration(conf);\n+\n+    // Remove unnecessary and problematic properties from the DistCp configuration. This is necessary since\n+    // files referenced by these properties may have already been deleted when the DistCp is being started.\n+    distCpConf.unset(\"mapreduce.job.cache.files\");\n+    distCpConf.unset(\"mapreduce.job.classpath.files\");\n+    distCpConf.unset(\"tmpjars\");\n+\n+    // Setup renaming for part files\n+    List<String> renames = Lists.newArrayList();\n+    for (Path s : srcs) {\n+      Path d = getDestFile(conf, s, path, s.getName().contains(\"-m-\"));\n+      renames.add(s.getName() + \":\" + d.getName());\n+    }\n+    distCpConf.setStrings(CrunchRenameCopyListing.DISTCP_PATH_RENAMES, renames.toArray(new String[renames.size()]));\n+    distCpConf.setClass(DistCpConstants.CONF_LABEL_COPY_LISTING_CLASS, CrunchRenameCopyListing.class, CopyListing.class);\n+\n+    // Once https://issues.apache.org/jira/browse/HADOOP-15281 is available, we can use the direct write\n+    // distcp optimization if the target path is in S3\n+    DistCpOptions options = new DistCpOptions(Arrays.asList(srcs), path);\n+    options.setMaxMaps(maxTasks);\n+    options.setMapBandwidth(maxBandwidthMB);\n+    options.setBlocking(true);\n+\n+    return new DistCp(distCpConf, options);\n+  }\n+\n   /**\n    * Extract the partition number from a raw reducer output filename.\n    *",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java",
                "sha": "ce47bcccc281dcd7a70ff9f6098956e08cc2ac59",
                "status": "modified"
            },
            {
                "additions": 272,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java",
                "changes": 272,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 0,
                "filename": "crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java",
                "patch": "@@ -0,0 +1,272 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the\n+ * Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.  You may obtain a\n+ * copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"\n+ * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language\n+ * governing permissions and limitations under the License.\n+ */\n+package org.apache.crunch.util;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.SequenceFile;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.security.Credentials;\n+import org.apache.hadoop.tools.CopyListing;\n+import org.apache.hadoop.tools.CopyListingFileStatus;\n+import org.apache.hadoop.tools.DistCpOptions;\n+import org.apache.hadoop.tools.DistCpOptions.FileAttribute;\n+import org.apache.hadoop.tools.SimpleCopyListing;\n+import org.apache.hadoop.tools.util.DistCpUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Stack;\n+\n+/**\n+ * A custom {@link CopyListing} implementation capable of dynamically renaming\n+ * the target paths according to a {@link #DISTCP_PATH_RENAMES configured set of values}.\n+ * <p>\n+ * Once https://issues.apache.org/jira/browse/HADOOP-16147 is available, this\n+ * class can be significantly simplified.\n+ * </p>\n+ */\n+public class CrunchRenameCopyListing extends SimpleCopyListing {\n+  /**\n+   * Comma-separated list of original-file:renamed-file path rename pairs.\n+   */\n+  public static final String DISTCP_PATH_RENAMES = \"crunch.distcp.path.renames\";\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CrunchRenameCopyListing.class);\n+  private final Map<String, String> pathRenames;\n+\n+  private long totalPaths = 0;\n+  private long totalBytesToCopy = 0;\n+\n+  /**\n+   * Constructor, to initialize configuration.\n+   *\n+   * @param configuration The input configuration, with which the source/target FileSystems may be accessed.\n+   * @param credentials - Credentials object on which the FS delegation tokens are cached. If null\n+   * delegation token caching is skipped\n+   */\n+  public CrunchRenameCopyListing(Configuration configuration, Credentials credentials) {\n+    super(configuration, credentials);\n+\n+    pathRenames = new HashMap<>();\n+\n+    String[] pathRenameConf = configuration.getStrings(DISTCP_PATH_RENAMES);\n+    if (pathRenameConf == null) {\n+      throw new IllegalArgumentException(\"Missing required configuration: \" + DISTCP_PATH_RENAMES);\n+    }\n+    for (String pathRename : pathRenameConf) {\n+      String[] pathRenameParts = pathRename.split(\":\");\n+      if (pathRenameParts.length != 2) {\n+        throw new IllegalArgumentException(\"Invalid path rename format: \" + pathRename);\n+      }\n+      if (pathRenames.put(pathRenameParts[0], pathRenameParts[1]) != null) {\n+        throw new IllegalArgumentException(\"Invalid duplicate path rename: \" + pathRenameParts[0]);\n+      }\n+    }\n+    LOG.info(\"Loaded {} path rename entries\", pathRenames.size());\n+\n+    // Clear out the rename configuration property, as it is no longer needed\n+    configuration.unset(DISTCP_PATH_RENAMES);\n+  }\n+\n+  @Override\n+  public void doBuildListing(SequenceFile.Writer fileListWriter, DistCpOptions options) throws IOException {\n+    try {\n+      for (Path path : options.getSourcePaths()) {\n+        FileSystem sourceFS = path.getFileSystem(getConf());\n+        final boolean preserveAcls = options.shouldPreserve(FileAttribute.ACL);\n+        final boolean preserveXAttrs = options.shouldPreserve(FileAttribute.XATTR);\n+        final boolean preserveRawXAttrs = options.shouldPreserveRawXattrs();\n+        path = makeQualified(path);\n+\n+        FileStatus rootStatus = sourceFS.getFileStatus(path);\n+        Path sourcePathRoot = computeSourceRootPath(rootStatus, options);\n+\n+        FileStatus[] sourceFiles = sourceFS.listStatus(path);\n+        boolean explore = (sourceFiles != null && sourceFiles.length > 0);\n+        if (!explore || rootStatus.isDirectory()) {\n+          CopyListingFileStatus rootCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, rootStatus, preserveAcls,\n+              preserveXAttrs, preserveRawXAttrs);\n+          writeToFileListingRoot(fileListWriter, rootCopyListingStatus, sourcePathRoot, options);\n+        }\n+        if (explore) {\n+          for (FileStatus sourceStatus : sourceFiles) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Recording source-path: {} for copy.\", sourceStatus.getPath());\n+            }\n+            CopyListingFileStatus sourceCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, sourceStatus,\n+                preserveAcls && sourceStatus.isDirectory(), preserveXAttrs && sourceStatus.isDirectory(),\n+                preserveRawXAttrs && sourceStatus.isDirectory());\n+            writeToFileListing(fileListWriter, sourceCopyListingStatus, sourcePathRoot, options);\n+\n+            if (isDirectoryAndNotEmpty(sourceFS, sourceStatus)) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Traversing non-empty source dir: {}\", sourceStatus.getPath());\n+              }\n+              traverseNonEmptyDirectory(fileListWriter, sourceStatus, sourcePathRoot, options);\n+            }\n+          }\n+        }\n+      }\n+      fileListWriter.close();\n+      fileListWriter = null;\n+    } finally {\n+      if (fileListWriter != null) {\n+        try {\n+          fileListWriter.close();\n+        } catch(IOException e) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Exception in closing {}\", fileListWriter, e);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private Path computeSourceRootPath(FileStatus sourceStatus, DistCpOptions options) throws IOException {\n+    Path target = options.getTargetPath();\n+    FileSystem targetFS = target.getFileSystem(getConf());\n+    final boolean targetPathExists = options.getTargetPathExists();\n+\n+    boolean solitaryFile = options.getSourcePaths().size() == 1 && !sourceStatus.isDirectory();\n+\n+    if (solitaryFile) {\n+      if (targetFS.isFile(target) || !targetPathExists) {\n+        return sourceStatus.getPath();\n+      } else {\n+        return sourceStatus.getPath().getParent();\n+      }\n+    } else {\n+      boolean specialHandling =\n+          (options.getSourcePaths().size() == 1 && !targetPathExists) || options.shouldSyncFolder() || options.shouldOverwrite();\n+\n+      return specialHandling && sourceStatus.isDirectory() ? sourceStatus.getPath() : sourceStatus.getPath().getParent();\n+    }\n+  }\n+\n+  private Path makeQualified(Path path) throws IOException {\n+    final FileSystem fs = path.getFileSystem(getConf());\n+    return path.makeQualified(fs.getUri(), fs.getWorkingDirectory());\n+  }\n+\n+  private static boolean isDirectoryAndNotEmpty(FileSystem fileSystem, FileStatus fileStatus) throws IOException {\n+    return fileStatus.isDirectory() && getChildren(fileSystem, fileStatus).length > 0;\n+  }\n+\n+  private static FileStatus[] getChildren(FileSystem fileSystem, FileStatus parent) throws IOException {\n+    return fileSystem.listStatus(parent.getPath());\n+  }\n+\n+  private void traverseNonEmptyDirectory(SequenceFile.Writer fileListWriter, FileStatus sourceStatus, Path sourcePathRoot,\n+      DistCpOptions options) throws IOException {\n+    FileSystem sourceFS = sourcePathRoot.getFileSystem(getConf());\n+    final boolean preserveAcls = options.shouldPreserve(FileAttribute.ACL);\n+    final boolean preserveXAttrs = options.shouldPreserve(FileAttribute.XATTR);\n+    final boolean preserveRawXattrs = options.shouldPreserveRawXattrs();\n+    Stack<FileStatus> pathStack = new Stack<>();\n+    pathStack.push(sourceStatus);\n+\n+    while (!pathStack.isEmpty()) {\n+      for (FileStatus child : getChildren(sourceFS, pathStack.pop())) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Recording source-path: {} for copy.\", sourceStatus.getPath());\n+        }\n+        CopyListingFileStatus childCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, child,\n+            preserveAcls && child.isDirectory(), preserveXAttrs && child.isDirectory(), preserveRawXattrs && child.isDirectory());\n+        writeToFileListing(fileListWriter, childCopyListingStatus, sourcePathRoot, options);\n+        if (isDirectoryAndNotEmpty(sourceFS, child)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Traversing non-empty source dir: {}\", sourceStatus.getPath());\n+          }\n+          pathStack.push(child);\n+        }\n+      }\n+    }\n+  }\n+\n+  private void writeToFileListingRoot(SequenceFile.Writer fileListWriter, CopyListingFileStatus fileStatus, Path sourcePathRoot,\n+      DistCpOptions options) throws IOException {\n+    boolean syncOrOverwrite = options.shouldSyncFolder() || options.shouldOverwrite();\n+    if (fileStatus.getPath().equals(sourcePathRoot) && fileStatus.isDirectory() && syncOrOverwrite) {\n+      // Skip the root-paths when syncOrOverwrite\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip {}\", fileStatus.getPath());\n+      }\n+      return;\n+    }\n+    writeToFileListing(fileListWriter, fileStatus, sourcePathRoot, options);\n+  }\n+\n+  private void writeToFileListing(SequenceFile.Writer fileListWriter, CopyListingFileStatus fileStatus, Path sourcePathRoot,\n+      DistCpOptions options) throws IOException {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"REL PATH: {}, FULL PATH: {}\",\n+          DistCpUtils.getRelativePath(sourcePathRoot, fileStatus.getPath()), fileStatus.getPath());\n+    }\n+\n+    if (!shouldCopy(fileStatus.getPath(), options)) {\n+      return;\n+    }\n+\n+    fileListWriter.append(getFileListingKey(sourcePathRoot, fileStatus),\n+        getFileListingValue(fileStatus));\n+    fileListWriter.sync();\n+\n+    if (!fileStatus.isDirectory()) {\n+      totalBytesToCopy += fileStatus.getLen();\n+    }\n+    totalPaths++;\n+  }\n+\n+  /**\n+   * Returns the key for an entry in the copy listing sequence file\n+   * @param sourcePathRoot the root source path for determining the relative target path\n+   * @param fileStatus the copy listing file status\n+   * @return the key for the sequence file entry\n+   */\n+  protected Text getFileListingKey(Path sourcePathRoot, CopyListingFileStatus fileStatus) {\n+    Path fileStatusPath = fileStatus.getPath();\n+    String pathName = fileStatusPath.getName();\n+    String renamedPathName = pathRenames.get(pathName);\n+\n+    if (renamedPathName != null && !pathName.equals(renamedPathName)) {\n+      LOG.info(\"Applying dynamic rename of {} to {}\", pathName, renamedPathName);\n+      fileStatusPath = new Path(fileStatusPath.getParent(), renamedPathName);\n+    }\n+    return new Text(DistCpUtils.getRelativePath(sourcePathRoot, fileStatusPath));\n+  }\n+\n+  /**\n+   * Returns the value for an entry in the copy listing sequence file\n+   * @param fileStatus the copy listing file status\n+   * @return the value for the sequence file entry\n+   */\n+  protected CopyListingFileStatus getFileListingValue(CopyListingFileStatus fileStatus) {\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  protected long getBytesToCopy() {\n+    return totalBytesToCopy;\n+  }\n+\n+  @Override\n+  protected long getNumberOfPaths() {\n+    return totalPaths;\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java",
                "sha": "b930bebc2baccda250a3c5844ad71c1772bd75ae",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 71,
                "filename": "crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java",
                "patch": "@@ -17,35 +17,22 @@\n  */\n package org.apache.crunch.io.hbase;\n \n-import org.apache.crunch.CrunchRuntimeException;\n-import org.apache.crunch.impl.mr.run.RuntimeParameters;\n import org.apache.crunch.io.SequentialFileNamingScheme;\n import org.apache.crunch.io.impl.FileTargetImpl;\n import org.apache.crunch.types.Converter;\n import org.apache.crunch.types.PTableType;\n import org.apache.crunch.types.PType;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.Cell;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n import org.apache.hadoop.hbase.mapreduce.KeyValueSerialization;\n import org.apache.hadoop.mapreduce.Job;\n-import org.apache.hadoop.tools.DistCp;\n-import org.apache.hadoop.tools.DistCpOptions;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import java.io.IOException;\n-import java.util.Arrays;\n \n public class HFileTarget extends FileTargetImpl {\n \n-  private static final Logger LOG = LoggerFactory.getLogger(HFileTarget.class);\n-\n   public HFileTarget(String path) {\n     this(new Path(path));\n   }\n@@ -90,64 +77,6 @@ public void configureForMapReduce(Job job, PType<?> ptype, Path outputPath, Stri\n     return new HBaseValueConverter<Cell>(Cell.class);\n   }\n \n-  @Override\n-  public void handleOutputs(Configuration conf, Path workingPath, int index) throws IOException {\n-    FileSystem srcFs = workingPath.getFileSystem(conf);\n-    Path src = getSourcePattern(workingPath, index);\n-    Path[] srcs = FileUtil.stat2Paths(srcFs.globStatus(src), src);\n-    FileSystem dstFs = path.getFileSystem(conf);\n-    if (!dstFs.exists(path)) {\n-      dstFs.mkdirs(path);\n-    }\n-    boolean sameFs = isCompatible(srcFs, path);\n-\n-    if (!sameFs) {\n-      if (srcs.length > 0) {\n-        int maxDistributedCopyTasks = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASKS, 1000);\n-        LOG.info(\n-                \"Source and destination are in different file systems, performing distcp of {} files from [{}] to [{}] \"\n-                        + \"using at most {} tasks\",\n-                new Object[] { srcs.length, src, path, maxDistributedCopyTasks });\n-        // Once https://issues.apache.org/jira/browse/HADOOP-15281 is available, we can use the direct write\n-        // distcp optimization if the target path is in S3\n-        DistCpOptions options = new DistCpOptions(Arrays.asList(srcs), path);\n-        options.setMaxMaps(maxDistributedCopyTasks);\n-        options.setOverwrite(true);\n-        options.setBlocking(true);\n-\n-        Configuration distCpConf = new Configuration(conf);\n-        // Remove unnecessary and problematic properties from the DistCp configuration. This is necessary since\n-        // files referenced by these properties may have already been deleted when the DistCp is being started.\n-        distCpConf.unset(\"mapreduce.job.cache.files\");\n-        distCpConf.unset(\"mapreduce.job.classpath.files\");\n-        distCpConf.unset(\"tmpjars\");\n-\n-        try {\n-          DistCp distCp = new DistCp(distCpConf, options);\n-          if (!distCp.execute().isSuccessful()) {\n-            throw new CrunchRuntimeException(\"Unable to move files through distcp from \" + src + \" to \" + path);\n-          }\n-          LOG.info(\"Distributed copy completed for {} files\", srcs.length);\n-        } catch (Exception e) {\n-          throw new CrunchRuntimeException(\"Unable to move files through distcp from \" + src + \" to \" + path, e);\n-        }\n-      } else {\n-        LOG.info(\"No files found at [{}], not attempting to copy HFiles\", src);\n-      }\n-    } else {\n-      LOG.info(\n-              \"Source and destination are in the same file system, performing rename of {} files from [{}] to [{}]\",\n-              new Object[] { srcs.length, src, path });\n-\n-      for (Path s : srcs) {\n-        Path d = getDestFile(conf, s, path, s.getName().contains(\"-m-\"));\n-        srcFs.rename(s, d);\n-      }\n-    }\n-    dstFs.create(getSuccessIndicator(), true).close();\n-    LOG.info(\"Created success indicator file\");\n-  }\n-\n   @Override\n   public String toString() {\n     return \"HFile(\" + path + \")\";",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java",
                "sha": "b1ce5ba34dcf7ef9d8ec5d472f7195ff750c7436",
                "status": "modified"
            }
        ],
        "message": "CRUNCH-679: Improvements for usage of DistCp (#20)\n\n* CRUNCH-679: Improvements for usage of DistCp\r\n\r\n* CRUNCH-679: Fix NPE bug by preserving IOUtils.cleanup logic\r\n\r\n* CRUNCH-679: CrunchRenameCopyListing's constructor needs to be public\r\n\r\n* CRUNCH-679: Unset rename configuration after loading into copy listing\r\n\r\n* CRUNCH-679: Reduce default max distcp map tasks from 1000 to 100\r\n\r\n* CRUNCH-679: Update log message formatting",
        "parent": "https://github.com/apache/crunch/commit/587e2c9eaaec8ae9a26e6b1b5c99be7f4d521951",
        "repo": "crunch",
        "unit_tests": [
            "HFileTargetTest.java"
        ]
    }
}