{
    "beam_0d500ef": {
        "bug_id": "beam_0d500ef",
        "commit": "https://github.com/apache/beam/commit/0d500efe3b2d4248e2889dfcf36423aa473ee0a5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/0d500efe3b2d4248e2889dfcf36423aa473ee0a5/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java?ref=0d500efe3b2d4248e2889dfcf36423aa473ee0a5",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import static com.google.common.base.Preconditions.checkNotNull;\n \n+import com.google.common.collect.Iterables;\n import com.google.common.collect.Sets;\n import java.io.IOException;\n import java.util.Collection;\n@@ -511,7 +512,7 @@ public DoFnExtraContextFactory(Collection<? extends BoundedWindow> windows, Pane\n \n     @Override\n     public BoundedWindow window() {\n-      return null;\n+      return Iterables.getOnlyElement(windows);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/0d500efe3b2d4248e2889dfcf36423aa473ee0a5/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "sha": "0360bc2758a69b0ae841b84d75a13f0a34d69331",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in SimpleDoFnRunner",
        "parent": "https://github.com/apache/beam/commit/b2350417f73ae6c34f849ff0e93d5bd93df3088d",
        "repo": "beam",
        "unit_tests": [
            "SimpleDoFnRunnerTest.java"
        ]
    },
    "beam_3682d0c": {
        "bug_id": "beam_3682d0c",
        "commit": "https://github.com/apache/beam/commit/3682d0c5fa8ad3970ebcbd4a89b399491af7994a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/3682d0c5fa8ad3970ebcbd4a89b399491af7994a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java?ref=3682d0c5fa8ad3970ebcbd4a89b399491af7994a",
                "deletions": 2,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "patch": "@@ -29,6 +29,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.stream.Collectors;\n@@ -434,9 +435,9 @@ public void open() throws Exception {\n   @Override\n   public void dispose() throws Exception {\n     try {\n-      checkFinishBundleTimer.cancel(true);\n+      Optional.ofNullable(checkFinishBundleTimer).ifPresent(timer -> timer.cancel(true));\n       FlinkClassloading.deleteStaticCaches();\n-      doFnInvoker.invokeTeardown();\n+      Optional.ofNullable(doFnInvoker).ifPresent(DoFnInvoker::invokeTeardown);\n     } finally {\n       // This releases all task's resources. We need to call this last\n       // to ensure that state, timers, or output buffers can still be",
                "raw_url": "https://github.com/apache/beam/raw/3682d0c5fa8ad3970ebcbd4a89b399491af7994a/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "sha": "aa1f18025faa78955e4a57352e6f1926024b4849",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #8327: [BEAM-7091] Fix NPE in DoFnOperator#dispose",
        "parent": "https://github.com/apache/beam/commit/9edd37c8c83731e0158782faae6697b1bbc594c5",
        "repo": "beam",
        "unit_tests": [
            "DoFnOperatorTest.java"
        ]
    },
    "beam_4078c22": {
        "bug_id": "beam_4078c22",
        "commit": "https://github.com/apache/beam/commit/4078c22fde9501bc28a5119b6f59522261776106",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.github/PULL_REQUEST_TEMPLATE.md",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.github/PULL_REQUEST_TEMPLATE.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": ".github/PULL_REQUEST_TEMPLATE.md",
                "patch": "@@ -8,6 +8,6 @@ quickly and easily:\n  - [ ] Replace `<Jira issue #>` in the title with the actual Jira issue\n        number, if there is one.\n  - [ ] If this contribution is large, please file an Apache\n-       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).\n+       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.pdf).\n \n ---",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.github/PULL_REQUEST_TEMPLATE.md",
                "sha": "9bbc9f73770d5209adda11fc47ce59f2c43b0518",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.gitignore",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.gitignore?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".gitignore",
                "patch": "@@ -19,6 +19,9 @@ build/\n dist/\n distribute-*\n env/\n+sdks/python/**/*.c\n+sdks/python/**/*.so\n+sdks/python/**/*.egg\n \n # Ignore IntelliJ files.\n .idea/",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.gitignore",
                "sha": "69946a9224185b25fb499a164fedde9912ac474e",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/common_job_properties.groovy",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/common_job_properties.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": ".test-infra/jenkins/common_job_properties.groovy",
                "patch": "@@ -69,6 +69,7 @@ class common_job_properties {\n         branch('${sha1}')\n         extensions {\n           cleanAfterCheckout()\n+          pruneBranches()\n         }\n       }\n     }\n@@ -205,7 +206,8 @@ class common_job_properties {\n   static void setPostCommit(context,\n                             String buildSchedule = '0 */6 * * *',\n                             boolean triggerEveryPush = true,\n-                            String notifyAddress = 'commits@beam.apache.org') {\n+                            String notifyAddress = 'commits@beam.apache.org',\n+                            boolean emailIndividuals = true) {\n     // Set build triggers\n     context.triggers {\n       // By default runs every 6 hours.\n@@ -217,7 +219,43 @@ class common_job_properties {\n \n     context.publishers {\n       // Notify an email address for each failed build (defaults to commits@).\n-      mailer(notifyAddress, false, true)\n+      mailer(notifyAddress, false, emailIndividuals)\n+    }\n+  }\n+\n+  // Configures the argument list for performance tests, adding the standard\n+  // performance test job arguments.\n+  private static def genPerformanceArgs(def argMap) {\n+    def standard_args = [\n+      project: 'apache-beam-testing',\n+      dpb_log_level: 'INFO',\n+      maven_binary: '/home/jenkins/tools/maven/latest/bin/mvn',\n+      bigquery_table: 'beam_performance.pkb_results',\n+      // Publishes results with official tag, for use in dashboards.\n+      official: 'true'\n+    ]\n+    // Note: in case of key collision, keys present in ArgMap win.\n+    def joined_args = standard_args.plus(argMap)\n+    def argList = []\n+    joined_args.each({\n+        // FYI: Replacement only works with double quotes.\n+        key, value -> argList.add(\"--$key=$value\")\n+    })\n+    return argList.join(' ')\n+  }\n+\n+  // Adds the standard performance test job steps.\n+  static def buildPerformanceTest(def context, def argMap) {\n+    def pkbArgs = genPerformanceArgs(argMap)\n+    context.steps {\n+        // Clean up environment.\n+        shell('rm -rf PerfKitBenchmarker')\n+        // Clone appropriate perfkit branch\n+        shell('git clone https://github.com/GoogleCloudPlatform/PerfKitBenchmarker.git')\n+        // Install job requirements.\n+        shell('pip install --user -r PerfKitBenchmarker/requirements.txt')\n+        // Launch performance test.\n+        shell(\"python PerfKitBenchmarker/pkb.py $pkbArgs\")\n     }\n   }\n }",
                "previous_filename": ".jenkins/common_job_properties.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/common_job_properties.groovy",
                "sha": "ee102812d413f1dec76ddf7ff0c911f163c70194",
                "status": "renamed"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy",
                "patch": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import common_job_properties\n+\n+// This job runs the Beam performance tests on PerfKit Benchmarker.\n+job('beam_PerformanceTests_Dataflow'){\n+    // Set default Beam job properties.\n+    common_job_properties.setTopLevelMainJobProperties(delegate)\n+\n+    // Run job in postcommit every 6 hours, don't trigger every push, and\n+    // don't email individual committers.\n+    common_job_properties.setPostCommit(\n+        delegate,\n+        '0 */6 * * *',\n+        false,\n+        'commits@beam.apache.org',\n+        false)\n+\n+    def argMap = [\n+      benchmarks: 'dpb_wordcount_benchmark',\n+      dpb_dataflow_staging_location: 'gs://temp-storage-for-perf-tests/staging',\n+      dpb_wordcount_input: 'dataflow-samples/shakespeare/kinglear.txt',\n+      config_override: 'dpb_wordcount_benchmark.dpb_service.service_type=dataflow'\n+    ]\n+\n+    common_job_properties.buildPerformanceTest(delegate, argMap)\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Dataflow.groovy",
                "sha": "51c73f34d59e0b0adca7855f5b2f6d6318d54b46",
                "status": "added"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy",
                "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import common_job_properties\n+\n+// This job runs the Beam performance tests on PerfKit Benchmarker.\n+job('beam_PerformanceTests_JDBC'){\n+    // Set default Beam job properties.\n+    common_job_properties.setTopLevelMainJobProperties(delegate)\n+\n+    // Run job in postcommit every 6 hours, don't trigger every push, and\n+    // don't email individual committers.\n+    common_job_properties.setPostCommit(\n+        delegate,\n+        '0 */6 * * *',\n+        false,\n+        'commits@beam.apache.org',\n+        false)\n+\n+    def pipelineArgs = [\n+        tempRoot: 'gs://temp-storage-for-end-to-end-tests',\n+        project: 'apache-beam-testing',\n+        postgresServerName: '10.36.0.11',\n+        postgresUsername: 'postgres',\n+        postgresDatabaseName: 'postgres',\n+        postgresPassword: 'uuinkks',\n+        postgresSsl: 'false'\n+    ]\n+    def pipelineArgList = []\n+    pipelineArgs.each({\n+        key, value -> pipelineArgList.add(\"--$key=$value\")\n+    })\n+    def pipelineArgsJoined = pipelineArgList.join(',')\n+\n+    def argMap = [\n+      benchmarks: 'beam_integration_benchmark',\n+      beam_it_module: 'sdks/java/io/jdbc',\n+      beam_it_args: pipelineArgsJoined,\n+      beam_it_class: 'org.apache.beam.sdk.io.jdbc.JdbcIOIT',\n+      // Profile is located in $BEAM_ROOT/sdks/java/io/pom.xml.\n+      beam_it_profile: 'io-it'\n+    ]\n+\n+    common_job_properties.buildPerformanceTest(delegate, argMap)\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_JDBC.groovy",
                "sha": "8e581c2a4a7cf70d11888d8418064f89e43e6988",
                "status": "added"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy",
                "patch": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import common_job_properties\n+\n+// This job runs the Beam performance tests on PerfKit Benchmarker.\n+job('beam_PerformanceTests_Spark'){\n+    // Set default Beam job properties.\n+    common_job_properties.setTopLevelMainJobProperties(delegate)\n+\n+    // Run job in postcommit every 6 hours, don't trigger every push, and\n+    // don't email individual committers.\n+    common_job_properties.setPostCommit(\n+        delegate,\n+        '0 */6 * * *',\n+        false,\n+        'commits@beam.apache.org',\n+        false)\n+\n+    def argMap = [\n+      benchmarks: 'dpb_wordcount_benchmark',\n+      // There are currently problems uploading to Dataproc, so we use a file\n+      // already present on the machines as input.\n+      dpb_wordcount_input: '/etc/hosts',\n+      config_override: 'dpb_wordcount_benchmark.dpb_service.service_type=dataproc'\n+    ]\n+\n+    common_job_properties.buildPerformanceTest(delegate, argMap)\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PerformanceTests_Spark.groovy",
                "sha": "ba719bfa5ba62eff84ae798bbcaadb486f61c66c",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_MavenInstall.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_MavenInstall.groovy",
                "sha": "a288a8448211345f4569e959b8712e199f2eb841",
                "status": "renamed"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy",
                "patch": "@@ -18,9 +18,10 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Apex runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Apex') {\n-  description('Runs the RunnableOnService suite on the Apex runner.')\n+// This job runs the suite of ValidatesRunner tests against the Apex runner.\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Apex') {\n+  description('Runs the ValidatesRunner suite on the Apex runner.')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Apex')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate)\n@@ -34,14 +35,14 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Apex') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Apex Runner RunnableOnService Tests',\n-    'Run Apex RunnableOnService')\n+    'Apache Apex Runner ValidatesRunner Tests',\n+    'Run Apex ValidatesRunner')\n \n   // Maven goals for this job.\n   goals('''clean verify --projects runners/apex \\\n       --also-make \\\n       --batch-mode \\\n       --errors \\\n-      --activate-profiles runnable-on-service-tests \\\n-      --activate-profiles local-runnable-on-service-tests''')\n+      --activate-profiles validates-runner-tests \\\n+      --activate-profiles local-validates-runner-tests''')\n }",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Apex.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Apex.groovy",
                "sha": "c16a1e2f9d00d86ad344096770b434dd76c068f0",
                "status": "renamed"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy",
                "patch": "@@ -18,12 +18,12 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Dataflow\n+// This job runs the suite of ValidatesRunner tests against the Dataflow\n // runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Dataflow') {\n-  description('Runs the RunnableOnService suite on the Dataflow runner.')\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Dataflow') {\n+  description('Runs the ValidatesRunner suite on the Dataflow runner.')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Dataflow')\n \n-  previousNames('beam_PostCommit_RunnableOnService_GoogleCloudDataflow')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate, 'master', 120)\n@@ -37,9 +37,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Dataflow') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Google Cloud Dataflow Runner RunnableOnService Tests',\n-    'Run Dataflow RunnableOnService')\n+    'Google Cloud Dataflow Runner ValidatesRunner Tests',\n+    'Run Dataflow ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/google-cloud-dataflow-java -DforkCount=0 -DrunnableOnServicePipelineOptions=\\'[ \"--runner=org.apache.beam.runners.dataflow.testing.TestDataflowRunner\", \"--project=apache-beam-testing\", \"--tempRoot=gs://temp-storage-for-runnable-on-service-tests/\" ]\\'')\n+  goals('-B -e clean verify -am -pl runners/google-cloud-dataflow-java -DforkCount=0 -DvalidatesRunnerPipelineOptions=\\'[ \"--runner=org.apache.beam.runners.dataflow.testing.TestDataflowRunner\", \"--project=apache-beam-testing\", \"--tempRoot=gs://temp-storage-for-validates-runner-tests/\" ]\\'')\n }",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Dataflow.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Dataflow.groovy",
                "sha": "33235ff833c2c9267638c6d545761fc99e4785eb",
                "status": "renamed"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 8,
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy",
                "patch": "@@ -18,11 +18,10 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Flink runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Flink') {\n-  description('Runs the RunnableOnService suite on the Flink runner.')\n-\n-  previousNames('beam_PostCommit_RunnableOnService_FlinkLocal')\n+// This job runs the suite of ValidatesRunner tests against the Flink runner.\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Flink') {\n+  description('Runs the ValidatesRunner suite on the Flink runner.')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Flink')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate)\n@@ -36,9 +35,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Flink') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Flink Runner RunnableOnService Tests',\n-    'Run Flink RunnableOnService')\n+    'Apache Flink Runner ValidatesRunner Tests',\n+    'Run Flink ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/flink/runner -Plocal-runnable-on-service-tests -Prunnable-on-service-tests')\n+  goals('-B -e clean verify -am -pl runners/flink -Plocal-validates-runner-tests -Pvalidates-runner-tests')\n }",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Flink.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Flink.groovy",
                "sha": "5b228bc9cb64cf2d34c86f13933f122b02c8134c",
                "status": "renamed"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy",
                "patch": "@@ -18,12 +18,12 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Gearpump\n+// This job runs the suite of ValidatesRunner tests against the Gearpump\n // runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Gearpump') {\n-  description('Runs the RunnableOnService suite on the Gearpump runner.')\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Gearpump') {\n+  description('Runs the ValidatesRunner suite on the Gearpump runner.')\n \n-  previousNames('beam_PostCommit_RunnableOnService_GearpumpLocal')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Gearpump')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(\n@@ -41,9 +41,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Gearpump') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Gearpump Runner RunnableOnService Tests',\n-    'Run Gearpump RunnableOnService')\n+    'Apache Gearpump Runner ValidatesRunner Tests',\n+    'Run Gearpump ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/gearpump  -Plocal-runnable-on-service-tests -Prunnable-on-service-tests')\n+  goals('-B -e clean verify -am -pl runners/gearpump -DforkCount=0 -DvalidatesRunnerPipelineOptions=\\'[ \"--runner=TestGearpumpRunner\"]\\'')\n }",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Gearpump.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Gearpump.groovy",
                "sha": "e1cbafe6e4b4af167c3b1e2372247e056bbe88ac",
                "status": "renamed"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy",
                "patch": "@@ -18,11 +18,11 @@\n \n import common_job_properties\n \n-// This job runs the suite of RunnableOnService tests against the Spark runner.\n-mavenJob('beam_PostCommit_Java_RunnableOnService_Spark') {\n-  description('Runs the RunnableOnService suite on the Spark runner.')\n+// This job runs the suite of ValidatesRunner tests against the Spark runner.\n+mavenJob('beam_PostCommit_Java_ValidatesRunner_Spark') {\n+  description('Runs the ValidatesRunner suite on the Spark runner.')\n \n-  previousNames('beam_PostCommit_RunnableOnService_SparkLocal')\n+  previousNames('beam_PostCommit_Java_RunnableOnService_Spark')\n \n   // Set common parameters.\n   common_job_properties.setTopLevelMainJobProperties(delegate)\n@@ -36,9 +36,9 @@ mavenJob('beam_PostCommit_Java_RunnableOnService_Spark') {\n   // Allows triggering this build against pull requests.\n   common_job_properties.enablePhraseTriggeringFromPullRequest(\n     delegate,\n-    'Apache Spark Runner RunnableOnService Tests',\n-    'Run Spark RunnableOnService')\n+    'Apache Spark Runner ValidatesRunner Tests',\n+    'Run Spark ValidatesRunner')\n \n   // Maven goals for this job.\n-  goals('-B -e clean verify -am -pl runners/spark -Prunnable-on-service-tests -Plocal-runnable-on-service-tests -Dspark.ui.enabled=false')\n+  goals('-B -e clean verify -am -pl runners/spark -Pvalidates-runner-tests -Plocal-validates-runner-tests -Dspark.ui.enabled=false')\n }",
                "previous_filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Spark.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Java_ValidatesRunner_Spark.groovy",
                "sha": "9fbc219a910e5976f4bb83d3e5284cc9e43c5cf4",
                "status": "renamed"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "patch": "@@ -36,6 +36,18 @@ job('beam_PostCommit_Python_Verify') {\n     'Python SDK PostCommit Tests',\n     'Run Python PostCommit')\n \n+  // Allow the test to only run on particular nodes\n+  // TODO(BEAM-1817): Remove once the tests can run on all nodes\n+  parameters {\n+      nodeParam('TEST_HOST') {\n+          description('select test host as either beam1, 2 or 3')\n+          defaultNodes(['beam3'])\n+          allowedNodes(['beam1', 'beam2', 'beam3'])\n+          trigger('multiSelectionDisallowed')\n+          eligibility('IgnoreOfflineNodeEligibility')\n+      }\n+  }\n+\n   // Execute shell command to test Python SDK.\n   steps {\n     shell('bash sdks/python/run_postcommit.sh')",
                "previous_filename": ".jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PostCommit_Python_Verify.groovy",
                "sha": "28cf77e6963d941d645d08daf15497ae95a9e16d",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy",
                "previous_filename": ".jenkins/job_beam_PreCommit_Java_MavenInstall.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Java_MavenInstall.groovy",
                "sha": "371855159984d7843857dda6541af2a6a8d8f8ff",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy",
                "previous_filename": ".jenkins/job_beam_PreCommit_Website_Stage.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Stage.groovy",
                "sha": "7c64f1119bb522dcd41fdd8e3bb90d6ee153b419",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy",
                "previous_filename": ".jenkins/job_beam_PreCommit_Website_Test.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_PreCommit_Website_Test.groovy",
                "sha": "421b58a804e8b0be8e0654bba0a5d278641a9816",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy",
                "previous_filename": ".jenkins/job_beam_Release_NightlySnapshot.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_beam_Release_NightlySnapshot.groovy",
                "sha": "f2c3ff0740d53c18116b9a7e7511256af531c337",
                "status": "renamed"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_seed.groovy",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/jenkins/job_seed.groovy?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": ".test-infra/jenkins/job_seed.groovy",
                "patch": "@@ -44,7 +44,7 @@ job('beam_SeedJob') {\n   steps {\n     dsl {\n       // A list or a glob of other groovy files to process.\n-      external('.jenkins/job_*.groovy')\n+      external('.test-infra/jenkins/job_*.groovy')\n \n       // If a job is removed from the script, disable it (rather than deleting).\n       removeAction('DISABLE')",
                "previous_filename": ".jenkins/job_seed.groovy",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/jenkins/job_seed.groovy",
                "sha": "2d1b07c7cd2c1cc54c4ca6bdf982865cbcac29f2",
                "status": "renamed"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 13,
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml",
                "patch": "@@ -13,20 +13,16 @@\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n \n+# Cassandra external service which is exposed as a load balancer.\n apiVersion: v1\n-kind: Pod\n+kind: Service\n metadata:\n-  name: postgres-no-pv\n   labels:\n-    name: postgres-no-pv\n+    app: cassandra\n+  name: cassandra-external\n spec:\n-  containers:\n-    - name: postgres\n-      image: postgres\n-      env:\n-        - name: POSTGRES_PASSWORD\n-          value: uuinkks\n-        - name: PGDATA\n-          value: /var/lib/postgresql/data/pgdata\n-      ports:\n-        - containerPort: 5432\n+  ports:\n+    - port: 9042\n+  selector:\n+    app: cassandra\n+  type: LoadBalancer",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/postgres-pod-no-vol.yml",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-service-for-local-dev.yaml",
                "sha": "dd0da93e10cb8e4be787fff811856cc0a82b2cd1",
                "status": "renamed"
            },
            {
                "additions": 114,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml",
                "patch": "@@ -0,0 +1,114 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# Kubernetes service for cassandra\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    app: cassandra\n+  name: cassandra\n+spec:\n+  clusterIP: None\n+  ports:\n+    - port: 9042\n+  selector:\n+    app: cassandra\n+  type: NodePort\n+---\n+# Kubernetes statefulset to set up cassandra multinode cluster\n+apiVersion: \"apps/v1beta1\"\n+kind: StatefulSet\n+metadata:\n+  name: cassandra\n+spec:\n+  serviceName: cassandra\n+  replicas: 3\n+  template:\n+    metadata:\n+      labels:\n+        app: cassandra\n+    spec:\n+      containers:\n+      - name: cassandra\n+# Tag v1.2 of cassandra image loads 3.10 version of Cassandra\n+        image: quay.io/vorstella/cassandra-k8s:v1.2\n+        imagePullPolicy: Always\n+        ports:\n+        - containerPort: 7000\n+          name: intra-node\n+        - containerPort: 7001\n+          name: tls-intra-node\n+        - containerPort: 7199\n+          name: jmx\n+        - containerPort: 9042\n+          name: cql\n+        securityContext:\n+          capabilities:\n+            add:\n+              - IPC_LOCK\n+        lifecycle:\n+          preStop:\n+            exec:\n+              command: [\"/bin/sh\", \"-c\", \"PID=$(pidof java) && kill $PID && while ps -p $PID > /dev/null; do sleep 1; done\"]\n+        env:\n+          - name: MAX_HEAP_SIZE\n+            value: 512M\n+          - name: HEAP_NEWSIZE\n+            value: 100M\n+          - name: CASSANDRA_SEEDS\n+            value: \"cassandra-0.cassandra.default.svc.cluster.local\"\n+          - name: CASSANDRA_CLUSTER_NAME\n+            value: \"K8Demo\"\n+          - name: CASSANDRA_DC\n+            value: \"DC1-K8Demo\"\n+          - name: CASSANDRA_RACK\n+            value: \"Rack1-K8Demo\"\n+          - name: CASSANDRA_AUTO_BOOTSTRAP\n+            value: \"false\"\n+          - name: POD_IP\n+            valueFrom:\n+              fieldRef:\n+                fieldPath: status.podIP\n+          - name: POD_NAMESPACE\n+            valueFrom:\n+              fieldRef:\n+                fieldPath: metadata.namespace\n+        readinessProbe:\n+          exec:\n+            command:\n+            - /bin/bash\n+            - -c\n+            - /ready-probe.sh\n+          initialDelaySeconds: 15\n+          timeoutSeconds: 5\n+        # These volume mounts are persistent. They are like inline claims,\n+        # but not exactly because the names need to match exactly one of\n+        # the stateful pod volumes.\n+        volumeMounts:\n+        - name: cassandra-data\n+          mountPath: /cassandra_data\n+  # These are converted to volume claims by the controller\n+  # and mounted at the paths mentioned above.\n+  volumeClaimTemplates:\n+  - metadata:\n+      name: cassandra-data\n+      annotations:\n+        volume.alpha.kubernetes.io/storage-class: anything\n+    spec:\n+      accessModes: [ \"ReadWriteOnce\" ]\n+      resources:\n+        requests:\n+          storage: 30Gi",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-statefulset.yaml",
                "sha": "f2ff571c89ebbf69cdf9f9672f18dde0831ba4cd",
                "status": "added"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml",
                "patch": "@@ -0,0 +1,74 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Temporary cassandra single node cluster set up \n+# to connect to production cluster through cqlsh remotely. \n+# Headless service that allows us to get the IP addresses of our Cassandra nodes\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    name: cassandra-temp\n+  name: cassandra-temp\n+spec:\n+  clusterIP: None\n+  ports:\n+    - port: 7000\n+      name: intra-node-communication\n+    - port: 7001\n+      name: tls-intra-node-communication\n+    - port: 9042\n+      name: cql\n+  selector:\n+    name: cassandra-temp\n+---\n+# Replication Controller for Cassandra which tracks the Cassandra pods.\n+apiVersion: v1\n+kind: ReplicationController\n+metadata:\n+  labels:\n+    name: cassandra-temp\n+  name: cassandra-temp\n+spec:\n+  replicas: 1\n+  selector:\n+    name: cassandra-temp\n+  template:\n+    metadata:\n+      labels:\n+        name: cassandra-temp\n+    spec:\n+      containers:\n+        - image: cassandra\n+          name: cassandra-temp\n+          env:\n+            - name: PEER_DISCOVERY_SERVICE\n+              value: cassandra-temp\n+            - name: CASSANDRA_CLUSTER_NAME\n+              value: Cassandra\n+            - name: CASSANDRA_DC\n+              value: DC1\n+            - name: CASSANDRA_RACK\n+              value: Kubernetes Cluster\n+          ports:\n+            - containerPort: 9042\n+              name: cql\n+          volumeMounts:\n+            - mountPath: /var/lib/cassandra/data\n+              name: data\n+      volumes:\n+        - name: data\n+          emptyDir: {}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/cassandra-svc-temp.yaml",
                "sha": "79139b70914efa8b7493e473bcd8733bb589a224",
                "status": "added"
            },
            {
                "additions": 122,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh",
                "patch": "@@ -0,0 +1,122 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Hashcode for 50m records is 85b9cec947fc5d849f0a778801696d2b\n+\n+# Script to load data using YCSB on Cassandra multi node cluster.\n+ \n+#!/bin/bash\n+\n+set -e\n+\n+# Record count set to 50000000, change this value to load as per requirement.\n+recordcount=50000000\n+\n+# Function to delete the temporary cassandra service in an erroneous and successful situation\n+function delete_service {\n+  cd ../LargeITCluster\n+  kubectl delete -f cassandra-svc-temp.yaml\n+}\n+\n+# Delete cassandra single node set up before exit \n+trap delete_service EXIT\n+\n+# Check and delete cassandra service if already exists\n+if [ \"$(kubectl get svc -o=name | grep cassandra-temp)\" ]; then\n+  echo \"Service cassandra-temp already exists\"\n+  echo \"Deleting service cassandra-temp \"\n+  delete_service\n+fi\n+  \n+# Temporarily set up cassandra single node cluster for invoking cqlsh on actual cluster remotely\n+kubectl create -f cassandra-svc-temp.yaml\n+\n+num_of_replicas=$(kubectl get statefulset cassandra --output=jsonpath={.spec.replicas})\n+\n+echo \"Script to load data on $num_of_replicas replicas\"\n+echo \"Waiting for Cassandra pods to be in ready state\"\n+\n+# Wait until all the pods configured as per number of replicas, come in running state\n+i=0\n+while [ $i -lt $num_of_replicas ]\n+do\n+   container_state=\"$(kubectl get pods -l app=cassandra -o jsonpath=\"{.items[$i].status.containerStatuses[0].ready}\")\"\n+   while ! $container_state; do\n+      sleep 10s\n+      container_state=\"$(kubectl get pods -l app=cassandra -o jsonpath=\"{.items[$i].status.containerStatuses[0].ready}\")\"\n+      echo \".\"\n+   done\n+   ready_pod=\"$(kubectl get pods -l app=cassandra -o jsonpath=\"{.items[$i].metadata.name}\")\"\n+   echo \"$ready_pod is ready\"\n+   i=$((i+1))\n+done\n+\n+echo \"Waiting for temporary pod to be in ready state\"\n+temp_container_state=\"$(kubectl get pods -l name=cassandra-temp -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+while ! $temp_container_state; do\n+  sleep 10s\n+  temp_container_state=\"$(kubectl get pods -l name=cassandra-temp -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+  echo \".\"\n+done\n+\n+temp_running_seed=\"$(kubectl get pods -l name=cassandra-temp -o jsonpath=\"{.items[0].metadata.name}\")\"\n+\n+# After starting the service, it takes couple of minutes to generate the external IP for the\n+# service. Hence, wait for sometime and identify external IP of the pod\n+external_ip=\"$(kubectl get svc cassandra-external -o jsonpath=\\\n+'{.status.loadBalancer.ingress[0].ip}')\"\n+\n+echo \"Waiting for the Cassandra service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc cassandra-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+echo \"External IP - $external_ip\"\n+\n+echo \"Loading data\"\n+# Create keyspace\n+keyspace_creation_command=\"drop keyspace if exists ycsb;create keyspace ycsb WITH REPLICATION = {\\\n+'class' : 'SimpleStrategy', 'replication_factor': 3 };\"\n+kubectl exec -ti $temp_running_seed -- cqlsh $external_ip -e \"$keyspace_creation_command\"\n+echo \"Keyspace creation............\"\n+echo \"-----------------------------\"\n+echo \"$keyspace_creation_command\"\n+echo\n+\n+# Create table\n+table_creation_command=\"use ycsb;drop table if exists usertable;create table usertable (\\\n+y_id varchar primary key,field0 varchar,field1 varchar,field2 varchar,field3 varchar,\\\n+field4 varchar,field5 varchar,field6 varchar,field7 varchar,field8 varchar,field9 varchar);\"\n+kubectl exec -ti $temp_running_seed -- cqlsh $external_ip -e \"$table_creation_command\"\n+echo \"Table creation ..............\"\n+echo \"-----------------------------\"\n+echo \"$table_creation_command\"\n+\n+# Create index\n+index_creation_command=\"CREATE INDEX IF NOT EXISTS field0_index ON ycsb.usertable (field0);\"\n+kubectl exec -ti $temp_running_seed -- cqlsh $external_ip -e \"$index_creation_command\"\n+\n+cd ../ycsb-0.12.0\n+\n+echo \"Starting to load data on ${external_ip}\"\n+echo \"-----------------------------\"\n+\n+# dataintegrity flag is set to true to load deterministic data\n+./bin/ycsb load cassandra-cql -p hosts=${external_ip} -p dataintegrity=true -p recordcount=\\\n+${recordcount} -p insertorder=ordered -p fieldlength=20 -threads 200 -P workloads/workloadd \\\n+-s > workloada_load_res.txt",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/data-load.sh",
                "sha": "38e856fdd9f9ab1e42b24ac3a1e75247a12f2d28",
                "status": "added"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh",
                "patch": "@@ -0,0 +1,47 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Output Cassandra cluster and pod status.\n+\n+#!/bin/bash\n+\n+find_cassandra_pods=\"kubectl get pods -l app=cassandra\"\n+\n+first_running_seed=\"$($find_cassandra_pods -o jsonpath=\"{.items[0].metadata.name}\")\"\n+\n+# Use nodetool status command to determine the status of pods and display\n+cluster_status=$(kubectl exec $first_running_seed \\\n+    -- /usr/local/apache-cassandra/bin/nodetool status -r)\n+echo\n+echo \"  Cassandra Node      Kubernetes Pod\"\n+echo \"  --------------      --------------\"\n+while read -r line; do\n+    node_name=$(echo $line | awk '{print $1}')\n+    status=$(echo \"$cluster_status\" | grep $node_name | awk '{print $1}')\n+\n+    long_status=$(echo \"$status\" | \\\n+        sed 's/U/  Up/g' | \\\n+\tsed 's/D/Down/g' | \\\n+\tsed 's/N/|Normal /g' | \\\n+\tsed 's/L/|Leaving/g' | \\\n+\tsed 's/J/|Joining/g' | \\\n+\tsed 's/M/|Moving /g')\n+\n+    : ${long_status:=\"            \"}\n+    echo \"$long_status           $line\"\n+done <<< \"$($find_cassandra_pods)\"\n+\n+echo",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/show_health.sh",
                "sha": "a538a9d5a40700ef09201fd26182c007fa371fc9",
                "status": "added"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh",
                "patch": "@@ -0,0 +1,22 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Create Cassandra services and statefulset.\n+kubectl create -f cassandra-service-for-local-dev.yaml\n+kubectl create -f cassandra-svc-statefulset.yaml",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/start-up.sh",
                "sha": "7341209ccaf8a723021d8140dd6f313e8dd69863",
                "status": "added"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh",
                "patch": "@@ -0,0 +1,25 @@\n+#\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+#\n+#!/bin/bash\n+\n+set -e\n+\n+# Delete Cassandra services and statefulset.\n+kubectl delete -f cassandra-svc-statefulset.yaml\n+kubectl delete -f cassandra-service-for-local-dev.yaml\n+# Delete the persistent storage media for the PersistentVolumes\n+kubectl delete pvc -l app=cassandra",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/LargeITCluster/teardown.sh",
                "sha": "367b6049fcbe60247278f8c44305bd13e122c46b",
                "status": "added"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml",
                "patch": "@@ -0,0 +1,30 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Kubernetes service exposing a public LoadBalancer cassandra service.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    name: cassandra-external\n+  name: cassandra-external\n+spec:\n+  ports:\n+    - port: 9042\n+      name: cql\n+  selector:\n+    name: cassandra\n+  type: LoadBalancer",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-service-for-local-dev.yaml",
                "sha": "f2f506934e88c7ca28bda69c44f22b95a1e405c3",
                "status": "added"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml",
                "patch": "@@ -0,0 +1,74 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Headless service that allows us to get the IP addresses of our Cassandra nodes\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  labels:\n+    name: cassandra-peers\n+  name: cassandra-peers\n+spec:\n+  clusterIP: None\n+  ports:\n+    - port: 7000\n+      name: intra-node-communication\n+    - port: 7001\n+      name: tls-intra-node-communication\n+  selector:\n+    name: cassandra\n+  type: NodePort\n+---\n+# Replication Controller for Cassandra which tracks the Cassandra pods.\n+apiVersion: v1\n+kind: ReplicationController\n+metadata:\n+  labels:\n+    name: cassandra\n+  name: cassandra\n+spec:\n+  replicas: 1\n+  selector:\n+    name: cassandra\n+  template:\n+    metadata:\n+      labels:\n+        name: cassandra\n+    spec:\n+      containers:\n+        - image: cassandra\n+          name: cassandra\n+          env:\n+            - name: PEER_DISCOVERY_SERVICE\n+              value: cassandra-peers\n+            - name: CASSANDRA_CLUSTER_NAME\n+              value: Cassandra\n+            - name: CASSANDRA_DC\n+              value: DC1\n+            - name: CASSANDRA_RACK\n+              value: Kubernetes Cluster\n+# Number of tokens currently configured to 1. If this is not configured, default value is 256. You can change it as per requirement.\t\t\t  \n+            - name: CASSANDRA_NUM_TOKENS\n+              value: '1'\n+          ports:\n+            - containerPort: 9042\n+              name: cql\n+          volumeMounts:\n+            - mountPath: /var/lib/cassandra/data\n+              name: data\n+      volumes:\n+        - name: data\n+          emptyDir: {}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/cassandra-svc-rc.yaml",
                "sha": "181689a7e2435317675ba153b991cd4cc74549ab",
                "status": "added"
            },
            {
                "additions": 86,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh",
                "changes": 86,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh",
                "patch": "@@ -0,0 +1,86 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Hashcode for 1000 records is 1a30ad400afe4ebf5fde75f5d2d95408, \n+# For test with query to select one record from 1000 docs, \n+# hashcode is 7bead6d6385c5f4dd0524720cd320b49\n+\n+# Script to load data using YCSB on Cassandra one node cluster.\n+\n+#!/bin/bash\n+set -e\n+\n+# Record count set to 1000, change this value to load as per requirement.\n+recordcount=1000\n+\n+# Identify the pod\n+cassandra_pods=\"kubectl get pods -l name=cassandra\"\n+running_seed=\"$(kubectl get pods -o json -l name=cassandra -o jsonpath=\\\n+'{.items[0].metadata.name}')\"\n+echo \"Detected Pod $running_seed\"\n+\n+echo \"Waiting for Cassandra pod to be in ready state\"\n+container_state=\"$(kubectl get pods -l name=cassandra -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+while ! $container_state; do\n+  sleep 10s\n+  container_state=\"$(kubectl get pods -l name=cassandra -o jsonpath=\"{.items[0].status.containerStatuses[0].ready}\")\"\n+  echo \".\"\n+done\n+\n+# After starting the service, it takes couple of minutes to generate the external IP for the\n+# service. Hence, wait for sometime.\n+# Identify external IP of the pod\n+external_ip=\"$(kubectl get svc cassandra-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+echo \"Waiting for the Cassandra service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc cassandra-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+echo \"External IP - $external_ip\"\n+\n+# Create keyspace\n+keyspace_creation_command=\"drop keyspace if exists ycsb;create keyspace ycsb WITH REPLICATION = {\\\n+'class' : 'SimpleStrategy', 'replication_factor': 3 };\"\n+kubectl exec -ti $running_seed -- cqlsh -e \"$keyspace_creation_command\"\n+echo \"Keyspace creation............\"\n+echo \"-----------------------------\"\n+echo \"$keyspace_creation_command\"\n+echo\n+\n+# Create table\n+table_creation_command=\"use ycsb;drop table if exists usertable;create table usertable (\\\n+y_id varchar primary key,field0 varchar,field1 varchar,field2 varchar,field3 varchar,\\\n+field4 varchar,field5 varchar,field6 varchar,field7 varchar,field8 varchar,field9 varchar);\"\n+kubectl exec -ti $running_seed -- cqlsh -e \"$table_creation_command\"\n+echo \"Table creation ..............\"\n+echo \"-----------------------------\"\n+echo \"$table_creation_command\"\n+\n+# Create index\n+index_creation_command=\"CREATE INDEX IF NOT EXISTS field0_index ON ycsb.usertable (field0);\"\n+kubectl exec -ti $running_seed -- cqlsh -e \"$index_creation_command\"\n+\n+cd ../ycsb-0.12.0\n+\n+echo \"Starting to load data on ${external_ip}\"\n+echo \"-----------------------------\"\n+# Record count set to 1000, change this value to load as per requirement.\n+# dataintegrity flag is set to true to load deterministic data\n+./bin/ycsb load cassandra-cql -p hosts=${external_ip} -p dataintegrity=true -p recordcount=\\\n+${recordcount} -p insertorder=ordered -p fieldlength=20 -P workloads/workloadd \\\n+-s > workloada_load_res.txt",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/data-load.sh",
                "sha": "203c8a859e551a9b24a75e68b7483ac51414825b",
                "status": "added"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh",
                "patch": "@@ -0,0 +1,47 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Output Cassandra cluster and pod status.\n+\n+#!/bin/bash\n+\n+find_cassandra_pods=\"kubectl get pods -l name=cassandra\"\n+\n+first_running_seed=\"$($find_cassandra_pods -o jsonpath=\"{.items[0].metadata.name}\")\"\n+\n+# Use nodetool status command to determine the status of pods and display\n+cluster_status=$(kubectl exec $first_running_seed \\\n+    -- nodetool status -r)\n+echo\n+echo \"  Cassandra Node      Kubernetes Pod\"\n+echo \"  --------------      --------------\"\n+while read -r line; do\n+    node_name=$(echo $line | awk '{print $1}')\n+    status=$(echo \"$cluster_status\" | grep $node_name | awk '{print $1}')\n+\n+    long_status=$(echo \"$status\" | \\\n+        sed 's/U/  Up/g' | \\\n+\tsed 's/D/Down/g' | \\\n+\tsed 's/N/|Normal /g' | \\\n+\tsed 's/L/|Leaving/g' | \\\n+\tsed 's/J/|Joining/g' | \\\n+\tsed 's/M/|Moving /g')\n+\n+    : ${long_status:=\"            \"}\n+    echo \"$long_status           $line\"\n+done <<< \"$($find_cassandra_pods)\"\n+\n+echo",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/show_health.sh",
                "sha": "a3ea94136f83dfd5fd2c415a9f15f20f80304577",
                "status": "added"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh",
                "patch": "@@ -0,0 +1,23 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Create Cassandra services and Replication controller.\n+kubectl create -f cassandra-service-for-local-dev.yaml\n+kubectl create -f cassandra-svc-rc.yaml\n+",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/start-up.sh",
                "sha": "9377a9c190a0b6bf18c18c54d4188d0c3d4ce606",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": ".test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh",
                "patch": "@@ -1,4 +1,3 @@\n-#!/bin/bash\n #\n #    Licensed to the Apache Software Foundation (ASF) under one or more\n #    contributor license agreements.  See the NOTICE file distributed with\n@@ -15,6 +14,9 @@\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n #\n+#!/bin/bash\n+set -e\n \n-kubectl delete service postgres-no-pv\n-kubectl delete pod postgres-no-pv\n+# Delete Cassandra services and Replication controller.\n+kubectl delete -f cassandra-svc-rc.yaml\n+kubectl delete -f cassandra-service-for-local-dev.yaml",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/teardown.sh",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/SmallITCluster/teardown.sh",
                "sha": "f4ad0be9045665bdee329167f438cba61e0fb1be",
                "status": "renamed"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/data-load-setup.sh",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/cassandra/data-load-setup.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/cassandra/data-load-setup.sh",
                "patch": "@@ -0,0 +1,29 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Load YCSB tool\n+echo \"Downloading YCSB tool\"\n+echo \"------------------------------\"\n+curl -O --location https://github.com/brianfrankcooper/YCSB/releases/download/0.12.0/ycsb-0.12.0.tar.gz\n+tar xfz ycsb-0.12.0.tar.gz\n+wget https://www.slf4j.org/dist/slf4j-1.7.22.tar.gz\n+tar xfz slf4j-1.7.22.tar.gz\n+cp slf4j-1.7.22/slf4j-simple-*.jar ycsb-0.12.0/lib/\n+cp slf4j-1.7.22/slf4j-api-*.jar ycsb-0.12.0/lib/\n+echo \"YCSB tool loaded\"",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/cassandra/data-load-setup.sh",
                "sha": "4e12f89479bd70e0c21eb961d0c18a68b461e830",
                "status": "added"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml",
                "patch": "@@ -0,0 +1,33 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# To create Elasticsearch frontend cluster Kubernetes service.\n+# It sets up a load balancer on TCP port 9200 that distributes network traffic to the ES client nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch-external\n+  labels:\n+    component: elasticsearch\n+    role: client\n+spec:\n+  type: LoadBalancer\n+  selector:\n+    component: elasticsearch\n+    role: client\n+  ports:\n+  - name: http\n+    port: 9200\n+    protocol: TCP",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/elasticsearch-service-for-local-dev.yaml",
                "sha": "d28d70ad98ae945b3d8b1e943d20230c9c468919",
                "status": "added"
            },
            {
                "additions": 258,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml",
                "changes": 258,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml",
                "patch": "@@ -0,0 +1,258 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# Service file containing services for ES discovery, elasticsearch and master node deployment.\n+\n+# Kubernetes headless service for Elasticsearch discovery of nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch-discovery\n+  labels:\n+    component: elasticsearch\n+    role: master\n+spec:\n+  selector:\n+    component: elasticsearch\n+    role: master\n+  ports:\n+  - name: transport\n+    port: 9300\n+    protocol: TCP\n+---\n+# The Kubernetes deployment script for Elasticsearch master nodes.\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es-master\n+  labels:\n+    component: elasticsearch\n+    role: master\n+spec:\n+  replicas: 3\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+        role: master\n+      annotations:\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es-master\n+        securityContext:\n+          privileged: false\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.\n+              - IPC_LOCK\n+# SYS_RESOURCE is docker capability key to control and override the resource limits.\n+# This could be needed to increase base limits.(e.g. File descriptor limit for elasticsearch)\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: NAMESPACE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.namespace\n+        - name: NODE_NAME\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.name\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: \"NUMBER_OF_MASTERS\"\n+          value: \"2\"\n+        - name: NODE_MASTER\n+          value: \"true\"\n+        - name: NODE_INGEST\n+          value: \"false\"\n+        - name: NODE_DATA\n+          value: \"false\"\n+        - name: HTTP_ENABLE\n+          value: \"false\"\n+        - name: \"ES_JAVA_OPTS\"\n+          value: \"-Xms2g -Xmx2g\"\n+        ports:\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - name: storage\n+          mountPath: /data\n+      volumes:\n+          - emptyDir:\n+              medium: \"\"\n+            name: \"storage\"\n+---\n+# Kubernetes deployment script for Elasticsearch client nodes (aka load balancing proxies).\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es-client\n+  labels:\n+    component: elasticsearch\n+    role: client\n+spec:\n+  # The no. of replicas can be incremented based on the client usage using HTTP API.\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+        role: client\n+      annotations:\n+      # Elasticsearch uses a hybrid mmapfs / niofs directory by default to store its indices.\n+      # The default operating system limits on mmap counts is likely to be too low, which may result\n+      # in out of memory exceptions. Therefore, the need to increase virtual memory\n+      # vm.max_map_count for large amount of data in the pod initialization annotation.\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es-client\n+        securityContext:\n+          privileged: false\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.\n+              - IPC_LOCK\n+# SYS_RESOURCE is docker capability key to control and override the resource limits.\n+# This could be needed to increase base limits.(e.g. File descriptor limit for elasticsearch)\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: NAMESPACE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.namespace\n+        - name: NODE_NAME\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.name\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: NODE_MASTER\n+          value: \"false\"\n+        - name: NODE_DATA\n+          value: \"false\"\n+        - name: HTTP_ENABLE\n+          value: \"true\"\n+        - name: \"ES_JAVA_OPTS\"\n+          value: \"-Xms2g -Xmx2g\"\n+        ports:\n+        - containerPort: 9200\n+          name: http\n+          protocol: TCP\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - name: storage\n+          mountPath: /data\n+      volumes:\n+          - emptyDir:\n+              medium: \"\"\n+            name: \"storage\"\n+---\n+# Kubernetes deployment script for Elasticsearch data nodes which store and index data.\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es-data\n+  labels:\n+    component: elasticsearch\n+    role: data\n+spec:\n+  replicas: 2\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+        role: data\n+      annotations:\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=1048575\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es-data\n+        securityContext:\n+          privileged: false\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.\n+              - IPC_LOCK\n+# SYS_RESOURCE is docker capability key to control and override the resource limits.\n+# This could be needed to increase base limits.(e.g. File descriptor limit for elasticsearch)\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: NAMESPACE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.namespace\n+        - name: NODE_NAME\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: metadata.name\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: NODE_MASTER\n+          value: \"false\"\n+        - name: NODE_INGEST\n+          value: \"false\"\n+        - name: HTTP_ENABLE\n+          value: \"false\"\n+        - name: \"ES_JAVA_OPTS\"\n+          value: \"-Xms2g -Xmx2g\"\n+        ports:\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - name: storage\n+          mountPath: /data\n+      volumes:\n+          - emptyDir:\n+              medium: \"\"\n+            name: \"storage\"",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/es-services-deployments.yaml",
                "sha": "8f29fb667736d5dc856b94570edf0e3a2ac7f053",
                "status": "added"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh",
                "patch": "@@ -0,0 +1,22 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+#\n+\n+#!/bin/sh\n+set -e\n+\n+# Create Elasticsearch services and deployments.\n+kubectl create -f elasticsearch-service-for-local-dev.yaml\n+kubectl create -f es-services-deployments.yaml",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/start-up.sh",
                "sha": "93022c7244000c7532d39ebc173fd73fbad4dfa8",
                "status": "added"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh",
                "patch": "@@ -0,0 +1,21 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Delete elasticsearch services and deployments.\n+kubectl delete -f es-services-deployments.yaml\n+kubectl delete -f elasticsearch-service-for-local-dev.yaml",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/LargeProductionCluster/teardown.sh",
                "sha": "bdc9ab9341a6fbac62ab6c96da2e9cd79f37e574",
                "status": "added"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml",
                "patch": "@@ -0,0 +1,34 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# To create Elasticsearch frontend cluster Kubernetes service. \n+# It sets up a load balancer on TCP port 9200 that distributes network traffic to the ES nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch-external\n+  labels:\n+    component: elasticsearch\n+spec:\n+  type: LoadBalancer\n+  selector:\n+    component: elasticsearch\n+  ports:\n+  - name: http\n+    port: 9200\n+    protocol: TCP\n+  - name: transport\n+    port: 9300\n+    protocol: TCP",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-service-for-local-dev.yaml",
                "sha": "0a16cdb03cbecf7ce62daa8fa66abd3ba565058b",
                "status": "added"
            },
            {
                "additions": 96,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml",
                "patch": "@@ -0,0 +1,96 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+# To create Elasticsearch frontend cluster Kubernetes service. \n+# It sets up a load balancer on TCP port 9200 that distributes network traffic to the ES nodes.\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: elasticsearch\n+  labels:\n+    component: elasticsearch\n+spec:\n+  selector:\n+    component: elasticsearch\n+  type: NodePort\n+  ports:\n+  - name: http\n+    port: 9200\n+    protocol: TCP\n+  - name: transport\n+    port: 9300\n+    protocol: TCP\n+---\n+# The Kubernetes deployment script for Elasticsearch replication nodes. It will create 1 node cluster.\n+# To scale the cluster as desired, you can create replicas of node use 'kubectl scale --replicas=3 rc es' command\n+apiVersion: extensions/v1beta1\n+kind: Deployment\n+metadata:\n+  name: es\n+  labels:\n+    component: elasticsearch\n+spec:\n+  replicas: 1\n+  template:\n+    metadata:\n+      labels:\n+        component: elasticsearch\n+      annotations:\n+        pod.beta.kubernetes.io/init-containers: '[\n+          {\n+          \"name\": \"sysctl\",\n+            \"image\": \"busybox\",\n+            \"imagePullPolicy\": \"IfNotPresent\",\n+            \"command\": [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"],\n+            \"securityContext\": {\n+              \"privileged\": true\n+            }\n+          }\n+        ]'\n+    spec:\n+      containers:\n+      - name: es\n+        securityContext:\n+          capabilities:\n+            add:\n+# IPC_LOCK capability is enabled to allow Elasticsearch to lock the heap in memory so it will not be swapped.   \n+              - IPC_LOCK\n+# SYS_RESOURCE capability is set to control and override various resource limits.\n+              - SYS_RESOURCE\n+        image: quay.io/pires/docker-elasticsearch-kubernetes:5.2.2\n+        env:\n+        - name: \"CLUSTER_NAME\"\n+          value: \"myesdb\"\n+        - name: \"DISCOVERY_SERVICE\"\n+          value: \"elasticsearch\"\n+        - name: NODE_MASTER\n+          value: \"true\"\n+        - name: NODE_DATA\n+          value: \"true\"\n+        - name: HTTP_ENABLE\n+          value: \"true\"\n+        ports:\n+        - containerPort: 9200\n+          name: http\n+          protocol: TCP\n+        - containerPort: 9300\n+          name: transport\n+          protocol: TCP\n+        volumeMounts:\n+        - mountPath: /data\n+          name: storage\n+      volumes:\n+      - name: storage\n+        emptyDir: {}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/elasticsearch-svc-rc.yaml",
                "sha": "a4e1ea367ee914b37487015597050bd3199a83f0",
                "status": "added"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh",
                "patch": "@@ -0,0 +1,23 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+#\n+\n+#!/bin/sh\n+set -e\n+\n+# Create Elasticsearch services and deployments.\n+kubectl create -f elasticsearch-service-for-local-dev.yaml\n+kubectl create -f elasticsearch-svc-rc.yaml\n+",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/start-up.sh",
                "sha": "2d6522ea9c54337aaa9e02372c6004cddd37547b",
                "status": "added"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": ".test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh",
                "patch": "@@ -1,5 +1,3 @@\n-#!/bin/bash\n-#\n #    Licensed to the Apache Software Foundation (ASF) under one or more\n #    contributor license agreements.  See the NOTICE file distributed with\n #    this work for additional information regarding copyright ownership.\n@@ -14,7 +12,10 @@\n #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n-#\n \n-kubectl create -f postgres-pod-no-vol.yml\n-kubectl create -f postgres-service-public.yml\n+#!/bin/bash\n+set -e\n+\n+# Delete elasticsearch services and deployments.\n+kubectl delete -f elasticsearch-svc-rc.yaml\n+kubectl delete -f elasticsearch-service-for-local-dev.yaml",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/setup.sh",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/SmallITCluster/teardown.sh",
                "sha": "61c079fed894d5c83389857d26ff3183903d3d23",
                "status": "renamed"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load-setup.sh",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/data-load-setup.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/data-load-setup.sh",
                "patch": "@@ -0,0 +1,26 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Install python\n+sudo apt-get update\n+sudo apt-get install python-pip\n+sudo pip install --upgrade pip\n+sudo apt-get install python-dev\n+sudo pip install tornado numpy\n+echo",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load-setup.sh",
                "sha": "00991bc6d3379d43c67414256b60515141fd4dc7",
                "status": "added"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load.sh",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/data-load.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/data-load.sh",
                "patch": "@@ -0,0 +1,33 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/bash\n+set -e\n+\n+# Identify external IP\n+external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+echo \"Waiting for the Elasticsearch service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+echo \"External IP - $external_ip\"\n+echo\n+\n+# Run the script\n+/usr/bin/python es_test_data.py --count=1000 --format=Txn_ID:int,Item_Code:int,Item_ID:int,User_Name:str,last_updated:ts,Price:int,Title:str,Description:str,Age:int,Item_Name:str,Item_Price:int,Availability:bool,Batch_Num:int,Last_Ordered:tstxt,City:text --es_url=http://$external_ip:9200 &",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/data-load.sh",
                "sha": "a7dd84ad1ea854075178243e2cb25f9c15c8b74a",
                "status": "added"
            },
            {
                "additions": 299,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/es_test_data.py",
                "changes": 299,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/es_test_data.py?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/es_test_data.py",
                "patch": "@@ -0,0 +1,299 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Script to populate data on Elasticsearch\n+# Hashcode for 1000 records is 42e254c8689050ed0a617ff5e80ea392, \n+# For test with query to select one record from 1000 docs, \n+# hashcode is d7a7e4e42c2ca7b83ef7c1ad1ebce000\n+# Hashcode for 50m records (~20 gigs) is 42e254c8689050ed0a617ff5e80ea392 \n+#!/usr/bin/python\n+\n+import json\n+import time\n+import logging\n+import random\n+import string\n+import uuid\n+import datetime\n+\n+import tornado.gen\n+import tornado.httpclient\n+import tornado.ioloop\n+import tornado.options\n+\n+async_http_client = tornado.httpclient.AsyncHTTPClient()\n+id_counter = 0\n+upload_data_count = 0\n+_dict_data = None\n+\n+\n+\n+def delete_index(idx_name):\n+    try:\n+        url = \"%s/%s?refresh=true\" % (tornado.options.options.es_url, idx_name)\n+        request = tornado.httpclient.HTTPRequest(url, method=\"DELETE\", request_timeout=240, \n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        response = tornado.httpclient.HTTPClient().fetch(request)\n+        logging.info('Deleting index  \"%s\" done   %s' % (idx_name, response.body))\n+    except tornado.httpclient.HTTPError:\n+        pass\n+\n+\n+def create_index(idx_name):\n+    schema = {\n+        \"settings\": {\n+            \"number_of_shards\":   tornado.options.options.num_of_shards,\n+            \"number_of_replicas\": tornado.options.options.num_of_replicas\n+        },\n+        \"refresh\": True\n+    }\n+\n+    body = json.dumps(schema)\n+    url = \"%s/%s\" % (tornado.options.options.es_url, idx_name)\n+    try:\n+        logging.info('Trying to create index %s' % (url))\n+        request = tornado.httpclient.HTTPRequest(url, method=\"PUT\", body=body, request_timeout=240,\n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        response = tornado.httpclient.HTTPClient().fetch(request)\n+        logging.info('Creating index \"%s\" done   %s' % (idx_name, response.body))\n+    except tornado.httpclient.HTTPError:\n+        logging.info('Looks like the index exists already')\n+        pass\n+\n+\n+@tornado.gen.coroutine\n+def upload_batch(upload_data_txt):\n+    try:\n+        request = tornado.httpclient.HTTPRequest(tornado.options.options.es_url + \"/_bulk\",\n+                                                 method=\"POST\", body=upload_data_txt,\n+                                                 request_timeout=\n+                                                 tornado.options.options.http_upload_timeout,\n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        response = yield async_http_client.fetch(request)\n+    except Exception as ex:\n+        logging.error(\"upload failed, error: %s\" % ex)\n+        return\n+\n+    result = json.loads(response.body.decode('utf-8'))\n+    res_txt = \"OK\" if not result['errors'] else \"FAILED\"\n+    took = int(result['took'])\n+    logging.info(\"Upload: %s - upload took: %5dms, total docs uploaded: %7d\" % (res_txt, took, \n+                                                                                upload_data_count))\n+\n+\n+def get_data_for_format(format,count):\n+    split_f = format.split(\":\")\n+    if not split_f:\n+        return None, None\n+\n+    field_name = split_f[0]\n+    field_type = split_f[1]\n+\n+    return_val = ''\n+\n+    if field_type == \"bool\":\n+        if count%2 == 0:\n+           return_val = True\n+        else:\n+           return_val = False\n+\n+    elif field_type == \"str\":\n+        return_val = field_name + str(count)\n+\n+    elif field_type == \"int\":\n+        return_val = count\n+    \n+    elif field_type == \"ipv4\":\n+        return_val = \"{0}.{1}.{2}.{3}\".format(1,2,3,count%255)\n+\n+    elif field_type in [\"ts\", \"tstxt\"]:\n+        return_val = int(count * 1000) if field_type == \"ts\" else\\\n+        \t\t\t datetime.datetime.fromtimestamp(count)\\\n+        \t\t\t .strftime(\"%Y-%m-%dT%H:%M:%S.000-0000\")\n+\n+    elif field_type == \"words\":\n+        return_val = field_name + str(count)\n+\n+    elif field_type == \"dict\":\n+        mydict = dict(a=field_name + str(count), b=field_name + str(count), c=field_name + str(count),\n+                      d=field_name + str(count), e=field_name + str(count), f=field_name + str(count),\n+                      g=field_name + str(count), h=field_name + str(count), i=field_name + str(count), \n+                      j=field_name + str(count))\n+        return_val = \", \".join(\"=\".join(_) for _ in mydict.items())\n+\n+    elif field_type == \"text\":\n+        return_val = field_name + str(count)\n+\n+    return field_name, return_val\n+\n+\n+def generate_count(min, max):\n+    if min == max:\n+        return max\n+    elif min > max:\n+        return random.randrange(max, min);\n+    else:\n+        return random.randrange(min, max);\n+\n+\n+def generate_random_doc(format,count):\n+    global id_counter\n+\n+    res = {}\n+\n+    for f in format:\n+        f_key, f_val = get_data_for_format(f,count)\n+        if f_key:\n+            res[f_key] = f_val\n+\n+    if not tornado.options.options.id_type:\n+        return res\n+\n+    if tornado.options.options.id_type == 'int':\n+        res['_id'] = id_counter\n+        id_counter += 1\n+    elif tornado.options.options.id_type == 'uuid4':\n+        res['_id'] = str(uuid.uuid4())\n+\n+    return res\n+\n+\n+def set_index_refresh(val):\n+\n+    params = {\"index\": {\"refresh_interval\": val}}\n+    body = json.dumps(params)\n+    url = \"%s/%s/_settings\" % (tornado.options.options.es_url, tornado.options.options.index_name)\n+    try:\n+        request = tornado.httpclient.HTTPRequest(url, method=\"PUT\", body=body, request_timeout=240,\n+                                                 auth_username=tornado.options.options.username, \n+                                                 auth_password=tornado.options.options.password)\n+        http_client = tornado.httpclient.HTTPClient()\n+        http_client.fetch(request)\n+        logging.info('Set index refresh to %s' % val)\n+    except Exception as ex:\n+        logging.exception(ex)\n+\n+\n+@tornado.gen.coroutine\n+def generate_test_data():\n+\n+    global upload_data_count\n+\n+    if tornado.options.options.force_init_index:\n+        delete_index(tornado.options.options.index_name)\n+\n+    create_index(tornado.options.options.index_name)\n+\n+    # todo: query what refresh is set to, then restore later\n+    if tornado.options.options.set_refresh:\n+        set_index_refresh(\"-1\")\n+\n+    if tornado.options.options.out_file:\n+        out_file = open(tornado.options.options.out_file, \"w\")\n+    else:\n+        out_file = None\n+\n+    if tornado.options.options.dict_file:\n+        global _dict_data\n+        with open(tornado.options.options.dict_file, 'r') as f:\n+            _dict_data = f.readlines()\n+        logging.info(\"Loaded %d words from the %s\" % (len(_dict_data), \n+                                                      tornado.options.options.dict_file))\n+\n+    format = tornado.options.options.format.split(',')\n+    if not format:\n+        logging.error('invalid format')\n+        exit(1)\n+\n+    ts_start = int(time.time())\n+    upload_data_txt = \"\"\n+    total_uploaded = 0\n+\n+    logging.info(\"Generating %d docs, upload batch size is %d\" % (tornado.options.options.count,\n+                                                                  tornado.options\n+                                                                  .options.batch_size))\n+    for num in range(0, tornado.options.options.count):\n+\n+        item = generate_random_doc(format,num)\n+\n+        if out_file:\n+            out_file.write(\"%s\\n\" % json.dumps(item))\n+\n+        cmd = {'index': {'_index': tornado.options.options.index_name,\n+                         '_type': tornado.options.options.index_type}}\n+        if '_id' in item:\n+            cmd['index']['_id'] = item['_id']\n+\n+        upload_data_txt += json.dumps(cmd) + \"\\n\"\n+        upload_data_txt += json.dumps(item) + \"\\n\"\n+        upload_data_count += 1\n+\n+        if upload_data_count % tornado.options.options.batch_size == 0:\n+            yield upload_batch(upload_data_txt)\n+            upload_data_txt = \"\"\n+\n+    # upload remaining items in `upload_data_txt`\n+    if upload_data_txt:\n+        yield upload_batch(upload_data_txt)\n+\n+    if tornado.options.options.set_refresh:\n+        set_index_refresh(\"1s\")\n+\n+    if out_file:\n+        out_file.close()\n+\n+    took_secs = int(time.time() - ts_start)\n+\n+    logging.info(\"Done - total docs uploaded: %d, took %d seconds\" % \n+    \t\t\t (tornado.options.options.count, took_secs))\n+\n+\n+if __name__ == '__main__':\n+    tornado.options.define(\"es_url\", type=str, default='http://localhost:9200/', \n+                           help=\"URL of your Elasticsearch node\")\n+    tornado.options.define(\"index_name\", type=str, default='test_data', \n+                           help=\"Name of the index to store your messages\")\n+    tornado.options.define(\"index_type\", type=str, default='test_type', help=\"Type\")\n+    tornado.options.define(\"batch_size\", type=int, default=1000, \n+                           help=\"Elasticsearch bulk index batch size\")\n+    tornado.options.define(\"num_of_shards\", type=int, default=2, \n+                           help=\"Number of shards for ES index\")\n+    tornado.options.define(\"http_upload_timeout\", type=int, default=3, \n+                           help=\"Timeout in seconds when uploading data\")\n+    tornado.options.define(\"count\", type=int, default=100000, help=\"Number of docs to generate\")\n+    tornado.options.define(\"format\", type=str, default='name:str,age:int,last_updated:ts', \n+                           help=\"message format\")\n+    tornado.options.define(\"num_of_replicas\", type=int, default=0, \n+                           help=\"Number of replicas for ES index\")\n+    tornado.options.define(\"force_init_index\", type=bool, default=False, \n+                           help=\"Force deleting and re-initializing the Elasticsearch index\")\n+    tornado.options.define(\"set_refresh\", type=bool, default=False, \n+                           help=\"Set refresh rate to -1 before starting the upload\")\n+    tornado.options.define(\"out_file\", type=str, default=False, \n+                           help=\"If set, write test data to out_file as well.\")\n+    tornado.options.define(\"id_type\", type=str, default=None, \n+                           help=\"Type of 'id' to use for the docs, \\\n+                           valid settings are int and uuid4, None is default\")\n+    tornado.options.define(\"dict_file\", type=str, default=None, \n+                           help=\"Name of dictionary file to use\")\n+    tornado.options.define(\"username\", type=str, default=None, help=\"Username for elasticsearch\")\n+    tornado.options.define(\"password\", type=str, default=None, help=\"Password for elasticsearch\")\n+    tornado.options.parse_command_line()\n+\n+    tornado.ioloop.IOLoop.instance().run_sync(generate_test_data)",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/es_test_data.py",
                "sha": "cf10d39af03ca09870b108c212729736bb4db054",
                "status": "added"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/show-health.sh",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/elasticsearch/show-health.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/elasticsearch/show-health.sh",
                "patch": "@@ -0,0 +1,33 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+#!/bin/sh\n+set -e\n+\n+external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+\n+echo \"Waiting for the Elasticsearch service to come up ........\"\n+while [ -z \"$external_ip\" ]\n+do\n+   sleep 10s\n+   external_ip=\"$(kubectl get svc elasticsearch-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\"\n+   echo \".\"\n+done\n+\n+echo \"Elasticsearch cluster health info\"\n+echo \"---------------------------------\"\n+curl $external_ip:9200/_cluster/health\n+echo # empty line since curl doesn't output CRLF",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/elasticsearch/show-health.sh",
                "sha": "abc3c89d9307e6c5957ce5db0fb19a48c3e0b270",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": ".test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml",
                "patch": "@@ -13,16 +13,16 @@\n #    See the License for the specific language governing permissions and\n #    limitations under the License.\n \n+\n apiVersion: v1\n kind: Service\n metadata:\n-  name: postgres-no-pv\n+  name: postgres-for-dev\n   labels:\n-    name: postgres-no-pv\n+    name: postgres\n spec:\n   ports:\n     - port: 5432\n-      nodePort: 31234\n   selector:\n-    name: postgres-no-pv\n-  type: NodePort\n+    name: postgres\n+  type: LoadBalancer",
                "previous_filename": "sdks/java/io/jdbc/src/test/resources/kubernetes/postgres-service-public.yml",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres-service-for-local-dev.yml",
                "sha": "5d2c6648590d39e93fb7d52ea96b29aa073df65d",
                "status": "renamed"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres.yml",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/kubernetes/postgres/postgres.yml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/kubernetes/postgres/postgres.yml",
                "patch": "@@ -0,0 +1,56 @@\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+\n+\n+apiVersion: v1\n+kind: Service\n+metadata:\n+  name: postgres\n+  labels:\n+    name: postgres\n+spec:\n+  ports:\n+    - port: 5432\n+      nodePort: 31234\n+  selector:\n+    name: postgres\n+  type: NodePort\n+\n+---\n+\n+apiVersion: v1\n+kind: ReplicationController\n+metadata:\n+  name: postgres\n+spec:\n+  replicas: 1\n+  selector:\n+    name: postgres\n+  template:\n+    metadata:\n+      name: postgres\n+      labels:\n+        name: postgres\n+    spec:\n+      containers:\n+        - name: postgres\n+          image: postgres\n+          env:\n+            - name: POSTGRES_PASSWORD\n+              value: uuinkks\n+            - name: PGDATA\n+              value: /var/lib/postgresql/data/pgdata\n+          ports:\n+            - containerPort: 5432",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/kubernetes/postgres/postgres.yml",
                "sha": "62449689b32be95b31d6ec2e8d5e0e616dcb8a1e",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/README.md",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/travis/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/travis/README.md",
                "previous_filename": ".travis/README.md",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/README.md",
                "sha": "526995aa69ccbab8d71d741747153293a309e0a0",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/settings.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/travis/settings.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/travis/settings.xml",
                "previous_filename": ".travis/settings.xml",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/settings.xml",
                "sha": "e086aec3a732d309e5de1a0783a7cd8c86eebe28",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/test_wordcount.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.test-infra/travis/test_wordcount.sh?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": ".test-infra/travis/test_wordcount.sh",
                "previous_filename": ".travis/test_wordcount.sh",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.test-infra/travis/test_wordcount.sh",
                "sha": "e059a3552bfbc142750bd1a76e58536cdb69477a",
                "status": "renamed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/.travis.yml",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.travis.yml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": ".travis.yml",
                "patch": "@@ -34,7 +34,7 @@ addons:\n     - python2.7\n env:\n   global:\n-   - MAVEN_OVERRIDE=\"--settings=.travis/settings.xml\"\n+   - MAVEN_OVERRIDE=\"--settings=.test-infra/travis/settings.xml\"\n    - MAVEN_CONTAINER_OVERRIDE=\"-DbeamSurefireArgline='-Xmx512m'\"\n \n matrix:\n@@ -68,7 +68,6 @@ before_install:\n   - if [ \"$TRAVIS_OS_NAME\" == \"linux\" ]; then jdk_switcher use \"$CUSTOM_JDK\"; fi\n   - export BEAM_SUREFIRE_ARGLINE=\"-Xmx512m\"\n   # Python SDK environment settings.\n-  - export TOX_ENV=py27\n   - if [ \"$TRAVIS_OS_NAME\" == \"osx\" ]; then export TOX_HOME=$HOME/Library/Python/2.7/bin; fi\n   - if [ \"$TRAVIS_OS_NAME\" == \"linux\" ]; then export TOX_HOME=$HOME/.local/bin; fi\n \n@@ -81,8 +80,8 @@ install:\n   - rm -rf \"$HOME/.m2/repository/org/apache/gearpump\"\n \n script:\n-  - if [ \"$TEST_PYTHON\" ]; then travis_retry $TOX_HOME/tox -e $TOX_ENV -c sdks/python/tox.ini; fi\n-  - if [ ! \"$TEST_PYTHON\" ]; then travis_retry mvn --batch-mode --update-snapshots --no-snapshot-updates --threads 1C $MAVEN_OVERRIDE install && travis_retry bash -ex .travis/test_wordcount.sh; fi\n+  - if [ \"$TEST_PYTHON\" ]; then travis_retry $TOX_HOME/tox -c sdks/python/tox.ini; fi\n+  - if [ ! \"$TEST_PYTHON\" ]; then travis_retry mvn --batch-mode --update-snapshots --no-snapshot-updates --threads 1C $MAVEN_OVERRIDE install && travis_retry bash -ex .test-infra/travis/test_wordcount.sh; fi\n \n cache:\n   directories:",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/.travis.yml",
                "sha": "a1b28f90862c3def718a71e95c6a31cdf308b384",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/README.md",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "README.md",
                "patch": "@@ -61,6 +61,7 @@ Have ideas for new SDKs or DSLs? See the [JIRA](https://issues.apache.org/jira/b\n Beam supports executing programs on multiple distributed processing backends through PipelineRunners. Currently, the following PipelineRunners are available:\n \n - The `DirectRunner` runs the pipeline on your local machine.\n+- The `ApexRunner` runs the pipeline on an Apache Hadoop YARN cluster (or in embedded mode).\n - The `DataflowRunner` submits the pipeline to the [Google Cloud Dataflow](http://cloud.google.com/dataflow/).\n - The `FlinkRunner` runs the pipeline on an Apache Flink cluster. The code has been donated from [dataArtisans/flink-dataflow](https://github.com/dataArtisans/flink-dataflow) and is now part of Beam.\n - The `SparkRunner` runs the pipeline on an Apache Spark cluster. The code has been donated from [cloudera/spark-dataflow](https://github.com/cloudera/spark-dataflow) and is now part of Beam.\n@@ -96,6 +97,8 @@ To get involved in Apache Beam:\n * [Subscribe](mailto:dev-subscribe@beam.apache.org) or [mail](mailto:dev@beam.apache.org) the [dev@beam.apache.org](http://mail-archives.apache.org/mod_mbox/beam-dev/) list.\n * Report issues on [JIRA](https://issues.apache.org/jira/browse/BEAM).\n \n+We also have a [contributor's guide](https://beam.apache.org/contribute/contribution-guide/).\n+\n ## More Information\n \n * [Apache Beam](http://beam.apache.org)",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/README.md",
                "sha": "23768ce64d3b47e8967903a5527d43233e05b19d",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/README.md",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 27,
                "filename": "examples/java/README.md",
                "patch": "@@ -20,25 +20,23 @@\n # Example Pipelines\n \n The examples included in this module serve to demonstrate the basic\n-functionality of Google Cloud Dataflow, and act as starting points for\n+functionality of Apache Beam, and act as starting points for\n the development of more complex pipelines.\n \n ## Word Count\n \n A good starting point for new users is our set of\n-[word count](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples) examples, which computes word frequencies.  This series of four successively more detailed pipelines is described in detail in the accompanying [walkthrough](https://cloud.google.com/dataflow/examples/wordcount-example).\n+[word count](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples) examples, which computes word frequencies.  This series of four successively more detailed pipelines is described in detail in the accompanying [walkthrough](https://beam.apache.org/get-started/wordcount-example/).\n \n-1. [`MinimalWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/MinimalWordCount.java) is the simplest word count pipeline and introduces basic concepts like [Pipelines](https://cloud.google.com/dataflow/model/pipelines),\n-[PCollections](https://cloud.google.com/dataflow/model/pcollection),\n-[ParDo](https://cloud.google.com/dataflow/model/par-do),\n-and [reading and writing data](https://cloud.google.com/dataflow/model/reading-and-writing-data) from external storage.\n+1. [`MinimalWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/MinimalWordCount.java) is the simplest word count pipeline and introduces basic concepts like [Pipelines](https://beam.apache.org/documentation/programming-guide/#pipeline),\n+[PCollections](https://beam.apache.org/documentation/programming-guide/#pcollection),\n+[ParDo](https://beam.apache.org/documentation/programming-guide/#transforms-pardo),\n+and [reading and writing data](https://beam.apache.org/documentation/programming-guide/#io) from external storage.\n \n-1. [`WordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WordCount.java) introduces Dataflow best practices like [PipelineOptions](https://cloud.google.com/dataflow/pipelines/constructing-your-pipeline#Creating) and custom [PTransforms](https://cloud.google.com/dataflow/model/composite-transforms).\n+1. [`WordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WordCount.java) introduces best practices like [PipelineOptions](https://beam.apache.org/documentation/programming-guide/#pipeline) and custom [PTransforms](https://beam.apache.org/documentation/programming-guide/#transforms-composite).\n \n 1. [`DebuggingWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java)\n-shows how to view live aggregators in the [Dataflow Monitoring Interface](https://cloud.google.com/dataflow/pipelines/dataflow-monitoring-intf), get the most out of\n-[Cloud Logging](https://cloud.google.com/dataflow/pipelines/logging) integration, and start writing\n-[good tests](https://cloud.google.com/dataflow/pipelines/testing-your-pipeline).\n+demonstrates some best practices for instrumenting your pipeline code.\n \n 1. [`WindowedWordCount`](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java) shows how to run the same pipeline over either unbounded PCollections in streaming mode or bounded PCollections in batch mode.\n \n@@ -50,46 +48,55 @@ Change directory into `examples/java` and run the examples:\n     -Dexec.mainClass=<MAIN CLASS> \\\n     -Dexec.args=\"<EXAMPLE-SPECIFIC ARGUMENTS>\"\n \n-For example, you can execute the `WordCount` pipeline on your local machine as follows:\n+Alternatively, you may choose to bundle all dependencies into a single JAR and\n+execute it outside of the Maven environment.\n+\n+### Direct Runner\n+\n+You can execute the `WordCount` pipeline on your local machine as follows:\n \n     mvn compile exec:java \\\n     -Dexec.mainClass=org.apache.beam.examples.WordCount \\\n     -Dexec.args=\"--inputFile=<LOCAL INPUT FILE> --output=<LOCAL OUTPUT FILE>\"\n \n-Once you have followed the general Cloud Dataflow\n-[Getting Started](https://cloud.google.com/dataflow/getting-started) instructions, you can execute\n-the same pipeline on fully managed resources in Google Cloud Platform:\n+To create the bundled JAR of the examples and execute it locally:\n+\n+    mvn package\n+\n+    java -cp examples/java/target/beam-examples-java-bundled-<VERSION>.jar \\\n+    org.apache.beam.examples.WordCount \\\n+    --inputFile=<INPUT FILE PATTERN> --output=<OUTPUT FILE>\n+\n+### Google Cloud Dataflow Runner\n+\n+After you have followed the general Cloud Dataflow\n+[prerequisites and setup](https://beam.apache.org/documentation/runners/dataflow/), you can execute\n+the pipeline on fully managed resources in Google Cloud Platform:\n \n     mvn compile exec:java \\\n     -Dexec.mainClass=org.apache.beam.examples.WordCount \\\n     -Dexec.args=\"--project=<YOUR CLOUD PLATFORM PROJECT ID> \\\n     --tempLocation=<YOUR CLOUD STORAGE LOCATION> \\\n-    --runner=BlockingDataflowRunner\"\n+    --runner=DataflowRunner\"\n \n Make sure to use your project id, not the project number or the descriptive name.\n-The Cloud Storage location should be entered in the form of\n+The Google Cloud Storage location should be entered in the form of\n `gs://bucket/path/to/staging/directory`.\n \n-Alternatively, you may choose to bundle all dependencies into a single JAR and\n-execute it outside of the Maven environment. For example, you can execute the\n-following commands to create the\n-bundled JAR of the examples and execute it both locally and in Cloud\n-Platform:\n+To create the bundled JAR of the examples and execute it in Google Cloud Platform:\n \n     mvn package\n \n-    java -cp examples/java/target/beam-examples-java-bundled-<VERSION>.jar \\\n-    org.apache.beam.examples.WordCount \\\n-    --inputFile=<INPUT FILE PATTERN> --output=<OUTPUT FILE>\n-\n     java -cp examples/java/target/beam-examples-java-bundled-<VERSION>.jar \\\n     org.apache.beam.examples.WordCount \\\n     --project=<YOUR CLOUD PLATFORM PROJECT ID> \\\n     --tempLocation=<YOUR CLOUD STORAGE LOCATION> \\\n-    --runner=BlockingDataflowRunner\n+    --runner=DataflowRunner\n+\n+## Other Examples\n \n Other examples can be run similarly by replacing the `WordCount` class path with the example classpath, e.g.\n-`org.apache.beam.examples.cookbook.BigQueryTornadoes`,\n+`org.apache.beam.examples.cookbook.CombinePerKeyExamples`,\n and adjusting runtime options under the `Dexec.args` parameter, as specified in\n the example itself.\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/README.md",
                "sha": "d891fb87842d35c9a1d63ed84b3b50fdc123ec4e",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/pom.xml",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/pom.xml",
                "patch": "@@ -280,6 +280,31 @@\n                   </systemPropertyVariables>\n                 </configuration>\n               </execution>\n+              <execution>\n+                <id>dataflow-runner-integration-tests-streaming</id>\n+                <goals>\n+                  <goal>integration-test</goal>\n+                  <goal>verify</goal>\n+                </goals>\n+                <configuration>\n+                  <includes>\n+                    <include>WordCountIT.java</include>\n+                    <include>WindowedWordCountIT.java</include>\n+                  </includes>\n+                  <parallel>all</parallel>\n+                  <threadCount>4</threadCount>\n+                  <systemPropertyVariables>\n+                    <beamTestPipelineOptions>\n+                      [\n+                      \"--project=apache-beam-testing\",\n+                      \"--tempRoot=gs://temp-storage-for-end-to-end-tests\",\n+                      \"--runner=org.apache.beam.runners.dataflow.testing.TestDataflowRunner\",\n+                      \"--streaming=true\"\n+                      ]\n+                    </beamTestPipelineOptions>\n+                  </systemPropertyVariables>\n+                </configuration>\n+              </execution>\n               <execution>\n                 <id>apex-runner-integration-tests</id>\n                 <goals>\n@@ -446,6 +471,11 @@\n       <artifactId>beam-sdks-java-core</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-java-extensions-gcp-core</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\n@@ -525,7 +555,7 @@\n \n     <!--\n       For testing the example itself, use the direct runner. This is separate from\n-      the use of RunnableOnService tests for testing a particular runner.\n+      the use of ValidatesRunner tests for testing a particular runner.\n     -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/pom.xml",
                "sha": "96d6917fe17999c9f40bceb6c02d356884f19832",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java",
                "patch": "@@ -151,7 +151,7 @@ public static void main(String[] args) {\n      * <p>Below we verify that the set of filtered words matches our expected counts. Note\n      * that PAssert does not provide any output and that successful completion of the\n      * Pipeline implies that the expectations were met. Learn more at\n-     * https://cloud.google.com/dataflow/pipelines/testing-your-pipeline on how to test\n+     * https://beam.apache.org/documentation/pipelines/test-your-pipeline/ on how to test\n      * your Pipeline and see {@link DebuggingWordCountTest} for an example unit test.\n      */\n     List<KV<String, Long>> expectedResults = Arrays.asList(",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/DebuggingWordCount.java",
                "sha": "031f317ea774440e3f96a133cf6dff2e58286415",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 28,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "patch": "@@ -21,7 +21,7 @@\n import java.util.concurrent.ThreadLocalRandom;\n import org.apache.beam.examples.common.ExampleBigQueryTableOptions;\n import org.apache.beam.examples.common.ExampleOptions;\n-import org.apache.beam.examples.common.WriteWindowedFilesDoFn;\n+import org.apache.beam.examples.common.WriteOneFilePerWindow;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.io.TextIO;\n@@ -31,11 +31,9 @@\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.MapElements;\n import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -203,33 +201,13 @@ public static void main(String[] args) throws IOException {\n     PCollection<KV<String, Long>> wordCounts = windowedWords.apply(new WordCount.CountWords());\n \n     /**\n-     * Concept #5: Customize the output format using windowing information\n-     *\n-     * <p>At this point, the data is organized by window. We're writing text files and and have no\n-     * late data, so for simplicity we can use the window as the key and {@link GroupByKey} to get\n-     * one output file per window. (if we had late data this key would not be unique)\n-     *\n-     * <p>To access the window in a {@link DoFn}, add a {@link BoundedWindow} parameter. This will\n-     * be automatically detected and populated with the window for the current element.\n-     */\n-    PCollection<KV<IntervalWindow, KV<String, Long>>> keyedByWindow =\n-        wordCounts.apply(\n-            ParDo.of(\n-                new DoFn<KV<String, Long>, KV<IntervalWindow, KV<String, Long>>>() {\n-                  @ProcessElement\n-                  public void processElement(ProcessContext context, IntervalWindow window) {\n-                    context.output(KV.of(window, context.element()));\n-                  }\n-                }));\n-\n-    /**\n-     * Concept #6: Format the results and write to a sharded file partitioned by window, using a\n+     * Concept #5: Format the results and write to a sharded file partitioned by window, using a\n      * simple ParDo operation. Because there may be failures followed by retries, the\n      * writes must be idempotent, but the details of writing to files is elided here.\n      */\n-    keyedByWindow\n-        .apply(GroupByKey.<IntervalWindow, KV<String, Long>>create())\n-        .apply(ParDo.of(new WriteWindowedFilesDoFn(output)));\n+    wordCounts\n+        .apply(MapElements.via(new WordCount.FormatAsTextFn()))\n+        .apply(new WriteOneFilePerWindow(output));\n \n     PipelineResult result = pipeline.run();\n     try {",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "sha": "d88de543cd87a654bd3897a6379a3e1bf0648209",
                "status": "modified"
            },
            {
                "additions": 91,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java",
                "changes": 91,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java",
                "patch": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples.common;\n+\n+import org.apache.beam.sdk.io.FileBasedSink.FilenamePolicy;\n+import org.apache.beam.sdk.io.TextIO;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PDone;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.joda.time.format.ISODateTimeFormat;\n+\n+/**\n+ * A {@link DoFn} that writes elements to files with names deterministically derived from the lower\n+ * and upper bounds of their key (an {@link IntervalWindow}).\n+ *\n+ * <p>This is test utility code, not for end-users, so examples can be focused on their primary\n+ * lessons.\n+ */\n+public class WriteOneFilePerWindow extends PTransform<PCollection<String>, PDone> {\n+\n+  private static DateTimeFormatter formatter = ISODateTimeFormat.hourMinute();\n+  private String filenamePrefix;\n+\n+  public WriteOneFilePerWindow(String filenamePrefix) {\n+    this.filenamePrefix = filenamePrefix;\n+  }\n+\n+  @Override\n+  public PDone expand(PCollection<String> input) {\n+    return input.apply(\n+        TextIO.Write.to(new PerWindowFiles(filenamePrefix)).withWindowedWrites().withNumShards(3));\n+  }\n+\n+  /**\n+   * A {@link FilenamePolicy} produces a base file name for a write based on metadata about the data\n+   * being written. This always includes the shard number and the total number of shards. For\n+   * windowed writes, it also includes the window and pane index (a sequence number assigned to each\n+   * trigger firing).\n+   */\n+  public static class PerWindowFiles extends FilenamePolicy {\n+\n+    private final String output;\n+\n+    public PerWindowFiles(String output) {\n+      this.output = output;\n+    }\n+\n+    @Override\n+    public ValueProvider<String> getBaseOutputFilenameProvider() {\n+      return StaticValueProvider.of(output);\n+    }\n+\n+    public String   filenamePrefixForWindow(IntervalWindow window) {\n+      return String.format(\n+          \"%s-%s-%s\", output, formatter.print(window.start()), formatter.print(window.end()));\n+    }\n+\n+    @Override\n+    public String windowedFilename(WindowedContext context) {\n+      IntervalWindow window = (IntervalWindow) context.getWindow();\n+      return String.format(\n+          \"%s-%s-of-%s\",\n+          filenamePrefixForWindow(window), context.getShardNumber(), context.getNumShards());\n+    }\n+\n+    @Override\n+    public String unwindowedFilename(Context context) {\n+      throw new UnsupportedOperationException(\"Unsupported.\");\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/common/WriteOneFilePerWindow.java",
                "sha": "2ed8a741de69be422417fd93ccfedc8bc081e622",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 77,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "patch": "@@ -1,77 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.examples.common;\n-\n-import com.google.common.annotations.VisibleForTesting;\n-import java.io.OutputStream;\n-import java.nio.channels.Channels;\n-import java.nio.charset.StandardCharsets;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n-import org.apache.beam.sdk.util.IOChannelFactory;\n-import org.apache.beam.sdk.util.IOChannelUtils;\n-import org.apache.beam.sdk.values.KV;\n-import org.joda.time.format.DateTimeFormatter;\n-import org.joda.time.format.ISODateTimeFormat;\n-\n-/**\n- * A {@link DoFn} that writes elements to files with names deterministically derived from the lower\n- * and upper bounds of their key (an {@link IntervalWindow}).\n- *\n- * <p>This is test utility code, not for end-users, so examples can be focused\n- * on their primary lessons.\n- */\n-public class WriteWindowedFilesDoFn\n-    extends DoFn<KV<IntervalWindow, Iterable<KV<String, Long>>>, Void> {\n-\n-  static final byte[] NEWLINE = \"\\n\".getBytes(StandardCharsets.UTF_8);\n-  static final Coder<String> STRING_CODER = StringUtf8Coder.of();\n-\n-  private static DateTimeFormatter formatter = ISODateTimeFormat.hourMinute();\n-\n-  private final String output;\n-\n-  public WriteWindowedFilesDoFn(String output) {\n-    this.output = output;\n-  }\n-\n-  @VisibleForTesting\n-  public static String fileForWindow(String output, IntervalWindow window) {\n-    return String.format(\n-        \"%s-%s-%s\", output, formatter.print(window.start()), formatter.print(window.end()));\n-  }\n-\n-  @ProcessElement\n-  public void processElement(ProcessContext context) throws Exception {\n-    // Build a file name from the window\n-    IntervalWindow window = context.element().getKey();\n-    String outputShard = fileForWindow(output, window);\n-\n-    // Open the file and write all the values\n-    IOChannelFactory factory = IOChannelUtils.getFactory(outputShard);\n-    OutputStream out = Channels.newOutputStream(factory.create(outputShard, \"text/plain\"));\n-    for (KV<String, Long> wordCount : context.element().getValue()) {\n-      STRING_CODER.encode(\n-          wordCount.getKey() + \": \" + wordCount.getValue(), out, Coder.Context.OUTER);\n-      out.write(NEWLINE);\n-    }\n-    out.close();\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "sha": "cd6baad44292018ac6c4d8a839c14197ffeb752e",
                "status": "removed"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "patch": "@@ -491,7 +491,7 @@ public static void main(String[] args) throws IOException {\n \n       toWrite\n         .apply(ParDo.of(new FormatForBigquery()))\n-        .apply(BigQueryIO.Write\n+        .apply(BigQueryIO.writeTableRows()\n                .to(tableRef)\n                .withSchema(FormatForBigquery.getSchema())\n                .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "sha": "e6621cead488497e6e24a1abf536fea9eef4fdfc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java",
                "patch": "@@ -136,7 +136,7 @@ public static void main(String[] args) throws IOException {\n         .apply(ParDo.of(new ExtractWords()))\n         .apply(ParDo.of(new Uppercase()))\n         .apply(ParDo.of(new StringToRowConverter()))\n-        .apply(BigQueryIO.Write.to(tableSpec)\n+        .apply(BigQueryIO.writeTableRows().to(tableSpec)\n             .withSchema(StringToRowConverter.getSchema()));\n \n     PipelineResult result = pipeline.run();",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java",
                "sha": "20cee01ad1e60e5042a8131c54e09379eb568563",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "patch": "@@ -323,7 +323,6 @@ public void processElement(ProcessContext c) {\n       // presented to each invocation of the DoFn.\n       PCollection<KV<String, Double>> wordToDf = wordToDocCount\n           .apply(\"ComputeDocFrequencies\", ParDo\n-              .withSideInputs(totalDocuments)\n               .of(new DoFn<KV<String, Long>, KV<String, Double>>() {\n                 @ProcessElement\n                 public void processElement(ProcessContext c) {\n@@ -335,7 +334,7 @@ public void processElement(ProcessContext c) {\n \n                   c.output(KV.of(word, documentFrequency));\n                 }\n-              }));\n+              }).withSideInputs(totalDocuments));\n \n       // Join the term frequency and document frequency\n       // collections, each keyed on the word.",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "sha": "f7904d3ea27a5c079c295f84e2dd02c52450fab4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "patch": "@@ -348,7 +348,7 @@ public static void main(String[] args) throws IOException {\n             Duration.standardMinutes(options.getWindowDuration())).\n             every(Duration.standardMinutes(options.getWindowSlideEvery()))))\n         .apply(new MaxLaneFlow())\n-        .apply(BigQueryIO.Write.to(tableRef)\n+        .apply(BigQueryIO.writeTableRows().to(tableRef)\n             .withSchema(FormatMaxesFn.getSchema()));\n \n     // Run the pipeline.",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "sha": "e57da93b0262cefd68971e7d2457165ce0acea08",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "patch": "@@ -359,7 +359,7 @@ public static void main(String[] args) throws IOException {\n             Duration.standardMinutes(options.getWindowDuration())).\n             every(Duration.standardMinutes(options.getWindowSlideEvery()))))\n         .apply(new TrackSpeed())\n-        .apply(BigQueryIO.Write.to(tableRef)\n+        .apply(BigQueryIO.writeTableRows().to(tableRef)\n             .withSchema(FormatStatsFn.getSchema()));\n \n     // Run the pipeline.",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "sha": "b1f938bca07aa4f01f5ff934e6f6d2a6536ac1ea",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 6,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "patch": "@@ -156,13 +156,13 @@ public static void main(String[] args) {\n     fields.add(new TableFieldSchema().setName(\"tornado_count\").setType(\"INTEGER\"));\n     TableSchema schema = new TableSchema().setFields(fields);\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(new CountTornadoes())\n-     .apply(BigQueryIO.Write\n-        .to(options.getOutput())\n-        .withSchema(schema)\n-        .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)\n-        .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));\n+     .apply(BigQueryIO.writeTableRows()\n+         .to(options.getOutput())\n+         .withSchema(schema)\n+         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)\n+         .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE));\n \n     p.run().waitUntilFinish();\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "sha": "07a3eddde88b48544eb36ade835d34f74a6efac0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "patch": "@@ -200,9 +200,9 @@ public static void main(String[] args)\n     fields.add(new TableFieldSchema().setName(\"all_plays\").setType(\"STRING\"));\n     TableSchema schema = new TableSchema().setFields(fields);\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(new PlaysForWord())\n-     .apply(BigQueryIO.Write\n+     .apply(BigQueryIO.writeTableRows()\n         .to(options.getOutput())\n         .withSchema(schema)\n         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "sha": "8d13b90a72b05ddb33956b36de4e32c5a54ab435",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "patch": "@@ -175,7 +175,6 @@ public BelowGlobalMean(Integer monthFilter) {\n       // We'll only output readings with temperatures below this mean.\n       PCollection<TableRow> filteredRows = monthFilteredRows\n           .apply(\"ParseAndFilter\", ParDo\n-              .withSideInputs(globalMeanTemp)\n               .of(new DoFn<TableRow, TableRow>() {\n                 @ProcessElement\n                 public void processElement(ProcessContext c) {\n@@ -185,7 +184,7 @@ public void processElement(ProcessContext c) {\n                     c.output(c.element());\n                   }\n                 }\n-              }));\n+              }).withSideInputs(globalMeanTemp));\n \n       return filteredRows;\n     }\n@@ -238,10 +237,10 @@ public static void main(String[] args)\n \n     TableSchema schema = buildWeatherSchemaProjection();\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(ParDo.of(new ProjectionFn()))\n      .apply(new BelowGlobalMean(options.getMonthFilter()))\n-     .apply(BigQueryIO.Write\n+     .apply(BigQueryIO.writeTableRows()\n         .to(options.getOutput())\n         .withSchema(schema)\n         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "sha": "fed9db79d1b274cfe9c91c34e616e8b25a035aca",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java",
                "patch": "@@ -166,8 +166,8 @@ public static void main(String[] args) throws Exception {\n     Pipeline p = Pipeline.create(options);\n     // the following two 'applys' create multiple inputs to our pipeline, one for each\n     // of our two input sources.\n-    PCollection<TableRow> eventsTable = p.apply(BigQueryIO.Read.from(GDELT_EVENTS_TABLE));\n-    PCollection<TableRow> countryCodes = p.apply(BigQueryIO.Read.from(COUNTRY_CODES));\n+    PCollection<TableRow> eventsTable = p.apply(BigQueryIO.read().from(GDELT_EVENTS_TABLE));\n+    PCollection<TableRow> countryCodes = p.apply(BigQueryIO.read().from(COUNTRY_CODES));\n     PCollection<String> formattedResults = joinEvents(eventsTable, countryCodes);\n     formattedResults.apply(TextIO.Write.to(options.getOutput()));\n     p.run().waitUntilFinish();",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/JoinExamples.java",
                "sha": "05a3ad34eb4ac16615ef57135d6834376c3085af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "patch": "@@ -149,9 +149,9 @@ public static void main(String[] args)\n     fields.add(new TableFieldSchema().setName(\"max_mean_temp\").setType(\"FLOAT\"));\n     TableSchema schema = new TableSchema().setFields(fields);\n \n-    p.apply(BigQueryIO.Read.from(options.getInput()))\n+    p.apply(BigQueryIO.read().from(options.getInput()))\n      .apply(new MaxMeanTemp())\n-     .apply(BigQueryIO.Write\n+     .apply(BigQueryIO.writeTableRows()\n         .to(options.getOutput())\n         .withSchema(schema)\n         .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "sha": "295b3f4a0e9c9dc4d14eb50dfd63f5bd31955fd6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md",
                "patch": "@@ -21,7 +21,7 @@\n \n This directory holds simple \"cookbook\" examples, which show how to define\n commonly-used data analysis patterns that you would likely incorporate into a\n-larger Dataflow pipeline. They include:\n+larger Apache Beam pipeline. They include:\n \n  <ul>\n   <li><a href=\"https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java\">BigQueryTornadoes</a>",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/README.md",
                "sha": "b167cd7a937b96acc909ad0ca4c58ae8882d0a3f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "patch": "@@ -452,7 +452,9 @@ public static void main(String[] args) throws Exception {\n         .apply(new CalculateTotalFlow(options.getWindowDuration()));\n \n     for (int i = 0; i < resultList.size(); i++){\n-      resultList.get(i).apply(BigQueryIO.Write.to(tableRef).withSchema(getSchema()));\n+      resultList.get(i).apply(BigQueryIO.writeTableRows()\n+          .to(tableRef)\n+          .withSchema(getSchema()));\n     }\n \n     PipelineResult result = pipeline.run();",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "sha": "0b5d9adc637fda50265d759599bbc00a4f22fe04",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 15,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "patch": "@@ -23,13 +23,14 @@\n import com.google.common.base.MoreObjects;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Lists;\n+import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Date;\n import java.util.List;\n import java.util.SortedMap;\n import java.util.TreeMap;\n import java.util.concurrent.ThreadLocalRandom;\n-import org.apache.beam.examples.common.WriteWindowedFilesDoFn;\n+import org.apache.beam.examples.common.WriteOneFilePerWindow.PerWindowFiles;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.options.StreamingOptions;\n@@ -42,6 +43,7 @@\n import org.apache.beam.sdk.util.ExplicitShardedFile;\n import org.apache.beam.sdk.util.FluentBackoff;\n import org.apache.beam.sdk.util.IOChannelUtils;\n+import org.apache.beam.sdk.util.NumberedShardedFile;\n import org.apache.beam.sdk.util.ShardedFile;\n import org.hamcrest.Description;\n import org.hamcrest.TypeSafeMatcher;\n@@ -64,7 +66,7 @@\n   @Rule public TestName testName = new TestName();\n \n   private static final String DEFAULT_INPUT =\n-      \"gs://apache-beam-samples/shakespeare/winterstale-personae\";\n+      \"gs://apache-beam-samples/shakespeare/sonnets.txt\";\n   static final int MAX_READ_RETRIES = 4;\n   static final Duration DEFAULT_SLEEP_DURATION = Duration.standardSeconds(10L);\n   static final FluentBackoff BACK_OFF_FACTORY =\n@@ -130,14 +132,18 @@ private void testWindowedWordCountPipeline(WindowedWordCountITOptions options) t\n \n     String outputPrefix = options.getOutput();\n \n-    List<String> expectedOutputFiles = Lists.newArrayListWithCapacity(6);\n+    PerWindowFiles filenamePolicy = new PerWindowFiles(outputPrefix);\n+\n+    List<ShardedFile> expectedOutputFiles = Lists.newArrayListWithCapacity(6);\n+\n     for (int startMinute : ImmutableList.of(0, 10, 20, 30, 40, 50)) {\n-      Instant windowStart =\n+      final Instant windowStart =\n           new Instant(options.getMinTimestampMillis()).plus(Duration.standardMinutes(startMinute));\n       expectedOutputFiles.add(\n-          WriteWindowedFilesDoFn.fileForWindow(\n-              outputPrefix,\n-              new IntervalWindow(windowStart, windowStart.plus(Duration.standardMinutes(10)))));\n+          new NumberedShardedFile(\n+              filenamePolicy.filenamePrefixForWindow(\n+                  new IntervalWindow(\n+                      windowStart, windowStart.plus(Duration.standardMinutes(10)))) + \"*\"));\n     }\n \n     ShardedFile inputFile = new ExplicitShardedFile(Collections.singleton(options.getInputFile()));\n@@ -157,7 +163,7 @@ private void testWindowedWordCountPipeline(WindowedWordCountITOptions options) t\n     }\n \n     options.setOnSuccessMatcher(\n-        new WordCountsMatcher(expectedWordCounts, new ExplicitShardedFile(expectedOutputFiles)));\n+        new WordCountsMatcher(expectedWordCounts, expectedOutputFiles));\n \n     WindowedWordCount.main(TestPipeline.convertToArgs(options));\n   }\n@@ -172,24 +178,28 @@ private void testWindowedWordCountPipeline(WindowedWordCountITOptions options) t\n     private static final Logger LOG = LoggerFactory.getLogger(FileChecksumMatcher.class);\n \n     private final SortedMap<String, Long> expectedWordCounts;\n-    private final ShardedFile outputFile;\n+    private final List<ShardedFile> outputFiles;\n     private SortedMap<String, Long> actualCounts;\n \n-    public WordCountsMatcher(SortedMap<String, Long> expectedWordCounts, ShardedFile outputFile) {\n+    public WordCountsMatcher(\n+        SortedMap<String, Long> expectedWordCounts, List<ShardedFile> outputFiles) {\n       this.expectedWordCounts = expectedWordCounts;\n-      this.outputFile = outputFile;\n+      this.outputFiles = outputFiles;\n     }\n \n     @Override\n     public boolean matchesSafely(PipelineResult pipelineResult) {\n       try {\n         // Load output data\n-        List<String> lines =\n-            outputFile.readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff());\n+        List<String> outputLines = new ArrayList<>();\n+        for (ShardedFile outputFile : outputFiles) {\n+          outputLines.addAll(\n+              outputFile.readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff()));\n+        }\n \n         // Since the windowing is nondeterministic we only check the sums\n         actualCounts = new TreeMap<>();\n-        for (String line : lines) {\n+        for (String line : outputLines) {\n           String[] splits = line.split(\": \");\n           String word = splits[0];\n           long count = Long.parseLong(splits[1]);\n@@ -205,7 +215,8 @@ public boolean matchesSafely(PipelineResult pipelineResult) {\n         return actualCounts.equals(expectedWordCounts);\n       } catch (Exception e) {\n         throw new RuntimeException(\n-            String.format(\"Failed to read from sharded output: %s\", outputFile));\n+            String.format(\"Failed to read from sharded output: %s due to exception\",\n+                outputFiles), e);\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "sha": "857f1d3eb5cacfef3ac539e1eb19293f9f9e63aa",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java",
                "patch": "@@ -24,8 +24,8 @@\n import org.apache.beam.examples.WordCount.FormatAsTextFn;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.DoFnTester;\n@@ -73,7 +73,7 @@ public void testExtractWordsFn() throws Exception {\n \n   /** Example test that tests a PTransform by using an in-memory input and inspecting the output. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testCountWords() throws Exception {\n     PCollection<String> input = p.apply(Create.of(WORDS).withCoder(StringUtf8Coder.of()));\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/WordCountTest.java",
                "sha": "54ce1e31beb186158f49bb44b3a24a1626413efa",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java",
                "patch": "@@ -21,8 +21,8 @@\n import java.util.Arrays;\n import org.apache.beam.sdk.coders.StringDelegateCoder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Distinct;\n import org.apache.beam.sdk.transforms.Keys;\n@@ -45,7 +45,7 @@\n \n   /** Test that the example runs. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testTfIdf() throws Exception {\n \n     pipeline.getCoderRegistry().registerCoder(URI.class, StringDelegateCoder.of(URI.class));",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TfIdfTest.java",
                "sha": "d263643807a0a8c203751520ca27f214d9da2ba3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java",
                "patch": "@@ -20,8 +20,8 @@\n import com.google.api.services.bigquery.model.TableRow;\n import java.util.Arrays;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.values.PCollection;\n import org.junit.Rule;\n@@ -38,7 +38,7 @@\n   public TestPipeline p = TestPipeline.create();\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testComputeTopUsers() {\n \n     PCollection<String> output =",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/complete/TopWikipediaSessionsTest.java",
                "sha": "5415281ac4555381cd738dfad094ee49227911d2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java",
                "patch": "@@ -21,8 +21,8 @@\n import java.util.List;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Distinct;\n import org.apache.beam.sdk.values.PCollection;\n@@ -40,7 +40,7 @@\n   public TestPipeline p = TestPipeline.create();\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testDistinct() {\n     List<String> strings = Arrays.asList(\n         \"k1\",\n@@ -64,7 +64,7 @@ public void testDistinct() {\n   }\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testDistinctEmpty() {\n     List<String> strings = Arrays.asList();\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/DistinctExampleTest.java",
                "sha": "c9dab801ae7134545161103cd4e9184c0fc91317",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java",
                "patch": "@@ -23,8 +23,8 @@\n import org.apache.beam.examples.cookbook.JoinExamples.ExtractCountryInfoFn;\n import org.apache.beam.examples.cookbook.JoinExamples.ExtractEventDataFn;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFnTester;\n import org.apache.beam.sdk.values.KV;\n@@ -103,7 +103,7 @@ public void testExtractCountryInfoFn() throws Exception {\n \n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testJoin() throws java.lang.Exception {\n     PCollection<TableRow> input1 = p.apply(\"CreateEvent\", Create.of(EVENT_ARRAY));\n     PCollection<TableRow> input2 = p.apply(\"CreateCC\", Create.of(CC_ARRAY));",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/JoinExamplesTest.java",
                "sha": "b2fcd736e4591401b9df510ad1e051467b05329b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java",
                "patch": "@@ -27,8 +27,8 @@\n import org.apache.beam.examples.cookbook.TriggerExample.ExtractFlowInfo;\n import org.apache.beam.examples.cookbook.TriggerExample.TotalFlow;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.DoFnTester;\n@@ -111,7 +111,7 @@ public void testExtractTotalFlow() throws Exception {\n   }\n \n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testTotalFlow () {\n     PCollection<KV<String, Integer>> flow = pipeline\n         .apply(Create.timestamped(TIME_STAMPED_INPUT))",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java/src/test/java/org/apache/beam/examples/cookbook/TriggerExampleTest.java",
                "sha": "706cfb91b435132b46e092a8f27801c059d58961",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/pom.xml",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java8/pom.xml",
                "patch": "@@ -153,6 +153,40 @@\n         </configuration>\n       </plugin>\n \n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-shade-plugin</artifactId>\n+        <executions>\n+          <execution>\n+            <phase>package</phase>\n+            <goals>\n+              <goal>shade</goal>\n+            </goals>\n+            <configuration>\n+              <finalName>${project.artifactId}-bundled-${project.version}</finalName>\n+              <artifactSet>\n+                <includes>\n+                  <include>*:*</include>\n+                </includes>\n+              </artifactSet>\n+              <filters>\n+                <filter>\n+                  <artifact>*:*</artifact>\n+                  <excludes>\n+                    <exclude>META-INF/*.SF</exclude>\n+                    <exclude>META-INF/*.DSA</exclude>\n+                    <exclude>META-INF/*.RSA</exclude>\n+                  </excludes>\n+                </filter>\n+              </filters>\n+              <transformers>\n+                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n+              </transformers>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+\n       <plugin>\n         <groupId>org.apache.maven.plugins</groupId>\n         <artifactId>maven-jar-plugin</artifactId>\n@@ -172,6 +206,11 @@\n       <artifactId>beam-sdks-java-core</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-java-extensions-gcp-core</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\n@@ -257,7 +296,7 @@\n \n     <!--\n       For testing the example itself, use the direct runner. This is separate from\n-      the use of RunnableOnService tests for testing a particular runner.\n+      the use of ValidatesRunner tests for testing a particular runner.\n     -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/pom.xml",
                "sha": "cd69acbd3cbc412132f567a8b7fd473cc3fcbd37",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java",
                "patch": "@@ -56,13 +56,14 @@ public static void main(String[] args) {\n     Pipeline p = Pipeline.create(options);\n \n     p.apply(TextIO.Read.from(\"gs://apache-beam-samples/shakespeare/*\"))\n-     .apply(FlatMapElements.via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\")))\n-         .withOutputType(TypeDescriptors.strings()))\n+     .apply(FlatMapElements\n+         .into(TypeDescriptors.strings())\n+         .via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\"))))\n      .apply(Filter.by((String word) -> !word.isEmpty()))\n      .apply(Count.<String>perElement())\n      .apply(MapElements\n-         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue())\n-         .withOutputType(TypeDescriptors.strings()))\n+         .into(TypeDescriptors.strings())\n+         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue()))\n \n      // CHANGE 3/3: The Google Cloud Storage path is required for outputting the results to.\n      .apply(TextIO.Write.to(\"gs://YOUR_OUTPUT_BUCKET/AND_OUTPUT_PREFIX\"));",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/MinimalWordCountJava8.java",
                "sha": "f424a7b7296439294a306c83e90019bfdb43e574",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 9,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "patch": "@@ -25,7 +25,7 @@\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.PubsubIO;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n import org.apache.beam.sdk.options.Default;\n import org.apache.beam.sdk.options.Description;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n@@ -125,7 +125,6 @@\n       PCollection<KV<String, Integer>> filtered = sumScores\n           .apply(\"ProcessAndFilter\", ParDo\n               // use the derived mean total score as a side input\n-              .withSideInputs(globalMeanScore)\n               .of(new DoFn<KV<String, Integer>, KV<String, Integer>>() {\n                 private final Aggregator<Long, Long> numSpammerUsers =\n                   createAggregator(\"SpammerUsers\", Sum.ofLongs());\n@@ -140,7 +139,7 @@ public void processElement(ProcessContext c) {\n                     c.output(c.element());\n                   }\n                 }\n-              }));\n+              }).withSideInputs(globalMeanScore));\n       return filtered;\n     }\n   }\n@@ -261,9 +260,9 @@ public static void main(String[] args) throws Exception {\n     // Extract username/score pairs from the event stream\n     PCollection<KV<String, Integer>> userEvents =\n         rawEvents.apply(\"ExtractUserScore\",\n-          MapElements.via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore()))\n-            .withOutputType(\n-                TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())));\n+          MapElements\n+              .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+              .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore())));\n \n     // Calculate the total score per user over fixed windows, and\n     // cumulative updates for late data.\n@@ -288,7 +287,6 @@ public static void main(String[] args) throws Exception {\n           FixedWindows.of(Duration.standardMinutes(options.getFixedWindowDuration()))))\n       // Filter out the detected spammer users, using the side input derived above.\n       .apply(\"FilterOutSpammers\", ParDo\n-              .withSideInputs(spammersView)\n               .of(new DoFn<GameActionInfo, GameActionInfo>() {\n                 @ProcessElement\n                 public void processElement(ProcessContext c) {\n@@ -297,8 +295,8 @@ public void processElement(ProcessContext c) {\n                     c.output(c.element());\n                   }\n                 }\n-              }))\n-      // Extract and sum teamname/score pairs from the event data.\n+              }).withSideInputs(spammersView))\n+        // Extract and sum teamname/score pairs from the event data.\n       .apply(\"ExtractTeamScore\", new ExtractAndSumScore(\"team\"))\n       // [END DocInclude_FilterAndCalc]\n       // Write the result to BigQuery",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "sha": "9c79fad4a77abdeef7ca13825e726f58829e1178",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "patch": "@@ -28,7 +28,7 @@\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.PubsubIO;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n import org.apache.beam.sdk.options.Default;\n import org.apache.beam.sdk.options.Description;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "sha": "96f42919e7028ac9c149361e6d9f5a5ff409bd87",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md",
                "patch": "@@ -20,10 +20,10 @@\n # 'Gaming' examples\n \n \n-This directory holds a series of example Dataflow pipelines in a simple 'mobile\n+This directory holds a series of example Apache Beam pipelines in a simple 'mobile\n gaming' domain. They all require Java 8.  Each pipeline successively introduces\n new concepts, and gives some examples of using Java 8 syntax in constructing\n-Dataflow pipelines. Other than usage of Java 8 lambda expressions, the concepts\n+Beam pipelines. Other than usage of Java 8 lambda expressions, the concepts\n that are used apply equally well in Java 7.\n \n In the gaming scenario, many users play, as members of different teams, over\n@@ -58,7 +58,7 @@ the day's cutoff point.\n \n The next pipeline in the series is `HourlyTeamScore`. This pipeline also\n processes data collected from gaming events in batch. It builds on `UserScore`,\n-but uses [fixed windows](https://cloud.google.com/dataflow/model/windowing), by\n+but uses [fixed windows](https://beam.apache.org/documentation/programming-guide/#windowing), by\n default an hour in duration. It calculates the sum of scores per team, for each\n window, optionally allowing specification of two timestamps before and after\n which data is filtered out. This allows a model where late data collected after",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/README.md",
                "sha": "fdce05cd240565bf3c5d27cabfe5488098ecee19",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "patch": "@@ -165,9 +165,8 @@ public void processElement(ProcessContext c) {\n \n       return gameInfo\n         .apply(MapElements\n-            .via((GameActionInfo gInfo) -> KV.of(gInfo.getKey(field), gInfo.getScore()))\n-            .withOutputType(\n-                TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())))\n+            .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+            .via((GameActionInfo gInfo) -> KV.of(gInfo.getKey(field), gInfo.getScore())))\n         .apply(Sum.<String>integersPerKey());\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "sha": "b4b023fdcd70442d883b107bc76e3140606bd6cc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java",
                "patch": "@@ -260,8 +260,7 @@ private static String generateEvent(Long currTime, int delayInMillis) {\n       user = team.getRandomUser();\n     }\n     String event = user + \",\" + teamName + \",\" + random.nextInt(MAX_SCORE);\n-    // Randomly introduce occasional parse errors. You can see a custom counter tracking the number\n-    // of such errors in the Dataflow Monitoring UI, as the example pipeline runs.\n+    // Randomly introduce occasional parse errors.\n     if (random.nextInt(parseErrorRate) == 0) {\n       System.out.println(\"Introducing a parse error.\");\n       event = \"THIS LINE REPRESENTS CORRUPT DATA AND WILL CAUSE A PARSE ERROR\";",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/injector/Injector.java",
                "sha": "b9a3ff23f77b1a03d30839fb8cef41cd9f472370",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "patch": "@@ -119,14 +119,13 @@ protected TableSchema getSchema() {\n \n   @Override\n   public PDone expand(PCollection<InputT> teamAndScore) {\n-    return teamAndScore\n+    teamAndScore\n       .apply(\"ConvertToRow\", ParDo.of(new BuildRowFn()))\n-      .apply(BigQueryIO.Write\n-                .to(getTable(teamAndScore.getPipeline(),\n-                    tableName))\n-                .withSchema(getSchema())\n-                .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n-                .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n+      .apply(BigQueryIO.writeTableRows().to(getTable(teamAndScore.getPipeline(), tableName))\n+          .withSchema(getSchema())\n+          .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n+          .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n+    return PDone.in(teamAndScore.getPipeline());\n   }\n \n   /** Utility to construct an output table reference. */",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "sha": "5eecddb4e82d6d8598cb675e2ecbb1c25763492a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import com.google.api.services.bigquery.model.TableRow;\n import java.util.Map;\n+\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;\n@@ -58,14 +59,14 @@ public void processElement(ProcessContext c, BoundedWindow window) {\n \n   @Override\n   public PDone expand(PCollection<T> teamAndScore) {\n-    return teamAndScore\n+    teamAndScore\n       .apply(\"ConvertToRow\", ParDo.of(new BuildRowFn()))\n-      .apply(BigQueryIO.Write\n-                .to(getTable(teamAndScore.getPipeline(),\n-                    tableName))\n+      .apply(BigQueryIO.writeTableRows()\n+                .to(getTable(teamAndScore.getPipeline(), tableName))\n                 .withSchema(getSchema())\n                 .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\n                 .withWriteDisposition(WriteDisposition.WRITE_APPEND));\n+    return PDone.in(teamAndScore.getPipeline());\n   }\n \n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "sha": "e602258c74fa3b6f2ac487868d2992f70055583b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java",
                "patch": "@@ -63,13 +63,14 @@ public void testMinimalWordCountJava8() throws Exception {\n     p.getOptions().as(GcsOptions.class).setGcsUtil(buildMockGcsUtil());\n \n     p.apply(TextIO.Read.from(\"gs://apache-beam-samples/shakespeare/*\"))\n-     .apply(FlatMapElements.via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\")))\n-         .withOutputType(TypeDescriptors.strings()))\n+     .apply(FlatMapElements\n+         .into(TypeDescriptors.strings())\n+         .via((String word) -> Arrays.asList(word.split(\"[^a-zA-Z']+\"))))\n      .apply(Filter.by((String word) -> !word.isEmpty()))\n      .apply(Count.<String>perElement())\n      .apply(MapElements\n-         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue())\n-         .withOutputType(TypeDescriptors.strings()))\n+         .into(TypeDescriptors.strings())\n+         .via((KV<String, Long> wordCount) -> wordCount.getKey() + \": \" + wordCount.getValue()))\n      .apply(TextIO.Write.to(\"gs://your-output-bucket/and-output-prefix\"));\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/MinimalWordCountJava8Test.java",
                "sha": "6c66d8f1b71ff01d29c43371d88944d784b7dee5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java",
                "patch": "@@ -23,8 +23,8 @@\n import org.apache.beam.examples.complete.game.GameStats.CalculateSpammyUsers;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -38,7 +38,7 @@\n  * Tests of GameStats.\n  * Because the pipeline was designed for easy readability and explanations, it lacks good\n  * modularity for testing. See our testing documentation for better ideas:\n- * https://cloud.google.com/dataflow/pipelines/testing-your-pipeline.\n+ * https://beam.apache.org/documentation/pipelines/test-your-pipeline/\n  */\n @RunWith(JUnit4.class)\n public class GameStatsTest implements Serializable {\n@@ -63,7 +63,7 @@\n \n   /** Test the calculation of 'spammy users'. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testCalculateSpammyUsers() throws Exception {\n     PCollection<KV<String, Integer>> input = p.apply(Create.of(USER_SCORES));\n     PCollection<KV<String, Integer>> output = input.apply(new CalculateSpammyUsers());",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/GameStatsTest.java",
                "sha": "44481c55df4d50d8b67baef995775b6cb3372f99",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 6,
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Filter;\n import org.apache.beam.sdk.transforms.MapElements;\n@@ -45,7 +45,7 @@\n  * Tests of HourlyTeamScore.\n  * Because the pipeline was designed for easy readability and explanations, it lacks good\n  * modularity for testing. See our testing documentation for better ideas:\n- * https://cloud.google.com/dataflow/pipelines/testing-your-pipeline.\n+ * https://beam.apache.org/documentation/pipelines/test-your-pipeline/\n  */\n @RunWith(JUnit4.class)\n public class HourlyTeamScoreTest implements Serializable {\n@@ -86,7 +86,7 @@\n \n   /** Test the filtering. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testUserScoresFilter() throws Exception {\n \n     final Instant startMinTimestamp = new Instant(1447965680000L);\n@@ -101,9 +101,8 @@ public void testUserScoresFilter() throws Exception {\n               -> gInfo.getTimestamp() > startMinTimestamp.getMillis()))\n       // run a map to access the fields in the result.\n       .apply(MapElements\n-          .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore()))\n-          .withOutputType(\n-              TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())));\n+          .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+          .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore())));\n \n       PAssert.that(output).containsInAnyOrder(FILTERED_EVENTS);\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/HourlyTeamScoreTest.java",
                "sha": "409fc923ed20901ad37d3a005793e1d3668d3b29",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": "examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.beam.examples.complete.game.UserScore.ParseEventFn;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.PAssert;\n-import org.apache.beam.sdk.testing.RunnableOnService;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.ValidatesRunner;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFnTester;\n import org.apache.beam.sdk.transforms.MapElements;\n@@ -99,7 +99,7 @@ public void testParseEventFn() throws Exception {\n \n   /** Tests ExtractAndSumScore(\"user\"). */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testUserScoreSums() throws Exception {\n \n     PCollection<String> input = p.apply(Create.of(GAME_EVENTS).withCoder(StringUtf8Coder.of()));\n@@ -117,7 +117,7 @@ public void testUserScoreSums() throws Exception {\n \n   /** Tests ExtractAndSumScore(\"team\"). */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testTeamScoreSums() throws Exception {\n \n     PCollection<String> input = p.apply(Create.of(GAME_EVENTS).withCoder(StringUtf8Coder.of()));\n@@ -135,17 +135,17 @@ public void testTeamScoreSums() throws Exception {\n \n   /** Test that bad input data is dropped appropriately. */\n   @Test\n-  @Category(RunnableOnService.class)\n+  @Category(ValidatesRunner.class)\n   public void testUserScoresBadInput() throws Exception {\n \n     PCollection<String> input = p.apply(Create.of(GAME_EVENTS2).withCoder(StringUtf8Coder.of()));\n \n     PCollection<KV<String, Integer>> extract = input\n       .apply(ParDo.of(new ParseEventFn()))\n       .apply(\n-          MapElements.via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore()))\n-          .withOutputType(\n-              TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers())));\n+          MapElements\n+              .into(TypeDescriptors.kvs(TypeDescriptors.strings(), TypeDescriptors.integers()))\n+              .via((GameActionInfo gInfo) -> KV.of(gInfo.getUser(), gInfo.getScore())));\n \n     PAssert.that(extract).empty();\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/examples/java8/src/test/java/org/apache/beam/examples/complete/game/UserScoreTest.java",
                "sha": "2eb63aa47202b3b0285ad56268633ee6f3976646",
                "status": "modified"
            },
            {
                "additions": 140,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/pom.xml",
                "changes": 161,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 21,
                "filename": "pom.xml",
                "patch": "@@ -102,6 +102,7 @@\n \n     <!-- If updating dependencies, please update any relevant javadoc offlineLinks -->\n     <apache.commons.lang.version>3.5</apache.commons.lang.version>\n+    <apache.commons.compress.version>1.9</apache.commons.compress.version>\n     <apex.kryo.version>2.24.0</apex.kryo.version>\n     <avro.version>1.8.1</avro.version>\n     <bigquery.version>v2-rev295-1.22.0</bigquery.version>\n@@ -114,28 +115,30 @@\n     <datastore.proto.version>1.3.0</datastore.proto.version>\n     <google-auto-service.version>1.0-rc2</google-auto-service.version>\n     <google-auto-value.version>1.3</google-auto-value.version>\n-    <google-auth.version>0.6.0</google-auth.version>\n+    <google-auth.version>0.6.1</google-auth.version>\n     <google-clients.version>1.22.0</google-clients.version>\n     <google-cloud-bigdataoss.version>1.4.5</google-cloud-bigdataoss.version>\n     <google-cloud-dataflow-java-proto-library-all.version>0.5.160304</google-cloud-dataflow-java-proto-library-all.version>\n     <guava.version>20.0</guava.version>\n-    <grpc.version>1.0.1</grpc.version>\n+    <grpc.version>1.2.0</grpc.version>\n+    <grpc-google-common-protos.version>0.1.0</grpc-google-common-protos.version>\n     <hamcrest.version>1.3</hamcrest.version>\n-    <jackson.version>2.7.2</jackson.version>\n+    <jackson.version>2.8.8</jackson.version>\n     <findbugs.version>3.0.1</findbugs.version>\n     <joda.version>2.4</joda.version>\n     <junit.version>4.12</junit.version>\n     <mockito.version>1.9.5</mockito.version>\n-    <netty.version>4.1.3.Final</netty.version>\n-    <os-maven-plugin.version>1.4.0.Final</os-maven-plugin.version>\n-    <protobuf.version>3.1.0</protobuf.version>\n+    <netty.version>4.1.8.Final</netty.version>\n+    <os-maven-plugin.version>1.5.0.Final</os-maven-plugin.version>\n+    <protobuf.version>3.2.0</protobuf.version>\n     <pubsub.version>v1-rev10-1.22.0</pubsub.version>\n     <slf4j.version>1.7.14</slf4j.version>\n     <spark.version>1.6.2</spark.version>\n     <stax2.version>3.1.4</stax2.version>\n     <storage.version>v1-rev71-1.22.0</storage.version>\n     <woodstox.version>4.4.1</woodstox.version>\n     <spring.version>4.3.5.RELEASE</spring.version>\n+    <groovy-maven-plugin.version>2.0</groovy-maven-plugin.version>\n     \n     <compiler.error.flag>-Werror</compiler.error.flag>\n     <compiler.default.pkginfo.flag>-Xpkginfo:always</compiler.default.pkginfo.flag>\n@@ -169,7 +172,7 @@\n               <artifactId>maven-javadoc-plugin</artifactId>\n               <executions>\n                 <execution>\n-                  <id>javadoc</id>\n+                  <id>attach-javadocs</id>\n                   <phase>package</phase>\n                   <goals>\n                     <goal>jar</goal>\n@@ -353,6 +356,12 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-extensions-gcp-core</artifactId>\n+        <version>${project.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>org.apache.beam</groupId>\n         <artifactId>beam-sdks-java-extensions-sorter</artifactId>\n@@ -365,6 +374,19 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-io-common</artifactId>\n+        <version>${project.version}</version>\n+      </dependency>\n+\n+      <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-io-common</artifactId>\n+        <classifier>tests</classifier>\n+        <version>${project.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>org.apache.beam</groupId>\n         <artifactId>beam-sdks-java-io-elasticsearch</artifactId>\n@@ -431,6 +453,12 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+\t  <dependency>\n+        <groupId>org.apache.beam</groupId>\n+        <artifactId>beam-sdks-java-io-hadoop-input-format</artifactId>\n+\t    <version>${project.version}</version>\n+      </dependency>\n+\t\n       <dependency>\n         <groupId>org.apache.beam</groupId>\n         <artifactId>beam-runners-core-construction-java</artifactId>\n@@ -491,6 +519,12 @@\n         <version>${project.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>org.apache.commons</groupId>\n+        <artifactId>commons-compress</artifactId>\n+        <version>${apache.commons.compress.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>org.apache.commons</groupId>\n         <artifactId>commons-lang3</artifactId>\n@@ -525,12 +559,12 @@\n         <groupId>io.grpc</groupId>\n         <artifactId>grpc-protobuf-lite</artifactId>\n         <version>${grpc.version}</version>\n-      </dependency>\n-\n-      <dependency>\n-        <groupId>com.google.protobuf</groupId>\n-        <artifactId>protobuf-lite</artifactId>\n-        <version>3.0.1</version>\n+        <exclusions>\n+          <exclusion>\n+            <groupId>com.google.protobuf</groupId>\n+            <artifactId>protobuf-lite</artifactId>\n+          </exclusion>\n+        </exclusions>\n       </dependency>\n \n       <dependency>\n@@ -823,6 +857,12 @@\n         <version>${protobuf.version}</version>\n       </dependency>\n \n+      <dependency>\n+        <groupId>com.google.api.grpc</groupId>\n+        <artifactId>grpc-google-common-protos</artifactId>\n+        <version>${grpc-google-common-protos.version}</version>\n+      </dependency>\n+\n       <dependency>\n         <groupId>com.fasterxml.jackson.core</groupId>\n         <artifactId>jackson-core</artifactId>\n@@ -1168,13 +1208,7 @@\n           <artifactId>versions-maven-plugin</artifactId>\n           <version>2.3</version>\n         </plugin>\n-\t\n-        <plugin>\n-          <groupId>org.codehaus.mojo</groupId>\n-          <artifactId>build-helper-maven-plugin</artifactId>\n-          <version>1.12</version>\n-        </plugin>\n-\t\n+\n         <plugin>\n           <groupId>org.codehaus.mojo</groupId>\n           <artifactId>exec-maven-plugin</artifactId>\n@@ -1317,6 +1351,51 @@\n           <groupId>org.apache.maven.plugins</groupId>\n           <artifactId>maven-shade-plugin</artifactId>\n           <version>3.0.0</version>\n+          <executions>\n+            <execution>\n+              <id>bundle-and-repackage</id>\n+              <phase>package</phase>\n+              <goals>\n+                <goal>shade</goal>\n+              </goals>\n+              <configuration>\n+                <artifactSet>\n+                  <includes>\n+                    <include>com.google.guava:guava</include>\n+                  </includes>\n+                </artifactSet>\n+                <filters>\n+                  <filter>\n+                    <artifact>*:*</artifact>\n+                    <excludes>\n+                      <exclude>META-INF/*.SF</exclude>\n+                      <exclude>META-INF/*.DSA</exclude>\n+                      <exclude>META-INF/*.RSA</exclude>\n+                    </excludes>\n+                  </filter>\n+                </filters>\n+                <relocations>\n+                  <relocation>\n+                    <pattern>com.google.common</pattern>\n+                    <!--suppress MavenModelInspection -->\n+                    <shadedPattern>\n+                      org.apache.${renderedArtifactId}.repackaged.com.google.common\n+                    </shadedPattern>\n+                  </relocation>\n+                  <relocation>\n+                    <pattern>com.google.thirdparty</pattern>\n+                    <!--suppress MavenModelInspection -->\n+                    <shadedPattern>\n+                      org.apache.${renderedArtifactId}.repackaged.com.google.thirdparty\n+                    </shadedPattern>\n+                  </relocation>\n+                </relocations>\n+                <transformers>\n+                  <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n+                </transformers>\n+              </configuration>\n+            </execution>\n+          </executions>\n         </plugin>\n \n         <plugin>\n@@ -1409,6 +1488,32 @@\n             </filesets>\n           </configuration>\n         </plugin>\n+\n+        <plugin>\n+          <groupId>org.codehaus.mojo</groupId>\n+          <artifactId>build-helper-maven-plugin</artifactId>\n+          <version>3.0.0</version>\n+          <executions>\n+            <execution>\n+              <id>render-artifact-id</id>\n+              <goals>\n+                <goal>regex-properties</goal>\n+              </goals>\n+              <phase>prepare-package</phase>\n+              <configuration>\n+                <regexPropertySettings>\n+                  <regexPropertySetting>\n+                    <name>renderedArtifactId</name>\n+                    <regex>[^A-Za-z0-9]</regex>\n+                    <replacement>.</replacement>\n+                    <value>${project.artifactId}</value>\n+                    <failIfNoMatch>false</failIfNoMatch>\n+                  </regexPropertySetting>\n+                </regexPropertySettings>\n+              </configuration>\n+            </execution>\n+          </executions>\n+        </plugin>\n       </plugins>\n     </pluginManagement>\n \n@@ -1454,9 +1559,14 @@\n                   <version>[1.7,)</version>\n                 </requireJavaVersion>\n                 <requireMavenVersion>\n-                  <!-- Keep aligned with prerequisite section below. -->\n+                  <!-- Keep aligned with preqrequisite section below. -->\n                   <version>[3.2,)</version>\n                 </requireMavenVersion>\n+                <bannedDependencies>\n+                  <excludes>\n+                    <exclude>com.google.protobuf:protobuf-lite</exclude>\n+                  </excludes>\n+                </bannedDependencies>\n               </rules>\n             </configuration>\n           </execution>\n@@ -1469,6 +1579,14 @@\n           </dependency>\n         </dependencies>\n       </plugin>\n+      <plugin>\n+        <groupId>org.codehaus.mojo</groupId>\n+        <artifactId>build-helper-maven-plugin</artifactId>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-shade-plugin</artifactId>\n+      </plugin>\n     </plugins>\n   </build>\n \n@@ -1489,6 +1607,7 @@\n       </plugin>\n     </plugins>\n   </reporting>\n+\n   <prerequisites>\n     <!-- Keep aligned with requireMavenVersion section above. -->\n     <maven>3.2</maven>",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/pom.xml",
                "sha": "2945e86dc21e1385ba23f121ec34e2e66e9e2579",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/pom.xml",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": "runners/apex/pom.xml",
                "patch": "@@ -42,7 +42,7 @@\n \n   <profiles>\n     <profile>\n-      <id>local-runnable-on-service-tests</id>\n+      <id>local-validates-runner-tests</id>\n       <activation><activeByDefault>false</activeByDefault></activation>\n       <properties>\n         <skipIntegrationTests>false</skipIntegrationTests>\n@@ -130,7 +130,7 @@\n       <scope>test</scope>\n     </dependency>\n \n-    <!-- Depend on test jar to scan for RunnableOnService tests -->\n+    <!-- Depend on test jar to scan for ValidatesRunner tests -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-core</artifactId>\n@@ -200,20 +200,21 @@\n         </configuration>\n         <executions>\n           <execution>\n-            <id>runnable-on-service-tests</id>\n+            <id>validates-runner-tests</id>\n             <phase>integration-test</phase>\n             <goals>\n               <goal>test</goal>\n             </goals>\n             <configuration>\n-              <groups>org.apache.beam.sdk.testing.RunnableOnService</groups>\n+              <groups>org.apache.beam.sdk.testing.ValidatesRunner</groups>\n               <excludedGroups>\n                 org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders,\n                 org.apache.beam.sdk.testing.UsesStatefulParDo,\n                 org.apache.beam.sdk.testing.UsesTimersInParDo,\n                 org.apache.beam.sdk.testing.UsesSplittableParDo,\n                 org.apache.beam.sdk.testing.UsesAttemptedMetrics,\n-                org.apache.beam.sdk.testing.UsesCommittedMetrics\n+                org.apache.beam.sdk.testing.UsesCommittedMetrics,\n+                org.apache.beam.sdk.testing.UsesTestStream\n               </excludedGroups>\n               <parallel>none</parallel>\n               <failIfNoTests>true</failIfNoTests>\n@@ -228,6 +229,7 @@\n                 </beamTestPipelineOptions>\n               </systemPropertyVariables>\n               <skipTests>${skipIntegrationTests}</skipTests>\n+              <threadCount>4</threadCount>\n             </configuration>\n           </execution>\n         </executions>",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/pom.xml",
                "sha": "f441e3d3217bdee7f2aafb3768ed64ddba232497",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 34,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "patch": "@@ -22,7 +22,7 @@\n import com.datatorrent.api.DAG;\n import com.datatorrent.api.StreamingApplication;\n import com.google.common.base.Throwables;\n-import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableList;\n import java.io.File;\n import java.io.IOException;\n import java.io.InputStream;\n@@ -31,7 +31,6 @@\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n-import java.util.Map;\n import java.util.Properties;\n import java.util.concurrent.atomic.AtomicReference;\n import org.apache.apex.api.EmbeddedAppLauncher;\n@@ -40,6 +39,7 @@\n import org.apache.apex.api.Launcher.LaunchMode;\n import org.apache.beam.runners.apex.translation.ApexPipelineTranslator;\n import org.apache.beam.runners.core.construction.PTransformMatchers;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.PrimitiveCreate;\n import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n import org.apache.beam.sdk.Pipeline;\n@@ -48,9 +48,9 @@\n import org.apache.beam.sdk.coders.ListCoder;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsValidator;\n-import org.apache.beam.sdk.runners.PTransformMatcher;\n-import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.runners.PTransformOverride;\n import org.apache.beam.sdk.runners.PipelineRunner;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.Combine.GloballyAsSingletonView;\n import org.apache.beam.sdk.transforms.Create;\n@@ -69,8 +69,6 @@\n  * A {@link PipelineRunner} that translates the\n  * pipeline to an Apex DAG and executes it on an Apex cluster.\n  *\n- * <p>Currently execution is always in embedded mode,\n- * launch on Hadoop cluster will be added in subsequent iteration.\n  */\n @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n public class ApexRunner extends PipelineRunner<ApexRunnerResult> {\n@@ -95,27 +93,30 @@ public static ApexRunner fromOptions(PipelineOptions options) {\n     return new ApexRunner(apexPipelineOptions);\n   }\n \n-  private Map<PTransformMatcher, PTransformOverrideFactory> getOverrides() {\n-    return ImmutableMap.<PTransformMatcher, PTransformOverrideFactory>builder()\n-        .put(PTransformMatchers.classEqualTo(Create.Values.class), new PrimitiveCreate.Factory())\n-        .put(\n-            PTransformMatchers.classEqualTo(View.AsSingleton.class),\n-            new StreamingViewAsSingleton.Factory())\n-        .put(\n-            PTransformMatchers.classEqualTo(View.AsIterable.class),\n-            new StreamingViewAsIterable.Factory())\n-        .put(\n-            PTransformMatchers.classEqualTo(Combine.GloballyAsSingletonView.class),\n-            new StreamingCombineGloballyAsSingletonView.Factory())\n+  private List<PTransformOverride> getOverrides() {\n+    return ImmutableList.<PTransformOverride>builder()\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(Create.Values.class),\n+                new PrimitiveCreate.Factory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(View.AsSingleton.class),\n+                new StreamingViewAsSingleton.Factory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(View.AsIterable.class),\n+                new StreamingViewAsIterable.Factory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(Combine.GloballyAsSingletonView.class),\n+                new StreamingCombineGloballyAsSingletonView.Factory()))\n         .build();\n   }\n \n   @Override\n   public ApexRunnerResult run(final Pipeline pipeline) {\n-    for (Map.Entry<PTransformMatcher, PTransformOverrideFactory> override :\n-        getOverrides().entrySet()) {\n-      pipeline.replace(override.getKey(), override.getValue());\n-    }\n+    pipeline.replaceAll(getOverrides());\n \n     final ApexPipelineTranslator translator = new ApexPipelineTranslator(options);\n     final AtomicReference<DAG> apexDAG = new AtomicReference<>();\n@@ -241,7 +242,7 @@ private StreamingCombineGloballyAsSingletonView(\n           .apply(Combine.globally(transform.getCombineFn())\n               .withoutDefaults().withFanout(transform.getFanout()));\n \n-      PCollectionView<OutputT> view = PCollectionViews.singletonView(combined.getPipeline(),\n+      PCollectionView<OutputT> view = PCollectionViews.singletonView(combined,\n           combined.getWindowingStrategy(), transform.getInsertDefault(),\n           transform.getInsertDefault() ? transform.getCombineFn().defaultValue() : null,\n               combined.getCoder());\n@@ -259,9 +260,15 @@ protected String getKindString() {\n             PCollection<InputT>, PCollectionView<OutputT>,\n             Combine.GloballyAsSingletonView<InputT, OutputT>> {\n       @Override\n-      public PTransform<PCollection<InputT>, PCollectionView<OutputT>> getReplacementTransform(\n-          GloballyAsSingletonView<InputT, OutputT> transform) {\n-        return new StreamingCombineGloballyAsSingletonView<>(transform);\n+      public PTransformReplacement<PCollection<InputT>, PCollectionView<OutputT>>\n+          getReplacementTransform(\n+              AppliedPTransform<\n+                      PCollection<InputT>, PCollectionView<OutputT>,\n+                      GloballyAsSingletonView<InputT, OutputT>>\n+                  transform) {\n+        return PTransformReplacement.of(\n+            PTransformReplacements.getSingletonMainInput(transform),\n+            new StreamingCombineGloballyAsSingletonView<>(transform.getTransform()));\n       }\n     }\n   }\n@@ -322,9 +329,11 @@ public T identity() {\n         extends SingleInputOutputOverrideFactory<\n             PCollection<T>, PCollectionView<T>, View.AsSingleton<T>> {\n       @Override\n-      public PTransform<PCollection<T>, PCollectionView<T>> getReplacementTransform(\n-          AsSingleton<T> transform) {\n-        return new StreamingViewAsSingleton<>(transform);\n+      public PTransformReplacement<PCollection<T>, PCollectionView<T>> getReplacementTransform(\n+          AppliedPTransform<PCollection<T>, PCollectionView<T>, AsSingleton<T>> transform) {\n+        return PTransformReplacement.of(\n+            PTransformReplacements.getSingletonMainInput(transform),\n+            new StreamingViewAsSingleton<>(transform.getTransform()));\n       }\n     }\n   }\n@@ -337,8 +346,8 @@ private StreamingViewAsIterable() {}\n \n     @Override\n     public PCollectionView<Iterable<T>> expand(PCollection<T> input) {\n-      PCollectionView<Iterable<T>> view = PCollectionViews.iterableView(input.getPipeline(),\n-          input.getWindowingStrategy(), input.getCoder());\n+      PCollectionView<Iterable<T>> view =\n+          PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n \n       return input.apply(Combine.globally(new Concatenate<T>()).withoutDefaults())\n           .apply(CreateApexPCollectionView.<T, Iterable<T>> of(view));\n@@ -353,9 +362,13 @@ protected String getKindString() {\n         extends SingleInputOutputOverrideFactory<\n             PCollection<T>, PCollectionView<Iterable<T>>, View.AsIterable<T>> {\n       @Override\n-      public PTransform<PCollection<T>, PCollectionView<Iterable<T>>> getReplacementTransform(\n-          AsIterable<T> transform) {\n-        return new StreamingViewAsIterable<>();\n+      public PTransformReplacement<PCollection<T>, PCollectionView<Iterable<T>>>\n+          getReplacementTransform(\n+              AppliedPTransform<PCollection<T>, PCollectionView<Iterable<T>>, AsIterable<T>>\n+                  transform) {\n+        return PTransformReplacement.of(\n+            PTransformReplacements.getSingletonMainInput(transform),\n+            new StreamingViewAsIterable<T>());\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "sha": "1c845c6c628b00a5378b5de8ceecf5b8f391c6f3",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "changes": 111,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 56,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "patch": "@@ -173,56 +173,57 @@ public void shutdown(ShutdownMode arg0) throws LauncherException {\n    * @throws IOException when dependency information cannot be read\n    */\n   public static List<File> getYarnDeployDependencies() throws IOException {\n-    InputStream dependencyTree = ApexRunner.class.getResourceAsStream(\"dependency-tree\");\n-    BufferedReader br = new BufferedReader(new InputStreamReader(dependencyTree));\n-    String line = null;\n-    List<String> excludes = new ArrayList<>();\n-    int excludeLevel = Integer.MAX_VALUE;\n-    while ((line = br.readLine()) != null) {\n-      for (int i = 0; i < line.length(); i++) {\n-        char c = line.charAt(i);\n-        if (Character.isLetter(c)) {\n-          if (i > excludeLevel) {\n-            excludes.add(line.substring(i));\n-          } else {\n-            if (line.substring(i).startsWith(\"org.apache.hadoop\")) {\n-              excludeLevel = i;\n-              excludes.add(line.substring(i));\n-            } else {\n-              excludeLevel = Integer.MAX_VALUE;\n+    try (InputStream dependencyTree = ApexRunner.class.getResourceAsStream(\"dependency-tree\")) {\n+      try (BufferedReader br = new BufferedReader(new InputStreamReader(dependencyTree))) {\n+        String line;\n+        List<String> excludes = new ArrayList<>();\n+        int excludeLevel = Integer.MAX_VALUE;\n+        while ((line = br.readLine()) != null) {\n+          for (int i = 0; i < line.length(); i++) {\n+            char c = line.charAt(i);\n+            if (Character.isLetter(c)) {\n+              if (i > excludeLevel) {\n+                excludes.add(line.substring(i));\n+              } else {\n+                if (line.substring(i).startsWith(\"org.apache.hadoop\")) {\n+                  excludeLevel = i;\n+                  excludes.add(line.substring(i));\n+                } else {\n+                  excludeLevel = Integer.MAX_VALUE;\n+                }\n+              }\n+              break;\n             }\n           }\n-          break;\n         }\n-      }\n-    }\n-    br.close();\n-\n-    Set<String> excludeJarFileNames = Sets.newHashSet();\n-    for (String exclude : excludes) {\n-      String[] mvnc = exclude.split(\":\");\n-      String fileName = mvnc[1] + \"-\";\n-      if (mvnc.length == 6) {\n-        fileName += mvnc[4] + \"-\" + mvnc[3]; // with classifier\n-      } else {\n-        fileName += mvnc[3];\n-      }\n-      fileName += \".jar\";\n-      excludeJarFileNames.add(fileName);\n-    }\n \n-    ClassLoader classLoader = ApexYarnLauncher.class.getClassLoader();\n-    URL[] urls = ((URLClassLoader) classLoader).getURLs();\n-    List<File> dependencyJars = new ArrayList<>();\n-    for (int i = 0; i < urls.length; i++) {\n-      File f = new File(urls[i].getFile());\n-      // dependencies can also be directories in the build reactor,\n-      // the Apex client will automatically create jar files for those.\n-      if (f.exists() && !excludeJarFileNames.contains(f.getName())) {\n-          dependencyJars.add(f);\n+        Set<String> excludeJarFileNames = Sets.newHashSet();\n+        for (String exclude : excludes) {\n+          String[] mvnc = exclude.split(\":\");\n+          String fileName = mvnc[1] + \"-\";\n+          if (mvnc.length == 6) {\n+            fileName += mvnc[4] + \"-\" + mvnc[3]; // with classifier\n+          } else {\n+            fileName += mvnc[3];\n+          }\n+          fileName += \".jar\";\n+          excludeJarFileNames.add(fileName);\n+        }\n+\n+        ClassLoader classLoader = ApexYarnLauncher.class.getClassLoader();\n+        URL[] urls = ((URLClassLoader) classLoader).getURLs();\n+        List<File> dependencyJars = new ArrayList<>();\n+        for (int i = 0; i < urls.length; i++) {\n+          File f = new File(urls[i].getFile());\n+          // dependencies can also be directories in the build reactor,\n+          // the Apex client will automatically create jar files for those.\n+          if (f.exists() && !excludeJarFileNames.contains(f.getName())) {\n+            dependencyJars.add(f);\n+          }\n+        }\n+        return dependencyJars;\n       }\n     }\n-    return dependencyJars;\n   }\n \n   /**\n@@ -238,17 +239,17 @@ public static void createJar(File dir, File jarFile) throws IOException {\n       throw new RuntimeException(\"Failed to remove \" + jarFile);\n     }\n     URI uri = URI.create(\"jar:\" + jarFile.toURI());\n-    try (final FileSystem zipfs = FileSystems.newFileSystem(uri, env);) {\n+    try (final FileSystem zipfs = FileSystems.newFileSystem(uri, env)) {\n \n       File manifestFile = new File(dir, JarFile.MANIFEST_NAME);\n       Files.createDirectory(zipfs.getPath(\"META-INF\"));\n-      final OutputStream out = Files.newOutputStream(zipfs.getPath(JarFile.MANIFEST_NAME));\n-      if (!manifestFile.exists()) {\n-        new Manifest().write(out);\n-      } else {\n-        FileUtils.copyFile(manifestFile, out);\n+      try (final OutputStream out = Files.newOutputStream(zipfs.getPath(JarFile.MANIFEST_NAME))) {\n+        if (!manifestFile.exists()) {\n+          new Manifest().write(out);\n+        } else {\n+          FileUtils.copyFile(manifestFile, out);\n+        }\n       }\n-      out.close();\n \n       final java.nio.file.Path root = dir.toPath();\n       Files.walkFileTree(root, new java.nio.file.SimpleFileVisitor<Path>() {\n@@ -274,9 +275,9 @@ public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs)\n         public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n           String name = relativePath + file.getFileName();\n           if (!JarFile.MANIFEST_NAME.equals(name)) {\n-            final OutputStream out = Files.newOutputStream(zipfs.getPath(name));\n-            FileUtils.copyFile(file.toFile(), out);\n-            out.close();\n+            try (final OutputStream out = Files.newOutputStream(zipfs.getPath(name))) {\n+              FileUtils.copyFile(file.toFile(), out);\n+            }\n           }\n           return super.visitFile(file, attrs);\n         }\n@@ -295,8 +296,6 @@ public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOEx\n \n   /**\n    * Transfer the properties to the configuration object.\n-   * @param conf\n-   * @param props\n    */\n   public static void addProperties(Configuration conf, Properties props) {\n     for (final String propertyName : props.stringPropertyNames()) {",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "sha": "b84144cd251ccdc3fb6a7688fd001c51168561a5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "patch": "@@ -24,8 +24,8 @@\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner.CreateApexPCollectionView;\n import org.apache.beam.runners.apex.translation.operators.ApexReadUnboundedInputOperator;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n import org.apache.beam.runners.core.construction.PrimitiveCreate;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n@@ -59,8 +59,7 @@\n \n   static {\n     // register TransformTranslators\n-    registerTransformTranslator(ParDo.Bound.class, new ParDoBoundTranslator());\n-    registerTransformTranslator(ParDo.BoundMulti.class, new ParDoBoundMultiTranslator<>());\n+    registerTransformTranslator(ParDo.MultiOutput.class, new ParDoTranslator<>());\n     registerTransformTranslator(Read.Unbounded.class, new ReadUnboundedTranslator());\n     registerTransformTranslator(Read.Bounded.class, new ReadBoundedTranslator());\n     registerTransformTranslator(GroupByKey.class, new GroupByKeyTranslator());",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "sha": "fdeefc70144783f08008ea296907f9172907923e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 6,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java",
                "patch": "@@ -32,7 +32,8 @@\n import org.apache.beam.sdk.io.UnboundedSource;\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * {@link Flatten.PCollections} translation to Apex operator.\n@@ -63,15 +64,15 @@ public void translate(Flatten.PCollections<T> transform, TranslationContext cont\n     }\n   }\n \n-  private List<PCollection<T>> extractPCollections(List<TaggedPValue> inputs) {\n+  private List<PCollection<T>> extractPCollections(Map<TupleTag<?>, PValue> inputs) {\n     List<PCollection<T>> collections = Lists.newArrayList();\n-    for (TaggedPValue pv : inputs) {\n+    for (PValue pv : inputs.values()) {\n       checkArgument(\n-          pv.getValue() instanceof PCollection,\n+          pv instanceof PCollection,\n           \"Non-PCollection provided as input to flatten: %s of type %s\",\n-          pv.getValue(),\n+          pv,\n           pv.getClass().getSimpleName());\n-      collections.add((PCollection<T>) pv.getValue());\n+      collections.add((PCollection<T>) pv);\n     }\n     return collections;\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslator.java",
                "sha": "440b8015e1f072fd001820cacc2672a386d155f2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java",
                "patch": "@@ -31,9 +31,9 @@\n \n   @Override\n   public void translate(GroupByKey<K, V> transform, TranslationContext context) {\n-    PCollection<KV<K, V>> input = (PCollection<KV<K, V>>) context.getInput();\n+    PCollection<KV<K, V>> input = context.getInput();\n     ApexGroupByKeyOperator<K, V> group = new ApexGroupByKeyOperator<>(context.getPipelineOptions(),\n-        input, context.<K>stateInternalsFactory()\n+        input, context.getStateBackend()\n         );\n     context.addOperator(group, group.output);\n     context.addStream(input, group.input);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslator.java",
                "sha": "2e0bae70186a443505221da86f5a39efd51f7483",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "changes": 95,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 95,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "patch": "@@ -1,95 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.beam.runners.apex.translation;\n-\n-import java.util.List;\n-import org.apache.beam.runners.apex.ApexRunner;\n-import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n-import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n-import org.apache.beam.sdk.util.WindowedValue.WindowedValueCoder;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.beam.sdk.values.TupleTagList;\n-\n-/** {@link ParDo.Bound} is translated to {link ApexParDoOperator} that wraps the {@link DoFn}. */\n-class ParDoBoundTranslator<InputT, OutputT>\n-    implements TransformTranslator<ParDo.Bound<InputT, OutputT>> {\n-  private static final long serialVersionUID = 1L;\n-\n-  @Override\n-  public void translate(ParDo.Bound<InputT, OutputT> transform, TranslationContext context) {\n-    DoFn<InputT, OutputT> doFn = transform.getFn();\n-    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n-\n-    if (signature.processElement().isSplittable()) {\n-      throw new UnsupportedOperationException(\n-          String.format(\n-              \"%s does not support splittable DoFn: %s\", ApexRunner.class.getSimpleName(), doFn));\n-    }\n-    if (signature.stateDeclarations().size() > 0) {\n-      throw new UnsupportedOperationException(\n-          String.format(\n-              \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-              DoFn.StateId.class.getSimpleName(),\n-              doFn.getClass().getName(),\n-              DoFn.class.getSimpleName(),\n-              ApexRunner.class.getSimpleName()));\n-    }\n-\n-    if (signature.timerDeclarations().size() > 0) {\n-      throw new UnsupportedOperationException(\n-          String.format(\n-              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n-              DoFn.TimerId.class.getSimpleName(),\n-              doFn.getClass().getName(),\n-              DoFn.class.getSimpleName(),\n-              ApexRunner.class.getSimpleName()));\n-    }\n-\n-    PCollection<OutputT> output = (PCollection<OutputT>) context.getOutput();\n-    PCollection<InputT> input = (PCollection<InputT>) context.getInput();\n-    List<PCollectionView<?>> sideInputs = transform.getSideInputs();\n-    Coder<InputT> inputCoder = input.getCoder();\n-    WindowedValueCoder<InputT> wvInputCoder =\n-        FullWindowedValueCoder.of(\n-            inputCoder, input.getWindowingStrategy().getWindowFn().windowCoder());\n-\n-    ApexParDoOperator<InputT, OutputT> operator =\n-        new ApexParDoOperator<>(\n-            context.getPipelineOptions(),\n-            doFn,\n-            new TupleTag<OutputT>(),\n-            TupleTagList.empty().getAll() /*sideOutputTags*/,\n-            output.getWindowingStrategy(),\n-            sideInputs,\n-            wvInputCoder,\n-            context.<Void>stateInternalsFactory());\n-    context.addOperator(operator, operator.output);\n-    context.addStream(context.getInput(), operator.input);\n-    if (!sideInputs.isEmpty()) {\n-      ParDoBoundMultiTranslator.addSideInputs(operator, sideInputs, context);\n-    }\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "sha": "5195809bdbbf3e3ccf23f09273f629a251f06a40",
                "status": "removed"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 21,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Map.Entry;\n import org.apache.beam.runners.apex.ApexRunner;\n import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n import org.apache.beam.sdk.coders.Coder;\n@@ -38,21 +39,21 @@\n import org.apache.beam.sdk.util.WindowedValue.WindowedValueCoder;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n /**\n- * {@link ParDo.BoundMulti} is translated to {@link ApexParDoOperator} that wraps the {@link DoFn}.\n+ * {@link ParDo.MultiOutput} is translated to {@link ApexParDoOperator} that wraps the {@link DoFn}.\n  */\n-class ParDoBoundMultiTranslator<InputT, OutputT>\n-    implements TransformTranslator<ParDo.BoundMulti<InputT, OutputT>> {\n+class ParDoTranslator<InputT, OutputT>\n+    implements TransformTranslator<ParDo.MultiOutput<InputT, OutputT>> {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LoggerFactory.getLogger(ParDoBoundMultiTranslator.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(ParDoTranslator.class);\n \n   @Override\n-  public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationContext context) {\n+  public void translate(ParDo.MultiOutput<InputT, OutputT> transform, TranslationContext context) {\n     DoFn<InputT, OutputT> doFn = transform.getFn();\n     DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n \n@@ -81,42 +82,41 @@ public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationCo\n               ApexRunner.class.getSimpleName()));\n     }\n \n-    List<TaggedPValue> outputs = context.getOutputs();\n+    Map<TupleTag<?>, PValue> outputs = context.getOutputs();\n     PCollection<InputT> input = (PCollection<InputT>) context.getInput();\n     List<PCollectionView<?>> sideInputs = transform.getSideInputs();\n     Coder<InputT> inputCoder = input.getCoder();\n     WindowedValueCoder<InputT> wvInputCoder =\n         FullWindowedValueCoder.of(\n             inputCoder, input.getWindowingStrategy().getWindowFn().windowCoder());\n \n-    ApexParDoOperator<InputT, OutputT> operator =\n-        new ApexParDoOperator<>(\n+    ApexParDoOperator<InputT, OutputT> operator = new ApexParDoOperator<>(\n             context.getPipelineOptions(),\n             doFn,\n             transform.getMainOutputTag(),\n-            transform.getSideOutputTags().getAll(),\n-            ((PCollection<InputT>) context.getInput()).getWindowingStrategy(),\n+            transform.getAdditionalOutputTags().getAll(),\n+            input.getWindowingStrategy(),\n             sideInputs,\n             wvInputCoder,\n-            context.<Void>stateInternalsFactory());\n+            context.getStateBackend());\n \n     Map<PCollection<?>, OutputPort<?>> ports = Maps.newHashMapWithExpectedSize(outputs.size());\n-    for (TaggedPValue output : outputs) {\n+    for (Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {\n       checkArgument(\n           output.getValue() instanceof PCollection,\n           \"%s %s outputs non-PCollection %s of type %s\",\n-          ParDo.BoundMulti.class.getSimpleName(),\n+          ParDo.MultiOutput.class.getSimpleName(),\n           context.getFullName(),\n           output.getValue(),\n           output.getValue().getClass().getSimpleName());\n       PCollection<?> pc = (PCollection<?>) output.getValue();\n-      if (output.getTag().equals(transform.getMainOutputTag())) {\n+      if (output.getKey().equals(transform.getMainOutputTag())) {\n         ports.put(pc, operator.output);\n       } else {\n         int portIndex = 0;\n-        for (TupleTag<?> tag : transform.getSideOutputTags().getAll()) {\n-          if (tag.equals(output.getTag())) {\n-            ports.put(pc, operator.sideOutputPorts[portIndex]);\n+        for (TupleTag<?> tag : transform.getAdditionalOutputTags().getAll()) {\n+          if (tag.equals(output.getKey())) {\n+            ports.put(pc, operator.additionalOutputPorts[portIndex]);\n             break;\n           }\n           portIndex++;\n@@ -126,15 +126,15 @@ public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationCo\n     context.addOperator(operator, ports);\n     context.addStream(context.getInput(), operator.input);\n     if (!sideInputs.isEmpty()) {\n-      addSideInputs(operator, sideInputs, context);\n+      addSideInputs(operator.sideInput1, sideInputs, context);\n     }\n   }\n \n   static void addSideInputs(\n-      ApexParDoOperator<?, ?> operator,\n+      Operator.InputPort<?> sideInputPort,\n       List<PCollectionView<?>> sideInputs,\n       TranslationContext context) {\n-    Operator.InputPort<?>[] sideInputPorts = {operator.sideInput1};\n+    Operator.InputPort<?>[] sideInputPorts = {sideInputPort};\n     if (sideInputs.size() > sideInputPorts.length) {\n       PCollection<?> unionCollection = unionSideInputs(sideInputs, context);\n       context.addStream(unionCollection, sideInputPorts[0]);",
                "previous_filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundMultiTranslator.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoTranslator.java",
                "sha": "2e3d902e01067187c6215418ff77f4f295951ab3",
                "status": "renamed"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "patch": "@@ -31,9 +31,9 @@\n import java.util.Map;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.translation.utils.ApexStateInternals;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.CoderAdapterStreamCodec;\n-import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n@@ -42,7 +42,7 @@\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.apache.commons.lang3.tuple.ImmutablePair;\n import org.apache.commons.lang3.tuple.Pair;\n \n@@ -85,20 +85,20 @@ public String getFullName() {\n     return getCurrentTransform().getFullName();\n   }\n \n-  public List<TaggedPValue> getInputs() {\n+  public Map<TupleTag<?>, PValue> getInputs() {\n     return getCurrentTransform().getInputs();\n   }\n \n-  public PValue getInput() {\n-    return Iterables.getOnlyElement(getCurrentTransform().getInputs()).getValue();\n+  public <InputT extends PValue> InputT getInput() {\n+    return (InputT) Iterables.getOnlyElement(getCurrentTransform().getInputs().values());\n   }\n \n-  public List<TaggedPValue> getOutputs() {\n+  public Map<TupleTag<?>, PValue> getOutputs() {\n     return getCurrentTransform().getOutputs();\n   }\n \n-  public PValue getOutput() {\n-    return Iterables.getOnlyElement(getCurrentTransform().getOutputs()).getValue();\n+  public <OutputT extends PValue> OutputT getOutput() {\n+    return (OutputT) Iterables.getOnlyElement(getCurrentTransform().getOutputs().values());\n   }\n \n   private AppliedPTransform<?, ?, ?> getCurrentTransform() {\n@@ -192,10 +192,10 @@ public void populateDAG(DAG dag) {\n   }\n \n   /**\n-   * Return the {@link StateInternalsFactory} for the pipeline translation.\n+   * Return the state backend for the pipeline translation.\n    * @return\n    */\n-  public <K> StateInternalsFactory<K> stateInternalsFactory() {\n-    return new ApexStateInternals.ApexStateInternalsFactory();\n+  public ApexStateBackend getStateBackend() {\n+    return new ApexStateInternals.ApexStateBackend();\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "sha": "c78028ec57203c8fc20ddfc4da949c423578b587",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 42,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java",
                "patch": "@@ -18,61 +18,35 @@\n \n package org.apache.beam.runners.apex.translation;\n \n-import java.util.Collections;\n-import org.apache.beam.runners.apex.ApexPipelineOptions;\n-import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n-import org.apache.beam.runners.core.AssignWindowsDoFn;\n-import org.apache.beam.runners.core.DoFnAdapters;\n-import org.apache.beam.runners.core.OldDoFn;\n-import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.runners.apex.translation.operators.ApexProcessFnOperator;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.beam.sdk.values.TupleTagList;\n \n /**\n- * {@link Window.Bound} is translated to {link ApexParDoOperator} that wraps an {@link\n- * AssignWindowsDoFn}.\n+ * {@link Window} is translated to {@link ApexProcessFnOperator#assignWindows}.\n  */\n class WindowAssignTranslator<T> implements TransformTranslator<Window.Assign<T>> {\n   private static final long serialVersionUID = 1L;\n \n   @Override\n   public void translate(Window.Assign<T> transform, TranslationContext context) {\n-    PCollection<T> output = (PCollection<T>) context.getOutput();\n-    PCollection<T> input = (PCollection<T>) context.getInput();\n-    @SuppressWarnings(\"unchecked\")\n-    WindowingStrategy<T, BoundedWindow> windowingStrategy =\n-        (WindowingStrategy<T, BoundedWindow>) output.getWindowingStrategy();\n+    PCollection<T> output = context.getOutput();\n+    PCollection<T> input = context.getInput();\n \n-    OldDoFn<T, T> fn =\n-        (transform.getWindowFn() == null)\n-            ? DoFnAdapters.toOldDoFn(new IdentityFn<T>())\n-            : new AssignWindowsDoFn<>(transform.getWindowFn());\n+   if (transform.getWindowFn() == null) {\n+     // no work to do\n+     context.addAlias(output, input);\n+   } else {\n+      @SuppressWarnings(\"unchecked\")\n+      WindowFn<T, BoundedWindow> windowFn = (WindowFn<T, BoundedWindow>) transform.getWindowFn();\n+      ApexProcessFnOperator<T> operator = ApexProcessFnOperator.assignWindows(windowFn,\n+          context.getPipelineOptions());\n+      context.addOperator(operator, operator.outputPort);\n+      context.addStream(context.getInput(), operator.inputPort);\n+   }\n \n-    ApexParDoOperator<T, T> operator =\n-        new ApexParDoOperator<T, T>(\n-            context.getPipelineOptions().as(ApexPipelineOptions.class),\n-            fn,\n-            new TupleTag<T>(),\n-            TupleTagList.empty().getAll(),\n-            windowingStrategy,\n-            Collections.<PCollectionView<?>>emptyList(),\n-            WindowedValue.getFullCoder(\n-                input.getCoder(), windowingStrategy.getWindowFn().windowCoder()),\n-            context.<Void>stateInternalsFactory());\n-    context.addOperator(operator, operator.output);\n-    context.addStream(context.getInput(), operator.input);\n   }\n \n-  private static class IdentityFn<T> extends DoFn<T, T> {\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      c.output(c.element());\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowAssignTranslator.java",
                "sha": "f34f9eecd9341ca2fc4db87c88a65aed667ec908",
                "status": "modified"
            },
            {
                "additions": 81,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "changes": 273,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 192,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "patch": "@@ -25,12 +25,12 @@\n import com.datatorrent.api.Operator;\n import com.datatorrent.api.StreamCodec;\n import com.datatorrent.api.annotation.OutputPortFieldAnnotation;\n+import com.datatorrent.netlet.util.Slice;\n import com.esotericsoftware.kryo.serializers.FieldSerializer.Bind;\n import com.esotericsoftware.kryo.serializers.JavaSerializer;\n import com.google.common.base.Throwables;\n import com.google.common.collect.HashMultimap;\n import com.google.common.collect.Multimap;\n-import java.nio.ByteBuffer;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n@@ -39,34 +39,32 @@\n import java.util.Map;\n import java.util.Set;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.SerializablePipelineOptions;\n-import org.apache.beam.runners.core.GroupAlsoByWindowViaWindowSetDoFn;\n-import org.apache.beam.runners.core.KeyedWorkItem;\n-import org.apache.beam.runners.core.KeyedWorkItems;\n-import org.apache.beam.runners.core.OldDoFn;\n+import org.apache.beam.runners.core.OutputWindowedValue;\n+import org.apache.beam.runners.core.ReduceFnRunner;\n import org.apache.beam.runners.core.StateInternals;\n import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.SystemReduceFn;\n import org.apache.beam.runners.core.TimerInternals;\n-import org.apache.beam.runners.core.WindowingInternals;\n+import org.apache.beam.runners.core.construction.Triggers;\n+import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n+import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderException;\n import org.apache.beam.sdk.coders.KvCoder;\n-import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.util.CoderUtils;\n+import org.apache.beam.sdk.util.NullSideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Instant;\n import org.slf4j.Logger;\n@@ -95,11 +93,8 @@\n   private final SerializablePipelineOptions serializedOptions;\n   @Bind(JavaSerializer.class)\n   private final StateInternalsFactory<K> stateInternalsFactory;\n-  private Map<ByteBuffer, StateInternals<K>> perKeyStateInternals = new HashMap<>();\n-  private Map<ByteBuffer, Set<TimerInternals.TimerData>> activeTimers = new HashMap<>();\n+  private Map<Slice, Set<TimerInternals.TimerData>> activeTimers = new HashMap<>();\n \n-  private transient ProcessContext context;\n-  private transient OldDoFn<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> fn;\n   private transient ApexTimerInternals timerInternals = new ApexTimerInternals();\n   private Instant inputWatermark = BoundedWindow.TIMESTAMP_MIN_VALUE;\n \n@@ -135,13 +130,13 @@ public void process(ApexStreamTuple<WindowedValue<KV<K, V>>> t) {\n \n   @SuppressWarnings(\"unchecked\")\n   public ApexGroupByKeyOperator(ApexPipelineOptions pipelineOptions, PCollection<KV<K, V>> input,\n-      StateInternalsFactory<K> stateInternalsFactory) {\n+      ApexStateBackend stateBackend) {\n     checkNotNull(pipelineOptions);\n     this.serializedOptions = new SerializablePipelineOptions(pipelineOptions);\n     this.windowingStrategy = (WindowingStrategy<V, BoundedWindow>) input.getWindowingStrategy();\n     this.keyCoder = ((KvCoder<K, V>) input.getCoder()).getKeyCoder();\n     this.valueCoder = ((KvCoder<K, V>) input.getCoder()).getValueCoder();\n-    this.stateInternalsFactory = stateInternalsFactory;\n+    this.stateInternalsFactory = stateBackend.newStateInternalsFactory(keyCoder);\n   }\n \n   @SuppressWarnings(\"unused\") // for Kryo\n@@ -161,34 +156,71 @@ public void endWindow() {\n   @Override\n   public void setup(OperatorContext context) {\n     this.traceTuples = ApexStreamTuple.Logging.isDebugEnabled(serializedOptions.get(), this);\n-    StateInternalsFactory<K> stateInternalsFactory = new GroupByKeyStateInternalsFactory();\n-    this.fn = GroupAlsoByWindowViaWindowSetDoFn.create(this.windowingStrategy,\n-        stateInternalsFactory, SystemReduceFn.<K, V, BoundedWindow>buffering(this.valueCoder));\n-    this.context = new ProcessContext(fn, this.timerInternals);\n   }\n \n   @Override\n   public void teardown() {\n   }\n \n+\n+  private ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> newReduceFnRunner(K key) {\n+    return new ReduceFnRunner<>(\n+        key,\n+        windowingStrategy,\n+        ExecutableTriggerStateMachine.create(\n+            TriggerStateMachines.stateMachineForTrigger(\n+                Triggers.toProto(windowingStrategy.getTrigger()))),\n+        stateInternalsFactory.stateInternalsForKey(key),\n+        timerInternals,\n+        new OutputWindowedValue<KV<K, Iterable<V>>>() {\n+          @Override\n+          public void outputWindowedValue(\n+              KV<K, Iterable<V>> output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            if (traceTuples) {\n+              LOG.debug(\"\\nemitting {} timestamp {}\\n\", output, timestamp);\n+            }\n+            ApexGroupByKeyOperator.this.output.emit(\n+                ApexStreamTuple.DataTuple.of(WindowedValue.of(output, timestamp, windows, pane)));\n+          }\n+\n+          @Override\n+          public <AdditionalOutputT> void outputWindowedValue(\n+              TupleTag<AdditionalOutputT> tag,\n+              AdditionalOutputT output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            throw new UnsupportedOperationException(\n+                \"GroupAlsoByWindow should not use side outputs\");\n+          }\n+        },\n+        NullSideInputReader.empty(),\n+        null,\n+        SystemReduceFn.<K, V, BoundedWindow>buffering(this.valueCoder),\n+        serializedOptions.get());\n+  }\n+\n   /**\n    * Returns the list of timers that are ready to fire. These are the timers\n    * that are registered to be triggered at a time before the current watermark.\n    * We keep these timers in a Set, so that they are deduplicated, as the same\n    * timer can be registered multiple times.\n    */\n-  private Multimap<ByteBuffer, TimerInternals.TimerData> getTimersReadyToProcess(\n+  private Multimap<Slice, TimerInternals.TimerData> getTimersReadyToProcess(\n       long currentWatermark) {\n \n     // we keep the timers to return in a different list and launch them later\n     // because we cannot prevent a trigger from registering another trigger,\n     // which would lead to concurrent modification exception.\n-    Multimap<ByteBuffer, TimerInternals.TimerData> toFire = HashMultimap.create();\n+    Multimap<Slice, TimerInternals.TimerData> toFire = HashMultimap.create();\n \n-    Iterator<Map.Entry<ByteBuffer, Set<TimerInternals.TimerData>>> it =\n+    Iterator<Map.Entry<Slice, Set<TimerInternals.TimerData>>> it =\n         activeTimers.entrySet().iterator();\n     while (it.hasNext()) {\n-      Map.Entry<ByteBuffer, Set<TimerInternals.TimerData>> keyWithTimers = it.next();\n+      Map.Entry<Slice, Set<TimerInternals.TimerData>> keyWithTimers = it.next();\n \n       Iterator<TimerInternals.TimerData> timerIt = keyWithTimers.getValue().iterator();\n       while (timerIt.hasNext()) {\n@@ -212,34 +244,21 @@ private void processElement(WindowedValue<KV<K, V>> windowedValue) throws Except\n         windowedValue.getTimestamp(),\n         windowedValue.getWindows(),\n         windowedValue.getPane());\n-\n-    KeyedWorkItem<K, V> kwi = KeyedWorkItems.elementsWorkItem(\n-            kv.getKey(),\n-            Collections.singletonList(updatedWindowedValue));\n-\n-    context.setElement(kwi, getStateInternalsForKey(kwi.key()));\n-    fn.processElement(context);\n+    timerInternals.setKey(kv.getKey());\n+    ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> reduceFnRunner =\n+        newReduceFnRunner(kv.getKey());\n+    reduceFnRunner.processElements(Collections.singletonList(updatedWindowedValue));\n+    reduceFnRunner.persist();\n   }\n \n   private StateInternals<K> getStateInternalsForKey(K key) {\n-    final ByteBuffer keyBytes;\n-    try {\n-      keyBytes = ByteBuffer.wrap(CoderUtils.encodeToByteArray(keyCoder, key));\n-    } catch (CoderException e) {\n-      throw new RuntimeException(e);\n-    }\n-    StateInternals<K> stateInternals = perKeyStateInternals.get(keyBytes);\n-    if (stateInternals == null) {\n-      stateInternals = stateInternalsFactory.stateInternalsForKey(key);\n-      perKeyStateInternals.put(keyBytes, stateInternals);\n-    }\n-    return stateInternals;\n+    return stateInternalsFactory.stateInternalsForKey(key);\n   }\n \n   private void registerActiveTimer(K key, TimerInternals.TimerData timer) {\n-    final ByteBuffer keyBytes;\n+    final Slice keyBytes;\n     try {\n-      keyBytes = ByteBuffer.wrap(CoderUtils.encodeToByteArray(keyCoder, key));\n+      keyBytes = new Slice(CoderUtils.encodeToByteArray(keyCoder, key));\n     } catch (CoderException e) {\n       throw new RuntimeException(e);\n     }\n@@ -252,9 +271,9 @@ private void registerActiveTimer(K key, TimerInternals.TimerData timer) {\n   }\n \n   private void unregisterActiveTimer(K key, TimerInternals.TimerData timer) {\n-    final ByteBuffer keyBytes;\n+    final Slice keyBytes;\n     try {\n-      keyBytes = ByteBuffer.wrap(CoderUtils.encodeToByteArray(keyCoder, key));\n+      keyBytes = new Slice(CoderUtils.encodeToByteArray(keyCoder, key));\n     } catch (CoderException e) {\n       throw new RuntimeException(e);\n     }\n@@ -271,164 +290,34 @@ private void unregisterActiveTimer(K key, TimerInternals.TimerData timer) {\n \n   private void processWatermark(ApexStreamTuple.WatermarkTuple<?> mark) throws Exception {\n     this.inputWatermark = new Instant(mark.getTimestamp());\n-    Multimap<ByteBuffer, TimerInternals.TimerData> timers = getTimersReadyToProcess(\n+    Multimap<Slice, TimerInternals.TimerData> timers = getTimersReadyToProcess(\n         mark.getTimestamp());\n     if (!timers.isEmpty()) {\n-      for (ByteBuffer keyBytes : timers.keySet()) {\n-        K key = CoderUtils.decodeFromByteArray(keyCoder, keyBytes.array());\n-        KeyedWorkItem<K, V> kwi = KeyedWorkItems.<K, V>timersWorkItem(key, timers.get(keyBytes));\n-        context.setElement(kwi, getStateInternalsForKey(kwi.key()));\n-        fn.processElement(context);\n+      for (Slice keyBytes : timers.keySet()) {\n+        K key = CoderUtils.decodeFromByteArray(keyCoder, keyBytes.buffer);\n+        timerInternals.setKey(key);\n+        ReduceFnRunner<K, V, Iterable<V>, BoundedWindow> reduceFnRunner = newReduceFnRunner(key);\n+        reduceFnRunner.onTimers(timers.get(keyBytes));\n+        reduceFnRunner.persist();\n       }\n     }\n   }\n \n-  private class ProcessContext extends GroupAlsoByWindowViaWindowSetDoFn<K, V, Iterable<V>, ?,\n-      KeyedWorkItem<K, V>>.ProcessContext {\n-\n-    private final ApexTimerInternals timerInternals;\n-    private StateInternals<K> stateInternals;\n-    private KeyedWorkItem<K, V> element;\n-\n-    public ProcessContext(OldDoFn<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> function,\n-                          ApexTimerInternals timerInternals) {\n-      function.super();\n-      this.timerInternals = checkNotNull(timerInternals);\n-    }\n-\n-    public void setElement(KeyedWorkItem<K, V> element, StateInternals<K> stateForKey) {\n-      this.element = element;\n-      this.stateInternals = stateForKey;\n-    }\n-\n-    @Override\n-    public KeyedWorkItem<K, V> element() {\n-      return this.element;\n-    }\n-\n-    @Override\n-    public Instant timestamp() {\n-      throw new UnsupportedOperationException(\n-          \"timestamp() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public PipelineOptions getPipelineOptions() {\n-      return serializedOptions.get();\n-    }\n-\n-    @Override\n-    public void output(KV<K, Iterable<V>> output) {\n-      throw new UnsupportedOperationException(\n-          \"output() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public void outputWithTimestamp(KV<K, Iterable<V>> output, Instant timestamp) {\n-      throw new UnsupportedOperationException(\n-          \"outputWithTimestamp() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public PaneInfo pane() {\n-      throw new UnsupportedOperationException(\n-          \"pane() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public BoundedWindow window() {\n-      throw new UnsupportedOperationException(\n-          \"window() is not available when processing KeyedWorkItems.\");\n-    }\n-\n-    @Override\n-    public WindowingInternals<KeyedWorkItem<K, V>, KV<K, Iterable<V>>> windowingInternals() {\n-      return new WindowingInternals<KeyedWorkItem<K, V>, KV<K, Iterable<V>>>() {\n-\n-        @Override\n-        public StateInternals<K> stateInternals() {\n-          return stateInternals;\n-        }\n-\n-        @Override\n-        public void outputWindowedValue(\n-            KV<K, Iterable<V>> output,\n-            Instant timestamp,\n-            Collection<? extends BoundedWindow> windows,\n-            PaneInfo pane) {\n-          if (traceTuples) {\n-            LOG.debug(\"\\nemitting {} timestamp {}\\n\", output, timestamp);\n-          }\n-          ApexGroupByKeyOperator.this.output.emit(\n-              ApexStreamTuple.DataTuple.of(WindowedValue.of(output, timestamp, windows, pane)));\n-        }\n-\n-        @Override\n-        public <SideOutputT> void sideOutputWindowedValue(\n-            TupleTag<SideOutputT> tag,\n-            SideOutputT output,\n-            Instant timestamp,\n-            Collection<? extends BoundedWindow> windows,\n-            PaneInfo pane) {\n-          throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use side outputs\");\n-        }\n-\n-        @Override\n-        public TimerInternals timerInternals() {\n-          return timerInternals;\n-        }\n-\n-        @Override\n-        public Collection<? extends BoundedWindow> windows() {\n-          throw new UnsupportedOperationException(\"windows() is not available in Streaming mode.\");\n-        }\n-\n-        @Override\n-        public PaneInfo pane() {\n-          throw new UnsupportedOperationException(\"pane() is not available in Streaming mode.\");\n-        }\n-\n-        @Override\n-        public <T> T sideInput(PCollectionView<T> view, BoundedWindow mainInputWindow) {\n-          throw new RuntimeException(\"sideInput() is not available in Streaming mode.\");\n-        }\n-      };\n-    }\n-\n-    @Override\n-    public <T> T sideInput(PCollectionView<T> view) {\n-      throw new RuntimeException(\"sideInput() is not supported in Streaming mode.\");\n-    }\n-\n-    @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      // ignore the side output, this can happen when a user does not register\n-      // side outputs but then outputs using a freshly created TupleTag.\n-      throw new RuntimeException(\"sideOutput() is not available when grouping by window.\");\n-    }\n-\n-    @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      sideOutput(tag, output);\n-    }\n-\n-    @Override\n-    public <AggInputT, AggOutputT> Aggregator<AggInputT, AggOutputT> createAggregatorInternal(\n-        String name, Combine.CombineFn<AggInputT, ?, AggOutputT> combiner) {\n-      throw new UnsupportedOperationException();\n-    }\n-  }\n-\n   /**\n    * An implementation of Beam's {@link TimerInternals}.\n    *\n    */\n-  public class ApexTimerInternals implements TimerInternals {\n+  private class ApexTimerInternals implements TimerInternals {\n+    private K key;\n+\n+    public void setKey(K key) {\n+      this.key = key;\n+    }\n \n     @Deprecated\n     @Override\n     public void setTimer(TimerData timerData) {\n-      registerActiveTimer(context.element().key(), timerData);\n+      registerActiveTimer(key, timerData);\n     }\n \n     @Override\n@@ -439,7 +328,7 @@ public void deleteTimer(StateNamespace namespace, String timerId, TimeDomain tim\n     @Deprecated\n     @Override\n     public void deleteTimer(TimerData timerKey) {\n-      unregisterActiveTimer(context.element().key(), timerKey);\n+      unregisterActiveTimer(key, timerKey);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "sha": "7d17ac660e477f77782c6284e029ca8d79d2a46f",
                "status": "modified"
            },
            {
                "additions": 177,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "changes": 238,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 61,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "patch": "@@ -25,41 +25,53 @@\n import com.datatorrent.common.util.BaseOperator;\n import com.esotericsoftware.kryo.serializers.FieldSerializer.Bind;\n import com.esotericsoftware.kryo.serializers.JavaSerializer;\n-import com.google.common.base.Throwables;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Maps;\n import java.util.ArrayList;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.NoOpStepContext;\n import org.apache.beam.runners.apex.translation.utils.SerializablePipelineOptions;\n+import org.apache.beam.runners.apex.translation.utils.StateInternalsProxy;\n import org.apache.beam.runners.apex.translation.utils.ValueAndCoderKryoSerializable;\n import org.apache.beam.runners.core.AggregatorFactory;\n-import org.apache.beam.runners.core.DoFnAdapters;\n import org.apache.beam.runners.core.DoFnRunner;\n import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.ExecutionContext;\n-import org.apache.beam.runners.core.OldDoFn;\n+import org.apache.beam.runners.core.InMemoryTimerInternals;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.PushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.SideInputHandler;\n+import org.apache.beam.runners.core.SimplePushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.StateInternals;\n-import org.apache.beam.runners.core.StateInternalsFactory;\n+import org.apache.beam.runners.core.StateNamespace;\n+import org.apache.beam.runners.core.StatefulDoFnRunner;\n+import org.apache.beam.runners.core.TimerInternals;\n+import org.apache.beam.runners.core.TimerInternalsFactory;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.ListCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;\n+import org.apache.beam.sdk.transforms.reflect.DoFnInvokers;\n import org.apache.beam.sdk.util.NullSideInputReader;\n import org.apache.beam.sdk.util.SideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n+import org.joda.time.Instant;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -73,16 +85,22 @@\n   @Bind(JavaSerializer.class)\n   private final SerializablePipelineOptions pipelineOptions;\n   @Bind(JavaSerializer.class)\n-  private final OldDoFn<InputT, OutputT> doFn;\n+  private final DoFn<InputT, OutputT> doFn;\n   @Bind(JavaSerializer.class)\n   private final TupleTag<OutputT> mainOutputTag;\n   @Bind(JavaSerializer.class)\n-  private final List<TupleTag<?>> sideOutputTags;\n+  private final List<TupleTag<?>> additionalOutputTags;\n   @Bind(JavaSerializer.class)\n   private final WindowingStrategy<?, ?> windowingStrategy;\n   @Bind(JavaSerializer.class)\n   private final List<PCollectionView<?>> sideInputs;\n \n+  private StateInternalsProxy<?> currentKeyStateInternals;\n+  // TODO: if the operator gets restored to checkpointed state due to a failure,\n+  // the timer state is lost.\n+  private final transient CurrentKeyTimerInternals<Object> currentKeyTimerInternals =\n+      new CurrentKeyTimerInternals<>();\n+\n   private final StateInternals<Void> sideInputStateInternals;\n   private final ValueAndCoderKryoSerializable<List<WindowedValue<InputT>>> pushedBack;\n   private LongMin pushedBackWatermark = new LongMin();\n@@ -91,31 +109,32 @@\n \n   private transient PushbackSideInputDoFnRunner<InputT, OutputT> pushbackDoFnRunner;\n   private transient SideInputHandler sideInputHandler;\n-  private transient Map<TupleTag<?>, DefaultOutputPort<ApexStreamTuple<?>>> sideOutputPortMapping =\n-      Maps.newHashMapWithExpectedSize(5);\n+  private transient Map<TupleTag<?>, DefaultOutputPort<ApexStreamTuple<?>>>\n+      additionalOutputPortMapping = Maps.newHashMapWithExpectedSize(5);\n+  private transient DoFnInvoker<InputT, OutputT> doFnInvoker;\n \n-  @Deprecated\n   public ApexParDoOperator(\n       ApexPipelineOptions pipelineOptions,\n-      OldDoFn<InputT, OutputT> doFn,\n+      DoFn<InputT, OutputT> doFn,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       WindowingStrategy<?, ?> windowingStrategy,\n       List<PCollectionView<?>> sideInputs,\n       Coder<WindowedValue<InputT>> inputCoder,\n-      StateInternalsFactory<Void> stateInternalsFactory\n+      ApexStateBackend stateBackend\n       ) {\n     this.pipelineOptions = new SerializablePipelineOptions(pipelineOptions);\n     this.doFn = doFn;\n     this.mainOutputTag = mainOutputTag;\n-    this.sideOutputTags = sideOutputTags;\n+    this.additionalOutputTags = additionalOutputTags;\n     this.windowingStrategy = windowingStrategy;\n     this.sideInputs = sideInputs;\n-    this.sideInputStateInternals = stateInternalsFactory.stateInternalsForKey(null);\n+    this.sideInputStateInternals = new StateInternalsProxy<>(\n+        stateBackend.newStateInternalsFactory(VoidCoder.of()));\n \n-    if (sideOutputTags.size() > sideOutputPorts.length) {\n-      String msg = String.format(\"Too many side outputs (currently only supporting %s).\",\n-          sideOutputPorts.length);\n+    if (additionalOutputTags.size() > additionalOutputPorts.length) {\n+      String msg = String.format(\"Too many additional outputs (currently only supporting %s).\",\n+          additionalOutputPorts.length);\n       throw new UnsupportedOperationException(msg);\n     }\n \n@@ -125,33 +144,12 @@ public ApexParDoOperator(\n \n   }\n \n-  public ApexParDoOperator(\n-      ApexPipelineOptions pipelineOptions,\n-      DoFn<InputT, OutputT> doFn,\n-      TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n-      WindowingStrategy<?, ?> windowingStrategy,\n-      List<PCollectionView<?>> sideInputs,\n-      Coder<WindowedValue<InputT>> inputCoder,\n-      StateInternalsFactory<Void> stateInternalsFactory\n-      ) {\n-    this(\n-        pipelineOptions,\n-        DoFnAdapters.toOldDoFn(doFn),\n-        mainOutputTag,\n-        sideOutputTags,\n-        windowingStrategy,\n-        sideInputs,\n-        inputCoder,\n-        stateInternalsFactory);\n-  }\n-\n   @SuppressWarnings(\"unused\") // for Kryo\n   private ApexParDoOperator() {\n     this.pipelineOptions = null;\n     this.doFn = null;\n     this.mainOutputTag = null;\n-    this.sideOutputTags = null;\n+    this.additionalOutputTags = null;\n     this.windowingStrategy = null;\n     this.sideInputs = null;\n     this.pushedBack = null;\n@@ -221,29 +219,31 @@ public void process(ApexStreamTuple<WindowedValue<Iterable<?>>> t) {\n   public final transient DefaultOutputPort<ApexStreamTuple<?>> output = new DefaultOutputPort<>();\n \n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput1 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput1 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput2 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput2 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput3 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput3 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput4 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput4 =\n       new DefaultOutputPort<>();\n   @OutputPortFieldAnnotation(optional = true)\n-  public final transient DefaultOutputPort<ApexStreamTuple<?>> sideOutput5 =\n+  public final transient DefaultOutputPort<ApexStreamTuple<?>> additionalOutput5 =\n       new DefaultOutputPort<>();\n \n-  public final transient DefaultOutputPort<?>[] sideOutputPorts = {sideOutput1, sideOutput2,\n-      sideOutput3, sideOutput4, sideOutput5};\n+  public final transient DefaultOutputPort<?>[] additionalOutputPorts = {\n+    additionalOutput1, additionalOutput2, additionalOutput3, additionalOutput4, additionalOutput5\n+  };\n \n   @Override\n   public <T> void output(TupleTag<T> tag, WindowedValue<T> tuple) {\n-    DefaultOutputPort<ApexStreamTuple<?>> sideOutputPort = sideOutputPortMapping.get(tag);\n-    if (sideOutputPort != null) {\n-      sideOutputPort.emit(ApexStreamTuple.DataTuple.of(tuple));\n+    DefaultOutputPort<ApexStreamTuple<?>> additionalOutputPort =\n+        additionalOutputPortMapping.get(tag);\n+    if (additionalOutputPort != null) {\n+      additionalOutputPort.emit(ApexStreamTuple.DataTuple.of(tuple));\n     } else {\n       output.emit(ApexStreamTuple.DataTuple.of(tuple));\n     }\n@@ -255,6 +255,17 @@ public void process(ApexStreamTuple<WindowedValue<Iterable<?>>> t) {\n   private Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem) {\n     try {\n       pushbackDoFnRunner.startBundle();\n+      if (currentKeyStateInternals != null) {\n+        InputT value = elem.getValue();\n+        Object key;\n+        if (value instanceof KeyedWorkItem) {\n+          key = ((KeyedWorkItem) value).key();\n+        } else {\n+          key = ((KV) value).getKey();\n+        }\n+        ((StateInternalsProxy) currentKeyStateInternals).setKey(key);\n+        currentKeyTimerInternals.currentKey = key;\n+      }\n       Iterable<WindowedValue<InputT>> pushedBack = pushbackDoFnRunner\n           .processElementInReadyWindows(elem);\n       pushbackDoFnRunner.finishBundle();\n@@ -298,35 +309,74 @@ public void setup(OperatorContext context) {\n       sideInputReader = sideInputHandler;\n     }\n \n-    for (int i = 0; i < sideOutputTags.size(); i++) {\n+    for (int i = 0; i < additionalOutputTags.size(); i++) {\n       @SuppressWarnings(\"unchecked\")\n       DefaultOutputPort<ApexStreamTuple<?>> port = (DefaultOutputPort<ApexStreamTuple<?>>)\n-          sideOutputPorts[i];\n-      sideOutputPortMapping.put(sideOutputTags.get(i), port);\n+          additionalOutputPorts[i];\n+      additionalOutputPortMapping.put(additionalOutputTags.get(i), port);\n     }\n \n+    NoOpStepContext stepContext = new NoOpStepContext() {\n+\n+      @Override\n+      public StateInternals<?> stateInternals() {\n+        return currentKeyStateInternals;\n+      }\n+\n+      @Override\n+      public TimerInternals timerInternals() {\n+        return currentKeyTimerInternals;\n+      }\n+\n+    };\n     DoFnRunner<InputT, OutputT> doFnRunner = DoFnRunners.simpleRunner(\n         pipelineOptions.get(),\n         doFn,\n         sideInputReader,\n         this,\n         mainOutputTag,\n-        sideOutputTags,\n-        new NoOpStepContext(),\n+        additionalOutputTags,\n+        stepContext,\n         new NoOpAggregatorFactory(),\n         windowingStrategy\n         );\n \n-    pushbackDoFnRunner =\n-        PushbackSideInputDoFnRunner.create(doFnRunner, sideInputs, sideInputHandler);\n+    doFnInvoker = DoFnInvokers.invokerFor(doFn);\n+    doFnInvoker.invokeSetup();\n \n-    try {\n-      doFn.setup();\n-    } catch (Exception e) {\n-      Throwables.propagateIfPossible(e);\n-      throw new RuntimeException(e);\n+    if (this.currentKeyStateInternals != null) {\n+\n+      StatefulDoFnRunner.CleanupTimer cleanupTimer =\n+          new StatefulDoFnRunner.TimeInternalsCleanupTimer(\n+              stepContext.timerInternals(), windowingStrategy);\n+\n+      @SuppressWarnings({\"rawtypes\"})\n+      Coder windowCoder = windowingStrategy.getWindowFn().windowCoder();\n+\n+      @SuppressWarnings({\"unchecked\"})\n+      StatefulDoFnRunner.StateCleaner<?> stateCleaner =\n+          new StatefulDoFnRunner.StateInternalsStateCleaner<>(\n+              doFn, stepContext.stateInternals(), windowCoder);\n+\n+      doFnRunner = DoFnRunners.defaultStatefulDoFnRunner(\n+          doFn,\n+          doFnRunner,\n+          stepContext,\n+          new NoOpAggregatorFactory(),\n+          windowingStrategy,\n+          cleanupTimer,\n+          stateCleaner);\n     }\n \n+    pushbackDoFnRunner =\n+        SimplePushbackSideInputDoFnRunner.create(doFnRunner, sideInputs, sideInputHandler);\n+\n+  }\n+\n+  @Override\n+  public void teardown() {\n+    doFnInvoker.invokeTeardown();\n+    super.teardown();\n   }\n \n   @Override\n@@ -393,4 +443,70 @@ public void clear() {\n \n   }\n \n+  private class CurrentKeyTimerInternals<K> implements TimerInternals {\n+\n+    private TimerInternalsFactory<K> factory = new TimerInternalsFactory<K>() {\n+      @Override\n+      public TimerInternals timerInternalsForKey(K key) {\n+        InMemoryTimerInternals timerInternals = perKeyTimerInternals.get(key);\n+        if (timerInternals == null) {\n+          perKeyTimerInternals.put(key, timerInternals = new InMemoryTimerInternals());\n+        }\n+        return timerInternals;\n+      }\n+    };\n+\n+    // TODO: durable state store\n+    final Map<K, InMemoryTimerInternals> perKeyTimerInternals = new HashMap<>();\n+    private K currentKey;\n+\n+    @Override\n+    public void setTimer(StateNamespace namespace, String timerId, Instant target,\n+        TimeDomain timeDomain) {\n+      factory.timerInternalsForKey(currentKey).setTimer(\n+          namespace, timerId, target, timeDomain);\n+    }\n+\n+    @Override\n+    public void setTimer(TimerData timerData) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void deleteTimer(StateNamespace namespace, String timerId, TimeDomain timeDomain) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void deleteTimer(StateNamespace namespace, String timerId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public void deleteTimer(TimerData timerKey) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Instant currentProcessingTime() {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Instant currentSynchronizedProcessingTime() {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Instant currentInputWatermarkTime() {\n+      return new Instant(currentInputWatermark);\n+    }\n+\n+    @Override\n+    public Instant currentOutputWatermarkTime() {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "sha": "52d1d430fdd926c083227c3cb9631a68f6686490",
                "status": "modified"
            },
            {
                "additions": 184,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java",
                "changes": 184,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java",
                "patch": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.apex.translation.operators;\n+\n+import com.datatorrent.api.DefaultInputPort;\n+import com.datatorrent.api.DefaultOutputPort;\n+import com.datatorrent.api.annotation.OutputPortFieldAnnotation;\n+import com.datatorrent.common.util.BaseOperator;\n+import com.esotericsoftware.kryo.serializers.FieldSerializer.Bind;\n+import com.esotericsoftware.kryo.serializers.JavaSerializer;\n+import com.google.common.base.Throwables;\n+import com.google.common.collect.Iterables;\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.Collections;\n+import org.apache.beam.runners.apex.ApexPipelineOptions;\n+import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Apex operator for simple native map operations.\n+ */\n+public class ApexProcessFnOperator<InputT> extends BaseOperator {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ApexProcessFnOperator.class);\n+  private boolean traceTuples = false;\n+  @Bind(JavaSerializer.class)\n+  private final ApexOperatorFn<InputT> fn;\n+\n+  public ApexProcessFnOperator(ApexOperatorFn<InputT> fn, boolean traceTuples) {\n+    super();\n+    this.traceTuples = traceTuples;\n+    this.fn = fn;\n+  }\n+\n+  @SuppressWarnings(\"unused\")\n+  private ApexProcessFnOperator() {\n+    // for Kryo\n+    fn = null;\n+  }\n+\n+  private final transient OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter =\n+      new OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>>() {\n+    @Override\n+    public void emit(ApexStreamTuple<? extends WindowedValue<?>> tuple) {\n+      if (traceTuples) {\n+        LOG.debug(\"\\nemitting {}\\n\", tuple);\n+      }\n+      outputPort.emit(tuple);\n+    }\n+  };\n+\n+  /**\n+   * Something that emits results.\n+   */\n+  public interface OutputEmitter<T> {\n+    void emit(T tuple);\n+  };\n+\n+  /**\n+   * The processing logic for this operator.\n+   */\n+  public interface ApexOperatorFn<InputT> extends Serializable {\n+    void process(ApexStreamTuple<WindowedValue<InputT>> input,\n+        OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter) throws Exception;\n+  }\n+\n+  /**\n+   * Convert {@link KV} into {@link KeyedWorkItem}s.\n+   */\n+  public static class ToKeyedWorkItems<K, V> implements ApexOperatorFn<KV<K, V>> {\n+    @Override\n+    public final void process(ApexStreamTuple<WindowedValue<KV<K, V>>> tuple,\n+        OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter) {\n+\n+      if (tuple instanceof ApexStreamTuple.WatermarkTuple) {\n+        outputEmitter.emit(tuple);\n+      } else {\n+        for (WindowedValue<KV<K, V>> in : tuple.getValue().explodeWindows()) {\n+          KeyedWorkItem<K, V> kwi = KeyedWorkItems.elementsWorkItem(in.getValue().getKey(),\n+              Collections.singletonList(in.withValue(in.getValue().getValue())));\n+          outputEmitter.emit(ApexStreamTuple.DataTuple.of(in.withValue(kwi)));\n+        }\n+      }\n+    }\n+  }\n+\n+  public static <T, W extends BoundedWindow> ApexProcessFnOperator<T> assignWindows(\n+      WindowFn<T, W> windowFn, ApexPipelineOptions options) {\n+    ApexOperatorFn<T> fn = new AssignWindows<T, W>(windowFn);\n+    return new ApexProcessFnOperator<T>(fn, options.isTupleTracingEnabled());\n+  }\n+\n+  /**\n+   * Function for implementing {@link org.apache.beam.sdk.transforms.windowing.Window.Assign}.\n+   */\n+  private static class AssignWindows<T, W extends BoundedWindow> implements ApexOperatorFn<T> {\n+    private final WindowFn<T, W> windowFn;\n+\n+    private AssignWindows(WindowFn<T, W> windowFn) {\n+      this.windowFn = windowFn;\n+    }\n+\n+    @Override\n+    public final void process(ApexStreamTuple<WindowedValue<T>> tuple,\n+        OutputEmitter<ApexStreamTuple<? extends WindowedValue<?>>> outputEmitter) throws Exception {\n+      if (tuple instanceof ApexStreamTuple.WatermarkTuple) {\n+        outputEmitter.emit(tuple);\n+      } else {\n+        final WindowedValue<T> input = tuple.getValue();\n+        Collection<W> windows =\n+            (windowFn).assignWindows(\n+                (windowFn).new AssignContext() {\n+                    @Override\n+                    public T element() {\n+                      return input.getValue();\n+                    }\n+\n+                    @Override\n+                    public Instant timestamp() {\n+                      return input.getTimestamp();\n+                    }\n+\n+                    @Override\n+                    public BoundedWindow window() {\n+                      return Iterables.getOnlyElement(input.getWindows());\n+                    }\n+                  });\n+        for (W w: windows) {\n+          WindowedValue<T> wv = WindowedValue.of(input.getValue(), input.getTimestamp(),\n+              w, input.getPane());\n+          outputEmitter.emit(ApexStreamTuple.DataTuple.of(wv));\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Input port.\n+   */\n+  public final transient DefaultInputPort<ApexStreamTuple<WindowedValue<InputT>>> inputPort =\n+      new DefaultInputPort<ApexStreamTuple<WindowedValue<InputT>>>() {\n+    @Override\n+    public void process(ApexStreamTuple<WindowedValue<InputT>> tuple) {\n+      try {\n+        fn.process(tuple, outputEmitter);\n+      } catch (Exception e) {\n+        Throwables.throwIfUnchecked(e);\n+        throw new RuntimeException(e);\n+      }\n+    }\n+  };\n+\n+  /**\n+   * Output port.\n+   */\n+  @OutputPortFieldAnnotation(optional = true)\n+  public final transient DefaultOutputPort<ApexStreamTuple<? extends WindowedValue<?>>>\n+    outputPort = new DefaultOutputPort<>();\n+\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexProcessFnOperator.java",
                "sha": "835c9e0cdec8f55e5239794dcb79214ca581988b",
                "status": "added"
            },
            {
                "additions": 67,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 34,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.apex.translation.utils;\n \n+import com.datatorrent.netlet.util.Slice;\n import com.esotericsoftware.kryo.DefaultSerializer;\n import com.esotericsoftware.kryo.io.Input;\n import com.esotericsoftware.kryo.serializers.JavaSerializer;\n@@ -27,24 +28,28 @@\n import java.io.Serializable;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Map;\n import org.apache.beam.runners.core.StateInternals;\n import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.StateTag;\n import org.apache.beam.runners.core.StateTag.StateBinder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.Coder.Context;\n+import org.apache.beam.sdk.coders.CoderException;\n import org.apache.beam.sdk.coders.InstantCoder;\n import org.apache.beam.sdk.coders.ListCoder;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.Combine.KeyedCombineFn;\n import org.apache.beam.sdk.transforms.CombineWithContext.KeyedCombineFnWithContext;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n+import org.apache.beam.sdk.util.CoderUtils;\n import org.apache.beam.sdk.util.CombineFnUtil;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.SetState;\n@@ -56,34 +61,25 @@\n import org.joda.time.Instant;\n \n /**\n- * Implementation of {@link StateInternals} that can be serialized and\n- * checkpointed with the operator. Suitable for small states, in the future this\n- * should be based on the incremental state saving components in the Apex\n- * library.\n+ * Implementation of {@link StateInternals} for transient use.\n+ *\n+ * <p>For fields that need to be serialized, use {@link ApexStateInternalsFactory}\n+ * or {@link StateInternalsProxy}\n  */\n-@DefaultSerializer(JavaSerializer.class)\n-public class ApexStateInternals<K> implements StateInternals<K>, Serializable {\n-  private static final long serialVersionUID = 1L;\n-  public static <K> ApexStateInternals<K> forKey(K key) {\n-    return new ApexStateInternals<>(key);\n-  }\n-\n+public class ApexStateInternals<K> implements StateInternals<K> {\n   private final K key;\n+  private final Table<String, String, byte[]> stateTable;\n \n-  protected ApexStateInternals(K key) {\n+  protected ApexStateInternals(K key, Table<String, String, byte[]> stateTable) {\n     this.key = key;\n+    this.stateTable = stateTable;\n   }\n \n   @Override\n   public K getKey() {\n     return key;\n   }\n \n-  /**\n-   * Serializable state for internals (namespace to state tag to coded value).\n-   */\n-  private final Table<String, String, byte[]> stateTable = HashBasedTable.create();\n-\n   @Override\n   public <T extends State> T state(StateNamespace namespace, StateTag<? super K, T> address) {\n     return state(namespace, address, StateContexts.nullContext());\n@@ -139,12 +135,12 @@ private ApexStateBinder(K key, StateNamespace namespace, StateTag<? super K, ?>\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             final CombineFn<InputT, AccumT, OutputT> combineFn) {\n-      return new ApexAccumulatorCombiningState<>(\n+      return new ApexCombiningState<>(\n           namespace,\n           address,\n           accumCoder,\n@@ -161,22 +157,22 @@ private ApexStateBinder(K key, StateNamespace namespace, StateTag<? super K, ?>\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n-      return new ApexAccumulatorCombiningState<>(\n+      return new ApexCombiningState<>(\n           namespace,\n           address,\n           accumCoder,\n           key, combineFn);\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValueWithContext(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n       return bindKeyedCombiningValue(address, accumCoder, CombineFnUtil.bindContext(combineFn, c));\n@@ -323,14 +319,14 @@ public Boolean read() {\n \n   }\n \n-  private final class ApexAccumulatorCombiningState<K, InputT, AccumT, OutputT>\n+  private final class ApexCombiningState<K, InputT, AccumT, OutputT>\n       extends AbstractState<AccumT>\n-      implements AccumulatorCombiningState<InputT, AccumT, OutputT> {\n+      implements CombiningState<InputT, AccumT, OutputT> {\n     private final K key;\n     private final KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn;\n \n-    private ApexAccumulatorCombiningState(StateNamespace namespace,\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+    private ApexCombiningState(StateNamespace namespace,\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n         Coder<AccumT> coder,\n         K key, KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n       super(namespace, address, coder);\n@@ -339,7 +335,7 @@ private ApexAccumulatorCombiningState(StateNamespace namespace,\n     }\n \n     @Override\n-    public ApexAccumulatorCombiningState<K, InputT, AccumT, OutputT> readLater() {\n+    public ApexCombiningState<K, InputT, AccumT, OutputT> readLater() {\n       return this;\n     }\n \n@@ -437,17 +433,54 @@ public Boolean read() {\n   }\n \n   /**\n-   * Factory for {@link ApexStateInternals}.\n+   * Implementation of {@link StateInternals} that can be serialized and\n+   * checkpointed with the operator. Suitable for small states, in the future this\n+   * should be based on the incremental state saving components in the Apex\n+   * library.\n    *\n    * @param <K> key type\n    */\n+  @DefaultSerializer(JavaSerializer.class)\n   public static class ApexStateInternalsFactory<K>\n       implements StateInternalsFactory<K>, Serializable {\n     private static final long serialVersionUID = 1L;\n+    /**\n+     * Serializable state for internals (namespace to state tag to coded value).\n+     */\n+    private Map<Slice, Table<String, String, byte[]>> perKeyState = new HashMap<>();\n+    private final Coder<K> keyCoder;\n+\n+    private ApexStateInternalsFactory(Coder<K> keyCoder) {\n+      this.keyCoder = keyCoder;\n+    }\n \n     @Override\n-    public StateInternals<K> stateInternalsForKey(K key) {\n-      return ApexStateInternals.forKey(key);\n+    public ApexStateInternals<K> stateInternalsForKey(K key) {\n+      final Slice keyBytes;\n+      try {\n+        keyBytes = (key != null) ? new Slice(CoderUtils.encodeToByteArray(keyCoder, key)) :\n+          new Slice(null);\n+      } catch (CoderException e) {\n+        throw new RuntimeException(e);\n+      }\n+      Table<String, String, byte[]> stateTable = perKeyState.get(keyBytes);\n+      if (stateTable == null) {\n+        stateTable = HashBasedTable.create();\n+        perKeyState.put(keyBytes, stateTable);\n+      }\n+      return new ApexStateInternals<>(key, stateTable);\n+    }\n+\n+  }\n+\n+  /**\n+   * Factory to create the state internals.\n+   */\n+  public static class ApexStateBackend implements Serializable {\n+    private static final long serialVersionUID = 1L;\n+\n+    public <K> ApexStateInternalsFactory<K> newStateInternalsFactory(Coder<K> keyCoder) {\n+      return new ApexStateInternalsFactory<K>(keyCoder);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternals.java",
                "sha": "cfc57cd5753651f3db3d9c188877e6ed810c0981",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java",
                "patch": "@@ -48,7 +48,7 @@ public void noteOutput(WindowedValue<?> output) {\n   }\n \n   @Override\n-  public void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output) {\n+  public void noteOutput(TupleTag<?> tag, WindowedValue<?> output) {\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/NoOpStepContext.java",
                "sha": "cc64c7c6bb17c57b4f64400c9b4aab1db39e8e3d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java",
                "patch": "@@ -18,20 +18,24 @@\n package org.apache.beam.runners.apex.translation.utils;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n-\n import java.io.Externalizable;\n import java.io.IOException;\n import java.io.ObjectInput;\n import java.io.ObjectOutput;\n-\n+import java.util.concurrent.atomic.AtomicBoolean;\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n+import org.apache.beam.sdk.io.FileSystems;\n import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.util.IOChannelUtils;\n \n /**\n  * A wrapper to enable serialization of {@link PipelineOptions}.\n  */\n public class SerializablePipelineOptions implements Externalizable {\n \n+  /* Used to ensure we initialize file systems exactly once, because it's a slow operation. */\n+  private static final AtomicBoolean FILE_SYSTEMS_INTIIALIZED = new AtomicBoolean(false);\n+\n   private transient ApexPipelineOptions pipelineOptions;\n \n   public SerializablePipelineOptions(ApexPipelineOptions pipelineOptions) {\n@@ -55,6 +59,11 @@ public void readExternal(ObjectInput in) throws IOException, ClassNotFoundExcept\n     String s = in.readUTF();\n     this.pipelineOptions = new ObjectMapper().readValue(s, PipelineOptions.class)\n         .as(ApexPipelineOptions.class);\n+\n+    if (FILE_SYSTEMS_INTIIALIZED.compareAndSet(false, true)) {\n+      IOChannelUtils.registerIOFactoriesAllowOverride(pipelineOptions);\n+      FileSystems.setDefaultConfigInWorkers(pipelineOptions);\n+    }\n   }\n \n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/SerializablePipelineOptions.java",
                "sha": "1a47ed574e2bffbf72274eb07d368f75979129d0",
                "status": "modified"
            },
            {
                "additions": 67,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java",
                "changes": 67,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java",
                "patch": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.apex.translation.utils;\n+\n+import com.esotericsoftware.kryo.DefaultSerializer;\n+import com.esotericsoftware.kryo.serializers.JavaSerializer;\n+import java.io.Serializable;\n+import org.apache.beam.runners.core.StateInternals;\n+import org.apache.beam.runners.core.StateInternalsFactory;\n+import org.apache.beam.runners.core.StateNamespace;\n+import org.apache.beam.runners.core.StateTag;\n+import org.apache.beam.sdk.util.state.State;\n+import org.apache.beam.sdk.util.state.StateContext;\n+\n+/**\n+ * State internals for reusable processing context.\n+ * @param <K>\n+ */\n+@DefaultSerializer(JavaSerializer.class)\n+public class StateInternalsProxy<K> implements StateInternals<K>, Serializable {\n+\n+  private final StateInternalsFactory<K> factory;\n+  private transient K currentKey;\n+\n+  public StateInternalsProxy(ApexStateInternals.ApexStateInternalsFactory<K> factory) {\n+    this.factory = factory;\n+  }\n+\n+  public StateInternalsFactory<K> getFactory() {\n+    return this.factory;\n+  }\n+\n+  public void setKey(K key) {\n+    currentKey = key;\n+  }\n+\n+  @Override\n+  public K getKey() {\n+    return currentKey;\n+  }\n+\n+  @Override\n+  public <T extends State> T state(StateNamespace namespace, StateTag<? super K, T> address) {\n+    return factory.stateInternalsForKey(currentKey).state(namespace, address);\n+  }\n+\n+  @Override\n+  public <T extends State> T state(StateNamespace namespace, StateTag<? super K, T> address,\n+      StateContext<?> c) {\n+    return factory.stateInternalsForKey(currentKey).state(namespace, address, c);\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/StateInternalsProxy.java",
                "sha": "1f28364269215c03b6ce866ab416b8033e542ef9",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java",
                "patch": "@@ -55,7 +55,7 @@ public ValuesSource(Iterable<T> values, Coder<T> coder) {\n   }\n \n   @Override\n-  public java.util.List<? extends UnboundedSource<T, CheckpointMark>> generateInitialSplits(\n+  public java.util.List<? extends UnboundedSource<T, CheckpointMark>> split(\n       int desiredNumSplits, PipelineOptions options) throws Exception {\n     return Collections.singletonList(this);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/utils/ValuesSource.java",
                "sha": "62c92a00489260732f22d9e508872a27ee6e79a8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java",
                "patch": "@@ -39,7 +39,7 @@\n   private static final long serialVersionUID = 1L;\n \n   @Override\n-  public List<? extends UnboundedSource<String, CheckpointMark>> generateInitialSplits(\n+  public List<? extends UnboundedSource<String, CheckpointMark>> split(\n       int desiredNumSplits, PipelineOptions options) throws Exception {\n     return Collections.<UnboundedSource<String, CheckpointMark>>singletonList(this);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/examples/UnboundedTextSource.java",
                "sha": "abe97f6de049a2615bcadbc5924380c7323e6ebf",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java",
                "patch": "@@ -66,7 +66,7 @@ public void testGlobalWindowMinTimestamp() throws Exception {\n     input.setCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of()));\n \n     ApexGroupByKeyOperator<String, Integer> operator = new ApexGroupByKeyOperator<>(options,\n-        input, new ApexStateInternals.ApexStateInternalsFactory<String>()\n+        input, new ApexStateInternals.ApexStateBackend()\n         );\n \n     operator.setup(null);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ApexGroupByKeyOperatorTest.java",
                "sha": "4b73114da4feee04697192a24af57a05f35afd8c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "patch": "@@ -110,7 +110,8 @@ public void testFlattenSingleCollection() {\n     PCollectionList.of(single).apply(Flatten.<String>pCollections())\n       .apply(ParDo.of(new EmbeddedCollector()));\n     translator.translate(p, dag);\n-    Assert.assertNotNull(dag.getOperatorMeta(\"ParDo(EmbeddedCollector)\"));\n+    Assert.assertNotNull(\n+        dag.getOperatorMeta(\"ParDo(EmbeddedCollector)/ParMultiDo(EmbeddedCollector)\"));\n   }\n \n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "sha": "64ca0ee4fd0769945d77f9f3ceb2517f76d760bc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "patch": "@@ -131,7 +131,7 @@ public TestSource(List<KV<String, Instant>> data, Instant watermark) {\n     }\n \n     @Override\n-    public List<? extends UnboundedSource<String, CheckpointMark>> generateInitialSplits(\n+    public List<? extends UnboundedSource<String, CheckpointMark>> split(\n         int desiredNumSplits, PipelineOptions options) throws Exception {\n       return Collections.<UnboundedSource<String, CheckpointMark>>singletonList(this);\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "sha": "193de718ceff67b8dd630b7d550d3869a9f6fc88",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 18,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java",
                "patch": "@@ -68,11 +68,11 @@\n import org.slf4j.LoggerFactory;\n \n /**\n- * integration test for {@link ParDoBoundTranslator}.\n+ * integration test for {@link ParDoTranslator}.\n  */\n @RunWith(JUnit4.class)\n-public class ParDoBoundTranslatorTest {\n-  private static final Logger LOG = LoggerFactory.getLogger(ParDoBoundTranslatorTest.class);\n+public class ParDoTranslatorTest {\n+  private static final Logger LOG = LoggerFactory.getLogger(ParDoTranslatorTest.class);\n   private static final long SLEEP_MILLIS = 500;\n   private static final long TIMEOUT_MILLIS = 30000;\n \n@@ -98,7 +98,7 @@ public void test() throws Exception {\n     Assert.assertNotNull(om);\n     Assert.assertEquals(om.getOperator().getClass(), ApexReadUnboundedInputOperator.class);\n \n-    om = dag.getOperatorMeta(\"ParDo(Add)\");\n+    om = dag.getOperatorMeta(\"ParDo(Add)/ParMultiDo(Add)\");\n     Assert.assertNotNull(om);\n     Assert.assertEquals(om.getOperator().getClass(), ApexParDoOperator.class);\n \n@@ -216,7 +216,7 @@ public void testSerialization() throws Exception {\n             WindowingStrategy.globalDefault(),\n             Collections.<PCollectionView<?>>singletonList(singletonView),\n             coder,\n-            new ApexStateInternals.ApexStateInternalsFactory<Void>());\n+            new ApexStateInternals.ApexStateBackend());\n     operator.setup(null);\n     operator.beginWindow(0);\n     WindowedValue<Integer> wv1 = WindowedValue.valueInGlobalWindow(1);\n@@ -267,7 +267,7 @@ public void testMultiOutputParDoWithSideInputs() throws Exception {\n \n     List<Integer> inputs = Arrays.asList(3, -42, 666);\n     final TupleTag<String> mainOutputTag = new TupleTag<>(\"main\");\n-    final TupleTag<Void> sideOutputTag = new TupleTag<>(\"sideOutput\");\n+    final TupleTag<Void> additionalOutputTag = new TupleTag<>(\"output\");\n \n     PCollectionView<Integer> sideInput1 = pipeline\n         .apply(\"CreateSideInput1\", Create.of(11))\n@@ -281,16 +281,17 @@ public void testMultiOutputParDoWithSideInputs() throws Exception {\n \n     PCollectionTuple outputs = pipeline\n         .apply(Create.of(inputs))\n-        .apply(ParDo.withSideInputs(sideInput1)\n-            .withSideInputs(sideInputUnread)\n-            .withSideInputs(sideInput2)\n-            .withOutputTags(mainOutputTag, TupleTagList.of(sideOutputTag))\n+        .apply(ParDo\n             .of(new TestMultiOutputWithSideInputsFn(\n                 Arrays.asList(sideInput1, sideInput2),\n-                Arrays.<TupleTag<String>>asList())));\n+                Arrays.<TupleTag<String>>asList()))\n+            .withSideInputs(sideInput1)\n+            .withSideInputs(sideInputUnread)\n+            .withSideInputs(sideInput2)\n+            .withOutputTags(mainOutputTag, TupleTagList.of(additionalOutputTag)));\n \n     outputs.get(mainOutputTag).apply(ParDo.of(new EmbeddedCollector()));\n-    outputs.get(sideOutputTag).setCoder(VoidCoder.of());\n+    outputs.get(additionalOutputTag).setCoder(VoidCoder.of());\n     ApexRunnerResult result = (ApexRunnerResult) pipeline.run();\n \n     HashSet<String> expected = Sets.newHashSet(\"processing: 3: [11, 222]\",\n@@ -311,12 +312,12 @@ public void testMultiOutputParDoWithSideInputs() throws Exception {\n     private static final long serialVersionUID = 1L;\n \n     final List<PCollectionView<Integer>> sideInputViews = new ArrayList<>();\n-    final List<TupleTag<String>> sideOutputTupleTags = new ArrayList<>();\n+    final List<TupleTag<String>> additionalOutputTupleTags = new ArrayList<>();\n \n     public TestMultiOutputWithSideInputsFn(List<PCollectionView<Integer>> sideInputViews,\n-        List<TupleTag<String>> sideOutputTupleTags) {\n+        List<TupleTag<String>> additionalOutputTupleTags) {\n       this.sideInputViews.addAll(sideInputViews);\n-      this.sideOutputTupleTags.addAll(sideOutputTupleTags);\n+      this.additionalOutputTupleTags.addAll(additionalOutputTupleTags);\n     }\n \n     @ProcessElement\n@@ -333,9 +334,9 @@ private void outputToAllWithSideInputs(ProcessContext c, String value) {\n         value += \": \" + sideInputValues;\n       }\n       c.output(value);\n-      for (TupleTag<String> sideOutputTupleTag : sideOutputTupleTags) {\n-        c.sideOutput(sideOutputTupleTag,\n-                     sideOutputTupleTag.getId() + \": \" + value);\n+      for (TupleTag<String> additionalOutputTupleTag : additionalOutputTupleTags) {\n+        c.output(additionalOutputTupleTag,\n+                     additionalOutputTupleTag.getId() + \": \" + value);\n       }\n     }\n ",
                "previous_filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslatorTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoTranslatorTest.java",
                "sha": "1a5c8be4da5cf66f8a00809200134d23e999e511",
                "status": "renamed"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 17,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java",
                "patch": "@@ -24,6 +24,8 @@\n \n import com.datatorrent.lib.util.KryoCloneUtils;\n import java.util.Arrays;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateBackend;\n+import org.apache.beam.runners.apex.translation.utils.ApexStateInternals.ApexStateInternalsFactory;\n import org.apache.beam.runners.core.StateMerging;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.StateNamespaceForTest;\n@@ -35,9 +37,9 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.ValueState;\n import org.apache.beam.sdk.util.state.WatermarkHoldState;\n@@ -58,7 +60,7 @@\n \n   private static final StateTag<Object, ValueState<String>> STRING_VALUE_ADDR =\n       StateTags.value(\"stringValue\", StringUtf8Coder.of());\n-  private static final StateTag<Object, AccumulatorCombiningState<Integer, int[], Integer>>\n+  private static final StateTag<Object, CombiningState<Integer, int[], Integer>>\n       SUM_INTEGER_ADDR = StateTags.combiningValueFromInputInternal(\n           \"sumInteger\", VarIntCoder.of(), Sum.ofIntegers());\n   private static final StateTag<Object, BagState<String>> STRING_BAG_ADDR =\n@@ -76,7 +78,9 @@\n \n   @Before\n   public void initStateInternals() {\n-    underTest = new ApexStateInternals<>(null);\n+    underTest = new ApexStateInternals.ApexStateBackend()\n+        .newStateInternalsFactory(StringUtf8Coder.of())\n+        .stateInternalsForKey((String) null);\n   }\n \n   @Test\n@@ -148,7 +152,7 @@ public void testMergeBagIntoNewNamespace() throws Exception {\n \n   @Test\n   public void testCombiningValue() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     // State instances are cached, but depend on the namespace.\n     assertEquals(value, underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR));\n@@ -168,7 +172,7 @@ public void testCombiningValue() throws Exception {\n \n   @Test\n   public void testCombiningIsEmpty() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     assertThat(value.isEmpty().read(), Matchers.is(true));\n     ReadableState<Boolean> readFuture = value.isEmpty();\n@@ -181,9 +185,9 @@ public void testCombiningIsEmpty() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoSource() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n \n     value1.add(5);\n@@ -202,11 +206,11 @@ public void testMergeCombiningValueIntoSource() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoNewNamespace() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value3 =\n+    CombiningState<Integer, int[], Integer> value3 =\n         underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR);\n \n     value1.add(5);\n@@ -344,16 +348,21 @@ public void testMergeLatestWatermarkIntoSource() throws Exception {\n \n   @Test\n   public void testSerialization() throws Exception {\n-    ApexStateInternals<String> original = new ApexStateInternals<String>(null);\n-    ValueState<String> value = original.state(NAMESPACE_1, STRING_VALUE_ADDR);\n-    assertEquals(original.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n+    ApexStateInternalsFactory<String> sif = new ApexStateBackend().\n+        newStateInternalsFactory(StringUtf8Coder.of());\n+    ApexStateInternals<String> keyAndState = sif.stateInternalsForKey(\"dummy\");\n+\n+    ValueState<String> value = keyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR);\n+    assertEquals(keyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n     value.write(\"hello\");\n \n-    ApexStateInternals<String> cloned;\n-    assertNotNull(\"Serialization\", cloned = KryoCloneUtils.cloneObject(original));\n-    ValueState<String> clonedValue = cloned.state(NAMESPACE_1, STRING_VALUE_ADDR);\n+    ApexStateInternalsFactory<String> cloned;\n+    assertNotNull(\"Serialization\", cloned = KryoCloneUtils.cloneObject(sif));\n+    ApexStateInternals<String> clonedKeyAndState = cloned.stateInternalsForKey(\"dummy\");\n+\n+    ValueState<String> clonedValue = clonedKeyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR);\n     assertThat(clonedValue.read(), Matchers.equalTo(\"hello\"));\n-    assertEquals(cloned.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n+    assertEquals(clonedKeyAndState.state(NAMESPACE_1, STRING_VALUE_ADDR), value);\n   }\n \n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/ApexStateInternalsTest.java",
                "sha": "7160e4544ab1e252e2e94126a4194da93f314256",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java",
                "patch": "@@ -47,7 +47,7 @@ public CollectionSource(Collection<T> collection, Coder<T> coder) {\n   }\n \n   @Override\n-  public List<? extends UnboundedSource<T, CheckpointMark>> generateInitialSplits(\n+  public List<? extends UnboundedSource<T, CheckpointMark>> split(\n       int desiredNumSplits, PipelineOptions options) throws Exception {\n     return Collections.singletonList(this);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/utils/CollectionSource.java",
                "sha": "92812b4f4f5dfc62f4b68efbfdb6fbce57d4bb6a",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/pom.xml",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 53,
                "filename": "runners/core-construction-java/pom.xml",
                "patch": "@@ -29,7 +29,7 @@\n   </parent>\n \n   <artifactId>beam-runners-core-construction-java</artifactId>\n-  <name>Apache Beam :: Runners :: Core Java Construction</name>\n+  <name>Apache Beam :: Runners :: Core Construction Java</name>\n   <description>Beam Runners Core provides utilities to aid runner authors interact with a Pipeline\n     prior to execution.\n   </description>\n@@ -50,67 +50,40 @@\n           </systemPropertyVariables>\n         </configuration>\n       </plugin>\n-\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-shade-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <id>bundle-and-repackage</id>\n-            <phase>package</phase>\n-            <goals>\n-              <goal>shade</goal>\n-            </goals>\n-            <configuration>\n-              <shadeTestJar>true</shadeTestJar>\n-              <artifactSet>\n-                <includes>\n-                  <include>com.google.guava:guava</include>\n-                </includes>\n-              </artifactSet>\n-              <filters>\n-                <filter>\n-                  <artifact>*:*</artifact>\n-                  <excludes>\n-                    <exclude>META-INF/*.SF</exclude>\n-                    <exclude>META-INF/*.DSA</exclude>\n-                    <exclude>META-INF/*.RSA</exclude>\n-                  </excludes>\n-                </filter>\n-              </filters>\n-              <relocations>\n-                <!-- TODO: Once ready, change the following pattern to 'com'\n-                   only, exclude 'org.apache.beam.**', and remove\n-                   the second relocation. -->\n-                <relocation>\n-                  <pattern>com.google.common</pattern>\n-                  <shadedPattern>\n-                    org.apache.beam.runners.core.construction.repackaged.com.google.common\n-                  </shadedPattern>\n-                </relocation>\n-                <relocation>\n-                  <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>\n-                    org.apache.beam.runners.core.construction.repackaged.com.google.thirdparty\n-                  </shadedPattern>\n-                </relocation>\n-              </relocations>\n-              <transformers>\n-                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n-              </transformers>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n     </plugins>\n   </build>\n \n   <dependencies>\n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-sdks-common-runner-api</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-core</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>com.google.protobuf</groupId>\n+      <artifactId>protobuf-java</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-annotations</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.fasterxml.jackson.core</groupId>\n+      <artifactId>jackson-databind</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.google.code.findbugs</groupId>\n+      <artifactId>jsr305</artifactId>\n+    </dependency>\n+\n     <dependency>\n       <groupId>joda-time</groupId>\n       <artifactId>joda-time</artifactId>\n@@ -121,6 +94,17 @@\n       <artifactId>guava</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.slf4j</groupId>\n+      <artifactId>slf4j-api</artifactId>\n+    </dependency>\n+\n+    <dependency>\n+      <groupId>com.google.auto.value</groupId>\n+      <artifactId>auto-value</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+\n     <!-- test dependencies -->\n \n     <dependency>\n@@ -134,5 +118,12 @@\n       <artifactId>junit</artifactId>\n       <scope>test</scope>\n     </dependency>\n+\n+    <dependency>\n+      <groupId>org.mockito</groupId>\n+      <artifactId>mockito-all</artifactId>\n+      <scope>test</scope>\n+    </dependency>\n+\n   </dependencies>\n </project>",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/pom.xml",
                "sha": "dfab3e2ca7004e193227c9d0ba05f1abaa52ee4d",
                "status": "modified"
            },
            {
                "additions": 174,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java",
                "changes": 174,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java",
                "patch": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.BiMap;\n+import com.google.common.collect.ImmutableBiMap;\n+import com.google.protobuf.Any;\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.BytesValue;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.IterableCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.StandardCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.FunctionSpec;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.SdkFunctionSpec;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow.IntervalWindowCoder;\n+import org.apache.beam.sdk.util.CloudObject;\n+import org.apache.beam.sdk.util.Serializer;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n+\n+/** Converts to and from Beam Runner API representations of {@link Coder Coders}. */\n+public class Coders {\n+  private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n+\n+  // This URN says that the coder is just a UDF blob this SDK understands\n+  // TODO: standardize such things\n+  public static final String CUSTOM_CODER_URN = \"urn:beam:coders:javasdk:0.1\";\n+\n+  // The URNs for coders which are shared across languages\n+  @VisibleForTesting\n+  static final BiMap<Class<? extends StandardCoder>, String> KNOWN_CODER_URNS =\n+      ImmutableBiMap.<Class<? extends StandardCoder>, String>builder()\n+          .put(ByteArrayCoder.class, \"urn:beam:coders:bytes:0.1\")\n+          .put(KvCoder.class, \"urn:beam:coders:kv:0.1\")\n+          .put(VarLongCoder.class, \"urn:beam:coders:varint:0.1\")\n+          .put(IntervalWindowCoder.class, \"urn:beam:coders:interval_window:0.1\")\n+          .put(IterableCoder.class, \"urn:beam:coders:stream:0.1\")\n+          .put(GlobalWindow.Coder.class, \"urn:beam:coders:global_window:0.1\")\n+          .put(FullWindowedValueCoder.class, \"urn:beam:coders:windowed_value:0.1\")\n+          .build();\n+\n+  public static RunnerApi.Coder toProto(\n+      Coder<?> coder, @SuppressWarnings(\"unused\") SdkComponents components) throws IOException {\n+    if (KNOWN_CODER_URNS.containsKey(coder.getClass())) {\n+      return toKnownCoder(coder, components);\n+    }\n+    return toCustomCoder(coder);\n+  }\n+\n+  private static RunnerApi.Coder toKnownCoder(Coder<?> coder, SdkComponents components)\n+      throws IOException {\n+    checkArgument(\n+        coder instanceof StandardCoder,\n+        \"A Known %s must implement %s, but %s of class %s does not\",\n+        Coder.class.getSimpleName(),\n+        StandardCoder.class.getSimpleName(),\n+        coder,\n+        coder.getClass().getName());\n+    StandardCoder<?> stdCoder = (StandardCoder<?>) coder;\n+    List<String> componentIds = new ArrayList<>();\n+    for (Coder<?> componentCoder : stdCoder.getComponents()) {\n+      componentIds.add(components.registerCoder(componentCoder));\n+    }\n+    return RunnerApi.Coder.newBuilder()\n+        .addAllComponentCoderIds(componentIds)\n+        .setSpec(\n+            SdkFunctionSpec.newBuilder()\n+                .setSpec(FunctionSpec.newBuilder().setUrn(KNOWN_CODER_URNS.get(coder.getClass()))))\n+        .build();\n+  }\n+\n+  private static RunnerApi.Coder toCustomCoder(Coder<?> coder) throws IOException {\n+    RunnerApi.Coder.Builder coderBuilder = RunnerApi.Coder.newBuilder();\n+    return coderBuilder\n+        .setSpec(\n+            SdkFunctionSpec.newBuilder()\n+                .setSpec(\n+                    FunctionSpec.newBuilder()\n+                        .setUrn(CUSTOM_CODER_URN)\n+                        .setParameter(\n+                            Any.pack(\n+                                BytesValue.newBuilder()\n+                                    .setValue(\n+                                        ByteString.copyFrom(\n+                                            OBJECT_MAPPER.writeValueAsBytes(coder.asCloudObject())))\n+                                    .build()))))\n+        .build();\n+  }\n+\n+  public static Coder<?> fromProto(RunnerApi.Coder protoCoder, Components components)\n+      throws IOException {\n+    String coderSpecUrn = protoCoder.getSpec().getSpec().getUrn();\n+    if (coderSpecUrn.equals(CUSTOM_CODER_URN)) {\n+      return fromCustomCoder(protoCoder, components);\n+    }\n+    return fromKnownCoder(protoCoder, components);\n+  }\n+\n+  private static Coder<?> fromKnownCoder(RunnerApi.Coder coder, Components components)\n+      throws IOException {\n+    String coderUrn = coder.getSpec().getSpec().getUrn();\n+    List<Coder<?>> coderComponents = new LinkedList<>();\n+    for (String componentId : coder.getComponentCoderIdsList()) {\n+      Coder<?> innerCoder = fromProto(components.getCodersOrThrow(componentId), components);\n+      coderComponents.add(innerCoder);\n+    }\n+    switch (coderUrn) {\n+      case \"urn:beam:coders:bytes:0.1\":\n+        return ByteArrayCoder.of();\n+      case \"urn:beam:coders:kv:0.1\":\n+        return KvCoder.of(coderComponents);\n+      case \"urn:beam:coders:varint:0.1\":\n+        return VarLongCoder.of();\n+      case \"urn:beam:coders:interval_window:0.1\":\n+        return IntervalWindowCoder.of();\n+      case \"urn:beam:coders:stream:0.1\":\n+        return IterableCoder.of(coderComponents);\n+      case \"urn:beam:coders:global_window:0.1\":\n+        return GlobalWindow.Coder.INSTANCE;\n+      case \"urn:beam:coders:windowed_value:0.1\":\n+        return WindowedValue.FullWindowedValueCoder.of(coderComponents);\n+      default:\n+        throw new IllegalStateException(\n+            String.format(\n+                \"Unknown coder URN %s. Known URNs: %s\", coderUrn, KNOWN_CODER_URNS.values()));\n+    }\n+  }\n+\n+  private static Coder<?> fromCustomCoder(\n+      RunnerApi.Coder protoCoder, @SuppressWarnings(\"unused\") Components components)\n+      throws IOException {\n+    CloudObject coderCloudObject =\n+        OBJECT_MAPPER.readValue(\n+            protoCoder\n+                .getSpec()\n+                .getSpec()\n+                .getParameter()\n+                .unpack(BytesValue.class)\n+                .getValue()\n+                .toByteArray(),\n+            CloudObject.class);\n+    return Serializer.deserialize(coderCloudObject, Coder.class);\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Coders.java",
                "sha": "043a01030d6ec016ffb4bca7b19e4558dbcca028",
                "status": "added"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java",
                "patch": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Flatten.PCollections;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ * A {@link PTransformOverrideFactory} that will apply a flatten where no element appears in the\n+ * input {@link PCollectionList} more than once.\n+ */\n+public class DeduplicatedFlattenFactory<T>\n+    implements PTransformOverrideFactory<\n+        PCollectionList<T>, PCollection<T>, Flatten.PCollections<T>> {\n+\n+  public static <T> DeduplicatedFlattenFactory<T> create() {\n+    return new DeduplicatedFlattenFactory<>();\n+  }\n+\n+  private DeduplicatedFlattenFactory() {}\n+\n+  @Override\n+  public PTransformReplacement<PCollectionList<T>, PCollection<T>> getReplacementTransform(\n+      AppliedPTransform<PCollectionList<T>, PCollection<T>, PCollections<T>> transform) {\n+    return PTransformReplacement.of(\n+        getInput(transform.getInputs(), transform.getPipeline()),\n+        new FlattenWithoutDuplicateInputs<T>());\n+  }\n+\n+  /**\n+   * {@inheritDoc}.\n+   *\n+   * <p>The input {@link PCollectionList} that is constructed will have the same values in the same\n+   */\n+  private PCollectionList<T> getInput(Map<TupleTag<?>, PValue> inputs, Pipeline p) {\n+    PCollectionList<T> pCollections = PCollectionList.empty(p);\n+    for (PValue input : inputs.values()) {\n+      PCollection<T> pcollection = (PCollection<T>) input;\n+      pCollections = pCollections.and(pcollection);\n+    }\n+    return pCollections;\n+  }\n+\n+  @Override\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n+    return ReplacementOutputs.singleton(outputs, newOutput);\n+  }\n+\n+  @VisibleForTesting\n+  static class FlattenWithoutDuplicateInputs<T>\n+      extends PTransform<PCollectionList<T>, PCollection<T>> {\n+    @Override\n+    public PCollection<T> expand(PCollectionList<T> input) {\n+      Map<PCollection<T>, Integer> instances = new HashMap<>();\n+      for (PCollection<T> pCollection : input.getAll()) {\n+        int existing = instances.get(pCollection) == null ? 0 : instances.get(pCollection);\n+        instances.put(pCollection, existing + 1);\n+      }\n+      PCollectionList<T> output = PCollectionList.empty(input.getPipeline());\n+      for (Map.Entry<PCollection<T>, Integer> instanceEntry : instances.entrySet()) {\n+        if (instanceEntry.getValue().equals(1)) {\n+          output = output.and(instanceEntry.getKey());\n+        } else {\n+          String duplicationName = String.format(\"Multiply %s\", instanceEntry.getKey().getName());\n+          PCollection<T> duplicated =\n+              instanceEntry\n+                  .getKey()\n+                  .apply(duplicationName, ParDo.of(new DuplicateFn<T>(instanceEntry.getValue())));\n+          output = output.and(duplicated);\n+        }\n+      }\n+      return output.apply(Flatten.<T>pCollections());\n+    }\n+  }\n+\n+  private static class DuplicateFn<T> extends DoFn<T, T> {\n+    private final int numTimes;\n+\n+    private DuplicateFn(int numTimes) {\n+      this.numTimes = numTimes;\n+    }\n+\n+    @ProcessElement\n+    public void emitCopies(ProcessContext context) {\n+      for (int i = 0; i < numTimes; i++) {\n+        context.output(context.element());\n+      }\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactory.java",
                "sha": "13e7593057d2c5908a2486805501fd5020b3ab32",
                "status": "added"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 14,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java",
                "patch": "@@ -20,18 +20,18 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n \n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Flatten.PCollections;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} that provides an empty {@link Create} to replace a {@link\n@@ -50,22 +50,28 @@\n   private EmptyFlattenAsCreateFactory() {}\n \n   @Override\n-  public PTransform<PCollectionList<T>, PCollection<T>> getReplacementTransform(\n-      Flatten.PCollections<T> transform) {\n-    return (PTransform) Create.empty(VoidCoder.of());\n-  }\n-\n-  @Override\n-  public PCollectionList<T> getInput(\n-      List<TaggedPValue> inputs, Pipeline p) {\n+  public PTransformReplacement<PCollectionList<T>, PCollection<T>> getReplacementTransform(\n+      AppliedPTransform<PCollectionList<T>, PCollection<T>, PCollections<T>> transform) {\n     checkArgument(\n-        inputs.isEmpty(), \"Must have an empty input to use %s\", getClass().getSimpleName());\n-    return PCollectionList.empty(p);\n+        transform.getInputs().isEmpty(),\n+        \"Unexpected nonempty input %s for %s\",\n+        transform.getInputs(),\n+        getClass().getSimpleName());\n+    return PTransformReplacement.of(\n+        PCollectionList.<T>empty(transform.getPipeline()), new CreateEmptyFromList<T>());\n   }\n \n   @Override\n   public Map<PValue, ReplacementOutput> mapOutputs(\n-      List<TaggedPValue> outputs, PCollection<T> newOutput) {\n+      Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n     return ReplacementOutputs.singleton(outputs, newOutput);\n   }\n+\n+  private static class CreateEmptyFromList<T>\n+      extends PTransform<PCollectionList<T>, PCollection<T>> {\n+    @Override\n+    public PCollection<T> expand(PCollectionList<T> input) {\n+      return (PCollection) input.getPipeline().apply(Create.empty(VoidCoder.of()));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactory.java",
                "sha": "a6982d4801f2e86ec6cd646f8bf1c45040a96ffb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.direct;\n+package org.apache.beam.runners.core.construction;\n \n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;",
                "previous_filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ForwardingPTransform.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ForwardingPTransform.java",
                "sha": "3bee2817a8a1fb60b12505396a7aa82bac5e5443",
                "status": "renamed"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java",
                "patch": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import com.google.protobuf.InvalidProtocolBufferException;\n+import java.io.IOException;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollection.IsBounded;\n+\n+/**\n+ * Utility methods for translating {@link PCollection PCollections} to and from Runner API protos.\n+ */\n+public class PCollections {\n+  private PCollections() {}\n+\n+  public static RunnerApi.PCollection toProto(PCollection<?> pCollection, SdkComponents components)\n+      throws IOException {\n+    String coderId = components.registerCoder(pCollection.getCoder());\n+    String windowingStrategyId =\n+        components.registerWindowingStrategy(pCollection.getWindowingStrategy());\n+    // TODO: Display Data\n+\n+    return RunnerApi.PCollection.newBuilder()\n+        .setUniqueName(pCollection.getName())\n+        .setCoderId(coderId)\n+        .setIsBounded(toProto(pCollection.isBounded()))\n+        .setWindowingStrategyId(windowingStrategyId)\n+        .build();\n+  }\n+\n+  public static IsBounded isBounded(RunnerApi.PCollection pCollection) {\n+    return fromProto(pCollection.getIsBounded());\n+  }\n+\n+  public static Coder<?> getCoder(\n+      RunnerApi.PCollection pCollection, RunnerApi.Components components) throws IOException {\n+    return Coders.fromProto(components.getCodersOrThrow(pCollection.getCoderId()), components);\n+  }\n+\n+  public static WindowingStrategy<?, ?> getWindowingStrategy(\n+      RunnerApi.PCollection pCollection, RunnerApi.Components components)\n+      throws InvalidProtocolBufferException {\n+    return WindowingStrategies.fromProto(\n+        components.getWindowingStrategiesOrThrow(pCollection.getWindowingStrategyId()), components);\n+  }\n+\n+  private static RunnerApi.IsBounded toProto(IsBounded bounded) {\n+    switch (bounded) {\n+      case BOUNDED:\n+        return RunnerApi.IsBounded.BOUNDED;\n+      case UNBOUNDED:\n+        return RunnerApi.IsBounded.UNBOUNDED;\n+      default:\n+        throw new IllegalArgumentException(\n+            String.format(\"Unknown %s %s\", IsBounded.class.getSimpleName(), bounded));\n+    }\n+  }\n+\n+  private static IsBounded fromProto(RunnerApi.IsBounded isBounded) {\n+    switch (isBounded) {\n+      case BOUNDED:\n+        return IsBounded.BOUNDED;\n+      case UNBOUNDED:\n+        return IsBounded.UNBOUNDED;\n+      case UNRECOGNIZED:\n+      default:\n+        // Whether or not this enum cannot be recognized by the proto (due to the version of the\n+        // generated code we link to) or the switch hasn't been updated to handle it,\n+        // the situation is the same: we don't know what this IsBounded means\n+        throw new IllegalArgumentException(\n+            String.format(\n+                \"Cannot convert unknown %s to %s: %s\",\n+                RunnerApi.IsBounded.class.getCanonicalName(),\n+                IsBounded.class.getCanonicalName(),\n+                isBounded));\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PCollections.java",
                "sha": "907e54dd21c153bc1be44ac07ea45505153ef556",
                "status": "added"
            },
            {
                "additions": 111,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java",
                "changes": 135,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 24,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java",
                "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.beam.runners.core.construction;\n \n+import com.google.common.base.MoreObjects;\n+import java.util.HashSet;\n+import java.util.Set;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.annotations.Experimental.Kind;\n import org.apache.beam.sdk.io.Write;\n@@ -26,10 +29,14 @@\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.transforms.ViewFn;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature.ProcessElementMethod;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n+import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n \n /**\n  * A {@link PTransformMatcher} that matches {@link PTransform PTransforms} based on the class of the\n@@ -45,8 +52,6 @@ private PTransformMatchers() {}\n   /**\n    * Returns a {@link PTransformMatcher} that matches a {@link PTransform} if the class of the\n    * {@link PTransform} is equal to the {@link Class} provided ot this matcher.\n-   * @param clazz\n-   * @return\n    */\n   public static PTransformMatcher classEqualTo(Class<? extends PTransform> clazz) {\n     return new EqualClassPTransformMatcher(clazz);\n@@ -63,68 +68,90 @@ private EqualClassPTransformMatcher(Class<? extends PTransform> clazz) {\n     public boolean matches(AppliedPTransform<?, ?, ?> application) {\n       return application.getTransform().getClass().equals(clazz);\n     }\n+\n+    @Override\n+    public String toString() {\n+      return MoreObjects.toStringHelper(EqualClassPTransformMatcher.class)\n+          .add(\"class\", clazz)\n+          .toString();\n+    }\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.Bound} containing a {@link DoFn} that\n-   * is splittable, as signified by {@link ProcessElementMethod#isSplittable()}.\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.SingleOutput} containing a {@link DoFn}\n+   * that is splittable, as signified by {@link ProcessElementMethod#isSplittable()}.\n    */\n   public static PTransformMatcher splittableParDoSingle() {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.Bound) {\n-          DoFn<?, ?> fn = ((ParDo.Bound<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.SingleOutput) {\n+          DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.processElement().isSplittable();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"SplittableParDoSingleMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.Bound} containing a {@link DoFn} that\n-   * uses state or timers, as specified by {@link DoFnSignature#usesState()} and\n-   * {@link DoFnSignature#usesTimers()}.\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.SingleOutput} containing a {@link DoFn}\n+   * that uses state or timers, as specified by {@link DoFnSignature#usesState()} and {@link\n+   * DoFnSignature#usesTimers()}.\n    */\n   public static PTransformMatcher stateOrTimerParDoSingle() {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.Bound) {\n-          DoFn<?, ?> fn = ((ParDo.Bound<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.SingleOutput) {\n+          DoFn<?, ?> fn = ((ParDo.SingleOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.usesState() || signature.usesTimers();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"StateOrTimerParDoSingleMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.BoundMulti} containing a {@link DoFn}\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.MultiOutput} containing a {@link DoFn}\n    * that is splittable, as signified by {@link ProcessElementMethod#isSplittable()}.\n    */\n   public static PTransformMatcher splittableParDoMulti() {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.BoundMulti) {\n-          DoFn<?, ?> fn = ((ParDo.BoundMulti<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.MultiOutput) {\n+          DoFn<?, ?> fn = ((ParDo.MultiOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.processElement().isSplittable();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"SplittableParDoMultiMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} that matches a {@link ParDo.BoundMulti} containing a {@link DoFn}\n+   * A {@link PTransformMatcher} that matches a {@link ParDo.MultiOutput} containing a {@link DoFn}\n    * that uses state or timers, as specified by {@link DoFnSignature#usesState()} and\n    * {@link DoFnSignature#usesTimers()}.\n    */\n@@ -133,34 +160,61 @@ public static PTransformMatcher stateOrTimerParDoMulti() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         PTransform<?, ?> transform = application.getTransform();\n-        if (transform instanceof ParDo.BoundMulti) {\n-          DoFn<?, ?> fn = ((ParDo.BoundMulti<?, ?>) transform).getFn();\n+        if (transform instanceof ParDo.MultiOutput) {\n+          DoFn<?, ?> fn = ((ParDo.MultiOutput<?, ?>) transform).getFn();\n           DoFnSignature signature = DoFnSignatures.signatureForDoFn(fn);\n           return signature.usesState() || signature.usesTimers();\n         }\n         return false;\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"StateOrTimerParDoMultiMatcher\").toString();\n+      }\n     };\n   }\n \n   /**\n-   * A {@link PTransformMatcher} which matches a {@link ParDo.Bound} or {@link ParDo.BoundMulti}\n-   * where the {@link DoFn} is of the provided type.\n+   * A {@link PTransformMatcher} which matches a {@link ParDo.SingleOutput} or {@link\n+   * ParDo.MultiOutput} where the {@link DoFn} is of the provided type.\n    */\n   public static PTransformMatcher parDoWithFnType(final Class<? extends DoFn> fnType) {\n     return new PTransformMatcher() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         DoFn<?, ?> fn;\n-        if (application.getTransform() instanceof ParDo.Bound) {\n-          fn = ((ParDo.Bound) application.getTransform()).getFn();\n-        } else if (application.getTransform() instanceof ParDo.BoundMulti) {\n-          fn = ((ParDo.BoundMulti) application.getTransform()).getFn();\n+        if (application.getTransform() instanceof ParDo.SingleOutput) {\n+          fn = ((ParDo.SingleOutput) application.getTransform()).getFn();\n+        } else if (application.getTransform() instanceof ParDo.MultiOutput) {\n+          fn = ((ParDo.MultiOutput) application.getTransform()).getFn();\n         } else {\n           return false;\n         }\n         return fnType.equals(fn.getClass());\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"ParDoWithFnTypeMatcher\")\n+            .add(\"fnType\", fnType)\n+            .toString();\n+      }\n+    };\n+  }\n+\n+  public static PTransformMatcher createViewWithViewFn(final Class<? extends ViewFn> viewFnType) {\n+    return new PTransformMatcher() {\n+      @Override\n+      public boolean matches(AppliedPTransform<?, ?, ?> application) {\n+        if (!(application.getTransform() instanceof CreatePCollectionView)) {\n+          return false;\n+        }\n+        CreatePCollectionView<?, ?> createView =\n+            (CreatePCollectionView<?, ?>) application.getTransform();\n+        ViewFn<Iterable<WindowedValue<?>>, ?> viewFn = createView.getView().getViewFn();\n+        return viewFn.getClass().equals(viewFnType);\n+      }\n     };\n   }\n \n@@ -175,6 +229,38 @@ public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         return (application.getTransform() instanceof Flatten.PCollections)\n             && application.getInputs().isEmpty();\n       }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"EmptyFlattenMatcher\").toString();\n+      }\n+    };\n+  }\n+\n+  /**\n+   * A {@link PTransformMatcher} which matches a {@link Flatten.PCollections} which\n+   * consumes a single input {@link PCollection} multiple times.\n+   */\n+  public static PTransformMatcher flattenWithDuplicateInputs() {\n+    return new PTransformMatcher() {\n+      @Override\n+      public boolean matches(AppliedPTransform<?, ?, ?> application) {\n+        if (application.getTransform() instanceof Flatten.PCollections) {\n+          Set<PValue> observed = new HashSet<>();\n+          for (PValue pvalue : application.getInputs().values()) {\n+            boolean firstInstance = observed.add(pvalue);\n+            if (!firstInstance) {\n+              return true;\n+            }\n+          }\n+        }\n+        return false;\n+      }\n+\n+      @Override\n+      public String toString() {\n+        return MoreObjects.toStringHelper(\"FlattenWithDuplicateInputsMatcher\").toString();\n+      }\n     };\n   }\n \n@@ -183,7 +269,8 @@ public static PTransformMatcher writeWithRunnerDeterminedSharding() {\n       @Override\n       public boolean matches(AppliedPTransform<?, ?, ?> application) {\n         if (application.getTransform() instanceof Write) {\n-          return ((Write) application.getTransform()).getSharding() == null;\n+          Write write = (Write) application.getTransform();\n+          return write.getSharding() == null && write.getNumShards() == null;\n         }\n         return false;\n       }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformMatchers.java",
                "sha": "09946bcb1580e8554e910e81bc87c24de0d91a36",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java",
                "patch": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ */\n+public class PTransformReplacements {\n+  /**\n+   * Gets the singleton input of an {@link AppliedPTransform}, ignoring any additional inputs\n+   * returned by {@link PTransform#getAdditionalInputs()}.\n+   */\n+  public static <T> PCollection<T> getSingletonMainInput(\n+      AppliedPTransform<? extends PCollection<? extends T>, ?, ?> application) {\n+    return getSingletonMainInput(\n+        application.getInputs(), application.getTransform().getAdditionalInputs().keySet());\n+  }\n+\n+  private static <T> PCollection<T> getSingletonMainInput(\n+      Map<TupleTag<?>, PValue> inputs, Set<TupleTag<?>> ignoredTags) {\n+    PCollection<T> mainInput = null;\n+    for (Map.Entry<TupleTag<?>, PValue> input : inputs.entrySet()) {\n+      if (!ignoredTags.contains(input.getKey())) {\n+        checkArgument(\n+            mainInput == null,\n+            \"Got multiple inputs that are not additional inputs for a \"\n+                + \"singleton main input: %s and %s\",\n+            mainInput,\n+            input.getValue());\n+        checkArgument(\n+            input.getValue() instanceof PCollection,\n+            \"Unexpected input type %s\",\n+            input.getValue().getClass());\n+        mainInput = (PCollection<T>) input.getValue();\n+      }\n+    }\n+    checkArgument(\n+        mainInput != null,\n+        \"No main input found in inputs: Inputs %s, Side Input tags %s\",\n+        inputs,\n+        ignoredTags);\n+    return mainInput;\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransformReplacements.java",
                "sha": "72a3425bada86e02e93210af0677277771e4ee2b",
                "status": "added"
            },
            {
                "additions": 107,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java",
                "changes": 107,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java",
                "patch": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.common.collect.ImmutableMap;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.FunctionSpec;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ * Utilities for converting {@link PTransform PTransforms} to and from {@link RunnerApi Runner API\n+ * protocol buffers}.\n+ */\n+public class PTransforms {\n+  private static final Map<Class<? extends PTransform>, TransformPayloadTranslator>\n+      KNOWN_PAYLOAD_TRANSLATORS =\n+          ImmutableMap.<Class<? extends PTransform>, TransformPayloadTranslator>builder().build();\n+  // TODO: ParDoPayload, WindowIntoPayload, ReadPayload, CombinePayload\n+  // TODO: \"Flatten Payload\", etc?\n+  // TODO: Load via service loader.\n+  private PTransforms() {}\n+\n+  /**\n+   * Translates an {@link AppliedPTransform} into a runner API proto.\n+   *\n+   * <p>Does not register the {@code appliedPTransform} within the provided {@link SdkComponents}.\n+   */\n+  static RunnerApi.PTransform toProto(\n+      AppliedPTransform<?, ?, ?> appliedPTransform,\n+      List<AppliedPTransform<?, ?, ?>> subtransforms,\n+      SdkComponents components)\n+      throws IOException {\n+    RunnerApi.PTransform.Builder transformBuilder = RunnerApi.PTransform.newBuilder();\n+    for (Map.Entry<TupleTag<?>, PValue> taggedInput : appliedPTransform.getInputs().entrySet()) {\n+      checkArgument(\n+          taggedInput.getValue() instanceof PCollection,\n+          \"Unexpected input type %s\",\n+          taggedInput.getValue().getClass());\n+      transformBuilder.putInputs(\n+          toProto(taggedInput.getKey()),\n+          components.registerPCollection((PCollection<?>) taggedInput.getValue()));\n+    }\n+    for (Map.Entry<TupleTag<?>, PValue> taggedOutput : appliedPTransform.getOutputs().entrySet()) {\n+      checkArgument(\n+          taggedOutput.getValue() instanceof PCollection,\n+          \"Unexpected output type %s\",\n+          taggedOutput.getValue().getClass());\n+      transformBuilder.putOutputs(\n+          toProto(taggedOutput.getKey()),\n+          components.registerPCollection((PCollection<?>) taggedOutput.getValue()));\n+    }\n+    for (AppliedPTransform<?, ?, ?> subtransform : subtransforms) {\n+      transformBuilder.addSubtransforms(components.getExistingPTransformId(subtransform));\n+    }\n+\n+    transformBuilder.setUniqueName(appliedPTransform.getFullName());\n+    // TODO: Display Data\n+\n+    PTransform<?, ?> transform = appliedPTransform.getTransform();\n+    if (KNOWN_PAYLOAD_TRANSLATORS.containsKey(transform.getClass())) {\n+      FunctionSpec payload =\n+          KNOWN_PAYLOAD_TRANSLATORS\n+              .get(transform.getClass())\n+              .translate(appliedPTransform, components);\n+      transformBuilder.setSpec(payload);\n+    }\n+\n+    return transformBuilder.build();\n+  }\n+\n+  private static String toProto(TupleTag<?> tag) {\n+    return tag.getId();\n+  }\n+\n+  /**\n+   * A translator consumes a {@link PTransform} application and produces the appropriate\n+   * FunctionSpec for a distinguished or primitive transform within the Beam runner API.\n+   */\n+  public interface TransformPayloadTranslator<T extends PTransform<?, ?>> {\n+    FunctionSpec translate(AppliedPTransform<?, ?, T> transform, SdkComponents components);\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PTransforms.java",
                "sha": "7ec0863860b6471774cf461e7b97dc8e35e30d95",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java",
                "patch": "@@ -18,10 +18,9 @@\n \n package org.apache.beam.runners.core.construction;\n \n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.Create.Values;\n import org.apache.beam.sdk.transforms.PTransform;\n@@ -30,7 +29,7 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * An implementation of {@link Create} that returns a primitive {@link PCollection}.\n@@ -58,18 +57,15 @@ private PrimitiveCreate(Create.Values<T> transform) {\n   public static class Factory<T>\n       implements PTransformOverrideFactory<PBegin, PCollection<T>, Values<T>> {\n     @Override\n-    public PTransform<PBegin, PCollection<T>> getReplacementTransform(Values<T> transform) {\n-      return new PrimitiveCreate<>(transform);\n-    }\n-\n-    @Override\n-    public PBegin getInput(List<TaggedPValue> inputs, Pipeline p) {\n-      return p.begin();\n+    public PTransformReplacement<PBegin, PCollection<T>> getReplacementTransform(\n+        AppliedPTransform<PBegin, PCollection<T>, Values<T>> transform) {\n+      return PTransformReplacement.of(\n+          transform.getPipeline().begin(), new PrimitiveCreate<T>(transform.getTransform()));\n     }\n \n     @Override\n     public Map<PValue, ReplacementOutput> mapOutputs(\n-        List<TaggedPValue> outputs, PCollection<T> newOutput) {\n+        Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/PrimitiveCreate.java",
                "sha": "5a2140b63ee4f430b928daa9f4b407441e69a820",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 41,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java",
                "patch": "@@ -21,10 +21,11 @@\n \n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Iterables;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n-import java.util.List;\n import java.util.Map;\n+import java.util.Map.Entry;\n import java.util.Set;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n import org.apache.beam.sdk.values.POutput;\n@@ -39,60 +40,40 @@\n   private ReplacementOutputs() {}\n \n   public static Map<PValue, ReplacementOutput> singleton(\n-      List<TaggedPValue> original, PValue replacement) {\n-    TaggedPValue taggedReplacement = Iterables.getOnlyElement(replacement.expand());\n-    return ImmutableMap.<PValue, ReplacementOutput>builder()\n-        .put(\n-            taggedReplacement.getValue(),\n-            ReplacementOutput.of(Iterables.getOnlyElement(original), taggedReplacement))\n-        .build();\n-  }\n-\n-  public static Map<PValue, ReplacementOutput> ordered(\n-      List<TaggedPValue> original, POutput replacement) {\n-    ImmutableMap.Builder<PValue, ReplacementOutput> result = ImmutableMap.builder();\n-    List<TaggedPValue> replacements = replacement.expand();\n-    checkArgument(\n-        original.size() == replacements.size(),\n-        \"Original and Replacements must be the same size. Original: %s Replacement: %s\",\n-        original.size(),\n-        replacements.size());\n-    int i = 0;\n-    for (TaggedPValue replacementPvalue : replacements) {\n-      result.put(\n-          replacementPvalue.getValue(), ReplacementOutput.of(original.get(i), replacementPvalue));\n-      i++;\n-    }\n-    return result.build();\n+      Map<TupleTag<?>, PValue> original, PValue replacement) {\n+    Entry<TupleTag<?>, PValue> originalElement = Iterables.getOnlyElement(original.entrySet());\n+    TupleTag<?> replacementTag = Iterables.getOnlyElement(replacement.expand().entrySet()).getKey();\n+    return Collections.singletonMap(\n+        replacement,\n+        ReplacementOutput.of(\n+            TaggedPValue.of(originalElement.getKey(), originalElement.getValue()),\n+            TaggedPValue.of(replacementTag, replacement)));\n   }\n \n   public static Map<PValue, ReplacementOutput> tagged(\n-      List<TaggedPValue> original, POutput replacement) {\n+      Map<TupleTag<?>, PValue> original, POutput replacement) {\n     Map<TupleTag<?>, TaggedPValue> originalTags = new HashMap<>();\n-    for (TaggedPValue value : original) {\n-      TaggedPValue former = originalTags.put(value.getTag(), value);\n-      checkArgument(\n-          former == null || former.equals(value),\n-          \"Found two tags in an expanded output which map to different values: output: %s \"\n-              + \"Values: %s and %s\",\n-          original,\n-          former,\n-          value);\n+    for (Map.Entry<TupleTag<?>, PValue> originalValue : original.entrySet()) {\n+      originalTags.put(\n+          originalValue.getKey(),\n+          TaggedPValue.of(originalValue.getKey(), originalValue.getValue()));\n     }\n     ImmutableMap.Builder<PValue, ReplacementOutput> resultBuilder = ImmutableMap.builder();\n     Set<TupleTag<?>> missingTags = new HashSet<>(originalTags.keySet());\n-    for (TaggedPValue replacementValue : replacement.expand()) {\n-      TaggedPValue mapped = originalTags.get(replacementValue.getTag());\n+    for (Map.Entry<TupleTag<?>, PValue> replacementValue : replacement.expand().entrySet()) {\n+      TaggedPValue mapped = originalTags.get(replacementValue.getKey());\n       checkArgument(\n           mapped != null,\n           \"Missing original output for Tag %s and Value %s Between original %s and replacement %s\",\n-          replacementValue.getTag(),\n+          replacementValue.getKey(),\n           replacementValue.getValue(),\n           original,\n           replacement.expand());\n       resultBuilder.put(\n-          replacementValue.getValue(), ReplacementOutput.of(mapped, replacementValue));\n-      missingTags.remove(replacementValue.getTag());\n+          replacementValue.getValue(),\n+          ReplacementOutput.of(\n+              mapped, TaggedPValue.of(replacementValue.getKey(), replacementValue.getValue())));\n+      missingTags.remove(replacementValue.getKey());\n     }\n     ImmutableMap<PValue, ReplacementOutput> result = resultBuilder.build();\n     checkArgument(",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/ReplacementOutputs.java",
                "sha": "3d485aebc743465d634a3c5ac060f9676fc420f0",
                "status": "modified"
            },
            {
                "additions": 195,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java",
                "changes": 195,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java",
                "patch": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.common.base.Equivalence;\n+import com.google.common.collect.BiMap;\n+import com.google.common.collect.HashBiMap;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.util.NameUtils;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.PCollection;\n+\n+/** SDK objects that will be represented at some later point within a {@link Components} object. */\n+class SdkComponents {\n+  private final RunnerApi.Components.Builder componentsBuilder;\n+\n+  private final BiMap<AppliedPTransform<?, ?, ?>, String> transformIds;\n+  private final BiMap<PCollection<?>, String> pCollectionIds;\n+  private final BiMap<WindowingStrategy<?, ?>, String> windowingStrategyIds;\n+\n+  /** A map of Coder to IDs. Coders are stored here with identity equivalence. */\n+  private final BiMap<Equivalence.Wrapper<? extends Coder<?>>, String> coderIds;\n+  // TODO: Specify environments\n+\n+  /** Create a new {@link SdkComponents} with no components. */\n+  static SdkComponents create() {\n+    return new SdkComponents();\n+  }\n+\n+  private SdkComponents() {\n+    this.componentsBuilder = RunnerApi.Components.newBuilder();\n+    this.transformIds = HashBiMap.create();\n+    this.pCollectionIds = HashBiMap.create();\n+    this.windowingStrategyIds = HashBiMap.create();\n+    this.coderIds = HashBiMap.create();\n+  }\n+\n+  /**\n+   * Registers the provided {@link AppliedPTransform} into this {@link SdkComponents}, returning a\n+   * unique ID for the {@link AppliedPTransform}. Multiple registrations of the same\n+   * {@link AppliedPTransform} will return the same unique ID.\n+   *\n+   * <p>All of the children must already be registered within this {@link SdkComponents}.\n+   */\n+  String registerPTransform(\n+      AppliedPTransform<?, ?, ?> appliedPTransform, List<AppliedPTransform<?, ?, ?>> children)\n+      throws IOException {\n+    String name = getApplicationName(appliedPTransform);\n+    // If this transform is present in the components, nothing to do. return the existing name.\n+    // Otherwise the transform must be translated and added to the components.\n+    if (componentsBuilder.getTransformsOrDefault(name, null) != null) {\n+      return name;\n+    }\n+    checkNotNull(children, \"child nodes may not be null\");\n+    componentsBuilder.putTransforms(name, PTransforms.toProto(appliedPTransform, children, this));\n+    return name;\n+  }\n+\n+  /**\n+   * Gets the ID for the provided {@link AppliedPTransform}. The provided {@link AppliedPTransform}\n+   * will not be added to the components produced by this {@link SdkComponents} until it is\n+   * translated via {@link #registerPTransform(AppliedPTransform, List)}.\n+   */\n+  private String getApplicationName(AppliedPTransform<?, ?, ?> appliedPTransform) {\n+    String existing = transformIds.get(appliedPTransform);\n+    if (existing != null) {\n+      return existing;\n+    }\n+\n+    String name = appliedPTransform.getFullName();\n+    if (name.isEmpty()) {\n+      name = \"unnamed-ptransform\";\n+    }\n+    name = uniqify(name, transformIds.values());\n+    transformIds.put(appliedPTransform, name);\n+    return name;\n+  }\n+\n+  String getExistingPTransformId(AppliedPTransform<?, ?, ?> appliedPTransform) {\n+    checkArgument(\n+        transformIds.containsKey(appliedPTransform),\n+        \"%s %s has not been previously registered\",\n+        AppliedPTransform.class.getSimpleName(),\n+        appliedPTransform);\n+    return transformIds.get(appliedPTransform);\n+  }\n+\n+  /**\n+   * Registers the provided {@link PCollection} into this {@link SdkComponents}, returning a unique\n+   * ID for the {@link PCollection}. Multiple registrations of the same {@link PCollection} will\n+   * return the same unique ID.\n+   */\n+  String registerPCollection(PCollection<?> pCollection) throws IOException {\n+    String existing = pCollectionIds.get(pCollection);\n+    if (existing != null) {\n+      return existing;\n+    }\n+    String uniqueName = uniqify(pCollection.getName(), pCollectionIds.values());\n+    pCollectionIds.put(pCollection, uniqueName);\n+    componentsBuilder.putPcollections(uniqueName, PCollections.toProto(pCollection, this));\n+    return uniqueName;\n+  }\n+\n+  /**\n+   * Registers the provided {@link WindowingStrategy} into this {@link SdkComponents}, returning a\n+   * unique ID for the {@link WindowingStrategy}. Multiple registrations of the same {@link\n+   * WindowingStrategy} will return the same unique ID.\n+   */\n+  String registerWindowingStrategy(WindowingStrategy<?, ?> windowingStrategy) throws IOException {\n+    String existing = windowingStrategyIds.get(windowingStrategy);\n+    if (existing != null) {\n+      return existing;\n+    }\n+    String baseName =\n+        String.format(\n+            \"%s(%s)\",\n+            NameUtils.approximateSimpleName(windowingStrategy),\n+            NameUtils.approximateSimpleName(windowingStrategy.getWindowFn()));\n+    String name = uniqify(baseName, windowingStrategyIds.values());\n+    windowingStrategyIds.put(windowingStrategy, name);\n+    RunnerApi.WindowingStrategy windowingStrategyProto =\n+        WindowingStrategies.toProto(windowingStrategy, this);\n+    componentsBuilder.putWindowingStrategies(name, windowingStrategyProto);\n+    return name;\n+  }\n+\n+  /**\n+   * Registers the provided {@link Coder} into this {@link SdkComponents}, returning a unique ID for\n+   * the {@link Coder}. Multiple registrations of the same {@link Coder} will return the same\n+   * unique ID.\n+   *\n+   * <p>Coders are stored by identity to ensure that coders with implementations of {@link\n+   * #equals(Object)} and {@link #hashCode()} but incompatible binary formats are not considered the\n+   * same coder.\n+   */\n+  String registerCoder(Coder<?> coder) throws IOException {\n+    String existing = coderIds.get(Equivalence.identity().wrap(coder));\n+    if (existing != null) {\n+      return existing;\n+    }\n+    String baseName = NameUtils.approximateSimpleName(coder);\n+    String name = uniqify(baseName, coderIds.values());\n+    coderIds.put(Equivalence.identity().wrap(coder), name);\n+    RunnerApi.Coder coderProto = Coders.toProto(coder, this);\n+    componentsBuilder.putCoders(name, coderProto);\n+    return name;\n+  }\n+\n+  private String uniqify(String baseName, Set<String> existing) {\n+    String name = baseName;\n+    int increment = 1;\n+    while (existing.contains(name)) {\n+      name = baseName + Integer.toString(increment);\n+      increment++;\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Convert this {@link SdkComponents} into a {@link RunnerApi.Components}, including all of the\n+   * contained {@link Coder coders}, {@link WindowingStrategy windowing strategies}, {@link\n+   * PCollection PCollections}, and {@link PTransform PTransforms}.\n+   */\n+  @Experimental\n+  RunnerApi.Components toComponents() {\n+    return componentsBuilder.build();\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SdkComponents.java",
                "sha": "35af3006d2fe519a2046687799128fc699585767",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java",
                "patch": "@@ -18,33 +18,25 @@\n \n package org.apache.beam.runners.core.construction;\n \n-import com.google.common.collect.Iterables;\n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} which consumes from a {@link PValue} and produces a\n- * {@link PValue}. {@link #getInput(List, Pipeline)} and {@link #mapOutputs(List, PValue)} are\n+ * {@link PValue}. {@link #mapOutputs(Map, PValue)} is\n  * implemented.\n  */\n public abstract class SingleInputOutputOverrideFactory<\n         InputT extends PValue,\n         OutputT extends PValue,\n         TransformT extends PTransform<InputT, OutputT>>\n     implements PTransformOverrideFactory<InputT, OutputT, TransformT> {\n-  @Override\n-  public final InputT getInput(List<TaggedPValue> inputs, Pipeline p) {\n-    return (InputT) Iterables.getOnlyElement(inputs).getValue();\n-  }\n-\n   @Override\n   public final Map<PValue, ReplacementOutput> mapOutputs(\n-      List<TaggedPValue> outputs, OutputT newOutput) {\n+      Map<TupleTag<?>, PValue> outputs, OutputT newOutput) {\n     return ReplacementOutputs.singleton(outputs, newOutput);\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactory.java",
                "sha": "7a59c1c1c80b124271c3901a140138ba443d9565",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.transforms.windowing;\n+package org.apache.beam.runners.core.construction;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.collect.Lists;\n@@ -25,8 +25,22 @@\n import java.util.List;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.transforms.windowing.AfterAll;\n+import org.apache.beam.sdk.transforms.windowing.AfterEach;\n+import org.apache.beam.sdk.transforms.windowing.AfterFirst;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.AfterProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterSynchronizedProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n import org.apache.beam.sdk.transforms.windowing.AfterWatermark.AfterWatermarkEarlyAndLate;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark.FromEndOfWindow;\n+import org.apache.beam.sdk.transforms.windowing.DefaultTrigger;\n+import org.apache.beam.sdk.transforms.windowing.Never;\n import org.apache.beam.sdk.transforms.windowing.Never.NeverTrigger;\n+import org.apache.beam.sdk.transforms.windowing.OrFinallyTrigger;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.TimestampTransform;\n+import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.apache.beam.sdk.transforms.windowing.Trigger.OnceTrigger;\n import org.apache.beam.sdk.util.ReshuffleTrigger;\n import org.apache.beam.sdk.util.TimeDomain;\n@@ -84,7 +98,7 @@ private Method getEvaluationMethod(Class<?> clazz) {\n           .build();\n     }\n \n-    private RunnerApi.Trigger convertSpecific(AfterWatermark.FromEndOfWindow v) {\n+    private RunnerApi.Trigger convertSpecific(FromEndOfWindow v) {\n       return RunnerApi.Trigger.newBuilder()\n           .setAfterEndOfWindow(RunnerApi.Trigger.AfterEndOfWindow.newBuilder())\n           .build();\n@@ -149,7 +163,7 @@ private Method getEvaluationMethod(Class<?> clazz) {\n           .build();\n     }\n \n-    private RunnerApi.Trigger convertSpecific(AfterWatermark.AfterWatermarkEarlyAndLate v) {\n+    private RunnerApi.Trigger convertSpecific(AfterWatermarkEarlyAndLate v) {\n       RunnerApi.Trigger.AfterEndOfWindow.Builder builder =\n           RunnerApi.Trigger.AfterEndOfWindow.newBuilder();\n \n@@ -287,6 +301,8 @@ public static Trigger fromProto(RunnerApi.Trigger triggerProto) {\n         return trigger;\n       case AFTER_SYNCHRONIZED_PROCESSING_TIME:\n         return AfterSynchronizedProcessingTime.ofFirstElement();\n+      case ALWAYS:\n+        return new ReshuffleTrigger();\n       case ELEMENT_COUNT:\n         return AfterPane.elementCountAtLeast(triggerProto.getElementCount().getElementCount());\n       case NEVER:",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/windowing/Triggers.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/Triggers.java",
                "sha": "81f738da0dd5bc06b3b89f3d8d07dd2e8a2ad4a8",
                "status": "renamed"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 6,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.core;\n+package org.apache.beam.runners.core.construction;\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkNotNull;\n@@ -61,7 +61,8 @@\n /**\n  * {@link PTransform} that converts a {@link BoundedSource} as an {@link UnboundedSource}.\n  *\n- * <p>{@link BoundedSource} is read directly without calling {@link BoundedSource#splitIntoBundles},\n+ * <p>{@link BoundedSource} is read directly without calling\n+ * {@link BoundedSource#split},\n  * and element timestamps are propagated. While any elements remain, the watermark is the beginning\n  * of time {@link BoundedWindow#TIMESTAMP_MIN_VALUE}, and after all elements have been produced\n  * the watermark goes to the end of time {@link BoundedWindow#TIMESTAMP_MAX_VALUE}.\n@@ -130,7 +131,7 @@ public void validate() {\n     }\n \n     @Override\n-    public List<BoundedToUnboundedSourceAdapter<T>> generateInitialSplits(\n+    public List<BoundedToUnboundedSourceAdapter<T>> split(\n         int desiredNumSplits, PipelineOptions options) throws Exception {\n       try {\n         long desiredBundleSize = boundedSource.getEstimatedSizeBytes(options) / desiredNumSplits;\n@@ -140,7 +141,7 @@ public void validate() {\n           return ImmutableList.of(this);\n         }\n         List<? extends BoundedSource<T>> splits =\n-            boundedSource.splitIntoBundles(desiredBundleSize, options);\n+            boundedSource.split(desiredBundleSize, options);\n         if (splits == null) {\n           LOG.warn(\"BoundedSource cannot split {}, skips the initial splits.\", boundedSource);\n           return ImmutableList.of(this);\n@@ -458,22 +459,28 @@ Instant getCurrentTimestamp() {\n       private PipelineOptions options;\n       private @Nullable BoundedReader<T> reader;\n       private boolean closed;\n+      private boolean readerDone;\n \n       public ResidualSource(BoundedSource<T> residualSource, PipelineOptions options) {\n         this.residualSource = checkNotNull(residualSource, \"residualSource\");\n         this.options = checkNotNull(options, \"options\");\n         this.reader = null;\n         this.closed = false;\n+        this.readerDone = false;\n       }\n \n       private boolean advance() throws IOException {\n         checkArgument(!closed, \"advance() call on closed %s\", getClass().getName());\n+        if (readerDone) {\n+          return false;\n+        }\n         if (reader == null) {\n           reader = residualSource.createReader(options);\n-          return reader.start();\n+          readerDone = !reader.start();\n         } else {\n-          return reader.advance();\n+          readerDone = !reader.advance();\n         }\n+        return !readerDone;\n       }\n \n       T getCurrent() throws NoSuchElementException {",
                "previous_filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSource.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSource.java",
                "sha": "f67af8a6ea070d83831eebfc4a517317d86140b3",
                "status": "renamed"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java",
                "patch": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor;\n+import org.apache.beam.sdk.io.Read;\n+import org.apache.beam.sdk.runners.TransformHierarchy.Node;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PValue;\n+\n+/**\n+ * Utilities for ensuring that all {@link Read} {@link PTransform PTransforms} are consumed by some\n+ * {@link PTransform}.\n+ */\n+public class UnconsumedReads {\n+  public static void ensureAllReadsConsumed(Pipeline pipeline) {\n+    final Set<PCollection<?>> unconsumed = new HashSet<>();\n+    pipeline.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            unconsumed.removeAll(node.getInputs().values());\n+          }\n+\n+          @Override\n+          public void visitValue(PValue value, Node producer) {\n+            if (producer.getTransform() instanceof Read.Bounded\n+                || producer.getTransform() instanceof Read.Unbounded) {\n+              unconsumed.add((PCollection<?>) value);\n+            }\n+          }\n+        });\n+    int i = 0;\n+    for (PCollection<?> unconsumedPCollection : unconsumed) {\n+      consume(unconsumedPCollection, i);\n+      i++;\n+    }\n+  }\n+\n+  private static <T> void consume(PCollection<T> unconsumedPCollection, int uniq) {\n+    // Multiple applications should never break due to stable unique names.\n+    String uniqueName = \"DropInputs\" + (uniq == 0 ? \"\" : uniq);\n+    unconsumedPCollection.apply(uniqueName, ParDo.of(new NoOpDoFn<T>()));\n+  }\n+\n+  private static class NoOpDoFn<T> extends DoFn<T, T> {\n+    @ProcessElement\n+    public void doNothing(ProcessContext context) {}\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnconsumedReads.java",
                "sha": "c191eeb8617d402c54ee6f663eaa804907f82683",
                "status": "added"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 12,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java",
                "patch": "@@ -18,20 +18,19 @@\n \n package org.apache.beam.runners.core.construction;\n \n-import java.util.List;\n import java.util.Map;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} that throws an exception when a call to\n- * {@link #getReplacementTransform(PTransform)} is made. This is for {@link PTransform PTransforms}\n- * which are not supported by a runner.\n+ * {@link #getReplacementTransform(AppliedPTransform)} is made. This is for\n+ * {@link PTransform PTransforms} which are not supported by a runner.\n  */\n public final class UnsupportedOverrideFactory<\n         InputT extends PInput,\n@@ -55,17 +54,14 @@ private UnsupportedOverrideFactory(String message) {\n   }\n \n   @Override\n-  public PTransform<InputT, OutputT> getReplacementTransform(TransformT transform) {\n+  public PTransformReplacement<InputT, OutputT> getReplacementTransform(\n+      AppliedPTransform<InputT, OutputT, TransformT> transform) {\n     throw new UnsupportedOperationException(message);\n   }\n \n   @Override\n-  public InputT getInput(List<TaggedPValue> inputs, Pipeline p) {\n-    throw new UnsupportedOperationException(message);\n-  }\n-\n-  @Override\n-  public Map<PValue, ReplacementOutput> mapOutputs(List<TaggedPValue> outputs, OutputT newOutput) {\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, OutputT newOutput) {\n     throw new UnsupportedOperationException(message);\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactory.java",
                "sha": "efafa33b3513e1fe1cddbfd456856653e621d26b",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 67,
                "filename": "runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java",
                "patch": "@@ -15,30 +15,27 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core.construction;\n \n import static com.google.common.base.Preconditions.checkArgument;\n \n-import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.protobuf.Any;\n import com.google.protobuf.ByteString;\n import com.google.protobuf.BytesValue;\n import com.google.protobuf.InvalidProtocolBufferException;\n import java.io.IOException;\n import java.io.Serializable;\n-import java.util.UUID;\n-import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n import org.apache.beam.sdk.common.runner.v1.RunnerApi.FunctionSpec;\n-import org.apache.beam.sdk.common.runner.v1.RunnerApi.UrnWithParameter;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.SdkFunctionSpec;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.SerializableUtils;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n import org.apache.beam.sdk.util.WindowingStrategy.CombineWindowFnOutputTimes;\n import org.joda.time.Duration;\n@@ -82,7 +79,7 @@ public static AccumulationMode fromProto(RunnerApi.AccumulationMode proto) {\n     }\n   }\n \n-  public static RunnerApi.ClosingBehavior toProto(Window.ClosingBehavior closingBehavior) {\n+  public static RunnerApi.ClosingBehavior toProto(ClosingBehavior closingBehavior) {\n     switch (closingBehavior) {\n       case FIRE_ALWAYS:\n         return RunnerApi.ClosingBehavior.EMIT_ALWAYS;\n@@ -126,62 +123,30 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n     }\n   }\n \n-  // This URN says that the coder is just a UDF blob the indicated SDK understands\n-  // TODO: standardize such things\n-  private static final String CUSTOM_CODER_URN = \"urn:beam:coders:javasdk:0.1\";\n-\n   // This URN says that the WindowFn is just a UDF blob the indicated SDK understands\n   // TODO: standardize such things\n-  private static final String CUSTOM_WINDOWFN_URN = \"urn:beam:windowfn:javasdk:0.1\";\n-\n-  private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n+  public static final String CUSTOM_WINDOWFN_URN = \"urn:beam:windowfn:javasdk:0.1\";\n \n   /**\n-   * Converts a {@link WindowFn} into a {@link RunnerApi.MessageWithComponents} where\n-   * {@link RunnerApi.MessageWithComponents#getFunctionSpec()} is a {@link RunnerApi.FunctionSpec}\n-   * for the input {@link WindowFn}.\n+   * Converts a {@link WindowFn} into a {@link RunnerApi.MessageWithComponents} where {@link\n+   * RunnerApi.MessageWithComponents#getFunctionSpec()} is a {@link RunnerApi.FunctionSpec} for the\n+   * input {@link WindowFn}.\n    */\n-  public static RunnerApi.MessageWithComponents toProto(WindowFn<?, ?> windowFn)\n+  public static SdkFunctionSpec toProto(\n+      WindowFn<?, ?> windowFn, @SuppressWarnings(\"unused\") SdkComponents components)\n       throws IOException {\n-    Coder<?> windowCoder = windowFn.windowCoder();\n-\n-    // TODO: re-use components\n-    String windowCoderId = UUID.randomUUID().toString();\n-\n-    RunnerApi.FunctionSpec windowFnSpec =\n-        RunnerApi.FunctionSpec.newBuilder()\n-            .setSpec(\n-                UrnWithParameter.newBuilder()\n-                    .setUrn(CUSTOM_WINDOWFN_URN)\n-                    .setParameter(\n-                        Any.pack(\n-                            BytesValue.newBuilder()\n-                                .setValue(\n-                                    ByteString.copyFrom(\n-                                        SerializableUtils.serializeToByteArray(windowFn)))\n-                                .build())))\n-            .build();\n-\n-    RunnerApi.Coder windowCoderProto =\n-        RunnerApi.Coder.newBuilder()\n-            .setSpec(\n-                FunctionSpec.newBuilder()\n-                    .setSpec(\n-                        UrnWithParameter.newBuilder()\n-                            .setUrn(CUSTOM_CODER_URN)\n-                            .setParameter(\n-                                Any.pack(\n-                                    BytesValue.newBuilder()\n-                                        .setValue(\n-                                            ByteString.copyFrom(\n-                                                OBJECT_MAPPER.writeValueAsBytes(\n-                                                    windowCoder.asCloudObject())))\n-                                        .build()))))\n-            .build();\n-\n-    return RunnerApi.MessageWithComponents.newBuilder()\n-        .setFunctionSpec(windowFnSpec)\n-        .setComponents(Components.newBuilder().putCoders(windowCoderId, windowCoderProto))\n+    return SdkFunctionSpec.newBuilder()\n+        // TODO: Set environment ID\n+        .setSpec(\n+            FunctionSpec.newBuilder()\n+                .setUrn(CUSTOM_WINDOWFN_URN)\n+                .setParameter(\n+                    Any.pack(\n+                        BytesValue.newBuilder()\n+                            .setValue(\n+                                ByteString.copyFrom(\n+                                    SerializableUtils.serializeToByteArray(windowFn)))\n+                            .build())))\n         .build();\n   }\n \n@@ -193,9 +158,22 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n    */\n   public static RunnerApi.MessageWithComponents toProto(WindowingStrategy<?, ?> windowingStrategy)\n       throws IOException {\n+    SdkComponents components = SdkComponents.create();\n+    RunnerApi.WindowingStrategy windowingStrategyProto = toProto(windowingStrategy, components);\n \n-    RunnerApi.MessageWithComponents windowFnWithComponents =\n-        toProto(windowingStrategy.getWindowFn());\n+    return RunnerApi.MessageWithComponents.newBuilder()\n+        .setWindowingStrategy(windowingStrategyProto)\n+        .setComponents(components.toComponents())\n+        .build();\n+  }\n+\n+  /**\n+   * Converts a {@link WindowingStrategy} into a {@link RunnerApi.WindowingStrategy}, registering\n+   * any components in the provided {@link SdkComponents}.\n+   */\n+  public static RunnerApi.WindowingStrategy toProto(\n+      WindowingStrategy<?, ?> windowingStrategy, SdkComponents components) throws IOException {\n+    SdkFunctionSpec windowFnSpec = toProto(windowingStrategy.getWindowFn(), components);\n \n     RunnerApi.WindowingStrategy.Builder windowingStrategyProto =\n         RunnerApi.WindowingStrategy.newBuilder()\n@@ -204,15 +182,15 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n             .setClosingBehavior(toProto(windowingStrategy.getClosingBehavior()))\n             .setAllowedLateness(windowingStrategy.getAllowedLateness().getMillis())\n             .setTrigger(Triggers.toProto(windowingStrategy.getTrigger()))\n-            .setWindowFn(windowFnWithComponents.getFunctionSpec());\n+            .setWindowFn(windowFnSpec)\n+            .setWindowCoderId(\n+                components.registerCoder(windowingStrategy.getWindowFn().windowCoder()));\n \n-    return RunnerApi.MessageWithComponents.newBuilder()\n-        .setWindowingStrategy(windowingStrategyProto)\n-        .setComponents(windowFnWithComponents.getComponents()).build();\n+    return windowingStrategyProto.build();\n   }\n \n   /**\n-   * Converts from a {@link RunnerApi.WindowingStrategy} accompanied by {@link RunnerApi.Components}\n+   * Converts from a {@link RunnerApi.WindowingStrategy} accompanied by {@link Components}\n    * to the SDK's {@link WindowingStrategy}.\n    */\n   public static WindowingStrategy<?, ?> fromProto(RunnerApi.MessageWithComponents proto)\n@@ -233,15 +211,16 @@ public static ClosingBehavior fromProto(RunnerApi.ClosingBehavior proto) {\n    * the provided components to dereferences identifiers found in the proto.\n    */\n   public static WindowingStrategy<?, ?> fromProto(\n-      RunnerApi.WindowingStrategy proto, RunnerApi.Components components)\n+      RunnerApi.WindowingStrategy proto, Components components)\n       throws InvalidProtocolBufferException {\n \n-    FunctionSpec windowFnSpec = proto.getWindowFn();\n+    SdkFunctionSpec windowFnSpec = proto.getWindowFn();\n \n     checkArgument(\n         windowFnSpec.getSpec().getUrn().equals(CUSTOM_WINDOWFN_URN),\n         \"Only Java-serialized %s instances are supported, with URN %s. But found URN %s\",\n         WindowFn.class.getSimpleName(),\n+        CUSTOM_WINDOWFN_URN,\n         windowFnSpec.getSpec().getUrn());\n \n     Object deserializedWindowFn =",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/WindowingStrategies.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/main/java/org/apache/beam/runners/core/construction/WindowingStrategies.java",
                "sha": "3d7deef1d7b84768091b51cf5dd4ba35174d7726",
                "status": "renamed"
            },
            {
                "additions": 163,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java",
                "changes": 163,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java",
                "patch": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderException;\n+import org.apache.beam.sdk.coders.CustomCoder;\n+import org.apache.beam.sdk.coders.IterableCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.SerializableCoder;\n+import org.apache.beam.sdk.coders.StandardCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow.IntervalWindowCoder;\n+import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n+import org.hamcrest.Matchers;\n+import org.junit.Test;\n+import org.junit.experimental.runners.Enclosed;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+/** Tests for {@link Coders}. */\n+@RunWith(Enclosed.class)\n+public class CodersTest {\n+  private static final Set<StandardCoder<?>> KNOWN_CODERS =\n+      ImmutableSet.<StandardCoder<?>>builder()\n+          .add(ByteArrayCoder.of())\n+          .add(KvCoder.of(VarLongCoder.of(), VarLongCoder.of()))\n+          .add(VarLongCoder.of())\n+          .add(IntervalWindowCoder.of())\n+          .add(IterableCoder.of(ByteArrayCoder.of()))\n+          .add(GlobalWindow.Coder.INSTANCE)\n+          .add(\n+              FullWindowedValueCoder.of(\n+                  IterableCoder.of(VarLongCoder.of()), IntervalWindowCoder.of()))\n+          .build();\n+\n+  /**\n+   * Tests that all known coders are present in the parameters that will be used by\n+   * {@link ToFromProtoTest}.\n+   */\n+  @RunWith(JUnit4.class)\n+  public static class ValidateKnownCodersPresentTest {\n+    @Test\n+    public void validateKnownCoders() {\n+      // Validates that every known coder in the Coders class is represented in a \"Known Coder\"\n+      // tests, which demonstrates that they are serialized via components and specified URNs rather\n+      // than java serialized\n+      Set<Class<? extends StandardCoder>> knownCoderClasses = Coders.KNOWN_CODER_URNS.keySet();\n+      Set<Class<? extends StandardCoder>> knownCoderTests = new HashSet<>();\n+      for (StandardCoder<?> coder : KNOWN_CODERS) {\n+        knownCoderTests.add(coder.getClass());\n+      }\n+      Set<Class<? extends StandardCoder>> missingKnownCoders = new HashSet<>(knownCoderClasses);\n+      missingKnownCoders.removeAll(knownCoderTests);\n+      checkState(\n+          missingKnownCoders.isEmpty(),\n+          \"Missing validation of known coder %s in %s\",\n+          missingKnownCoders,\n+          CodersTest.class.getSimpleName());\n+    }\n+  }\n+\n+\n+  /**\n+   * Tests round-trip coder encodings for both known and unknown {@link Coder coders}.\n+   */\n+  @RunWith(Parameterized.class)\n+  public static class ToFromProtoTest {\n+    @Parameters(name = \"{index}: {0}\")\n+    public static Iterable<Coder<?>> data() {\n+      return ImmutableList.<Coder<?>>builder()\n+          .addAll(KNOWN_CODERS)\n+          .add(\n+              StringUtf8Coder.of(),\n+              SerializableCoder.of(Record.class),\n+              new RecordCoder(),\n+              KvCoder.of(new RecordCoder(), AvroCoder.of(Record.class)))\n+          .build();\n+    }\n+\n+    @Parameter(0)\n+    public Coder<?> coder;\n+\n+    @Test\n+    public void toAndFromProto() throws Exception {\n+      SdkComponents componentsBuilder = SdkComponents.create();\n+      RunnerApi.Coder coderProto = Coders.toProto(coder, componentsBuilder);\n+\n+      Components encodedComponents = componentsBuilder.toComponents();\n+      Coder<?> decodedCoder = Coders.fromProto(coderProto, encodedComponents);\n+      assertThat(decodedCoder, Matchers.<Coder<?>>equalTo(coder));\n+\n+      if (KNOWN_CODERS.contains(coder)) {\n+        for (RunnerApi.Coder encodedCoder : encodedComponents.getCodersMap().values()) {\n+          assertThat(\n+              encodedCoder.getSpec().getSpec().getUrn(), not(equalTo(Coders.CUSTOM_CODER_URN)));\n+        }\n+      }\n+    }\n+\n+    static class Record implements Serializable {}\n+\n+    private static class RecordCoder extends CustomCoder<Record> {\n+      @Override\n+      public void encode(Record value, OutputStream outStream, Context context)\n+          throws CoderException, IOException {}\n+\n+      @Override\n+      public Record decode(InputStream inStream, Context context)\n+          throws CoderException, IOException {\n+        return new Record();\n+      }\n+\n+      @Override\n+      public boolean equals(Object other) {\n+        return other != null && getClass().equals(other.getClass());\n+      }\n+\n+      @Override\n+      public int hashCode() {\n+        return getClass().hashCode();\n+      }\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/CodersTest.java",
                "sha": "b2b99558922aa247a460a17a96eded32b45a2dbe",
                "status": "added"
            },
            {
                "additions": 104,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java",
                "changes": 104,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java",
                "patch": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertThat;\n+\n+import org.apache.beam.runners.core.construction.DeduplicatedFlattenFactory.FlattenWithoutDuplicateInputs;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor.Defaults;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n+import org.apache.beam.sdk.runners.TransformHierarchy;\n+import org.apache.beam.sdk.testing.NeedsRunner;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TaggedPValue;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link DeduplicatedFlattenFactory}.\n+ */\n+@RunWith(JUnit4.class)\n+public class DeduplicatedFlattenFactoryTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create();\n+\n+  private PCollection<String> first = pipeline.apply(\"FirstCreate\", Create.of(\"one\"));\n+  private PCollection<String> second = pipeline.apply(\"SecondCreate\", Create.of(\"two\"));\n+  private DeduplicatedFlattenFactory<String> factory = DeduplicatedFlattenFactory.create();\n+\n+  @Test\n+  public void duplicatesInsertsMultipliers() {\n+    PTransform<PCollectionList<String>, PCollection<String>> replacement =\n+        new DeduplicatedFlattenFactory.FlattenWithoutDuplicateInputs<>();\n+    final PCollectionList<String> inputList =\n+        PCollectionList.of(first).and(second).and(first).and(first);\n+    inputList.apply(replacement);\n+    pipeline.traverseTopologically(\n+        new Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n+            if (node.getTransform() instanceof Flatten.PCollections) {\n+              assertThat(node.getInputs(), not(equalTo(inputList.expand())));\n+            }\n+          }\n+        });\n+  }\n+\n+  @Test\n+  @Category(NeedsRunner.class)\n+  public void testOverride() {\n+    final PCollectionList<String> inputList =\n+        PCollectionList.of(first).and(second).and(first).and(first);\n+    PTransform<PCollectionList<String>, PCollection<String>> replacement =\n+        new FlattenWithoutDuplicateInputs<>();\n+    PCollection<String> flattened = inputList.apply(replacement);\n+\n+    PAssert.that(flattened).containsInAnyOrder(\"one\", \"two\", \"one\", \"one\");\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void outputMapping() {\n+    final PCollectionList<String> inputList =\n+        PCollectionList.of(first).and(second).and(first).and(first);\n+    PCollection<String> original =\n+        inputList.apply(Flatten.<String>pCollections());\n+    PCollection<String> replacement = inputList.apply(new FlattenWithoutDuplicateInputs<String>());\n+\n+    assertThat(\n+        factory.mapOutputs(original.expand(), replacement),\n+        Matchers.<PValue, ReplacementOutput>hasEntry(\n+            replacement,\n+            ReplacementOutput.of(\n+                TaggedPValue.ofExpandedValue(original),\n+                TaggedPValue.ofExpandedValue(replacement))));\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/DeduplicatedFlattenFactoryTest.java",
                "sha": "4e08c213ff1f61378d27afe4dcd671df0aae9700",
                "status": "added"
            },
            {
                "additions": 122,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java",
                "patch": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.junit.Assert.assertThat;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.PTransformReplacement;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n+import org.apache.beam.sdk.testing.NeedsRunner;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Flatten.PCollections;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link EmptyFlattenAsCreateFactory}.\n+ */\n+@RunWith(JUnit4.class)\n+public class EmptyFlattenAsCreateFactoryTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  private EmptyFlattenAsCreateFactory<Long> factory = EmptyFlattenAsCreateFactory.instance();\n+\n+  @Test\n+  public void getInputEmptySucceeds() {\n+    PTransformReplacement<PCollectionList<Long>, PCollection<Long>> replacement =\n+        factory.getReplacementTransform(\n+            AppliedPTransform.<PCollectionList<Long>, PCollection<Long>, PCollections<Long>>of(\n+                \"nonEmptyInput\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Flatten.<Long>pCollections(),\n+                pipeline));\n+    assertThat(replacement.getInput().getAll(), emptyIterable());\n+  }\n+\n+  @Test\n+  public void getInputNonEmptyThrows() {\n+    PCollectionList<Long> nonEmpty =\n+        PCollectionList.of(pipeline.apply(CountingInput.unbounded()))\n+            .and(pipeline.apply(CountingInput.upTo(100L)));\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(nonEmpty.expand().toString());\n+    thrown.expectMessage(EmptyFlattenAsCreateFactory.class.getSimpleName());\n+    factory.getReplacementTransform(\n+        AppliedPTransform.<PCollectionList<Long>, PCollection<Long>, Flatten.PCollections<Long>>of(\n+            \"nonEmptyInput\",\n+            nonEmpty.expand(),\n+            Collections.<TupleTag<?>, PValue>emptyMap(),\n+            Flatten.<Long>pCollections(),\n+            pipeline));\n+  }\n+\n+  @Test\n+  public void mapOutputsSucceeds() {\n+    PCollection<Long> original = pipeline.apply(\"Original\", CountingInput.unbounded());\n+    PCollection<Long> replacement = pipeline.apply(\"Replacement\", CountingInput.unbounded());\n+    Map<PValue, ReplacementOutput> mapping = factory.mapOutputs(original.expand(), replacement);\n+\n+    assertThat(\n+        mapping,\n+        Matchers.<PValue, ReplacementOutput>hasEntry(\n+            replacement,\n+            ReplacementOutput.of(\n+                TaggedPValue.ofExpandedValue(original),\n+                TaggedPValue.ofExpandedValue(replacement))));\n+  }\n+\n+  @Test\n+  @Category(NeedsRunner.class)\n+  public void testOverride() {\n+    PCollectionList<Long> empty = PCollectionList.empty(pipeline);\n+    PCollection<Long> emptyFlattened =\n+        empty.apply(\n+            factory\n+                .getReplacementTransform(\n+                    AppliedPTransform\n+                        .<PCollectionList<Long>, PCollection<Long>, Flatten.PCollections<Long>>of(\n+                            \"nonEmptyInput\",\n+                            Collections.<TupleTag<?>, PValue>emptyMap(),\n+                            Collections.<TupleTag<?>, PValue>emptyMap(),\n+                            Flatten.<Long>pCollections(),\n+                            pipeline))\n+                .getTransform());\n+    PAssert.that(emptyFlattened).empty();\n+    pipeline.run();\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/EmptyFlattenAsCreateFactoryTest.java",
                "sha": "ae2d0a9d17659d761fd0222032fd19b103d1728a",
                "status": "added"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 16,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java",
                "patch": "@@ -15,14 +15,11 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.direct;\n+package org.apache.beam.runners.core.construction;\n \n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n import static org.mockito.Matchers.any;\n-import static org.mockito.Mockito.doThrow;\n-import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.when;\n \n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.PTransform;\n@@ -35,6 +32,7 @@\n import org.junit.runner.RunWith;\n import org.junit.runners.JUnit4;\n import org.mockito.Mock;\n+import org.mockito.Mockito;\n import org.mockito.MockitoAnnotations;\n \n /**\n@@ -63,26 +61,26 @@ public void setup() {\n   @Test\n   public void applyDelegates() {\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<Integer> collection = mock(PCollection.class);\n+    PCollection<Integer> collection = Mockito.mock(PCollection.class);\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<String> output = mock(PCollection.class);\n-    when(delegate.expand(collection)).thenReturn(output);\n+    PCollection<String> output = Mockito.mock(PCollection.class);\n+    Mockito.when(delegate.expand(collection)).thenReturn(output);\n     PCollection<String> result = forwarding.expand(collection);\n     assertThat(result, equalTo(output));\n   }\n \n   @Test\n   public void getNameDelegates() {\n     String name = \"My_forwardingptransform-name;for!thisTest\";\n-    when(delegate.getName()).thenReturn(name);\n+    Mockito.when(delegate.getName()).thenReturn(name);\n     assertThat(forwarding.getName(), equalTo(name));\n   }\n \n   @Test\n   public void validateDelegates() {\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<Integer> input = mock(PCollection.class);\n-    doThrow(RuntimeException.class).when(delegate).validate(input);\n+    PCollection<Integer> input = Mockito.mock(PCollection.class);\n+    Mockito.doThrow(RuntimeException.class).when(delegate).validate(input);\n \n     thrown.expect(RuntimeException.class);\n     forwarding.validate(input);\n@@ -91,20 +89,21 @@ public void validateDelegates() {\n   @Test\n   public void getDefaultOutputCoderDelegates() throws Exception {\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<Integer> input = mock(PCollection.class);\n+    PCollection<Integer> input = Mockito.mock(PCollection.class);\n     @SuppressWarnings(\"unchecked\")\n-    PCollection<String> output = mock(PCollection.class);\n+    PCollection<String> output = Mockito.mock(PCollection.class);\n     @SuppressWarnings(\"unchecked\")\n-    Coder<String> outputCoder = mock(Coder.class);\n+    Coder<String> outputCoder = Mockito.mock(Coder.class);\n \n-    when(delegate.getDefaultOutputCoder(input, output)).thenReturn(outputCoder);\n+    Mockito.when(delegate.getDefaultOutputCoder(input, output)).thenReturn(outputCoder);\n     assertThat(forwarding.getDefaultOutputCoder(input, output), equalTo(outputCoder));\n   }\n \n   @Test\n   public void populateDisplayDataDelegates() {\n-    doThrow(RuntimeException.class)\n-        .when(delegate).populateDisplayData(any(DisplayData.Builder.class));\n+    Mockito.doThrow(RuntimeException.class)\n+        .when(delegate)\n+        .populateDisplayData(any(DisplayData.Builder.class));\n \n     thrown.expect(RuntimeException.class);\n     DisplayData.from(forwarding);",
                "previous_filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ForwardingPTransformTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ForwardingPTransformTest.java",
                "sha": "7d3bfd8c6a8492a01d73bb5bd65ff063f0277755",
                "status": "renamed"
            },
            {
                "additions": 188,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java",
                "changes": 188,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java",
                "patch": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.auto.value.AutoValue;\n+import com.google.common.collect.ImmutableList;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.Collection;\n+import java.util.Collections;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.BigEndianLongCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CustomCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.windowing.AfterFirst;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.AfterProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.NonMergingWindowFn;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n+import org.apache.beam.sdk.util.VarInt;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollection.IsBounded;\n+import org.hamcrest.Matchers;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+/**\n+ * Tests for {@link PCollections}.\n+ */\n+@RunWith(Parameterized.class)\n+public class PCollectionsTest {\n+  // Each spec activates tests of all subsets of its fields\n+  @Parameters(name = \"{index}: {0}\")\n+  public static Iterable<PCollection<?>> data() {\n+    Pipeline pipeline = TestPipeline.create();\n+    PCollection<Integer> ints = pipeline.apply(\"ints\", Create.of(1, 2, 3));\n+    PCollection<Long> longs = pipeline.apply(\"unbounded longs\", CountingInput.unbounded());\n+    PCollection<Long> windowedLongs =\n+        longs.apply(\n+            \"into fixed windows\",\n+            Window.<Long>into(FixedWindows.of(Duration.standardMinutes(10L))));\n+    PCollection<KV<String, Iterable<String>>> groupedStrings =\n+        pipeline\n+            .apply(\n+                \"kvs\", Create.of(KV.of(\"foo\", \"spam\"), KV.of(\"bar\", \"ham\"), KV.of(\"baz\", \"eggs\")))\n+            .apply(\"group\", GroupByKey.<String, String>create());\n+    PCollection<Long> coderLongs =\n+        pipeline\n+            .apply(\"counts with alternative coder\", CountingInput.upTo(10L))\n+            .setCoder(BigEndianLongCoder.of());\n+    PCollection<Integer> allCustomInts =\n+        pipeline\n+            .apply(\n+                \"intsWithCustomCoder\",\n+                Create.of(1, 2).withCoder(new AutoValue_PCollectionsTest_CustomIntCoder()))\n+            .apply(\n+                \"into custom windows\",\n+                Window.<Integer>into(new CustomWindows())\n+                    .triggering(\n+                        AfterWatermark.pastEndOfWindow()\n+                            .withEarlyFirings(\n+                                AfterFirst.of(\n+                                    AfterPane.elementCountAtLeast(5),\n+                                    AfterProcessingTime.pastFirstElementInPane()\n+                                        .plusDelayOf(Duration.millis(227L)))))\n+                    .accumulatingFiredPanes()\n+                    .withAllowedLateness(Duration.standardMinutes(12L)));\n+    return ImmutableList.<PCollection<?>>of(ints, longs, windowedLongs, coderLongs, groupedStrings);\n+  }\n+\n+  @Parameter(0)\n+  public PCollection<?> testCollection;\n+\n+  @Test\n+  public void testEncodeDecodeCycle() throws Exception {\n+    SdkComponents sdkComponents = SdkComponents.create();\n+    RunnerApi.PCollection protoCollection = PCollections.toProto(testCollection, sdkComponents);\n+    RunnerApi.Components protoComponents = sdkComponents.toComponents();\n+    Coder<?> decodedCoder = PCollections.getCoder(protoCollection, protoComponents);\n+    WindowingStrategy<?, ?> decodedStrategy =\n+        PCollections.getWindowingStrategy(protoCollection, protoComponents);\n+    IsBounded decodedIsBounded = PCollections.isBounded(protoCollection);\n+\n+    assertThat(decodedCoder, Matchers.<Coder<?>>equalTo(testCollection.getCoder()));\n+    assertThat(\n+        decodedStrategy,\n+        Matchers.<WindowingStrategy<?, ?>>equalTo(\n+            testCollection.getWindowingStrategy().fixDefaults()));\n+    assertThat(decodedIsBounded, equalTo(testCollection.isBounded()));\n+  }\n+\n+  @AutoValue\n+  abstract static class CustomIntCoder extends CustomCoder<Integer> {\n+    @Override\n+    public void encode(Integer value, OutputStream outStream, Context context) throws IOException {\n+      VarInt.encode(value, outStream);\n+    }\n+\n+    @Override\n+    public Integer decode(InputStream inStream, Context context) throws IOException {\n+      return VarInt.decodeInt(inStream);\n+    }\n+  }\n+\n+  private static class CustomWindows extends NonMergingWindowFn<Integer, BoundedWindow> {\n+    @Override\n+    public Collection<BoundedWindow> assignWindows(final AssignContext c) throws Exception {\n+      return Collections.<BoundedWindow>singleton(\n+          new BoundedWindow() {\n+            @Override\n+            public Instant maxTimestamp() {\n+              return new Instant(c.element().longValue());\n+            }\n+          });\n+    }\n+\n+    @Override\n+    public boolean isCompatible(WindowFn<?, ?> other) {\n+      return other != null && this.getClass().equals(other.getClass());\n+    }\n+\n+    @Override\n+    public Coder<BoundedWindow> windowCoder() {\n+      return new CustomCoder<BoundedWindow>() {\n+        @Override public void verifyDeterministic() {}\n+\n+        @Override\n+        public void encode(BoundedWindow value, OutputStream outStream, Context context)\n+            throws IOException {\n+          VarInt.encode(value.maxTimestamp().getMillis(), outStream);\n+        }\n+\n+        @Override\n+        public BoundedWindow decode(InputStream inStream, Context context) throws IOException {\n+          final Instant ts = new Instant(VarInt.decodeLong(inStream));\n+          return new BoundedWindow() {\n+            @Override\n+            public Instant maxTimestamp() {\n+              return ts;\n+            }\n+          };\n+        }\n+      };\n+    }\n+\n+    @Override\n+    public WindowMappingFn<BoundedWindow> getDefaultWindowMappingFn() {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PCollectionsTest.java",
                "sha": "636d2459293b7610f3675db7ab09c370733adee2",
                "status": "added"
            },
            {
                "additions": 155,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java",
                "changes": 198,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 43,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java",
                "patch": "@@ -24,6 +24,7 @@\n import static org.junit.Assert.assertThat;\n \n import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n import java.io.Serializable;\n import java.util.Collections;\n import org.apache.beam.sdk.coders.VarIntCoder;\n@@ -37,16 +38,23 @@\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Materialization;\n+import org.apache.beam.sdk.transforms.Materializations;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.Sum;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.transforms.ViewFn;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.util.PCollectionViews;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.Timer;\n import org.apache.beam.sdk.util.TimerSpec;\n import org.apache.beam.sdk.util.TimerSpecs;\n+import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.state.StateSpec;\n import org.apache.beam.sdk.util.state.StateSpecs;\n@@ -55,8 +63,9 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PDone;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.apache.beam.sdk.values.TupleTagList;\n import org.hamcrest.Matchers;\n@@ -91,7 +100,7 @@\n \n   @Test\n   public void classEqualToMatchesSameClass() {\n-    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.Bound.class);\n+    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.SingleOutput.class);\n     AppliedPTransform<?, ?, ?> application =\n         getAppliedTransform(\n             ParDo.of(\n@@ -126,7 +135,7 @@ public void classEqualToDoesNotMatchSubclass() {\n \n   @Test\n   public void classEqualToDoesNotMatchUnrelatedClass() {\n-    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.Bound.class);\n+    PTransformMatcher matcher = PTransformMatchers.classEqualTo(ParDo.SingleOutput.class);\n     AppliedPTransform<?, ?, ?> application =\n         getAppliedTransform(Window.<KV<String, Integer>>into(new GlobalWindows()));\n \n@@ -191,7 +200,7 @@ public void onTimer(OnTimerContext context) {\n       };\n \n   /**\n-   * Demonstrates that a {@link ParDo.Bound} does not match any ParDo matcher.\n+   * Demonstrates that a {@link ParDo.SingleOutput} does not match any ParDo matcher.\n    */\n   @Test\n   public void parDoSingle() {\n@@ -323,21 +332,68 @@ public void parDoWithFnTypeNotParDo() {\n     assertThat(matcher.matches(notParDo), is(false));\n   }\n \n+  @Test\n+  public void createViewWithViewFn() {\n+    PCollection<Integer> input = p.apply(Create.of(1));\n+    PCollectionView<Iterable<Integer>> view =\n+        PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n+    ViewFn<Iterable<WindowedValue<?>>, Iterable<Integer>> viewFn = view.getViewFn();\n+    CreatePCollectionView<?, ?> createView = CreatePCollectionView.of(view);\n+\n+    PTransformMatcher matcher = PTransformMatchers.createViewWithViewFn(viewFn.getClass());\n+    assertThat(matcher.matches(getAppliedTransform(createView)), is(true));\n+  }\n+\n+  @Test\n+  public void createViewWithViewFnDifferentViewFn() {\n+    PCollection<Integer> input = p.apply(Create.of(1));\n+    PCollectionView<Iterable<Integer>> view =\n+        PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n+    ViewFn<Iterable<WindowedValue<?>>, Iterable<Integer>> viewFn =\n+        new ViewFn<Iterable<WindowedValue<?>>, Iterable<Integer>>() {\n+          @Override\n+          public Materialization<Iterable<WindowedValue<?>>> getMaterialization() {\n+            @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+            Materialization<Iterable<WindowedValue<?>>> materialization =\n+                (Materialization) Materializations.iterable();\n+            return materialization;\n+          }\n+\n+          @Override\n+          public Iterable<Integer> apply(Iterable<WindowedValue<?>> contents) {\n+            return Collections.emptyList();\n+          }\n+        };\n+    CreatePCollectionView<?, ?> createView = CreatePCollectionView.of(view);\n+\n+    PTransformMatcher matcher = PTransformMatchers.createViewWithViewFn(viewFn.getClass());\n+    assertThat(matcher.matches(getAppliedTransform(createView)), is(false));\n+  }\n+\n+  @Test\n+  public void createViewWithViewFnNotCreatePCollectionView() {\n+    PCollection<Integer> input = p.apply(Create.of(1));\n+    PCollectionView<Iterable<Integer>> view =\n+        PCollectionViews.iterableView(input, input.getWindowingStrategy(), input.getCoder());\n+\n+    PTransformMatcher matcher =\n+        PTransformMatchers.createViewWithViewFn(view.getViewFn().getClass());\n+    assertThat(matcher.matches(getAppliedTransform(View.asIterable())), is(false));\n+  }\n+\n   @Test\n   public void emptyFlattenWithEmptyFlatten() {\n     AppliedPTransform application =\n         AppliedPTransform\n-            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>\n-                of(\n-                    \"EmptyFlatten\",\n-                    Collections.<TaggedPValue>emptyList(),\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Flatten.pCollections(),\n-                    p);\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"EmptyFlatten\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n \n     assertThat(PTransformMatchers.emptyFlatten().matches(application), is(true));\n   }\n@@ -346,21 +402,18 @@ public void emptyFlattenWithEmptyFlatten() {\n   public void emptyFlattenWithNonEmptyFlatten() {\n     AppliedPTransform application =\n         AppliedPTransform\n-            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>\n-                of(\n-                    \"Flatten\",\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Flatten.pCollections(),\n-                    p);\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"Flatten\",\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n \n     assertThat(PTransformMatchers.emptyFlatten().matches(application), is(false));\n   }\n@@ -369,22 +422,81 @@ public void emptyFlattenWithNonEmptyFlatten() {\n   public void emptyFlattenWithNonFlatten() {\n     AppliedPTransform application =\n         AppliedPTransform\n-            .<PCollection<Iterable<Object>>, PCollection<Object>, Flatten.Iterables<Object>>\n-                of(\n-                    \"EmptyFlatten\",\n-                    Collections.<TaggedPValue>emptyList(),\n-                    Collections.singletonList(\n-                        TaggedPValue.of(\n-                            new TupleTag<Object>(),\n-                            PCollection.createPrimitiveOutputInternal(\n-                                p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED))),\n-                    Flatten.iterables() /* This isn't actually possible to construct,\n+            .<PCollection<Iterable<Object>>, PCollection<Object>, Flatten.Iterables<Object>>of(\n+                \"EmptyFlatten\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.iterables() /* This isn't actually possible to construct,\n                                  * but for the sake of example */,\n-                    p);\n+                p);\n \n     assertThat(PTransformMatchers.emptyFlatten().matches(application), is(false));\n   }\n \n+  @Test\n+  public void flattenWithDuplicateInputsWithoutDuplicates() {\n+    AppliedPTransform application =\n+        AppliedPTransform\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"Flatten\",\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n+\n+    assertThat(PTransformMatchers.flattenWithDuplicateInputs().matches(application), is(false));\n+  }\n+\n+  @Test\n+  public void flattenWithDuplicateInputsWithDuplicates() {\n+    PCollection<Object> duplicate =\n+        PCollection.createPrimitiveOutputInternal(\n+            p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED);\n+    AppliedPTransform application =\n+        AppliedPTransform\n+            .<PCollectionList<Object>, PCollection<Object>, Flatten.PCollections<Object>>of(\n+                \"Flatten\",\n+                ImmutableMap.<TupleTag<?>, PValue>builder()\n+                    .put(new TupleTag<Object>(), duplicate)\n+                    .put(new TupleTag<Object>(), duplicate)\n+                    .build(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.pCollections(),\n+                p);\n+\n+    assertThat(PTransformMatchers.flattenWithDuplicateInputs().matches(application), is(true));\n+  }\n+\n+  @Test\n+  public void flattenWithDuplicateInputsNonFlatten() {\n+    AppliedPTransform application =\n+        AppliedPTransform\n+            .<PCollection<Iterable<Object>>, PCollection<Object>, Flatten.Iterables<Object>>of(\n+                \"EmptyFlatten\",\n+                Collections.<TupleTag<?>, PValue>emptyMap(),\n+                Collections.<TupleTag<?>, PValue>singletonMap(\n+                    new TupleTag<Object>(),\n+                    PCollection.createPrimitiveOutputInternal(\n+                        p, WindowingStrategy.globalDefault(), IsBounded.BOUNDED)),\n+                Flatten.iterables() /* This isn't actually possible to construct,\n+                                 * but for the sake of example */,\n+                p);\n+\n+    assertThat(PTransformMatchers.flattenWithDuplicateInputs().matches(application), is(false));\n+  }\n+\n   @Test\n   public void writeWithRunnerDeterminedSharding() {\n     Write<Integer> write =\n@@ -417,8 +529,8 @@ public void writeWithRunnerDeterminedSharding() {\n   private AppliedPTransform<?, ?, ?> appliedWrite(Write<Integer> write) {\n     return AppliedPTransform.<PCollection<Integer>, PDone, Write<Integer>>of(\n         \"Write\",\n-        Collections.<TaggedPValue>emptyList(),\n-        Collections.<TaggedPValue>emptyList(),\n+        Collections.<TupleTag<?>, PValue>emptyMap(),\n+        Collections.<TupleTag<?>, PValue>emptyMap(),\n         write,\n         p);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformMatchersTest.java",
                "sha": "4084cdcaed15803fe9b1153d1a71ae7f2f1b16de",
                "status": "modified"
            },
            {
                "additions": 131,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java",
                "changes": 131,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java",
                "patch": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.common.collect.ImmutableMap;\n+import java.util.Collections;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link PTransformReplacements}.\n+ */\n+@RunWith(JUnit4.class)\n+public class PTransformReplacementsTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+  private PCollection<Long> mainInput = pipeline.apply(CountingInput.unbounded());\n+  private PCollectionView<String> sideInput =\n+      pipeline.apply(Create.of(\"foo\")).apply(View.<String>asSingleton());\n+\n+  private PCollection<Long> output = mainInput.apply(ParDo.of(new TestDoFn()));\n+\n+  @Test\n+  public void getMainInputSingleOutputSingleInput() {\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), mainInput),\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()),\n+            pipeline);\n+    PCollection<Long> input = PTransformReplacements.getSingletonMainInput(application);\n+    assertThat(input, equalTo(mainInput));\n+  }\n+\n+  @Test\n+  public void getMainInputSingleOutputSideInputs() {\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            ImmutableMap.<TupleTag<?>, PValue>builder()\n+                .put(new TupleTag<Long>(), mainInput)\n+                .put(sideInput.getTagInternal(), sideInput.getPCollection())\n+                .build(),\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()).withSideInputs(sideInput),\n+            pipeline);\n+    PCollection<Long> input = PTransformReplacements.getSingletonMainInput(application);\n+    assertThat(input, equalTo(mainInput));\n+  }\n+\n+  @Test\n+  public void getMainInputExtraMainInputsThrows() {\n+    PCollection<Long> notInParDo = pipeline.apply(\"otherPCollection\", Create.of(1L, 2L, 3L));\n+    ImmutableMap<TupleTag<?>, PValue> inputs =\n+        ImmutableMap.<TupleTag<?>, PValue>builder()\n+            .putAll(mainInput.expand())\n+            // Not represnted as an input\n+            .put(new TupleTag<Long>(), notInParDo)\n+            .put(sideInput.getTagInternal(), sideInput.getPCollection())\n+            .build();\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            inputs,\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()).withSideInputs(sideInput),\n+            pipeline);\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(\"multiple inputs\");\n+    thrown.expectMessage(\"not additional inputs\");\n+    thrown.expectMessage(mainInput.toString());\n+    thrown.expectMessage(notInParDo.toString());\n+    PTransformReplacements.getSingletonMainInput(application);\n+  }\n+\n+  @Test\n+  public void getMainInputNoMainInputsThrows() {\n+    ImmutableMap<TupleTag<?>, PValue> inputs =\n+        ImmutableMap.<TupleTag<?>, PValue>builder()\n+            .put(sideInput.getTagInternal(), sideInput.getPCollection())\n+            .build();\n+    AppliedPTransform<PCollection<Long>, ?, ?> application =\n+        AppliedPTransform.of(\n+            \"application\",\n+            inputs,\n+            Collections.<TupleTag<?>, PValue>singletonMap(new TupleTag<Long>(), output),\n+            ParDo.of(new TestDoFn()).withSideInputs(sideInput),\n+            pipeline);\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(\"No main input\");\n+    PTransformReplacements.getSingletonMainInput(application);\n+  }\n+\n+  private static class TestDoFn extends DoFn<Long, Long> {\n+    @ProcessElement public void process(ProcessContext context) {}\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformReplacementsTest.java",
                "sha": "b0656177abe2b787a86746aecfa0cb8056397eab",
                "status": "added"
            },
            {
                "additions": 189,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java",
                "changes": 189,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java",
                "patch": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.auto.value.AutoValue;\n+import com.google.common.collect.ImmutableList;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.PTransform;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.io.CountingInput.UnboundedCountingInput;\n+import org.apache.beam.sdk.io.CountingSource;\n+import org.apache.beam.sdk.io.Read;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionTuple;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.apache.beam.sdk.values.TupleTagList;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+/**\n+ * Tests for {@link PTransforms}.\n+ */\n+@RunWith(Parameterized.class)\n+public class PTransformsTest {\n+\n+  @Parameters(name = \"{index}: {0}\")\n+  public static Iterable<ToAndFromProtoSpec> data() {\n+    // This pipeline exists for construction, not to run any test.\n+    // TODO: Leaf node with understood payload - i.e. validate payloads\n+    ToAndFromProtoSpec readLeaf = ToAndFromProtoSpec.leaf(read(TestPipeline.create()));\n+    ToAndFromProtoSpec readMultipleInAndOut =\n+        ToAndFromProtoSpec.leaf(multiMultiParDo(TestPipeline.create()));\n+    TestPipeline compositeReadPipeline = TestPipeline.create();\n+    ToAndFromProtoSpec compositeRead =\n+        ToAndFromProtoSpec.composite(\n+            countingInput(compositeReadPipeline),\n+            ToAndFromProtoSpec.leaf(read(compositeReadPipeline)));\n+    return ImmutableList.<ToAndFromProtoSpec>builder()\n+        .add(readLeaf)\n+        .add(readMultipleInAndOut)\n+        .add(compositeRead)\n+        // TODO: Composite with multiple children\n+        // TODO: Composite with a composite child\n+        .build();\n+  }\n+\n+  @AutoValue\n+  abstract static class ToAndFromProtoSpec {\n+    public static ToAndFromProtoSpec leaf(AppliedPTransform<?, ?, ?> transform) {\n+      return new AutoValue_PTransformsTest_ToAndFromProtoSpec(\n+          transform, Collections.<ToAndFromProtoSpec>emptyList());\n+    }\n+\n+    public static ToAndFromProtoSpec composite(\n+        AppliedPTransform<?, ?, ?> topLevel, ToAndFromProtoSpec spec, ToAndFromProtoSpec... specs) {\n+      List<ToAndFromProtoSpec> childSpecs = new ArrayList<>();\n+      childSpecs.add(spec);\n+      childSpecs.addAll(Arrays.asList(specs));\n+      return new AutoValue_PTransformsTest_ToAndFromProtoSpec(topLevel, childSpecs);\n+    }\n+\n+    abstract AppliedPTransform<?, ?, ?> getTransform();\n+    abstract Collection<ToAndFromProtoSpec> getChildren();\n+  }\n+\n+  @Parameter(0)\n+  public ToAndFromProtoSpec spec;\n+\n+  @Test\n+  public void toAndFromProto() throws IOException {\n+    SdkComponents components = SdkComponents.create();\n+    RunnerApi.PTransform converted = convert(spec, components);\n+    Components protoComponents = components.toComponents();\n+\n+    // Sanity checks\n+    assertThat(converted.getInputsCount(), equalTo(spec.getTransform().getInputs().size()));\n+    assertThat(converted.getOutputsCount(), equalTo(spec.getTransform().getOutputs().size()));\n+    assertThat(converted.getSubtransformsCount(), equalTo(spec.getChildren().size()));\n+\n+    assertThat(converted.getUniqueName(), equalTo(spec.getTransform().getFullName()));\n+    for (PValue inputValue : spec.getTransform().getInputs().values()) {\n+      PCollection<?> inputPc = (PCollection<?>) inputValue;\n+      protoComponents.getPcollectionsOrThrow(components.registerPCollection(inputPc));\n+    }\n+    for (PValue outputValue : spec.getTransform().getOutputs().values()) {\n+      PCollection<?> outputPc = (PCollection<?>) outputValue;\n+      protoComponents.getPcollectionsOrThrow(components.registerPCollection(outputPc));\n+    }\n+  }\n+\n+  private RunnerApi.PTransform convert(ToAndFromProtoSpec spec, SdkComponents components)\n+      throws IOException {\n+    List<AppliedPTransform<?, ?, ?>> childTransforms = new ArrayList<>();\n+    for (ToAndFromProtoSpec child : spec.getChildren()) {\n+      childTransforms.add(child.getTransform());\n+      System.out.println(\"Converting child \" + child);\n+      convert(child, components);\n+      // Sanity call\n+      components.getExistingPTransformId(child.getTransform());\n+    }\n+    PTransform convert = PTransforms.toProto(spec.getTransform(), childTransforms, components);\n+    // Make sure the converted transform is registered. Convert it independently, but if this is a\n+    // child spec, the child must be in the components.\n+    components.registerPTransform(spec.getTransform(), childTransforms);\n+    return convert;\n+  }\n+\n+  private static class TestDoFn extends DoFn<Long, KV<Long, String>> {\n+    // Exists to stop the ParDo application from throwing\n+    @ProcessElement public void process(ProcessContext context) {}\n+  }\n+\n+  private static AppliedPTransform<?, ?, ?> countingInput(Pipeline pipeline) {\n+    UnboundedCountingInput input = CountingInput.unbounded();\n+    PCollection<Long> pcollection = pipeline.apply(input);\n+    return AppliedPTransform.<PBegin, PCollection<Long>, UnboundedCountingInput>of(\n+        \"Count\", pipeline.begin().expand(), pcollection.expand(), input, pipeline);\n+  }\n+\n+  private static AppliedPTransform<?, ?, ?> read(Pipeline pipeline) {\n+    Read.Unbounded<Long> transform = Read.from(CountingSource.unbounded());\n+    PCollection<Long> pcollection = pipeline.apply(transform);\n+    return AppliedPTransform.<PBegin, PCollection<Long>, Read.Unbounded<Long>>of(\n+        \"ReadTheCount\", pipeline.begin().expand(), pcollection.expand(), transform, pipeline);\n+  }\n+\n+  private static AppliedPTransform<?, ?, ?> multiMultiParDo(Pipeline pipeline) {\n+    PCollectionView<String> view =\n+        pipeline.apply(Create.of(\"foo\")).apply(View.<String>asSingleton());\n+    PCollection<Long> input = pipeline.apply(CountingInput.unbounded());\n+    ParDo.MultiOutput<Long, KV<Long, String>> parDo =\n+        ParDo.of(new TestDoFn())\n+            .withSideInputs(view)\n+            .withOutputTags(\n+                new TupleTag<KV<Long, String>>() {},\n+                TupleTagList.of(new TupleTag<KV<String, Long>>() {}));\n+    PCollectionTuple output = input.apply(parDo);\n+\n+    Map<TupleTag<?>, PValue> inputs = new HashMap<>();\n+    inputs.putAll(parDo.getAdditionalInputs());\n+    inputs.putAll(input.expand());\n+\n+    return AppliedPTransform\n+        .<PCollection<Long>, PCollectionTuple, ParDo.MultiOutput<Long, KV<Long, String>>>of(\n+            \"MultiParDoInAndOut\", inputs, output.expand(), parDo, pipeline);\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/PTransformsTest.java",
                "sha": "4e3cdb63bcb5a9b17b128307b4be74fbe0b0ca84",
                "status": "added"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 99,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java",
                "patch": "@@ -21,18 +21,15 @@\n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n \n-import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.Iterables;\n-import java.util.List;\n import java.util.Map;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n-import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.PCollectionTuple;\n-import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TaggedPValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -79,53 +76,22 @@ public void singletonSucceeds() {\n     assertThat(replacements, Matchers.<PValue>hasKey(replacementInts));\n \n     ReplacementOutput replacement = replacements.get(replacementInts);\n-    TaggedPValue taggedInts = Iterables.getOnlyElement(ints.expand());\n-    assertThat(replacement.getOriginal(), equalTo(taggedInts));\n+    Map.Entry<TupleTag<?>, PValue> taggedInts = Iterables.getOnlyElement(ints.expand().entrySet());\n+    assertThat(\n+        replacement.getOriginal().getTag(), Matchers.<TupleTag<?>>equalTo(taggedInts.getKey()));\n+    assertThat(replacement.getOriginal().getValue(), equalTo(taggedInts.getValue()));\n     assertThat(replacement.getReplacement().getValue(), Matchers.<PValue>equalTo(replacementInts));\n   }\n \n   @Test\n   public void singletonMultipleOriginalsThrows() {\n     thrown.expect(IllegalArgumentException.class);\n     ReplacementOutputs.singleton(\n-        ImmutableList.copyOf(Iterables.concat(ints.expand(), moreInts.expand())), replacementInts);\n-  }\n-\n-  @Test\n-  public void orderedSucceeds() {\n-    List<TaggedPValue> originals = PCollectionList.of(ints).and(moreInts).expand();\n-    Map<PValue, ReplacementOutput> replacements =\n-        ReplacementOutputs.ordered(\n-            originals, PCollectionList.of(replacementInts).and(moreReplacementInts));\n-    assertThat(\n-        replacements.keySet(),\n-        Matchers.<PValue>containsInAnyOrder(replacementInts, moreReplacementInts));\n-\n-    ReplacementOutput intsMapping = replacements.get(replacementInts);\n-    assertThat(intsMapping.getOriginal().getValue(), Matchers.<PValue>equalTo(ints));\n-    assertThat(intsMapping.getReplacement().getValue(), Matchers.<PValue>equalTo(replacementInts));\n-\n-    ReplacementOutput moreIntsMapping = replacements.get(moreReplacementInts);\n-    assertThat(moreIntsMapping.getOriginal().getValue(), Matchers.<PValue>equalTo(moreInts));\n-    assertThat(\n-        moreIntsMapping.getReplacement().getValue(), Matchers.<PValue>equalTo(moreReplacementInts));\n-  }\n-\n-  @Test\n-  public void orderedTooManyReplacements() {\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"same size\");\n-    ReplacementOutputs.ordered(\n-        PCollectionList.of(ints).expand(),\n-        PCollectionList.of(replacementInts).and(moreReplacementInts));\n-  }\n-\n-  @Test\n-  public void orderedTooFewReplacements() {\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"same size\");\n-    ReplacementOutputs.ordered(\n-        PCollectionList.of(ints).and(moreInts).expand(), PCollectionList.of(moreReplacementInts));\n+        ImmutableMap.<TupleTag<?>, PValue>builder()\n+            .putAll(ints.expand())\n+            .putAll(moreInts.expand())\n+            .build(),\n+        replacementInts);\n   }\n \n   private TupleTag<Integer> intsTag = new TupleTag<>();\n@@ -168,61 +134,6 @@ public void taggedSucceeds() {\n                 TaggedPValue.of(moreIntsTag, moreReplacementInts))));\n   }\n \n-  /**\n-   * When a call to {@link ReplacementOutputs#tagged(List, POutput)} is made where the first\n-   * argument contains multiple copies of the same {@link TaggedPValue}, the call succeeds using\n-   * that mapping.\n-   */\n-  @Test\n-  public void taggedMultipleInstances() {\n-    List<TaggedPValue> original =\n-        ImmutableList.of(\n-            TaggedPValue.of(intsTag, ints),\n-            TaggedPValue.of(strsTag, strs),\n-            TaggedPValue.of(intsTag, ints));\n-\n-    Map<PValue, ReplacementOutput> replacements =\n-        ReplacementOutputs.tagged(\n-            original, PCollectionTuple.of(strsTag, replacementStrs).and(intsTag, replacementInts));\n-    assertThat(\n-        replacements.keySet(),\n-        Matchers.<PValue>containsInAnyOrder(replacementStrs, replacementInts));\n-    ReplacementOutput intsReplacement = replacements.get(replacementInts);\n-    ReplacementOutput strsReplacement = replacements.get(replacementStrs);\n-\n-    assertThat(\n-        intsReplacement,\n-        equalTo(\n-            ReplacementOutput.of(\n-                TaggedPValue.of(intsTag, ints), TaggedPValue.of(intsTag, replacementInts))));\n-    assertThat(\n-        strsReplacement,\n-        equalTo(\n-            ReplacementOutput.of(\n-                TaggedPValue.of(strsTag, strs), TaggedPValue.of(strsTag, replacementStrs))));\n-  }\n-\n-  /**\n-   * When a call to {@link ReplacementOutputs#tagged(List, POutput)} is made where a single tag\n-   * has multiple {@link PValue PValues} mapped to it, the call fails.\n-   */\n-  @Test\n-  public void taggedMultipleConflictingInstancesThrows() {\n-    List<TaggedPValue> original =\n-        ImmutableList.of(\n-            TaggedPValue.of(intsTag, ints), TaggedPValue.of(intsTag, moreReplacementInts));\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"different values\");\n-    thrown.expectMessage(intsTag.toString());\n-    thrown.expectMessage(ints.toString());\n-    thrown.expectMessage(moreReplacementInts.toString());\n-    ReplacementOutputs.tagged(\n-        original,\n-        PCollectionTuple.of(strsTag, replacementStrs)\n-            .and(moreIntsTag, moreReplacementInts)\n-            .and(intsTag, replacementInts));\n-  }\n-\n   @Test\n   public void taggedMissingReplacementThrows() {\n     PCollectionTuple original =",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/ReplacementOutputsTest.java",
                "sha": "00c436d5cb3feead112c58fd95788320d6782ded",
                "status": "modified"
            },
            {
                "additions": 223,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java",
                "changes": 223,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java",
                "patch": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.isEmptyOrNullString;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertThat;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.IterableCoder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.SetCoder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi.Components;\n+import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Tests for {@link SdkComponents}. */\n+@RunWith(JUnit4.class)\n+public class SdkComponentsTest {\n+  @Rule\n+  public TestPipeline pipeline = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+  @Rule\n+  public ExpectedException thrown = ExpectedException.none();\n+\n+  private SdkComponents components = SdkComponents.create();\n+\n+  @Test\n+  public void registerCoder() throws IOException {\n+    Coder<?> coder =\n+        KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));\n+    String id = components.registerCoder(coder);\n+    assertThat(components.registerCoder(coder), equalTo(id));\n+    assertThat(id, not(isEmptyOrNullString()));\n+    VarLongCoder otherCoder = VarLongCoder.of();\n+    assertThat(components.registerCoder(otherCoder), not(equalTo(id)));\n+\n+    components.toComponents().getCodersOrThrow(id);\n+    components.toComponents().getCodersOrThrow(components.registerCoder(otherCoder));\n+  }\n+\n+  @Test\n+  public void registerCoderEqualsNotSame() throws IOException {\n+    Coder<?> coder =\n+        KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));\n+    Coder<?> otherCoder =\n+        KvCoder.of(StringUtf8Coder.of(), IterableCoder.of(SetCoder.of(ByteArrayCoder.of())));\n+    assertThat(coder, Matchers.<Coder<?>>equalTo(otherCoder));\n+    String id = components.registerCoder(coder);\n+    String otherId = components.registerCoder(otherCoder);\n+    assertThat(otherId, not(equalTo(id)));\n+\n+    components.toComponents().getCodersOrThrow(id);\n+    components.toComponents().getCodersOrThrow(otherId);\n+  }\n+\n+  @Test\n+  public void registerTransformNoChildren() throws IOException {\n+    Create.Values<Integer> create = Create.of(1, 2, 3);\n+    PCollection<Integer> pt = pipeline.apply(create);\n+    String userName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Integer>, Create.Values<Integer>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    String componentName =\n+        components.registerPTransform(\n+            transform, Collections.<AppliedPTransform<?, ?, ?>>emptyList());\n+    assertThat(componentName, equalTo(userName));\n+    assertThat(components.getExistingPTransformId(transform), equalTo(componentName));\n+  }\n+\n+  @Test\n+  public void registerTransformAfterChildren() throws IOException {\n+    Create.Values<Long> create = Create.of(1L, 2L, 3L);\n+    CountingInput.UnboundedCountingInput createChild = CountingInput.unbounded();\n+\n+    PCollection<Long> pt = pipeline.apply(create);\n+    String userName = \"my_transform\";\n+    String childUserName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, Create.Values<Long>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    AppliedPTransform<?, ?, ?> childTransform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, CountingInput.UnboundedCountingInput>of(\n+            childUserName, pipeline.begin().expand(), pt.expand(), createChild, pipeline);\n+\n+    String childId = components.registerPTransform(childTransform,\n+        Collections.<AppliedPTransform<?, ?, ?>>emptyList());\n+    String parentId = components.registerPTransform(transform,\n+        Collections.<AppliedPTransform<?, ?, ?>>singletonList(childTransform));\n+    Components components = this.components.toComponents();\n+    assertThat(components.getTransformsOrThrow(parentId).getSubtransforms(0), equalTo(childId));\n+    assertThat(components.getTransformsOrThrow(childId).getSubtransformsCount(), equalTo(0));\n+  }\n+\n+  @Test\n+  public void registerTransformEmptyFullName() throws IOException {\n+    Create.Values<Integer> create = Create.of(1, 2, 3);\n+    PCollection<Integer> pt = pipeline.apply(create);\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Integer>, Create.Values<Integer>>of(\n+            \"\", pipeline.begin().expand(), pt.expand(), create, pipeline);\n+\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(transform.toString());\n+    components.getExistingPTransformId(transform);\n+  }\n+\n+  @Test\n+  public void registerTransformNullComponents() throws IOException {\n+    Create.Values<Integer> create = Create.of(1, 2, 3);\n+    PCollection<Integer> pt = pipeline.apply(create);\n+    String userName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Integer>, Create.Values<Integer>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    thrown.expect(NullPointerException.class);\n+    thrown.expectMessage(\"child nodes may not be null\");\n+    components.registerPTransform(transform, null);\n+  }\n+\n+  /**\n+   * Tests that trying to register a transform which has unregistered children throws.\n+   */\n+  @Test\n+  public void registerTransformWithUnregisteredChildren() throws IOException {\n+    Create.Values<Long> create = Create.of(1L, 2L, 3L);\n+    CountingInput.UnboundedCountingInput createChild = CountingInput.unbounded();\n+\n+    PCollection<Long> pt = pipeline.apply(create);\n+    String userName = \"my_transform\";\n+    String childUserName = \"my_transform/my_nesting\";\n+    AppliedPTransform<?, ?, ?> transform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, Create.Values<Long>>of(\n+            userName, pipeline.begin().expand(), pt.expand(), create, pipeline);\n+    AppliedPTransform<?, ?, ?> childTransform =\n+        AppliedPTransform.<PBegin, PCollection<Long>, CountingInput.UnboundedCountingInput>of(\n+            childUserName, pipeline.begin().expand(), pt.expand(), createChild, pipeline);\n+\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(childTransform.toString());\n+    components.registerPTransform(\n+        transform, Collections.<AppliedPTransform<?, ?, ?>>singletonList(childTransform));\n+  }\n+\n+  @Test\n+  public void registerPCollection() throws IOException {\n+    PCollection<Long> pCollection = pipeline.apply(CountingInput.unbounded()).setName(\"foo\");\n+    String id = components.registerPCollection(pCollection);\n+    assertThat(id, equalTo(\"foo\"));\n+    components.toComponents().getPcollectionsOrThrow(id);\n+  }\n+\n+  @Test\n+  public void registerPCollectionExistingNameCollision() throws IOException {\n+    PCollection<Long> pCollection =\n+        pipeline.apply(\"FirstCount\", CountingInput.unbounded()).setName(\"foo\");\n+    String firstId = components.registerPCollection(pCollection);\n+    PCollection<Long> duplicate =\n+        pipeline.apply(\"SecondCount\", CountingInput.unbounded()).setName(\"foo\");\n+    String secondId = components.registerPCollection(duplicate);\n+    assertThat(firstId, equalTo(\"foo\"));\n+    assertThat(secondId, containsString(\"foo\"));\n+    assertThat(secondId, not(equalTo(\"foo\")));\n+    components.toComponents().getPcollectionsOrThrow(firstId);\n+    components.toComponents().getPcollectionsOrThrow(secondId);\n+  }\n+\n+  @Test\n+  public void registerWindowingStrategy() throws IOException {\n+    WindowingStrategy<?, ?> strategy =\n+        WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);\n+    String name = components.registerWindowingStrategy(strategy);\n+    assertThat(name, not(isEmptyOrNullString()));\n+\n+    components.toComponents().getWindowingStrategiesOrThrow(name);\n+  }\n+\n+  @Test\n+  public void registerWindowingStrategyIdEqualStrategies() throws IOException {\n+    WindowingStrategy<?, ?> strategy =\n+        WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);\n+    String name = components.registerWindowingStrategy(strategy);\n+    String duplicateName =\n+        components.registerWindowingStrategy(\n+            WindowingStrategy.globalDefault().withMode(AccumulationMode.ACCUMULATING_FIRED_PANES));\n+    assertThat(name, equalTo(duplicateName));\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SdkComponentsTest.java",
                "sha": "895aec48572d95f86bdff26c9aa3990925cbc8ad",
                "status": "added"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 24,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java",
                "patch": "@@ -20,18 +20,18 @@\n \n import static org.junit.Assert.assertThat;\n \n-import com.google.common.collect.Iterables;\n import java.io.Serializable;\n import java.util.Map;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory.ReplacementOutput;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.MapElements;\n-import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.SimpleFunction;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TaggedPValue;\n import org.hamcrest.Matchers;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -55,9 +55,15 @@\n               PCollection<? extends Integer>, PCollection<Integer>,\n               MapElements<Integer, Integer>>() {\n             @Override\n-            public PTransform<PCollection<? extends Integer>, PCollection<Integer>>\n-                getReplacementTransform(MapElements<Integer, Integer> transform) {\n-              return transform;\n+            public PTransformReplacement<PCollection<? extends Integer>, PCollection<Integer>>\n+                getReplacementTransform(\n+                    AppliedPTransform<\n+                            PCollection<? extends Integer>, PCollection<Integer>,\n+                            MapElements<Integer, Integer>>\n+                        transform) {\n+              return PTransformReplacement.of(\n+                  PTransformReplacements.getSingletonMainInput(transform),\n+                  transform.getTransform());\n             }\n           };\n \n@@ -68,23 +74,6 @@ public Integer apply(Integer input) {\n       }\n     };\n \n-  @Test\n-  public void testGetInput() {\n-    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));\n-    assertThat(\n-        factory.getInput(input.expand(), pipeline),\n-        Matchers.<PCollection<? extends Integer>>equalTo(input));\n-  }\n-\n-  @Test\n-  public void testGetInputMultipleInputsFails() {\n-    PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));\n-    PCollection<Integer> otherInput = pipeline.apply(\"OtherCreate\", Create.of(1, 2, 3));\n-\n-    thrown.expect(IllegalArgumentException.class);\n-    factory.getInput(PCollectionList.of(input).and(otherInput).expand(), pipeline);\n-  }\n-\n   @Test\n   public void testMapOutputs() {\n     PCollection<Integer> input = pipeline.apply(Create.of(1, 2, 3));\n@@ -97,8 +86,8 @@ public void testMapOutputs() {\n         Matchers.<PValue, ReplacementOutput>hasEntry(\n             reappliedOutput,\n             ReplacementOutput.of(\n-                Iterables.getOnlyElement(output.expand()),\n-                Iterables.getOnlyElement(reappliedOutput.expand()))));\n+                TaggedPValue.ofExpandedValue(output),\n+                TaggedPValue.ofExpandedValue(reappliedOutput))));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/SingleInputOutputOverrideFactoryTest.java",
                "sha": "acca5cd28b8490604aa3007d843596287808605b",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java",
                "patch": "@@ -15,13 +15,24 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.transforms.windowing;\n+package org.apache.beam.runners.core.construction;\n \n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n \n import com.google.auto.value.AutoValue;\n import com.google.common.collect.ImmutableList;\n+import org.apache.beam.sdk.transforms.windowing.AfterAll;\n+import org.apache.beam.sdk.transforms.windowing.AfterEach;\n+import org.apache.beam.sdk.transforms.windowing.AfterFirst;\n+import org.apache.beam.sdk.transforms.windowing.AfterPane;\n+import org.apache.beam.sdk.transforms.windowing.AfterProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterSynchronizedProcessingTime;\n+import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n+import org.apache.beam.sdk.transforms.windowing.DefaultTrigger;\n+import org.apache.beam.sdk.transforms.windowing.Never;\n+import org.apache.beam.sdk.transforms.windowing.Repeatedly;\n+import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.junit.Test;",
                "previous_filename": "sdks/java/core/src/test/java/org/apache/beam/sdk/transforms/windowing/TriggersTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/TriggersTest.java",
                "sha": "cf9d40c8c9cfb11bef42da57df47e8025d4ee2e3",
                "status": "renamed"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.runners.core;\n+package org.apache.beam.runners.core.construction;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n@@ -33,9 +33,9 @@\n import java.util.List;\n import java.util.NoSuchElementException;\n import java.util.Random;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.Checkpoint;\n-import org.apache.beam.runners.core.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.CheckpointCoder;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.Checkpoint;\n+import org.apache.beam.runners.core.construction.UnboundedReadFromBoundedSource.BoundedToUnboundedSourceAdapter.CheckpointCoder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.SerializableCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;",
                "previous_filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSourceTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnboundedReadFromBoundedSourceTest.java",
                "sha": "c905cf573a5a016edc32fbcae7258bc235f91c4c",
                "status": "renamed"
            },
            {
                "additions": 105,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java",
                "changes": 105,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java",
                "patch": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.core.construction;\n+\n+import static org.junit.Assert.assertThat;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor;\n+import org.apache.beam.sdk.io.CountingSource;\n+import org.apache.beam.sdk.io.Read;\n+import org.apache.beam.sdk.io.Read.Bounded;\n+import org.apache.beam.sdk.io.Read.Unbounded;\n+import org.apache.beam.sdk.runners.TransformHierarchy.Node;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.PValue;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/**\n+ * Tests for {@link UnconsumedReads}.\n+ */\n+@RunWith(JUnit4.class)\n+public class UnconsumedReadsTest {\n+  @Rule public TestPipeline pipeline = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+\n+  @Test\n+  public void matcherProducesUnconsumedValueBoundedRead() {\n+    Bounded<Long> transform = Read.from(CountingSource.upTo(20L));\n+    PCollection<Long> output = pipeline.apply(transform);\n+    UnconsumedReads.ensureAllReadsConsumed(pipeline);\n+    validateConsumed();\n+  }\n+\n+  @Test\n+  public void matcherProducesUnconsumedValueUnboundedRead() {\n+    Unbounded<Long> transform = Read.from(CountingSource.unbounded());\n+    PCollection<Long> output = pipeline.apply(transform);\n+    UnconsumedReads.ensureAllReadsConsumed(pipeline);\n+    validateConsumed();\n+  }\n+\n+  @Test\n+  public void doesNotConsumeAlreadyConsumedRead() {\n+    Unbounded<Long> transform = Read.from(CountingSource.unbounded());\n+    final PCollection<Long> output = pipeline.apply(transform);\n+    final Flatten.PCollections<Long> consumer = Flatten.<Long>pCollections();\n+    PCollectionList.of(output).apply(consumer);\n+    UnconsumedReads.ensureAllReadsConsumed(pipeline);\n+    pipeline.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            // The output should only be consumed by a single consumer\n+            if (node.getInputs().values().contains(output)) {\n+              assertThat(node.getTransform(), Matchers.<PTransform<?, ?>>is(consumer));\n+            }\n+          }\n+        });\n+  }\n+\n+  private void validateConsumed() {\n+    final Set<PValue> consumedOutputs = new HashSet<PValue>();\n+    final Set<PValue> allReadOutputs = new HashSet<PValue>();\n+    pipeline.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            consumedOutputs.addAll(node.getInputs().values());\n+          }\n+\n+          @Override\n+          public void visitValue(PValue value, Node producer) {\n+            if (producer.getTransform() instanceof Read.Bounded\n+                || producer.getTransform() instanceof Read.Unbounded) {\n+              allReadOutputs.add(value);\n+            }\n+          }\n+        });\n+    assertThat(consumedOutputs, Matchers.hasItems(allReadOutputs.toArray(new PValue[0])));\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnconsumedReadsTest.java",
                "sha": "1966a93ec4d9f73ac8764040b8d7142d76ae6896",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 12,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java",
                "patch": "@@ -19,11 +19,10 @@\n package org.apache.beam.runners.core.construction;\n \n import java.util.Collections;\n-import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.values.PDone;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.ExpectedException;\n@@ -46,20 +45,13 @@\n   public void getReplacementTransformThrows() {\n     thrown.expect(UnsupportedOperationException.class);\n     thrown.expectMessage(message);\n-    factory.getReplacementTransform(Create.empty(VoidCoder.of()));\n-  }\n-\n-  @Test\n-  public void getInputThrows() {\n-    thrown.expect(UnsupportedOperationException.class);\n-    thrown.expectMessage(message);\n-    factory.getInput(Collections.<TaggedPValue>emptyList(), pipeline);\n+    factory.getReplacementTransform(null);\n   }\n \n   @Test\n   public void mapOutputThrows() {\n     thrown.expect(UnsupportedOperationException.class);\n     thrown.expectMessage(message);\n-    factory.mapOutputs(Collections.<TaggedPValue>emptyList(), PDone.in(pipeline));\n+    factory.mapOutputs(Collections.<TupleTag<?>, PValue>emptyMap(), PDone.in(pipeline));\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/UnsupportedOverrideFactoryTest.java",
                "sha": "6d3b263c2618468c9a17c270a51b824b85391b41",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java",
                "patch": "@@ -15,20 +15,23 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core.construction;\n \n import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n \n import com.google.auto.value.AutoValue;\n import com.google.common.collect.ImmutableList;\n+import org.apache.beam.sdk.common.runner.v1.RunnerApi;\n import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n+import org.hamcrest.Matchers;\n import org.joda.time.Duration;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -88,4 +91,20 @@ public void testToProtoAndBack() throws Exception {\n         toProtoAndBackWindowingStrategy,\n         equalTo((WindowingStrategy) windowingStrategy.fixDefaults()));\n   }\n+\n+  @Test\n+  public void testToProtoAndBackWithComponents() throws Exception {\n+    WindowingStrategy<?, ?> windowingStrategy = toProtoAndBackSpec.getWindowingStrategy();\n+    SdkComponents components = SdkComponents.create();\n+    RunnerApi.WindowingStrategy proto =\n+        WindowingStrategies.toProto(windowingStrategy, components);\n+    RunnerApi.Components protoComponents = components.toComponents();\n+\n+    assertThat(\n+        WindowingStrategies.fromProto(proto, protoComponents).fixDefaults(),\n+        Matchers.<WindowingStrategy<?, ?>>equalTo(windowingStrategy.fixDefaults()));\n+\n+    protoComponents.getCodersOrThrow(\n+        components.registerCoder(windowingStrategy.getWindowFn().windowCoder()));\n+  }\n }",
                "previous_filename": "sdks/java/core/src/test/java/org/apache/beam/sdk/util/WindowingStrategiesTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-construction-java/src/test/java/org/apache/beam/runners/core/construction/WindowingStrategiesTest.java",
                "sha": "62bba8eb40ae76d600e80aa7dd68774e7ad0ebb4",
                "status": "renamed"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/pom.xml",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 53,
                "filename": "runners/core-java/pom.xml",
                "patch": "@@ -57,54 +57,6 @@\n         <artifactId>maven-jar-plugin</artifactId>\n       </plugin>\n \n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-shade-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <id>bundle-and-repackage</id>\n-            <phase>package</phase>\n-            <goals>\n-              <goal>shade</goal>\n-            </goals>\n-            <configuration>\n-              <shadeTestJar>true</shadeTestJar>\n-              <artifactSet>\n-                <includes>\n-                  <include>com.google.guava:guava</include>\n-                </includes>\n-              </artifactSet>\n-              <filters>\n-                <filter>\n-                  <artifact>*:*</artifact>\n-                  <excludes>\n-                    <exclude>META-INF/*.SF</exclude>\n-                    <exclude>META-INF/*.DSA</exclude>\n-                    <exclude>META-INF/*.RSA</exclude>\n-                  </excludes>\n-                </filter>\n-              </filters>\n-              <relocations>\n-                <!-- TODO: Once ready, change the following pattern to 'com'\n-                     only, exclude 'org.apache.beam.**', and remove\n-                     the second relocation. -->\n-                <relocation>\n-                  <pattern>com.google.common</pattern>\n-                  <shadedPattern>org.apache.beam.runners.core.repackaged.com.google.common</shadedPattern>\n-                </relocation>\n-                <relocation>\n-                  <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>org.apache.beam.runners.core.repackaged.com.google.thirdparty</shadedPattern>\n-                </relocation>\n-              </relocations>\n-              <transformers>\n-                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n-              </transformers>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n-\n       <!-- Coverage analysis for unit tests. -->\n       <plugin>\n         <groupId>org.jacoco</groupId>\n@@ -125,6 +77,11 @@\n       <artifactId>beam-sdks-common-runner-api</artifactId>\n     </dependency>\n \n+    <dependency>\n+      <groupId>org.apache.beam</groupId>\n+      <artifactId>beam-runners-core-construction-java</artifactId>\n+    </dependency>\n+\n     <!-- build dependencies -->\n \n     <dependency>\n@@ -153,11 +110,6 @@\n       <artifactId>joda-time</artifactId>\n     </dependency>\n \n-    <dependency>\n-      <groupId>org.slf4j</groupId>\n-      <artifactId>slf4j-api</artifactId>\n-    </dependency>\n-\n     <!-- test dependencies -->\n \n     <!-- Utilities such as WindowMatchers -->",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/pom.xml",
                "sha": "f066abf449ba1f20077fbd7c07feb362e3a9bb83",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.HashMap;\n import java.util.Map;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn.Context;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -106,19 +107,17 @@ protected final T getOrCreateStepContext(String stepName,\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@code DoFn.Context#output}\n-   * is called.\n+   * {@link Context#output(Object)} is called.\n    */\n   @Override\n   public void noteOutput(WindowedValue<?> output) {}\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@code DoFn.Context#sideOutput}\n-   * is called.\n+   * {@link Context#output(TupleTag, Object)} is called.\n    */\n   @Override\n-  public void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output) {}\n+  public void noteOutput(TupleTag<?> tag, WindowedValue<?> output) {}\n \n   /**\n    * Base class for implementations of {@link ExecutionContext.StepContext}.\n@@ -153,8 +152,8 @@ public void noteOutput(WindowedValue<?> output) {\n     }\n \n     @Override\n-    public void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output) {\n-      executionContext.noteSideOutput(tag, output);\n+    public void noteOutput(TupleTag<?> tag, WindowedValue<?> output) {\n+      executionContext.noteOutput(tag, output);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/BaseExecutionContext.java",
                "sha": "cc7b5747c243cd372cd0ff41b6cb337d41ce98b4",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 8,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java",
                "patch": "@@ -162,13 +162,13 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      context.sideOutput(tag, output);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      context.output(tag, output);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.outputWithTimestamp(tag, output, timestamp);\n     }\n \n     @Override\n@@ -255,13 +255,13 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      context.sideOutput(tag, output);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      context.output(tag, output);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.outputWithTimestamp(tag, output, timestamp);\n     }\n \n     @Override\n@@ -285,6 +285,11 @@ public PaneInfo pane() {\n       return context.pane();\n     }\n \n+    @Override\n+    public void updateWatermark(Instant watermark) {\n+      throw new UnsupportedOperationException(\"Only splittable DoFn's can use updateWatermark()\");\n+    }\n+\n     @Override\n     public BoundedWindow window() {\n       return context.window();",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnAdapters.java",
                "sha": "66ad7360fe40b23bcd41e358465a0e05e9e822e3",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "patch": "@@ -17,19 +17,23 @@\n  */\n package org.apache.beam.runners.core;\n \n+import java.util.Collection;\n import java.util.List;\n import org.apache.beam.runners.core.ExecutionContext.StepContext;\n+import org.apache.beam.runners.core.SplittableParDo.ProcessFn;\n import org.apache.beam.runners.core.StatefulDoFnRunner.CleanupTimer;\n import org.apache.beam.runners.core.StatefulDoFnRunner.StateCleaner;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n \n /**\n@@ -59,7 +63,7 @@\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       StepContext stepContext,\n       AggregatorFactory aggregatorFactory,\n       WindowingStrategy<?, ?> windowingStrategy) {\n@@ -69,7 +73,7 @@\n         sideInputReader,\n         outputManager,\n         mainOutputTag,\n-        sideOutputTags,\n+        additionalOutputTags,\n         stepContext,\n         aggregatorFactory,\n         windowingStrategy);\n@@ -86,7 +90,7 @@\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       StepContext stepContext,\n       AggregatorFactory aggregatorFactory,\n       WindowingStrategy<?, ?> windowingStrategy) {\n@@ -96,7 +100,7 @@\n         sideInputReader,\n         outputManager,\n         mainOutputTag,\n-        sideOutputTags,\n+        additionalOutputTags,\n         stepContext,\n         aggregatorFactory,\n         windowingStrategy);\n@@ -146,4 +150,32 @@\n         stateCleaner,\n         droppedDueToLateness);\n   }\n+\n+  public static <InputT, OutputT, RestrictionT>\n+  ProcessFnRunner<InputT, OutputT, RestrictionT>\n+  newProcessFnRunner(\n+      ProcessFn<InputT, OutputT, RestrictionT, ?> fn,\n+      PipelineOptions options,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader,\n+      OutputManager outputManager,\n+      TupleTag<OutputT> mainOutputTag,\n+      List<TupleTag<?>> additionalOutputTags,\n+      StepContext stepContext,\n+      AggregatorFactory aggregatorFactory,\n+      WindowingStrategy<?, ?> windowingStrategy) {\n+    return new ProcessFnRunner<>(\n+        simpleRunner(\n+            options,\n+            fn,\n+            sideInputReader,\n+            outputManager,\n+            mainOutputTag,\n+            additionalOutputTags,\n+            stepContext,\n+            aggregatorFactory,\n+            windowingStrategy),\n+        views,\n+        sideInputReader);\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "sha": "8501e7270b9eafa5f20fc8deb2c0892bd3a9bb33",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.IOException;\n import java.util.Collection;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn.Context;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -41,17 +42,15 @@\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@link org.apache.beam.sdk.transforms.DoFn.Context#output}\n-   * is called.\n+   * {@link Context#output(TupleTag, Object)} is called.\n    */\n   void noteOutput(WindowedValue<?> output);\n \n   /**\n    * Hook for subclasses to implement that will be called whenever\n-   * {@link org.apache.beam.sdk.transforms.DoFn.Context#sideOutput}\n-   * is called.\n+   * {@link Context#output(TupleTag, Object)} is called.\n    */\n-  void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output);\n+  void noteOutput(TupleTag<?> tag, WindowedValue<?> output);\n \n   /**\n    * Per-step, per-key context used for retrieving state.\n@@ -77,10 +76,10 @@\n \n     /**\n      * Hook for subclasses to implement that will be called whenever\n-     * {@link org.apache.beam.sdk.transforms.DoFn.Context#sideOutput}\n+     * {@link org.apache.beam.sdk.transforms.DoFn.Context#output}\n      * is called.\n      */\n-    void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output);\n+    void noteOutput(TupleTag<?> tag, WindowedValue<?> output);\n \n     /**\n      * Writes the given {@code PCollectionView} data to a globally accessible location.",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ExecutionContext.java",
                "sha": "ecd30c01dda6332ef8999d9c6313278d3687cabe",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java",
                "patch": "@@ -19,10 +19,10 @@\n \n import java.util.ArrayList;\n import java.util.List;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.joda.time.Instant;",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaOutputBufferDoFn.java",
                "sha": "5508b2e206d4e04272a44cc2fb502264a2110574",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "patch": "@@ -17,12 +17,12 @@\n  */\n package org.apache.beam.runners.core;\n \n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "sha": "bf48df19f980345ade1591b5b2e16a5fe900c3d2",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java",
                "patch": "@@ -18,14 +18,14 @@\n package org.apache.beam.runners.core;\n \n import java.util.Collection;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -104,9 +104,9 @@ public void outputWindowedValue(\n       }\n \n       @Override\n-      public <SideOutputT> void sideOutputWindowedValue(\n-              TupleTag<SideOutputT> tag,\n-              SideOutputT output,\n+      public <AdditionalOutputT> void outputWindowedValue(\n+              TupleTag<AdditionalOutputT> tag,\n+              AdditionalOutputT output,\n               Instant timestamp,\n               Collection<? extends BoundedWindow> windows,\n               PaneInfo pane) {",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetNewDoFn.java",
                "sha": "0cf6e2d49c3861b61687bca2825345415080d8e5",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 85,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java",
                "patch": "@@ -17,8 +17,6 @@\n  */\n package org.apache.beam.runners.core;\n \n-import static com.google.common.base.Preconditions.checkNotNull;\n-\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n@@ -38,10 +36,11 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.util.CombineFnUtil;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.ReadableState;\n+import org.apache.beam.sdk.util.state.ReadableStates;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n import org.apache.beam.sdk.util.state.StateContext;\n@@ -148,12 +147,12 @@ public InMemoryStateBinder(K key, StateContext<?> c) {\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             final CombineFn<InputT, AccumT, OutputT> combineFn) {\n-      return new InMemoryCombiningValue<K, InputT, AccumT, OutputT>(key, combineFn.<K>asKeyedFn());\n+      return new InMemoryCombiningState<K, InputT, AccumT, OutputT>(key, combineFn.<K>asKeyedFn());\n     }\n \n     @Override\n@@ -164,18 +163,18 @@ public InMemoryStateBinder(K key, StateContext<?> c) {\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValue(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n-      return new InMemoryCombiningValue<K, InputT, AccumT, OutputT>(key, combineFn);\n+      return new InMemoryCombiningState<K, InputT, AccumT, OutputT>(key, combineFn);\n     }\n \n     @Override\n-    public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+    public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n         bindKeyedCombiningValueWithContext(\n-            StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+            StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n             Coder<AccumT> accumCoder,\n             KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n       return bindKeyedCombiningValue(address, accumCoder, CombineFnUtil.bindContext(combineFn, c));\n@@ -307,25 +306,25 @@ public String toString() {\n   }\n \n   /**\n-   * An {@link InMemoryState} implementation of {@link AccumulatorCombiningState}.\n+   * An {@link InMemoryState} implementation of {@link CombiningState}.\n    */\n-  public static final class InMemoryCombiningValue<K, InputT, AccumT, OutputT>\n-      implements AccumulatorCombiningState<InputT, AccumT, OutputT>,\n-          InMemoryState<InMemoryCombiningValue<K, InputT, AccumT, OutputT>> {\n+  public static final class InMemoryCombiningState<K, InputT, AccumT, OutputT>\n+      implements CombiningState<InputT, AccumT, OutputT>,\n+          InMemoryState<InMemoryCombiningState<K, InputT, AccumT, OutputT>> {\n     private final K key;\n     private boolean isCleared = true;\n     private final KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn;\n     private AccumT accum;\n \n-    public InMemoryCombiningValue(\n+    public InMemoryCombiningState(\n         K key, KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n       this.key = key;\n       this.combineFn = combineFn;\n       accum = combineFn.createAccumulator(key);\n     }\n \n     @Override\n-    public InMemoryCombiningValue<K, InputT, AccumT, OutputT> readLater() {\n+    public InMemoryCombiningState<K, InputT, AccumT, OutputT> readLater() {\n       return this;\n     }\n \n@@ -384,9 +383,9 @@ public boolean isCleared() {\n     }\n \n     @Override\n-    public InMemoryCombiningValue<K, InputT, AccumT, OutputT> copy() {\n-      InMemoryCombiningValue<K, InputT, AccumT, OutputT> that =\n-          new InMemoryCombiningValue<>(key, combineFn);\n+    public InMemoryCombiningState<K, InputT, AccumT, OutputT> copy() {\n+      InMemoryCombiningState<K, InputT, AccumT, OutputT> that =\n+          new InMemoryCombiningState<>(key, combineFn);\n       if (!this.isCleared) {\n         that.isCleared = this.isCleared;\n         that.addAccum(accum);\n@@ -468,47 +467,22 @@ public void clear() {\n     }\n \n     @Override\n-    public boolean contains(T t) {\n-      return contents.contains(t);\n+    public ReadableState<Boolean> contains(T t) {\n+      return ReadableStates.immediate(contents.contains(t));\n     }\n \n     @Override\n-    public boolean addIfAbsent(T t) {\n-      return contents.add(t);\n+    public ReadableState<Boolean> addIfAbsent(T t) {\n+      boolean alreadyContained = contents.contains(t);\n+      contents.add(t);\n+      return ReadableStates.immediate(!alreadyContained);\n     }\n \n     @Override\n     public void remove(T t) {\n       contents.remove(t);\n     }\n \n-    @Override\n-    public SetState<T> readLater(Iterable<T> elements) {\n-      return this;\n-    }\n-\n-    @Override\n-    public boolean containsAny(Iterable<T> elements) {\n-      elements = checkNotNull(elements);\n-      for (T t : elements) {\n-        if (contents.contains(t)) {\n-          return true;\n-        }\n-      }\n-      return false;\n-    }\n-\n-    @Override\n-    public boolean containsAll(Iterable<T> elements) {\n-      elements = checkNotNull(elements);\n-      for (T t : elements) {\n-        if (!contents.contains(t)) {\n-          return false;\n-        }\n-      }\n-      return true;\n-    }\n-\n     @Override\n     public InMemorySet<T> readLater() {\n       return this;\n@@ -565,8 +539,8 @@ public void clear() {\n     }\n \n     @Override\n-    public V get(K key) {\n-      return contents.get(key);\n+    public ReadableState<V> get(K key) {\n+      return ReadableStates.immediate(contents.get(key));\n     }\n \n     @Override\n@@ -575,13 +549,13 @@ public void put(K key, V value) {\n     }\n \n     @Override\n-    public V putIfAbsent(K key, V value) {\n+    public ReadableState<V> putIfAbsent(K key, V value) {\n       V v = contents.get(key);\n       if (v == null) {\n         v = contents.put(key, value);\n       }\n \n-      return v;\n+      return ReadableStates.immediate(v);\n     }\n \n     @Override\n@@ -590,42 +564,18 @@ public void remove(K key) {\n     }\n \n     @Override\n-    public Iterable<V> get(Iterable<K> keys) {\n-      List<V> values = new ArrayList<>();\n-      for (K k : keys) {\n-        values.add(contents.get(k));\n-      }\n-      return values;\n+    public ReadableState<Iterable<K>> keys() {\n+      return ReadableStates.immediate((Iterable<K>) contents.keySet());\n     }\n \n     @Override\n-    public MapState<K, V> getLater(K k) {\n-      return this;\n-    }\n-\n-    @Override\n-    public MapState<K, V> getLater(Iterable<K> keys) {\n-      return this;\n-    }\n-\n-    @Override\n-    public Iterable<K> keys() {\n-      return contents.keySet();\n-    }\n-\n-    @Override\n-    public Iterable<V> values() {\n-      return contents.values();\n-    }\n-\n-    @Override\n-    public MapState<K, V> iterateLater() {\n-      return this;\n+    public ReadableState<Iterable<V>> values() {\n+      return ReadableStates.immediate((Iterable<V>) contents.values());\n     }\n \n     @Override\n-    public Iterable<Map.Entry<K, V>> iterate() {\n-      return contents.entrySet();\n+    public ReadableState<Iterable<Map.Entry<K, V>>> entries() {\n+      return ReadableStates.immediate((Iterable<Map.Entry<K, V>>) contents.entrySet());\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/InMemoryStateInternals.java",
                "sha": "55b7fc2967bcad59fbd89fd88524ba224ed56abb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java",
                "patch": "@@ -22,7 +22,7 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.ReadableState;\n \n /**\n@@ -113,7 +113,7 @@ public void onMerge(MergingStateAccessor<K, W> context) {\n   private static class GeneralNonEmptyPanes<K, W extends BoundedWindow>\n       extends NonEmptyPanes<K, W> {\n \n-    private static final StateTag<Object, AccumulatorCombiningState<Long, long[], Long>>\n+    private static final StateTag<Object, CombiningState<Long, long[], Long>>\n         PANE_ADDITIONS_TAG =\n         StateTags.makeSystemTagInternal(StateTags.combiningValueFromInputInternal(\n             \"count\", VarLongCoder.of(), Sum.ofLongs()));",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/NonEmptyPanes.java",
                "sha": "3e875c2c07b81771b7d8c5f2e7cf5817b1c69e5a",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 27,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java",
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.beam.sdk.transforms.display.HasDisplayData;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n@@ -135,16 +136,15 @@\n     public abstract void outputWithTimestamp(OutputT output, Instant timestamp);\n \n     /**\n-     * Adds the given element to the side output {@code PCollection} with the\n+     * Adds the given element to the output {@code PCollection} with the\n      * given tag.\n      *\n-     * <p>Once passed to {@code sideOutput} the element should not be modified\n+     * <p>Once passed to {@code output} the element should not be modified\n      * in any way.\n      *\n-     * <p>The caller of {@code ParDo} uses {@link ParDo#withOutputTags withOutputTags} to\n-     * specify the tags of side outputs that it consumes. Non-consumed side\n-     * outputs, e.g., outputs for monitoring purposes only, don't necessarily\n-     * need to be specified.\n+     * <p>The caller of {@code ParDo} uses {@link ParDo.SingleOutput#withOutputTags withOutputTags}\n+     * to specify the tags of outputs that it consumes. Outputs that are not consumed, e.g., outputs\n+     * for monitoring purposes only, don't necessarily need to be specified.\n      *\n      * <p>The output element will have the same timestamp and be in the same\n      * windows as the input element passed to {@link OldDoFn#processElement processElement}.\n@@ -157,34 +157,29 @@\n      * to access any information about the input element. The output element\n      * will have a timestamp of negative infinity.\n      *\n-     * @see ParDo#withOutputTags\n+     * @see ParDo.SingleOutput#withOutputTags\n      */\n-    public abstract <T> void sideOutput(TupleTag<T> tag, T output);\n+    public abstract <T> void output(TupleTag<T> tag, T output);\n \n     /**\n-     * Adds the given element to the specified side output {@code PCollection},\n-     * with the given timestamp.\n+     * Adds the given element to the specified output {@code PCollection}, with the given timestamp.\n      *\n-     * <p>Once passed to {@code sideOutputWithTimestamp} the element should not be\n-     * modified in any way.\n+     * <p>Once passed to {@code outputWithTimestamp} the element should not be modified in any way.\n      *\n-     * <p>If invoked from {@link OldDoFn#processElement processElement}, the timestamp\n-     * must not be older than the input element's timestamp minus\n-     * {@link OldDoFn#getAllowedTimestampSkew getAllowedTimestampSkew}.  The output element will\n-     * be in the same windows as the input element.\n+     * <p>If invoked from {@link OldDoFn#processElement processElement}, the timestamp must not be\n+     * older than the input element's timestamp minus {@link OldDoFn#getAllowedTimestampSkew\n+     * getAllowedTimestampSkew}. The output element will be in the same windows as the input\n+     * element.\n      *\n      * <p>If invoked from {@link #startBundle startBundle} or {@link #finishBundle finishBundle},\n-     * this will attempt to use the\n-     * {@link org.apache.beam.sdk.transforms.windowing.WindowFn}\n-     * of the input {@code PCollection} to determine what windows the element\n-     * should be in, throwing an exception if the {@code WindowFn} attempts\n-     * to access any information about the input element except for the\n-     * timestamp.\n+     * this will attempt to use the {@link org.apache.beam.sdk.transforms.windowing.WindowFn} of the\n+     * input {@code PCollection} to determine what windows the element should be in, throwing an\n+     * exception if the {@code WindowFn} attempts to access any information about the input element\n+     * except for the timestamp.\n      *\n-     * @see ParDo#withOutputTags\n+     * @see ParDo.SingleOutput#withOutputTags\n      */\n-    public abstract <T> void sideOutputWithTimestamp(\n-        TupleTag<T> tag, T output, Instant timestamp);\n+    public abstract <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp);\n \n     /**\n      * Creates an {@link Aggregator} in the {@link OldDoFn} context with the\n@@ -247,11 +242,11 @@ protected final void setupDelegateAggregators() {\n      * window of the main input element.\n      *\n      * <p>See\n-     * {@link org.apache.beam.sdk.transforms.windowing.WindowFn#getSideInputWindow}\n+     * {@link WindowMappingFn#getSideInputWindow}\n      * for how this corresponding window is determined.\n      *\n      * @throws IllegalArgumentException if this is not a side input\n-     * @see ParDo#withSideInputs\n+     * @see ParDo.SingleOutput#withSideInputs\n      */\n     public abstract <T> T sideInput(PCollectionView<T> view);\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OldDoFn.java",
                "sha": "323edf9419e526e677c51731b18255b996528891",
                "status": "modified"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java",
                "changes": 136,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 70,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java",
                "patch": "@@ -97,70 +97,57 @@ public Result invokeProcessElement(\n       final WindowedValue<InputT> element,\n       final TrackerT tracker) {\n     final ProcessContext processContext = new ProcessContext(element, tracker);\n-    DoFn.ProcessContinuation cont =\n-        invoker.invokeProcessElement(\n-            new DoFnInvoker.ArgumentProvider<InputT, OutputT>() {\n-              @Override\n-              public DoFn<InputT, OutputT>.ProcessContext processContext(\n-                  DoFn<InputT, OutputT> doFn) {\n-                return processContext;\n-              }\n-\n-              @Override\n-              public RestrictionTracker<?> restrictionTracker() {\n-                return tracker;\n-              }\n-\n-              // Unsupported methods below.\n-\n-              @Override\n-              public BoundedWindow window() {\n-                throw new UnsupportedOperationException(\n-                    \"Access to window of the element not supported in Splittable DoFn\");\n-              }\n-\n-              @Override\n-              public DoFn<InputT, OutputT>.Context context(DoFn<InputT, OutputT> doFn) {\n-                throw new IllegalStateException(\n-                    \"Should not access context() from @\"\n-                        + DoFn.ProcessElement.class.getSimpleName());\n-              }\n-\n-              @Override\n-              public DoFn<InputT, OutputT>.OnTimerContext onTimerContext(\n-                  DoFn<InputT, OutputT> doFn) {\n-                throw new UnsupportedOperationException(\n-                    \"Access to timers not supported in Splittable DoFn\");\n-              }\n-\n-              @Override\n-              public State state(String stateId) {\n-                throw new UnsupportedOperationException(\n-                    \"Access to state not supported in Splittable DoFn\");\n-              }\n-\n-              @Override\n-              public Timer timer(String timerId) {\n-                throw new UnsupportedOperationException(\n-                    \"Access to timers not supported in Splittable DoFn\");\n-              }\n-            });\n-    RestrictionT residual;\n-    RestrictionT forcedCheckpoint = processContext.extractCheckpoint();\n-    if (cont.shouldResume()) {\n-      if (forcedCheckpoint == null) {\n-        // If no checkpoint was forced, the call returned voluntarily (i.e. all tryClaim() calls\n-        // succeeded) - but we still need to have a checkpoint to resume from.\n-        residual = tracker.checkpoint();\n-      } else {\n-        // A checkpoint was forced - i.e. the call probably (but not guaranteed) returned because of\n-        // a failed tryClaim() call.\n-        residual = forcedCheckpoint;\n-      }\n-    } else {\n-      residual = null;\n-    }\n-    return new Result(residual, cont);\n+    invoker.invokeProcessElement(\n+        new DoFnInvoker.ArgumentProvider<InputT, OutputT>() {\n+          @Override\n+          public DoFn<InputT, OutputT>.ProcessContext processContext(\n+              DoFn<InputT, OutputT> doFn) {\n+            return processContext;\n+          }\n+\n+          @Override\n+          public RestrictionTracker<?> restrictionTracker() {\n+            return tracker;\n+          }\n+\n+          // Unsupported methods below.\n+\n+          @Override\n+          public BoundedWindow window() {\n+            throw new UnsupportedOperationException(\n+                \"Access to window of the element not supported in Splittable DoFn\");\n+          }\n+\n+          @Override\n+          public DoFn<InputT, OutputT>.Context context(DoFn<InputT, OutputT> doFn) {\n+            throw new IllegalStateException(\n+                \"Should not access context() from @\"\n+                    + DoFn.ProcessElement.class.getSimpleName());\n+          }\n+\n+          @Override\n+          public DoFn<InputT, OutputT>.OnTimerContext onTimerContext(\n+              DoFn<InputT, OutputT> doFn) {\n+            throw new UnsupportedOperationException(\n+                \"Access to timers not supported in Splittable DoFn\");\n+          }\n+\n+          @Override\n+          public State state(String stateId) {\n+            throw new UnsupportedOperationException(\n+                \"Access to state not supported in Splittable DoFn\");\n+          }\n+\n+          @Override\n+          public Timer timer(String timerId) {\n+            throw new UnsupportedOperationException(\n+                \"Access to timers not supported in Splittable DoFn\");\n+          }\n+        });\n+\n+    tracker.checkDone();\n+    return new Result(\n+        processContext.extractCheckpoint(), processContext.getLastReportedWatermark());\n   }\n \n   private class ProcessContext extends DoFn<InputT, OutputT>.ProcessContext {\n@@ -176,6 +163,7 @@ public Timer timer(String timerId) {\n     private RestrictionT checkpoint;\n     // A handle on the scheduled action to take a checkpoint.\n     private Future<?> scheduledCheckpoint;\n+    private Instant lastReportedWatermark;\n \n     public ProcessContext(WindowedValue<InputT> element, TrackerT tracker) {\n       fn.super();\n@@ -226,8 +214,7 @@ public InputT element() {\n     public <T> T sideInput(PCollectionView<T> view) {\n       return sideInputReader.get(\n           view,\n-          view.getWindowingStrategyInternal()\n-              .getWindowFn()\n+          view.getWindowMappingFn()\n               .getSideInputWindow(Iterables.getOnlyElement(element.getWindows())));\n     }\n \n@@ -241,6 +228,15 @@ public PaneInfo pane() {\n       return element.getPane();\n     }\n \n+    @Override\n+    public synchronized void updateWatermark(Instant watermark) {\n+      lastReportedWatermark = watermark;\n+    }\n+\n+    public synchronized Instant getLastReportedWatermark() {\n+      return lastReportedWatermark;\n+    }\n+\n     @Override\n     public PipelineOptions getPipelineOptions() {\n       return pipelineOptions;\n@@ -258,13 +254,13 @@ public void outputWithTimestamp(OutputT value, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T value) {\n-      sideOutputWithTimestamp(tag, value, element.getTimestamp());\n+    public <T> void output(TupleTag<T> tag, T value) {\n+      outputWithTimestamp(tag, value, element.getTimestamp());\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T value, Instant timestamp) {\n-      output.sideOutputWindowedValue(\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T value, Instant timestamp) {\n+      output.outputWindowedValue(\n           tag, value, timestamp, element.getWindows(), element.getPane());\n       noteOutput();\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvoker.java",
                "sha": "d132af672fb2e41e54fa173b09f99768f2f126bf",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java",
                "patch": "@@ -25,7 +25,7 @@\n \n /**\n  * An object that can output a value with all of its windowing information to the main output or\n- * a side output.\n+ * any tagged output.\n  */\n public interface OutputWindowedValue<OutputT> {\n   /** Outputs a value with windowing information to the main output. */\n@@ -35,10 +35,10 @@ void outputWindowedValue(\n       Collection<? extends BoundedWindow> windows,\n       PaneInfo pane);\n \n-  /** Outputs a value with windowing information to a side output. */\n-  <SideOutputT> void sideOutputWindowedValue(\n-      TupleTag<SideOutputT> tag,\n-      SideOutputT output,\n+  /** Outputs a value with windowing information to a tagged output. */\n+  <AdditionalOutputT> void outputWindowedValue(\n+      TupleTag<AdditionalOutputT> tag,\n+      AdditionalOutputT output,\n       Instant timestamp,\n       Collection<? extends BoundedWindow> windows,\n       PaneInfo pane);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/OutputWindowedValue.java",
                "sha": "35d6737fa561d99742d2dec82e7bebfb2fa4dc11",
                "status": "modified"
            },
            {
                "additions": 127,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java",
                "changes": 127,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java",
                "patch": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.core;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.runners.core.SplittableParDo.ProcessFn;\n+\n+import com.google.common.collect.Iterables;\n+import java.util.Collection;\n+import java.util.Collections;\n+import org.apache.beam.runners.core.StateNamespaces.WindowNamespace;\n+import org.apache.beam.runners.core.TimerInternals.TimerData;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.joda.time.Instant;\n+\n+/** Runs a {@link ProcessFn} by constructing the appropriate contexts and passing them in. */\n+public class ProcessFnRunner<InputT, OutputT, RestrictionT>\n+    implements PushbackSideInputDoFnRunner<\n+        KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT> {\n+  private final DoFnRunner<\n+          KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+      underlying;\n+  private final Collection<PCollectionView<?>> views;\n+  private final ReadyCheckingSideInputReader sideInputReader;\n+\n+  ProcessFnRunner(\n+      DoFnRunner<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+          underlying,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader) {\n+    this.underlying = underlying;\n+    this.views = views;\n+    this.sideInputReader = sideInputReader;\n+  }\n+\n+  @Override\n+  public void startBundle() {\n+    underlying.startBundle();\n+  }\n+\n+  @Override\n+  public Iterable<WindowedValue<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>>>\n+      processElementInReadyWindows(\n+          WindowedValue<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>>\n+              windowedKWI) {\n+    checkTrivialOuterWindows(windowedKWI);\n+    BoundedWindow window = getUnderlyingWindow(windowedKWI.getValue());\n+    if (!isReady(window)) {\n+      return Collections.singletonList(windowedKWI);\n+    }\n+    underlying.processElement(windowedKWI);\n+    return Collections.emptyList();\n+  }\n+\n+  @Override\n+  public void finishBundle() {\n+    underlying.finishBundle();\n+  }\n+\n+  @Override\n+  public void onTimer(\n+      String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain) {\n+    throw new UnsupportedOperationException(\"User timers unsupported in ProcessFn\");\n+  }\n+\n+  private static <T> void checkTrivialOuterWindows(\n+      WindowedValue<KeyedWorkItem<String, T>> windowedKWI) {\n+    // In practice it will be in 0 or 1 windows (ValueInEmptyWindows or ValueInGlobalWindow)\n+    Collection<? extends BoundedWindow> outerWindows = windowedKWI.getWindows();\n+    if (!outerWindows.isEmpty()) {\n+      checkArgument(\n+          outerWindows.size() == 1,\n+          \"The KeyedWorkItem itself must not be in multiple windows, but was in: %s\",\n+          outerWindows);\n+      BoundedWindow onlyWindow = Iterables.getOnlyElement(outerWindows);\n+      checkArgument(\n+          onlyWindow instanceof GlobalWindow,\n+          \"KeyedWorkItem must be in the Global window, but was in: %s\",\n+          onlyWindow);\n+    }\n+  }\n+\n+  private static <T> BoundedWindow getUnderlyingWindow(KeyedWorkItem<String, T> kwi) {\n+    if (Iterables.isEmpty(kwi.elementsIterable())) {\n+      // ProcessFn sets only a single timer.\n+      TimerData timer = Iterables.getOnlyElement(kwi.timersIterable());\n+      return ((WindowNamespace) timer.getNamespace()).getWindow();\n+    } else {\n+      // KWI must have a single element in elementsIterable, because it follows a GBK by a\n+      // uniquely generated key.\n+      // Additionally, windows must be exploded before GBKIntoKeyedWorkItems, so there's also\n+      // only a single window.\n+      WindowedValue<T> value = Iterables.getOnlyElement(kwi.elementsIterable());\n+      return Iterables.getOnlyElement(value.getWindows());\n+    }\n+  }\n+\n+  private boolean isReady(BoundedWindow mainInputWindow) {\n+    for (PCollectionView<?> view : views) {\n+      BoundedWindow sideInputWindow = view.getWindowMappingFn().getSideInputWindow(mainInputWindow);\n+      if (!sideInputReader.isReady(view, sideInputWindow)) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ProcessFnRunner.java",
                "sha": "3ae3f5068c4607d24ed31f90f02b853871fe522b",
                "status": "added"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "changes": 106,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 92,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "patch": "@@ -17,113 +17,35 @@\n  */\n package org.apache.beam.runners.core;\n \n-import com.google.common.collect.ImmutableList;\n-import com.google.common.collect.Iterables;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.Set;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.values.PCollectionView;\n import org.joda.time.Instant;\n \n /**\n- * A {@link DoFnRunner} that can refuse to process elements that are not ready, instead returning\n- * them via the {@link #processElementInReadyWindows(WindowedValue)}.\n+ * Interface for runners of {@link DoFn}'s that support pushback when reading side inputs,\n+ * i.e. return elements that could not be processed because they require reading a side input\n+ * window that is not ready.\n  */\n-public class PushbackSideInputDoFnRunner<InputT, OutputT> implements DoFnRunner<InputT, OutputT> {\n-  private final DoFnRunner<InputT, OutputT> underlying;\n-  private final Collection<PCollectionView<?>> views;\n-  private final ReadyCheckingSideInputReader sideInputReader;\n-\n-  private Set<BoundedWindow> notReadyWindows;\n-\n-  public static <InputT, OutputT> PushbackSideInputDoFnRunner<InputT, OutputT> create(\n-      DoFnRunner<InputT, OutputT> underlying,\n-      Collection<PCollectionView<?>> views,\n-      ReadyCheckingSideInputReader sideInputReader) {\n-    return new PushbackSideInputDoFnRunner<>(underlying, views, sideInputReader);\n-  }\n-\n-  private PushbackSideInputDoFnRunner(\n-      DoFnRunner<InputT, OutputT> underlying,\n-      Collection<PCollectionView<?>> views,\n-      ReadyCheckingSideInputReader sideInputReader) {\n-    this.underlying = underlying;\n-    this.views = views;\n-    this.sideInputReader = sideInputReader;\n-  }\n-\n-  @Override\n-  public void startBundle() {\n-    notReadyWindows = new HashSet<>();\n-    underlying.startBundle();\n-  }\n+public interface PushbackSideInputDoFnRunner<InputT, OutputT> {\n+  /** Calls the underlying {@link DoFn.StartBundle} method. */\n+  void startBundle();\n \n   /**\n-   * Call the underlying {@link DoFnRunner#processElement(WindowedValue)} for the provided element\n+   * Call the underlying {@link DoFn.ProcessElement} method for the provided element\n    * for each window the element is in that is ready.\n    *\n    * @param elem the element to process in all ready windows\n    * @return each element that could not be processed because it requires a side input window\n    * that is not ready.\n    */\n-  public Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem) {\n-    if (views.isEmpty()) {\n-      // When there are no side inputs, we can preserve the compressed representation.\n-      processElement(elem);\n-      return Collections.emptyList();\n-    }\n-    ImmutableList.Builder<WindowedValue<InputT>> pushedBack = ImmutableList.builder();\n-    for (WindowedValue<InputT> windowElem : elem.explodeWindows()) {\n-      BoundedWindow mainInputWindow = Iterables.getOnlyElement(windowElem.getWindows());\n-      if (isReady(mainInputWindow)) {\n-        // When there are any side inputs, we have to process the element in each window\n-        // individually, to disambiguate access to per-window side inputs.\n-        processElement(windowElem);\n-      } else {\n-        notReadyWindows.add(mainInputWindow);\n-        pushedBack.add(windowElem);\n-      }\n-    }\n-    return pushedBack.build();\n-  }\n-\n-  private boolean isReady(BoundedWindow mainInputWindow) {\n-    if (notReadyWindows.contains(mainInputWindow)) {\n-      return false;\n-    }\n-    for (PCollectionView<?> view : views) {\n-      BoundedWindow sideInputWindow =\n-          view.getWindowingStrategyInternal().getWindowFn().getSideInputWindow(mainInputWindow);\n-      if (!sideInputReader.isReady(view, sideInputWindow)) {\n-        return false;\n-      }\n-    }\n-    return true;\n-  }\n+  Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem);\n \n-  @Override\n-  public void processElement(WindowedValue<InputT> elem) {\n-    underlying.processElement(elem);\n-  }\n+  /** Calls the underlying {@link DoFn.OnTimer} method. */\n+  void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+               TimeDomain timeDomain);\n \n-  @Override\n-  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n-      TimeDomain timeDomain) {\n-    underlying.onTimer(timerId, window, timestamp, timeDomain);\n-  }\n-\n-  /**\n-   * Call the underlying {@link DoFnRunner#finishBundle()}.\n-   */\n-  @Override\n-  public void finishBundle() {\n-    notReadyWindows = null;\n-    underlying.finishBundle();\n-  }\n+  /** Calls the underlying {@link DoFn.FinishBundle} method. */\n+  void finishBundle();\n }\n-",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "sha": "bab1dc7317fc7cc9f043fa8562c738c724d8a98f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "patch": "@@ -514,8 +514,7 @@ public PipelineOptions getPipelineOptions() {\n         public <T> T sideInput(PCollectionView<T> view) {\n           return sideInputReader.get(\n               view,\n-              view.getWindowingStrategyInternal()\n-                  .getWindowFn()\n+              view.getWindowMappingFn()\n                   .getSideInputWindow(mainInputWindow));\n         }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "sha": "8493474c8d285d1f7d2bdd95102e13c5965af33f",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 9,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java",
                "patch": "@@ -31,7 +31,7 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.ValueState;\n import org.apache.beam.sdk.values.PCollectionView;\n \n@@ -71,10 +71,10 @@\n       PCollectionView<?>,\n       StateTag<\n           Object,\n-          AccumulatorCombiningState<\n-              BoundedWindow,\n-              Set<BoundedWindow>,\n-              Set<BoundedWindow>>>> availableWindowsTags;\n+          CombiningState<\n+                        BoundedWindow,\n+                        Set<BoundedWindow>,\n+                        Set<BoundedWindow>>>> availableWindowsTags;\n \n   /**\n    * State tag for the actual contents of each side input per window.\n@@ -106,10 +106,10 @@ public SideInputHandler(\n \n       StateTag<\n           Object,\n-          AccumulatorCombiningState<\n-              BoundedWindow,\n-              Set<BoundedWindow>,\n-              Set<BoundedWindow>>> availableTag = StateTags.combiningValue(\n+          CombiningState<\n+                        BoundedWindow,\n+                        Set<BoundedWindow>,\n+                        Set<BoundedWindow>>> availableTag = StateTags.combiningValue(\n           \"side-input-available-windows-\" + sideInput.getTagInternal().getId(),\n           SetCoder.of(windowCoder),\n           new WindowSetCombineFn());",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SideInputHandler.java",
                "sha": "26e920abd1bc51184eb2d58807a8160733daa85a",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 31,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Sets;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Set;\n@@ -105,7 +106,7 @@ public SimpleDoFnRunner(\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       StepContext stepContext,\n       AggregatorFactory aggregatorFactory,\n       WindowingStrategy<?, ?> windowingStrategy) {\n@@ -132,7 +133,7 @@ public SimpleDoFnRunner(\n             sideInputReader,\n             outputManager,\n             mainOutputTag,\n-            sideOutputTags,\n+            additionalOutputTags,\n             stepContext,\n             aggregatorFactory,\n             windowingStrategy.getWindowFn());\n@@ -256,7 +257,7 @@ public DoFnContext(\n         SideInputReader sideInputReader,\n         OutputManager outputManager,\n         TupleTag<OutputT> mainOutputTag,\n-        List<TupleTag<?>> sideOutputTags,\n+        List<TupleTag<?>> additionalOutputTags,\n         StepContext stepContext,\n         AggregatorFactory aggregatorFactory,\n         WindowFn<?, ?> windowFn) {\n@@ -269,8 +270,8 @@ public DoFnContext(\n       this.outputTags = Sets.newHashSet();\n \n       outputTags.add(mainOutputTag);\n-      for (TupleTag<?> sideOutputTag : sideOutputTags) {\n-        outputTags.add(sideOutputTag);\n+      for (TupleTag<?> additionalOutputTag : additionalOutputTags) {\n+        outputTags.add(additionalOutputTag);\n       }\n \n       this.stepContext = stepContext;\n@@ -354,35 +355,35 @@ void outputWindowedValue(WindowedValue<OutputT> windowedElem) {\n       }\n     }\n \n-    private <T> void sideOutputWindowedValue(\n+    private <T> void outputWindowedValue(\n         TupleTag<T> tag,\n         T output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      sideOutputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n+      outputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n     }\n \n-    private <T> void sideOutputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n+    private <T> void outputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n       if (!outputTags.contains(tag)) {\n         // This tag wasn't declared nor was it seen before during this execution.\n         // Thus, this must be a new, undeclared and unconsumed output.\n         // To prevent likely user errors, enforce the limit on the number of side\n         // outputs.\n         if (outputTags.size() >= MAX_SIDE_OUTPUTS) {\n           throw new IllegalArgumentException(\n-              \"the number of side outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n+              \"the number of outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n         }\n         outputTags.add(tag);\n       }\n \n       outputManager.output(tag, windowedElem);\n       if (stepContext != null) {\n-        stepContext.noteSideOutput(tag, windowedElem);\n+        stepContext.noteOutput(tag, windowedElem);\n       }\n     }\n \n-    // Following implementations of output, outputWithTimestamp, and sideOutput\n+    // Following implementations of output, outputWithTimestamp, and output\n     // are only accessible in DoFn.startBundle and DoFn.finishBundle, and will be shadowed by\n     // ProcessContext's versions in DoFn.processElement.\n     @Override\n@@ -396,15 +397,15 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutput cannot be null\");\n-      sideOutputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"TupleTag passed to output cannot be null\");\n+      outputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutputWithTimestamp cannot be null\");\n-      sideOutputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"TupleTag passed to outputWithTimestamp cannot be null\");\n+      outputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n@@ -532,14 +533,19 @@ public InputT element() {\n         }\n       }\n       return context.sideInput(\n-          view, view.getWindowingStrategyInternal().getWindowFn().getSideInputWindow(window));\n+          view, view.getWindowMappingFn().getSideInputWindow(window));\n     }\n \n     @Override\n     public PaneInfo pane() {\n       return windowedValue.getPane();\n     }\n \n+    @Override\n+    public void updateWatermark(Instant watermark) {\n+      throw new UnsupportedOperationException(\"Only splittable DoFn's can use updateWatermark()\");\n+    }\n+\n     @Override\n     public void output(OutputT output) {\n       context.outputWindowedValue(windowedValue.withValue(output));\n@@ -553,16 +559,16 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"Tag passed to sideOutput cannot be null\");\n-      context.sideOutputWindowedValue(tag, windowedValue.withValue(output));\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"Tag passed to output cannot be null\");\n+      context.outputWindowedValue(tag, windowedValue.withValue(output));\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"Tag passed to sideOutputWithTimestamp cannot be null\");\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"Tag passed to outputWithTimestamp cannot be null\");\n       checkTimestamp(timestamp);\n-      context.sideOutputWindowedValue(\n+      context.outputWindowedValue(\n           tag, output, timestamp, windowedValue.getWindows(), windowedValue.getPane());\n     }\n \n@@ -575,8 +581,12 @@ public Instant timestamp() {\n       return windowedValue.getWindows();\n     }\n \n+    @SuppressWarnings(\"deprecation\") // Allowed Skew is deprecated for users, but must be respected\n     private void checkTimestamp(Instant timestamp) {\n-      if (timestamp.isBefore(windowedValue.getTimestamp().minus(fn.getAllowedTimestampSkew()))) {\n+      // The documentation of getAllowedTimestampSkew explicitly permits Long.MAX_VALUE to be used\n+      // for infinite skew. Defend against underflow in that case for timestamps before the epoch\n+      if (fn.getAllowedTimestampSkew().getMillis() != Long.MAX_VALUE\n+          && timestamp.isBefore(windowedValue.getTimestamp().minus(fn.getAllowedTimestampSkew()))) {\n         throw new IllegalArgumentException(\n             String.format(\n                 \"Cannot output with timestamp %s. Output timestamps must be no earlier than the \"\n@@ -766,22 +776,26 @@ public PipelineOptions getPipelineOptions() {\n \n     @Override\n     public void output(OutputT output) {\n-      context.outputWithTimestamp(output, timestamp);\n+      context.outputWindowedValue(\n+          output, timestamp(), Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override\n     public void outputWithTimestamp(OutputT output, Instant timestamp) {\n-      context.outputWithTimestamp(output, timestamp);\n+      context.outputWindowedValue(\n+          output, timestamp, Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      context.outputWindowedValue(\n+          tag, output, timestamp, Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.outputWindowedValue(\n+          tag, output, timestamp, Collections.singleton(window()), PaneInfo.NO_FIRING);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "sha": "141bf20ee6e41606a9cf96e93bce2d6378cc9797",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 30,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "patch": "@@ -60,19 +60,24 @@\n   /** The context used for running the {@link OldDoFn}. */\n   private final DoFnContext<InputT, OutputT> context;\n \n-  public SimpleOldDoFnRunner(PipelineOptions options, OldDoFn<InputT, OutputT> fn,\n+  public SimpleOldDoFnRunner(\n+      PipelineOptions options,\n+      OldDoFn<InputT, OutputT> fn,\n       SideInputReader sideInputReader,\n       OutputManager outputManager,\n-      TupleTag<OutputT> mainOutputTag, List<TupleTag<?>> sideOutputTags, StepContext stepContext,\n-      AggregatorFactory aggregatorFactory, WindowingStrategy<?, ?> windowingStrategy) {\n+      TupleTag<OutputT> mainOutputTag,\n+      List<TupleTag<?>> additionalOutputTags,\n+      StepContext stepContext,\n+      AggregatorFactory aggregatorFactory,\n+      WindowingStrategy<?, ?> windowingStrategy) {\n     this.fn = fn;\n     this.context = new DoFnContext<>(\n         options,\n         fn,\n         sideInputReader,\n         outputManager,\n         mainOutputTag,\n-        sideOutputTags,\n+        additionalOutputTags,\n         stepContext,\n         aggregatorFactory,\n         windowingStrategy == null ? null : windowingStrategy.getWindowFn());\n@@ -177,7 +182,7 @@ public DoFnContext(PipelineOptions options,\n                        SideInputReader sideInputReader,\n                        OutputManager outputManager,\n                        TupleTag<OutputT> mainOutputTag,\n-                       List<TupleTag<?>> sideOutputTags,\n+                       List<TupleTag<?>> additionalOutputTags,\n                        StepContext stepContext,\n                        AggregatorFactory aggregatorFactory,\n                        WindowFn<?, ?> windowFn) {\n@@ -190,8 +195,8 @@ public DoFnContext(PipelineOptions options,\n       this.outputTags = Sets.newHashSet();\n \n       outputTags.add(mainOutputTag);\n-      for (TupleTag<?> sideOutputTag : sideOutputTags) {\n-        outputTags.add(sideOutputTag);\n+      for (TupleTag<?> additionalOutputTag : additionalOutputTags) {\n+        outputTags.add(additionalOutputTag);\n       }\n \n       this.stepContext = stepContext;\n@@ -273,34 +278,34 @@ void outputWindowedValue(WindowedValue<OutputT> windowedElem) {\n       }\n     }\n \n-    private <T> void sideOutputWindowedValue(TupleTag<T> tag,\n+    private <T> void outputWindowedValue(TupleTag<T> tag,\n                                                T output,\n                                                Instant timestamp,\n                                                Collection<? extends BoundedWindow> windows,\n                                                PaneInfo pane) {\n-      sideOutputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n+      outputWindowedValue(tag, makeWindowedValue(output, timestamp, windows, pane));\n     }\n \n-    private <T> void sideOutputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n+    private <T> void outputWindowedValue(TupleTag<T> tag, WindowedValue<T> windowedElem) {\n       if (!outputTags.contains(tag)) {\n         // This tag wasn't declared nor was it seen before during this execution.\n         // Thus, this must be a new, undeclared and unconsumed output.\n         // To prevent likely user errors, enforce the limit on the number of side\n         // outputs.\n         if (outputTags.size() >= MAX_SIDE_OUTPUTS) {\n           throw new IllegalArgumentException(\n-              \"the number of side outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n+              \"the number of outputs has exceeded a limit of \" + MAX_SIDE_OUTPUTS);\n         }\n         outputTags.add(tag);\n       }\n \n       outputManager.output(tag, windowedElem);\n       if (stepContext != null) {\n-        stepContext.noteSideOutput(tag, windowedElem);\n+        stepContext.noteOutput(tag, windowedElem);\n       }\n     }\n \n-    // Following implementations of output, outputWithTimestamp, and sideOutput\n+    // Following implementations of output, outputWithTimestamp, and output\n     // are only accessible in OldDoFn.startBundle and OldDoFn.finishBundle, and will be shadowed by\n     // ProcessContext's versions in OldDoFn.processElement.\n     @Override\n@@ -314,15 +319,15 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutput cannot be null\");\n-      sideOutputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"TupleTag passed to output cannot be null\");\n+      outputWindowedValue(tag, output, null, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"TupleTag passed to sideOutputWithTimestamp cannot be null\");\n-      sideOutputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"TupleTag passed to outputWithTimestamp cannot be null\");\n+      outputWindowedValue(tag, output, timestamp, null, PaneInfo.NO_FIRING);\n     }\n \n     @Override\n@@ -389,7 +394,7 @@ public InputT element() {\n         }\n       }\n       return context.sideInput(\n-          view, view.getWindowingStrategyInternal().getWindowFn().getSideInputWindow(window));\n+          view, view.getWindowMappingFn().getSideInputWindow(window));\n     }\n \n     @Override\n@@ -428,16 +433,16 @@ void outputWindowedValue(\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n-      checkNotNull(tag, \"Tag passed to sideOutput cannot be null\");\n-      context.sideOutputWindowedValue(tag, windowedValue.withValue(output));\n+    public <T> void output(TupleTag<T> tag, T output) {\n+      checkNotNull(tag, \"Tag passed to output cannot be null\");\n+      context.outputWindowedValue(tag, windowedValue.withValue(output));\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-      checkNotNull(tag, \"Tag passed to sideOutputWithTimestamp cannot be null\");\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      checkNotNull(tag, \"Tag passed to outputWithTimestamp cannot be null\");\n       checkTimestamp(timestamp);\n-      context.sideOutputWindowedValue(\n+      context.outputWindowedValue(\n           tag, output, timestamp, windowedValue.getWindows(), windowedValue.getPane());\n     }\n \n@@ -471,13 +476,13 @@ public void outputWindowedValue(OutputT output, Instant timestamp,\n         }\n \n         @Override\n-        public <SideOutputT> void sideOutputWindowedValue(\n-            TupleTag<SideOutputT> tag,\n-            SideOutputT output,\n+        public <AdditionalOutputT> void outputWindowedValue(\n+            TupleTag<AdditionalOutputT> tag,\n+            AdditionalOutputT output,\n             Instant timestamp,\n             Collection<? extends BoundedWindow> windows,\n             PaneInfo pane) {\n-          context.sideOutputWindowedValue(tag, output, timestamp, windows, pane);\n+          context.outputWindowedValue(tag, output, timestamp, windows, pane);\n         }\n \n         @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "sha": "6320a3ac04cbfb0630ff600156619e0dc1307dea",
                "status": "modified"
            },
            {
                "additions": 115,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java",
                "changes": 115,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java",
                "patch": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.core;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.joda.time.Instant;\n+\n+/**\n+ * A {@link DoFnRunner} that can refuse to process elements that are not ready, instead returning\n+ * them via the {@link #processElementInReadyWindows(WindowedValue)}.\n+ */\n+public class SimplePushbackSideInputDoFnRunner<InputT, OutputT>\n+    implements PushbackSideInputDoFnRunner<InputT, OutputT> {\n+  private final DoFnRunner<InputT, OutputT> underlying;\n+  private final Collection<PCollectionView<?>> views;\n+  private final ReadyCheckingSideInputReader sideInputReader;\n+\n+  private Set<BoundedWindow> notReadyWindows;\n+\n+  public static <InputT, OutputT> SimplePushbackSideInputDoFnRunner<InputT, OutputT> create(\n+      DoFnRunner<InputT, OutputT> underlying,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader) {\n+    return new SimplePushbackSideInputDoFnRunner<>(underlying, views, sideInputReader);\n+  }\n+\n+  private SimplePushbackSideInputDoFnRunner(\n+      DoFnRunner<InputT, OutputT> underlying,\n+      Collection<PCollectionView<?>> views,\n+      ReadyCheckingSideInputReader sideInputReader) {\n+    this.underlying = underlying;\n+    this.views = views;\n+    this.sideInputReader = sideInputReader;\n+  }\n+\n+  @Override\n+  public void startBundle() {\n+    notReadyWindows = new HashSet<>();\n+    underlying.startBundle();\n+  }\n+\n+  @Override\n+  public Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem) {\n+    if (views.isEmpty()) {\n+      // When there are no side inputs, we can preserve the compressed representation.\n+      underlying.processElement(elem);\n+      return Collections.emptyList();\n+    }\n+    ImmutableList.Builder<WindowedValue<InputT>> pushedBack = ImmutableList.builder();\n+    for (WindowedValue<InputT> windowElem : elem.explodeWindows()) {\n+      BoundedWindow mainInputWindow = Iterables.getOnlyElement(windowElem.getWindows());\n+      if (isReady(mainInputWindow)) {\n+        // When there are any side inputs, we have to process the element in each window\n+        // individually, to disambiguate access to per-window side inputs.\n+        underlying.processElement(windowElem);\n+      } else {\n+        notReadyWindows.add(mainInputWindow);\n+        pushedBack.add(windowElem);\n+      }\n+    }\n+    return pushedBack.build();\n+  }\n+\n+  private boolean isReady(BoundedWindow mainInputWindow) {\n+    if (notReadyWindows.contains(mainInputWindow)) {\n+      return false;\n+    }\n+    for (PCollectionView<?> view : views) {\n+      BoundedWindow sideInputWindow =\n+          view.getWindowMappingFn().getSideInputWindow(mainInputWindow);\n+      if (!sideInputReader.isReady(view, sideInputWindow)) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+                      TimeDomain timeDomain) {\n+    underlying.onTimer(timerId, window, timestamp, timeDomain);\n+  }\n+\n+  @Override\n+  public void finishBundle() {\n+    notReadyWindows = null;\n+    underlying.finishBundle();\n+  }\n+}\n+",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunner.java",
                "sha": "50d301bc18e6ac7f363aabbe0d6c329f85110af1",
                "status": "added"
            },
            {
                "additions": 86,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "changes": 154,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 68,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "patch": "@@ -19,10 +19,8 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkNotNull;\n-import static com.google.common.base.Preconditions.checkState;\n \n import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Iterables;\n import java.util.List;\n import java.util.UUID;\n@@ -82,14 +80,14 @@\n @Experimental(Experimental.Kind.SPLITTABLE_DO_FN)\n public class SplittableParDo<InputT, OutputT, RestrictionT>\n     extends PTransform<PCollection<InputT>, PCollectionTuple> {\n-  private final ParDo.BoundMulti<InputT, OutputT> parDo;\n+  private final ParDo.MultiOutput<InputT, OutputT> parDo;\n \n   /**\n    * Creates the transform for the given original multi-output {@link ParDo}.\n    *\n    * @param parDo The splittable {@link ParDo} transform.\n    */\n-  public SplittableParDo(ParDo.BoundMulti<InputT, OutputT> parDo) {\n+  public SplittableParDo(ParDo.MultiOutput<InputT, OutputT> parDo) {\n     checkNotNull(parDo, \"parDo must not be null\");\n     this.parDo = parDo;\n     checkArgument(\n@@ -115,10 +113,10 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n             fn,\n             input.getCoder(),\n             restrictionCoder,\n-            input.getWindowingStrategy(),\n+            (WindowingStrategy<InputT, ?>) input.getWindowingStrategy(),\n             parDo.getSideInputs(),\n             parDo.getMainOutputTag(),\n-            parDo.getSideOutputTags()));\n+            parDo.getAdditionalOutputTags()));\n   }\n \n   private static <InputT, OutputT, RestrictionT>\n@@ -138,6 +136,12 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n             .setCoder(splitCoder)\n             .apply(\"Split restriction\", ParDo.of(new SplitRestrictionFn<InputT, RestrictionT>(fn)))\n             .setCoder(splitCoder)\n+            // ProcessFn requires all input elements to be in a single window and have a single\n+            // element per work item. This must precede the unique keying so each key has a single\n+            // associated element.\n+            .apply(\n+                \"Explode windows\",\n+                ParDo.of(new ExplodeWindowsFn<ElementAndRestriction<InputT, RestrictionT>>()))\n             .apply(\n                 \"Assign unique key\",\n                 WithKeys.of(new RandomUniqueKeyFn<ElementAndRestriction<InputT, RestrictionT>>()))\n@@ -157,6 +161,18 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n     return keyedWorkItems;\n   }\n \n+  /**\n+   * A {@link DoFn} that forces each of its outputs to be in a single window, by indicating to the\n+   * runner that it observes the window of its input element, so the runner is forced to apply it to\n+   * each input in a single window and thus its output is also in a single window.\n+   */\n+  private static class ExplodeWindowsFn<InputT> extends DoFn<InputT, InputT> {\n+    @ProcessElement\n+    public void process(ProcessContext c, BoundedWindow window) {\n+      c.output(c.element());\n+    }\n+  }\n+\n   /**\n    * Runner-specific primitive {@link GroupByKey GroupByKey-like} {@link PTransform} that produces\n    * {@link KeyedWorkItem KeyedWorkItems} so that downstream transforms can access state and timers.\n@@ -185,33 +201,34 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n     private final DoFn<InputT, OutputT> fn;\n     private final Coder<InputT> elementCoder;\n     private final Coder<RestrictionT> restrictionCoder;\n-    private final WindowingStrategy<?, ?> windowingStrategy;\n+    private final WindowingStrategy<InputT, ?> windowingStrategy;\n     private final List<PCollectionView<?>> sideInputs;\n     private final TupleTag<OutputT> mainOutputTag;\n-    private final TupleTagList sideOutputTags;\n+    private final TupleTagList additionalOutputTags;\n \n     /**\n      * @param fn the splittable {@link DoFn}.\n      * @param windowingStrategy the {@link WindowingStrategy} of the input collection.\n      * @param sideInputs list of side inputs that should be available to the {@link DoFn}.\n      * @param mainOutputTag {@link TupleTag Tag} of the {@link DoFn DoFn's} main output.\n-     * @param sideOutputTags {@link TupleTagList Tags} of the {@link DoFn DoFn's} side outputs.\n+     * @param additionalOutputTags {@link TupleTagList Tags} of the {@link DoFn DoFn's} additional\n+     *     outputs.\n      */\n     public ProcessElements(\n         DoFn<InputT, OutputT> fn,\n         Coder<InputT> elementCoder,\n         Coder<RestrictionT> restrictionCoder,\n-        WindowingStrategy<?, ?> windowingStrategy,\n+        WindowingStrategy<InputT, ?> windowingStrategy,\n         List<PCollectionView<?>> sideInputs,\n         TupleTag<OutputT> mainOutputTag,\n-        TupleTagList sideOutputTags) {\n+        TupleTagList additionalOutputTags) {\n       this.fn = fn;\n       this.elementCoder = elementCoder;\n       this.restrictionCoder = restrictionCoder;\n       this.windowingStrategy = windowingStrategy;\n       this.sideInputs = sideInputs;\n       this.mainOutputTag = mainOutputTag;\n-      this.sideOutputTags = sideOutputTags;\n+      this.additionalOutputTags = additionalOutputTags;\n     }\n \n     public DoFn<InputT, OutputT> getFn() {\n@@ -226,14 +243,14 @@ public ProcessElements(\n       return mainOutputTag;\n     }\n \n-    public TupleTagList getSideOutputTags() {\n-      return sideOutputTags;\n+    public TupleTagList getAdditionalOutputTags() {\n+      return additionalOutputTags;\n     }\n \n     public ProcessFn<InputT, OutputT, RestrictionT, TrackerT> newProcessFn(\n         DoFn<InputT, OutputT> fn) {\n       return new SplittableParDo.ProcessFn<>(\n-          fn, elementCoder, restrictionCoder, windowingStrategy.getWindowFn().windowCoder());\n+          fn, elementCoder, restrictionCoder, windowingStrategy);\n     }\n \n     @Override\n@@ -244,11 +261,11 @@ public PCollectionTuple expand(\n       PCollectionTuple outputs =\n           PCollectionTuple.ofPrimitiveOutputsInternal(\n               input.getPipeline(),\n-              TupleTagList.of(mainOutputTag).and(sideOutputTags.getAll()),\n+              TupleTagList.of(mainOutputTag).and(additionalOutputTags.getAll()),\n               windowingStrategy,\n               input.isBounded().and(signature.isBoundedPerElement()));\n \n-      // Set output type descriptor similarly to how ParDo.BoundMulti does it.\n+      // Set output type descriptor similarly to how ParDo.MultiOutput does it.\n       outputs.get(mainOutputTag).setTypeDescriptor(fn.getOutputTypeDescriptor());\n \n       return outputs;\n@@ -260,7 +277,7 @@ public PCollectionTuple expand(\n             input,\n         TypedPValue<T> output)\n         throws CannotProvideCoderException {\n-      // Similar logic to ParDo.BoundMulti.getDefaultOutputCoder.\n+      // Similar logic to ParDo.MultiOutput.getDefaultOutputCoder.\n       @SuppressWarnings(\"unchecked\")\n       KeyedWorkItemCoder<String, ElementAndRestriction<InputT, RestrictionT>> kwiCoder =\n           (KeyedWorkItemCoder) input.getCoder();\n@@ -316,6 +333,13 @@ public void processElement(ProcessContext context) {\n    * The heart of splittable {@link DoFn} execution: processes a single (element, restriction) pair\n    * by creating a tracker for the restriction and checkpointing/resuming processing later if\n    * necessary.\n+   *\n+   * <p>Takes {@link KeyedWorkItem} and assumes that the KeyedWorkItem contains a single element\n+   * (or a single timer set by {@link ProcessFn itself}, in a single window. This is necessary\n+   * because {@link ProcessFn} sets timers, and timers are namespaced to a single window and it\n+   * should be the window of the input element.\n+   *\n+   * <p>See also: https://issues.apache.org/jira/browse/BEAM-1983\n    */\n   @VisibleForTesting\n   public static class ProcessFn<\n@@ -324,14 +348,12 @@ public void processElement(ProcessContext context) {\n     /**\n      * The state cell containing a watermark hold for the output of this {@link DoFn}. The hold is\n      * acquired during the first {@link DoFn.ProcessElement} call for each element and restriction,\n-     * and is released when the {@link DoFn.ProcessElement} call returns {@link\n-     * DoFn.ProcessContinuation#stop}.\n+     * and is released when the {@link DoFn.ProcessElement} call returns and there is no residual\n+     * restriction captured by the {@link SplittableProcessElementInvoker}.\n      *\n      * <p>A hold is needed to avoid letting the output watermark immediately progress together with\n      * the input watermark when the first {@link DoFn.ProcessElement} call for this element\n      * completes.\n-     *\n-     * <p>The hold is updated with the future output watermark reported by ProcessContinuation.\n      */\n     private static final StateTag<Object, WatermarkHoldState<GlobalWindow>> watermarkHoldTag =\n         StateTags.makeSystemTagInternal(\n@@ -352,7 +374,9 @@ public void processElement(ProcessContext context) {\n     private StateTag<Object, ValueState<RestrictionT>> restrictionTag;\n \n     private final DoFn<InputT, OutputT> fn;\n-    private final Coder<? extends BoundedWindow> windowCoder;\n+    private final Coder<InputT> elementCoder;\n+    private final Coder<RestrictionT> restrictionCoder;\n+    private final WindowingStrategy<InputT, ?> inputWindowingStrategy;\n \n     private transient StateInternalsFactory<String> stateInternalsFactory;\n     private transient TimerInternalsFactory<String> timerInternalsFactory;\n@@ -365,11 +389,16 @@ public ProcessFn(\n         DoFn<InputT, OutputT> fn,\n         Coder<InputT> elementCoder,\n         Coder<RestrictionT> restrictionCoder,\n-        Coder<? extends BoundedWindow> windowCoder) {\n+        WindowingStrategy<InputT, ?> inputWindowingStrategy) {\n       this.fn = fn;\n-      this.windowCoder = windowCoder;\n+      this.elementCoder = elementCoder;\n+      this.restrictionCoder = restrictionCoder;\n+      this.inputWindowingStrategy = inputWindowingStrategy;\n       this.elementTag =\n-          StateTags.value(\"element\", WindowedValue.getFullCoder(elementCoder, this.windowCoder));\n+          StateTags.value(\n+              \"element\",\n+              WindowedValue.getFullCoder(\n+                  elementCoder, inputWindowingStrategy.getWindowFn().windowCoder()));\n       this.restrictionTag = StateTags.value(\"restriction\", restrictionCoder);\n     }\n \n@@ -390,6 +419,18 @@ public void setProcessElementInvoker(\n       return fn;\n     }\n \n+    public Coder<InputT> getElementCoder() {\n+      return elementCoder;\n+    }\n+\n+    public Coder<RestrictionT> getRestrictionCoder() {\n+      return restrictionCoder;\n+    }\n+\n+    public WindowingStrategy<InputT, ?> getInputWindowingStrategy() {\n+      return inputWindowingStrategy;\n+    }\n+\n     @Setup\n     public void setup() throws Exception {\n       invoker = DoFnInvokers.invokerFor(fn);\n@@ -423,7 +464,18 @@ public void processElement(final ProcessContext c) {\n       // Subsequent calls are timer firings and the element has to be retrieved from the state.\n       TimerInternals.TimerData timer = Iterables.getOnlyElement(c.element().timersIterable(), null);\n       boolean isSeedCall = (timer == null);\n-      StateNamespace stateNamespace = isSeedCall ? StateNamespaces.global() : timer.getNamespace();\n+      StateNamespace stateNamespace;\n+      if (isSeedCall) {\n+        WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue =\n+            Iterables.getOnlyElement(c.element().elementsIterable());\n+        BoundedWindow window = Iterables.getOnlyElement(windowedValue.getWindows());\n+        stateNamespace =\n+            StateNamespaces.window(\n+                (Coder<BoundedWindow>) inputWindowingStrategy.getWindowFn().windowCoder(), window);\n+      } else {\n+        stateNamespace = timer.getNamespace();\n+      }\n+\n       ValueState<WindowedValue<InputT>> elementState =\n           stateInternals.state(stateNamespace, elementTag);\n       ValueState<RestrictionT> restrictionState =\n@@ -433,15 +485,8 @@ public void processElement(final ProcessContext c) {\n \n       ElementAndRestriction<WindowedValue<InputT>, RestrictionT> elementAndRestriction;\n       if (isSeedCall) {\n-        // The element and restriction are available in c.element().\n-        // elementsIterable() will, by construction of SplittableParDo, contain the same value\n-        // potentially in several different windows. We implode this into a single WindowedValue\n-        // in order to simplify the rest of the code and avoid iterating over elementsIterable()\n-        // explicitly. The windows of this WindowedValue will be propagated to windows of the\n-        // output. This is correct because a splittable DoFn is not allowed to inspect the window\n-        // of its element.\n         WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue =\n-            implodeWindows(c.element().elementsIterable());\n+            Iterables.getOnlyElement(c.element().elementsIterable());\n         WindowedValue<InputT> element = windowedValue.withValue(windowedValue.getValue().element());\n         elementState.write(element);\n         elementAndRestriction =\n@@ -461,50 +506,23 @@ public void processElement(final ProcessContext c) {\n               invoker, elementAndRestriction.element(), tracker);\n \n       // Save state for resuming.\n-      if (!result.getContinuation().shouldResume()) {\n+      if (result.getResidualRestriction() == null) {\n         // All work for this element/restriction is completed. Clear state and release hold.\n         elementState.clear();\n         restrictionState.clear();\n         holdState.clear();\n         return;\n       }\n       restrictionState.write(result.getResidualRestriction());\n-      Instant futureOutputWatermark = result.getContinuation().getWatermark();\n+      Instant futureOutputWatermark = result.getFutureOutputWatermark();\n       if (futureOutputWatermark == null) {\n         futureOutputWatermark = elementAndRestriction.element().getTimestamp();\n       }\n-      Instant wakeupTime =\n-          timerInternals.currentProcessingTime().plus(result.getContinuation().resumeDelay());\n       holdState.add(futureOutputWatermark);\n       // Set a timer to continue processing this element.\n       timerInternals.setTimer(\n-          TimerInternals.TimerData.of(stateNamespace, wakeupTime, TimeDomain.PROCESSING_TIME));\n-    }\n-\n-    /**\n-     * Does the opposite of {@link WindowedValue#explodeWindows()} - creates a single {@link\n-     * WindowedValue} from a collection of {@link WindowedValue}'s that is known to contain copies\n-     * of the same value with the same timestamp, but different window sets.\n-     *\n-     * <p>This is only legal to do because we know that {@link RandomUniqueKeyFn} created unique\n-     * keys for every {@link ElementAndRestriction}, so if there's multiple {@link WindowedValue}'s\n-     * for the same key, that means only that the windows of that {@link ElementAndRestriction} are\n-     * being delivered separately rather than all at once. It is also legal to do because splittable\n-     * {@link DoFn} is not allowed to access the window of its element, so we can propagate the full\n-     * set of windows of its input to its output.\n-     */\n-    private static <InputT, RestrictionT>\n-        WindowedValue<ElementAndRestriction<InputT, RestrictionT>> implodeWindows(\n-            Iterable<WindowedValue<ElementAndRestriction<InputT, RestrictionT>>> values) {\n-      WindowedValue<ElementAndRestriction<InputT, RestrictionT>> first =\n-          Iterables.getFirst(values, null);\n-      checkState(first != null, \"Got a KeyedWorkItem with no elements and no timers\");\n-      ImmutableList.Builder<BoundedWindow> windows = ImmutableList.builder();\n-      for (WindowedValue<ElementAndRestriction<InputT, RestrictionT>> value : values) {\n-        windows.addAll(value.getWindows());\n-      }\n-      return WindowedValue.of(\n-          first.getValue(), first.getTimestamp(), windows.build(), first.getPane());\n+          TimerInternals.TimerData.of(\n+              stateNamespace, timerInternals.currentProcessingTime(), TimeDomain.PROCESSING_TIME));\n     }\n \n     private DoFn<InputT, OutputT>.Context wrapContext(final Context baseContext) {\n@@ -525,12 +543,12 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n         }\n \n         @Override\n-        public <T> void sideOutput(TupleTag<T> tag, T output) {\n+        public <T> void output(TupleTag<T> tag, T output) {\n           throwUnsupportedOutput();\n         }\n \n         @Override\n-        public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+        public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n           throwUnsupportedOutput();\n         }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "sha": "31d89eec265b72fbe5aebacb073003ebb20d036b",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.joda.time.Instant;\n \n /**\n  * A runner-specific hook for invoking a {@link DoFn.ProcessElement} method for a splittable {@link\n@@ -31,34 +32,33 @@\n     InputT, OutputT, RestrictionT, TrackerT extends RestrictionTracker<RestrictionT>> {\n   /** Specifies how to resume a splittable {@link DoFn.ProcessElement} call. */\n   public class Result {\n-    @Nullable private final RestrictionT residualRestriction;\n-    private final DoFn.ProcessContinuation continuation;\n+    @Nullable\n+    private final RestrictionT residualRestriction;\n+    private final Instant futureOutputWatermark;\n \n     public Result(\n-        @Nullable RestrictionT residualRestriction, DoFn.ProcessContinuation continuation) {\n+        @Nullable RestrictionT residualRestriction, Instant futureOutputWatermark) {\n       this.residualRestriction = residualRestriction;\n-      this.continuation = continuation;\n+      this.futureOutputWatermark = futureOutputWatermark;\n     }\n \n-    /**\n-     * Can be {@code null} only if {@link #getContinuation} specifies the call should not resume.\n-     */\n+    /** If {@code null}, means the call should not resume. */\n     @Nullable\n     public RestrictionT getResidualRestriction() {\n       return residualRestriction;\n     }\n \n-    public DoFn.ProcessContinuation getContinuation() {\n-      return continuation;\n+    public Instant getFutureOutputWatermark() {\n+      return futureOutputWatermark;\n     }\n   }\n \n   /**\n    * Invokes the {@link DoFn.ProcessElement} method using the given {@link DoFnInvoker} for the\n    * original {@link DoFn}, on the given element and with the given {@link RestrictionTracker}.\n    *\n-   * @return Information on how to resume the call: residual restriction and a {@link\n-   *     DoFn.ProcessContinuation}.\n+   * @return Information on how to resume the call: residual restriction and a\n+   * future output watermark.\n    */\n   public abstract Result invokeProcessElement(\n       DoFnInvoker<InputT, OutputT> invoker, WindowedValue<InputT> element, TrackerT tracker);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableProcessElementInvoker.java",
                "sha": "ced6c015039f81062376051faaa77c60b4d2f74d",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 8,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java",
                "patch": "@@ -24,9 +24,9 @@\n import java.util.List;\n import java.util.Map;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -159,7 +159,7 @@\n    * Prefetch all combining value state for {@code address} across all merging windows in {@code\n    * context}.\n    */\n-  public static <K, StateT extends CombiningState<?, ?>, W extends BoundedWindow> void\n+  public static <K, StateT extends GroupingState<?, ?>, W extends BoundedWindow> void\n       prefetchCombiningValues(MergingStateAccessor<K, W> context,\n           StateTag<? super K, StateT> address) {\n     for (StateT state : context.accessInEachMergingWindow(address).values()) {\n@@ -172,7 +172,7 @@\n    */\n   public static <K, InputT, AccumT, OutputT, W extends BoundedWindow> void mergeCombiningValues(\n       MergingStateAccessor<K, W> context,\n-      StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address) {\n+      StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address) {\n     mergeCombiningValues(\n         context.accessInEachMergingWindow(address).values(), context.access(address));\n   }\n@@ -182,8 +182,8 @@\n    * {@code result}.\n    */\n   public static <InputT, AccumT, OutputT, W extends BoundedWindow> void mergeCombiningValues(\n-      Collection<AccumulatorCombiningState<InputT, AccumT, OutputT>> sources,\n-      AccumulatorCombiningState<InputT, AccumT, OutputT> result) {\n+      Collection<CombiningState<InputT, AccumT, OutputT>> sources,\n+      CombiningState<InputT, AccumT, OutputT> result) {\n     if (sources.isEmpty()) {\n       // Nothing to merge.\n       return;\n@@ -194,18 +194,18 @@\n     }\n     // Prefetch.\n     List<ReadableState<AccumT>> futures = new ArrayList<>(sources.size());\n-    for (AccumulatorCombiningState<InputT, AccumT, OutputT> source : sources) {\n+    for (CombiningState<InputT, AccumT, OutputT> source : sources) {\n       prefetchRead(source);\n     }\n     // Read.\n     List<AccumT> accumulators = new ArrayList<>(futures.size());\n-    for (AccumulatorCombiningState<InputT, AccumT, OutputT> source : sources) {\n+    for (CombiningState<InputT, AccumT, OutputT> source : sources) {\n       accumulators.add(source.getAccum());\n     }\n     // Merge (possibly update and return one of the existing accumulators).\n     AccumT merged = result.mergeAccumulators(accumulators);\n     // Clear sources.\n-    for (AccumulatorCombiningState<InputT, AccumT, OutputT> source : sources) {\n+    for (CombiningState<InputT, AccumT, OutputT> source : sources) {\n       source.clear();\n     }\n     // Update result.",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateMerging.java",
                "sha": "34108507d051258085136f643b5c23d4c2eca369",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java",
                "patch": "@@ -28,8 +28,8 @@\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -94,20 +94,20 @@\n         StateTag<? super K, MapState<KeyT, ValueT>> spec,\n         Coder<KeyT> mapKeyCoder, Coder<ValueT> mapValueCoder);\n \n-    <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT> bindCombiningValue(\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+    <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT> bindCombiningValue(\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n         Coder<AccumT> accumCoder,\n         CombineFn<InputT, AccumT, OutputT> combineFn);\n \n     <InputT, AccumT, OutputT>\n-    AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValue(\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+    CombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValue(\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n         Coder<AccumT> accumCoder,\n         KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn);\n \n     <InputT, AccumT, OutputT>\n-    AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValueWithContext(\n-        StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+    CombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValueWithContext(\n+        StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n         Coder<AccumT> accumCoder,\n         KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT>\n             combineFn);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTag.java",
                "sha": "12c59adb8e449aef268950203e749a4e02dd19a1",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 16,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java",
                "patch": "@@ -30,8 +30,8 @@\n import org.apache.beam.sdk.transforms.CombineWithContext.KeyedCombineFnWithContext;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -84,29 +84,29 @@\n \n       @Override\n       public <InputT, AccumT, OutputT>\n-          AccumulatorCombiningState<InputT, AccumT, OutputT> bindCombiningValue(\n+      CombiningState<InputT, AccumT, OutputT> bindCombining(\n               String id,\n-              StateSpec<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+              StateSpec<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n               Coder<AccumT> accumCoder,\n               CombineFn<InputT, AccumT, OutputT> combineFn) {\n         return binder.bindCombiningValue(tagForSpec(id, spec), accumCoder, combineFn);\n       }\n \n       @Override\n       public <InputT, AccumT, OutputT>\n-          AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValue(\n+      CombiningState<InputT, AccumT, OutputT> bindKeyedCombining(\n               String id,\n-              StateSpec<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+              StateSpec<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n               Coder<AccumT> accumCoder,\n               KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n         return binder.bindKeyedCombiningValue(tagForSpec(id, spec), accumCoder, combineFn);\n       }\n \n       @Override\n       public <InputT, AccumT, OutputT>\n-          AccumulatorCombiningState<InputT, AccumT, OutputT> bindKeyedCombiningValueWithContext(\n+      CombiningState<InputT, AccumT, OutputT> bindKeyedCombiningWithContext(\n               String id,\n-              StateSpec<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> spec,\n+              StateSpec<? super K, CombiningState<InputT, AccumT, OutputT>> spec,\n               Coder<AccumT> accumCoder,\n               KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n         return binder.bindKeyedCombiningValueWithContext(\n@@ -158,37 +158,37 @@ private StateTags() { }\n    * multiple {@code InputT}s into a single {@code OutputT}.\n    */\n   public static <InputT, AccumT, OutputT>\n-    StateTag<Object, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+    StateTag<Object, CombiningState<InputT, AccumT, OutputT>>\n     combiningValue(\n       String id, Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.combiningValue(accumCoder, combineFn));\n+        new StructuredId(id), StateSpecs.combining(accumCoder, combineFn));\n   }\n \n   /**\n    * Create a state tag for values that use a {@link KeyedCombineFn} to automatically merge\n    * multiple {@code InputT}s into a single {@code OutputT}.\n    */\n   public static <K, InputT, AccumT,\n-      OutputT> StateTag<K, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+      OutputT> StateTag<K, CombiningState<InputT, AccumT, OutputT>>\n       keyedCombiningValue(String id, Coder<AccumT> accumCoder,\n           KeyedCombineFn<K, InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.keyedCombiningValue(accumCoder, combineFn));\n+        new StructuredId(id), StateSpecs.keyedCombining(accumCoder, combineFn));\n   }\n \n   /**\n    * Create a state tag for values that use a {@link KeyedCombineFnWithContext} to automatically\n    * merge multiple {@code InputT}s into a single {@code OutputT}.\n    */\n   public static <K, InputT, AccumT, OutputT>\n-      StateTag<K, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+      StateTag<K, CombiningState<InputT, AccumT, OutputT>>\n       keyedCombiningValueWithContext(\n           String id,\n           Coder<AccumT> accumCoder,\n           KeyedCombineFnWithContext<K, InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.keyedCombiningValueWithContext(accumCoder, combineFn));\n+        new StructuredId(id), StateSpecs.keyedCombiningWithContext(accumCoder, combineFn));\n   }\n \n   /**\n@@ -199,11 +199,11 @@ private StateTags() { }\n    * should only be used to initialize static values.\n    */\n   public static <InputT, AccumT, OutputT>\n-      StateTag<Object, AccumulatorCombiningState<InputT, AccumT, OutputT>>\n+      StateTag<Object, CombiningState<InputT, AccumT, OutputT>>\n       combiningValueFromInputInternal(\n           String id, Coder<InputT> inputCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n     return new SimpleStateTag<>(\n-        new StructuredId(id), StateSpecs.combiningValueFromInputInternal(inputCoder, combineFn));\n+        new StructuredId(id), StateSpecs.combiningFromInputInternal(inputCoder, combineFn));\n   }\n \n   /**\n@@ -255,7 +255,7 @@ private StateTags() { }\n \n   public static <K, InputT, AccumT, OutputT> StateTag<Object, BagState<AccumT>>\n       convertToBagTagInternal(\n-          StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> combiningTag) {\n+          StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> combiningTag) {\n     return new SimpleStateTag<>(\n         new StructuredId(combiningTag.getId()),\n         StateSpecs.convertToBagSpecInternal(combiningTag.getSpec()));",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StateTags.java",
                "sha": "77ae8f533d173abb2ff6dd46eec49f4149c0f469",
                "status": "modified"
            },
            {
                "additions": 96,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java",
                "patch": "@@ -17,15 +17,21 @@\n  */\n package org.apache.beam.runners.core;\n \n+import java.util.Map;\n+import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.NonMergingWindowFn;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowTracing;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.util.state.State;\n+import org.apache.beam.sdk.util.state.StateSpec;\n import org.joda.time.Instant;\n \n /**\n@@ -168,4 +174,94 @@ boolean isForWindow(\n \n     void clearForWindow(W window);\n   }\n+\n+  /**\n+   * A {@link StatefulDoFnRunner.CleanupTimer} implemented via {@link TimerInternals}.\n+   */\n+  public static class TimeInternalsCleanupTimer implements StatefulDoFnRunner.CleanupTimer {\n+\n+    public static final String GC_TIMER_ID = \"__StatefulParDoGcTimerId\";\n+\n+    /**\n+     * The amount of milliseconds by which to delay cleanup. We use this to ensure that state is\n+     * still available when a user timer for {@code window.maxTimestamp()} fires.\n+     */\n+    public static final long GC_DELAY_MS = 1;\n+\n+    private final TimerInternals timerInternals;\n+    private final WindowingStrategy<?, ?> windowingStrategy;\n+    private final Coder<BoundedWindow> windowCoder;\n+\n+    public TimeInternalsCleanupTimer(\n+        TimerInternals timerInternals,\n+        WindowingStrategy<?, ?> windowingStrategy) {\n+      this.windowingStrategy = windowingStrategy;\n+      WindowFn<?, ?> windowFn = windowingStrategy.getWindowFn();\n+      windowCoder = (Coder<BoundedWindow>) windowFn.windowCoder();\n+      this.timerInternals = timerInternals;\n+    }\n+\n+    @Override\n+    public Instant currentInputWatermarkTime() {\n+      return timerInternals.currentInputWatermarkTime();\n+    }\n+\n+    @Override\n+    public void setForWindow(BoundedWindow window) {\n+      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n+      // make sure this fires after any window.maxTimestamp() timers\n+      gcTime = gcTime.plus(GC_DELAY_MS);\n+      timerInternals.setTimer(StateNamespaces.window(windowCoder, window),\n+          GC_TIMER_ID, gcTime, TimeDomain.EVENT_TIME);\n+    }\n+\n+    @Override\n+    public boolean isForWindow(\n+        String timerId,\n+        BoundedWindow window,\n+        Instant timestamp,\n+        TimeDomain timeDomain) {\n+      boolean isEventTimer = timeDomain.equals(TimeDomain.EVENT_TIME);\n+      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n+      gcTime = gcTime.plus(GC_DELAY_MS);\n+      return isEventTimer && GC_TIMER_ID.equals(timerId) && gcTime.equals(timestamp);\n+    }\n+  }\n+\n+  /**\n+   * A {@link StatefulDoFnRunner.StateCleaner} implemented via {@link StateInternals}.\n+   */\n+  public static class StateInternalsStateCleaner<W extends BoundedWindow>\n+      implements StatefulDoFnRunner.StateCleaner<W> {\n+\n+    private final DoFn<?, ?> fn;\n+    private final DoFnSignature signature;\n+    private final StateInternals<?> stateInternals;\n+    private final Coder<W> windowCoder;\n+\n+    public StateInternalsStateCleaner(\n+        DoFn<?, ?> fn,\n+        StateInternals<?> stateInternals,\n+        Coder<W> windowCoder) {\n+      this.fn = fn;\n+      this.signature = DoFnSignatures.getSignature(fn.getClass());\n+      this.stateInternals = stateInternals;\n+      this.windowCoder = windowCoder;\n+    }\n+\n+    @Override\n+    public void clearForWindow(W window) {\n+      for (Map.Entry<String, DoFnSignature.StateDeclaration> entry :\n+          signature.stateDeclarations().entrySet()) {\n+        try {\n+          StateSpec<?, ?> spec = (StateSpec<?, ?>) entry.getValue().field().get(fn);\n+          State state = stateInternals.state(StateNamespaces.window(windowCoder, window),\n+              StateTags.tagForSpec(entry.getKey(), (StateSpec) spec));\n+          state.clear();\n+        } catch (IllegalAccessException e) {\n+          throw new RuntimeException(e);\n+        }\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/StatefulDoFnRunner.java",
                "sha": "4f158224287fa9aa6e8bf400899a821d09d636c4",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java",
                "patch": "@@ -25,9 +25,9 @@\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.AppliedCombineFn;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.ReadableState;\n \n /**\n@@ -71,7 +71,7 @@ public void onMerge(OnMergeContext c) throws Exception {\n       AccumT, OutputT, W>\n       combining(\n           final Coder<K> keyCoder, final AppliedCombineFn<K, InputT, AccumT, OutputT> combineFn) {\n-    final StateTag<K, AccumulatorCombiningState<InputT, AccumT, OutputT>> bufferTag;\n+    final StateTag<K, CombiningState<InputT, AccumT, OutputT>> bufferTag;\n     if (combineFn.getFn() instanceof KeyedCombineFnWithContext) {\n       bufferTag = StateTags.makeSystemTagInternal(\n           StateTags.<K, InputT, AccumT, OutputT>keyedCombiningValueWithContext(\n@@ -97,10 +97,10 @@ public void onMerge(OnMergeContext c) throws Exception {\n     };\n   }\n \n-  private StateTag<? super K, ? extends CombiningState<InputT, OutputT>> bufferTag;\n+  private StateTag<? super K, ? extends GroupingState<InputT, OutputT>> bufferTag;\n \n   public SystemReduceFn(\n-      StateTag<? super K, ? extends CombiningState<InputT, OutputT>> bufferTag) {\n+      StateTag<? super K, ? extends GroupingState<InputT, OutputT>> bufferTag) {\n     this.bufferTag = bufferTag;\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/SystemReduceFn.java",
                "sha": "f618d889d7b8d2c836ead5079214d6fc7d5856ad",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java",
                "patch": "@@ -49,11 +49,11 @@ void outputWindowedValue(OutputT output, Instant timestamp,\n       Collection<? extends BoundedWindow> windows, PaneInfo pane);\n \n   /**\n-   * Output the value to a side output at the specified timestamp in the listed windows.\n+   * Output the value to a tagged output at the specified timestamp in the listed windows.\n    */\n-  <SideOutputT> void sideOutputWindowedValue(\n-      TupleTag<SideOutputT> tag,\n-      SideOutputT output,\n+  <AdditionalOutputT> void outputWindowedValue(\n+      TupleTag<AdditionalOutputT> tag,\n+      AdditionalOutputT output,\n       Instant timestamp,\n       Collection<? extends BoundedWindow> windows,\n       PaneInfo pane);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternals.java",
                "sha": "50050653c486797ac6be2dcd49e8e525bfc8c75e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java",
                "patch": "@@ -62,13 +62,13 @@ public void outputWindowedValue(\n       }\n \n       @Override\n-      public <SideOutputT> void sideOutputWindowedValue(\n-          TupleTag<SideOutputT> tag,\n-          SideOutputT output,\n+      public <AdditionalOutputT> void outputWindowedValue(\n+          TupleTag<AdditionalOutputT> tag,\n+          AdditionalOutputT output,\n           Instant timestamp,\n           Collection<? extends BoundedWindow> windows,\n           PaneInfo pane) {\n-        windowingInternals.sideOutputWindowedValue(tag, output, timestamp, windows, pane);\n+        windowingInternals.outputWindowedValue(tag, output, timestamp, windows, pane);\n       }\n     };\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/WindowingInternalsAdapters.java",
                "sha": "1b36bf9c487222df2d7268d13b4ea563673de4dc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java",
                "patch": "@@ -30,12 +30,12 @@\n import org.apache.beam.runners.core.triggers.TriggerStateMachine.OnceTriggerStateMachine;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.coders.InstantCoder;\n-import org.apache.beam.sdk.transforms.Combine;\n+import org.apache.beam.sdk.transforms.Combine.Holder;\n import org.apache.beam.sdk.transforms.Min;\n import org.apache.beam.sdk.transforms.SerializableFunction;\n import org.apache.beam.sdk.util.TimeDomain;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.joda.time.format.PeriodFormat;\n@@ -55,8 +55,8 @@\n   protected static final List<SerializableFunction<Instant, Instant>> IDENTITY =\n       ImmutableList.<SerializableFunction<Instant, Instant>>of();\n \n-  protected static final StateTag<Object, AccumulatorCombiningState<Instant,\n-                                              Combine.Holder<Instant>, Instant>> DELAYED_UNTIL_TAG =\n+  protected static final StateTag<Object, CombiningState<Instant,\n+                                                Holder<Instant>, Instant>> DELAYED_UNTIL_TAG =\n       StateTags.makeSystemTagInternal(StateTags.combiningValueFromInputInternal(\n           \"delayed\", InstantCoder.of(), Min.<Instant>naturalOrder()));\n \n@@ -169,7 +169,7 @@ public void prefetchOnElement(StateAccessor<?> state) {\n \n   @Override\n   public void onElement(OnElementContext c) throws Exception {\n-    CombiningState<Instant, Instant> delayUntilState = c.state().access(DELAYED_UNTIL_TAG);\n+    GroupingState<Instant, Instant> delayUntilState = c.state().access(DELAYED_UNTIL_TAG);\n     Instant oldDelayUntil = delayUntilState.read();\n \n     // Since processing time can only advance, resulting in target wake-up times we would",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterDelayFromFirstElementStateMachine.java",
                "sha": "b416788fd06f800cc6d0486bc50f473eaa0aa87a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java",
                "patch": "@@ -27,15 +27,15 @@\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.coders.VarLongCoder;\n import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n \n /**\n  * {@link TriggerStateMachine}s that fire based on properties of the elements in the current pane.\n  */\n @Experimental(Experimental.Kind.TRIGGER)\n public class AfterPaneStateMachine extends OnceTriggerStateMachine {\n \n-private static final StateTag<Object, AccumulatorCombiningState<Long, long[], Long>>\n+private static final StateTag<Object, CombiningState<Long, long[], Long>>\n       ELEMENTS_IN_PANE_TAG =\n       StateTags.makeSystemTagInternal(StateTags.combiningValueFromInputInternal(\n           \"count\", VarLongCoder.of(), Sum.ofLongs()));",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterPaneStateMachine.java",
                "sha": "11323cc69eb27477713664ea827f131468e6939e",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 8,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java",
                "patch": "@@ -31,18 +31,16 @@\n  * lower-bound, sometimes heuristically established, on event times that have been fully processed\n  * by the pipeline.\n  *\n- * <p>For sources that provide non-heuristic watermarks (e.g.\n- * {@link org.apache.beam.sdk.io.PubsubIO} when using arrival times as event times), the\n- * watermark is a strict guarantee that no data with an event time earlier than\n+ * <p>For sources that provide non-heuristic watermarks (e.g. PubsubIO when using arrival times as\n+ * event times), the watermark is a strict guarantee that no data with an event time earlier than\n  * that watermark will ever be observed in the pipeline. In this case, it's safe to assume that any\n  * pane triggered by an {@code AfterWatermark} trigger with a reference point at or beyond the end\n  * of the window will be the last pane ever for that window.\n  *\n- * <p>For sources that provide heuristic watermarks (e.g.\n- * {@link org.apache.beam.sdk.io.PubsubIO} when using user-supplied event times), the\n- * watermark itself becomes an <i>estimate</i> that no data with an event time earlier than that\n- * watermark (i.e. \"late data\") will ever be observed in the pipeline. These heuristics can\n- * often be quite accurate, but the chance of seeing late data for any given window is non-zero.\n+ * <p>For sources that provide heuristic watermarks (e.g. PubsubIO when using user-supplied event\n+ * times), the watermark itself becomes an <i>estimate</i> that no data with an event time earlier\n+ * than that watermark (i.e. \"late data\") will ever be observed in the pipeline. These heuristics\n+ * can often be quite accurate, but the chance of seeing late data for any given window is non-zero.\n  * Thus, if absolute correctness over time is important to your use case, you may want to consider\n  * using a trigger that accounts for late data. The default trigger,\n  * {@code Repeatedly.forever(AfterWatermark.pastEndOfWindow())}, which fires",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/AfterWatermarkStateMachine.java",
                "sha": "1b117d2e6923ba96e51965de725568a14c8b1a67",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java",
                "patch": "@@ -677,9 +677,9 @@ public void outputWindowedValue(\n         }\n \n         @Override\n-        public <SideOutputT> void sideOutputWindowedValue(\n-            TupleTag<SideOutputT> tag,\n-            SideOutputT output,\n+        public <AdditionalOutputT> void outputWindowedValue(\n+            TupleTag<AdditionalOutputT> tag,\n+            AdditionalOutputT output,\n             Instant timestamp,\n             Collection<? extends BoundedWindow> windows,\n             PaneInfo pane) {\n@@ -729,12 +729,12 @@ public void outputWithTimestamp(KV<K, OutputT> output, Instant timestamp) {\n     }\n \n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n+    public <T> void output(TupleTag<T> tag, T output) {\n       throw new UnsupportedOperationException();\n     }\n \n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n       throw new UnsupportedOperationException();\n     }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/GroupAlsoByWindowsProperties.java",
                "sha": "d0a89236f9b6713dfc40e10933c6798c806513c3",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java",
                "changes": 105,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 58,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java",
                "patch": "@@ -17,7 +17,9 @@\n  */\n package org.apache.beam.runners.core;\n \n+import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasItems;\n import static org.hamcrest.Matchers.not;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -26,7 +28,6 @@\n import static org.junit.Assert.assertTrue;\n \n import java.util.Arrays;\n-import java.util.Collections;\n import java.util.Map;\n import java.util.Objects;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n@@ -35,9 +36,9 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.SetState;\n@@ -61,7 +62,7 @@\n \n   private static final StateTag<Object, ValueState<String>> STRING_VALUE_ADDR =\n       StateTags.value(\"stringValue\", StringUtf8Coder.of());\n-  private static final StateTag<Object, AccumulatorCombiningState<Integer, int[], Integer>>\n+  private static final StateTag<Object, CombiningState<Integer, int[], Integer>>\n       SUM_INTEGER_ADDR = StateTags.combiningValueFromInputInternal(\n           \"sumInteger\", VarIntCoder.of(), Sum.ofIntegers());\n   private static final StateTag<Object, BagState<String>> STRING_BAG_ADDR =\n@@ -112,10 +113,10 @@ public void testBag() throws Exception {\n \n     assertThat(value.read(), Matchers.emptyIterable());\n     value.add(\"hello\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"hello\"));\n+    assertThat(value.read(), containsInAnyOrder(\"hello\"));\n \n     value.add(\"world\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"hello\", \"world\"));\n+    assertThat(value.read(), containsInAnyOrder(\"hello\", \"world\"));\n \n     value.clear();\n     assertThat(value.read(), Matchers.emptyIterable());\n@@ -147,7 +148,7 @@ public void testMergeBagIntoSource() throws Exception {\n     StateMerging.mergeBags(Arrays.asList(bag1, bag2), bag1);\n \n     // Reading the merged bag gets both the contents\n-    assertThat(bag1.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(bag1.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(bag2.read(), Matchers.emptyIterable());\n   }\n \n@@ -164,7 +165,7 @@ public void testMergeBagIntoNewNamespace() throws Exception {\n     StateMerging.mergeBags(Arrays.asList(bag1, bag2, bag3), bag3);\n \n     // Reading the merged bag gets both the contents\n-    assertThat(bag3.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(bag3.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(bag1.read(), Matchers.emptyIterable());\n     assertThat(bag2.read(), Matchers.emptyIterable());\n   }\n@@ -179,41 +180,32 @@ public void testSet() throws Exception {\n \n     // empty\n     assertThat(value.read(), Matchers.emptyIterable());\n-    assertFalse(value.contains(\"A\"));\n-    assertFalse(value.containsAny(Collections.singletonList(\"A\")));\n+    assertFalse(value.contains(\"A\").read());\n \n     // add\n     value.add(\"A\");\n     value.add(\"B\");\n     value.add(\"A\");\n-    assertFalse(value.addIfAbsent(\"B\"));\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"A\", \"B\"));\n+    assertFalse(value.addIfAbsent(\"B\").read());\n+    assertThat(value.read(), containsInAnyOrder(\"A\", \"B\"));\n \n     // remove\n     value.remove(\"A\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"B\"));\n+    assertThat(value.read(), containsInAnyOrder(\"B\"));\n     value.remove(\"C\");\n-    assertThat(value.read(), Matchers.containsInAnyOrder(\"B\"));\n+    assertThat(value.read(), containsInAnyOrder(\"B\"));\n \n     // contains\n-    assertFalse(value.contains(\"A\"));\n-    assertTrue(value.contains(\"B\"));\n+    assertFalse(value.contains(\"A\").read());\n+    assertTrue(value.contains(\"B\").read());\n     value.add(\"C\");\n     value.add(\"D\");\n \n-    // containsAny\n-    assertTrue(value.containsAny(Arrays.asList(\"A\", \"C\")));\n-    assertFalse(value.containsAny(Arrays.asList(\"A\", \"E\")));\n-\n-    // containsAll\n-    assertTrue(value.containsAll(Arrays.asList(\"B\", \"C\")));\n-    assertFalse(value.containsAll(Arrays.asList(\"A\", \"B\")));\n-\n     // readLater\n-    assertThat(value.readLater().read(), Matchers.containsInAnyOrder(\"B\", \"C\", \"D\"));\n-    SetState<String> later = value.readLater(Arrays.asList(\"A\", \"C\", \"D\"));\n-    assertTrue(later.containsAll(Arrays.asList(\"C\", \"D\")));\n-    assertFalse(later.contains(\"A\"));\n+    assertThat(value.readLater().read(), containsInAnyOrder(\"B\", \"C\", \"D\"));\n+    SetState<String> later = value.readLater();\n+    assertThat(later.read(), hasItems(\"C\", \"D\"));\n+    assertFalse(later.contains(\"A\").read());\n \n     // clear\n     value.clear();\n@@ -248,7 +240,7 @@ public void testMergeSetIntoSource() throws Exception {\n     StateMerging.mergeSets(Arrays.asList(set1, set2), set1);\n \n     // Reading the merged set gets both the contents\n-    assertThat(set1.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(set1.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(set2.read(), Matchers.emptyIterable());\n   }\n \n@@ -266,7 +258,7 @@ public void testMergeSetIntoNewNamespace() throws Exception {\n     StateMerging.mergeSets(Arrays.asList(set1, set2, set3), set3);\n \n     // Reading the merged set gets both the contents\n-    assertThat(set3.read(), Matchers.containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n+    assertThat(set3.read(), containsInAnyOrder(\"Hello\", \"World\", \"!\"));\n     assertThat(set1.read(), Matchers.emptyIterable());\n     assertThat(set2.read(), Matchers.emptyIterable());\n   }\n@@ -330,55 +322,52 @@ public void testMap() throws Exception {\n     assertThat(value, not(equalTo(underTest.state(NAMESPACE_2, STRING_MAP_ADDR))));\n \n     // put\n-    assertThat(value.iterate(), Matchers.emptyIterable());\n+    assertThat(value.entries().read(), Matchers.emptyIterable());\n     value.put(\"A\", 1);\n     value.put(\"B\", 2);\n     value.put(\"A\", 11);\n-    assertThat(value.putIfAbsent(\"B\", 22), equalTo(2));\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(MapEntry.of(\"A\", 11),\n+    assertThat(value.putIfAbsent(\"B\", 22).read(), equalTo(2));\n+    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(\"A\", 11),\n         MapEntry.of(\"B\", 2)));\n \n     // remove\n     value.remove(\"A\");\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(MapEntry.of(\"B\", 2)));\n+    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(\"B\", 2)));\n     value.remove(\"C\");\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(MapEntry.of(\"B\", 2)));\n+    assertThat(value.entries().read(), containsInAnyOrder(MapEntry.of(\"B\", 2)));\n \n     // get\n-    assertNull(value.get(\"A\"));\n-    assertThat(value.get(\"B\"), equalTo(2));\n+    assertNull(value.get(\"A\").read());\n+    assertThat(value.get(\"B\").read(), equalTo(2));\n     value.put(\"C\", 3);\n     value.put(\"D\", 4);\n-    assertThat(value.get(\"C\"), equalTo(3));\n-    assertThat(value.get(Collections.singletonList(\"D\")), Matchers.containsInAnyOrder(4));\n-    assertThat(value.get(Arrays.asList(\"B\", \"C\")), Matchers.containsInAnyOrder(2, 3));\n+    assertThat(value.get(\"C\").read(), equalTo(3));\n \n     // iterate\n     value.put(\"E\", 5);\n     value.remove(\"C\");\n-    assertThat(value.keys(), Matchers.containsInAnyOrder(\"B\", \"D\", \"E\"));\n-    assertThat(value.values(), Matchers.containsInAnyOrder(2, 4, 5));\n-    assertThat(value.iterate(), Matchers.containsInAnyOrder(\n-        MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n+    assertThat(value.keys().read(), containsInAnyOrder(\"B\", \"D\", \"E\"));\n+    assertThat(value.values().read(), containsInAnyOrder(2, 4, 5));\n+    assertThat(\n+        value.entries().read(),\n+        containsInAnyOrder(MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n \n     // readLater\n-    assertThat(value.getLater(\"B\").get(\"B\"), equalTo(2));\n-    assertNull(value.getLater(\"A\").get(\"A\"));\n-    MapState<String, Integer> later = value.getLater(Arrays.asList(\"C\", \"D\"));\n-    assertNull(later.get(\"C\"));\n-    assertThat(later.get(\"D\"), equalTo(4));\n-    assertThat(value.iterateLater().iterate(), Matchers.containsInAnyOrder(\n-        MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n+    assertThat(value.get(\"B\").readLater().read(), equalTo(2));\n+    assertNull(value.get(\"A\").readLater().read());\n+    assertThat(\n+        value.entries().readLater().read(),\n+        containsInAnyOrder(MapEntry.of(\"B\", 2), MapEntry.of(\"D\", 4), MapEntry.of(\"E\", 5)));\n \n     // clear\n     value.clear();\n-    assertThat(value.iterate(), Matchers.emptyIterable());\n+    assertThat(value.entries().read(), Matchers.emptyIterable());\n     assertThat(underTest.state(NAMESPACE_1, STRING_MAP_ADDR), Matchers.sameInstance(value));\n   }\n \n   @Test\n   public void testCombiningValue() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     // State instances are cached, but depend on the namespace.\n     assertEquals(value, underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR));\n@@ -398,7 +387,7 @@ public void testCombiningValue() throws Exception {\n \n   @Test\n   public void testCombiningIsEmpty() throws Exception {\n-    CombiningState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n+    GroupingState<Integer, Integer> value = underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n \n     assertThat(value.isEmpty().read(), Matchers.is(true));\n     ReadableState<Boolean> readFuture = value.isEmpty();\n@@ -411,9 +400,9 @@ public void testCombiningIsEmpty() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoSource() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n \n     value1.add(5);\n@@ -432,11 +421,11 @@ public void testMergeCombiningValueIntoSource() throws Exception {\n \n   @Test\n   public void testMergeCombiningValueIntoNewNamespace() throws Exception {\n-    AccumulatorCombiningState<Integer, int[], Integer> value1 =\n+    CombiningState<Integer, int[], Integer> value1 =\n         underTest.state(NAMESPACE_1, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value2 =\n+    CombiningState<Integer, int[], Integer> value2 =\n         underTest.state(NAMESPACE_2, SUM_INTEGER_ADDR);\n-    AccumulatorCombiningState<Integer, int[], Integer> value3 =\n+    CombiningState<Integer, int[], Integer> value3 =\n         underTest.state(NAMESPACE_3, SUM_INTEGER_ADDR);\n \n     value1.add(5);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/InMemoryStateInternalsTest.java",
                "sha": "34ddae6fc47a3c9160d280048b7428bf250e449f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java",
                "patch": "@@ -57,10 +57,10 @@ public void output(OutputT output) {\n     public void outputWithTimestamp(OutputT output, Instant timestamp) {\n     }\n     @Override\n-    public <T> void sideOutput(TupleTag<T> tag, T output) {\n+    public <T> void output(TupleTag<T> tag, T output) {\n     }\n     @Override\n-    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output,\n+    public <T> void outputWithTimestamp(TupleTag<T> tag, T output,\n         Instant timestamp) {\n     }\n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/NoOpOldDoFn.java",
                "sha": "2e5cd6dee85707025e9f7e74a9b393ba544eaeff",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java",
                "patch": "@@ -160,12 +160,12 @@ public void outputWithTimestamp(String output, Instant timestamp) {\n       }\n \n       @Override\n-      public <T> void sideOutput(TupleTag<T> tag, T output) {\n+      public <T> void output(TupleTag<T> tag, T output) {\n         throw new UnsupportedOperationException();\n       }\n \n       @Override\n-      public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      public <T> void outputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n         throw new UnsupportedOperationException();\n       }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OldDoFnTest.java",
                "sha": "425de073589d17a6d57726b5bbcff90f69b0fec0",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 22,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java",
                "patch": "@@ -17,15 +17,11 @@\n  */\n package org.apache.beam.runners.core;\n \n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.resume;\n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.stop;\n import static org.hamcrest.Matchers.greaterThan;\n import static org.hamcrest.Matchers.lessThan;\n import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertThat;\n-import static org.junit.Assert.assertTrue;\n \n import java.util.Collection;\n import java.util.concurrent.Executors;\n@@ -54,28 +50,18 @@ private SomeFn(Duration sleepBeforeEachOutput) {\n     }\n \n     @ProcessElement\n-    public ProcessContinuation process(ProcessContext context, OffsetRangeTracker tracker)\n+    public void process(ProcessContext context, OffsetRangeTracker tracker)\n         throws Exception {\n-      OffsetRange range = tracker.currentRestriction();\n-      for (int i = (int) range.getFrom(); i < range.getTo(); ++i) {\n-        if (!tracker.tryClaim(i)) {\n-          return resume();\n-        }\n+      for (long i = tracker.currentRestriction().getFrom(); tracker.tryClaim(i); ++i) {\n         Thread.sleep(sleepBeforeEachOutput.getMillis());\n         context.output(\"\" + i);\n       }\n-      return stop();\n     }\n \n     @GetInitialRestriction\n     public OffsetRange getInitialRestriction(Integer element) {\n       throw new UnsupportedOperationException(\"Should not be called in this test\");\n     }\n-\n-    @NewTracker\n-    public OffsetRangeTracker newTracker(OffsetRange range) {\n-      throw new UnsupportedOperationException(\"Should not be called in this test\");\n-    }\n   }\n \n   private SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result\n@@ -94,9 +80,9 @@ public void outputWindowedValue(\n                   PaneInfo pane) {}\n \n               @Override\n-              public <SideOutputT> void sideOutputWindowedValue(\n-                  TupleTag<SideOutputT> tag,\n-                  SideOutputT output,\n+              public <AdditionalOutputT> void outputWindowedValue(\n+                  TupleTag<AdditionalOutputT> tag,\n+                  AdditionalOutputT output,\n                   Instant timestamp,\n                   Collection<? extends BoundedWindow> windows,\n                   PaneInfo pane) {}\n@@ -116,7 +102,6 @@ public void outputWindowedValue(\n   public void testInvokeProcessElementOutputBounded() throws Exception {\n     SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result res =\n         runTest(10000, Duration.ZERO);\n-    assertTrue(res.getContinuation().shouldResume());\n     OffsetRange residualRange = res.getResidualRestriction();\n     // Should process the first 100 elements.\n     assertEquals(1000, residualRange.getFrom());\n@@ -127,7 +112,6 @@ public void testInvokeProcessElementOutputBounded() throws Exception {\n   public void testInvokeProcessElementTimeBounded() throws Exception {\n     SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result res =\n         runTest(10000, Duration.millis(100));\n-    assertTrue(res.getContinuation().shouldResume());\n     OffsetRange residualRange = res.getResidualRestriction();\n     // Should process ideally around 30 elements - but due to timing flakiness, we can't enforce\n     // that precisely. Just test that it's not egregiously off.\n@@ -140,7 +124,6 @@ public void testInvokeProcessElementTimeBounded() throws Exception {\n   public void testInvokeProcessElementVoluntaryReturn() throws Exception {\n     SplittableProcessElementInvoker<Integer, String, OffsetRange, OffsetRangeTracker>.Result res =\n         runTest(5, Duration.millis(100));\n-    assertFalse(res.getContinuation().shouldResume());\n     assertNull(res.getResidualRestriction());\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/OutputAndTimeBoundedSplittableProcessElementInvokerTest.java",
                "sha": "541e2383913e6a05156b977240617379df07eb15",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 6,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "patch": "@@ -65,6 +65,7 @@\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -360,8 +361,9 @@ public void testOnElementCombiningWithContext() throws Exception {\n         WindowingStrategy.of(FixedWindows.of(Duration.millis(2)))\n             .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);\n \n-    WindowingStrategy<?, IntervalWindow> sideInputWindowingStrategy =\n-        WindowingStrategy.of(FixedWindows.of(Duration.millis(4)));\n+    WindowMappingFn<?> sideInputWindowMappingFn =\n+        FixedWindows.of(Duration.millis(4)).getDefaultWindowMappingFn();\n+    when(mockView.getWindowMappingFn()).thenReturn((WindowMappingFn) sideInputWindowMappingFn);\n \n     TestOptions options = PipelineOptionsFactory.as(TestOptions.class);\n     options.setValue(expectedValue);\n@@ -384,10 +386,6 @@ public Integer answer(InvocationOnMock invocation) throws Throwable {\n               }\n             });\n \n-    @SuppressWarnings({\"rawtypes\", \"unchecked\", \"unused\"})\n-    Object suppressWarningsVar = when(mockView.getWindowingStrategyInternal())\n-        .thenReturn((WindowingStrategy) sideInputWindowingStrategy);\n-\n     SumAndVerifyContextFn combineFn = new SumAndVerifyContextFn(mockView, expectedValue);\n     ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(\n         mainInputWindowingStrategy, mockTriggerStateMachine, combineFn.<String>asKeyedFn(),",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "sha": "0d4d992a9386fad91f27398033538d992e7b2d93",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "patch": "@@ -38,6 +38,7 @@\n import java.util.Set;\n import javax.annotation.Nullable;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachineRunner;\n@@ -60,7 +61,6 @@\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.transforms.windowing.Trigger;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.AppliedCombineFn;\n@@ -574,13 +574,13 @@ public void outputWindowedValue(\n     }\n \n     @Override\n-    public <SideOutputT> void sideOutputWindowedValue(\n-        TupleTag<SideOutputT> tag,\n-        SideOutputT output,\n+    public <AdditionalOutputT> void outputWindowedValue(\n+        TupleTag<AdditionalOutputT> tag,\n+        AdditionalOutputT output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use side outputs\");\n+      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use tagged outputs\");\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "sha": "914550e2b570b8f1fc736430f5a9d4a458c6d6db",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 10,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java",
                "patch": "@@ -51,20 +51,22 @@\n   private WindowingStrategy<Object, IntervalWindow> windowingStrategy1 =\n       WindowingStrategy.of(FixedWindows.of(new Duration(WINDOW_MSECS_1)));\n \n-  private PCollectionView<Iterable<String>> view1 = PCollectionViewTesting.testingView(\n-      new TupleTag<Iterable<WindowedValue<String>>>() {},\n-      new PCollectionViewTesting.IdentityViewFn<String>(),\n-      StringUtf8Coder.of(),\n-      windowingStrategy1);\n+  private PCollectionView<Iterable<String>> view1 =\n+      PCollectionViewTesting.testingView(\n+          new TupleTag<Iterable<WindowedValue<String>>>() {},\n+          new PCollectionViewTesting.IdentityViewFn<String>(),\n+          StringUtf8Coder.of(),\n+          windowingStrategy1);\n \n   private WindowingStrategy<Object, IntervalWindow> windowingStrategy2 =\n       WindowingStrategy.of(FixedWindows.of(new Duration(WINDOW_MSECS_2)));\n \n-  private PCollectionView<Iterable<String>> view2 = PCollectionViewTesting.testingView(\n-      new TupleTag<Iterable<WindowedValue<String>>>() {},\n-      new PCollectionViewTesting.IdentityViewFn<String>(),\n-      StringUtf8Coder.of(),\n-      windowingStrategy2);\n+  private PCollectionView<Iterable<String>> view2 =\n+      PCollectionViewTesting.testingView(\n+          new TupleTag<Iterable<WindowedValue<String>>>() {},\n+          new PCollectionViewTesting.IdentityViewFn<String>(),\n+          StringUtf8Coder.of(),\n+          windowingStrategy2);\n \n   @Test\n   public void testIsEmpty() {",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SideInputHandlerTest.java",
                "sha": "335aedecd190d8cdce9331940e17b75bffa2a9f2",
                "status": "modified"
            },
            {
                "additions": 145,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "changes": 145,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "patch": "@@ -19,14 +19,18 @@\n \n import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.isA;\n import static org.junit.Assert.assertThat;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+import com.google.common.collect.ArrayListMultimap;\n+import com.google.common.collect.ListMultimap;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n import org.apache.beam.runners.core.BaseExecutionContext.StepContext;\n+import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.DoFn;\n@@ -45,6 +49,7 @@\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n+import org.joda.time.format.PeriodFormat;\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -234,6 +239,115 @@ public void testOnTimerCalled() {\n                 TimeDomain.EVENT_TIME)));\n   }\n \n+  /**\n+   * Demonstrates that attempting to output an element before the timestamp of the current element\n+   * with zero {@link DoFn#getAllowedTimestampSkew() allowed timestamp skew} throws.\n+   */\n+  @Test\n+  public void testBackwardsInTimeNoSkew() {\n+    SkewingDoFn fn = new SkewingDoFn(Duration.ZERO);\n+    DoFnRunner<Duration, Duration> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            new ListOutputManager(),\n+            new TupleTag<Duration>(),\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    runner.startBundle();\n+    // An element output at the current timestamp is fine.\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.ZERO, new Instant(0)));\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(isA(IllegalArgumentException.class));\n+    thrown.expectMessage(\"must be no earlier\");\n+    thrown.expectMessage(\n+        String.format(\"timestamp of the current input (%s)\", new Instant(0).toString()));\n+    thrown.expectMessage(\n+        String.format(\n+            \"the allowed skew (%s)\", PeriodFormat.getDefault().print(Duration.ZERO.toPeriod())));\n+    // An element output before (current time - skew) is forbidden\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));\n+  }\n+\n+  /**\n+   * Demonstrates that attempting to output an element before the timestamp of the current element\n+   * plus the value of {@link DoFn#getAllowedTimestampSkew()} throws, but between that value and\n+   * the current timestamp succeeds.\n+   */\n+  @Test\n+  public void testSkew() {\n+    SkewingDoFn fn = new SkewingDoFn(Duration.standardMinutes(10L));\n+    DoFnRunner<Duration, Duration> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            new ListOutputManager(),\n+            new TupleTag<Duration>(),\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    runner.startBundle();\n+    // Outputting between \"now\" and \"now - allowed skew\" succeeds.\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.standardMinutes(5L), new Instant(0)));\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(isA(IllegalArgumentException.class));\n+    thrown.expectMessage(\"must be no earlier\");\n+    thrown.expectMessage(\n+        String.format(\"timestamp of the current input (%s)\", new Instant(0).toString()));\n+    thrown.expectMessage(\n+        String.format(\n+            \"the allowed skew (%s)\",\n+            PeriodFormat.getDefault().print(Duration.standardMinutes(10L).toPeriod())));\n+    // Outputting before \"now - allowed skew\" fails.\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.standardHours(1L), new Instant(0)));\n+  }\n+\n+  /**\n+   * Demonstrates that attempting to output an element with a timestamp before the current one\n+   * always succeeds when {@link DoFn#getAllowedTimestampSkew()} is equal to\n+   * {@link Long#MAX_VALUE} milliseconds.\n+   */\n+  @Test\n+  public void testInfiniteSkew() {\n+    SkewingDoFn fn = new SkewingDoFn(Duration.millis(Long.MAX_VALUE));\n+    DoFnRunner<Duration, Duration> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            new ListOutputManager(),\n+            new TupleTag<Duration>(),\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    runner.startBundle();\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(Duration.millis(1L), new Instant(0)));\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(\n+            Duration.millis(1L), BoundedWindow.TIMESTAMP_MIN_VALUE.plus(Duration.millis(1))));\n+    runner.processElement(\n+        WindowedValue.timestampedValueInGlobalWindow(\n+            // This is the maximum amount a timestamp in beam can move (from the maximum timestamp\n+            // to the minimum timestamp).\n+            Duration.millis(BoundedWindow.TIMESTAMP_MAX_VALUE.getMillis())\n+                .minus(Duration.millis(BoundedWindow.TIMESTAMP_MIN_VALUE.getMillis())),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE));\n+  }\n+\n   static class ThrowingDoFn extends DoFn<String, String> {\n     final Exception exceptionToThrow = new UnsupportedOperationException(\"Expected exception\");\n \n@@ -296,4 +410,35 @@ public void onTimer(OnTimerContext context) {\n               context.timeDomain()));\n     }\n   }\n+\n+\n+  /**\n+   * A {@link DoFn} that outputs elements with timestamp equal to the input timestamp minus the\n+   * input element.\n+   */\n+  private static class SkewingDoFn extends DoFn<Duration, Duration> {\n+    private final Duration allowedSkew;\n+\n+    private SkewingDoFn(Duration allowedSkew) {\n+      this.allowedSkew = allowedSkew;\n+    }\n+\n+    @ProcessElement\n+    public void processElement(ProcessContext context) {\n+      context.outputWithTimestamp(context.element(), context.timestamp().minus(context.element()));\n+    }\n+\n+    @Override\n+    public Duration getAllowedTimestampSkew() {\n+      return allowedSkew;\n+    }\n+  }\n+\n+  private static class ListOutputManager implements OutputManager {\n+    private ListMultimap<TupleTag<?>, WindowedValue<?>> outputs = ArrayListMultimap.create();\n+    @Override\n+    public <T> void output(TupleTag<T> tag, WindowedValue<T> output) {\n+      outputs.put(tag, output);\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "sha": "4ae533257b556a8759ff38516484caee866bda5b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java",
                "patch": "@@ -64,10 +64,10 @@ public void testSystemDoFnInternalExceptionsNotWrapped() {\n \n   private DoFnRunner<String, String> createRunner(OldDoFn<String, String> fn) {\n     // Pass in only necessary parameters for the test\n-    List<TupleTag<?>> sideOutputTags = Arrays.asList();\n+    List<TupleTag<?>> additionalOutputTags = Arrays.asList();\n     StepContext context = mock(StepContext.class);\n     return new SimpleOldDoFnRunner<>(\n-          null, fn, null, null, null, sideOutputTags, context, null, null);\n+        null, fn, null, null, null, additionalOutputTags, context, null, null);\n   }\n \n   static class ThrowingDoFn extends OldDoFn<String, String> {",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleOldDoFnRunnerTest.java",
                "sha": "8ded2dcdfb3997cbe08e4522227cba993ab28bb2",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 10,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java",
                "patch": "@@ -55,10 +55,10 @@\n import org.mockito.MockitoAnnotations;\n \n /**\n- * Tests for {@link PushbackSideInputDoFnRunner}.\n+ * Tests for {@link SimplePushbackSideInputDoFnRunner}.\n  */\n @RunWith(JUnit4.class)\n-public class PushbackSideInputDoFnRunnerTest {\n+public class SimplePushbackSideInputDoFnRunnerTest {\n   @Mock private ReadyCheckingSideInputReader reader;\n   private TestDoFnRunner<Integer, Integer> underlying;\n   private PCollectionView<Integer> singletonView;\n@@ -78,10 +78,10 @@ public void setup() {\n     underlying = new TestDoFnRunner<>();\n   }\n \n-  private PushbackSideInputDoFnRunner<Integer, Integer> createRunner(\n+  private SimplePushbackSideInputDoFnRunner<Integer, Integer> createRunner(\n       ImmutableList<PCollectionView<?>> views) {\n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n-        PushbackSideInputDoFnRunner.create(underlying, views, reader);\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n+        SimplePushbackSideInputDoFnRunner.create(underlying, views, reader);\n     runner.startBundle();\n     return runner;\n   }\n@@ -102,7 +102,7 @@ public void processElementSideInputNotReady() {\n     when(reader.isReady(Mockito.eq(singletonView), Mockito.any(BoundedWindow.class)))\n         .thenReturn(false);\n \n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of(singletonView));\n \n     WindowedValue<Integer> oneWindow =\n@@ -122,7 +122,7 @@ public void processElementSideInputNotReadyMultipleWindows() {\n     when(reader.isReady(Mockito.eq(singletonView), Mockito.any(BoundedWindow.class)))\n         .thenReturn(false);\n \n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of(singletonView));\n \n     WindowedValue<Integer> multiWindow =\n@@ -150,7 +150,7 @@ public void processElementSideInputNotReadySomeWindows() {\n                 org.mockito.AdditionalMatchers.not(Mockito.eq(GlobalWindow.INSTANCE))))\n         .thenReturn(true);\n \n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of(singletonView));\n \n     IntervalWindow littleWindow = new IntervalWindow(new Instant(-500L), new Instant(0L));\n@@ -181,7 +181,7 @@ public void processElementSideInputReadyAllWindows() {\n         .thenReturn(true);\n \n     ImmutableList<PCollectionView<?>> views = ImmutableList.<PCollectionView<?>>of(singletonView);\n-    PushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(views);\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner = createRunner(views);\n \n     WindowedValue<Integer> multiWindow =\n         WindowedValue.of(\n@@ -202,7 +202,7 @@ public void processElementSideInputReadyAllWindows() {\n \n   @Test\n   public void processElementNoSideInputs() {\n-    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+    SimplePushbackSideInputDoFnRunner<Integer, Integer> runner =\n         createRunner(ImmutableList.<PCollectionView<?>>of());\n \n     WindowedValue<Integer> multiWindow =",
                "previous_filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SimplePushbackSideInputDoFnRunnerTest.java",
                "sha": "ba3f9263f8848199650f7466776c2938df9c6e07",
                "status": "renamed"
            },
            {
                "additions": 117,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "changes": 332,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 215,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "patch": "@@ -17,11 +17,9 @@\n  */\n package org.apache.beam.runners.core;\n \n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.resume;\n-import static org.apache.beam.sdk.transforms.DoFn.ProcessContinuation.stop;\n-import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n import static org.hamcrest.Matchers.hasItem;\n+import static org.hamcrest.Matchers.hasItems;\n import static org.hamcrest.Matchers.not;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -32,32 +30,41 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.List;\n import java.util.NoSuchElementException;\n import java.util.concurrent.Executors;\n import javax.annotation.Nullable;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.BigEndianIntegerCoder;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.InstantCoder;\n import org.apache.beam.sdk.coders.SerializableCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.testing.ValueInSingleWindow;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.BoundedPerElement;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n import org.apache.beam.sdk.transforms.DoFnTester;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.splittabledofn.HasDefaultTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRange;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.apache.beam.sdk.values.TupleTagList;\n+import org.apache.beam.sdk.values.ValueInSingleWindow;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.junit.Rule;\n@@ -72,10 +79,20 @@\n   private static final Duration MAX_BUNDLE_DURATION = Duration.standardSeconds(5);\n \n   // ----------------- Tests for whether the transform sets boundedness correctly --------------\n-  private static class SomeRestriction implements Serializable {}\n+  private static class SomeRestriction\n+      implements Serializable, HasDefaultTracker<SomeRestriction, SomeRestrictionTracker> {\n+    @Override\n+    public SomeRestrictionTracker newTracker() {\n+      return new SomeRestrictionTracker(this);\n+    }\n+  }\n \n   private static class SomeRestrictionTracker implements RestrictionTracker<SomeRestriction> {\n-    private final SomeRestriction someRestriction = new SomeRestriction();\n+    private final SomeRestriction someRestriction;\n+\n+    public SomeRestrictionTracker(SomeRestriction someRestriction) {\n+      this.someRestriction = someRestriction;\n+    }\n \n     @Override\n     public SomeRestriction currentRestriction() {\n@@ -86,8 +103,12 @@ public SomeRestriction currentRestriction() {\n     public SomeRestriction checkpoint() {\n       return someRestriction;\n     }\n+\n+    @Override\n+    public void checkDone() {}\n   }\n \n+  @BoundedPerElement\n   private static class BoundedFakeFn extends DoFn<Integer, String> {\n     @ProcessElement\n     public void processElement(ProcessContext context, SomeRestrictionTracker tracker) {}\n@@ -96,29 +117,17 @@ public void processElement(ProcessContext context, SomeRestrictionTracker tracke\n     public SomeRestriction getInitialRestriction(Integer element) {\n       return null;\n     }\n-\n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return null;\n-    }\n   }\n \n+  @UnboundedPerElement\n   private static class UnboundedFakeFn extends DoFn<Integer, String> {\n     @ProcessElement\n-    public ProcessContinuation processElement(\n-        ProcessContext context, SomeRestrictionTracker tracker) {\n-      return stop();\n-    }\n+    public void processElement(ProcessContext context, SomeRestrictionTracker tracker) {}\n \n     @GetInitialRestriction\n     public SomeRestriction getInitialRestriction(Integer element) {\n       return null;\n     }\n-\n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return null;\n-    }\n   }\n \n   private static PCollection<Integer> makeUnboundedCollection(Pipeline pipeline) {\n@@ -135,7 +144,7 @@ public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n \n   private static final TupleTag<String> MAIN_OUTPUT_TAG = new TupleTag<String>() {};\n \n-  private ParDo.BoundMulti<Integer, String> makeParDo(DoFn<Integer, String> fn) {\n+  private ParDo.MultiOutput<Integer, String> makeParDo(DoFn<Integer, String> fn) {\n     return ParDo.of(fn).withOutputTags(MAIN_OUTPUT_TAG, TupleTagList.empty());\n   }\n \n@@ -186,11 +195,6 @@ public void testBoundednessForUnboundedFn() {\n \n   // ------------------------------- Tests for ProcessFn ---------------------------------\n \n-  enum WindowExplosion {\n-    EXPLODE_WINDOWS,\n-    DO_NOT_EXPLODE_WINDOWS\n-  }\n-\n   /**\n    * A helper for testing {@link SplittableParDo.ProcessFn} on 1 element (but possibly over multiple\n    * {@link DoFn.ProcessElement} calls).\n@@ -204,7 +208,7 @@ public void testBoundednessForUnboundedFn() {\n     private Instant currentProcessingTime;\n \n     private InMemoryTimerInternals timerInternals;\n-    private InMemoryStateInternals<String> stateInternals;\n+    private TestInMemoryStateInternals<String> stateInternals;\n \n     ProcessFnTester(\n         Instant currentProcessingTime,\n@@ -214,12 +218,16 @@ public void testBoundednessForUnboundedFn() {\n         int maxOutputsPerBundle,\n         Duration maxBundleDuration)\n         throws Exception {\n+      // The exact windowing strategy doesn't matter in this test, but it should be able to\n+      // encode IntervalWindow's because that's what all tests here use.\n+      WindowingStrategy<InputT, BoundedWindow> windowingStrategy =\n+          (WindowingStrategy) WindowingStrategy.of(FixedWindows.of(Duration.standardSeconds(1)));\n       final SplittableParDo.ProcessFn<InputT, OutputT, RestrictionT, TrackerT> processFn =\n           new SplittableParDo.ProcessFn<>(\n-              fn, inputCoder, restrictionCoder, IntervalWindow.getCoder());\n+              fn, inputCoder, restrictionCoder, windowingStrategy);\n       this.tester = DoFnTester.of(processFn);\n       this.timerInternals = new InMemoryTimerInternals();\n-      this.stateInternals = InMemoryStateInternals.forKey(\"dummy\");\n+      this.stateInternals = new TestInMemoryStateInternals<>(\"dummy\");\n       processFn.setStateInternalsFactory(\n           new StateInternalsFactory<String>() {\n             @Override\n@@ -281,24 +289,13 @@ void startElement(InputT element, RestrictionT restriction) throws Exception {\n               ElementAndRestriction.of(element, restriction),\n               currentProcessingTime,\n               GlobalWindow.INSTANCE,\n-              PaneInfo.ON_TIME_AND_ONLY_FIRING),\n-          WindowExplosion.DO_NOT_EXPLODE_WINDOWS);\n+              PaneInfo.ON_TIME_AND_ONLY_FIRING));\n     }\n \n-    void startElement(\n-        WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue,\n-        WindowExplosion explosion)\n+    void startElement(WindowedValue<ElementAndRestriction<InputT, RestrictionT>> windowedValue)\n         throws Exception {\n-      switch (explosion) {\n-        case EXPLODE_WINDOWS:\n-          tester.processElement(\n-              KeyedWorkItems.elementsWorkItem(\"key\", windowedValue.explodeWindows()));\n-          break;\n-        case DO_NOT_EXPLODE_WINDOWS:\n-          tester.processElement(\n-              KeyedWorkItems.elementsWorkItem(\"key\", Arrays.asList(windowedValue)));\n-          break;\n-      }\n+      tester.processElement(\n+          KeyedWorkItems.elementsWorkItem(\"key\", Collections.singletonList(windowedValue)));\n     }\n \n     /**\n@@ -331,6 +328,9 @@ boolean advanceProcessingTimeBy(Duration duration) throws Exception {\n       return tester.takeOutputElements();\n     }\n \n+    public Instant getWatermarkHold() {\n+      return stateInternals.earliestWatermarkHold();\n+    }\n   }\n \n   private static class OutputWindowedValueToDoFnTester<OutputT>\n@@ -347,13 +347,13 @@ public void outputWindowedValue(\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      sideOutputWindowedValue(tester.getMainOutputTag(), output, timestamp, windows, pane);\n+      outputWindowedValue(tester.getMainOutputTag(), output, timestamp, windows, pane);\n     }\n \n     @Override\n-    public <SideOutputT> void sideOutputWindowedValue(\n-        TupleTag<SideOutputT> tag,\n-        SideOutputT output,\n+    public <AdditionalOutputT> void outputWindowedValue(\n+        TupleTag<AdditionalOutputT> tag,\n+        AdditionalOutputT output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n@@ -376,218 +376,125 @@ public void process(ProcessContext c, SomeRestrictionTracker tracker) {\n     public SomeRestriction getInitialRestriction(Integer elem) {\n       return new SomeRestriction();\n     }\n-\n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return new SomeRestrictionTracker();\n-    }\n   }\n \n   @Test\n-  public void testTrivialProcessFnPropagatesOutputsWindowsAndTimestamp() throws Exception {\n-    // Tests that ProcessFn correctly propagates windows and timestamp of the element\n+  public void testTrivialProcessFnPropagatesOutputWindowAndTimestamp() throws Exception {\n+    // Tests that ProcessFn correctly propagates the window and timestamp of the element\n     // inside the KeyedWorkItem.\n     // The underlying DoFn is actually monolithic, so this doesn't test splitting.\n     DoFn<Integer, String> fn = new ToStringFn();\n \n     Instant base = Instant.now();\n \n-    IntervalWindow w1 =\n+    IntervalWindow w =\n         new IntervalWindow(\n             base.minus(Duration.standardMinutes(1)), base.plus(Duration.standardMinutes(1)));\n-    IntervalWindow w2 =\n-        new IntervalWindow(\n-            base.minus(Duration.standardMinutes(2)), base.plus(Duration.standardMinutes(2)));\n-    IntervalWindow w3 =\n-        new IntervalWindow(\n-            base.minus(Duration.standardMinutes(3)), base.plus(Duration.standardMinutes(3)));\n-\n-    for (WindowExplosion explosion : WindowExplosion.values()) {\n-      ProcessFnTester<Integer, String, SomeRestriction, SomeRestrictionTracker> tester =\n-          new ProcessFnTester<>(\n-              base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeRestriction.class),\n-              MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);\n-      tester.startElement(\n-          WindowedValue.of(\n-              ElementAndRestriction.of(42, new SomeRestriction()),\n-              base,\n-              Arrays.asList(w1, w2, w3),\n-              PaneInfo.ON_TIME_AND_ONLY_FIRING),\n-          explosion);\n-\n-      for (IntervalWindow w : new IntervalWindow[] {w1, w2, w3}) {\n-        assertEquals(\n-            Arrays.asList(\n-                TimestampedValue.of(\"42a\", base),\n-                TimestampedValue.of(\"42b\", base),\n-                TimestampedValue.of(\"42c\", base)),\n-            tester.peekOutputElementsInWindow(w));\n-      }\n-    }\n+\n+    ProcessFnTester<Integer, String, SomeRestriction, SomeRestrictionTracker> tester =\n+        new ProcessFnTester<>(\n+            base,\n+            fn,\n+            BigEndianIntegerCoder.of(),\n+            SerializableCoder.of(SomeRestriction.class),\n+            MAX_OUTPUTS_PER_BUNDLE,\n+            MAX_BUNDLE_DURATION);\n+    tester.startElement(\n+        WindowedValue.of(\n+            ElementAndRestriction.of(42, new SomeRestriction()),\n+            base,\n+            Collections.singletonList(w),\n+            PaneInfo.ON_TIME_AND_ONLY_FIRING));\n+\n+    assertEquals(\n+        Arrays.asList(\n+            TimestampedValue.of(\"42a\", base),\n+            TimestampedValue.of(\"42b\", base),\n+            TimestampedValue.of(\"42c\", base)),\n+        tester.peekOutputElementsInWindow(w));\n   }\n \n-  /** A simple splittable {@link DoFn} that outputs the given element every 5 seconds forever. */\n-  private static class SelfInitiatedResumeFn extends DoFn<Integer, String> {\n+  private static class WatermarkUpdateFn extends DoFn<Instant, String> {\n     @ProcessElement\n-    public ProcessContinuation process(ProcessContext c, SomeRestrictionTracker tracker) {\n-      c.output(c.element().toString());\n-      return resume().withResumeDelay(Duration.standardSeconds(5)).withWatermark(c.timestamp());\n+    public void process(ProcessContext c, OffsetRangeTracker tracker) {\n+      for (long i = tracker.currentRestriction().getFrom(); tracker.tryClaim(i); ++i) {\n+        c.updateWatermark(c.element().plus(Duration.standardSeconds(i)));\n+        c.output(String.valueOf(i));\n+      }\n     }\n \n     @GetInitialRestriction\n-    public SomeRestriction getInitialRestriction(Integer elem) {\n-      return new SomeRestriction();\n+    public OffsetRange getInitialRestriction(Instant elem) {\n+      throw new IllegalStateException(\"Expected to be supplied explicitly in this test\");\n     }\n \n     @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return new SomeRestrictionTracker();\n+    public OffsetRangeTracker newTracker(OffsetRange range) {\n+      return new OffsetRangeTracker(range);\n     }\n   }\n \n   @Test\n-  public void testResumeSetsTimer() throws Exception {\n-    DoFn<Integer, String> fn = new SelfInitiatedResumeFn();\n+  public void testUpdatesWatermark() throws Exception {\n+    DoFn<Instant, String> fn = new WatermarkUpdateFn();\n     Instant base = Instant.now();\n-    ProcessFnTester<Integer, String, SomeRestriction, SomeRestrictionTracker> tester =\n-        new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeRestriction.class),\n-            MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);\n \n-    tester.startElement(42, new SomeRestriction());\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-\n-    // Should resume after 5 seconds: advancing by 3 seconds should have no effect.\n-    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertTrue(tester.takeOutputElements().isEmpty());\n-\n-    // 6 seconds should be enough - should invoke the fn again.\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-\n-    // Should again resume after 5 seconds: advancing by 3 seconds should again have no effect.\n-    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertTrue(tester.takeOutputElements().isEmpty());\n-\n-    // 6 seconds should again be enough.\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(3)));\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-  }\n-\n-  private static class SomeCheckpoint implements Serializable {\n-    private int firstUnprocessedIndex;\n-\n-    private SomeCheckpoint(int firstUnprocessedIndex) {\n-      this.firstUnprocessedIndex = firstUnprocessedIndex;\n-    }\n-  }\n-\n-  private static class SomeCheckpointTracker implements RestrictionTracker<SomeCheckpoint> {\n-    private SomeCheckpoint current;\n-    private boolean isActive = true;\n-\n-    private SomeCheckpointTracker(SomeCheckpoint current) {\n-      this.current = current;\n-    }\n+    ProcessFnTester<Instant, String, OffsetRange, OffsetRangeTracker> tester =\n+        new ProcessFnTester<>(\n+            base,\n+            fn,\n+            InstantCoder.of(),\n+            SerializableCoder.of(OffsetRange.class),\n+            3,\n+            MAX_BUNDLE_DURATION);\n \n-    @Override\n-    public SomeCheckpoint currentRestriction() {\n-      return current;\n-    }\n+    tester.startElement(base, new OffsetRange(0, 8));\n+    assertThat(tester.takeOutputElements(), hasItems(\"0\", \"1\", \"2\"));\n+    assertEquals(base.plus(Duration.standardSeconds(2)), tester.getWatermarkHold());\n \n-    public boolean tryUpdateCheckpoint(int firstUnprocessedIndex) {\n-      if (!isActive) {\n-        return false;\n-      }\n-      current = new SomeCheckpoint(firstUnprocessedIndex);\n-      return true;\n-    }\n+    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n+    assertThat(tester.takeOutputElements(), hasItems(\"3\", \"4\", \"5\"));\n+    assertEquals(base.plus(Duration.standardSeconds(5)), tester.getWatermarkHold());\n \n-    @Override\n-    public SomeCheckpoint checkpoint() {\n-      isActive = false;\n-      return current;\n-    }\n+    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n+    assertThat(tester.takeOutputElements(), hasItems(\"6\", \"7\"));\n+    assertEquals(null, tester.getWatermarkHold());\n   }\n \n   /**\n-   * A splittable {@link DoFn} that generates the sequence [init, init + total) in batches of given\n-   * size.\n+   * A splittable {@link DoFn} that generates the sequence [init, init + total).\n    */\n   private static class CounterFn extends DoFn<Integer, String> {\n-    private final int numTotalOutputs;\n-    private final int numOutputsPerCall;\n-\n-    private CounterFn(int numTotalOutputs, int numOutputsPerCall) {\n-      this.numTotalOutputs = numTotalOutputs;\n-      this.numOutputsPerCall = numOutputsPerCall;\n-    }\n-\n     @ProcessElement\n-    public ProcessContinuation process(ProcessContext c, SomeCheckpointTracker tracker) {\n-      int start = tracker.currentRestriction().firstUnprocessedIndex;\n-      for (int i = 0; i < numOutputsPerCall; ++i) {\n-        int index = start + i;\n-        if (!tracker.tryUpdateCheckpoint(index + 1)) {\n-          return resume();\n-        }\n-        if (index >= numTotalOutputs) {\n-          return stop();\n-        }\n-        c.output(String.valueOf(c.element() + index));\n+    public void process(ProcessContext c, OffsetRangeTracker tracker) {\n+      for (long i = tracker.currentRestriction().getFrom();\n+          tracker.tryClaim(i); ++i) {\n+        c.output(String.valueOf(c.element() + i));\n       }\n-      return resume();\n     }\n \n     @GetInitialRestriction\n-    public SomeCheckpoint getInitialRestriction(Integer elem) {\n+    public OffsetRange getInitialRestriction(Integer elem) {\n       throw new UnsupportedOperationException(\"Expected to be supplied explicitly in this test\");\n     }\n-\n-    @NewTracker\n-    public SomeCheckpointTracker newTracker(SomeCheckpoint restriction) {\n-      return new SomeCheckpointTracker(restriction);\n-    }\n-  }\n-\n-  @Test\n-  public void testResumeCarriesOverState() throws Exception {\n-    DoFn<Integer, String> fn = new CounterFn(3, 1);\n-    Instant base = Instant.now();\n-    ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester =\n-        new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class),\n-            MAX_OUTPUTS_PER_BUNDLE, MAX_BUNDLE_DURATION);\n-\n-    tester.startElement(42, new SomeCheckpoint(0));\n-    assertThat(tester.takeOutputElements(), contains(\"42\"));\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n-    assertThat(tester.takeOutputElements(), contains(\"43\"));\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n-    assertThat(tester.takeOutputElements(), contains(\"44\"));\n-    assertTrue(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n-    // After outputting all 3 items, should not output anything more.\n-    assertEquals(0, tester.takeOutputElements().size());\n-    // Should also not ask to resume.\n-    assertFalse(tester.advanceProcessingTimeBy(Duration.standardSeconds(1)));\n   }\n \n   @Test\n   public void testCheckpointsAfterNumOutputs() throws Exception {\n     int max = 100;\n-    // Create an fn that attempts to 2x output more than checkpointing allows.\n-    DoFn<Integer, String> fn = new CounterFn(2 * max + max / 2, 2 * max);\n+    DoFn<Integer, String> fn = new CounterFn();\n     Instant base = Instant.now();\n     int baseIndex = 42;\n \n-    ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester =\n+    ProcessFnTester<Integer, String, OffsetRange, OffsetRangeTracker> tester =\n         new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class),\n+            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class),\n             max, MAX_BUNDLE_DURATION);\n \n     List<String> elements;\n \n-    tester.startElement(baseIndex, new SomeCheckpoint(0));\n+    // Create an fn that attempts to 2x output more than checkpointing allows.\n+    tester.startElement(baseIndex, new OffsetRange(0, 2 * max + max / 2));\n     elements = tester.takeOutputElements();\n     assertEquals(max, elements.size());\n     // Should output the range [0, max)\n@@ -617,18 +524,18 @@ public void testCheckpointsAfterDuration() throws Exception {\n     // But bound bundle duration - the bundle should terminate.\n     Duration maxBundleDuration = Duration.standardSeconds(1);\n     // Create an fn that attempts to 2x output more than checkpointing allows.\n-    DoFn<Integer, String> fn = new CounterFn(max, max);\n+    DoFn<Integer, String> fn = new CounterFn();\n     Instant base = Instant.now();\n     int baseIndex = 42;\n \n-    ProcessFnTester<Integer, String, SomeCheckpoint, SomeCheckpointTracker> tester =\n+    ProcessFnTester<Integer, String, OffsetRange, OffsetRangeTracker> tester =\n         new ProcessFnTester<>(\n-            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(SomeCheckpoint.class),\n+            base, fn, BigEndianIntegerCoder.of(), SerializableCoder.of(OffsetRange.class),\n             max, maxBundleDuration);\n \n     List<String> elements;\n \n-    tester.startElement(baseIndex, new SomeCheckpoint(0));\n+    tester.startElement(baseIndex, new OffsetRange(0, Long.MAX_VALUE));\n     // Bundle should terminate, and should do at least some processing.\n     elements = tester.takeOutputElements();\n     assertFalse(elements.isEmpty());\n@@ -658,11 +565,6 @@ public SomeRestriction getInitialRestriction(Integer element) {\n       return new SomeRestriction();\n     }\n \n-    @NewTracker\n-    public SomeRestrictionTracker newTracker(SomeRestriction restriction) {\n-      return new SomeRestrictionTracker();\n-    }\n-\n     @Setup\n     public void setup() {\n       assertEquals(State.BEFORE_SETUP, state);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "sha": "1a444534dc29d169afcf874dcd51696089bff199",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 93,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java",
                "patch": "@@ -24,26 +24,20 @@\n \n import com.google.common.base.MoreObjects;\n import java.util.Collections;\n-import java.util.Map;\n import org.apache.beam.runners.core.BaseExecutionContext.StepContext;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.VarIntCoder;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.NullSideInputReader;\n-import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n-import org.apache.beam.sdk.util.state.State;\n import org.apache.beam.sdk.util.state.StateSpec;\n import org.apache.beam.sdk.util.state.StateSpecs;\n import org.apache.beam.sdk.util.state.ValueState;\n@@ -124,8 +118,8 @@ public void testLateDropping() throws Exception {\n         mockStepContext,\n         aggregatorFactory,\n         WINDOWING_STRATEGY,\n-        new TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n-        new StateInternalsStateCleaner<>(\n+        new StatefulDoFnRunner.TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n+        new StatefulDoFnRunner.StateInternalsStateCleaner<>(\n             fn, stateInternals, (Coder) WINDOWING_STRATEGY.getWindowFn().windowCoder()));\n \n     runner.startBundle();\n@@ -153,8 +147,8 @@ public void testGarbageCollect() throws Exception {\n         mockStepContext,\n         aggregatorFactory,\n         WINDOWING_STRATEGY,\n-        new TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n-        new StateInternalsStateCleaner<>(\n+        new StatefulDoFnRunner.TimeInternalsCleanupTimer(timerInternals, WINDOWING_STRATEGY),\n+        new StatefulDoFnRunner.StateInternalsStateCleaner<>(\n             fn, stateInternals, (Coder) WINDOWING_STRATEGY.getWindowFn().windowCoder()));\n \n     Instant elementTime = new Instant(1);\n@@ -179,8 +173,16 @@ public void testGarbageCollect() throws Exception {\n         2, (int) stateInternals.state(windowNamespace(WINDOW_2), stateTag).read());\n \n     // advance watermark past end of WINDOW_1 + allowed lateness\n+    // the cleanup timer is set to window.maxTimestamp() + allowed lateness + 1\n+    // to ensure that state is still available when a user timer for window.maxTimestamp() fires\n     advanceInputWatermark(\n-        timerInternals, WINDOW_1.maxTimestamp().plus(ALLOWED_LATENESS + 1), runner);\n+        timerInternals,\n+        WINDOW_1.maxTimestamp()\n+            .plus(ALLOWED_LATENESS)\n+            .plus(StatefulDoFnRunner.TimeInternalsCleanupTimer.GC_DELAY_MS)\n+            .plus(1), // so the watermark is past the GC horizon, not on it\n+        runner);\n+\n     assertTrue(\n         stateInternals.isEmptyForTesting(\n             stateInternals.state(windowNamespace(WINDOW_1), stateTag)));\n@@ -190,7 +192,13 @@ public void testGarbageCollect() throws Exception {\n \n     // advance watermark past end of WINDOW_2 + allowed lateness\n     advanceInputWatermark(\n-        timerInternals, WINDOW_2.maxTimestamp().plus(ALLOWED_LATENESS + 1), runner);\n+        timerInternals,\n+        WINDOW_2.maxTimestamp()\n+            .plus(ALLOWED_LATENESS)\n+            .plus(StatefulDoFnRunner.TimeInternalsCleanupTimer.GC_DELAY_MS)\n+            .plus(1), // so the watermark is past the GC horizon, not on it\n+        runner);\n+\n     assertTrue(\n         stateInternals.isEmptyForTesting(\n             stateInternals.state(windowNamespace(WINDOW_2), stateTag)));\n@@ -263,85 +271,4 @@ public String getName() {\n       return Sum.ofLongs();\n     }\n   }\n-\n-  /**\n-   * A {@link StatefulDoFnRunner.CleanupTimer} implemented by TimerInternals.\n-   */\n-  public static class TimeInternalsCleanupTimer implements StatefulDoFnRunner.CleanupTimer {\n-\n-    public static final String GC_TIMER_ID = \"__StatefulParDoGcTimerId\";\n-\n-    private final TimerInternals timerInternals;\n-    private final WindowingStrategy<?, ?> windowingStrategy;\n-    private final Coder<BoundedWindow> windowCoder;\n-\n-    public TimeInternalsCleanupTimer(\n-        TimerInternals timerInternals,\n-        WindowingStrategy<?, ?> windowingStrategy) {\n-      this.windowingStrategy = windowingStrategy;\n-      WindowFn<?, ?> windowFn = windowingStrategy.getWindowFn();\n-      windowCoder = (Coder<BoundedWindow>) windowFn.windowCoder();\n-      this.timerInternals = timerInternals;\n-    }\n-\n-    @Override\n-    public Instant currentInputWatermarkTime() {\n-      return timerInternals.currentInputWatermarkTime();\n-    }\n-\n-    @Override\n-    public void setForWindow(BoundedWindow window) {\n-      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n-      timerInternals.setTimer(StateNamespaces.window(windowCoder, window),\n-          GC_TIMER_ID, gcTime, TimeDomain.EVENT_TIME);\n-    }\n-\n-    @Override\n-    public boolean isForWindow(\n-        String timerId,\n-        BoundedWindow window,\n-        Instant timestamp,\n-        TimeDomain timeDomain) {\n-      boolean isEventTimer = timeDomain.equals(TimeDomain.EVENT_TIME);\n-      Instant gcTime = window.maxTimestamp().plus(windowingStrategy.getAllowedLateness());\n-      return isEventTimer && GC_TIMER_ID.equals(timerId) && gcTime.equals(timestamp);\n-    }\n-  }\n-\n-  /**\n-   * A {@link StatefulDoFnRunner.StateCleaner} implemented by StateInternals.\n-   */\n-  public static class StateInternalsStateCleaner<W extends BoundedWindow>\n-      implements StatefulDoFnRunner.StateCleaner<W> {\n-\n-    private final DoFn<?, ?> fn;\n-    private final DoFnSignature signature;\n-    private final StateInternals<?> stateInternals;\n-    private final Coder<W> windowCoder;\n-\n-    public StateInternalsStateCleaner(\n-        DoFn<?, ?> fn,\n-        StateInternals<?> stateInternals,\n-        Coder<W> windowCoder) {\n-      this.fn = fn;\n-      this.signature = DoFnSignatures.getSignature(fn.getClass());\n-      this.stateInternals = stateInternals;\n-      this.windowCoder = windowCoder;\n-    }\n-\n-    @Override\n-    public void clearForWindow(W window) {\n-      for (Map.Entry<String, DoFnSignature.StateDeclaration> entry :\n-          signature.stateDeclarations().entrySet()) {\n-        try {\n-          StateSpec<?, ?> spec = (StateSpec<?, ?>) entry.getValue().field().get(fn);\n-          State state = stateInternals.state(StateNamespaces.window(windowCoder, window),\n-              StateTags.tagForSpec(entry.getKey(), (StateSpec) spec));\n-          state.clear();\n-        } catch (IllegalAccessException e) {\n-          throw new RuntimeException(e);\n-        }\n-      }\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/core-java/src/test/java/org/apache/beam/runners/core/StatefulDoFnRunnerTest.java",
                "sha": "46cbd7db266ce0af8e260f47a0e4ad0780b9b69b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/pom.xml",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 53,
                "filename": "runners/direct-java/pom.xml",
                "patch": "@@ -61,7 +61,7 @@\n         </configuration>\n         <executions>\n           <execution>\n-            <id>runnable-on-service-tests</id>\n+            <id>validates-runner-tests</id>\n             <phase>integration-test</phase>\n             <goals>\n               <goal>test</goal>\n@@ -81,58 +81,7 @@\n                   ]\n                 </beamTestPipelineOptions>\n               </systemPropertyVariables>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n-\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-shade-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <id>bundle-and-repackage</id>\n-            <phase>package</phase>\n-            <goals>\n-              <goal>shade</goal>\n-            </goals>\n-            <configuration>\n-              <shadeTestJar>true</shadeTestJar>\n-              <artifactSet>\n-                <includes>\n-                  <include>com.google.guava:guava</include>\n-                </includes>\n-              </artifactSet>\n-              <filters>\n-                <filter>\n-                  <artifact>*:*</artifact>\n-                  <excludes>\n-                    <exclude>META-INF/*.SF</exclude>\n-                    <exclude>META-INF/*.DSA</exclude>\n-                    <exclude>META-INF/*.RSA</exclude>\n-                  </excludes>\n-                </filter>\n-              </filters>\n-              <relocations>\n-                <!-- TODO: Once ready, change the following pattern to 'com'\n-                     only, exclude 'org.apache.beam.**', and remove\n-                     the second relocation. -->\n-                <relocation>\n-                  <pattern>com.google.common</pattern>\n-                  <excludes>\n-                    <!-- com.google.common is too generic, need to exclude guava-testlib -->\n-                    <exclude>com.google.common.**.testing.*</exclude>\n-                  </excludes>\n-                  <shadedPattern>org.apache.beam.runners.direct.repackaged.com.google.common</shadedPattern>\n-                </relocation>\n-                <relocation>\n-                  <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>org.apache.beam.runners.direct.repackaged.com.google.thirdparty</shadedPattern>\n-                </relocation>\n-              </relocations>\n-              <transformers>\n-                <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\n-              </transformers>\n+              <threadCount>4</threadCount>\n             </configuration>\n           </execution>\n         </executions>",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/pom.xml",
                "sha": "fc28fd6ea667ff032de8885c67a88c44453f35d8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java",
                "patch": "@@ -117,7 +117,7 @@ public BoundedReadEvaluator(\n         ExecutorService executor) {\n       this.evaluationContext = evaluationContext;\n       this.outputPCollection =\n-          (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs()).getValue();\n+          (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs().values());\n       this.resultBuilder = StepTransformResult.withoutHold(transform);\n       this.minimumDynamicSplitSize = minimumDynamicSplitSize;\n       this.produceSplitExecutor = executor;\n@@ -196,7 +196,7 @@ public void processElement(WindowedValue<BoundedSourceShard<OutputT>> element)\n       long estimatedBytes = source.getEstimatedSizeBytes(options);\n       long bytesPerBundle = estimatedBytes / targetParallelism;\n       List<? extends BoundedSource<T>> bundles =\n-          source.splitIntoBundles(bytesPerBundle, options);\n+          source.split(bytesPerBundle, options);\n       ImmutableList.Builder<CommittedBundle<BoundedSourceShard<T>>> shards =\n           ImmutableList.builder();\n       for (BoundedSource<T> bundle : bundles) {",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactory.java",
                "sha": "0c2afe8c013046434721bb2139881b436f377f69",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 24,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "patch": "@@ -26,7 +26,7 @@\n import java.util.Map;\n import javax.annotation.Nullable;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryBag;\n-import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryCombiningValue;\n+import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryCombiningState;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryMap;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemorySet;\n import org.apache.beam.runners.core.InMemoryStateInternals.InMemoryState;\n@@ -45,8 +45,8 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.util.CombineFnUtil;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.State;\n@@ -306,19 +306,18 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n             if (containedInUnderlying(namespace, address)) {\n               @SuppressWarnings(\"unchecked\")\n-              InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT, OutputT>>\n-                  existingState = (\n-                  InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n-                                            OutputT>>) underlying.get().get(namespace, address, c);\n+              InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>> existingState =\n+                  (InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>>)\n+                      underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryCombiningValue<>(\n+              return new InMemoryCombiningState<>(\n                   key, combineFn.asKeyedFn());\n             }\n           }\n@@ -367,27 +366,26 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindKeyedCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n             if (containedInUnderlying(namespace, address)) {\n               @SuppressWarnings(\"unchecked\")\n-              InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT, OutputT>>\n-                  existingState = (\n-                  InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n-                                            OutputT>>) underlying.get().get(namespace, address, c);\n+              InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>> existingState =\n+                  (InMemoryState<? extends CombiningState<InputT, AccumT, OutputT>>)\n+                      underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryCombiningValue<>(key, combineFn);\n+              return new InMemoryCombiningState<>(key, combineFn);\n             }\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n           bindKeyedCombiningValueWithContext(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n             return bindKeyedCombiningValue(\n@@ -449,9 +447,9 @@ public Instant readThroughAndGetEarliestHold(StateTable<K> readTo) {\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder, CombineFn<InputT, AccumT, OutputT> combineFn) {\n             return underlying.get(namespace, address, c);\n           }\n@@ -476,18 +474,18 @@ public Instant readThroughAndGetEarliestHold(StateTable<K> readTo) {\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n               bindKeyedCombiningValue(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFn<? super K, InputT, AccumT, OutputT> combineFn) {\n             return underlying.get(namespace, address, c);\n           }\n \n           @Override\n-          public <InputT, AccumT, OutputT> AccumulatorCombiningState<InputT, AccumT, OutputT>\n+          public <InputT, AccumT, OutputT> CombiningState<InputT, AccumT, OutputT>\n           bindKeyedCombiningValueWithContext(\n-                  StateTag<? super K, AccumulatorCombiningState<InputT, AccumT, OutputT>> address,\n+                  StateTag<? super K, CombiningState<InputT, AccumT, OutputT>> address,\n                   Coder<AccumT> accumCoder,\n                   KeyedCombineFnWithContext<? super K, InputT, AccumT, OutputT> combineFn) {\n             return bindKeyedCombiningValue(",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "sha": "0665812e685f12ad503e7a73ca6ba92cd09c38ae",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "patch": "@@ -19,8 +19,9 @@\n \n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.SplittableParDo.GBKIntoKeyedWorkItems;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n-import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n \n@@ -33,8 +34,15 @@\n         PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>,\n         GBKIntoKeyedWorkItems<KeyT, InputT>> {\n   @Override\n-  public PTransform<PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>>\n-      getReplacementTransform(GBKIntoKeyedWorkItems<KeyT, InputT> transform) {\n-    return new DirectGroupByKey.DirectGroupByKeyOnly<>();\n+  public PTransformReplacement<\n+          PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>>\n+      getReplacementTransform(\n+          AppliedPTransform<\n+                  PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>,\n+                  GBKIntoKeyedWorkItems<KeyT, InputT>>\n+              transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        new DirectGroupByKey.DirectGroupByKeyOnly<KeyT, InputT>());\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "sha": "112024341aa93bb052748cc386388c4c203b750d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n \n /**\n  * Tracks the {@link AppliedPTransform AppliedPTransforms} that consume each {@link PValue} in the\n@@ -83,8 +82,8 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n     if (node.getInputs().isEmpty()) {\n       rootTransforms.add(appliedTransform);\n     } else {\n-      for (TaggedPValue value : node.getInputs()) {\n-        primitiveConsumers.put(value.getValue(), appliedTransform);\n+      for (PValue value : node.getInputs().values()) {\n+        primitiveConsumers.put(value, appliedTransform);\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "sha": "c3421363bbe8a3a1a4161fe612cbd01d6e9265e2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.KeyedWorkItemCoder;\n+import org.apache.beam.runners.core.construction.ForwardingPTransform;\n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.IterableCoder;",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "sha": "36050023b399c6157fe7e06babe2c17bf6f22e81",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "patch": "@@ -17,10 +17,11 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.GroupByKey;\n-import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n \n@@ -29,8 +30,13 @@\n     extends SingleInputOutputOverrideFactory<\n         PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupByKey<K, V>> {\n   @Override\n-  public PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> getReplacementTransform(\n-      GroupByKey<K, V> transform) {\n-    return new DirectGroupByKey<>(transform);\n+  public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+      getReplacementTransform(\n+          AppliedPTransform<\n+                  PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupByKey<K, V>>\n+              transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        new DirectGroupByKey<>(transform.getTransform()));\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "sha": "4eb036300c4e8737267faa0e2d103abbdd937ad6",
                "status": "modified"
            },
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java",
                "changes": 129,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 71,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java",
                "patch": "@@ -20,12 +20,10 @@\n import static java.util.Arrays.asList;\n \n import com.google.auto.value.AutoValue;\n-import com.google.common.base.Objects;\n import com.google.common.collect.ImmutableList;\n import java.util.ArrayList;\n import java.util.Map;\n import java.util.Map.Entry;\n-import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutorService;\n@@ -35,9 +33,11 @@\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.metrics.DistributionData;\n import org.apache.beam.sdk.metrics.DistributionResult;\n+import org.apache.beam.sdk.metrics.GaugeData;\n+import org.apache.beam.sdk.metrics.GaugeResult;\n+import org.apache.beam.sdk.metrics.MetricFiltering;\n import org.apache.beam.sdk.metrics.MetricKey;\n import org.apache.beam.sdk.metrics.MetricName;\n-import org.apache.beam.sdk.metrics.MetricNameFilter;\n import org.apache.beam.sdk.metrics.MetricQueryResults;\n import org.apache.beam.sdk.metrics.MetricResult;\n import org.apache.beam.sdk.metrics.MetricResults;\n@@ -195,6 +195,28 @@ public DistributionResult extract(DistributionData data) {\n         }\n       };\n \n+  private static final MetricAggregation<GaugeData, GaugeResult> GAUGE =\n+      new MetricAggregation<GaugeData, GaugeResult>() {\n+        @Override\n+        public GaugeData zero() {\n+          return GaugeData.empty();\n+        }\n+\n+        @Override\n+        public GaugeData combine(Iterable<GaugeData> updates) {\n+          GaugeData result = GaugeData.empty();\n+          for (GaugeData update : updates) {\n+            result = result.combine(update);\n+          }\n+          return result;\n+        }\n+\n+        @Override\n+        public GaugeResult extract(GaugeData data) {\n+          return data.extractResult();\n+        }\n+      };\n+\n   /** The current values of counters in memory. */\n   private MetricsMap<MetricKey, DirectMetric<Long, Long>> counters =\n       new MetricsMap<>(new MetricsMap.Factory<MetricKey, DirectMetric<Long, Long>>() {\n@@ -212,13 +234,23 @@ public DistributionResult extract(DistributionData data) {\n           return new DirectMetric<>(DISTRIBUTION);\n         }\n       });\n+  private MetricsMap<MetricKey, DirectMetric<GaugeData, GaugeResult>> gauges =\n+      new MetricsMap<>(\n+          new MetricsMap.Factory<MetricKey, DirectMetric<GaugeData, GaugeResult>>() {\n+            @Override\n+            public DirectMetric<GaugeData, GaugeResult> createInstance(\n+                MetricKey unusedKey) {\n+              return new DirectMetric<>(GAUGE);\n+            }\n+          });\n \n   @AutoValue\n   abstract static class DirectMetricQueryResults implements MetricQueryResults {\n     public static MetricQueryResults create(\n         Iterable<MetricResult<Long>> counters,\n-        Iterable<MetricResult<DistributionResult>> distributions) {\n-      return new AutoValue_DirectMetrics_DirectMetricQueryResults(counters, distributions);\n+        Iterable<MetricResult<DistributionResult>> distributions,\n+        Iterable<MetricResult<GaugeResult>> gauges) {\n+      return new AutoValue_DirectMetrics_DirectMetricQueryResults(counters, distributions, gauges);\n     }\n   }\n \n@@ -250,15 +282,22 @@ public MetricQueryResults queryMetrics(MetricsFilter filter) {\n         : distributions.entries()) {\n       maybeExtractResult(filter, distributionResults, distribution);\n     }\n+    ImmutableList.Builder<MetricResult<GaugeResult>> gaugeResults =\n+        ImmutableList.builder();\n+    for (Entry<MetricKey, DirectMetric<GaugeData, GaugeResult>> gauge\n+        : gauges.entries()) {\n+      maybeExtractResult(filter, gaugeResults, gauge);\n+    }\n \n-    return DirectMetricQueryResults.create(counterResults.build(), distributionResults.build());\n+    return DirectMetricQueryResults.create(counterResults.build(), distributionResults.build(),\n+        gaugeResults.build());\n   }\n \n   private <ResultT> void maybeExtractResult(\n       MetricsFilter filter,\n       ImmutableList.Builder<MetricResult<ResultT>> resultsBuilder,\n       Map.Entry<MetricKey, ? extends DirectMetric<?, ResultT>> entry) {\n-    if (matches(filter, entry.getKey())) {\n+    if (MetricFiltering.matches(filter, entry.getKey())) {\n       resultsBuilder.add(DirectMetricResult.create(\n           entry.getKey().metricName(),\n           entry.getKey().stepName(),\n@@ -267,70 +306,6 @@ public MetricQueryResults queryMetrics(MetricsFilter filter) {\n     }\n   }\n \n-  // Matching logic is implemented here rather than in MetricsFilter because we would like\n-  // MetricsFilter to act as a \"dumb\" value-object, with the possibility of replacing it with\n-  // a Proto/JSON/etc. schema object.\n-  private boolean matches(MetricsFilter filter, MetricKey key) {\n-    return matchesName(key.metricName(), filter.names())\n-        && matchesScope(key.stepName(), filter.steps());\n-  }\n-\n-  /**\n-  * {@code subPathMatches(haystack, needle)} returns true if {@code needle}\n-  * represents a path within {@code haystack}. For example, \"foo/bar\" is in \"a/foo/bar/b\",\n-  * but not \"a/fool/bar/b\" or \"a/foo/bart/b\".\n-  */\n-  public boolean subPathMatches(String haystack, String needle) {\n-    int location = haystack.indexOf(needle);\n-    int end = location + needle.length();\n-    if (location == -1) {\n-      return false;  // needle not found\n-    } else if (location != 0 && haystack.charAt(location - 1) != '/') {\n-      return false; // the first entry in needle wasn't exactly matched\n-    } else if (end != haystack.length() && haystack.charAt(end) != '/') {\n-      return false; // the last entry in needle wasn't exactly matched\n-    } else {\n-      return true;\n-    }\n-  }\n-\n-  /**\n-   * {@code matchesScope(actualScope, scopes)} returns true if the scope of a metric is matched\n-   * by any of the filters in {@code scopes}. A metric scope is a path of type \"A/B/D\". A\n-   * path is matched by a filter if the filter is equal to the path (e.g. \"A/B/D\", or\n-   * if it represents a subpath within it (e.g. \"A/B\" or \"B/D\", but not \"A/D\"). */\n-  public boolean matchesScope(String actualScope, Set<String> scopes) {\n-    if (scopes.isEmpty() || scopes.contains(actualScope)) {\n-      return true;\n-    }\n-\n-    // If there is no perfect match, a stage name-level match is tried.\n-    // This is done by a substring search over the levels of the scope.\n-    // e.g. a scope \"A/B/C/D\" is matched by \"A/B\", but not by \"A/C\".\n-    for (String scope : scopes) {\n-      if (subPathMatches(actualScope, scope)) {\n-        return true;\n-      }\n-    }\n-\n-    return false;\n-  }\n-\n-  private boolean matchesName(MetricName metricName, Set<MetricNameFilter> nameFilters) {\n-    if (nameFilters.isEmpty()) {\n-      return true;\n-    }\n-\n-    for (MetricNameFilter nameFilter : nameFilters) {\n-      if ((nameFilter.getName() == null || nameFilter.getName().equals(metricName.name()))\n-          && Objects.equal(metricName.namespace(), nameFilter.getNamespace())) {\n-        return true;\n-      }\n-    }\n-\n-    return false;\n-  }\n-\n   /** Apply metric updates that represent physical counter deltas to the current metric values. */\n   public void updatePhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n     for (MetricUpdate<Long> counter : updates.counterUpdates()) {\n@@ -340,6 +315,10 @@ public void updatePhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n       distributions.get(distribution.getKey())\n           .updatePhysical(bundle, distribution.getUpdate());\n     }\n+    for (MetricUpdate<GaugeData> gauge : updates.gaugeUpdates()) {\n+      gauges.get(gauge.getKey())\n+          .updatePhysical(bundle, gauge.getUpdate());\n+    }\n   }\n \n   public void commitPhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n@@ -350,6 +329,10 @@ public void commitPhysical(CommittedBundle<?> bundle, MetricUpdates updates) {\n       distributions.get(distribution.getKey())\n           .commitPhysical(bundle, distribution.getUpdate());\n     }\n+    for (MetricUpdate<GaugeData> gauge : updates.gaugeUpdates()) {\n+      gauges.get(gauge.getKey())\n+          .commitPhysical(bundle, gauge.getUpdate());\n+    }\n   }\n \n   /** Apply metric updates that represent new logical values from a bundle being committed. */\n@@ -361,5 +344,9 @@ public void commitLogical(CommittedBundle<?> bundle, MetricUpdates updates) {\n       distributions.get(distribution.getKey())\n           .commitLogical(bundle, distribution.getUpdate());\n     }\n+    for (MetricUpdate<GaugeData> gauge : updates.gaugeUpdates()) {\n+      gauges.get(gauge.getKey())\n+          .commitLogical(bundle, gauge.getUpdate());\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectMetrics.java",
                "sha": "fb126fb393b36fdd42a0b62ab4a629cd2c37bd34",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java",
                "patch": "@@ -27,17 +27,6 @@\n  * Options that can be used to configure the {@link org.apache.beam.runners.direct.DirectRunner}.\n  */\n public interface DirectOptions extends PipelineOptions, ApplicationNameOptions {\n-  @Default.Boolean(true)\n-  @Description(\n-      \"If the pipeline should shut down producers which have reached the maximum \"\n-          + \"representable watermark. If this is set to true, a pipeline in which all PTransforms \"\n-          + \"have reached the maximum watermark will be shut down, even if there are unbounded \"\n-          + \"sources that could produce additional (late) data. By default, if the pipeline \"\n-          + \"contains any unbounded PCollections, it will run until explicitly shut down.\")\n-  boolean isShutdownUnboundedProducersWithMaxWatermark();\n-\n-  void setShutdownUnboundedProducersWithMaxWatermark(boolean shutdown);\n-\n   @Default.Boolean(true)\n   @Description(\n       \"If the pipeline should block awaiting completion of the pipeline. If set to true, \"",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectOptions.java",
                "sha": "3b66cc661aae491342728030e31fc2b0be1f5525",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "changes": 132,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 69,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "patch": "@@ -22,11 +22,11 @@\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n-import java.io.IOException;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.EnumSet;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import javax.annotation.Nullable;\n@@ -35,7 +35,6 @@\n import org.apache.beam.runners.core.construction.PTransformMatchers;\n import org.apache.beam.runners.direct.DirectRunner.DirectPipelineResult;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.DirectTestStreamFactory;\n-import org.apache.beam.runners.direct.ViewEvaluatorFactory.ViewOverrideFactory;\n import org.apache.beam.sdk.AggregatorRetrievalException;\n import org.apache.beam.sdk.AggregatorValues;\n import org.apache.beam.sdk.Pipeline;\n@@ -45,21 +44,19 @@\n import org.apache.beam.sdk.metrics.MetricResults;\n import org.apache.beam.sdk.metrics.MetricsEnvironment;\n import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.runners.PTransformMatcher;\n-import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.runners.PTransformOverride;\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.ParDo.BoundMulti;\n+import org.apache.beam.sdk.transforms.ParDo.MultiOutput;\n import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n@@ -170,7 +167,7 @@\n   /** The set of {@link PTransform PTransforms} that execute a UDF. Useful for some enforcements. */\n   private static final Set<Class<? extends PTransform>> CONTAINS_UDF =\n       ImmutableSet.of(\n-          Read.Bounded.class, Read.Unbounded.class, ParDo.Bound.class, ParDo.BoundMulti.class);\n+          Read.Bounded.class, Read.Unbounded.class, ParDo.SingleOutput.class, MultiOutput.class);\n \n   enum Enforcement {\n     ENCODABILITY {\n@@ -223,8 +220,8 @@ public static BundleFactory bundleFactoryFor(Set<Enforcement> enforcements, Dire\n         enabledParDoEnforcements.add(ImmutabilityEnforcementFactory.create());\n       }\n       Collection<ModelEnforcementFactory> parDoEnforcements = enabledParDoEnforcements.build();\n-      enforcements.put(ParDo.Bound.class, parDoEnforcements);\n-      enforcements.put(ParDo.BoundMulti.class, parDoEnforcements);\n+      enforcements.put(ParDo.SingleOutput.class, parDoEnforcements);\n+      enforcements.put(MultiOutput.class, parDoEnforcements);\n       return enforcements.build();\n     }\n \n@@ -261,10 +258,7 @@ void setClockSupplier(Supplier<Clock> supplier) {\n \n   @Override\n   public DirectPipelineResult run(Pipeline pipeline) {\n-    for (Map.Entry<PTransformMatcher, PTransformOverrideFactory> override :\n-        defaultTransformOverrides().entrySet()) {\n-      pipeline.replace(override.getKey(), override.getValue());\n-    }\n+    pipeline.replaceAll(defaultTransformOverrides());\n     MetricsEnvironment.setMetricsSupported(true);\n     DirectGraphVisitor graphVisitor = new DirectGraphVisitor();\n     pipeline.traverseTopologically(graphVisitor);\n@@ -322,40 +316,37 @@ public DirectPipelineResult run(Pipeline pipeline) {\n    * iteration order based on the order at which elements are added to it.\n    */\n   @SuppressWarnings(\"rawtypes\")\n-  private Map<PTransformMatcher, PTransformOverrideFactory> defaultTransformOverrides() {\n-    return ImmutableMap.<PTransformMatcher, PTransformOverrideFactory>builder()\n-        .put(\n-            PTransformMatchers.writeWithRunnerDeterminedSharding(),\n-            new WriteWithShardingFactory()) /* Uses a view internally. */\n-        .put(\n-            PTransformMatchers.classEqualTo(CreatePCollectionView.class),\n-            new ViewOverrideFactory()) /* Uses pardos and GBKs */\n-        .put(\n-            PTransformMatchers.classEqualTo(TestStream.class),\n-            new DirectTestStreamFactory(this)) /* primitive */\n-        /* Single-output ParDos are implemented in terms of Multi-output ParDos. Any override\n-        that is applied to a multi-output ParDo must first have all matching Single-output ParDos\n-        converted to match.\n-         */\n-        .put(PTransformMatchers.splittableParDoSingle(), new ParDoSingleViaMultiOverrideFactory())\n-        .put(PTransformMatchers.stateOrTimerParDoSingle(), new ParDoSingleViaMultiOverrideFactory())\n-        // SplittableParMultiDo is implemented in terms of nonsplittable single ParDos\n-        .put(PTransformMatchers.splittableParDoMulti(), new ParDoMultiOverrideFactory())\n-        // state and timer pardos are implemented in terms of nonsplittable single ParDos\n-        .put(PTransformMatchers.stateOrTimerParDoMulti(), new ParDoMultiOverrideFactory())\n-        .put(\n-            PTransformMatchers.classEqualTo(ParDo.Bound.class),\n-            new ParDoSingleViaMultiOverrideFactory()) /* returns a BoundMulti */\n-        .put(\n-            PTransformMatchers.classEqualTo(BoundMulti.class),\n-            /* returns one of two primitives; SplittableParDos are replaced above. */\n-            new ParDoMultiOverrideFactory())\n-        .put(\n-            PTransformMatchers.classEqualTo(GBKIntoKeyedWorkItems.class),\n-            new DirectGBKIntoKeyedWorkItemsOverrideFactory()) /* Returns a GBKO */\n-        .put(\n-            PTransformMatchers.classEqualTo(GroupByKey.class),\n-            new DirectGroupByKeyOverrideFactory()) /* returns two chained primitives. */\n+  private List<PTransformOverride> defaultTransformOverrides() {\n+    return ImmutableList.<PTransformOverride>builder()\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.writeWithRunnerDeterminedSharding(),\n+                new WriteWithShardingFactory())) /* Uses a view internally. */\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(CreatePCollectionView.class),\n+                new ViewOverrideFactory())) /* Uses pardos and GBKs */\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(TestStream.class),\n+                new DirectTestStreamFactory(this))) /* primitive */\n+        // SplittableParMultiDo is implemented in terms of nonsplittable simple ParDos and extra\n+        // primitives\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.splittableParDoMulti(), new ParDoMultiOverrideFactory()))\n+        // state and timer pardos are implemented in terms of simple ParDos and extra primitives\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.stateOrTimerParDoMulti(), new ParDoMultiOverrideFactory()))\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(GBKIntoKeyedWorkItems.class),\n+                new DirectGBKIntoKeyedWorkItemsOverrideFactory())) /* Returns a GBKO */\n+        .add(\n+            PTransformOverride.of(\n+                PTransformMatchers.classEqualTo(GroupByKey.class),\n+                new DirectGroupByKeyOverrideFactory())) /* returns two chained primitives. */\n         .build();\n   }\n \n@@ -429,19 +420,34 @@ public MetricResults metrics() {\n      * exception. Future calls to {@link #getState()} will return\n      * {@link org.apache.beam.sdk.PipelineResult.State#FAILED}.\n      *\n-     * <p>NOTE: if the {@link Pipeline} contains an {@link IsBounded#UNBOUNDED unbounded}\n-     * {@link PCollection}, and the {@link PipelineRunner} was created with\n-     * {@link DirectOptions#isShutdownUnboundedProducersWithMaxWatermark()} set to false,\n-     * this method will never return.\n-     *\n-     * <p>See also {@link PipelineExecutor#awaitCompletion()}.\n+     * <p>See also {@link PipelineExecutor#waitUntilFinish(Duration)}.\n      */\n     @Override\n     public State waitUntilFinish() {\n-      if (!state.isTerminal()) {\n+      return waitUntilFinish(Duration.ZERO);\n+    }\n+\n+    @Override\n+    public State cancel() {\n+      this.state = executor.getPipelineState();\n+      if (!this.state.isTerminal()) {\n+        executor.stop();\n+        this.state = executor.getPipelineState();\n+      }\n+      return executor.getPipelineState();\n+    }\n+\n+    @Override\n+    public State waitUntilFinish(Duration duration) {\n+      State startState = this.state;\n+      if (!startState.isTerminal()) {\n         try {\n-          executor.awaitCompletion();\n-          state = State.DONE;\n+          state = executor.waitUntilFinish(duration);\n+        } catch (UserCodeException uce) {\n+          // Emulates the behavior of Pipeline#run(), where a stack trace caused by a\n+          // UserCodeException is truncated and replaced with the stack starting at the call to\n+          // waitToFinish\n+          throw new Pipeline.PipelineExecutionException(uce.getCause());\n         } catch (Exception e) {\n           if (e instanceof InterruptedException) {\n             Thread.currentThread().interrupt();\n@@ -452,19 +458,7 @@ public State waitUntilFinish() {\n           throw new RuntimeException(e);\n         }\n       }\n-      return state;\n-    }\n-\n-    @Override\n-    public State cancel() throws IOException {\n-      throw new UnsupportedOperationException(\"DirectPipelineResult does not support cancel.\");\n-    }\n-\n-    @Override\n-    public State waitUntilFinish(Duration duration) {\n-      throw new UnsupportedOperationException(\n-          \"DirectPipelineResult does not support waitUntilFinish with a Duration parameter. See\"\n-              + \" BEAM-596.\");\n+      return this.state;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "sha": "43147a02564d49a974072420eaff932e7000cc35",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 3,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java",
                "patch": "@@ -31,16 +31,16 @@\n class DoFnLifecycleManagerRemovingTransformEvaluator<InputT> implements TransformEvaluator<InputT> {\n   private static final Logger LOG =\n       LoggerFactory.getLogger(DoFnLifecycleManagerRemovingTransformEvaluator.class);\n-  private final ParDoEvaluator<InputT, ?> underlying;\n+  private final ParDoEvaluator<InputT> underlying;\n   private final DoFnLifecycleManager lifecycleManager;\n \n   public static <InputT> DoFnLifecycleManagerRemovingTransformEvaluator<InputT> wrapping(\n-      ParDoEvaluator<InputT, ?> underlying, DoFnLifecycleManager lifecycleManager) {\n+      ParDoEvaluator<InputT> underlying, DoFnLifecycleManager lifecycleManager) {\n     return new DoFnLifecycleManagerRemovingTransformEvaluator<>(underlying, lifecycleManager);\n   }\n \n   private DoFnLifecycleManagerRemovingTransformEvaluator(\n-      ParDoEvaluator<InputT, ?> underlying, DoFnLifecycleManager lifecycleManager) {\n+      ParDoEvaluator<InputT> underlying, DoFnLifecycleManager lifecycleManager) {\n     this.underlying = underlying;\n     this.lifecycleManager = lifecycleManager;\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluator.java",
                "sha": "e537962e0ff1f04e36c1d82dbbab27576a1f7c2a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 31,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "patch": "@@ -50,10 +50,8 @@\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.joda.time.Instant;\n \n /**\n@@ -402,37 +400,11 @@ void forceRefresh() {\n \n   /**\n    * Returns true if the step will not produce additional output.\n-   *\n-   * <p>If the provided transform produces only {@link IsBounded#BOUNDED}\n-   * {@link PCollection PCollections}, returns true if the watermark is at\n-   * {@link BoundedWindow#TIMESTAMP_MAX_VALUE positive infinity}.\n-   *\n-   * <p>If the provided transform produces any {@link IsBounded#UNBOUNDED}\n-   * {@link PCollection PCollections}, returns the value of\n-   * {@link DirectOptions#isShutdownUnboundedProducersWithMaxWatermark()}.\n    */\n   public boolean isDone(AppliedPTransform<?, ?, ?> transform) {\n-    // if the PTransform's watermark isn't at the max value, it isn't done\n-    if (watermarkManager\n-        .getWatermarks(transform)\n-        .getOutputWatermark()\n-        .isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {\n-      return false;\n-    }\n-    // If the PTransform has any unbounded outputs, and unbounded producers should not be shut down,\n-    // the PTransform may produce additional output. It is not done.\n-    for (TaggedPValue output : transform.getOutputs()) {\n-      if (output.getValue() instanceof PCollection) {\n-        IsBounded bounded = ((PCollection<?>) output.getValue()).isBounded();\n-        if (bounded.equals(IsBounded.UNBOUNDED)\n-            && !options.isShutdownUnboundedProducersWithMaxWatermark()) {\n-          return false;\n-        }\n-      }\n-    }\n-    // The PTransform's watermark was at positive infinity and all of its outputs are known to be\n-    // done. It is done.\n-    return true;\n+    // the PTransform is done only if watermark is at the max value\n+    Instant stepWatermark = watermarkManager.getWatermarks(transform).getOutputWatermark();\n+    return !stepWatermark.isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "sha": "54ce027279c3de9e4e8df9549795f33133222731",
                "status": "modified"
            },
            {
                "additions": 105,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "changes": 141,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 36,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "patch": "@@ -25,6 +25,8 @@\n import com.google.common.cache.CacheBuilder;\n import com.google.common.cache.CacheLoader;\n import com.google.common.cache.LoadingCache;\n+import com.google.common.cache.RemovalListener;\n+import com.google.common.cache.RemovalNotification;\n import com.google.common.collect.Iterables;\n import java.util.ArrayList;\n import java.util.Collection;\n@@ -48,13 +50,16 @@\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.WatermarkManager.FiredTimers;\n import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult.State;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PValue;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -99,6 +104,7 @@\n    * {@link CompletionCallback} decrement this value.\n    */\n   private final AtomicLong outstandingWork = new AtomicLong();\n+  private AtomicReference<State> pipelineState = new AtomicReference<>(State.RUNNING);\n \n   public static ExecutorServiceParallelExecutor create(\n       int targetParallelism,\n@@ -138,7 +144,10 @@ private ExecutorServiceParallelExecutor(\n     // Executing TransformExecutorServices have a strong reference to their TransformExecutorService\n     // which stops the TransformExecutorServices from being prematurely garbage collected\n     executorServices =\n-        CacheBuilder.newBuilder().weakValues().build(serialTransformExecutorServiceCacheLoader());\n+        CacheBuilder.newBuilder()\n+            .weakValues()\n+            .removalListener(shutdownExecutorServiceListener())\n+            .build(serialTransformExecutorServiceCacheLoader());\n \n     this.allUpdates = new ConcurrentLinkedQueue<>();\n     this.visibleUpdates = new LinkedBlockingQueue<>();\n@@ -159,6 +168,19 @@ public TransformExecutorService load(StepAndKey stepAndKey) throws Exception {\n     };\n   }\n \n+  private RemovalListener<StepAndKey, TransformExecutorService> shutdownExecutorServiceListener() {\n+    return new RemovalListener<StepAndKey, TransformExecutorService>() {\n+      @Override\n+      public void onRemoval(\n+          RemovalNotification<StepAndKey, TransformExecutorService> notification) {\n+        TransformExecutorService service = notification.getValue();\n+        if (service != null) {\n+          service.shutdown();\n+        }\n+      }\n+    };\n+  }\n+\n   @Override\n   public void start(Collection<AppliedPTransform<?, ?, ?>> roots) {\n     int numTargetSplits = Math.max(3, targetParallelism);\n@@ -179,7 +201,7 @@ public void start(Collection<AppliedPTransform<?, ?, ?>> roots) {\n   }\n \n   @SuppressWarnings(\"unchecked\")\n-  public void scheduleConsumption(\n+  private void scheduleConsumption(\n       AppliedPTransform<?, ?, ?> consumer,\n       CommittedBundle<?> bundle,\n       CompletionCallback onComplete) {\n@@ -219,7 +241,9 @@ public void scheduleConsumption(\n             onComplete,\n             transformExecutor);\n     outstandingWork.incrementAndGet();\n-    transformExecutor.schedule(callable);\n+    if (!pipelineState.get().isTerminal()) {\n+      transformExecutor.schedule(callable);\n+    }\n   }\n \n   private boolean isKeyed(PValue pvalue) {\n@@ -234,20 +258,66 @@ private void scheduleConsumers(ExecutorUpdate update) {\n   }\n \n   @Override\n-  public void awaitCompletion() throws Exception {\n-    VisibleExecutorUpdate update;\n-    do {\n-      // Get an update; don't block forever if another thread has handled it\n-      update = visibleUpdates.poll(2L, TimeUnit.SECONDS);\n-      if (update == null && executorService.isShutdown()) {\n+  public State waitUntilFinish(Duration duration) throws Exception {\n+    Instant completionTime;\n+    if (duration.equals(Duration.ZERO)) {\n+      completionTime = new Instant(Long.MAX_VALUE);\n+    } else {\n+      completionTime = Instant.now().plus(duration);\n+    }\n+\n+    VisibleExecutorUpdate update = null;\n+    while (Instant.now().isBefore(completionTime)\n+        && (update == null || isTerminalStateUpdate(update))) {\n+      // Get an update; don't block forever if another thread has handled it. The call to poll will\n+      // wait the entire timeout; this call primarily exists to relinquish any core.\n+      update = visibleUpdates.poll(25L, TimeUnit.MILLISECONDS);\n+      if (update == null && pipelineState.get().isTerminal()) {\n         // there are no updates to process and no updates will ever be published because the\n         // executor is shutdown\n-        return;\n+        return pipelineState.get();\n       } else if (update != null && update.exception.isPresent()) {\n         throw update.exception.get();\n       }\n-    } while (update == null || !update.isDone());\n+    }\n+    return pipelineState.get();\n+  }\n+\n+  @Override\n+  public State getPipelineState() {\n+    return pipelineState.get();\n+  }\n+\n+  private boolean isTerminalStateUpdate(VisibleExecutorUpdate update) {\n+    return !(update.getNewState() == null && update.getNewState().isTerminal());\n+  }\n+\n+  @Override\n+  public void stop() {\n+    shutdownIfNecessary(State.CANCELLED);\n+    while (!visibleUpdates.offer(VisibleExecutorUpdate.cancelled())) {\n+      // Make sure \"This Pipeline was Cancelled\" notification arrives.\n+      visibleUpdates.poll();\n+    }\n+  }\n+\n+  private void shutdownIfNecessary(State newState) {\n+    if (!newState.isTerminal()) {\n+      return;\n+    }\n+    LOG.debug(\"Pipeline has terminated. Shutting down.\");\n+    pipelineState.compareAndSet(State.RUNNING, newState);\n+    // Stop accepting new work before shutting down the executor. This ensures that thread don't try\n+    // to add work to the shutdown executor.\n+    executorServices.invalidateAll();\n+    executorServices.cleanUp();\n+    parallelExecutorService.shutdown();\n     executorService.shutdown();\n+    try {\n+      registry.cleanup();\n+    } catch (Exception e) {\n+      visibleUpdates.add(VisibleExecutorUpdate.fromException(e));\n+    }\n   }\n \n   /**\n@@ -341,29 +411,35 @@ public static ExecutorUpdate fromException(Exception e) {\n   }\n \n   /**\n-   * An update of interest to the user. Used in {@link #awaitCompletion} to decide whether to\n+   * An update of interest to the user. Used in {@link #waitUntilFinish} to decide whether to\n    * return normally or throw an exception.\n    */\n   private static class VisibleExecutorUpdate {\n     private final Optional<? extends Exception> exception;\n-    private final boolean done;\n+    @Nullable\n+    private final State newState;\n \n     public static VisibleExecutorUpdate fromException(Exception e) {\n-      return new VisibleExecutorUpdate(false, e);\n+      return new VisibleExecutorUpdate(null, e);\n     }\n \n     public static VisibleExecutorUpdate finished() {\n-      return new VisibleExecutorUpdate(true, null);\n+      return new VisibleExecutorUpdate(State.DONE, null);\n+    }\n+\n+    public static VisibleExecutorUpdate cancelled() {\n+      return new VisibleExecutorUpdate(State.CANCELLED, null);\n     }\n \n-    private VisibleExecutorUpdate(boolean done, @Nullable Exception exception) {\n+    private VisibleExecutorUpdate(State newState, @Nullable Exception exception) {\n       this.exception = Optional.fromNullable(exception);\n-      this.done = done;\n+      this.newState = newState;\n     }\n \n-    public boolean isDone() {\n-      return done;\n+    public State getNewState() {\n+      return newState;\n     }\n+\n   }\n \n   private class MonitorRunnable implements Runnable {\n@@ -458,8 +534,8 @@ private void fireTimers() throws Exception {\n                   .createKeyedBundle(\n                       transformTimers.getKey(),\n                       (PCollection)\n-                          Iterables.getOnlyElement(transformTimers.getTransform().getInputs())\n-                              .getValue())\n+                          Iterables.getOnlyElement(\n+                              transformTimers.getTransform().getInputs().values()))\n                   .add(WindowedValue.valueInGlobalWindow(work))\n                   .commit(evaluationContext.now());\n           scheduleConsumption(\n@@ -475,22 +551,15 @@ private void fireTimers() throws Exception {\n     }\n \n     private boolean shouldShutdown() {\n-      boolean shouldShutdown = exceptionThrown || evaluationContext.isDone();\n-      if (shouldShutdown) {\n-        LOG.debug(\"Pipeline has terminated. Shutting down.\");\n-        executorService.shutdown();\n-        try {\n-          registry.cleanup();\n-        } catch (Exception e) {\n-          visibleUpdates.add(VisibleExecutorUpdate.fromException(e));\n-        }\n-        if (evaluationContext.isDone()) {\n-          while (!visibleUpdates.offer(VisibleExecutorUpdate.finished())) {\n-            visibleUpdates.poll();\n-          }\n-        }\n+      State nextState = State.UNKNOWN;\n+      if (exceptionThrown) {\n+        nextState = State.FAILED;\n+      } else if (evaluationContext.isDone()) {\n+        visibleUpdates.offer(VisibleExecutorUpdate.finished());\n+        nextState = State.DONE;\n       }\n-      return shouldShutdown;\n+      shutdownIfNecessary(nextState);\n+      return pipelineState.get().isTerminal();\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "sha": "c802c58be6e52bbba5b7e9c068e3c115b38e8065",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java",
                "patch": "@@ -57,7 +57,7 @@ public void cleanup() throws Exception {}\n           application) {\n     final UncommittedBundle<InputT> outputBundle =\n         evaluationContext.createBundle(\n-            (PCollection<InputT>) Iterables.getOnlyElement(application.getOutputs()).getValue());\n+            (PCollection<InputT>) Iterables.getOnlyElement(application.getOutputs().values()));\n     final TransformResult<InputT> result =\n         StepTransformResult.<InputT>withoutHold(application).addOutput(outputBundle).build();\n     return new FlattenEvaluator<>(outputBundle, result);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/FlattenEvaluatorFactory.java",
                "sha": "7c6d2a166269de523091b87a062218835203e3b8",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 6,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.beam.runners.core.SystemReduceFn;\n import org.apache.beam.runners.core.TimerInternals;\n import org.apache.beam.runners.core.UnsupportedSideInputReader;\n+import org.apache.beam.runners.core.construction.Triggers;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.runners.direct.DirectExecutionContext.DirectStepContext;\n@@ -47,7 +48,6 @@\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.Triggers;\n import org.apache.beam.sdk.util.WindowTracing;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n@@ -162,7 +162,7 @@ public void processElement(WindowedValue<KeyedWorkItem<K, V>> element) throws Ex\n           evaluationContext.createKeyedBundle(\n               structuralKey,\n               (PCollection<KV<K, Iterable<V>>>)\n-                  Iterables.getOnlyElement(application.getOutputs()).getValue());\n+                  Iterables.getOnlyElement(application.getOutputs().values()));\n       outputBundles.add(bundle);\n       CopyOnAccessInMemoryStateInternals<K> stateInternals =\n           (CopyOnAccessInMemoryStateInternals<K>) stepContext.stateInternals();\n@@ -264,13 +264,13 @@ public void outputWindowedValue(\n     }\n \n     @Override\n-    public <SideOutputT> void sideOutputWindowedValue(\n-        TupleTag<SideOutputT> tag,\n-        SideOutputT output,\n+    public <AdditionalOutputT> void outputWindowedValue(\n+        TupleTag<AdditionalOutputT> tag,\n+        AdditionalOutputT output,\n         Instant timestamp,\n         Collection<? extends BoundedWindow> windows,\n         PaneInfo pane) {\n-      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use side outputs\");\n+      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use tagged outputs\");\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "sha": "ce7b12a4ed80685ce4f7affce553d437b98e2d89",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "patch": "@@ -105,7 +105,7 @@ public GroupByKeyOnlyEvaluator(\n       this.application = application;\n       this.keyCoder =\n           getKeyCoder(\n-              ((PCollection<KV<K, V>>) Iterables.getOnlyElement(application.getInputs()).getValue())\n+              ((PCollection<KV<K, V>>) Iterables.getOnlyElement(application.getInputs().values()))\n                   .getCoder());\n       this.groupingMap = new HashMap<>();\n     }\n@@ -158,7 +158,7 @@ public void processElement(WindowedValue<KV<K, V>> element) {\n             evaluationContext.createKeyedBundle(\n                 StructuralKey.of(key, keyCoder),\n                 (PCollection<KeyedWorkItem<K, V>>)\n-                    Iterables.getOnlyElement(application.getOutputs()).getValue());\n+                    Iterables.getOnlyElement(application.getOutputs().values()));\n         bundle.add(WindowedValue.valueInGlobalWindow(groupedKv));\n         resultBuilder.addOutput(bundle);\n       }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "sha": "ac0b14ffba34e629e2c68084fca5fb86fccdeea7",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 9,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "patch": "@@ -21,7 +21,7 @@\n \n import com.google.common.collect.ImmutableSet;\n import java.util.HashSet;\n-import java.util.List;\n+import java.util.Map;\n import java.util.Set;\n import org.apache.beam.runners.core.SplittableParDo;\n import org.apache.beam.runners.direct.DirectGroupByKey.DirectGroupAlsoByWindow;\n@@ -32,7 +32,7 @@\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A pipeline visitor that tracks all keyed {@link PValue PValues}. A {@link PValue} is keyed if it\n@@ -83,9 +83,9 @@ public void leaveCompositeTransform(TransformHierarchy.Node node) {\n     if (node.isRootNode()) {\n       finalized = true;\n     } else if (PRODUCES_KEYED_OUTPUTS.contains(node.getTransform().getClass())) {\n-      List<TaggedPValue> outputs = node.getOutputs();\n-      for (TaggedPValue output : outputs) {\n-        keyedValues.add(output.getValue());\n+      Map<TupleTag<?>, PValue> outputs = node.getOutputs();\n+      for (PValue output : outputs.values()) {\n+        keyedValues.add(output);\n       }\n     }\n   }\n@@ -96,8 +96,8 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {}\n   @Override\n   public void visitValue(PValue value, TransformHierarchy.Node producer) {\n     boolean inputsAreKeyed = true;\n-    for (TaggedPValue input : producer.getInputs()) {\n-      inputsAreKeyed = inputsAreKeyed && keyedValues.contains(input.getValue());\n+    for (PValue input : producer.getInputs().values()) {\n+      inputsAreKeyed = inputsAreKeyed && keyedValues.contains(input);\n     }\n     if (PRODUCES_KEYED_OUTPUTS.contains(producer.getTransform().getClass())\n         || (isKeyPreserving(producer.getTransform()) && inputsAreKeyed)) {\n@@ -116,8 +116,8 @@ private static boolean isKeyPreserving(PTransform<?, ?> transform) {\n     // The most obvious alternative would be a package-private marker interface, but\n     // better to make this obviously hacky so it is less likely to proliferate. Meanwhile\n     // we intend to allow explicit expression of key-preserving DoFn in the model.\n-    if (transform instanceof ParDo.BoundMulti) {\n-      ParDo.BoundMulti<?, ?> parDo = (ParDo.BoundMulti<?, ?>) transform;\n+    if (transform instanceof ParDo.MultiOutput) {\n+      ParDo.MultiOutput<?, ?> parDo = (ParDo.MultiOutput<?, ?>) transform;\n       return parDo.getFn() instanceof ParDoMultiOverrideFactory.ToKeyedWorkItem;\n     } else {\n       return false;",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "sha": "f9b6eba215f253b56eeb8545b96c3a3c9bf0f030",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java",
                "patch": "@@ -17,7 +17,6 @@\n  */\n package org.apache.beam.runners.direct;\n \n-import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -28,9 +27,9 @@\n  *\n  * <p>ModelEnforcement is performed on a per-element and per-bundle basis. The\n  * {@link ModelEnforcement} is provided with the input bundle as part of\n- * {@link ModelEnforcementFactory#forBundle(CommittedBundle, AppliedPTransform)}, each element\n- * before and after that element is provided to an underlying {@link TransformEvaluator}, and the\n- * output {@link TransformResult} and committed output bundles after the\n+ * {@link ModelEnforcementFactory#forBundle(DirectRunner.CommittedBundle, AppliedPTransform)} each\n+ * element before and after that element is provided to an underlying {@link TransformEvaluator},\n+ * and the output {@link TransformResult} and committed output bundles after the\n  * {@link TransformEvaluator} has completed.\n  *\n  * <p>Typically, {@link ModelEnforcement} will obtain required metadata (such as the {@link Coder}\n@@ -54,10 +53,10 @@\n   /**\n    * Called after a bundle has been completed and {@link TransformEvaluator#finishBundle()} has been\n    * called, producing the provided {@link TransformResult} and\n-   * {@link CommittedBundle output bundles}.\n+   * {@link DirectRunner.CommittedBundle output bundles}.\n    */\n   void afterFinish(\n-      CommittedBundle<T> input,\n+      DirectRunner.CommittedBundle<T> input,\n       TransformResult<T> result,\n-      Iterable<? extends CommittedBundle<?>> outputs);\n+      Iterable<? extends DirectRunner.CommittedBundle<?>> outputs);\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ModelEnforcement.java",
                "sha": "96dbc2bc2aeaafc8c7ae40038a408707c231c0cf",
                "status": "modified"
            },
            {
                "additions": 93,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "changes": 129,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 36,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "patch": "@@ -26,9 +26,11 @@\n import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.PushbackSideInputDoFnRunner;\n+import org.apache.beam.runners.core.SimplePushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n import org.apache.beam.runners.direct.DirectExecutionContext.DirectStepContext;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n+import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n@@ -40,9 +42,53 @@\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n \n-class ParDoEvaluator<InputT, OutputT> implements TransformEvaluator<InputT> {\n+class ParDoEvaluator<InputT> implements TransformEvaluator<InputT> {\n \n-  public static <InputT, OutputT> ParDoEvaluator<InputT, OutputT> create(\n+  public interface DoFnRunnerFactory<InputT, OutputT> {\n+    PushbackSideInputDoFnRunner<InputT, OutputT> createRunner(\n+        PipelineOptions options,\n+        DoFn<InputT, OutputT> fn,\n+        List<PCollectionView<?>> sideInputs,\n+        ReadyCheckingSideInputReader sideInputReader,\n+        OutputManager outputManager,\n+        TupleTag<OutputT> mainOutputTag,\n+        List<TupleTag<?>> additionalOutputTags,\n+        DirectStepContext stepContext,\n+        AggregatorContainer.Mutator aggregatorChanges,\n+        WindowingStrategy<?, ? extends BoundedWindow> windowingStrategy);\n+  }\n+\n+  public static <InputT, OutputT> DoFnRunnerFactory<InputT, OutputT> defaultRunnerFactory() {\n+    return new DoFnRunnerFactory<InputT, OutputT>() {\n+      @Override\n+      public PushbackSideInputDoFnRunner<InputT, OutputT> createRunner(\n+          PipelineOptions options,\n+          DoFn<InputT, OutputT> fn,\n+          List<PCollectionView<?>> sideInputs,\n+          ReadyCheckingSideInputReader sideInputReader,\n+          OutputManager outputManager,\n+          TupleTag<OutputT> mainOutputTag,\n+          List<TupleTag<?>> additionalOutputTags,\n+          DirectStepContext stepContext,\n+          AggregatorContainer.Mutator aggregatorChanges,\n+          WindowingStrategy<?, ? extends BoundedWindow> windowingStrategy) {\n+        DoFnRunner<InputT, OutputT> underlying =\n+            DoFnRunners.simpleRunner(\n+                options,\n+                fn,\n+                sideInputReader,\n+                outputManager,\n+                mainOutputTag,\n+                additionalOutputTags,\n+                stepContext,\n+                aggregatorChanges,\n+                windowingStrategy);\n+        return SimplePushbackSideInputDoFnRunner.create(underlying, sideInputs, sideInputReader);\n+      }\n+    };\n+  }\n+\n+  public static <InputT, OutputT> ParDoEvaluator<InputT> create(\n       EvaluationContext evaluationContext,\n       DirectStepContext stepContext,\n       AppliedPTransform<?, ?, ?> application,\n@@ -51,10 +97,44 @@\n       StructuralKey<?> key,\n       List<PCollectionView<?>> sideInputs,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n-      Map<TupleTag<?>, PCollection<?>> outputs) {\n+      List<TupleTag<?>> additionalOutputTags,\n+      Map<TupleTag<?>, PCollection<?>> outputs,\n+      DoFnRunnerFactory<InputT, OutputT> runnerFactory) {\n     AggregatorContainer.Mutator aggregatorChanges = evaluationContext.getAggregatorMutator();\n \n+    BundleOutputManager outputManager = createOutputManager(evaluationContext, key, outputs);\n+\n+    ReadyCheckingSideInputReader sideInputReader =\n+        evaluationContext.createSideInputReader(sideInputs);\n+\n+    PushbackSideInputDoFnRunner<InputT, OutputT> runner = runnerFactory.createRunner(\n+        evaluationContext.getPipelineOptions(),\n+        fn,\n+        sideInputs,\n+        sideInputReader,\n+        outputManager,\n+        mainOutputTag,\n+        additionalOutputTags,\n+        stepContext,\n+        aggregatorChanges,\n+        windowingStrategy);\n+\n+    return create(runner, stepContext, application, aggregatorChanges, outputManager);\n+  }\n+\n+  public static <InputT, OutputT> ParDoEvaluator<InputT> create(\n+      PushbackSideInputDoFnRunner<InputT, OutputT> runner,\n+      DirectStepContext stepContext,\n+      AppliedPTransform<?, ?, ?> application,\n+      AggregatorContainer.Mutator aggregatorChanges,\n+      BundleOutputManager outputManager) {\n+    return new ParDoEvaluator<>(runner, application, aggregatorChanges, outputManager, stepContext);\n+  }\n+\n+  static BundleOutputManager createOutputManager(\n+      EvaluationContext evaluationContext,\n+      StructuralKey<?> key,\n+      Map<TupleTag<?>, PCollection<?>> outputs) {\n     Map<TupleTag<?>, UncommittedBundle<?>> outputBundles = new HashMap<>();\n     for (Map.Entry<TupleTag<?>, PCollection<?>> outputEntry : outputs.entrySet()) {\n       // Just trust the context's decision as to whether the output should be keyed.\n@@ -68,38 +148,11 @@\n             outputEntry.getKey(), evaluationContext.createBundle(outputEntry.getValue()));\n       }\n     }\n-    BundleOutputManager outputManager = BundleOutputManager.create(outputBundles);\n-\n-    ReadyCheckingSideInputReader sideInputReader =\n-        evaluationContext.createSideInputReader(sideInputs);\n-\n-    DoFnRunner<InputT, OutputT> underlying =\n-        DoFnRunners.simpleRunner(\n-            evaluationContext.getPipelineOptions(),\n-            fn,\n-            sideInputReader,\n-            outputManager,\n-            mainOutputTag,\n-            sideOutputTags,\n-            stepContext,\n-            aggregatorChanges,\n-            windowingStrategy);\n-    PushbackSideInputDoFnRunner<InputT, OutputT> runner =\n-        PushbackSideInputDoFnRunner.create(underlying, sideInputs, sideInputReader);\n-\n-    try {\n-      runner.startBundle();\n-    } catch (Exception e) {\n-      throw UserCodeException.wrap(e);\n-    }\n-\n-    return new ParDoEvaluator<>(\n-        evaluationContext, runner, application, aggregatorChanges, outputManager, stepContext);\n+    return BundleOutputManager.create(outputBundles);\n   }\n \n   ////////////////////////////////////////////////////////////////////////////////////////////////\n \n-  private final EvaluationContext evaluationContext;\n   private final PushbackSideInputDoFnRunner<InputT, ?> fnRunner;\n   private final AppliedPTransform<?, ?, ?> transform;\n   private final AggregatorContainer.Mutator aggregatorChanges;\n@@ -109,19 +162,23 @@\n   private final ImmutableList.Builder<WindowedValue<InputT>> unprocessedElements;\n \n   private ParDoEvaluator(\n-      EvaluationContext evaluationContext,\n       PushbackSideInputDoFnRunner<InputT, ?> fnRunner,\n       AppliedPTransform<?, ?, ?> transform,\n       AggregatorContainer.Mutator aggregatorChanges,\n       BundleOutputManager outputManager,\n       DirectStepContext stepContext) {\n-    this.evaluationContext = evaluationContext;\n     this.fnRunner = fnRunner;\n     this.transform = transform;\n     this.outputManager = outputManager;\n     this.stepContext = stepContext;\n     this.aggregatorChanges = aggregatorChanges;\n     this.unprocessedElements = ImmutableList.builder();\n+\n+    try {\n+      fnRunner.startBundle();\n+    } catch (Exception e) {\n+      throw UserCodeException.wrap(e);\n+    }\n   }\n \n   public BundleOutputManager getOutputManager() {\n@@ -153,11 +210,11 @@ public void onTimer(TimerData timer, BoundedWindow window) {\n     } catch (Exception e) {\n       throw UserCodeException.wrap(e);\n     }\n-    StepTransformResult.Builder resultBuilder;\n+    StepTransformResult.Builder<InputT> resultBuilder;\n     CopyOnAccessInMemoryStateInternals<?> state = stepContext.commitState();\n     if (state != null) {\n       resultBuilder =\n-          StepTransformResult.withHold(transform, state.getEarliestWatermarkHold())\n+          StepTransformResult.<InputT>withHold(transform, state.getEarliestWatermarkHold())\n               .withState(state);\n     } else {\n       resultBuilder = StepTransformResult.withoutHold(transform);",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "sha": "cab11db2abe43a46078677e91bc812d713f28664",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 17,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "patch": "@@ -32,20 +32,24 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-/** A {@link TransformEvaluatorFactory} for {@link ParDo.BoundMulti}. */\n+/** A {@link TransformEvaluatorFactory} for {@link ParDo.MultiOutput}. */\n final class ParDoEvaluatorFactory<InputT, OutputT> implements TransformEvaluatorFactory {\n \n   private static final Logger LOG = LoggerFactory.getLogger(ParDoEvaluatorFactory.class);\n   private final LoadingCache<DoFn<?, ?>, DoFnLifecycleManager> fnClones;\n   private final EvaluationContext evaluationContext;\n+  private final ParDoEvaluator.DoFnRunnerFactory<InputT, OutputT> runnerFactory;\n \n-  ParDoEvaluatorFactory(EvaluationContext evaluationContext) {\n+  ParDoEvaluatorFactory(\n+      EvaluationContext evaluationContext,\n+      ParDoEvaluator.DoFnRunnerFactory<InputT, OutputT> runnerFactory) {\n     this.evaluationContext = evaluationContext;\n+    this.runnerFactory = runnerFactory;\n     fnClones =\n         CacheBuilder.newBuilder()\n             .build(\n@@ -62,13 +66,13 @@ public DoFnLifecycleManager load(DoFn<?, ?> key) throws Exception {\n       AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle) throws Exception {\n \n     @SuppressWarnings(\"unchecked\")\n-    AppliedPTransform<PCollection<InputT>, PCollectionTuple, ParDo.BoundMulti<InputT, OutputT>>\n+    AppliedPTransform<PCollection<InputT>, PCollectionTuple, ParDo.MultiOutput<InputT, OutputT>>\n         parDoApplication =\n             (AppliedPTransform<\n-                    PCollection<InputT>, PCollectionTuple, ParDo.BoundMulti<InputT, OutputT>>)\n+                    PCollection<InputT>, PCollectionTuple, ParDo.MultiOutput<InputT, OutputT>>)\n                 application;\n \n-    ParDo.BoundMulti<InputT, OutputT> transform = parDoApplication.getTransform();\n+    ParDo.MultiOutput<InputT, OutputT> transform = parDoApplication.getTransform();\n     final DoFn<InputT, OutputT> doFn = transform.getFn();\n \n     @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n@@ -80,7 +84,7 @@ public DoFnLifecycleManager load(DoFn<?, ?> key) throws Exception {\n                 doFn,\n                 transform.getSideInputs(),\n                 transform.getMainOutputTag(),\n-                transform.getSideOutputTags().getAll());\n+                transform.getAdditionalOutputTags().getAll());\n     return evaluator;\n   }\n \n@@ -103,7 +107,7 @@ public void cleanup() throws Exception {\n       DoFn<InputT, OutputT> doFn,\n       List<PCollectionView<?>> sideInputs,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags)\n+      List<TupleTag<?>> additionalOutputTags)\n       throws Exception {\n     String stepName = evaluationContext.getStepName(application);\n     DirectStepContext stepContext =\n@@ -119,19 +123,19 @@ public void cleanup() throws Exception {\n             inputBundleKey,\n             sideInputs,\n             mainOutputTag,\n-            sideOutputTags,\n+            additionalOutputTags,\n             stepContext,\n             fnManager.<InputT, OutputT>get(),\n             fnManager),\n         fnManager);\n   }\n \n-  ParDoEvaluator<InputT, OutputT> createParDoEvaluator(\n+  ParDoEvaluator<InputT> createParDoEvaluator(\n       AppliedPTransform<PCollection<InputT>, PCollectionTuple, ?> application,\n       StructuralKey<?> key,\n       List<PCollectionView<?>> sideInputs,\n       TupleTag<OutputT> mainOutputTag,\n-      List<TupleTag<?>> sideOutputTags,\n+      List<TupleTag<?>> additionalOutputTags,\n       DirectStepContext stepContext,\n       DoFn<InputT, OutputT> fn,\n       DoFnLifecycleManager fnManager)\n@@ -141,14 +145,15 @@ public void cleanup() throws Exception {\n           evaluationContext,\n           stepContext,\n           application,\n-          ((PCollection<InputT>) Iterables.getOnlyElement(application.getInputs()).getValue())\n+          ((PCollection<InputT>) Iterables.getOnlyElement(application.getInputs().values()))\n               .getWindowingStrategy(),\n           fn,\n           key,\n           sideInputs,\n           mainOutputTag,\n-          sideOutputTags,\n-          pcollections(application.getOutputs()));\n+          additionalOutputTags,\n+          pcollections(application.getOutputs()),\n+          runnerFactory);\n     } catch (Exception e) {\n       try {\n         fnManager.remove();\n@@ -162,10 +167,10 @@ public void cleanup() throws Exception {\n     }\n   }\n \n-  private Map<TupleTag<?>, PCollection<?>> pcollections(List<TaggedPValue> outputs) {\n+  static Map<TupleTag<?>, PCollection<?>> pcollections(Map<TupleTag<?>, PValue> outputs) {\n     Map<TupleTag<?>, PCollection<?>> pcs = new HashMap<>();\n-    for (TaggedPValue output : outputs) {\n-      pcs.put(output.getTag(), (PCollection<?>) output.getValue());\n+    for (Map.Entry<TupleTag<?>, PValue> output : outputs.entrySet()) {\n+      pcs.put(output.getKey(), (PCollection<?>) output.getValue());\n     }\n     return pcs;\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "sha": "b00c2b67a5d8b29e6d2cf64fc2786f366a319742",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 25,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "patch": "@@ -19,24 +19,23 @@\n \n import static com.google.common.base.Preconditions.checkState;\n \n-import com.google.common.collect.Iterables;\n-import java.util.List;\n import java.util.Map;\n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.KeyedWorkItemCoder;\n import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.core.SplittableParDo;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.runners.core.construction.ReplacementOutputs;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.ParDo.BoundMulti;\n+import org.apache.beam.sdk.transforms.ParDo.MultiOutput;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.AfterPane;\n@@ -50,7 +49,7 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.apache.beam.sdk.values.TupleTagList;\n import org.apache.beam.sdk.values.TypedPValue;\n \n@@ -61,11 +60,21 @@\n  */\n class ParDoMultiOverrideFactory<InputT, OutputT>\n     implements PTransformOverrideFactory<\n-        PCollection<? extends InputT>, PCollectionTuple, BoundMulti<InputT, OutputT>> {\n+        PCollection<? extends InputT>, PCollectionTuple, MultiOutput<InputT, OutputT>> {\n   @Override\n+  public PTransformReplacement<PCollection<? extends InputT>, PCollectionTuple>\n+      getReplacementTransform(\n+          AppliedPTransform<\n+                  PCollection<? extends InputT>, PCollectionTuple, MultiOutput<InputT, OutputT>>\n+              transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        getReplacementTransform(transform.getTransform()));\n+  }\n+\n   @SuppressWarnings(\"unchecked\")\n-  public PTransform<PCollection<? extends InputT>, PCollectionTuple> getReplacementTransform(\n-      BoundMulti<InputT, OutputT> transform) {\n+  private PTransform<PCollection<? extends InputT>, PCollectionTuple> getReplacementTransform(\n+      MultiOutput<InputT, OutputT> transform) {\n \n     DoFn<InputT, OutputT> fn = transform.getFn();\n     DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());\n@@ -75,32 +84,26 @@\n         || signature.timerDeclarations().size() > 0) {\n       // Based on the fact that the signature is stateful, DoFnSignatures ensures\n       // that it is also keyed\n-      ParDo.BoundMulti<KV<?, ?>, OutputT> keyedTransform =\n-          (ParDo.BoundMulti<KV<?, ?>, OutputT>) transform;\n+      MultiOutput<KV<?, ?>, OutputT> keyedTransform =\n+          (MultiOutput<KV<?, ?>, OutputT>) transform;\n \n       return new GbkThenStatefulParDo(keyedTransform);\n     } else {\n       return transform;\n     }\n   }\n \n-  @Override\n-  public PCollection<? extends InputT> getInput(\n-      List<TaggedPValue> inputs, Pipeline p) {\n-    return (PCollection<? extends InputT>) Iterables.getOnlyElement(inputs).getValue();\n-  }\n-\n   @Override\n   public Map<PValue, ReplacementOutput> mapOutputs(\n-      List<TaggedPValue> outputs, PCollectionTuple newOutput) {\n+      Map<TupleTag<?>, PValue> outputs, PCollectionTuple newOutput) {\n     return ReplacementOutputs.tagged(outputs, newOutput);\n   }\n \n   static class GbkThenStatefulParDo<K, InputT, OutputT>\n       extends PTransform<PCollection<KV<K, InputT>>, PCollectionTuple> {\n-    private final ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo;\n+    private final MultiOutput<KV<K, InputT>, OutputT> underlyingParDo;\n \n-    public GbkThenStatefulParDo(ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo) {\n+    public GbkThenStatefulParDo(MultiOutput<KV<K, InputT>, OutputT> underlyingParDo) {\n       this.underlyingParDo = underlyingParDo;\n     }\n \n@@ -135,8 +138,8 @@ public PCollectionTuple expand(PCollection<KV<K, InputT>> input) {\n               //  - ensure this GBK holds to the minimum of those timestamps (via OutputTimeFn)\n               //  - discard past panes as it is \"just a stream\" of elements\n               .apply(\n-                  Window.<KV<K, WindowedValue<KV<K, InputT>>>>triggering(\n-                          Repeatedly.forever(AfterPane.elementCountAtLeast(1)))\n+                  Window.<KV<K, WindowedValue<KV<K, InputT>>>>configure()\n+                      .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1)))\n                       .discardingFiredPanes()\n                       .withAllowedLateness(inputWindowingStrategy.getAllowedLateness())\n                       .withOutputTimeFn(OutputTimeFns.outputAtEarliestInputTimestamp()))\n@@ -165,17 +168,17 @@ public PCollectionTuple expand(PCollection<KV<K, InputT>> input) {\n \n   static class StatefulParDo<K, InputT, OutputT>\n       extends PTransform<PCollection<? extends KeyedWorkItem<K, KV<K, InputT>>>, PCollectionTuple> {\n-    private final transient ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo;\n+    private final transient MultiOutput<KV<K, InputT>, OutputT> underlyingParDo;\n     private final transient PCollection<KV<K, InputT>> originalInput;\n \n     public StatefulParDo(\n-        ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingParDo,\n+        MultiOutput<KV<K, InputT>, OutputT> underlyingParDo,\n         PCollection<KV<K, InputT>> originalInput) {\n       this.underlyingParDo = underlyingParDo;\n       this.originalInput = originalInput;\n     }\n \n-    public ParDo.BoundMulti<KV<K, InputT>, OutputT> getUnderlyingParDo() {\n+    public MultiOutput<KV<K, InputT>, OutputT> getUnderlyingParDo() {\n       return underlyingParDo;\n     }\n \n@@ -193,7 +196,7 @@ public PCollectionTuple expand(PCollection<? extends KeyedWorkItem<K, KV<K, Inpu\n           PCollectionTuple.ofPrimitiveOutputsInternal(\n               input.getPipeline(),\n               TupleTagList.of(underlyingParDo.getMainOutputTag())\n-                  .and(underlyingParDo.getSideOutputTags().getAll()),\n+                  .and(underlyingParDo.getAdditionalOutputTags().getAll()),\n               input.getWindowingStrategy(),\n               input.isBounded());\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "sha": "b08aa8ebc4d19801c97f446d00926d6fc292965f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 70,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "patch": "@@ -1,70 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.direct;\n-\n-import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n-import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.ParDo.Bound;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionTuple;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.beam.sdk.values.TupleTagList;\n-\n-/**\n- * A {@link PTransformOverrideFactory} that overrides single-output {@link ParDo} to implement\n- * it in terms of multi-output {@link ParDo}.\n- */\n-class ParDoSingleViaMultiOverrideFactory<InputT, OutputT>\n-    extends SingleInputOutputOverrideFactory<\n-            PCollection<? extends InputT>, PCollection<OutputT>, Bound<InputT, OutputT>> {\n-  @Override\n-  public PTransform<PCollection<? extends InputT>, PCollection<OutputT>> getReplacementTransform(\n-      Bound<InputT, OutputT> transform) {\n-    return new ParDoSingleViaMulti<>(transform);\n-  }\n-\n-  static class ParDoSingleViaMulti<InputT, OutputT>\n-      extends PTransform<PCollection<? extends InputT>, PCollection<OutputT>> {\n-    private static final String MAIN_OUTPUT_TAG = \"main\";\n-\n-    private final ParDo.Bound<InputT, OutputT> underlyingParDo;\n-\n-    public ParDoSingleViaMulti(ParDo.Bound<InputT, OutputT> underlyingParDo) {\n-      this.underlyingParDo = underlyingParDo;\n-    }\n-\n-    @Override\n-    public PCollection<OutputT> expand(PCollection<? extends InputT> input) {\n-\n-      // Output tags for ParDo need only be unique up to applied transform\n-      TupleTag<OutputT> mainOutputTag = new TupleTag<OutputT>(MAIN_OUTPUT_TAG);\n-\n-      PCollectionTuple outputs =\n-          input.apply(\n-              ParDo.of(underlyingParDo.getFn())\n-                  .withSideInputs(underlyingParDo.getSideInputs())\n-                  .withOutputTags(mainOutputTag, TupleTagList.empty()));\n-      PCollection<OutputT> output = outputs.get(mainOutputTag);\n-\n-      output.setTypeDescriptor(underlyingParDo.getFn().getOutputTypeDescriptor());\n-      return output;\n-    }\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "sha": "f8597299217b41c90f02d96fd3d436c22e51dd66",
                "status": "removed"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java",
                "patch": "@@ -19,8 +19,11 @@\n \n import java.util.Collection;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult.State;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n+import org.joda.time.Duration;\n \n /**\n  * An executor that schedules and executes {@link AppliedPTransform AppliedPTransforms} for both\n@@ -40,8 +43,24 @@\n    * root {@link AppliedPTransform AppliedPTransforms} have completed, and all\n    * {@link CommittedBundle Bundles} have been consumed. Jobs may also terminate abnormally.\n    *\n-   * @throws Throwable whenever an executor thread throws anything, transfers the throwable to the\n+   * <p>Waits for up to the provided duration, or forever if the provided duration is less than or\n+   * equal to zero.\n+   *\n+   * @return The terminal state of the Pipeline.\n+   * @throws Exception whenever an executor thread throws anything, transfers to the\n    *                   waiting thread and rethrows it\n    */\n-  void awaitCompletion() throws Exception;\n+  State waitUntilFinish(Duration duration) throws Exception;\n+\n+  /**\n+   * Gets the current state of the {@link Pipeline}.\n+   */\n+  State getPipelineState();\n+\n+  /**\n+   * Shuts down the executor.\n+   *\n+   * <p>The executor may continue to run for a short time after this method returns.\n+   */\n+  void stop();\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/PipelineExecutor.java",
                "sha": "82f59a7c328820450c695e97a23f49515a60cca1",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 28,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "patch": "@@ -18,25 +18,34 @@\n package org.apache.beam.runners.direct;\n \n import java.util.Collection;\n+import java.util.List;\n import java.util.concurrent.Executors;\n+import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.ElementAndRestriction;\n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.OutputAndTimeBoundedSplittableProcessElementInvoker;\n import org.apache.beam.runners.core.OutputWindowedValue;\n+import org.apache.beam.runners.core.PushbackSideInputDoFnRunner;\n import org.apache.beam.runners.core.SplittableParDo;\n+import org.apache.beam.runners.core.SplittableParDo.ProcessFn;\n import org.apache.beam.runners.core.StateInternals;\n import org.apache.beam.runners.core.StateInternalsFactory;\n import org.apache.beam.runners.core.TimerInternals;\n import org.apache.beam.runners.core.TimerInternalsFactory;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n+import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n+import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n@@ -51,7 +60,11 @@\n \n   SplittableProcessElementsEvaluatorFactory(EvaluationContext evaluationContext) {\n     this.evaluationContext = evaluationContext;\n-    this.delegateFactory = new ParDoEvaluatorFactory<>(evaluationContext);\n+    this.delegateFactory =\n+        new ParDoEvaluatorFactory<>(\n+            evaluationContext,\n+            SplittableProcessElementsEvaluatorFactory\n+                .<InputT, OutputT, RestrictionT>processFnRunnerFactory());\n   }\n \n   @Override\n@@ -82,12 +95,12 @@ public void cleanup() throws Exception {\n     final SplittableParDo.ProcessElements<InputT, OutputT, RestrictionT, TrackerT> transform =\n         application.getTransform();\n \n-    SplittableParDo.ProcessFn<InputT, OutputT, RestrictionT, TrackerT> processFn =\n+    ProcessFn<InputT, OutputT, RestrictionT, TrackerT> processFn =\n         transform.newProcessFn(transform.getFn());\n \n     DoFnLifecycleManager fnManager = DoFnLifecycleManager.of(processFn);\n     processFn =\n-        ((SplittableParDo.ProcessFn<InputT, OutputT, RestrictionT, TrackerT>)\n+        ((ProcessFn<InputT, OutputT, RestrictionT, TrackerT>)\n             fnManager\n                 .<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n                     get());\n@@ -98,14 +111,14 @@ public void cleanup() throws Exception {\n             .getExecutionContext(application, inputBundle.getKey())\n             .getOrCreateStepContext(stepName, stepName);\n \n-    ParDoEvaluator<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+    final ParDoEvaluator<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>>\n         parDoEvaluator =\n             delegateFactory.createParDoEvaluator(\n                 application,\n                 inputBundle.getKey(),\n                 transform.getSideInputs(),\n                 transform.getMainOutputTag(),\n-                transform.getSideOutputTags().getAll(),\n+                transform.getAdditionalOutputTags().getAll(),\n                 stepContext,\n                 processFn,\n                 fnManager);\n@@ -127,34 +140,36 @@ public TimerInternals timerInternalsForKey(String key) {\n           }\n         });\n \n-    final OutputManager outputManager = parDoEvaluator.getOutputManager();\n+    OutputWindowedValue<OutputT> outputWindowedValue =\n+        new OutputWindowedValue<OutputT>() {\n+          private final OutputManager outputManager = parDoEvaluator.getOutputManager();\n+\n+          @Override\n+          public void outputWindowedValue(\n+              OutputT output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            outputManager.output(\n+                transform.getMainOutputTag(), WindowedValue.of(output, timestamp, windows, pane));\n+          }\n+\n+          @Override\n+          public <AdditionalOutputT> void outputWindowedValue(\n+              TupleTag<AdditionalOutputT> tag,\n+              AdditionalOutputT output,\n+              Instant timestamp,\n+              Collection<? extends BoundedWindow> windows,\n+              PaneInfo pane) {\n+            outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));\n+          }\n+        };\n     processFn.setProcessElementInvoker(\n         new OutputAndTimeBoundedSplittableProcessElementInvoker<\n             InputT, OutputT, RestrictionT, TrackerT>(\n             transform.getFn(),\n             evaluationContext.getPipelineOptions(),\n-            new OutputWindowedValue<OutputT>() {\n-              @Override\n-              public void outputWindowedValue(\n-                  OutputT output,\n-                  Instant timestamp,\n-                  Collection<? extends BoundedWindow> windows,\n-                  PaneInfo pane) {\n-                outputManager.output(\n-                    transform.getMainOutputTag(),\n-                    WindowedValue.of(output, timestamp, windows, pane));\n-              }\n-\n-              @Override\n-              public <SideOutputT> void sideOutputWindowedValue(\n-                  TupleTag<SideOutputT> tag,\n-                  SideOutputT output,\n-                  Instant timestamp,\n-                  Collection<? extends BoundedWindow> windows,\n-                  PaneInfo pane) {\n-                outputManager.output(tag, WindowedValue.of(output, timestamp, windows, pane));\n-              }\n-            },\n+            outputWindowedValue,\n             evaluationContext.createSideInputReader(transform.getSideInputs()),\n             // TODO: For better performance, use a higher-level executor?\n             Executors.newSingleThreadScheduledExecutor(Executors.defaultThreadFactory()),\n@@ -163,4 +178,41 @@ public void outputWindowedValue(\n \n     return DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(parDoEvaluator, fnManager);\n   }\n+\n+  private static <InputT, OutputT, RestrictionT>\n+  ParDoEvaluator.DoFnRunnerFactory<\n+                KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+          processFnRunnerFactory() {\n+    return new ParDoEvaluator.DoFnRunnerFactory<\n+            KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>() {\n+      @Override\n+      public PushbackSideInputDoFnRunner<\n+          KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT>\n+      createRunner(\n+          PipelineOptions options,\n+          DoFn<KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>, OutputT> fn,\n+          List<PCollectionView<?>> sideInputs,\n+          ReadyCheckingSideInputReader sideInputReader,\n+          OutputManager outputManager,\n+          TupleTag<OutputT> mainOutputTag,\n+          List<TupleTag<?>> additionalOutputTags,\n+          DirectExecutionContext.DirectStepContext stepContext,\n+          AggregatorContainer.Mutator aggregatorChanges,\n+          WindowingStrategy<?, ? extends BoundedWindow> windowingStrategy) {\n+        ProcessFn<InputT, OutputT, RestrictionT, ?> processFn =\n+            (ProcessFn) fn;\n+        return DoFnRunners.newProcessFnRunner(\n+            processFn,\n+            options,\n+            sideInputs,\n+            sideInputReader,\n+            outputManager,\n+            mainOutputTag,\n+            additionalOutputTags,\n+            stepContext,\n+            aggregatorChanges,\n+            windowingStrategy);\n+      }\n+    };\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "sha": "7efdb5263f02ed14b01aaf43ce76d00a9ec24b9d",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 10,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "patch": "@@ -17,19 +17,22 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import static com.google.common.base.Preconditions.checkState;\n+\n import com.google.auto.value.AutoValue;\n import com.google.common.cache.CacheBuilder;\n import com.google.common.cache.CacheLoader;\n import com.google.common.cache.LoadingCache;\n-import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.Map.Entry;\n import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.core.StateNamespace;\n import org.apache.beam.runners.core.StateNamespaces;\n+import org.apache.beam.runners.core.StateNamespaces.WindowNamespace;\n import org.apache.beam.runners.core.StateTag;\n import org.apache.beam.runners.core.StateTags;\n import org.apache.beam.runners.core.TimerInternals.TimerData;\n@@ -50,7 +53,7 @@\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TupleTag;\n \n /** A {@link TransformEvaluatorFactory} for stateful {@link ParDo}. */\n@@ -62,7 +65,9 @@\n   private final ParDoEvaluatorFactory<KV<K, InputT>, OutputT> delegateFactory;\n \n   StatefulParDoEvaluatorFactory(EvaluationContext evaluationContext) {\n-    this.delegateFactory = new ParDoEvaluatorFactory<>(evaluationContext);\n+    this.delegateFactory =\n+        new ParDoEvaluatorFactory<>(\n+            evaluationContext, ParDoEvaluator.<KV<K, InputT>, OutputT>defaultRunnerFactory());\n     this.cleanupRegistry =\n         CacheBuilder.newBuilder()\n             .weakValues()\n@@ -117,7 +122,7 @@ public void cleanup() throws Exception {\n             doFn,\n             application.getTransform().getUnderlyingParDo().getSideInputs(),\n             application.getTransform().getUnderlyingParDo().getMainOutputTag(),\n-            application.getTransform().getUnderlyingParDo().getSideOutputTags().getAll());\n+            application.getTransform().getUnderlyingParDo().getAdditionalOutputTags().getAll());\n \n     return new StatefulParDoEvaluator<>(delegateEvaluator);\n   }\n@@ -137,8 +142,9 @@ public Runnable load(\n       String stepName = evaluationContext.getStepName(transformOutputWindow.getTransform());\n \n       Map<TupleTag<?>, PCollection<?>> taggedValues = new HashMap<>();\n-      for (TaggedPValue pv : transformOutputWindow.getTransform().getOutputs()) {\n-        taggedValues.put(pv.getTag(), (PCollection<?>) pv.getValue());\n+      for (Entry<TupleTag<?>, PValue> pv :\n+          transformOutputWindow.getTransform().getOutputs().entrySet()) {\n+        taggedValues.put(pv.getKey(), (PCollection<?>) pv.getValue());\n       }\n       PCollection<?> pc =\n           taggedValues\n@@ -228,15 +234,20 @@ public StatefulParDoEvaluator(\n     @Override\n     public void processElement(WindowedValue<KeyedWorkItem<K, KV<K, InputT>>> gbkResult)\n         throws Exception {\n-\n-      BoundedWindow window = Iterables.getOnlyElement(gbkResult.getWindows());\n-\n       for (WindowedValue<KV<K, InputT>> windowedValue : gbkResult.getValue().elementsIterable()) {\n         delegateEvaluator.processElement(windowedValue);\n       }\n \n       for (TimerData timer : gbkResult.getValue().timersIterable()) {\n-        delegateEvaluator.onTimer(timer, window);\n+        checkState(\n+            timer.getNamespace() instanceof WindowNamespace,\n+            \"Expected Timer %s to be in a %s, but got %s\",\n+            timer,\n+            WindowNamespace.class.getSimpleName(),\n+            timer.getNamespace().getClass().getName());\n+        WindowNamespace<?> windowNamespace = (WindowNamespace) timer.getNamespace();\n+        BoundedWindow timerWindow = windowNamespace.getWindow();\n+        delegateEvaluator.onTimer(timer, timerWindow);\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "sha": "8793ae887f966f6b97b6f8557ba64b91d26c1a0d",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 12,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.beam.runners.core.construction.ReplacementOutputs;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.testing.TestStream.ElementEvent;\n@@ -48,8 +47,8 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n \n@@ -108,7 +107,7 @@ public void processElement(WindowedValue<TestStreamIndex<T>> element) throws Exc\n       if (event.getType().equals(EventType.ELEMENT)) {\n         UncommittedBundle<T> bundle =\n             context.createBundle(\n-                (PCollection<T>) Iterables.getOnlyElement(application.getOutputs()).getValue());\n+                (PCollection<T>) Iterables.getOnlyElement(application.getOutputs().values()));\n         for (TimestampedValue<T> elem : ((ElementEvent<T>) event).getElements()) {\n           bundle.add(\n               WindowedValue.timestampedValueInGlobalWindow(elem.getValue(), elem.getTimestamp()));\n@@ -170,19 +169,16 @@ public Clock get() {\n     }\n \n     @Override\n-    public PTransform<PBegin, PCollection<T>> getReplacementTransform(\n-        TestStream<T> transform) {\n-      return new DirectTestStream<>(runner, transform);\n-    }\n-\n-    @Override\n-    public PBegin getInput(List<TaggedPValue> inputs, Pipeline p) {\n-      return p.begin();\n+    public PTransformReplacement<PBegin, PCollection<T>> getReplacementTransform(\n+        AppliedPTransform<PBegin, PCollection<T>, TestStream<T>> transform) {\n+      return PTransformReplacement.of(\n+          transform.getPipeline().begin(),\n+          new DirectTestStream<T>(runner, transform.getTransform()));\n     }\n \n     @Override\n     public Map<PValue, ReplacementOutput> mapOutputs(\n-        List<TaggedPValue> outputs, PCollection<T> newOutput) {\n+        Map<TupleTag<?>, PValue> outputs, PCollection<T> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "sha": "cba754ee9438607c4ddc292908c571d1b1610f78",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 5,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.beam.runners.direct;\n \n import javax.annotation.Nullable;\n-import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n@@ -48,13 +47,14 @@\n    */\n   @Nullable\n   <InputT> TransformEvaluator<InputT> forApplication(\n-      AppliedPTransform<?, ?, ?> application, CommittedBundle<?> inputBundle)\n+      AppliedPTransform<?, ?, ?> application, DirectRunner.CommittedBundle<?> inputBundle)\n       throws Exception;\n \n   /**\n-   * Cleans up any state maintained by this {@link TransformEvaluatorFactory}. Called after a {@link\n-   * Pipeline} is shut down. No more calls to {@link #forApplication(AppliedPTransform,\n-   * CommittedBundle)} will be made after a call to {@link #cleanup()}.\n+   * Cleans up any state maintained by this {@link TransformEvaluatorFactory}. Called after a\n+   * {@link Pipeline} is shut down. No more calls to\n+   * {@link #forApplication(AppliedPTransform, DirectRunner.CommittedBundle)} will be made after\n+   * a call to {@link #cleanup()}.\n    */\n   void cleanup() throws Exception;\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorFactory.java",
                "sha": "c7bc46f5da39029d06de71796b21ae9d47908ebc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.beam.runners.direct.DirectGroupByKey.DirectGroupByKeyOnly;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.ParDoMultiOverrideFactory.StatefulParDo;\n+import org.apache.beam.runners.direct.ViewOverrideFactory.WriteView;\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Flatten.PCollections;\n@@ -51,10 +52,12 @@ public static TransformEvaluatorRegistry defaultRegistry(EvaluationContext ctxt)\n         ImmutableMap.<Class<? extends PTransform>, TransformEvaluatorFactory>builder()\n             .put(Read.Bounded.class, new BoundedReadEvaluatorFactory(ctxt))\n             .put(Read.Unbounded.class, new UnboundedReadEvaluatorFactory(ctxt))\n-            .put(ParDo.BoundMulti.class, new ParDoEvaluatorFactory<>(ctxt))\n+            .put(\n+                ParDo.MultiOutput.class,\n+                new ParDoEvaluatorFactory<>(ctxt, ParDoEvaluator.defaultRunnerFactory()))\n             .put(StatefulParDo.class, new StatefulParDoEvaluatorFactory<>(ctxt))\n             .put(PCollections.class, new FlattenEvaluatorFactory(ctxt))\n-            .put(ViewEvaluatorFactory.WriteView.class, new ViewEvaluatorFactory(ctxt))\n+            .put(WriteView.class, new ViewEvaluatorFactory(ctxt))\n             .put(Window.Assign.class, new WindowEvaluatorFactory(ctxt))\n             // Runner-specific primitives used in expansion of GroupByKey\n             .put(DirectGroupByKeyOnly.class, new GroupByKeyOnlyEvaluatorFactory(ctxt))",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformEvaluatorRegistry.java",
                "sha": "d06c4601c2808f6c93fea690978633db4c7d4c67",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java",
                "patch": "@@ -32,4 +32,10 @@\n    * {@link TransformExecutor TransformExecutors} to be evaluated.\n    */\n   void complete(TransformExecutor<?> completed);\n+\n+  /**\n+   * Cancel any outstanding work, if possible. Any future calls to schedule should ignore any\n+   * work.\n+   */\n+  void shutdown();\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorService.java",
                "sha": "c6f770f8012e1f2f946a4e1994a950656d242075",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 9,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java",
                "patch": "@@ -21,7 +21,11 @@\n import java.util.Queue;\n import java.util.concurrent.ConcurrentLinkedQueue;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.RejectedExecutionException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicReference;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n /**\n  * Static factory methods for constructing instances of {@link TransformExecutorService}.\n@@ -36,15 +40,15 @@ private TransformExecutorServices() {\n    * parallel.\n    */\n   public static TransformExecutorService parallel(ExecutorService executor) {\n-    return new ParallelEvaluationState(executor);\n+    return new ParallelTransformExecutor(executor);\n   }\n \n   /**\n    * Returns an EvaluationState that evaluates {@link TransformExecutor TransformExecutors} in\n    * serial.\n    */\n   public static TransformExecutorService serial(ExecutorService executor) {\n-    return new SerialEvaluationState(executor);\n+    return new SerialTransformExecutor(executor);\n   }\n \n   /**\n@@ -54,21 +58,47 @@ public static TransformExecutorService serial(ExecutorService executor) {\n    * <p>A principal use of this is for the evaluation of an unkeyed Step. Unkeyed computations are\n    * processed in parallel.\n    */\n-  private static class ParallelEvaluationState implements TransformExecutorService {\n+  private static class ParallelTransformExecutor implements TransformExecutorService {\n+    private static final Logger LOG = LoggerFactory.getLogger(ParallelTransformExecutor.class);\n+\n     private final ExecutorService executor;\n+    private final AtomicBoolean active = new AtomicBoolean(true);\n \n-    private ParallelEvaluationState(ExecutorService executor) {\n+    private ParallelTransformExecutor(ExecutorService executor) {\n       this.executor = executor;\n     }\n \n     @Override\n     public void schedule(TransformExecutor<?> work) {\n-      executor.submit(work);\n+      if (active.get()) {\n+        try {\n+          executor.submit(work);\n+        } catch (RejectedExecutionException rejected) {\n+          boolean stillActive = active.get();\n+          if (stillActive) {\n+            throw new IllegalStateException(\n+                String.format(\n+                    \"Execution of Work %s was rejected, but the %s is still active\",\n+                    work, ParallelTransformExecutor.class.getSimpleName()));\n+          } else {\n+            LOG.debug(\n+                \"Rejected execution of Work {} on executor {}. \"\n+                    + \"Suppressed exception because evaluator is not active\",\n+                work,\n+                this);\n+          }\n+        }\n+      }\n     }\n \n     @Override\n     public void complete(TransformExecutor<?> completed) {\n     }\n+\n+    @Override\n+    public void shutdown() {\n+      active.set(false);\n+    }\n   }\n \n   /**\n@@ -79,13 +109,14 @@ public void complete(TransformExecutor<?> completed) {\n    * <p>A principal use of this is for the serial evaluation of a (Step, Key) pair.\n    * Keyed computations are processed serially per step.\n    */\n-  private static class SerialEvaluationState implements TransformExecutorService {\n+  private static class SerialTransformExecutor implements TransformExecutorService {\n     private final ExecutorService executor;\n \n     private AtomicReference<TransformExecutor<?>> currentlyEvaluating;\n     private final Queue<TransformExecutor<?>> workQueue;\n+    private boolean active = true;\n \n-    private SerialEvaluationState(ExecutorService executor) {\n+    private SerialTransformExecutor(ExecutorService executor) {\n       this.executor = executor;\n       this.currentlyEvaluating = new AtomicReference<>();\n       this.workQueue = new ConcurrentLinkedQueue<>();\n@@ -113,12 +144,20 @@ public void complete(TransformExecutor<?> completed) {\n       updateCurrentlyEvaluating();\n     }\n \n+    @Override\n+    public void shutdown() {\n+      synchronized (this) {\n+        active = false;\n+      }\n+      workQueue.clear();\n+    }\n+\n     private void updateCurrentlyEvaluating() {\n       if (currentlyEvaluating.get() == null) {\n         // Only synchronize if we need to update what's currently evaluating\n         synchronized (this) {\n           TransformExecutor<?> newWork = workQueue.poll();\n-          if (newWork != null) {\n+          if (active && newWork != null) {\n             if (currentlyEvaluating.compareAndSet(null, newWork)) {\n               executor.submit(newWork);\n             } else {\n@@ -131,7 +170,7 @@ private void updateCurrentlyEvaluating() {\n \n     @Override\n     public String toString() {\n-      return MoreObjects.toStringHelper(SerialEvaluationState.class)\n+      return MoreObjects.toStringHelper(SerialTransformExecutor.class)\n           .add(\"currentlyEvaluating\", currentlyEvaluating)\n           .add(\"workQueue\", workQueue)\n           .toString();",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformExecutorServices.java",
                "sha": "53087bfa32b72020544eaa7f623abc4a384e3a73",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 14,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.CoderUtils;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n@@ -120,7 +121,7 @@ public void processElement(\n         WindowedValue<UnboundedSourceShard<OutputT, CheckpointMarkT>> element) throws IOException {\n       UncommittedBundle<OutputT> output =\n           evaluationContext.createBundle(\n-              (PCollection<OutputT>) getOnlyElement(transform.getOutputs()).getValue());\n+              (PCollection<OutputT>) getOnlyElement(transform.getOutputs().values()));\n       UnboundedSourceShard<OutputT, CheckpointMarkT> shard = element.getValue();\n       UnboundedReader<OutputT> reader = null;\n       try {\n@@ -139,7 +140,24 @@ public void processElement(\n             numElements++;\n           } while (numElements < ARBITRARY_MAX_ELEMENTS && reader.advance());\n           Instant watermark = reader.getWatermark();\n-          UnboundedSourceShard<OutputT, CheckpointMarkT> residual = finishRead(reader, shard);\n+\n+          CheckpointMarkT finishedCheckpoint = finishRead(reader, shard);\n+          UnboundedSourceShard<OutputT, CheckpointMarkT> residual;\n+          // Sometimes resume from a checkpoint even if it's not required\n+          if (ThreadLocalRandom.current().nextDouble(1.0) >= readerReuseChance) {\n+            UnboundedReader<OutputT> toClose = reader;\n+            // Prevent double-close. UnboundedReader is AutoCloseable, which does not require\n+            // idempotency of close. Nulling out the reader here prevents trying to re-close it\n+            // if the call to close throws an IOException.\n+            reader = null;\n+            toClose.close();\n+            residual =\n+                UnboundedSourceShard.of(\n+                    shard.getSource(), shard.getDeduplicator(), null, finishedCheckpoint);\n+          } else {\n+            residual = shard.withCheckpoint(finishedCheckpoint);\n+          }\n+\n           resultBuilder\n               .addOutput(output)\n               .addUnprocessedElements(\n@@ -171,7 +189,7 @@ public void processElement(\n       if (existing == null) {\n         CheckpointMarkT checkpoint = shard.getCheckpoint();\n         if (checkpoint != null) {\n-          checkpoint.finalizeCheckpoint();\n+          checkpoint = CoderUtils.clone(shard.getSource().getCheckpointMarkCoder(), checkpoint);\n         }\n         return shard\n             .getSource()\n@@ -195,7 +213,7 @@ private boolean startReader(\n      * Checkpoint the current reader, finalize the previous checkpoint, and return the residual\n      * {@link UnboundedSourceShard}.\n      */\n-    private UnboundedSourceShard<OutputT, CheckpointMarkT> finishRead(\n+    private CheckpointMarkT finishRead(\n         UnboundedReader<OutputT> reader, UnboundedSourceShard<OutputT, CheckpointMarkT> shard)\n         throws IOException {\n       final CheckpointMark oldMark = shard.getCheckpoint();\n@@ -209,7 +227,7 @@ private boolean startReader(\n       // committing the output.\n       if (!reader.getWatermark().isBefore(BoundedWindow.TIMESTAMP_MAX_VALUE)) {\n         PCollection<OutputT> outputPc =\n-            (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs()).getValue();\n+            (PCollection<OutputT>) Iterables.getOnlyElement(transform.getOutputs().values());\n         evaluationContext.scheduleAfterOutputWouldBeProduced(\n             outputPc,\n             GlobalWindow.INSTANCE,\n@@ -226,14 +244,7 @@ public void run() {\n               }\n             });\n       }\n-\n-      // Sometimes resume from a checkpoint even if it's not required\n-      if (ThreadLocalRandom.current().nextDouble(1.0) >= readerReuseChance) {\n-        reader.close();\n-        return UnboundedSourceShard.of(shard.getSource(), shard.getDeduplicator(), null, mark);\n-      } else {\n-        return shard.withCheckpoint(mark);\n-      }\n+      return mark;\n     }\n \n     @Override\n@@ -290,7 +301,7 @@ public void run() {\n         throws Exception {\n       UnboundedSource<OutputT, ?> source = transform.getTransform().getSource();\n       List<? extends UnboundedSource<OutputT, ?>> splits =\n-          source.generateInitialSplits(targetParallelism, evaluationContext.getPipelineOptions());\n+          source.split(targetParallelism, evaluationContext.getPipelineOptions());\n       UnboundedReadDeduplicator deduplicator =\n           source.requiresDeduping()\n               ? UnboundedReadDeduplicator.CachedIdDeduplicator.create()",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "sha": "d3609f89dc4509a4387298c65c571f90349af2f7",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 73,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "patch": "@@ -20,31 +20,25 @@\n import com.google.common.collect.Iterables;\n import java.util.ArrayList;\n import java.util.List;\n-import org.apache.beam.runners.core.construction.SingleInputOutputOverrideFactory;\n import org.apache.beam.runners.direct.CommittedResult.OutputType;\n import org.apache.beam.runners.direct.DirectRunner.PCollectionViewWriter;\n import org.apache.beam.runners.direct.StepTransformResult.Builder;\n-import org.apache.beam.sdk.coders.KvCoder;\n-import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.runners.direct.ViewOverrideFactory.WriteView;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n-import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.Values;\n import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n-import org.apache.beam.sdk.transforms.WithKeys;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n \n /**\n- * The {@link DirectRunner} {@link TransformEvaluatorFactory} for the\n- * {@link CreatePCollectionView} primitive {@link PTransform}.\n+ * The {@link DirectRunner} {@link TransformEvaluatorFactory} for the {@link CreatePCollectionView}\n+ * primitive {@link PTransform}.\n  *\n  * <p>The {@link ViewEvaluatorFactory} produces {@link TransformEvaluator TransformEvaluators} for\n- * the {@link WriteView} {@link PTransform}, which is part of the\n- * {@link DirectCreatePCollectionView} composite transform. This transform is an override for the\n- * {@link CreatePCollectionView} transform that applies windowing and triggers before the view is\n- * written.\n+ * the {@link WriteView} {@link PTransform}, which is part of the {@link DirectRunner} override.\n+ * This transform is an override for the {@link CreatePCollectionView} transform that applies\n+ * windowing and triggers before the view is written.\n  */\n class ViewEvaluatorFactory implements TransformEvaluatorFactory {\n   private final EvaluationContext context;\n@@ -70,9 +64,9 @@ public void cleanup() throws Exception {}\n       final AppliedPTransform<PCollection<Iterable<InT>>, PCollectionView<OuT>, WriteView<InT, OuT>>\n           application) {\n     PCollection<Iterable<InT>> input =\n-        (PCollection<Iterable<InT>>) Iterables.getOnlyElement(application.getInputs()).getValue();\n+        (PCollection<Iterable<InT>>) Iterables.getOnlyElement(application.getInputs().values());\n     final PCollectionViewWriter<InT, OuT> writer = context.createPCollectionViewWriter(input,\n-        (PCollectionView<OuT>) Iterables.getOnlyElement(application.getOutputs()).getValue());\n+        (PCollectionView<OuT>) Iterables.getOnlyElement(application.getOutputs().values()));\n     return new TransformEvaluator<Iterable<InT>>() {\n       private final List<WindowedValue<InT>> elements = new ArrayList<>();\n \n@@ -90,67 +84,9 @@ public void processElement(WindowedValue<Iterable<InT>> element) {\n         if (!elements.isEmpty()) {\n           resultBuilder = resultBuilder.withAdditionalOutput(OutputType.PCOLLECTION_VIEW);\n         }\n-        return resultBuilder\n-            .build();\n+        return resultBuilder.build();\n       }\n     };\n   }\n \n-  public static class ViewOverrideFactory<ElemT, ViewT>\n-      extends SingleInputOutputOverrideFactory<\n-                PCollection<ElemT>, PCollectionView<ViewT>, CreatePCollectionView<ElemT, ViewT>> {\n-    @Override\n-    public PTransform<PCollection<ElemT>, PCollectionView<ViewT>> getReplacementTransform(\n-        CreatePCollectionView<ElemT, ViewT> transform) {\n-      return new DirectCreatePCollectionView<>(transform);\n-    }\n-  }\n-\n-  /**\n-   * An in-process override for {@link CreatePCollectionView}.\n-   */\n-  private static class DirectCreatePCollectionView<ElemT, ViewT>\n-      extends ForwardingPTransform<PCollection<ElemT>, PCollectionView<ViewT>> {\n-    private final CreatePCollectionView<ElemT, ViewT> og;\n-\n-    private DirectCreatePCollectionView(CreatePCollectionView<ElemT, ViewT> og) {\n-      this.og = og;\n-    }\n-\n-    @Override\n-    public PCollectionView<ViewT> expand(PCollection<ElemT> input) {\n-      return input.apply(WithKeys.<Void, ElemT>of((Void) null))\n-          .setCoder(KvCoder.of(VoidCoder.of(), input.getCoder()))\n-          .apply(GroupByKey.<Void, ElemT>create())\n-          .apply(Values.<Iterable<ElemT>>create())\n-          .apply(new WriteView<ElemT, ViewT>(og));\n-    }\n-\n-    @Override\n-    protected PTransform<PCollection<ElemT>, PCollectionView<ViewT>> delegate() {\n-      return og;\n-    }\n-  }\n-\n-  /**\n-   * An in-process implementation of the {@link CreatePCollectionView} primitive.\n-   *\n-   * <p>This implementation requires the input {@link PCollection} to be an iterable\n-   * of {@code WindowedValue<ElemT>}, which is provided\n-   * to {@link PCollectionView#getViewFn()} for conversion to {@link ViewT}.\n-   */\n-  public static final class WriteView<ElemT, ViewT>\n-      extends PTransform<PCollection<Iterable<ElemT>>, PCollectionView<ViewT>> {\n-    private final CreatePCollectionView<ElemT, ViewT> og;\n-\n-    WriteView(CreatePCollectionView<ElemT, ViewT> og) {\n-      this.og = og;\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"deprecation\")\n-    public PCollectionView<ViewT> expand(PCollection<Iterable<ElemT>> input) {\n-      return og.getView();\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "sha": "8cbe8fc4e08c5c7a133f8029306ac1830344914e",
                "status": "modified"
            },
            {
                "additions": 114,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java",
                "patch": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.direct;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+import org.apache.beam.runners.core.construction.ForwardingPTransform;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.transforms.WithKeys;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n+\n+/**\n+ * A {@link PTransformOverrideFactory} that provides overrides for the {@link CreatePCollectionView}\n+ * {@link PTransform}.\n+ */\n+class ViewOverrideFactory<ElemT, ViewT>\n+    implements PTransformOverrideFactory<\n+        PCollection<ElemT>, PCollectionView<ViewT>, CreatePCollectionView<ElemT, ViewT>> {\n+\n+  @Override\n+  public PTransformReplacement<PCollection<ElemT>, PCollectionView<ViewT>> getReplacementTransform(\n+      AppliedPTransform<\n+              PCollection<ElemT>, PCollectionView<ViewT>, CreatePCollectionView<ElemT, ViewT>>\n+          transform) {\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        new GroupAndWriteView<>(transform.getTransform()));\n+  }\n+\n+  @Override\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, PCollectionView<ViewT> newOutput) {\n+    return Collections.emptyMap();\n+  }\n+\n+  /** The {@link DirectRunner} composite override for {@link CreatePCollectionView}. */\n+  static class GroupAndWriteView<ElemT, ViewT>\n+      extends ForwardingPTransform<PCollection<ElemT>, PCollectionView<ViewT>> {\n+    private final CreatePCollectionView<ElemT, ViewT> og;\n+\n+    private GroupAndWriteView(CreatePCollectionView<ElemT, ViewT> og) {\n+      this.og = og;\n+    }\n+\n+    @Override\n+    public PCollectionView<ViewT> expand(PCollection<ElemT> input) {\n+      return input\n+          .apply(WithKeys.<Void, ElemT>of((Void) null))\n+          .setCoder(KvCoder.of(VoidCoder.of(), input.getCoder()))\n+          .apply(GroupByKey.<Void, ElemT>create())\n+          .apply(Values.<Iterable<ElemT>>create())\n+          .apply(new WriteView<ElemT, ViewT>(og));\n+    }\n+\n+    @Override\n+    protected PTransform<PCollection<ElemT>, PCollectionView<ViewT>> delegate() {\n+      return og;\n+    }\n+  }\n+\n+  /**\n+   * The {@link DirectRunner} implementation of the {@link CreatePCollectionView} primitive.\n+   *\n+   * <p>This implementation requires the input {@link PCollection} to be an iterable of {@code\n+   * WindowedValue<ElemT>}, which is provided to {@link PCollectionView#getViewFn()} for conversion\n+   * to {@link ViewT}.\n+   */\n+  static final class WriteView<ElemT, ViewT>\n+      extends PTransform<PCollection<Iterable<ElemT>>, PCollectionView<ViewT>> {\n+    private final CreatePCollectionView<ElemT, ViewT> og;\n+\n+    WriteView(CreatePCollectionView<ElemT, ViewT> og) {\n+      this.og = og;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"deprecation\")\n+    public PCollectionView<ViewT> expand(PCollection<Iterable<ElemT>> input) {\n+      return og.getView();\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    public PCollectionView<ViewT> getView() {\n+      return og.getView();\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewOverrideFactory.java",
                "sha": "d4fd18fa1414cda6555c9fdbe4f084e9fd5952ae",
                "status": "added"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 9,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java",
                "patch": "@@ -61,7 +61,8 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.joda.time.Instant;\n \n /**\n@@ -818,13 +819,13 @@ private TransformWatermarks getTransformWatermark(AppliedPTransform<?, ?, ?> tra\n \n   private Collection<Watermark> getInputProcessingWatermarks(AppliedPTransform<?, ?, ?> transform) {\n     ImmutableList.Builder<Watermark> inputWmsBuilder = ImmutableList.builder();\n-    List<TaggedPValue> inputs = transform.getInputs();\n+    Map<TupleTag<?>, PValue> inputs = transform.getInputs();\n     if (inputs.isEmpty()) {\n       inputWmsBuilder.add(THE_END_OF_TIME);\n     }\n-    for (TaggedPValue pvalue : inputs) {\n+    for (PValue pvalue : inputs.values()) {\n       Watermark producerOutputWatermark =\n-          getTransformWatermark(graph.getProducer(pvalue.getValue()))\n+          getTransformWatermark(graph.getProducer(pvalue))\n               .synchronizedProcessingOutputWatermark;\n       inputWmsBuilder.add(producerOutputWatermark);\n     }\n@@ -833,13 +834,13 @@ private TransformWatermarks getTransformWatermark(AppliedPTransform<?, ?, ?> tra\n \n   private List<Watermark> getInputWatermarks(AppliedPTransform<?, ?, ?> transform) {\n     ImmutableList.Builder<Watermark> inputWatermarksBuilder = ImmutableList.builder();\n-    List<TaggedPValue> inputs = transform.getInputs();\n+    Map<TupleTag<?>, PValue> inputs = transform.getInputs();\n     if (inputs.isEmpty()) {\n       inputWatermarksBuilder.add(THE_END_OF_TIME);\n     }\n-    for (TaggedPValue pvalue : inputs) {\n+    for (PValue pvalue : inputs.values()) {\n       Watermark producerOutputWatermark =\n-          getTransformWatermark(graph.getProducer(pvalue.getValue())).outputWatermark;\n+          getTransformWatermark(graph.getProducer(pvalue)).outputWatermark;\n       inputWatermarksBuilder.add(producerOutputWatermark);\n     }\n     List<Watermark> inputCollectionWatermarks = inputWatermarksBuilder.build();\n@@ -1023,8 +1024,8 @@ synchronized void refreshAll() {\n     WatermarkUpdate updateResult = myWatermarks.refresh();\n     if (updateResult.isAdvanced()) {\n       Set<AppliedPTransform<?, ?, ?>> additionalRefreshes = new HashSet<>();\n-      for (TaggedPValue outputPValue : toRefresh.getOutputs()) {\n-        additionalRefreshes.addAll(graph.getPrimitiveConsumers(outputPValue.getValue()));\n+      for (PValue outputPValue : toRefresh.getOutputs().values()) {\n+        additionalRefreshes.addAll(graph.getPrimitiveConsumers(outputPValue));\n       }\n       return additionalRefreshes;\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WatermarkManager.java",
                "sha": "8c043629b08587f72b4ba2e074bcd058e64cdfad",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java",
                "patch": "@@ -57,7 +57,7 @@\n     WindowFn<? super InputT, ?> fn = transform.getTransform().getWindowFn();\n     UncommittedBundle<InputT> outputBundle =\n         evaluationContext.createBundle(\n-            (PCollection<InputT>) Iterables.getOnlyElement(transform.getOutputs()).getValue());\n+            (PCollection<InputT>) Iterables.getOnlyElement(transform.getOutputs().values()));\n     if (fn == null) {\n       return PassthroughTransformEvaluator.create(transform, outputBundle);\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WindowEvaluatorFactory.java",
                "sha": "25509249a609a4ed8bcc77802caec425d7da2171",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 13,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "patch": "@@ -21,25 +21,26 @@\n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Supplier;\n import com.google.common.base.Suppliers;\n-import com.google.common.collect.Iterables;\n import java.io.Serializable;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ThreadLocalRandom;\n-import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.runners.core.construction.PTransformReplacements;\n import org.apache.beam.sdk.io.Write;\n import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Count;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PDone;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.beam.sdk.values.TaggedPValue;\n+import org.apache.beam.sdk.values.TupleTag;\n \n /**\n  * A {@link PTransformOverrideFactory} that overrides {@link Write} {@link PTransform PTransforms}\n@@ -52,19 +53,17 @@\n   @VisibleForTesting static final int MIN_SHARDS_FOR_LOG = 3;\n \n   @Override\n-  public PTransform<PCollection<InputT>, PDone> getReplacementTransform(\n-      Write<InputT> transform) {\n+  public PTransformReplacement<PCollection<InputT>, PDone> getReplacementTransform(\n+      AppliedPTransform<PCollection<InputT>, PDone, Write<InputT>> transform) {\n \n-      return transform.withSharding(new LogElementShardsWithDrift<InputT>());\n+    return PTransformReplacement.of(\n+        PTransformReplacements.getSingletonMainInput(transform),\n+        transform.getTransform().withSharding(new LogElementShardsWithDrift<InputT>()));\n   }\n \n   @Override\n-  public PCollection<InputT> getInput(List<TaggedPValue> inputs, Pipeline p) {\n-    return (PCollection<InputT>) Iterables.getOnlyElement(inputs).getValue();\n-  }\n-\n-  @Override\n-  public Map<PValue, ReplacementOutput> mapOutputs(List<TaggedPValue> outputs, PDone newOutput) {\n+  public Map<PValue, ReplacementOutput> mapOutputs(\n+      Map<TupleTag<?>, PValue> outputs, PDone newOutput) {\n     return Collections.emptyMap();\n   }\n \n@@ -74,6 +73,7 @@\n     @Override\n     public PCollectionView<Integer> expand(PCollection<T> records) {\n       return records\n+          .apply(Window.<T>into(new GlobalWindows()))\n           .apply(\"CountRecords\", Count.<T>globally())\n           .apply(\"GenerateShardCount\", ParDo.of(new CalculateShardsFn()))\n           .apply(View.<Integer>asSingleton());",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "sha": "a23ab94f54d742fd71c6a609e655b657ec8d4612",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "patch": "@@ -265,7 +265,7 @@ public void getInitialInputsSplitsIntoBundles() throws Exception {\n   public void boundedSourceInMemoryTransformEvaluatorShardsOfSource() throws Exception {\n     PipelineOptions options = PipelineOptionsFactory.create();\n     List<? extends BoundedSource<Long>> splits =\n-        source.splitIntoBundles(source.getEstimatedSizeBytes(options) / 2, options);\n+        source.split(source.getEstimatedSizeBytes(options) / 2, options);\n \n     UncommittedBundle<BoundedSourceShard<Long>> rootBundle = bundleFactory.createRootBundle();\n     for (BoundedSource<Long> split : splits) {\n@@ -365,7 +365,7 @@ public TestSource(Coder<T> coder, int firstSplitIndex, T... elems) {\n     }\n \n     @Override\n-    public List<? extends OffsetBasedSource<T>> splitIntoBundles(\n+    public List<? extends OffsetBasedSource<T>> split(\n         long desiredBundleSizeBytes, PipelineOptions options) throws Exception {\n       return ImmutableList.of(this);\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "sha": "2b5b46d64eca628c72c3c9c84394aaff87a48ec2",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 17,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "patch": "@@ -45,9 +45,9 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n-import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n import org.apache.beam.sdk.util.state.BagState;\n import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.GroupingState;\n import org.apache.beam.sdk.util.state.MapState;\n import org.apache.beam.sdk.util.state.SetState;\n import org.apache.beam.sdk.util.state.ValueState;\n@@ -201,24 +201,24 @@ public void testMapStateWithUnderlying() {\n     StateTag<Object, MapState<String, Integer>> valueTag =\n         StateTags.map(\"foo\", StringUtf8Coder.of(), VarIntCoder.of());\n     MapState<String, Integer> underlyingValue = underlying.state(namespace, valueTag);\n-    assertThat(underlyingValue.iterate(), emptyIterable());\n+    assertThat(underlyingValue.entries().read(), emptyIterable());\n \n     underlyingValue.put(\"hello\", 1);\n-    assertThat(underlyingValue.get(\"hello\"), equalTo(1));\n+    assertThat(underlyingValue.get(\"hello\").read(), equalTo(1));\n \n     CopyOnAccessInMemoryStateInternals<String> internals =\n         CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);\n     MapState<String, Integer> copyOnAccessState = internals.state(namespace, valueTag);\n-    assertThat(copyOnAccessState.get(\"hello\"), equalTo(1));\n+    assertThat(copyOnAccessState.get(\"hello\").read(), equalTo(1));\n \n     copyOnAccessState.put(\"world\", 4);\n-    assertThat(copyOnAccessState.get(\"hello\"), equalTo(1));\n-    assertThat(copyOnAccessState.get(\"world\"), equalTo(4));\n-    assertThat(underlyingValue.get(\"hello\"), equalTo(1));\n-    assertNull(underlyingValue.get(\"world\"));\n+    assertThat(copyOnAccessState.get(\"hello\").read(), equalTo(1));\n+    assertThat(copyOnAccessState.get(\"world\").read(), equalTo(4));\n+    assertThat(underlyingValue.get(\"hello\").read(), equalTo(1));\n+    assertNull(underlyingValue.get(\"world\").read());\n \n     MapState<String, Integer> reReadUnderlyingValue = underlying.state(namespace, valueTag);\n-    assertThat(underlyingValue.iterate(), equalTo(reReadUnderlyingValue.iterate()));\n+    assertThat(underlyingValue.entries().read(), equalTo(reReadUnderlyingValue.entries().read()));\n   }\n \n   @Test\n@@ -229,25 +229,25 @@ public void testAccumulatorCombiningStateWithUnderlying() throws CannotProvideCo\n \n     StateNamespace namespace = new StateNamespaceForTest(\"foo\");\n     CoderRegistry reg = pipeline.getCoderRegistry();\n-    StateTag<Object, AccumulatorCombiningState<Long, long[], Long>> stateTag =\n+    StateTag<Object, CombiningState<Long, long[], Long>> stateTag =\n         StateTags.combiningValue(\"summer\",\n             sumLongFn.getAccumulatorCoder(reg, reg.getDefaultCoder(Long.class)), sumLongFn);\n-    CombiningState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(0L));\n \n     underlyingValue.add(1L);\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n     CopyOnAccessInMemoryStateInternals<String> internals =\n         CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);\n-    CombiningState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n+    GroupingState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n     assertThat(copyOnAccessState.read(), equalTo(1L));\n \n     copyOnAccessState.add(4L);\n     assertThat(copyOnAccessState.read(), equalTo(5L));\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n-    CombiningState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(reReadUnderlyingValue.read()));\n   }\n \n@@ -259,28 +259,28 @@ public void testKeyedAccumulatorCombiningStateWithUnderlying() throws Exception\n \n     StateNamespace namespace = new StateNamespaceForTest(\"foo\");\n     CoderRegistry reg = pipeline.getCoderRegistry();\n-    StateTag<String, AccumulatorCombiningState<Long, long[], Long>> stateTag =\n+    StateTag<String, CombiningState<Long, long[], Long>> stateTag =\n         StateTags.keyedCombiningValue(\n             \"summer\",\n             sumLongFn.getAccumulatorCoder(\n                 reg, StringUtf8Coder.of(), reg.getDefaultCoder(Long.class)),\n             sumLongFn);\n-    CombiningState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> underlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(0L));\n \n     underlyingValue.add(1L);\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n     CopyOnAccessInMemoryStateInternals<String> internals =\n         CopyOnAccessInMemoryStateInternals.withUnderlying(key, underlying);\n-    CombiningState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n+    GroupingState<Long, Long> copyOnAccessState = internals.state(namespace, stateTag);\n     assertThat(copyOnAccessState.read(), equalTo(1L));\n \n     copyOnAccessState.add(4L);\n     assertThat(copyOnAccessState.read(), equalTo(5L));\n     assertThat(underlyingValue.read(), equalTo(1L));\n \n-    CombiningState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n+    GroupingState<Long, Long> reReadUnderlyingValue = underlying.state(namespace, stateTag);\n     assertThat(underlyingValue.read(), equalTo(reReadUnderlyingValue.read()));\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "sha": "68c6613848796a19df0e267a85be7282df1a7ada",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "patch": "@@ -45,7 +45,6 @@\n import org.apache.beam.sdk.values.PDone;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.hamcrest.Matchers;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -101,9 +100,9 @@ public void getRootTransformsContainsRootTransforms() {\n             graph.getProducer(created), graph.getProducer(counted), graph.getProducer(unCounted)));\n     for (AppliedPTransform<?, ?, ?> root : graph.getRootTransforms())  {\n       // Root transforms will have no inputs\n-      assertThat(root.getInputs(), emptyIterable());\n+      assertThat(root.getInputs().entrySet(), emptyIterable());\n       assertThat(\n-          Iterables.getOnlyElement(root.getOutputs()).getValue(),\n+          Iterables.getOnlyElement(root.getOutputs().values()),\n           Matchers.<POutput>isOneOf(created, counted, unCounted));\n     }\n   }\n@@ -121,7 +120,7 @@ public void getRootTransformsContainsEmptyFlatten() {\n         Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(graph.getProducer(empty)));\n     AppliedPTransform<?, ?, ?> onlyRoot = Iterables.getOnlyElement(graph.getRootTransforms());\n     assertThat(onlyRoot.getTransform(), Matchers.<PTransform<?, ?>>equalTo(flatten));\n-    assertThat(onlyRoot.getInputs(), Matchers.<TaggedPValue>emptyIterable());\n+    assertThat(onlyRoot.getInputs().entrySet(), emptyIterable());\n     assertThat(onlyRoot.getOutputs(), equalTo(empty.expand()));\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "sha": "b44c89046a3ce1955a24edbc6eb3f3250e89e371",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java",
                "patch": "@@ -23,8 +23,11 @@\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VarIntCoder;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.PTransformReplacement;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.hamcrest.Matchers;\n@@ -45,7 +48,12 @@ public void getInputSucceeds() {\n         p.apply(\n             Create.of(KV.of(\"foo\", 1))\n                 .withCoder(KvCoder.of(StringUtf8Coder.of(), VarIntCoder.of())));\n-    PCollection<?> reconstructed = factory.getInput(input.expand(), p);\n-    assertThat(reconstructed, Matchers.<PCollection<?>>equalTo(input));\n+    PCollection<KV<String, Iterable<Integer>>> grouped =\n+        input.apply(GroupByKey.<String, Integer>create());\n+    AppliedPTransform<?, ?, ?> producer = DirectGraphs.getProducer(grouped);\n+    PTransformReplacement<\n+            PCollection<KV<String, Integer>>, PCollection<KV<String, Iterable<Integer>>>>\n+        replacement = factory.getReplacementTransform((AppliedPTransform) producer);\n+    assertThat(replacement.getInput(), Matchers.<PCollection<?>>equalTo(input));\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactoryTest.java",
                "sha": "28fef4c7e5d54b96890034b3dec9523b60b1f71c",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java",
                "changes": 124,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 68,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java",
                "patch": "@@ -23,22 +23,21 @@\n import static org.apache.beam.sdk.metrics.MetricNameFilter.inNamespace;\n import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertThat;\n-import static org.junit.Assert.assertTrue;\n \n import com.google.common.collect.ImmutableList;\n-import java.util.HashSet;\n-import java.util.Set;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.metrics.DistributionData;\n import org.apache.beam.sdk.metrics.DistributionResult;\n+import org.apache.beam.sdk.metrics.GaugeData;\n+import org.apache.beam.sdk.metrics.GaugeResult;\n import org.apache.beam.sdk.metrics.MetricKey;\n import org.apache.beam.sdk.metrics.MetricName;\n import org.apache.beam.sdk.metrics.MetricQueryResults;\n import org.apache.beam.sdk.metrics.MetricUpdates;\n import org.apache.beam.sdk.metrics.MetricUpdates.MetricUpdate;\n import org.apache.beam.sdk.metrics.MetricsFilter;\n+import org.joda.time.Instant;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -60,6 +59,7 @@\n   private static final MetricName NAME1 = MetricName.named(\"ns1\", \"name1\");\n   private static final MetricName NAME2 = MetricName.named(\"ns1\", \"name2\");\n   private static final MetricName NAME3 = MetricName.named(\"ns2\", \"name1\");\n+  private static final MetricName NAME4 = MetricName.named(\"ns2\", \"name2\");\n \n   private DirectMetrics metrics = new DirectMetrics();\n \n@@ -77,14 +77,20 @@ public void testApplyCommittedNoFilter() {\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME2), 8L)),\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME1),\n-                DistributionData.create(8, 2, 3, 5)))));\n+                DistributionData.create(8, 2, 3, 5))),\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"step1\", NAME4), GaugeData.create(15L)))\n+        ));\n     metrics.commitLogical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step2\", NAME1), 7L),\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME2), 4L)),\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME1),\n-                DistributionData.create(4, 1, 4, 4)))));\n+                DistributionData.create(4, 1, 4, 4))),\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"step1\", NAME4), GaugeData.create(27L)))\n+    ));\n \n     MetricQueryResults results = metrics.queryMetrics(MetricsFilter.builder().build());\n     assertThat(results.counters(), containsInAnyOrder(\n@@ -99,6 +105,12 @@ public void testApplyCommittedNoFilter() {\n         attemptedMetricsResult(\"ns1\", \"name1\", \"step1\", DistributionResult.ZERO)));\n     assertThat(results.distributions(), contains(\n         committedMetricsResult(\"ns1\", \"name1\", \"step1\", DistributionResult.create(12, 3, 3, 5))));\n+    assertThat(results.gauges(), contains(\n+        attemptedMetricsResult(\"ns2\", \"name2\", \"step1\", GaugeResult.empty())\n+    ));\n+    assertThat(results.gauges(), contains(\n+        committedMetricsResult(\"ns2\", \"name2\", \"step1\", GaugeResult.create(27L, Instant.now()))\n+    ));\n   }\n \n   @SuppressWarnings(\"unchecked\")\n@@ -108,12 +120,16 @@ public void testApplyAttemptedCountersQueryOneNamespace() {\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME1), 5L),\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME3), 8L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n     metrics.updatePhysical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"step2\", NAME1), 7L),\n             MetricUpdate.create(MetricKey.create(\"step1\", NAME3), 4L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n \n     MetricQueryResults results = metrics.queryMetrics(\n         MetricsFilter.builder().addNameFilter(inNamespace(\"ns1\")).build());\n@@ -129,54 +145,54 @@ public void testApplyAttemptedCountersQueryOneNamespace() {\n             committedMetricsResult(\"ns1\", \"name1\", \"step2\", 0L)));\n   }\n \n-  private boolean matchesSubPath(String actualScope, String subPath) {\n-    return metrics.subPathMatches(actualScope, subPath);\n-  }\n-\n+  @SuppressWarnings(\"unchecked\")\n   @Test\n-  public void testMatchesSubPath() {\n-    assertTrue(\"Match of the first element\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Top1\"));\n-    assertTrue(\"Match of the first elements\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1\"));\n-    assertTrue(\"Match of the last elements\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Inner1/Bottom1\"));\n-    assertFalse(\"Substring match but no subpath match\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"op1/Outer1/Inner1\"));\n-    assertFalse(\"Substring match from start - but no subpath match\",\n-        matchesSubPath(\"Top1/Outer1/Inner1/Bottom1\", \"Top\"));\n-  }\n+  public void testApplyAttemptedQueryCompositeScope() {\n+    metrics.updatePhysical(bundle1, MetricUpdates.create(\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 5L),\n+            MetricUpdate.create(MetricKey.create(\"Outer1/Inner2\", NAME1), 8L)),\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()));\n+    metrics.updatePhysical(bundle1, MetricUpdates.create(\n+        ImmutableList.of(\n+            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 12L),\n+            MetricUpdate.create(MetricKey.create(\"Outer2/Inner2\", NAME1), 18L)),\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()));\n \n-  private boolean matchesScopeWithSingleFilter(String actualScope, String filter) {\n-    Set<String> scopeFilter = new HashSet<String>();\n-    scopeFilter.add(filter);\n-    return metrics.matchesScope(actualScope, scopeFilter);\n-  }\n+    MetricQueryResults results = metrics.queryMetrics(\n+        MetricsFilter.builder().addStep(\"Outer1\").build());\n \n-  @Test\n-  public void testMatchesScope() {\n-    assertTrue(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1\"));\n-    assertTrue(matchesScopeWithSingleFilter(\n-        \"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1/Inner1/Bottom1\"));\n-    assertTrue(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1\"));\n-    assertTrue(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1/Inner1\"));\n-    assertFalse(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Inner1\"));\n-    assertFalse(matchesScopeWithSingleFilter(\"Top1/Outer1/Inner1/Bottom1\", \"Top1/Outer1/Inn\"));\n+    assertThat(results.counters(),\n+        containsInAnyOrder(\n+            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 12L),\n+            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 8L)));\n+\n+    assertThat(results.counters(),\n+        containsInAnyOrder(\n+            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 0L),\n+            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 0L)));\n   }\n \n+\n   @SuppressWarnings(\"unchecked\")\n   @Test\n   public void testPartialScopeMatchingInMetricsQuery() {\n     metrics.updatePhysical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"Top1/Outer1/Inner1\", NAME1), 5L),\n             MetricUpdate.create(MetricKey.create(\"Top1/Outer1/Inner2\", NAME1), 8L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n     metrics.updatePhysical(bundle1, MetricUpdates.create(\n         ImmutableList.of(\n             MetricUpdate.create(MetricKey.create(\"Top2/Outer1/Inner1\", NAME1), 12L),\n             MetricUpdate.create(MetricKey.create(\"Top1/Outer2/Inner2\", NAME1), 18L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n+        ImmutableList.<MetricUpdate<DistributionData>>of(),\n+        ImmutableList.<MetricUpdate<GaugeData>>of()\n+    ));\n \n     MetricQueryResults results = metrics.queryMetrics(\n         MetricsFilter.builder().addStep(\"Top1/Outer1\").build());\n@@ -194,32 +210,4 @@ public void testPartialScopeMatchingInMetricsQuery() {\n             attemptedMetricsResult(\"ns1\", \"name1\", \"Top1/Outer1/Inner2\", 8L),\n             attemptedMetricsResult(\"ns1\", \"name1\", \"Top1/Outer2/Inner2\", 18L)));\n   }\n-\n-  @SuppressWarnings(\"unchecked\")\n-  @Test\n-  public void testApplyAttemptedQueryCompositeScope() {\n-    metrics.updatePhysical(bundle1, MetricUpdates.create(\n-        ImmutableList.of(\n-            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 5L),\n-            MetricUpdate.create(MetricKey.create(\"Outer1/Inner2\", NAME1), 8L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n-    metrics.updatePhysical(bundle1, MetricUpdates.create(\n-        ImmutableList.of(\n-            MetricUpdate.create(MetricKey.create(\"Outer1/Inner1\", NAME1), 12L),\n-            MetricUpdate.create(MetricKey.create(\"Outer2/Inner2\", NAME1), 18L)),\n-        ImmutableList.<MetricUpdate<DistributionData>>of()));\n-\n-    MetricQueryResults results = metrics.queryMetrics(\n-        MetricsFilter.builder().addStep(\"Outer1\").build());\n-\n-    assertThat(results.counters(),\n-        containsInAnyOrder(\n-            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 12L),\n-            attemptedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 8L)));\n-\n-    assertThat(results.counters(),\n-        containsInAnyOrder(\n-            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner1\", 0L),\n-            committedMetricsResult(\"ns1\", \"name1\", \"Outer1/Inner2\", 0L)));\n-  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectMetricsTest.java",
                "sha": "ee51e9a904acfcb918e753a9213cb025e862d753",
                "status": "modified"
            },
            {
                "additions": 85,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "patch": "@@ -32,9 +32,17 @@\n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.ArrayBlockingQueue;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.beam.runners.direct.DirectRunner.DirectPipelineResult;\n import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.PipelineResult.State;\n import org.apache.beam.sdk.coders.AtomicCoder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderException;\n@@ -49,6 +57,7 @@\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.transforms.Count;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n@@ -65,6 +74,7 @@\n import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.TypeDescriptor;\n import org.hamcrest.Matchers;\n+import org.joda.time.Duration;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.internal.matchers.ThrowableMessageMatcher;\n@@ -182,9 +192,10 @@ public void byteArrayCountShouldSucceed() {\n     TypeDescriptor<byte[]> td = new TypeDescriptor<byte[]>() {\n     };\n     PCollection<byte[]> foos =\n-        p.apply(Create.of(1, 1, 1, 2, 2, 3)).apply(MapElements.via(getBytes).withOutputType(td));\n+        p.apply(Create.of(1, 1, 1, 2, 2, 3))\n+            .apply(MapElements.into(td).via(getBytes));\n     PCollection<byte[]> msync =\n-        p.apply(Create.of(1, -2, -8, -16)).apply(MapElements.via(getBytes).withOutputType(td));\n+        p.apply(Create.of(1, -2, -8, -16)).apply(MapElements.into(td).via(getBytes));\n     PCollection<byte[]> bytes =\n         PCollectionList.of(foos).and(msync).apply(Flatten.<byte[]>pCollections());\n     PCollection<KV<byte[], Long>> counts = bytes.apply(Count.<byte[]>perElement());\n@@ -221,6 +232,76 @@ public void splitsInputs() {\n     p.run();\n   }\n \n+  @Test\n+  public void cancelShouldStopPipeline() throws Exception {\n+    PipelineOptions opts = TestPipeline.testingPipelineOptions();\n+    opts.as(DirectOptions.class).setBlockOnRun(false);\n+    opts.setRunner(DirectRunner.class);\n+\n+    final Pipeline p = Pipeline.create(opts);\n+    p.apply(CountingInput.unbounded().withRate(1L, Duration.standardSeconds(1)));\n+\n+    final BlockingQueue<PipelineResult> resultExchange = new ArrayBlockingQueue<>(1);\n+    Runnable cancelRunnable = new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          resultExchange.take().cancel();\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+          throw new IllegalStateException(e);\n+        } catch (IOException e) {\n+          throw new IllegalStateException(e);\n+        }\n+      }\n+    };\n+\n+    Callable<PipelineResult> runPipelineRunnable = new Callable<PipelineResult>() {\n+      @Override\n+      public PipelineResult call() {\n+        PipelineResult res = p.run();\n+        try {\n+          resultExchange.put(res);\n+        } catch (InterruptedException e) {\n+          Thread.currentThread().interrupt();\n+          throw new IllegalStateException(e);\n+        }\n+        return res;\n+      }\n+    };\n+\n+    ExecutorService executor = Executors.newCachedThreadPool();\n+    executor.submit(cancelRunnable);\n+    Future<PipelineResult> result = executor.submit(runPipelineRunnable);\n+\n+    // If cancel doesn't work, this will hang forever\n+    result.get().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void testWaitUntilFinishTimeout() throws Exception {\n+    DirectOptions options = PipelineOptionsFactory.as(DirectOptions.class);\n+    options.setBlockOnRun(false);\n+    options.setRunner(DirectRunner.class);\n+    Pipeline p = Pipeline.create(options);\n+    p\n+      .apply(Create.of(1L))\n+      .apply(ParDo.of(\n+          new DoFn<Long, Long>() {\n+            @ProcessElement\n+            public void hang(ProcessContext context) throws InterruptedException {\n+              // Hangs \"forever\"\n+              Thread.sleep(Long.MAX_VALUE);\n+            }\n+          }));\n+    PipelineResult result = p.run();\n+    // The pipeline should never complete;\n+    assertThat(result.getState(), is(State.RUNNING));\n+    // Must time out, otherwise this test will never complete\n+    result.waitUntilFinish(Duration.millis(1L));\n+    assertThat(result.getState(), is(State.RUNNING));\n+  }\n+\n   @Test\n   public void transformDisplayDataExceptionShouldFail() {\n     DoFn<Integer, Integer> brokenDoFn = new DoFn<Integer, Integer>() {\n@@ -468,13 +549,13 @@ public MustSplitSource(BoundedSource<T> underlying) {\n     }\n \n     @Override\n-    public List<? extends BoundedSource<T>> splitIntoBundles(\n+    public List<? extends BoundedSource<T>> split(\n         long desiredBundleSizeBytes, PipelineOptions options) throws Exception {\n       // Must have more than\n       checkState(\n           desiredBundleSizeBytes < getEstimatedSizeBytes(options),\n           \"Must split into more than one source\");\n-      return underlying.splitIntoBundles(desiredBundleSizeBytes, options);\n+      return underlying.split(desiredBundleSizeBytes, options);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "sha": "ed19be2876945d4b4869aff1b69e4496ab13e4e7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 4,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java",
                "patch": "@@ -53,7 +53,7 @@ public void setup() {\n \n   @Test\n   public void delegatesToUnderlying() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     DoFn<?, ?> original = lifecycleManager.get();\n     TransformEvaluator<Object> evaluator =\n         DoFnLifecycleManagerRemovingTransformEvaluator.wrapping(underlying, lifecycleManager);\n@@ -72,7 +72,7 @@ public void delegatesToUnderlying() throws Exception {\n \n   @Test\n   public void removesOnExceptionInProcessElement() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     doThrow(Exception.class).when(underlying).processElement(any(WindowedValue.class));\n \n     DoFn<?, ?> original = lifecycleManager.get();\n@@ -91,7 +91,7 @@ public void removesOnExceptionInProcessElement() throws Exception {\n \n   @Test\n   public void removesOnExceptionInOnTimer() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     doThrow(Exception.class)\n         .when(underlying)\n         .onTimer(any(TimerData.class), any(BoundedWindow.class));\n@@ -114,7 +114,7 @@ public void removesOnExceptionInOnTimer() throws Exception {\n \n   @Test\n   public void removesOnExceptionInFinishBundle() throws Exception {\n-    ParDoEvaluator<Object, Object> underlying = mock(ParDoEvaluator.class);\n+    ParDoEvaluator<Object> underlying = mock(ParDoEvaluator.class);\n     doThrow(Exception.class).when(underlying).finishBundle();\n \n     DoFn<?, ?> original = lifecycleManager.get();",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DoFnLifecycleManagerRemovingTransformEvaluatorTest.java",
                "sha": "1ac4d6d2773a6f87f3376a1597f67743a2ab6039",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 56,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "patch": "@@ -414,8 +414,7 @@ public void createKeyedBundleKeyed() {\n   }\n \n   @Test\n-  public void isDoneWithUnboundedPCollectionAndShutdown() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(true);\n+  public void isDoneWithUnboundedPCollection() {\n     assertThat(context.isDone(unboundedProducer), is(false));\n \n     context.handleResult(\n@@ -426,34 +425,8 @@ public void isDoneWithUnboundedPCollectionAndShutdown() {\n     assertThat(context.isDone(unboundedProducer), is(true));\n   }\n \n-  @Test\n-  public void isDoneWithUnboundedPCollectionAndNotShutdown() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(false);\n-    assertThat(context.isDone(graph.getProducer(unbounded)), is(false));\n-\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(graph.getProducer(unbounded)).build());\n-    assertThat(context.isDone(graph.getProducer(unbounded)), is(false));\n-  }\n-\n-  @Test\n-  public void isDoneWithOnlyBoundedPCollections() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(false);\n-    assertThat(context.isDone(createdProducer), is(false));\n-\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(createdProducer).build());\n-    context.extractFiredTimers();\n-    assertThat(context.isDone(createdProducer), is(true));\n-  }\n-\n   @Test\n   public void isDoneWithPartiallyDone() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(true);\n     assertThat(context.isDone(), is(false));\n \n     UncommittedBundle<Integer> rootBundle = context.createBundle(created);\n@@ -484,34 +457,6 @@ public void isDoneWithPartiallyDone() {\n     assertThat(context.isDone(), is(true));\n   }\n \n-  @Test\n-  public void isDoneWithUnboundedAndNotShutdown() {\n-    context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(false);\n-    assertThat(context.isDone(), is(false));\n-\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(createdProducer).build());\n-    context.handleResult(\n-        null,\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(unboundedProducer).build());\n-    context.handleResult(\n-        context.createBundle(created).commit(Instant.now()),\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(downstreamProducer).build());\n-    context.extractFiredTimers();\n-    assertThat(context.isDone(), is(false));\n-\n-    context.handleResult(\n-        context.createBundle(created).commit(Instant.now()),\n-        ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(viewProducer).build());\n-    context.extractFiredTimers();\n-    assertThat(context.isDone(), is(false));\n-  }\n-\n   private static class TestBoundedWindow extends BoundedWindow {\n     private final Instant ts;\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "sha": "7a65493467e36e8945d77c12f9fe194f78b3a3bd",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 7,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "patch": "@@ -70,7 +70,7 @@\n   @Mock private EvaluationContext evaluationContext;\n   private PCollection<Integer> inputPc;\n   private TupleTag<Integer> mainOutputTag;\n-  private List<TupleTag<?>> sideOutputTags;\n+  private List<TupleTag<?>> additionalOutputTags;\n   private BundleFactory bundleFactory;\n \n   @Rule\n@@ -81,7 +81,7 @@ public void setup() {\n     MockitoAnnotations.initMocks(this);\n     inputPc = p.apply(Create.of(1, 2, 3));\n     mainOutputTag = new TupleTag<Integer>() {};\n-    sideOutputTags = TupleTagList.empty().getAll();\n+    additionalOutputTags = TupleTagList.empty().getAll();\n \n     bundleFactory = ImmutableListBundleFactory.create();\n   }\n@@ -98,7 +98,7 @@ public void sideInputsNotReadyResultHasUnprocessedElements() {\n     UncommittedBundle<Integer> outputBundle = bundleFactory.createBundle(output);\n     when(evaluationContext.createBundle(output)).thenReturn(outputBundle);\n \n-    ParDoEvaluator<Integer, Integer> evaluator =\n+    ParDoEvaluator<Integer> evaluator =\n         createEvaluator(singletonView, fn, output);\n \n     IntervalWindow nonGlobalWindow = new IntervalWindow(new Instant(0), new Instant(10_000L));\n@@ -130,7 +130,7 @@ public void sideInputsNotReadyResultHasUnprocessedElements() {\n             WindowedValue.timestampedValueInGlobalWindow(6, new Instant(2468L))));\n   }\n \n-  private ParDoEvaluator<Integer, Integer> createEvaluator(\n+  private ParDoEvaluator<Integer> createEvaluator(\n       PCollectionView<Integer> singletonView,\n       RecorderFn fn,\n       PCollection<Integer> output) {\n@@ -162,14 +162,15 @@ public void sideInputsNotReadyResultHasUnprocessedElements() {\n         evaluationContext,\n         stepContext,\n         transform,\n-        ((PCollection<?>) Iterables.getOnlyElement(transform.getInputs()).getValue())\n+        ((PCollection<?>) Iterables.getOnlyElement(transform.getInputs().values()))\n             .getWindowingStrategy(),\n         fn,\n         null /* key */,\n         ImmutableList.<PCollectionView<?>>of(singletonView),\n         mainOutputTag,\n-        sideOutputTags,\n-        ImmutableMap.<TupleTag<?>, PCollection<?>>of(mainOutputTag, output));\n+        additionalOutputTags,\n+        ImmutableMap.<TupleTag<?>, PCollection<?>>of(mainOutputTag, output),\n+        ParDoEvaluator.<Integer, Integer>defaultRunnerFactory());\n   }\n \n   private static class RecorderFn extends DoFn<Integer, Integer> {",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "sha": "e99e4bff329bd96a919def05ac491e44e8e4c427",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 46,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java",
                "patch": "@@ -1,46 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.beam.runners.direct;\n-\n-import static org.junit.Assert.assertThat;\n-\n-import org.apache.beam.sdk.testing.TestPipeline;\n-import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.hamcrest.Matchers;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.JUnit4;\n-\n-/**\n- * Tests for {@link ParDoSingleViaMultiOverrideFactory}.\n- */\n-@RunWith(JUnit4.class)\n-public class ParDoSingleViaMultiOverrideFactoryTest {\n-  private ParDoSingleViaMultiOverrideFactory<Integer, Integer> factory =\n-      new ParDoSingleViaMultiOverrideFactory<>();\n-\n-  @Test\n-  public void getInputSucceeds() {\n-    TestPipeline p = TestPipeline.create();\n-    PCollection<Integer> input = p.apply(Create.of(1, 2, 3));\n-    PCollection<?> reconstructed = factory.getInput(input.expand(), p);\n-    assertThat(reconstructed, Matchers.<PCollection<?>>equalTo(input));\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactoryTest.java",
                "sha": "59577a82b3b931ff658a0cf30d468051c82f2873",
                "status": "removed"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 15,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n import org.joda.time.Instant;\n import org.junit.Before;\n import org.junit.Rule;\n@@ -203,25 +204,13 @@ public void getNotReadyThrows() throws Exception {\n         .get(mapView, GlobalWindow.INSTANCE);\n   }\n \n-  @Test\n-  public void withPCollectionViewsErrorsForContainsNotInViews() {\n-    PCollectionView<Map<String, Iterable<String>>> newView =\n-        PCollectionViews.multimapView(\n-            pipeline,\n-            WindowingStrategy.globalDefault(),\n-            KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()));\n-\n-    thrown.expect(IllegalArgumentException.class);\n-    thrown.expectMessage(\"with unknown views \" + ImmutableList.of(newView).toString());\n-\n-    container.createReaderForViews(ImmutableList.<PCollectionView<?>>of(newView));\n-  }\n-\n   @Test\n   public void withViewsForViewNotInContainerFails() {\n+    PCollection<KV<String, String>> input =\n+        pipeline.apply(Create.empty(new TypeDescriptor<KV<String, String>>() {}));\n     PCollectionView<Map<String, Iterable<String>>> newView =\n         PCollectionViews.multimapView(\n-            pipeline,\n+            input,\n             WindowingStrategy.globalDefault(),\n             KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()));\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/SideInputContainerTest.java",
                "sha": "f4de8839c28c4ea550ce6a2c1c4c548e226e2e4d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 2,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "patch": "@@ -243,7 +243,7 @@ public void testUnprocessedElements() throws Exception {\n         mainInput\n             .apply(\n                 new ParDoMultiOverrideFactory.GbkThenStatefulParDo<>(\n-                    ParDo.withSideInputs(sideInput)\n+                    ParDo\n                         .of(\n                             new DoFn<KV<String, Integer>, Integer>() {\n                               @StateId(stateId)\n@@ -253,6 +253,7 @@ public void testUnprocessedElements() throws Exception {\n                               @ProcessElement\n                               public void process(ProcessContext c) {}\n                             })\n+                        .withSideInputs(sideInput)\n                         .withOutputTags(mainOutput, TupleTagList.empty())))\n             .get(mainOutput)\n             .setCoder(VarIntCoder.of());\n@@ -307,7 +308,7 @@ public void process(ProcessContext c) {}\n         BUNDLE_FACTORY\n             .createBundle(\n                 (PCollection<KeyedWorkItem<String, KV<String, Integer>>>)\n-                    Iterables.getOnlyElement(producingTransform.getInputs()).getValue())\n+                    Iterables.getOnlyElement(producingTransform.getInputs().values()))\n             .add(gbkOutputElement)\n             .commit(Instant.now());\n     TransformEvaluator<KeyedWorkItem<String, KV<String, Integer>>> evaluator =",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "sha": "ecb81307270307a8478dab2066c37f02192f22c5",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "patch": "@@ -27,20 +27,16 @@\n import java.util.Collection;\n import java.util.Collections;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n-import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.DirectTestStreamFactory;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.DirectTestStreamFactory.DirectTestStream;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.TestClock;\n import org.apache.beam.runners.direct.TestStreamEvaluatorFactory.TestStreamIndex;\n-import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.VarIntCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TaggedPValue;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.hamcrest.Matchers;\n import org.joda.time.Duration;\n@@ -179,11 +175,4 @@ public void producesElementsInSequence() throws Exception {\n     assertThat(fifthResult.getWatermarkHold(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE));\n     assertThat(fifthResult.getUnprocessedElements(), Matchers.emptyIterable());\n   }\n-\n-  @Test\n-  public void overrideFactoryGetInputSucceeds() {\n-    DirectTestStreamFactory<?> factory = new DirectTestStreamFactory<>(runner);\n-    PBegin begin = factory.getInput(Collections.<TaggedPValue>emptyList(), p);\n-    assertThat(begin.getPipeline(), Matchers.<Pipeline>equalTo(p));\n-  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "sha": "b9c6e64de77cf2cc1ac8a8a56c972514868f905e",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java",
                "patch": "@@ -63,6 +63,31 @@ public void parallelScheduleMultipleSchedulesBothImmediately() {\n     parallel.complete(second);\n   }\n \n+  @Test\n+  public void parallelRejectedStillActiveThrows() {\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> first = mock(TransformExecutor.class);\n+\n+    TransformExecutorService parallel =\n+        TransformExecutorServices.parallel(executorService);\n+    executorService.shutdown();\n+    thrown.expect(IllegalStateException.class);\n+    thrown.expectMessage(\"still active\");\n+    parallel.schedule(first);\n+  }\n+\n+  @Test\n+  public void parallelRejectedShutdownSucceeds() {\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> first = mock(TransformExecutor.class);\n+\n+    TransformExecutorService parallel =\n+        TransformExecutorServices.parallel(executorService);\n+    executorService.shutdown();\n+    parallel.shutdown();\n+    parallel.schedule(first);\n+  }\n+\n   @Test\n   public void serialScheduleTwoWaitsForFirstToComplete() {\n     @SuppressWarnings(\"unchecked\")\n@@ -97,4 +122,27 @@ public void serialCompleteNotExecutingTaskThrows() {\n \n     serial.complete(second);\n   }\n+\n+  /**\n+   * Tests that a Serial {@link TransformExecutorService} does not schedule follow up work if the\n+   * executor is shut down when the initial work completes.\n+   */\n+  @Test\n+  public void serialShutdownCompleteActive() {\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> first = mock(TransformExecutor.class);\n+    @SuppressWarnings(\"unchecked\")\n+    TransformExecutor<Object> second = mock(TransformExecutor.class);\n+\n+    TransformExecutorService serial = TransformExecutorServices.serial(executorService);\n+    serial.schedule(first);\n+    verify(first).run();\n+\n+    serial.schedule(second);\n+    verify(second, never()).run();\n+\n+    serial.shutdown();\n+    serial.complete(first);\n+    verify(second, never()).run();\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorServicesTest.java",
                "sha": "77652b2c2694104a338f646d43fb06c8f63fd105",
                "status": "modified"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import static com.google.common.base.Preconditions.checkState;\n import static org.apache.beam.runners.direct.DirectGraphs.getProducer;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.equalTo;\n@@ -75,6 +76,7 @@\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n import org.junit.runner.RunWith;\n import org.junit.runners.JUnit4;\n import org.mockito.invocation.InvocationOnMock;\n@@ -95,8 +97,8 @@\n   private UnboundedSource<Long, ?> source;\n   private DirectGraph graph;\n \n-  @Rule\n-  public TestPipeline p = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+  @Rule public TestPipeline p = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n \n   @Before\n   public void setup() {\n@@ -373,6 +375,41 @@ public void evaluatorClosesReaderAndResumesFromCheckpoint() throws Exception {\n     secondEvaluator.finishBundle();\n \n     assertThat(TestUnboundedSource.readerClosedCount, equalTo(2));\n+    assertThat(\n+        Iterables.getOnlyElement(residual.getElements()).getValue().getCheckpoint().isFinalized(),\n+        is(true));\n+  }\n+\n+  @Test\n+  public void evaluatorThrowsInCloseRethrows() throws Exception {\n+    ContiguousSet<Long> elems = ContiguousSet.create(Range.closed(0L, 20L), DiscreteDomain.longs());\n+    TestUnboundedSource<Long> source =\n+        new TestUnboundedSource<>(BigEndianLongCoder.of(), elems.toArray(new Long[0]))\n+            .throwsOnClose();\n+\n+    PCollection<Long> pcollection = p.apply(Read.from(source));\n+    AppliedPTransform<?, ?, ?> sourceTransform =\n+        DirectGraphs.getGraph(p).getProducer(pcollection);\n+\n+    when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n+    UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);\n+    when(context.createBundle(pcollection)).thenReturn(output);\n+\n+    WindowedValue<UnboundedSourceShard<Long, TestCheckpointMark>> shard =\n+        WindowedValue.valueInGlobalWindow(\n+            UnboundedSourceShard.unstarted(source, NeverDeduplicator.create()));\n+    CommittedBundle<UnboundedSourceShard<Long, TestCheckpointMark>> inputBundle =\n+        bundleFactory\n+            .<UnboundedSourceShard<Long, TestCheckpointMark>>createRootBundle()\n+            .add(shard)\n+            .commit(Instant.now());\n+    UnboundedReadEvaluatorFactory factory =\n+        new UnboundedReadEvaluatorFactory(context, 0.0 /* never reuse */);\n+    TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> evaluator =\n+        factory.forApplication(sourceTransform, inputBundle);\n+    thrown.expect(IOException.class);\n+    thrown.expectMessage(\"throws on close\");\n+    evaluator.processElement(shard);\n   }\n \n   /**\n@@ -398,26 +435,32 @@ public Instant apply(Long input) {\n     private final Coder<T> coder;\n     private final List<T> elems;\n     private boolean dedupes = false;\n+    private boolean throwOnClose;\n \n     public TestUnboundedSource(Coder<T> coder, T... elems) {\n+      this(coder, false, Arrays.asList(elems));\n+    }\n+\n+   private TestUnboundedSource(Coder<T> coder, boolean throwOnClose, List<T> elems) {\n       readerAdvancedCount = 0;\n       readerClosedCount = 0;\n       this.coder = coder;\n-      this.elems = Arrays.asList(elems);\n+      this.elems = elems;\n+      this.throwOnClose = throwOnClose;\n     }\n \n     @Override\n-    public List<? extends UnboundedSource<T, TestCheckpointMark>> generateInitialSplits(\n+    public List<? extends UnboundedSource<T, TestCheckpointMark>> split(\n         int desiredNumSplits, PipelineOptions options) throws Exception {\n       return ImmutableList.of(this);\n     }\n \n     @Override\n     public UnboundedSource.UnboundedReader<T> createReader(\n         PipelineOptions options, @Nullable TestCheckpointMark checkpointMark) {\n-      if (checkpointMark != null) {\n-        assertThat(checkpointMark.isFinalized(), is(true));\n-      }\n+      checkState(\n+          checkpointMark == null || checkpointMark.decoded,\n+          \"Cannot resume from a checkpoint that has not been decoded\");\n       return new TestUnboundedReader(elems, checkpointMark == null ? -1 : checkpointMark.index);\n     }\n \n@@ -440,9 +483,14 @@ public void validate() {}\n       return coder;\n     }\n \n+    public TestUnboundedSource<T> throwsOnClose() {\n+      return new TestUnboundedSource<>(coder, true, elems);\n+    }\n+\n     private class TestUnboundedReader extends UnboundedReader<T> {\n       private final List<T> elems;\n       private int index;\n+      private boolean closed = false;\n \n       public TestUnboundedReader(List<T> elems, int startIndex) {\n         this.elems = elems;\n@@ -502,21 +550,37 @@ public Instant getCurrentTimestamp() throws NoSuchElementException {\n \n       @Override\n       public void close() throws IOException {\n-        readerClosedCount++;\n+        try {\n+          readerClosedCount++;\n+          // Enforce the AutoCloseable contract. Close is not idempotent.\n+          assertThat(closed, is(false));\n+          if (throwOnClose) {\n+            throw new IOException(String.format(\"%s throws on close\", TestUnboundedSource.this));\n+          }\n+        } finally {\n+          closed = true;\n+        }\n       }\n     }\n   }\n \n   private static class TestCheckpointMark implements CheckpointMark {\n     final int index;\n     private boolean finalized = false;\n+    private boolean decoded = false;\n \n     private TestCheckpointMark(int index) {\n       this.index = index;\n     }\n \n     @Override\n     public void finalizeCheckpoint() throws IOException {\n+      checkState(\n+          !finalized, \"%s was finalized more than once\", TestCheckpointMark.class.getSimpleName());\n+      checkState(\n+          !decoded,\n+          \"%s was finalized after being decoded\",\n+          TestCheckpointMark.class.getSimpleName());\n       finalized = true;\n     }\n \n@@ -530,15 +594,17 @@ public void encode(\n           TestCheckpointMark value,\n           OutputStream outStream,\n           org.apache.beam.sdk.coders.Coder.Context context)\n-          throws CoderException, IOException {\n+          throws IOException {\n         VarInt.encode(value.index, outStream);\n       }\n \n       @Override\n       public TestCheckpointMark decode(\n           InputStream inStream, org.apache.beam.sdk.coders.Coder.Context context)\n-          throws CoderException, IOException {\n-        return new TestCheckpointMark(VarInt.decodeInt(inStream));\n+          throws IOException {\n+        TestCheckpointMark decoded = new TestCheckpointMark(VarInt.decodeInt(inStream));\n+        decoded.decoded = true;\n+        return decoded;\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "sha": "567ee984c31608177762bc71482ed804e9e307f0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.beam.runners.direct;\n \n import static org.hamcrest.Matchers.containsInAnyOrder;\n-import static org.hamcrest.Matchers.equalTo;\n import static org.hamcrest.Matchers.nullValue;\n import static org.junit.Assert.assertThat;\n import static org.mockito.Mockito.mock;\n@@ -27,7 +26,6 @@\n import com.google.common.collect.ImmutableList;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.PCollectionViewWriter;\n-import org.apache.beam.runners.direct.ViewEvaluatorFactory.ViewOverrideFactory;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VoidCoder;\n@@ -63,14 +61,15 @@ public void testInMemoryEvaluator() throws Exception {\n     PCollection<String> input = p.apply(Create.of(\"foo\", \"bar\"));\n     CreatePCollectionView<String, Iterable<String>> createView =\n         CreatePCollectionView.of(\n-            PCollectionViews.iterableView(p, input.getWindowingStrategy(), StringUtf8Coder.of()));\n+            PCollectionViews.iterableView(\n+                input, input.getWindowingStrategy(), StringUtf8Coder.of()));\n     PCollection<Iterable<String>> concat =\n         input.apply(WithKeys.<Void, String>of((Void) null))\n             .setCoder(KvCoder.of(VoidCoder.of(), StringUtf8Coder.of()))\n             .apply(GroupByKey.<Void, String>create())\n             .apply(Values.<Iterable<String>>create());\n     PCollectionView<Iterable<String>> view =\n-        concat.apply(new ViewEvaluatorFactory.WriteView<>(createView));\n+        concat.apply(new ViewOverrideFactory.WriteView<>(createView));\n \n     EvaluationContext context = mock(EvaluationContext.class);\n     TestViewWriter<String, Iterable<String>> viewWriter = new TestViewWriter<>();\n@@ -93,13 +92,6 @@ public void testInMemoryEvaluator() throws Exception {\n             WindowedValue.valueInGlobalWindow(\"foo\"), WindowedValue.valueInGlobalWindow(\"bar\")));\n   }\n \n-  @Test\n-  public void overrideFactoryGetInputSucceeds() {\n-    ViewOverrideFactory<String, String> factory = new ViewOverrideFactory<>();\n-    PCollection<String> input = p.apply(Create.of(\"foo\", \"bar\"));\n-    assertThat(factory.getInput(input.expand(), p), equalTo(input));\n-  }\n-\n   private static class TestViewWriter<ElemT, ViewT> implements PCollectionViewWriter<ElemT, ViewT> {\n     private Iterable<WindowedValue<ElemT>> latest;\n ",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "sha": "fe55a5f470fadbcbfcc3864eb0d0107def88bb56",
                "status": "modified"
            },
            {
                "additions": 138,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java",
                "changes": 138,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java",
                "patch": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.direct;\n+\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertThat;\n+\n+import com.google.common.collect.ImmutableSet;\n+import java.io.Serializable;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import org.apache.beam.runners.direct.ViewOverrideFactory.WriteView;\n+import org.apache.beam.sdk.Pipeline.PipelineVisitor;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory.PTransformReplacement;\n+import org.apache.beam.sdk.runners.TransformHierarchy.Node;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.util.PCollectionViews;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.hamcrest.Matchers;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Tests for {@link ViewOverrideFactory}. */\n+@RunWith(JUnit4.class)\n+public class ViewOverrideFactoryTest implements Serializable {\n+  @Rule\n+  public transient TestPipeline p = TestPipeline.create().enableAbandonedNodeEnforcement(false);\n+\n+  private transient ViewOverrideFactory<Integer, List<Integer>> factory =\n+      new ViewOverrideFactory<>();\n+\n+  @Test\n+  public void replacementSucceeds() {\n+    PCollection<Integer> ints = p.apply(\"CreateContents\", Create.of(1, 2, 3));\n+    final PCollectionView<List<Integer>> view =\n+        PCollectionViews.listView(ints, WindowingStrategy.globalDefault(), ints.getCoder());\n+    PTransformReplacement<PCollection<Integer>, PCollectionView<List<Integer>>>\n+        replacementTransform =\n+            factory.getReplacementTransform(\n+                AppliedPTransform\n+                    .<PCollection<Integer>, PCollectionView<List<Integer>>,\n+                        CreatePCollectionView<Integer, List<Integer>>>\n+                        of(\n+                            \"foo\",\n+                            ints.expand(),\n+                            view.expand(),\n+                            CreatePCollectionView.<Integer, List<Integer>>of(view),\n+                            p));\n+    PCollectionView<List<Integer>> afterReplacement =\n+        ints.apply(replacementTransform.getTransform());\n+    assertThat(\n+        \"The CreatePCollectionView replacement should return the same View\",\n+        afterReplacement,\n+        equalTo(view));\n+\n+    PCollection<Set<Integer>> outputViewContents =\n+        p.apply(\"CreateSingleton\", Create.of(0))\n+            .apply(\n+                \"OutputContents\",\n+                ParDo.of(\n+                        new DoFn<Integer, Set<Integer>>() {\n+                          @ProcessElement\n+                          public void outputSideInput(ProcessContext context) {\n+                            context.output(ImmutableSet.copyOf(context.sideInput(view)));\n+                          }\n+                        })\n+                    .withSideInputs(view));\n+    PAssert.thatSingleton(outputViewContents).isEqualTo(ImmutableSet.of(1, 2, 3));\n+\n+    p.run();\n+  }\n+\n+  @Test\n+  public void replacementGetViewReturnsOriginal() {\n+    final PCollection<Integer> ints = p.apply(\"CreateContents\", Create.of(1, 2, 3));\n+    final PCollectionView<List<Integer>> view =\n+        PCollectionViews.listView(ints, WindowingStrategy.globalDefault(), ints.getCoder());\n+    PTransformReplacement<PCollection<Integer>, PCollectionView<List<Integer>>> replacement =\n+        factory.getReplacementTransform(\n+            AppliedPTransform\n+                .<PCollection<Integer>, PCollectionView<List<Integer>>,\n+                    CreatePCollectionView<Integer, List<Integer>>>\n+                    of(\n+                        \"foo\",\n+                        ints.expand(),\n+                        view.expand(),\n+                        CreatePCollectionView.<Integer, List<Integer>>of(view),\n+                        p));\n+    ints.apply(replacement.getTransform());\n+    final AtomicBoolean writeViewVisited = new AtomicBoolean();\n+    p.traverseTopologically(\n+        new PipelineVisitor.Defaults() {\n+          @Override\n+          public void visitPrimitiveTransform(Node node) {\n+            if (node.getTransform() instanceof WriteView) {\n+              assertThat(\n+                  \"There should only be one WriteView primitive in the graph\",\n+                  writeViewVisited.getAndSet(true),\n+                  is(false));\n+              PCollectionView replacementView = ((WriteView) node.getTransform()).getView();\n+              assertThat(replacementView, Matchers.<PCollectionView>theInstance(view));\n+              assertThat(node.getInputs().entrySet(), hasSize(1));\n+            }\n+          }\n+        });\n+\n+    assertThat(writeViewVisited.get(), is(true));\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewOverrideFactoryTest.java",
                "sha": "6875e1a939cb8b05e7fcc84754910dd7ba3431b6",
                "status": "added"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 6,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java",
                "patch": "@@ -43,8 +43,8 @@\n import org.apache.beam.sdk.transforms.windowing.PaneInfo.Timing;\n import org.apache.beam.sdk.transforms.windowing.SlidingWindows;\n import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.Window.Bound;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.transforms.windowing.WindowMappingFn;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n import org.hamcrest.Matchers;\n@@ -112,7 +112,7 @@ public void setup() {\n   @Test\n   public void singleWindowFnSucceeds() throws Exception {\n     Duration windowDuration = Duration.standardDays(7);\n-    Bound<Long> transform = Window.<Long>into(FixedWindows.of(windowDuration));\n+    Window<Long> transform = Window.<Long>into(FixedWindows.of(windowDuration));\n     PCollection<Long> windowed = input.apply(transform);\n \n     CommittedBundle<Long> inputBundle = createInputBundle();\n@@ -151,7 +151,7 @@ public void singleWindowFnSucceeds() throws Exception {\n   public void multipleWindowsWindowFnSucceeds() throws Exception {\n     Duration windowDuration = Duration.standardDays(6);\n     Duration slidingBy = Duration.standardDays(3);\n-    Bound<Long> transform = Window.into(SlidingWindows.of(windowDuration).every(slidingBy));\n+    Window<Long> transform = Window.into(SlidingWindows.of(windowDuration).every(slidingBy));\n     PCollection<Long> windowed = input.apply(transform);\n \n     CommittedBundle<Long> inputBundle = createInputBundle();\n@@ -208,7 +208,7 @@ public void multipleWindowsWindowFnSucceeds() throws Exception {\n \n   @Test\n   public void referencesEarlierWindowsSucceeds() throws Exception {\n-    Bound<Long> transform = Window.into(new EvaluatorTestWindowFn());\n+    Window<Long> transform = Window.into(new EvaluatorTestWindowFn());\n     PCollection<Long> windowed = input.apply(transform);\n \n     CommittedBundle<Long> inputBundle = createInputBundle();\n@@ -313,8 +313,8 @@ public boolean isCompatible(WindowFn<?, ?> other) {\n     }\n \n     @Override\n-    public BoundedWindow getSideInputWindow(BoundedWindow window) {\n-      return null;\n+    public WindowMappingFn<BoundedWindow> getDefaultWindowMappingFn() {\n+      throw new UnsupportedOperationException(\"Cannot be used as a side input\");\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WindowEvaluatorFactoryTest.java",
                "sha": "eb58629d3004ee0b658db5788f594350835d7e5b",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 11,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "patch": "@@ -38,11 +38,13 @@\n import java.util.UUID;\n import org.apache.beam.runners.direct.WriteWithShardingFactory.CalculateShardsFn;\n import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.io.Sink;\n import org.apache.beam.sdk.io.TextIO;\n import org.apache.beam.sdk.io.Write;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.DoFnTester;\n@@ -52,7 +54,9 @@\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n-import org.hamcrest.Matchers;\n+import org.apache.beam.sdk.values.PDone;\n+import org.apache.beam.sdk.values.PValue;\n+import org.apache.beam.sdk.values.TupleTag;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n@@ -118,7 +122,15 @@ public void dynamicallyReshardedWrite() throws Exception {\n   @Test\n   public void withNoShardingSpecifiedReturnsNewTransform() {\n     Write<Object> original = Write.to(new TestSink());\n-    assertThat(factory.getReplacementTransform(original), not(equalTo((Object) original)));\n+    PCollection<Object> objs = (PCollection) p.apply(Create.empty(VoidCoder.of()));\n+\n+    AppliedPTransform<PCollection<Object>, PDone, Write<Object>> originalApplication =\n+        AppliedPTransform.of(\n+            \"write\", objs.expand(), Collections.<TupleTag<?>, PValue>emptyMap(), original, p);\n+\n+    assertThat(\n+        factory.getReplacementTransform(originalApplication).getTransform(),\n+        not(equalTo((Object) original)));\n   }\n \n   @Test\n@@ -170,13 +182,14 @@ public void keyBasedOnCountFnManyElements() throws Exception {\n \n   @Test\n   public void keyBasedOnCountFnFewElementsExtraShards() throws Exception {\n+    long countValue = (long) WriteWithShardingFactory.MIN_SHARDS_FOR_LOG + 3;\n+    PCollection<Long> inputCount = p.apply(Create.of(countValue));\n     PCollectionView<Long> elementCountView =\n         PCollectionViews.singletonView(\n-            p, WindowingStrategy.globalDefault(), true, 0L, VarLongCoder.of());\n+            inputCount, WindowingStrategy.globalDefault(), true, 0L, VarLongCoder.of());\n     CalculateShardsFn fn = new CalculateShardsFn(3);\n     DoFnTester<Long, Integer> fnTester = DoFnTester.of(fn);\n \n-    long countValue = (long) WriteWithShardingFactory.MIN_SHARDS_FOR_LOG + 3;\n     fnTester.setSideInput(elementCountView, GlobalWindow.INSTANCE, countValue);\n \n     List<Integer> kvs = fnTester.processBundle(10L);\n@@ -194,13 +207,6 @@ public void keyBasedOnCountFnManyElementsExtraShards() throws Exception {\n     assertThat(shards, containsInAnyOrder(13));\n   }\n \n-  @Test\n-  public void getInputSucceeds() {\n-    PCollection<String> original = p.apply(Create.of(\"foo\"));\n-    PCollection<?> input = factory.getInput(original.expand(), p);\n-    assertThat(input, Matchers.<PCollection<?>>equalTo(original));\n-  }\n-\n   private static class TestSink extends Sink<Object> {\n     @Override\n     public void validate(PipelineOptions options) {}",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "sha": "361850ddb1ac8f6ed1536c083cf4da99c24b2bd1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/pom.xml",
                "changes": 126,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/pom.xml?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 126,
                "filename": "runners/flink/examples/pom.xml",
                "patch": "@@ -1,126 +0,0 @@\n-<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n-<!--\n-    Licensed to the Apache Software Foundation (ASF) under one or more\n-    contributor license agreements.  See the NOTICE file distributed with\n-    this work for additional information regarding copyright ownership.\n-    The ASF licenses this file to You under the Apache License, Version 2.0\n-    (the \"License\"); you may not use this file except in compliance with\n-    the License.  You may obtain a copy of the License at\n-\n-       http://www.apache.org/licenses/LICENSE-2.0\n-\n-    Unless required by applicable law or agreed to in writing, software\n-    distributed under the License is distributed on an \"AS IS\" BASIS,\n-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-    See the License for the specific language governing permissions and\n-    limitations under the License.\n--->\n-<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n-\n-  <modelVersion>4.0.0</modelVersion>\n-\n-  <parent>\n-    <groupId>org.apache.beam</groupId>\n-    <artifactId>beam-runners-flink-parent</artifactId>\n-    <version>0.7.0-SNAPSHOT</version>\n-    <relativePath>../pom.xml</relativePath>\n-  </parent>\n-\n-  <artifactId>beam-runners-flink_2.10-examples</artifactId>\n-\n-  <name>Apache Beam :: Runners :: Flink :: Examples</name>\n-\n-  <packaging>jar</packaging>\n-\n-  <properties>\n-    <!-- Default parameters for mvn exec:java -->\n-    <flink.examples.input>kinglear.txt</flink.examples.input>\n-    <flink.examples.output>wordcounts.txt</flink.examples.output>\n-    <flink.examples.parallelism>-1</flink.examples.parallelism>\n-  </properties>\n-\n-  <profiles>\n-    <profile>\n-      <id>disable-runnable-on-service-tests</id>\n-      <activation>\n-        <activeByDefault>true</activeByDefault>\n-      </activation>\n-      <build>\n-        <plugins>\n-          <plugin>\n-            <groupId>org.apache.maven.plugins</groupId>\n-            <artifactId>maven-surefire-plugin</artifactId>\n-            <executions>\n-              <execution>\n-                <id>runnable-on-service-tests</id>\n-                <configuration>\n-                  <skip>true</skip>\n-                </configuration>\n-              </execution>\n-            </executions>\n-          </plugin>\n-        </plugins>\n-      </build>\n-    </profile>\n-  </profiles>\n-\n-  <dependencies>\n-\n-    <dependency>\n-      <groupId>org.apache.beam</groupId>\n-      <artifactId>beam-runners-flink_2.10</artifactId>\n-      <version>${project.version}</version>\n-    </dependency>\n-\n-    <dependency>\n-      <groupId>org.apache.flink</groupId>\n-      <artifactId>flink-connector-kafka-0.8_2.10</artifactId>\n-      <version>${flink.version}</version>\n-    </dependency>\n-\n-  </dependencies>\n-\n-  <build>\n-    <plugins>\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-compiler-plugin</artifactId>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-jar-plugin</artifactId>\n-      </plugin>\n-\n-      <plugin>\n-        <groupId>org.apache.maven.plugins</groupId>\n-        <artifactId>maven-dependency-plugin</artifactId>\n-        <executions>\n-          <execution>\n-            <goals><goal>analyze-only</goal></goals>\n-            <configuration>\n-              <!-- disable for now until dependencies are cleaned up -->\n-              <failOnWarning>false</failOnWarning>\n-            </configuration>\n-          </execution>\n-        </executions>\n-      </plugin>\n-\n-      <plugin>\n-        <groupId>org.codehaus.mojo</groupId>\n-        <artifactId>exec-maven-plugin</artifactId>\n-        <configuration>\n-          <executable>java</executable>\n-          <arguments>\n-            <argument>--runner=org.apache.beam.runners.flink.FlinkRunner</argument>\n-            <argument>--parallelism=${flink.examples.parallelism}</argument>\n-            <argument>--input=${flink.examples.input}</argument>\n-            <argument>--output=${flink.examples.output}</argument>\n-          </arguments>\n-        </configuration>\n-      </plugin>\n-\n-    </plugins>\n-\n-  </build>\n-\n-</project>",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/pom.xml",
                "sha": "661ed432b087b38a82f36ccba44b805e553e237e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "changes": 456,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 456,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "patch": "@@ -1,456 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.util.HashSet;\n-import java.util.Set;\n-import org.apache.beam.runners.flink.FlinkPipelineOptions;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.coders.KvCoder;\n-import org.apache.beam.sdk.coders.StringDelegateCoder;\n-import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.GcsOptions;\n-import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.options.Validation;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.Distinct;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.Flatten;\n-import org.apache.beam.sdk.transforms.Keys;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Values;\n-import org.apache.beam.sdk.transforms.View;\n-import org.apache.beam.sdk.transforms.WithKeys;\n-import org.apache.beam.sdk.transforms.join.CoGbkResult;\n-import org.apache.beam.sdk.transforms.join.CoGroupByKey;\n-import org.apache.beam.sdk.transforms.join.KeyedPCollectionTuple;\n-import org.apache.beam.sdk.util.GcsUtil;\n-import org.apache.beam.sdk.util.gcsfs.GcsPath;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PBegin;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionList;\n-import org.apache.beam.sdk.values.PCollectionView;\n-import org.apache.beam.sdk.values.PDone;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * An example that computes a basic TF-IDF search table for a directory or GCS prefix.\n- *\n- * <p>Concepts: joining data; side inputs; logging\n- *\n- * <p>To execute this pipeline locally, specify general pipeline configuration:\n- * <pre>{@code\n- *   --project=YOUR_PROJECT_ID\n- * }</pre>\n- * and a local output file or output prefix on GCS:\n- * <pre>{@code\n- *   --output=[YOUR_LOCAL_FILE | gs://YOUR_OUTPUT_PREFIX]\n- * }</pre>\n- *\n- * <p>To execute this pipeline using the Dataflow service, specify pipeline configuration:\n- * <pre>{@code\n- *   --project=YOUR_PROJECT_ID\n- *   --stagingLocation=gs://YOUR_STAGING_DIRECTORY\n- *   --runner=BlockingDataflowRunner\n- * and an output prefix on GCS:\n- *   --output=gs://YOUR_OUTPUT_PREFIX\n- * }</pre>\n- *\n- * <p>The default input is {@code gs://dataflow-samples/shakespeare/} and can be overridden with\n- * {@code --input}.\n- */\n-public class TFIDF {\n-  /**\n-   * Options supported by {@link TFIDF}.\n-   *\n-   * <p>Inherits standard configuration options.\n-   */\n-  private interface Options extends PipelineOptions, FlinkPipelineOptions {\n-    @Description(\"Path to the directory or GCS prefix containing files to read from\")\n-    @Default.String(\"gs://dataflow-samples/shakespeare/\")\n-    String getInput();\n-    void setInput(String value);\n-\n-    @Description(\"Prefix of output URI to write to\")\n-    @Validation.Required\n-    String getOutput();\n-    void setOutput(String value);\n-  }\n-\n-  /**\n-   * Lists documents contained beneath the {@code options.input} prefix/directory.\n-   */\n-  public static Set<URI> listInputDocuments(Options options)\n-      throws URISyntaxException, IOException {\n-    URI baseUri = new URI(options.getInput());\n-\n-    // List all documents in the directory or GCS prefix.\n-    URI absoluteUri;\n-    if (baseUri.getScheme() != null) {\n-      absoluteUri = baseUri;\n-    } else {\n-      absoluteUri = new URI(\n-          \"file\",\n-          baseUri.getAuthority(),\n-          baseUri.getPath(),\n-          baseUri.getQuery(),\n-          baseUri.getFragment());\n-    }\n-\n-    Set<URI> uris = new HashSet<>();\n-    if (absoluteUri.getScheme().equals(\"file\")) {\n-      File directory = new File(absoluteUri);\n-      String[] directoryListing = directory.list();\n-      if (directoryListing == null) {\n-        throw new IOException(\n-            \"Directory \" + absoluteUri + \" is not a valid path or IO Error occurred.\");\n-      }\n-      for (String entry : directoryListing) {\n-        File path = new File(directory, entry);\n-        uris.add(path.toURI());\n-      }\n-    } else if (absoluteUri.getScheme().equals(\"gs\")) {\n-      GcsUtil gcsUtil = options.as(GcsOptions.class).getGcsUtil();\n-      URI gcsUriGlob = new URI(\n-          absoluteUri.getScheme(),\n-          absoluteUri.getAuthority(),\n-          absoluteUri.getPath() + \"*\",\n-          absoluteUri.getQuery(),\n-          absoluteUri.getFragment());\n-      for (GcsPath entry : gcsUtil.expand(GcsPath.fromUri(gcsUriGlob))) {\n-        uris.add(entry.toUri());\n-      }\n-    }\n-\n-    return uris;\n-  }\n-\n-  /**\n-   * Reads the documents at the provided uris and returns all lines\n-   * from the documents tagged with which document they are from.\n-   */\n-  public static class ReadDocuments\n-      extends PTransform<PBegin, PCollection<KV<URI, String>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    // transient because PTransform is not really meant to be serialized.\n-    // see note on PTransform\n-    private final transient Iterable<URI> uris;\n-\n-    public ReadDocuments(Iterable<URI> uris) {\n-      this.uris = uris;\n-    }\n-\n-    @Override\n-    public Coder<?> getDefaultOutputCoder() {\n-      return KvCoder.of(StringDelegateCoder.of(URI.class), StringUtf8Coder.of());\n-    }\n-\n-    @Override\n-    public PCollection<KV<URI, String>> expand(PBegin input) {\n-      Pipeline pipeline = input.getPipeline();\n-\n-      // Create one TextIO.Read transform for each document\n-      // and add its output to a PCollectionList\n-      PCollectionList<KV<URI, String>> urisToLines =\n-          PCollectionList.empty(pipeline);\n-\n-      // TextIO.Read supports:\n-      //  - file: URIs and paths locally\n-      //  - gs: URIs on the service\n-      for (final URI uri : uris) {\n-        String uriString;\n-        if (uri.getScheme().equals(\"file\")) {\n-          uriString = new File(uri).getPath();\n-        } else {\n-          uriString = uri.toString();\n-        }\n-\n-        PCollection<KV<URI, String>> oneUriToLines = pipeline\n-            .apply(\"TextIO.Read(\" + uriString + \")\", TextIO.Read.from(uriString))\n-            .apply(\"WithKeys(\" + uriString + \")\", WithKeys.<URI, String>of(uri));\n-\n-        urisToLines = urisToLines.and(oneUriToLines);\n-      }\n-\n-      return urisToLines.apply(Flatten.<KV<URI, String>>pCollections());\n-    }\n-  }\n-\n-  /**\n-   * A transform containing a basic TF-IDF pipeline. The input consists of KV objects\n-   * where the key is the document's URI and the value is a piece\n-   * of the document's content. The output is mapping from terms to\n-   * scores for each document URI.\n-   */\n-  public static class ComputeTfIdf\n-      extends PTransform<PCollection<KV<URI, String>>, PCollection<KV<String, KV<URI, Double>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    public ComputeTfIdf() { }\n-\n-    @Override\n-    public PCollection<KV<String, KV<URI, Double>>> expand(\n-        PCollection<KV<URI, String>> uriToContent) {\n-\n-      // Compute the total number of documents, and\n-      // prepare this singleton PCollectionView for\n-      // use as a side input.\n-      final PCollectionView<Long> totalDocuments =\n-          uriToContent\n-              .apply(\"GetURIs\", Keys.<URI>create())\n-              .apply(\"DistinctDocs\", Distinct.<URI>create())\n-              .apply(Count.<URI>globally())\n-              .apply(View.<Long>asSingleton());\n-\n-      // Create a collection of pairs mapping a URI to each\n-      // of the words in the document associated with that that URI.\n-      PCollection<KV<URI, String>> uriToWords = uriToContent\n-          .apply(\"SplitWords\", ParDo.of(new DoFn<KV<URI, String>, KV<URI, String>>() {\n-            private static final long serialVersionUID = 0;\n-\n-            @ProcessElement\n-            public void processElement(ProcessContext c) {\n-              URI uri = c.element().getKey();\n-              String line = c.element().getValue();\n-              for (String word : line.split(\"\\\\W+\")) {\n-                // Log INFO messages when the word \u201clove\u201d is found.\n-                if (word.toLowerCase().equals(\"love\")) {\n-                  LOG.info(\"Found {}\", word.toLowerCase());\n-                }\n-\n-                if (!word.isEmpty()) {\n-                  c.output(KV.of(uri, word.toLowerCase()));\n-                }\n-              }\n-            }\n-          }));\n-\n-      // Compute a mapping from each word to the total\n-      // number of documents in which it appears.\n-      PCollection<KV<String, Long>> wordToDocCount = uriToWords\n-          .apply(\"DistinctWords\", Distinct.<KV<URI, String>>create())\n-          .apply(Values.<String>create())\n-          .apply(\"CountDocs\", Count.<String>perElement());\n-\n-      // Compute a mapping from each URI to the total\n-      // number of words in the document associated with that URI.\n-      PCollection<KV<URI, Long>> uriToWordTotal = uriToWords\n-          .apply(\"GetURIs2\", Keys.<URI>create())\n-          .apply(\"CountWords\", Count.<URI>perElement());\n-\n-      // Count, for each (URI, word) pair, the number of\n-      // occurrences of that word in the document associated\n-      // with the URI.\n-      PCollection<KV<KV<URI, String>, Long>> uriAndWordToCount = uriToWords\n-          .apply(\"CountWordDocPairs\", Count.<KV<URI, String>>perElement());\n-\n-      // Adjust the above collection to a mapping from\n-      // (URI, word) pairs to counts into an isomorphic mapping\n-      // from URI to (word, count) pairs, to prepare for a join\n-      // by the URI key.\n-      PCollection<KV<URI, KV<String, Long>>> uriToWordAndCount = uriAndWordToCount\n-          .apply(\"ShiftKeys\", ParDo.of(\n-              new DoFn<KV<KV<URI, String>, Long>, KV<URI, KV<String, Long>>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  URI uri = c.element().getKey().getKey();\n-                  String word = c.element().getKey().getValue();\n-                  Long occurrences = c.element().getValue();\n-                  c.output(KV.of(uri, KV.of(word, occurrences)));\n-                }\n-              }));\n-\n-      // Prepare to join the mapping of URI to (word, count) pairs with\n-      // the mapping of URI to total word counts, by associating\n-      // each of the input PCollection<KV<URI, ...>> with\n-      // a tuple tag. Each input must have the same key type, URI\n-      // in this case. The type parameter of the tuple tag matches\n-      // the types of the values for each collection.\n-      final TupleTag<Long> wordTotalsTag = new TupleTag<>();\n-      final TupleTag<KV<String, Long>> wordCountsTag = new TupleTag<>();\n-      KeyedPCollectionTuple<URI> coGbkInput = KeyedPCollectionTuple\n-          .of(wordTotalsTag, uriToWordTotal)\n-          .and(wordCountsTag, uriToWordAndCount);\n-\n-      // Perform a CoGroupByKey (a sort of pre-join) on the prepared\n-      // inputs. This yields a mapping from URI to a CoGbkResult\n-      // (CoGroupByKey Result). The CoGbkResult is a mapping\n-      // from the above tuple tags to the values in each input\n-      // associated with a particular URI. In this case, each\n-      // KV<URI, CoGbkResult> group a URI with the total number of\n-      // words in that document as well as all the (word, count)\n-      // pairs for particular words.\n-      PCollection<KV<URI, CoGbkResult>> uriToWordAndCountAndTotal = coGbkInput\n-          .apply(\"CoGroupByUri\", CoGroupByKey.<URI>create());\n-\n-      // Compute a mapping from each word to a (URI, term frequency)\n-      // pair for each URI. A word's term frequency for a document\n-      // is simply the number of times that word occurs in the document\n-      // divided by the total number of words in the document.\n-      PCollection<KV<String, KV<URI, Double>>> wordToUriAndTf = uriToWordAndCountAndTotal\n-          .apply(\"ComputeTermFrequencies\", ParDo.of(\n-              new DoFn<KV<URI, CoGbkResult>, KV<String, KV<URI, Double>>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  URI uri = c.element().getKey();\n-                  Long wordTotal = c.element().getValue().getOnly(wordTotalsTag);\n-\n-                  for (KV<String, Long> wordAndCount\n-                      : c.element().getValue().getAll(wordCountsTag)) {\n-                    String word = wordAndCount.getKey();\n-                    Long wordCount = wordAndCount.getValue();\n-                    Double termFrequency = wordCount.doubleValue() / wordTotal.doubleValue();\n-                    c.output(KV.of(word, KV.of(uri, termFrequency)));\n-                  }\n-                }\n-              }));\n-\n-      // Compute a mapping from each word to its document frequency.\n-      // A word's document frequency in a corpus is the number of\n-      // documents in which the word appears divided by the total\n-      // number of documents in the corpus. Note how the total number of\n-      // documents is passed as a side input; the same value is\n-      // presented to each invocation of the DoFn.\n-      PCollection<KV<String, Double>> wordToDf = wordToDocCount\n-          .apply(\"ComputeDocFrequencies\", ParDo\n-              .withSideInputs(totalDocuments)\n-              .of(new DoFn<KV<String, Long>, KV<String, Double>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  String word = c.element().getKey();\n-                  Long documentCount = c.element().getValue();\n-                  Long documentTotal = c.sideInput(totalDocuments);\n-                  Double documentFrequency = documentCount.doubleValue()\n-                      / documentTotal.doubleValue();\n-\n-                  c.output(KV.of(word, documentFrequency));\n-                }\n-              }));\n-\n-      // Join the term frequency and document frequency\n-      // collections, each keyed on the word.\n-      final TupleTag<KV<URI, Double>> tfTag = new TupleTag<>();\n-      final TupleTag<Double> dfTag = new TupleTag<>();\n-      PCollection<KV<String, CoGbkResult>> wordToUriAndTfAndDf = KeyedPCollectionTuple\n-          .of(tfTag, wordToUriAndTf)\n-          .and(dfTag, wordToDf)\n-          .apply(CoGroupByKey.<String>create());\n-\n-      // Compute a mapping from each word to a (URI, TF-IDF) score\n-      // for each URI. There are a variety of definitions of TF-IDF\n-      // (\"term frequency - inverse document frequency\") score;\n-      // here we use a basic version that is the term frequency\n-      // divided by the log of the document frequency.\n-\n-      return wordToUriAndTfAndDf\n-          .apply(\"ComputeTfIdf\", ParDo.of(\n-              new DoFn<KV<String, CoGbkResult>, KV<String, KV<URI, Double>>>() {\n-                private static final long serialVersionUID = 0;\n-\n-                @ProcessElement\n-                public void processElement(ProcessContext c) {\n-                  String word = c.element().getKey();\n-                  Double df = c.element().getValue().getOnly(dfTag);\n-\n-                  for (KV<URI, Double> uriAndTf : c.element().getValue().getAll(tfTag)) {\n-                    URI uri = uriAndTf.getKey();\n-                    Double tf = uriAndTf.getValue();\n-                    Double tfIdf = tf * Math.log(1 / df);\n-                    c.output(KV.of(word, KV.of(uri, tfIdf)));\n-                  }\n-                }\n-              }));\n-    }\n-\n-    // Instantiate Logger.\n-    // It is suggested that the user specify the class name of the containing class\n-    // (in this case ComputeTfIdf).\n-    private static final Logger LOG = LoggerFactory.getLogger(ComputeTfIdf.class);\n-  }\n-\n-  /**\n-   * A {@link PTransform} to write, in CSV format, a mapping from term and URI\n-   * to score.\n-   */\n-  public static class WriteTfIdf\n-      extends PTransform<PCollection<KV<String, KV<URI, Double>>>, PDone> {\n-    private static final long serialVersionUID = 0;\n-\n-    private String output;\n-\n-    public WriteTfIdf(String output) {\n-      this.output = output;\n-    }\n-\n-    @Override\n-    public PDone expand(PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf) {\n-      return wordToUriAndTfIdf\n-          .apply(\"Format\", ParDo.of(new DoFn<KV<String, KV<URI, Double>>, String>() {\n-            private static final long serialVersionUID = 0;\n-\n-            @ProcessElement\n-            public void processElement(ProcessContext c) {\n-              c.output(String.format(\"%s,\\t%s,\\t%f\",\n-                  c.element().getKey(),\n-                  c.element().getValue().getKey(),\n-                  c.element().getValue().getValue()));\n-            }\n-          }))\n-          .apply(TextIO.Write\n-              .to(output)\n-              .withSuffix(\".csv\"));\n-    }\n-  }\n-\n-  public static void main(String[] args) throws Exception {\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\n-\n-    options.setRunner(FlinkRunner.class);\n-\n-    Pipeline pipeline = Pipeline.create(options);\n-    pipeline.getCoderRegistry().registerCoder(URI.class, StringDelegateCoder.of(URI.class));\n-\n-    pipeline\n-        .apply(new ReadDocuments(listInputDocuments(options)))\n-        .apply(new ComputeTfIdf())\n-        .apply(new WriteTfIdf(options.getOutput()));\n-\n-    pipeline.run();\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "sha": "89e261b52b3066faadf79eb25540463f9cc7713d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "changes": 129,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 129,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "patch": "@@ -1,129 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples;\n-\n-import org.apache.beam.runners.flink.FlinkPipelineOptions;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.options.Validation;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.MapElements;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.SimpleFunction;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-\n-/**\n- * Wordcount pipeline.\n- */\n-public class WordCount {\n-\n-  /**\n-   * Function to extract words.\n-   */\n-  public static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-        createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * PTransform counting words.\n-   */\n-  public static class CountWords extends PTransform<PCollection<String>,\n-                    PCollection<KV<String, Long>>> {\n-    @Override\n-    public PCollection<KV<String, Long>> expand(PCollection<String> lines) {\n-\n-      // Convert lines of text into individual words.\n-      PCollection<String> words = lines.apply(\n-          ParDo.of(new ExtractWordsFn()));\n-\n-      // Count the number of times each word occurs.\n-      PCollection<KV<String, Long>> wordCounts =\n-          words.apply(Count.<String>perElement());\n-\n-      return wordCounts;\n-    }\n-  }\n-\n-  /** A SimpleFunction that converts a Word and Count into a printable string. */\n-  public static class FormatAsTextFn extends SimpleFunction<KV<String, Long>, String> {\n-    @Override\n-    public String apply(KV<String, Long> input) {\n-      return input.getKey() + \": \" + input.getValue();\n-    }\n-  }\n-\n-  /**\n-   * Options supported by {@link WordCount}.\n-   *\n-   * <p>Inherits standard configuration options.\n-   */\n-  public interface Options extends PipelineOptions, FlinkPipelineOptions {\n-    @Description(\"Path of the file to read from\")\n-    String getInput();\n-    void setInput(String value);\n-\n-    @Description(\"Path of the file to write to\")\n-    @Validation.Required\n-    String getOutput();\n-    void setOutput(String value);\n-  }\n-\n-  public static void main(String[] args) {\n-\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation()\n-        .as(Options.class);\n-    options.setRunner(FlinkRunner.class);\n-\n-    Pipeline p = Pipeline.create(options);\n-\n-    p.apply(\"ReadLines\", TextIO.Read.from(options.getInput()))\n-        .apply(new CountWords())\n-        .apply(MapElements.via(new FormatAsTextFn()))\n-        .apply(\"WriteCounts\", TextIO.Write.to(options.getOutput()));\n-\n-    p.run();\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "sha": "6ae4cf84352425f3cdfeda76bb96c7e3f022f18f",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "changes": 400,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 400,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "patch": "@@ -1,400 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedSocketSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.coders.AvroCoder;\n-import org.apache.beam.sdk.coders.DefaultCoder;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.Filter;\n-import org.apache.beam.sdk.transforms.Flatten;\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Partition;\n-import org.apache.beam.sdk.transforms.Partition.PartitionFn;\n-import org.apache.beam.sdk.transforms.SerializableFunction;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.Top;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PCollectionList;\n-import org.joda.time.Duration;\n-\n-/**\n- * To run the example, first open a socket on a terminal by executing the command:\n- * <ul>\n- *   <li><code>nc -lk 9999</code>\n- * </ul>\n- * and then launch the example. Now whatever you type in the terminal is going to be\n- * the input to the program.\n- * */\n-public class AutoComplete {\n-\n-  /**\n-   * A PTransform that takes as input a list of tokens and returns\n-   * the most common tokens per prefix.\n-   */\n-  public static class ComputeTopCompletions\n-      extends PTransform<PCollection<String>, PCollection<KV<String, List<CompletionCandidate>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int candidatesPerPrefix;\n-    private final boolean recursive;\n-\n-    protected ComputeTopCompletions(int candidatesPerPrefix, boolean recursive) {\n-      this.candidatesPerPrefix = candidatesPerPrefix;\n-      this.recursive = recursive;\n-    }\n-\n-    public static ComputeTopCompletions top(int candidatesPerPrefix, boolean recursive) {\n-      return new ComputeTopCompletions(candidatesPerPrefix, recursive);\n-    }\n-\n-    @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> expand(PCollection<String> input) {\n-      PCollection<CompletionCandidate> candidates = input\n-        // First count how often each token appears.\n-        .apply(Count.<String>perElement())\n-\n-        // Map the KV outputs of Count into our own CompletionCandiate class.\n-        .apply(\"CreateCompletionCandidates\", ParDo.of(\n-            new DoFn<KV<String, Long>, CompletionCandidate>() {\n-              private static final long serialVersionUID = 0;\n-\n-              @ProcessElement\n-              public void processElement(ProcessContext c) {\n-                CompletionCandidate cand = new CompletionCandidate(c.element().getKey(),\n-                    c.element().getValue());\n-                c.output(cand);\n-              }\n-            }));\n-\n-      // Compute the top via either a flat or recursive algorithm.\n-      if (recursive) {\n-        return candidates\n-          .apply(new ComputeTopRecursive(candidatesPerPrefix, 1))\n-          .apply(Flatten.<KV<String, List<CompletionCandidate>>>pCollections());\n-      } else {\n-        return candidates\n-          .apply(new ComputeTopFlat(candidatesPerPrefix, 1));\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Lower latency, but more expensive.\n-   */\n-  private static class ComputeTopFlat\n-      extends PTransform<PCollection<CompletionCandidate>,\n-                         PCollection<KV<String, List<CompletionCandidate>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int candidatesPerPrefix;\n-    private final int minPrefix;\n-\n-    public ComputeTopFlat(int candidatesPerPrefix, int minPrefix) {\n-      this.candidatesPerPrefix = candidatesPerPrefix;\n-      this.minPrefix = minPrefix;\n-    }\n-\n-    @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> expand(\n-        PCollection<CompletionCandidate> input) {\n-      return input\n-        // For each completion candidate, map it to all prefixes.\n-        .apply(ParDo.of(new AllPrefixes(minPrefix)))\n-\n-        // Find and return the top candiates for each prefix.\n-        .apply(Top.<String, CompletionCandidate>largestPerKey(candidatesPerPrefix)\n-             .withHotKeyFanout(new HotKeyFanout()));\n-    }\n-\n-    private static class HotKeyFanout implements SerializableFunction<String, Integer> {\n-      private static final long serialVersionUID = 0;\n-\n-      @Override\n-      public Integer apply(String input) {\n-        return (int) Math.pow(4, 5 - input.length());\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Cheaper but higher latency.\n-   *\n-   * <p>Returns two PCollections, the first is top prefixes of size greater\n-   * than minPrefix, and the second is top prefixes of size exactly\n-   * minPrefix.\n-   */\n-  private static class ComputeTopRecursive\n-      extends PTransform<PCollection<CompletionCandidate>,\n-                         PCollectionList<KV<String, List<CompletionCandidate>>>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int candidatesPerPrefix;\n-    private final int minPrefix;\n-\n-    public ComputeTopRecursive(int candidatesPerPrefix, int minPrefix) {\n-      this.candidatesPerPrefix = candidatesPerPrefix;\n-      this.minPrefix = minPrefix;\n-    }\n-\n-    private class KeySizePartitionFn implements PartitionFn<KV<String, List<CompletionCandidate>>> {\n-      private static final long serialVersionUID = 0;\n-\n-      @Override\n-      public int partitionFor(KV<String, List<CompletionCandidate>> elem, int numPartitions) {\n-        return elem.getKey().length() > minPrefix ? 0 : 1;\n-      }\n-    }\n-\n-    private static class FlattenTops\n-        extends DoFn<KV<String, List<CompletionCandidate>>, CompletionCandidate> {\n-      private static final long serialVersionUID = 0;\n-\n-      @ProcessElement\n-      public void processElement(ProcessContext c) {\n-        for (CompletionCandidate cc : c.element().getValue()) {\n-          c.output(cc);\n-        }\n-      }\n-    }\n-\n-    @Override\n-    public PCollectionList<KV<String, List<CompletionCandidate>>> expand(\n-          PCollection<CompletionCandidate> input) {\n-        if (minPrefix > 10) {\n-          // Base case, partitioning to return the output in the expected format.\n-          return input\n-            .apply(new ComputeTopFlat(candidatesPerPrefix, minPrefix))\n-            .apply(Partition.of(2, new KeySizePartitionFn()));\n-        } else {\n-          // If a candidate is in the top N for prefix a...b, it must also be in the top\n-          // N for a...bX for every X, which is typlically a much smaller set to consider.\n-          // First, compute the top candidate for prefixes of size at least minPrefix + 1.\n-          PCollectionList<KV<String, List<CompletionCandidate>>> larger = input\n-            .apply(new ComputeTopRecursive(candidatesPerPrefix, minPrefix + 1));\n-          // Consider the top candidates for each prefix of length minPrefix + 1...\n-          PCollection<KV<String, List<CompletionCandidate>>> small =\n-            PCollectionList\n-            .of(larger.get(1).apply(ParDo.of(new FlattenTops())))\n-            // ...together with those (previously excluded) candidates of length\n-            // exactly minPrefix...\n-            .and(input.apply(Filter.by(new SerializableFunction<CompletionCandidate, Boolean>() {\n-              private static final long serialVersionUID = 0;\n-\n-              @Override\n-              public Boolean apply(CompletionCandidate c) {\n-                return c.getValue().length() == minPrefix;\n-              }\n-            })))\n-            .apply(\"FlattenSmall\", Flatten.<CompletionCandidate>pCollections())\n-            // ...set the key to be the minPrefix-length prefix...\n-            .apply(ParDo.of(new AllPrefixes(minPrefix, minPrefix)))\n-            // ...and (re)apply the Top operator to all of them together.\n-            .apply(Top.<String, CompletionCandidate>largestPerKey(candidatesPerPrefix));\n-\n-          PCollection<KV<String, List<CompletionCandidate>>> flattenLarger = larger\n-              .apply(\"FlattenLarge\", Flatten.<KV<String, List<CompletionCandidate>>>pCollections());\n-\n-          return PCollectionList.of(flattenLarger).and(small);\n-        }\n-    }\n-  }\n-\n-  /**\n-   * A DoFn that keys each candidate by all its prefixes.\n-   */\n-  private static class AllPrefixes\n-      extends DoFn<CompletionCandidate, KV<String, CompletionCandidate>> {\n-    private static final long serialVersionUID = 0;\n-\n-    private final int minPrefix;\n-    private final int maxPrefix;\n-    public AllPrefixes(int minPrefix) {\n-      this(minPrefix, Integer.MAX_VALUE);\n-    }\n-    public AllPrefixes(int minPrefix, int maxPrefix) {\n-      this.minPrefix = minPrefix;\n-      this.maxPrefix = maxPrefix;\n-    }\n-    @ProcessElement\n-      public void processElement(ProcessContext c) {\n-      String word = c.element().value;\n-      for (int i = minPrefix; i <= Math.min(word.length(), maxPrefix); i++) {\n-        KV<String, CompletionCandidate> kv = KV.of(word.substring(0, i), c.element());\n-        c.output(kv);\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Class used to store tag-count pairs.\n-   */\n-  @DefaultCoder(AvroCoder.class)\n-  static class CompletionCandidate implements Comparable<CompletionCandidate> {\n-    private long count;\n-    private String value;\n-\n-    public CompletionCandidate(String value, long count) {\n-      this.value = value;\n-      this.count = count;\n-    }\n-\n-    public String getValue() {\n-      return value;\n-    }\n-\n-    // Empty constructor required for Avro decoding.\n-    @SuppressWarnings(\"unused\")\n-    public CompletionCandidate() {}\n-\n-    @Override\n-    public int compareTo(CompletionCandidate o) {\n-      if (this.count < o.count) {\n-        return -1;\n-      } else if (this.count == o.count) {\n-        return this.value.compareTo(o.value);\n-      } else {\n-        return 1;\n-      }\n-    }\n-\n-    @Override\n-    public boolean equals(Object other) {\n-      if (other instanceof CompletionCandidate) {\n-        CompletionCandidate that = (CompletionCandidate) other;\n-        return this.count == that.count && this.value.equals(that.value);\n-      } else {\n-        return false;\n-      }\n-    }\n-\n-    @Override\n-    public int hashCode() {\n-      return Long.valueOf(count).hashCode() ^ value.hashCode();\n-    }\n-\n-    @Override\n-    public String toString() {\n-      return \"CompletionCandidate[\" + value + \", \" + count + \"]\";\n-    }\n-  }\n-\n-  static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-            createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Takes as input a the top candidates per prefix, and emits an entity suitable for writing to\n-   * Datastore.\n-   */\n-  static class FormatForPerTaskLocalFile\n-      extends DoFn<KV<String, List<CompletionCandidate>>, String> {\n-\n-    private static final long serialVersionUID = 0;\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c, BoundedWindow window) {\n-      StringBuilder str = new StringBuilder();\n-      KV<String, List<CompletionCandidate>> elem = c.element();\n-\n-      str.append(elem.getKey() + \" @ \" + window + \" -> \");\n-      for (CompletionCandidate cand: elem.getValue()) {\n-        str.append(cand.toString() + \" \");\n-      }\n-      System.out.println(str.toString());\n-      c.output(str.toString());\n-    }\n-  }\n-\n-  /**\n-   * Options supported by this class.\n-   *\n-   * <p>Inherits standard Dataflow configuration options.\n-   */\n-  private interface Options extends WindowedWordCount.StreamingWordCountOptions {\n-    @Description(\"Whether to use the recursive algorithm\")\n-    @Default.Boolean(true)\n-    Boolean getRecursive();\n-    void setRecursive(Boolean value);\n-  }\n-\n-  public static void main(String[] args) throws IOException {\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\n-    options.setStreaming(true);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-\n-    WindowFn<Object, ?> windowFn =\n-        FixedWindows.of(Duration.standardSeconds(options.getWindowSize()));\n-\n-    // Create the pipeline.\n-    Pipeline p = Pipeline.create(options);\n-    PCollection<KV<String, List<CompletionCandidate>>> toWrite = p\n-      .apply(\"WordStream\", Read.from(new UnboundedSocketSource<>(\"localhost\", 9999, '\\n', 3)))\n-      .apply(ParDo.of(new ExtractWordsFn()))\n-      .apply(Window.<String>into(windowFn)\n-              .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes())\n-      .apply(ComputeTopCompletions.top(10, options.getRecursive()));\n-\n-    toWrite\n-      .apply(\"FormatForPerTaskFile\", ParDo.of(new FormatForPerTaskLocalFile()))\n-      .apply(TextIO.Write.to(\"./outputAutoComplete.txt\"));\n-\n-    p.run();\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "sha": "d07df29c004d57c1ee7260c2bfbfd25b5e61060e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java",
                "changes": 154,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 154,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java",
                "patch": "@@ -1,154 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedSocketSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.join.CoGbkResult;\n-import org.apache.beam.sdk.transforms.join.CoGroupByKey;\n-import org.apache.beam.sdk.transforms.join.KeyedPCollectionTuple;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.TupleTag;\n-import org.joda.time.Duration;\n-\n-/**\n- * To run the example, first open two sockets on two terminals by executing the commands:\n- * <ul>\n- *   <li><code>nc -lk 9999</code>, and\n- *   <li><code>nc -lk 9998</code>\n- * </ul>\n- * and then launch the example. Now whatever you type in the terminal is going to be\n- * the input to the program.\n- * */\n-public class JoinExamples {\n-\n-  static PCollection<String> joinEvents(PCollection<String> streamA,\n-                      PCollection<String> streamB) throws Exception {\n-\n-    final TupleTag<String> firstInfoTag = new TupleTag<>();\n-    final TupleTag<String> secondInfoTag = new TupleTag<>();\n-\n-    // transform both input collections to tuple collections, where the keys are country\n-    // codes in both cases.\n-    PCollection<KV<String, String>> firstInfo = streamA.apply(\n-        ParDo.of(new ExtractEventDataFn()));\n-    PCollection<KV<String, String>> secondInfo = streamB.apply(\n-        ParDo.of(new ExtractEventDataFn()));\n-\n-    // country code 'key' -> CGBKR (<event info>, <country name>)\n-    PCollection<KV<String, CoGbkResult>> kvpCollection = KeyedPCollectionTuple\n-        .of(firstInfoTag, firstInfo)\n-        .and(secondInfoTag, secondInfo)\n-        .apply(CoGroupByKey.<String>create());\n-\n-    // Process the CoGbkResult elements generated by the CoGroupByKey transform.\n-    // country code 'key' -> string of <event info>, <country name>\n-    PCollection<KV<String, String>> finalResultCollection =\n-        kvpCollection.apply(\"Process\", ParDo.of(\n-            new DoFn<KV<String, CoGbkResult>, KV<String, String>>() {\n-              private static final long serialVersionUID = 0;\n-\n-              @ProcessElement\n-              public void processElement(ProcessContext c) {\n-                KV<String, CoGbkResult> e = c.element();\n-                String key = e.getKey();\n-\n-                String defaultA = \"NO_VALUE\";\n-\n-                // the following getOnly is a bit tricky because it expects to have\n-                // EXACTLY ONE value in the corresponding stream and for the corresponding key.\n-\n-                String lineA = e.getValue().getOnly(firstInfoTag, defaultA);\n-                for (String lineB : c.element().getValue().getAll(secondInfoTag)) {\n-                  // Generate a string that combines information from both collection values\n-                  c.output(KV.of(key, \"Value A: \" + lineA + \" - Value B: \" + lineB));\n-                }\n-              }\n-            }));\n-\n-    return finalResultCollection\n-        .apply(\"Format\", ParDo.of(new DoFn<KV<String, String>, String>() {\n-          private static final long serialVersionUID = 0;\n-\n-          @ProcessElement\n-          public void processElement(ProcessContext c) {\n-            String result = c.element().getKey() + \" -> \" + c.element().getValue();\n-            System.out.println(result);\n-            c.output(result);\n-          }\n-        }));\n-  }\n-\n-  static class ExtractEventDataFn extends DoFn<String, KV<String, String>> {\n-    private static final long serialVersionUID = 0;\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      String line = c.element().toLowerCase();\n-      String key = line.split(\"\\\\s\")[0];\n-      c.output(KV.of(key, line));\n-    }\n-  }\n-\n-  private interface Options extends WindowedWordCount.StreamingWordCountOptions {\n-\n-  }\n-\n-  public static void main(String[] args) throws Exception {\n-    Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\n-    options.setStreaming(true);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-    WindowFn<Object, ?> windowFn = FixedWindows.of(\n-        Duration.standardSeconds(options.getWindowSize()));\n-\n-    Pipeline p = Pipeline.create(options);\n-\n-    // the following two 'applys' create multiple inputs to our pipeline, one for each\n-    // of our two input sources.\n-    PCollection<String> streamA = p\n-        .apply(\"FirstStream\", Read.from(new UnboundedSocketSource<>(\"localhost\", 9999, '\\n', 3)))\n-        .apply(Window.<String>into(windowFn)\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-    PCollection<String> streamB = p\n-        .apply(\"SecondStream\", Read.from(new UnboundedSocketSource<>(\"localhost\", 9998, '\\n', 3)))\n-        .apply(Window.<String>into(windowFn)\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-\n-    PCollection<String> formattedResults = joinEvents(streamA, streamB);\n-    formattedResults.apply(TextIO.Write.to(\"./outputJoin.txt\"));\n-    p.run();\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/JoinExamples.java",
                "sha": "8fefc9f2a529e12e158c4b34de40d44831826d10",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java",
                "changes": 338,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 338,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java",
                "patch": "@@ -1,338 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.io.ByteArrayInputStream;\n-import java.io.ByteArrayOutputStream;\n-import java.io.IOException;\n-import java.io.Serializable;\n-import java.util.Properties;\n-import org.apache.beam.runners.flink.FlinkPipelineOptions;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedFlinkSink;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedFlinkSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.coders.AvroCoder;\n-import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.coders.StringUtf8Coder;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.Write;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.java.typeutils.TypeExtractor;\n-import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08;\n-import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer08;\n-import org.apache.flink.streaming.util.serialization.DeserializationSchema;\n-import org.apache.flink.streaming.util.serialization.SerializationSchema;\n-import org.apache.flink.streaming.util.serialization.SimpleStringSchema;\n-\n-/**\n- * Recipes/Examples that demonstrate how to read/write data from/to Kafka.\n- */\n-public class KafkaIOExamples {\n-\n-\n-  private static final String KAFKA_TOPIC = \"input\";  // Default kafka topic to read from\n-  private static final String KAFKA_AVRO_TOPIC = \"output\";  // Default kafka topic to read from\n-  private static final String KAFKA_BROKER = \"localhost:9092\";  // Default kafka broker to contact\n-  private static final String GROUP_ID = \"myGroup\";  // Default groupId\n-  private static final String ZOOKEEPER = \"localhost:2181\";  // Default zookeeper to connect (Kafka)\n-\n-  /**\n-   * Read/Write String data to Kafka.\n-   */\n-  public static class KafkaString {\n-\n-    /**\n-     * Read String data from Kafka.\n-     */\n-    public static class ReadStringFromKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        FlinkKafkaConsumer08<String> kafkaConsumer =\n-            new FlinkKafkaConsumer08<>(options.getKafkaTopic(),\n-                new SimpleStringSchema(), getKafkaProps(options));\n-\n-        p\n-            .apply(Read.from(UnboundedFlinkSource.of(kafkaConsumer))).setCoder(StringUtf8Coder.of())\n-            .apply(ParDo.of(new PrintFn<>()));\n-\n-        p.run();\n-\n-      }\n-\n-    }\n-\n-    /**\n-     * Write String data to Kafka.\n-     */\n-    public static class WriteStringToKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        PCollection<String> words =\n-            p.apply(Create.of(\"These\", \"are\", \"some\", \"words\"));\n-\n-        FlinkKafkaProducer08<String> kafkaSink =\n-            new FlinkKafkaProducer08<>(options.getKafkaTopic(),\n-                new SimpleStringSchema(), getKafkaProps(options));\n-\n-        words.apply(Write.to(UnboundedFlinkSink.of(kafkaSink)));\n-\n-        p.run();\n-      }\n-\n-    }\n-  }\n-\n-  /**\n-   * Read/Write Avro data to Kafka.\n-   */\n-  public static class KafkaAvro {\n-\n-    /**\n-     * Read Avro data from Kafka.\n-     */\n-    public static class ReadAvroFromKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        FlinkKafkaConsumer08<MyType> kafkaConsumer =\n-            new FlinkKafkaConsumer08<>(options.getKafkaAvroTopic(),\n-                new AvroSerializationDeserializationSchema<>(MyType.class), getKafkaProps(options));\n-\n-        p\n-            .apply(Read.from(UnboundedFlinkSource.of(kafkaConsumer)))\n-                .setCoder(AvroCoder.of(MyType.class))\n-            .apply(ParDo.of(new PrintFn<>()));\n-\n-        p.run();\n-\n-      }\n-\n-    }\n-\n-    /**\n-     * Write Avro data to Kafka.\n-     */\n-    public static class WriteAvroToKafka {\n-\n-      public static void main(String[] args) {\n-\n-        Pipeline p = initializePipeline(args);\n-        KafkaOptions options = getOptions(p);\n-\n-        PCollection<MyType> words =\n-            p.apply(Create.of(\n-                new MyType(\"word\", 1L),\n-                new MyType(\"another\", 2L),\n-                new MyType(\"yet another\", 3L)));\n-\n-        FlinkKafkaProducer08<MyType> kafkaSink =\n-            new FlinkKafkaProducer08<>(options.getKafkaAvroTopic(),\n-                new AvroSerializationDeserializationSchema<>(MyType.class), getKafkaProps(options));\n-\n-        words.apply(Write.to(UnboundedFlinkSink.of(kafkaSink)));\n-\n-        p.run();\n-\n-      }\n-    }\n-\n-    /**\n-     * Serialiation/Deserialiation schema for Avro types.\n-     * @param <T> the type being encoded\n-     */\n-    static class AvroSerializationDeserializationSchema<T>\n-        implements SerializationSchema<T>, DeserializationSchema<T> {\n-\n-      private final Class<T> avroType;\n-\n-      private final AvroCoder<T> coder;\n-      private transient ByteArrayOutputStream out;\n-\n-      AvroSerializationDeserializationSchema(Class<T> clazz) {\n-        this.avroType = clazz;\n-        this.coder = AvroCoder.of(clazz);\n-        this.out = new ByteArrayOutputStream();\n-      }\n-\n-      @Override\n-      public byte[] serialize(T element) {\n-        if (out == null) {\n-          out = new ByteArrayOutputStream();\n-        }\n-        try {\n-          out.reset();\n-          coder.encode(element, out, Coder.Context.NESTED);\n-        } catch (IOException e) {\n-          throw new RuntimeException(\"Avro encoding failed.\", e);\n-        }\n-        return out.toByteArray();\n-      }\n-\n-      @Override\n-      public T deserialize(byte[] message) throws IOException {\n-        return coder.decode(new ByteArrayInputStream(message), Coder.Context.NESTED);\n-      }\n-\n-      @Override\n-      public boolean isEndOfStream(T nextElement) {\n-        return false;\n-      }\n-\n-      @Override\n-      public TypeInformation<T> getProducedType() {\n-        return TypeExtractor.getForClass(avroType);\n-      }\n-    }\n-\n-    /**\n-     * Custom type for Avro serialization.\n-     */\n-    static class MyType implements Serializable {\n-\n-      public MyType() {}\n-\n-      MyType(String word, long count) {\n-        this.word = word;\n-        this.count = count;\n-      }\n-\n-      String word;\n-      long count;\n-\n-      @Override\n-      public String toString() {\n-        return \"MyType{\"\n-            + \"word='\" + word + '\\''\n-            + \", count=\" + count\n-            + '}';\n-      }\n-    }\n-  }\n-\n-  // -------------- Utilities --------------\n-\n-  /**\n-   * Custom options for the Pipeline.\n-   */\n-  public interface KafkaOptions extends FlinkPipelineOptions {\n-    @Description(\"The Kafka topic to read from\")\n-    @Default.String(KAFKA_TOPIC)\n-    String getKafkaTopic();\n-\n-    void setKafkaTopic(String value);\n-\n-    void setKafkaAvroTopic(String value);\n-\n-    @Description(\"The Kafka topic to read from\")\n-    @Default.String(KAFKA_AVRO_TOPIC)\n-    String getKafkaAvroTopic();\n-\n-    @Description(\"The Kafka Broker to read from\")\n-    @Default.String(KAFKA_BROKER)\n-    String getBroker();\n-\n-    void setBroker(String value);\n-\n-    @Description(\"The Zookeeper server to connect to\")\n-    @Default.String(ZOOKEEPER)\n-    String getZookeeper();\n-\n-    void setZookeeper(String value);\n-\n-    @Description(\"The groupId\")\n-    @Default.String(GROUP_ID)\n-    String getGroup();\n-\n-    void setGroup(String value);\n-  }\n-\n-  /**\n-   * Initializes some options for the Flink runner.\n-   * @param args The command line args\n-   * @return the pipeline\n-   */\n-  private static Pipeline initializePipeline(String[] args) {\n-    KafkaOptions options =\n-        PipelineOptionsFactory.fromArgs(args).as(KafkaOptions.class);\n-\n-    options.setStreaming(true);\n-    options.setRunner(FlinkRunner.class);\n-\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-\n-    return Pipeline.create(options);\n-  }\n-\n-  /**\n-   * Gets KafkaOptions from the Pipeline.\n-   * @param p the pipeline\n-   * @return KafkaOptions\n-   */\n-  private static KafkaOptions getOptions(Pipeline p) {\n-    return p.getOptions().as(KafkaOptions.class);\n-  }\n-\n-  /**\n-   * Helper method to set the Kafka props from the pipeline options.\n-   * @param options KafkaOptions\n-   * @return Kafka props\n-   */\n-  private static Properties getKafkaProps(KafkaOptions options) {\n-\n-    Properties props = new Properties();\n-    props.setProperty(\"zookeeper.connect\", options.getZookeeper());\n-    props.setProperty(\"bootstrap.servers\", options.getBroker());\n-    props.setProperty(\"group.id\", options.getGroup());\n-\n-    return props;\n-  }\n-\n-  /**\n-   * Print contents to stdout.\n-   * @param <T> type of the input\n-   */\n-  private static class PrintFn<T> extends DoFn<T, T> {\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) throws Exception {\n-      System.out.println(c.element().toString());\n-    }\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaIOExamples.java",
                "sha": "616e276bab75877db9875b280b84f42ed013a18e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java",
                "changes": 164,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 164,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java",
                "patch": "@@ -1,164 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.util.Properties;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedFlinkSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer08;\n-import org.apache.flink.streaming.util.serialization.SimpleStringSchema;\n-import org.joda.time.Duration;\n-\n-/**\n- * Wordcount example using Kafka topic.\n- */\n-public class KafkaWindowedWordCountExample {\n-\n-  static final String KAFKA_TOPIC = \"test\";  // Default kafka topic to read from\n-  static final String KAFKA_BROKER = \"localhost:9092\";  // Default kafka broker to contact\n-  static final String GROUP_ID = \"myGroup\";  // Default groupId\n-  static final String ZOOKEEPER = \"localhost:2181\";  // Default zookeeper to connect to for Kafka\n-\n-  /**\n-   * Function to extract words.\n-   */\n-  public static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-        createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Function to format KV as String.\n-   */\n-  public static class FormatAsStringFn extends DoFn<KV<String, Long>, String> {\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      String row = c.element().getKey() + \" - \" + c.element().getValue() + \" @ \"\n-          + c.timestamp().toString();\n-      System.out.println(row);\n-      c.output(row);\n-    }\n-  }\n-\n-  /**\n-   * Pipeline options.\n-   */\n-  public interface KafkaStreamingWordCountOptions\n-      extends WindowedWordCount.StreamingWordCountOptions {\n-    @Description(\"The Kafka topic to read from\")\n-    @Default.String(KAFKA_TOPIC)\n-    String getKafkaTopic();\n-\n-    void setKafkaTopic(String value);\n-\n-    @Description(\"The Kafka Broker to read from\")\n-    @Default.String(KAFKA_BROKER)\n-    String getBroker();\n-\n-    void setBroker(String value);\n-\n-    @Description(\"The Zookeeper server to connect to\")\n-    @Default.String(ZOOKEEPER)\n-    String getZookeeper();\n-\n-    void setZookeeper(String value);\n-\n-    @Description(\"The groupId\")\n-    @Default.String(GROUP_ID)\n-    String getGroup();\n-\n-    void setGroup(String value);\n-\n-  }\n-\n-  public static void main(String[] args) {\n-    PipelineOptionsFactory.register(KafkaStreamingWordCountOptions.class);\n-    KafkaStreamingWordCountOptions options = PipelineOptionsFactory.fromArgs(args)\n-        .as(KafkaStreamingWordCountOptions.class);\n-    options.setJobName(\"KafkaExample - WindowSize: \" + options.getWindowSize() + \" seconds\");\n-    options.setStreaming(true);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-    System.out.println(options.getKafkaTopic() + \" \" + options.getZookeeper() + \" \"\n-        + options.getBroker() + \" \" + options.getGroup());\n-    Pipeline pipeline = Pipeline.create(options);\n-\n-    Properties p = new Properties();\n-    p.setProperty(\"zookeeper.connect\", options.getZookeeper());\n-    p.setProperty(\"bootstrap.servers\", options.getBroker());\n-    p.setProperty(\"group.id\", options.getGroup());\n-\n-    // this is the Flink consumer that reads the input to\n-    // the program from a kafka topic.\n-    FlinkKafkaConsumer08<String> kafkaConsumer = new FlinkKafkaConsumer08<>(\n-        options.getKafkaTopic(),\n-        new SimpleStringSchema(), p);\n-\n-    PCollection<String> words = pipeline\n-        .apply(\"StreamingWordCount\", Read.from(UnboundedFlinkSource.of(kafkaConsumer)))\n-        .apply(ParDo.of(new ExtractWordsFn()))\n-        .apply(Window.<String>into(FixedWindows.of(\n-            Duration.standardSeconds(options.getWindowSize())))\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-\n-    PCollection<KV<String, Long>> wordCounts =\n-        words.apply(Count.<String>perElement());\n-\n-    wordCounts.apply(ParDo.of(new FormatAsStringFn()))\n-        .apply(TextIO.Write.to(\"./outputKafka.txt\"));\n-\n-    pipeline.run();\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/KafkaWindowedWordCountExample.java",
                "sha": "ee0e874b713a40cef19695c0f6fe675019b0de4f",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java",
                "changes": 141,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 141,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java",
                "patch": "@@ -1,141 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.flink.examples.streaming;\n-\n-import java.io.IOException;\n-import org.apache.beam.runners.flink.FlinkRunner;\n-import org.apache.beam.runners.flink.translation.wrappers.streaming.io.UnboundedSocketSource;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.options.Default;\n-import org.apache.beam.sdk.options.Description;\n-import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.Sum;\n-import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n-import org.apache.beam.sdk.transforms.windowing.SlidingWindows;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.joda.time.Duration;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-/**\n- * To run the example, first open a socket on a terminal by executing the command:\n- * <ul>\n- *   <li><code>nc -lk 9999</code>\n- * </ul>\n- * and then launch the example. Now whatever you type in the terminal is going to be\n- * the input to the program.\n- * */\n-public class WindowedWordCount {\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(WindowedWordCount.class);\n-\n-  static final long WINDOW_SIZE = 10;  // Default window duration in seconds\n-  static final long SLIDE_SIZE = 5;  // Default window slide in seconds\n-\n-  static class FormatAsStringFn extends DoFn<KV<String, Long>, String> {\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      String row = c.element().getKey() + \" - \" + c.element().getValue() + \" @ \"\n-          + c.timestamp().toString();\n-      c.output(row);\n-    }\n-  }\n-\n-  static class ExtractWordsFn extends DoFn<String, String> {\n-    private final Aggregator<Long, Long> emptyLines =\n-        createAggregator(\"emptyLines\", Sum.ofLongs());\n-\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      if (c.element().trim().isEmpty()) {\n-        emptyLines.addValue(1L);\n-      }\n-\n-      // Split the line into words.\n-      String[] words = c.element().split(\"[^a-zA-Z']+\");\n-\n-      // Output each word encountered into the output PCollection.\n-      for (String word : words) {\n-        if (!word.isEmpty()) {\n-          c.output(word);\n-        }\n-      }\n-    }\n-  }\n-\n-  /**\n-   * Pipeline options.\n-   */\n-  public interface StreamingWordCountOptions\n-      extends org.apache.beam.runners.flink.examples.WordCount.Options {\n-    @Description(\"Sliding window duration, in seconds\")\n-    @Default.Long(WINDOW_SIZE)\n-    Long getWindowSize();\n-\n-    void setWindowSize(Long value);\n-\n-    @Description(\"Window slide, in seconds\")\n-    @Default.Long(SLIDE_SIZE)\n-    Long getSlide();\n-\n-    void setSlide(Long value);\n-  }\n-\n-  public static void main(String[] args) throws IOException {\n-    StreamingWordCountOptions options = PipelineOptionsFactory.fromArgs(args).withValidation()\n-        .as(StreamingWordCountOptions.class);\n-    options.setStreaming(true);\n-    options.setWindowSize(10L);\n-    options.setSlide(5L);\n-    options.setCheckpointingInterval(1000L);\n-    options.setNumberOfExecutionRetries(5);\n-    options.setExecutionRetryDelay(3000L);\n-    options.setRunner(FlinkRunner.class);\n-\n-    LOG.info(\"Windpwed WordCount with Sliding Windows of \" + options.getWindowSize()\n-        + \" sec. and a slide of \" + options.getSlide());\n-\n-    Pipeline pipeline = Pipeline.create(options);\n-\n-    PCollection<String> words = pipeline\n-        .apply(\"StreamingWordCount\",\n-            Read.from(new UnboundedSocketSource<>(\"localhost\", 9999, '\\n', 3)))\n-        .apply(ParDo.of(new ExtractWordsFn()))\n-        .apply(Window.<String>into(SlidingWindows.of(\n-            Duration.standardSeconds(options.getWindowSize()))\n-            .every(Duration.standardSeconds(options.getSlide())))\n-            .triggering(AfterWatermark.pastEndOfWindow()).withAllowedLateness(Duration.ZERO)\n-            .discardingFiredPanes());\n-\n-    PCollection<KV<String, Long>> wordCounts =\n-        words.apply(Count.<String>perElement());\n-\n-    wordCounts.apply(ParDo.of(new FormatAsStringFn()))\n-        .apply(TextIO.Write.to(\"./outputWordCount.txt\"));\n-\n-    pipeline.run();\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/WindowedWordCount.java",
                "sha": "792c214acc8960b8c3eceebe55061ebaf0616457",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/pom.xml?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/pom.xml",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/pom.xml",
                "sha": "808219b7457367704576ad90f9d35051dea528f2",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/pom.xml?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 0,
                "filename": "runners/flink/runner/pom.xml",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/pom.xml",
                "sha": "f2c2d018174554f4a9f488e8000a3f1c34c9e9d9",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 0,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/FlinkCoder.java",
                "sha": "8b90c73a26fbd2537381097ce92aff3697567604",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 0,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSink.java",
                "sha": "301d84171901fd70ad2d8eab103a49118e26f803",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java?ref=ebbb6139057deda05691fc357799506e5f9f3bf2",
                "deletions": 0,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java",
                "raw_url": "https://github.com/apache/beam/raw/ebbb6139057deda05691fc357799506e5f9f3bf2/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/io/UnboundedFlinkSource.java",
                "sha": "ac20c34ff204f5dbfd98bce3f6cec23c3a424398",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/DefaultParallelismFactory.java",
                "sha": "b745f0bd441acb8fdc7cc5b5f4c0a1223c23c4bc",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchPipelineTranslator.java",
                "sha": "854b67460ae2872cd2e2c2101de90cdf8e371c92",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTransformTranslators.java",
                "sha": "ff9521c009f21a44054395bf693b60cac3288269",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkBatchTranslationContext.java",
                "sha": "98dd0fb858ebb806557498872b5e1a86fb5fa3bc",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkDetachedRunnerResult.java",
                "sha": "bf4395fcdcca1b559bd1ad7241b239b61704cc97",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "sha": "ba00036f89e3ceb6a351d5e77677fb0bc401124c",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "sha": "ef9afeaed52b5bfe1f828f294171bf92b4300d02",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineTranslator.java",
                "sha": "65f416d94fc1309c747c62177dd889647626de16",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "sha": "096f0302a387a493fd44110d86cce808d6384d74",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerRegistrar.java",
                "sha": "681459a819d745bad2c55649aff72db55f8a0cd2",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkRunnerResult.java",
                "sha": "0682b5695f74b8bf9bd6bd80279215b43e98329b",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslator.java",
                "sha": "0459ef775166586b7766dc50425995dee0098023",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java",
                "sha": "123d5e720b7b5be4b8c2ee3cac182aa94391b6df",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java",
                "sha": "1a943a3dbb1b910437c83bbb40da3d9cab17e42f",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingViewOverrides.java",
                "sha": "f955f2a573ff4aadc6cba4d5febab68119dc75af",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/PipelineTranslationOptimizer.java",
                "sha": "3acc3eafca13a02dbd24797d1b733d504aed8da3",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TestFlinkRunner.java",
                "sha": "8f50105a55b98869ed239c7e4d1a15839ec48381",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/TranslationMode.java",
                "sha": "ad547506f5658394ce728a24bff64ff62569f3af",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/package-info.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/package-info.java",
                "sha": "57f1e599ee50250e5fffbe75c5c308955bfbf069",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAggregatorFactory.java",
                "sha": "fb2493bbe7e1624ac429b421c175c30a2ead3a44",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignContext.java",
                "sha": "447b1e507e1a5f0847965cc4b9ea43633ad50dff",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkAssignWindows.java",
                "sha": "c3a5095bc1eb61db1eb99d183f91e7026f8b984d",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "sha": "51582afda7fbc3fcc4442a428fea8d6a17c6efa8",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "sha": "26fd0b4f784562558ad202a71bf2df4197795cc4",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "sha": "c68f155d35557b700e512c6a94bd55951665f5f7",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "sha": "84b3adc38756902ed50a0b104fb126c1e2d47a7d",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputPruningFunction.java",
                "sha": "9071cc52ac1d409ddb1b56346dd5669ed5a6eeeb",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkNoOpStepContext.java",
                "sha": "847a00abdd82a8b7550ba620bde9fe66c76e1f6a",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "sha": "1d1ff9fc7ca9d5d215b0090375eacf9f97cfc0f7",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "sha": "3e4f742dfb0a6cd855eb600747345f8957cbc003",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java",
                "sha": "c317182ffde949be7ccf77bbedc4b018787e180d",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkStatefulDoFnFunction.java",
                "sha": "c8193d29c9f134db723102ba2cebc8ca24bbea4f",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/SideInputInitializer.java",
                "sha": "12222b499d74616f469ddd06f08d3334a23327aa",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/package-info.java",
                "sha": "9f1121225b8814e9ce509776c99e10a6441a46c9",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/package-info.java",
                "sha": "af4b35491ba41f9bffd1110eed0f30bb0494cd6f",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeInformation.java",
                "sha": "9b449aabc8b59f287725ccd444cec4deace21886",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/CoderTypeSerializer.java",
                "sha": "e210ed9d7b982c660e884483e8ae68fa139be936",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java?ref=4078c22fde9501bc28a5119b6f59522261776106",
                "deletions": 0,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
                "previous_filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
                "raw_url": "https://github.com/apache/beam/raw/4078c22fde9501bc28a5119b6f59522261776106/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/types/EncodedValueComparator.java",
                "sha": "667ef4591a3f0f4a9acac976a24e124f9aa87262",
                "status": "renamed"
            }
        ],
        "message": "This closes #2610: Merge master into gearpump-runner branch\n\n  Update gearpump-runner against master changes.\n  add temp dataset location for non-query BigQuerySource\n  added module option, use more common zero test, show module name in log\n  Modify types for input PCollections of Flatten transform to that of the output PCollection\n  [BEAM-1871] Remove another depedendency by moving TestCredential\n  [BEAM-2017] Fix NPE in DataflowRunner when there are no metrics\n  [BEAM-2013] Upgrade to Jackson 2.8.8\n  [BEAM-2014] Upgrade to Google Auth 0.6.1\n  [BEAM-2015] Remove shared profile in runners/pom.xml and fix Dataflow ValidatesRunner PostCommit\n  Cache result of BigQuerySourceBase.split\n  Ensure all Read outputs are consumed in Dataflow\n  [BEAM-1441] Remove deprecated ChannelFactory\n  [BEAM-1994] Remove Flink examples package\n  Pin default commons-compress version to beam-parent pom\n  [BEAM-1914]\u00a0XmlIO now complies with PTransform style guide\n  Separate streaming writes into two pluggable components - CreateTables, and StreamingWriteTables. Also address many code review comments. Also merge with master.\n  Fix tests to properly fake out BigQueryService, and add tests for dynamic-table functionality.\n  Refactor batch loads, and add support for windowed writes.\n  Refactor batch load job path, and add support for data-dependent tables.\n  Refactor streaming write branch into separate reusable components.\n  Add PrepareWrite transform.\n  Use tableRefFunction throughout BigQueryIO. Constant table writes use ConstantTableSpecFunction.\n  Explodes windows before GBKIKWI\n  Creates ProcessFnRunner and wires it through ParDoEvaluator\n  Extracts interface from PushbackSideInputDoFnRunner\n  Minor cleanups in ParDoEvaluator\n  ProcessFn remembers more info about its application context\n  Separates side input test and side output test\n  Changed snappy version to 1.1.4-M3\n  Upgrade worker to not depend on deprecated now deleted code\n  Delete AppEngineEnvironment\n  Delete IntervalBoundedExponentialBackoff\n  Delete AttemptBoundedExponentialBackoff\n  Remove deprecated/unused code from Pipeline\n  Remove deprecated method in IOChannelUtils\n  Delete deprecated AttemptAndTimeBoundedExponentialBackoff\n  [BEAM-1871] Create new GCP core module package and move several GCP related classes from beam-sdks-java-core over.\n  [BEAM-1964] Upgrade Pylint\n  Remove options_id concept from templated runs.\n  Revert \"Revert \"Throw specialized exception in value providers\"\"\n  Revert \"Revert \"Revert \"Revert \"Add ValueProvider class for FileBasedSource I/O Transforms\"\"\"\"\n  Removes unused validation parameter\n  Converts TFRecordIO.Write to AutoValue\n  Gets rid of TFRecordIO.Write.Bound\n  Converts TFRecordIO.Read to AutoValue\n  Gets rid of TFRecordIO.Read.Bound\n  runners-core-construction-java fix artifact name\n  Rename SideOutputValue to OutputValue\n  [BEAM-1990] Comment: Don't use Window.Assign\n  [BEAM-1272] Align the naming of \"generateInitialSplits\" and \"splitIntoBundles\" to better reflect their intention\n  Revert \"Removes final minor usages of OldDoFn outside OldDoFn itself\"\n  Fix Hadoop pom.xml\n  Making metrics usage in datastore_wordcount consistent\n  Remove overloading of __call__ in DirectRunner\n  Clean up DirectRunner Clock and TransformResult\n  Translate PTransforms to and from Runner API Protos\n  [BEAM-1993] Remove special unbounded Flink source/sink\n  Remove flink-annotations dependency\n  Fix Javadoc warnings on Flink Runner\n  Enable flink dependency enforcement and make dependencies explicit\n  [BEAM-59] Register standard FileSystems wherever we register IOChannelFactories\n  [BEAM-1991] Sum.SumDoubleFn => Sum.ofDoubles\n  clean up description for sdk_location\n  Set the Project of a Table Reference at Runtime\n  Only compile HIFIO ITs when compiling with java 8.\n  Update assertions of source_test_utils from camelcase to underscore-separated.\n  Add no-else return to pylintrc\n  Remove getSideInputWindow\n  Remove reference to the isStreaming flag\n  Javadoc fixups after style guide changes\n  Update Dataflow Worker Version\n  [BEAM-1922] Close datasource in JdbcIO when possible\n  Fix javadoc warnings\n  Add javadoc to getCheckpointMark in UnboundedSource\n  Removes final minor usages of OldDoFn outside OldDoFn itself\n  [BEAM-1915] Removes use of OldDoFn from Apex\n  Update Signature of PTransformOverrideFactory\n  [BEAM-1964] Fix lint issues and pylint upgrade\n  Rename DoFn.Context#sideOutput to output\n  [BEAM-1964] Fix lint issues for linter upgrade -3\n  [BEAM-1964] Fix lint issues for linter upgrade -2\n  Avoi repackaging bigtable classes in dataflow runner.\n  ApexRunner: register standard IOs when deserializing pipeline options\n  Add PCollections Utilities\n  Free PTransform Names if they are being Replaced\n  [BEAM-1347] Update protos related to State API for prototyping purposes.\n  Update java8 examples pom files to include maven-shade-plugin.\n  fix the simplest typo\n  [BEAM-1964] Fix lint issues for linter upgrade\n  Merge PR#2423: Add Kubernetes scripts for clusters for Performance and Integration tests of Cassandra and ES for Hadoop Input Format IO\n  Remove Triggers.java from SDK entirely\n  [BEAM-1708] Improve error message when GCP not installed\n  Improve gcloud logging message\n  [BEAM-1101, BEAM-1068] Remove service account name credential pipeline options\n  Update user_score.py\n  Pin versions in tox script\n  Improve Empty Create Default Coder Error Message\n  Represent a Pipeline via a list of Top-level Transforms\n  Test all Known Coders to ensure they Serialize via URN\n  [BEAM-1950] Add missing 'static' keyword to MicrobatchSource#initReaderCache\n  ...",
        "parent": "https://github.com/apache/beam/commit/ebbb6139057deda05691fc357799506e5f9f3bf2",
        "repo": "beam",
        "unit_tests": [
            "EncodedValueComparatorTest.java"
        ]
    },
    "beam_4c445dd": {
        "bug_id": "beam_4c445dd",
        "commit": "https://github.com/apache/beam/commit/4c445dd0b6de0f5045c02579cb432da4fbc5d486",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/.jenkins/common_job_properties.groovy",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.jenkins/common_job_properties.groovy?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": ".jenkins/common_job_properties.groovy",
                "patch": "@@ -24,6 +24,7 @@ class common_job_properties {\n   static def setTopLevelJobProperties(def context,\n                                       def default_branch = 'master',\n                                       def default_timeout = 100) {\n+\n     // GitHub project.\n     context.properties {\n       githubProjectUrl('https://github.com/apache/incubator-beam/')\n@@ -134,9 +135,15 @@ class common_job_properties {\n   // Sets common config for Maven jobs.\n   static def setMavenConfig(def context) {\n     context.mavenInstallation('Maven 3.3.3')\n+    context.mavenOpts('-Dorg.slf4j.simpleLogger.showDateTime=true')\n+    context.mavenOpts('-Dorg.slf4j.simpleLogger.dateTimeFormat=yyyy-MM-dd\\\\\\'T\\\\\\'HH:mm:ss.SSS')\n     context.rootPOM('pom.xml')\n     // Use a repository local to the workspace for better isolation of jobs.\n     context.localRepository(LocalRepositoryLocation.LOCAL_TO_WORKSPACE)\n+    // Disable archiving the built artifacts by default, as this is slow and flaky.\n+    // We can usually recreate them easily, and we can also opt-in individual jobs\n+    // to artifact archiving.\n+    context.archivingDisabled(true)\n   }\n \n   // Sets common config for PreCommit jobs.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/.jenkins/common_job_properties.groovy",
                "sha": "e1688ec060cf196de25cfe18a1ee56fba9ad98bd",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/.jenkins/job_beam_PostCommit_Java_RunnableOnService_Apex.groovy",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/.jenkins/job_beam_PostCommit_Java_RunnableOnService_Apex.groovy?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": ".jenkins/job_beam_PostCommit_Java_RunnableOnService_Apex.groovy",
                "patch": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import common_job_properties\n+\n+// This job runs the suite of RunnableOnService tests against the Apex runner.\n+mavenJob('beam_PostCommit_Java_RunnableOnService_Apex') {\n+  description('Runs the RunnableOnService suite on the Apex runner.')\n+\n+  // Set common parameters.\n+  common_job_properties.setTopLevelJobProperties(delegate)\n+\n+  // Set maven parameters.\n+  common_job_properties.setMavenConfig(delegate)\n+\n+  // Sets that this is a PostCommit job.\n+  common_job_properties.setPostCommit(delegate)\n+\n+  // Maven goals for this job.\n+  goals('''clean verify --projects runners/apex \\\n+      --also-make \\\n+      --batch-mode \\\n+      --errors \\\n+      --activate-profiles runnable-on-service-tests \\\n+      --activate-profiles local-runnable-on-service-tests''')\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/.jenkins/job_beam_PostCommit_Java_RunnableOnService_Apex.groovy",
                "sha": "232c94e1d5c457c30e8f0c1952940d66a9c6baf9",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/README.md",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/README.md?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "README.md",
                "patch": "@@ -39,7 +39,7 @@ fully endorsed by the ASF.\n _**The Apache Beam project is in the process of bootstrapping. This includes the creation of project resources, the refactoring of the initial code submissions, and the formulation of project documentation, planning, and design documents. Please expect a significant amount of churn and breaking changes in the near future.**_\n \n [![Build Status](https://api.travis-ci.org/apache/incubator-beam.svg?branch=master)](https://travis-ci.org/apache/incubator-beam?branch=master)\n-[![Build Status](https://builds.apache.org/buildStatus/icon?job=beam_PostCommit_MavenVerify)](https://builds.apache.org/job/beam_PostCommit_MavenVerify/)\n+[![Build Status](https://builds.apache.org/buildStatus/icon?job=beam_PostCommit_Java_MavenInstall)](https://builds.apache.org/job/beam_PostCommit_MavenVerify/)\n [![Coverage Status](https://coveralls.io/repos/github/apache/incubator-beam/badge.svg?branch=master)](https://coveralls.io/github/apache/incubator-beam?branch=master)\n \n ## Overview",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/README.md",
                "sha": "d919e5bd14a2eb1d8809f7b6c76f99b16cb4d284",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/pom.xml",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-examples-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -131,6 +131,24 @@\n         <skipITs>false</skipITs>\n         <skipDefaultIT>true</skipDefaultIT>\n       </properties>\n+\n+      <dependencies>\n+        <!--\n+          This profile requires -P apex-runner and -P spark-runner already. When they are both\n+          included, the Spark runner's Kryo dependency will pull in a version that is insufficiently\n+          recent for the Apex runner.\n+\n+          This dependency makes the Apex runner version \"closer\" by Maven resolution rules,\n+          so it will not be overridden.\n+        -->\n+        <dependency>\n+          <groupId>com.esotericsoftware.kryo</groupId>\n+          <artifactId>kryo</artifactId>\n+          <version>${apex.kryo.version}</version>\n+          <scope>runtime</scope>\n+        </dependency>\n+      </dependencies>\n+\n       <build>\n         <plugins>\n           <plugin>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/pom.xml",
                "sha": "f1e0fe16ccc05bd94f337db395698181d61486db",
                "status": "modified"
            },
            {
                "additions": 95,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "changes": 177,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 82,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "patch": "@@ -17,26 +17,25 @@\n  */\n package org.apache.beam.examples;\n \n-import com.google.api.services.bigquery.model.TableFieldSchema;\n-import com.google.api.services.bigquery.model.TableReference;\n-import com.google.api.services.bigquery.model.TableRow;\n-import com.google.api.services.bigquery.model.TableSchema;\n import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.List;\n+import java.util.concurrent.ThreadLocalRandom;\n import org.apache.beam.examples.common.ExampleBigQueryTableOptions;\n import org.apache.beam.examples.common.ExampleOptions;\n-import org.apache.beam.examples.common.ExampleUtils;\n+import org.apache.beam.examples.common.WriteWindowedFilesDoFn;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.io.TextIO;\n-import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;\n import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.DefaultValueFactory;\n import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -63,7 +62,8 @@\n  *   2. Adding timestamps to data\n  *   3. Windowing\n  *   4. Re-using PTransforms over windowed PCollections\n- *   5. Writing to BigQuery\n+ *   5. Accessing the window of an element\n+ *   6. Writing data to per-window text files\n  * </pre>\n  *\n  * <p>By default, the examples will run with the {@code DirectRunner}.\n@@ -74,25 +74,23 @@\n  * </pre>\n  * See examples/java/README.md for instructions about how to configure different runners.\n  *\n- * <p>Optionally specify the input file path via:\n- * {@code --inputFile=gs://INPUT_PATH},\n- * which defaults to {@code gs://apache-beam-samples/shakespeare/kinglear.txt}.\n+ * <p>To execute this pipeline locally, specify a local output file (if using the\n+ * {@code DirectRunner}) or output prefix on a supported distributed file system.\n+ * <pre>{@code\n+ *   --output=[YOUR_LOCAL_FILE | YOUR_OUTPUT_PREFIX]\n+ * }</pre>\n  *\n- * <p>Specify an output BigQuery dataset and optionally, a table for the output. If you don't\n- * specify the table, one will be created for you using the job name. If you don't specify the\n- * dataset, a dataset called {@code beam_examples} must already exist in your project.\n- * {@code --bigQueryDataset=YOUR-DATASET --bigQueryTable=YOUR-NEW-TABLE-NAME}.\n+ * <p>The input file defaults to a public data set containing the text of of King Lear,\n+ * by William Shakespeare. You can override it and choose your own input with {@code --inputFile}.\n  *\n  * <p>By default, the pipeline will do fixed windowing, on 1-minute windows.  You can\n  * change this interval by setting the {@code --windowSize} parameter, e.g. {@code --windowSize=10}\n  * for 10-minute windows.\n  *\n- * <p>The example will try to cancel the pipelines on the signal to terminate the process (CTRL-C)\n- * and then exits.\n+ * <p>The example will try to cancel the pipeline on the signal to terminate the process (CTRL-C).\n  */\n public class WindowedWordCount {\n-    static final int WINDOW_SIZE = 1;  // Default window duration in minutes\n-\n+    static final int WINDOW_SIZE = 10;  // Default window duration in minutes\n   /**\n    * Concept #2: A DoFn that sets the data element timestamp. This is a silly method, just for\n    * this example, for the bounded data case.\n@@ -102,84 +100,77 @@\n    * 2-hour period.\n    */\n   static class AddTimestampFn extends DoFn<String, String> {\n-    private static final Duration RAND_RANGE = Duration.standardHours(2);\n+    private static final Duration RAND_RANGE = Duration.standardHours(1);\n     private final Instant minTimestamp;\n+    private final Instant maxTimestamp;\n \n-    AddTimestampFn() {\n-      this.minTimestamp = new Instant(System.currentTimeMillis());\n+    AddTimestampFn(Instant minTimestamp, Instant maxTimestamp) {\n+      this.minTimestamp = minTimestamp;\n+      this.maxTimestamp = maxTimestamp;\n     }\n \n     @ProcessElement\n     public void processElement(ProcessContext c) {\n-      // Generate a timestamp that falls somewhere in the past two hours.\n-      long randMillis = (long) (Math.random() * RAND_RANGE.getMillis());\n-      Instant randomTimestamp = minTimestamp.plus(randMillis);\n+      Instant randomTimestamp =\n+          new Instant(\n+              ThreadLocalRandom.current()\n+                  .nextLong(minTimestamp.getMillis(), maxTimestamp.getMillis()));\n+\n       /**\n        * Concept #2: Set the data element with that timestamp.\n        */\n       c.outputWithTimestamp(c.element(), new Instant(randomTimestamp));\n     }\n   }\n \n-  /** A DoFn that converts a Word and Count into a BigQuery table row. */\n-  static class FormatAsTableRowFn extends DoFn<KV<String, Long>, TableRow> {\n-    @ProcessElement\n-    public void processElement(ProcessContext c) {\n-      TableRow row = new TableRow()\n-          .set(\"word\", c.element().getKey())\n-          .set(\"count\", c.element().getValue())\n-          // include a field for the window timestamp\n-         .set(\"window_timestamp\", c.timestamp().toString());\n-      c.output(row);\n+  /** A {@link DefaultValueFactory} that returns the current system time. */\n+  public static class DefaultToCurrentSystemTime implements DefaultValueFactory<Long> {\n+    @Override\n+    public Long create(PipelineOptions options) {\n+      return System.currentTimeMillis();\n     }\n   }\n \n-  /**\n-   * Helper method that defines the BigQuery schema used for the output.\n-   */\n-  private static TableSchema getSchema() {\n-    List<TableFieldSchema> fields = new ArrayList<>();\n-    fields.add(new TableFieldSchema().setName(\"word\").setType(\"STRING\"));\n-    fields.add(new TableFieldSchema().setName(\"count\").setType(\"INTEGER\"));\n-    fields.add(new TableFieldSchema().setName(\"window_timestamp\").setType(\"TIMESTAMP\"));\n-    TableSchema schema = new TableSchema().setFields(fields);\n-    return schema;\n-  }\n-\n-  /**\n-   * Concept #5: We'll stream the results to a BigQuery table. The BigQuery output source is one\n-   * that supports both bounded and unbounded data. This is a helper method that creates a\n-   * TableReference from input options, to tell the pipeline where to write its BigQuery results.\n-   */\n-  private static TableReference getTableReference(Options options) {\n-    TableReference tableRef = new TableReference();\n-    tableRef.setProjectId(options.getProject());\n-    tableRef.setDatasetId(options.getBigQueryDataset());\n-    tableRef.setTableId(options.getBigQueryTable());\n-    return tableRef;\n+  /** A {@link DefaultValueFactory} that returns the minimum timestamp plus one hour. */\n+  public static class DefaultToMinTimestampPlusOneHour implements DefaultValueFactory<Long> {\n+    @Override\n+    public Long create(PipelineOptions options) {\n+      return options.as(Options.class).getMinTimestampMillis()\n+          + Duration.standardHours(1).getMillis();\n+    }\n   }\n \n   /**\n-   * Options supported by {@link WindowedWordCount}.\n+   * Options for {@link WindowedWordCount}.\n    *\n-   * <p>Inherits standard example configuration options, which allow specification of the BigQuery\n-   * table, as well as the {@link WordCount.WordCountOptions} support for\n-   * specification of the input file.\n+   * <p>Inherits standard example configuration options, which allow specification of the\n+   * runner, as well as the {@link WordCount.WordCountOptions} support for\n+   * specification of the input and output files.\n    */\n   public interface Options extends WordCount.WordCountOptions,\n       ExampleOptions, ExampleBigQueryTableOptions {\n     @Description(\"Fixed window duration, in minutes\")\n     @Default.Integer(WINDOW_SIZE)\n     Integer getWindowSize();\n     void setWindowSize(Integer value);\n+\n+    @Description(\"Minimum randomly assigned timestamp, in milliseconds-since-epoch\")\n+    @Default.InstanceFactory(DefaultToCurrentSystemTime.class)\n+    Long getMinTimestampMillis();\n+    void setMinTimestampMillis(Long value);\n+\n+    @Description(\"Maximum randomly assigned timestamp, in milliseconds-since-epoch\")\n+    @Default.InstanceFactory(DefaultToMinTimestampPlusOneHour.class)\n+    Long getMaxTimestampMillis();\n+    void setMaxTimestampMillis(Long value);\n   }\n \n   public static void main(String[] args) throws IOException {\n     Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\n-    options.setBigQuerySchema(getSchema());\n-    // ExampleUtils creates the necessary input sources to simplify execution of this Pipeline.\n-    ExampleUtils exampleUtils = new ExampleUtils(options);\n-    exampleUtils.setup();\n+    final String output = options.getOutput();\n+    final Duration windowSize = Duration.standardMinutes(options.getWindowSize());\n+    final Instant minTimestamp = new Instant(options.getMinTimestampMillis());\n+    final Instant maxTimestamp = new Instant(options.getMaxTimestampMillis());\n \n     Pipeline pipeline = Pipeline.create(options);\n \n@@ -192,17 +183,18 @@ public static void main(String[] args) throws IOException {\n       .apply(TextIO.Read.from(options.getInputFile()))\n       // Concept #2: Add an element timestamp, using an artificial time just to show windowing.\n       // See AddTimestampFn for more detail on this.\n-      .apply(ParDo.of(new AddTimestampFn()));\n+      .apply(ParDo.of(new AddTimestampFn(minTimestamp, maxTimestamp)));\n \n     /**\n      * Concept #3: Window into fixed windows. The fixed window size for this example defaults to 1\n      * minute (you can change this with a command-line option). See the documentation for more\n      * information on how fixed windows work, and for information on the other types of windowing\n      * available (e.g., sliding windows).\n      */\n-    PCollection<String> windowedWords = input\n-      .apply(Window.<String>into(\n-        FixedWindows.of(Duration.standardMinutes(options.getWindowSize()))));\n+    PCollection<String> windowedWords =\n+        input.apply(\n+            Window.<String>into(\n+                FixedWindows.of(Duration.standardMinutes(options.getWindowSize()))));\n \n     /**\n      * Concept #4: Re-use our existing CountWords transform that does not have knowledge of\n@@ -211,19 +203,40 @@ public static void main(String[] args) throws IOException {\n     PCollection<KV<String, Long>> wordCounts = windowedWords.apply(new WordCount.CountWords());\n \n     /**\n-     * Concept #5: Format the results for a BigQuery table, then write to BigQuery.\n-     * The BigQuery output source supports both bounded and unbounded data.\n+     * Concept #5: Customize the output format using windowing information\n+     *\n+     * <p>At this point, the data is organized by window. We're writing text files and and have no\n+     * late data, so for simplicity we can use the window as the key and {@link GroupByKey} to get\n+     * one output file per window. (if we had late data this key would not be unique)\n+     *\n+     * <p>To access the window in a {@link DoFn}, add a {@link BoundedWindow} parameter. This will\n+     * be automatically detected and populated with the window for the current element.\n      */\n-    wordCounts.apply(ParDo.of(new FormatAsTableRowFn()))\n-        .apply(BigQueryIO.Write\n-          .to(getTableReference(options))\n-          .withSchema(getSchema())\n-          .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)\n-          .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND));\n+    PCollection<KV<IntervalWindow, KV<String, Long>>> keyedByWindow =\n+        wordCounts.apply(\n+            ParDo.of(\n+                new DoFn<KV<String, Long>, KV<IntervalWindow, KV<String, Long>>>() {\n+                  @ProcessElement\n+                  public void processElement(ProcessContext context, IntervalWindow window) {\n+                    context.output(KV.of(window, context.element()));\n+                  }\n+                }));\n \n-    PipelineResult result = pipeline.run();\n+    /**\n+     * Concept #6: Format the results and write to a sharded file partitioned by window, using a\n+     * simple ParDo operation. Because there may be failures followed by retries, the\n+     * writes must be idempotent, but the details of writing to files is elided here.\n+     */\n+    keyedByWindow\n+        .apply(GroupByKey.<IntervalWindow, KV<String, Long>>create())\n+        .apply(ParDo.of(new WriteWindowedFilesDoFn(output)));\n \n-    // ExampleUtils will try to cancel the pipeline before the program exists.\n-    exampleUtils.waitToFinish(result);\n+    PipelineResult result = pipeline.run();\n+    try {\n+      result.waitUntilFinish();\n+    } catch (Exception exc) {\n+      result.cancel();\n+    }\n   }\n+\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/WindowedWordCount.java",
                "sha": "5c19454d244ba0a5fb3ead747feda7b1857b4638",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/WordCount.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/WordCount.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/WordCount.java",
                "patch": "@@ -126,7 +126,7 @@ public String apply(KV<String, Long> input) {\n   public static class CountWords extends PTransform<PCollection<String>,\n       PCollection<KV<String, Long>>> {\n     @Override\n-    public PCollection<KV<String, Long>> apply(PCollection<String> lines) {\n+    public PCollection<KV<String, Long>> expand(PCollection<String> lines) {\n \n       // Convert lines of text into individual words.\n       PCollection<String> words = lines.apply(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/WordCount.java",
                "sha": "d4da54229c6b8ff260be3b05d71e7a539d95ca48",
                "status": "modified"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "patch": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples.common;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import java.io.OutputStream;\n+import java.nio.channels.Channels;\n+import java.nio.charset.StandardCharsets;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n+import org.apache.beam.sdk.util.IOChannelFactory;\n+import org.apache.beam.sdk.util.IOChannelUtils;\n+import org.apache.beam.sdk.values.KV;\n+import org.joda.time.format.DateTimeFormatter;\n+import org.joda.time.format.ISODateTimeFormat;\n+\n+/**\n+ * A {@link DoFn} that writes elements to files with names deterministically derived from the lower\n+ * and upper bounds of their key (an {@link IntervalWindow}).\n+ *\n+ * <p>This is test utility code, not for end-users, so examples can be focused\n+ * on their primary lessons.\n+ */\n+public class WriteWindowedFilesDoFn\n+    extends DoFn<KV<IntervalWindow, Iterable<KV<String, Long>>>, Void> {\n+\n+  static final byte[] NEWLINE = \"\\n\".getBytes(StandardCharsets.UTF_8);\n+  static final Coder<String> STRING_CODER = StringUtf8Coder.of();\n+\n+  private static DateTimeFormatter formatter = ISODateTimeFormat.hourMinute();\n+\n+  private final String output;\n+\n+  public WriteWindowedFilesDoFn(String output) {\n+    this.output = output;\n+  }\n+\n+  @VisibleForTesting\n+  public static String fileForWindow(String output, IntervalWindow window) {\n+    return String.format(\n+        \"%s-%s-%s\", output, formatter.print(window.start()), formatter.print(window.end()));\n+  }\n+\n+  @ProcessElement\n+  public void processElement(ProcessContext context) throws Exception {\n+    // Build a file name from the window\n+    IntervalWindow window = context.element().getKey();\n+    String outputShard = fileForWindow(output, window);\n+\n+    // Open the file and write all the values\n+    IOChannelFactory factory = IOChannelUtils.getFactory(outputShard);\n+    OutputStream out = Channels.newOutputStream(factory.create(outputShard, \"text/plain\"));\n+    for (KV<String, Long> wordCount : context.element().getValue()) {\n+      STRING_CODER.encode(\n+          wordCount.getKey() + \": \" + wordCount.getValue(), out, Coder.Context.OUTER);\n+      out.write(NEWLINE);\n+    }\n+    out.close();\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/common/WriteWindowedFilesDoFn.java",
                "sha": "cd6baad44292018ac6c4d8a839c14197ffeb752e",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "patch": "@@ -113,7 +113,7 @@ public static ComputeTopCompletions top(int candidatesPerPrefix, boolean recursi\n     }\n \n     @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> apply(PCollection<String> input) {\n+    public PCollection<KV<String, List<CompletionCandidate>>> expand(PCollection<String> input) {\n       PCollection<CompletionCandidate> candidates = input\n         // First count how often each token appears.\n         .apply(new Count.PerElement<String>())\n@@ -154,7 +154,7 @@ public ComputeTopFlat(int candidatesPerPrefix, int minPrefix) {\n     }\n \n     @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> apply(\n+    public PCollection<KV<String, List<CompletionCandidate>>> expand(\n         PCollection<CompletionCandidate> input) {\n       return input\n         // For each completion candidate, map it to all prefixes.\n@@ -209,7 +209,7 @@ public void processElement(ProcessContext c) {\n     }\n \n     @Override\n-    public PCollectionList<KV<String, List<CompletionCandidate>>> apply(\n+    public PCollectionList<KV<String, List<CompletionCandidate>>> expand(\n           PCollection<CompletionCandidate> input) {\n         if (minPrefix > 10) {\n           // Base case, partitioning to return the output in the expected format.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/AutoComplete.java",
                "sha": "31b06c9f0bc09dbda09538e7ee2e7b40194f2c00",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "patch": "@@ -159,7 +159,7 @@ public ReadDocuments(Iterable<URI> uris) {\n     }\n \n     @Override\n-    public PCollection<KV<URI, String>> apply(PBegin input) {\n+    public PCollection<KV<URI, String>> expand(PBegin input) {\n       Pipeline pipeline = input.getPipeline();\n \n       // Create one TextIO.Read transform for each document\n@@ -200,7 +200,7 @@ public ReadDocuments(Iterable<URI> uris) {\n     public ComputeTfIdf() { }\n \n     @Override\n-    public PCollection<KV<String, KV<URI, Double>>> apply(\n+    public PCollection<KV<String, KV<URI, Double>>> expand(\n       PCollection<KV<URI, String>> uriToContent) {\n \n       // Compute the total number of documents, and\n@@ -390,7 +390,7 @@ public WriteTfIdf(String output) {\n     }\n \n     @Override\n-    public PDone apply(PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf) {\n+    public PDone expand(PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf) {\n       return wordToUriAndTfIdf\n           .apply(\"Format\", ParDo.of(new DoFn<KV<String, KV<URI, Double>>, String>() {\n             @ProcessElement",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TfIdf.java",
                "sha": "ea015ae73447c891d49fa9fcffcf6d4ca616f64f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TopWikipediaSessions.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TopWikipediaSessions.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 12,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TopWikipediaSessions.java",
                "patch": "@@ -62,15 +62,6 @@\n  *\n  * <p>The default input is {@code gs://apache-beam-samples/wikipedia_edits/*.json} and can be\n  * overridden with {@code --input}.\n- *\n- * <p>The input for this example is large enough that it's a good place to enable (experimental)\n- * autoscaling:\n- * <pre>{@code\n- *   --autoscalingAlgorithm=BASIC\n- *   --maxNumWorkers=20\n- * }\n- * </pre>\n- * This will automatically scale the number of workers up over time until the job completes.\n  */\n public class TopWikipediaSessions {\n   private static final String EXPORTED_WIKI_TABLE =\n@@ -99,7 +90,7 @@ public void processElement(ProcessContext c) {\n   static class ComputeSessions\n       extends PTransform<PCollection<String>, PCollection<KV<String, Long>>> {\n     @Override\n-    public PCollection<KV<String, Long>> apply(PCollection<String> actions) {\n+    public PCollection<KV<String, Long>> expand(PCollection<String> actions) {\n       return actions\n           .apply(Window.<String>into(Sessions.withGapDuration(Duration.standardHours(1))))\n \n@@ -113,7 +104,7 @@ public void processElement(ProcessContext c) {\n   private static class TopPerMonth\n       extends PTransform<PCollection<KV<String, Long>>, PCollection<List<KV<String, Long>>>> {\n     @Override\n-    public PCollection<List<KV<String, Long>>> apply(PCollection<KV<String, Long>> sessions) {\n+    public PCollection<List<KV<String, Long>>> expand(PCollection<KV<String, Long>> sessions) {\n       return sessions\n         .apply(Window.<KV<String, Long>>into(CalendarWindows.months(1)))\n \n@@ -154,7 +145,7 @@ public ComputeTopSessions(double samplingThreshold) {\n     }\n \n     @Override\n-    public PCollection<String> apply(PCollection<TableRow> input) {\n+    public PCollection<String> expand(PCollection<TableRow> input) {\n       return input\n           .apply(ParDo.of(new ExtractUserAndTimestamp()))\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TopWikipediaSessions.java",
                "sha": "8e0b815bf2607379d5119abf3dae96a7bbe153bd",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "patch": "@@ -267,7 +267,7 @@ static TableSchema getSchema() {\n   static class MaxLaneFlow\n       extends PTransform<PCollection<KV<String, LaneInfo>>, PCollection<TableRow>> {\n     @Override\n-    public PCollection<TableRow> apply(PCollection<KV<String, LaneInfo>> flowInfo) {\n+    public PCollection<TableRow> expand(PCollection<KV<String, LaneInfo>> flowInfo) {\n       // stationId, LaneInfo => stationId + max lane flow info\n       PCollection<KV<String, LaneInfo>> flowMaxes =\n           flowInfo.apply(Combine.<String, LaneInfo>perKey(\n@@ -289,7 +289,7 @@ public ReadFileAndExtractTimestamps(String inputFile) {\n     }\n \n     @Override\n-    public PCollection<String> apply(PBegin begin) {\n+    public PCollection<String> expand(PBegin begin) {\n       return begin\n           .apply(TextIO.Read.from(inputFile))\n           .apply(ParDo.of(new ExtractTimestamps()));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficMaxLaneFlow.java",
                "sha": "c1032b9720893176f438fa0fe8d8a8466166e2bf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "patch": "@@ -274,7 +274,7 @@ static TableSchema getSchema() {\n   static class TrackSpeed extends\n       PTransform<PCollection<KV<String, StationSpeed>>, PCollection<TableRow>> {\n     @Override\n-    public PCollection<TableRow> apply(PCollection<KV<String, StationSpeed>> stationSpeed) {\n+    public PCollection<TableRow> expand(PCollection<KV<String, StationSpeed>> stationSpeed) {\n       // Apply a GroupByKey transform to collect a list of all station\n       // readings for a given route.\n       PCollection<KV<String, Iterable<StationSpeed>>> timeGroup = stationSpeed.apply(\n@@ -299,7 +299,7 @@ public ReadFileAndExtractTimestamps(String inputFile) {\n     }\n \n     @Override\n-    public PCollection<String> apply(PBegin begin) {\n+    public PCollection<String> expand(PBegin begin) {\n       return begin\n           .apply(TextIO.Read.from(inputFile))\n           .apply(ParDo.of(new ExtractTimestamps()));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/complete/TrafficRoutes.java",
                "sha": "9b5d5779bebc19c889e5fcd82a35b8666dd8c49a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "patch": "@@ -108,7 +108,7 @@ public void processElement(ProcessContext c) {\n   static class CountTornadoes\n       extends PTransform<PCollection<TableRow>, PCollection<TableRow>> {\n     @Override\n-    public PCollection<TableRow> apply(PCollection<TableRow> rows) {\n+    public PCollection<TableRow> expand(PCollection<TableRow> rows) {\n \n       // row... => month...\n       PCollection<Integer> tornadoes = rows.apply(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/BigQueryTornadoes.java",
                "sha": "14d0f582b223a44a7916cb6e079a6ef9ba2cfcb2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "patch": "@@ -125,7 +125,7 @@ public void processElement(ProcessContext c) {\n   static class PlaysForWord\n       extends PTransform<PCollection<TableRow>, PCollection<TableRow>> {\n     @Override\n-    public PCollection<TableRow> apply(PCollection<TableRow> rows) {\n+    public PCollection<TableRow> expand(PCollection<TableRow> rows) {\n \n       // row... => <word, play_name> ...\n       PCollection<KV<String, String>> words = rows.apply(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/CombinePerKeyExamples.java",
                "sha": "29655ea861273299bbc05d8a355e28cf59e3c94e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "patch": "@@ -153,7 +153,7 @@ public BelowGlobalMean(Integer monthFilter) {\n \n \n     @Override\n-    public PCollection<TableRow> apply(PCollection<TableRow> rows) {\n+    public PCollection<TableRow> expand(PCollection<TableRow> rows) {\n \n       // Extract the mean_temp from each row.\n       PCollection<Double> meanTemps = rows.apply(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/FilterExamples.java",
                "sha": "fb6b50710ca258ed964464a80c8b8b45a10ca045",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "patch": "@@ -100,7 +100,7 @@ public void processElement(ProcessContext c) {\n   static class MaxMeanTemp\n       extends PTransform<PCollection<TableRow>, PCollection<TableRow>> {\n     @Override\n-    public PCollection<TableRow> apply(PCollection<TableRow> rows) {\n+    public PCollection<TableRow> expand(PCollection<TableRow> rows) {\n \n       // row... => <month, mean_temp> ...\n       PCollection<KV<Integer, Double>> temps = rows.apply(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/MaxPerKeyExamples.java",
                "sha": "eabc42b9fbcbffbba86ceba2c4cb68e7233f8807",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "patch": "@@ -171,7 +171,7 @@\n     }\n \n     @Override\n-    public PCollectionList<TableRow> apply(PCollection<KV<String, Integer>> flowInfo) {\n+    public PCollectionList<TableRow> expand(PCollection<KV<String, Integer>> flowInfo) {\n \n       // Concept #1: The default triggering behavior\n       // By default Beam uses a trigger which fires when the watermark has passed the end of the\n@@ -332,7 +332,7 @@ public TotalFlow(String triggerType) {\n     }\n \n     @Override\n-    public PCollection<TableRow> apply(PCollection<KV<String, Integer>> flowInfo) {\n+    public PCollection<TableRow> expand(PCollection<KV<String, Integer>> flowInfo) {\n       PCollection<KV<String, Iterable<Integer>>> flowPerFreeway = flowInfo\n           .apply(GroupByKey.<String, Integer>create());\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/main/java/org/apache/beam/examples/cookbook/TriggerExample.java",
                "sha": "bf3afca51485ce8240ef675f832decc37e77ece2",
                "status": "modified"
            },
            {
                "additions": 154,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "changes": 182,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 28,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "patch": "@@ -17,74 +17,200 @@\n  */\n package org.apache.beam.examples;\n \n-import java.io.IOException;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+import com.google.api.client.util.Sleeper;\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import java.util.Collections;\n import java.util.Date;\n+import java.util.List;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import org.apache.beam.examples.common.WriteWindowedFilesDoFn;\n+import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.options.StreamingOptions;\n-import org.apache.beam.sdk.testing.BigqueryMatcher;\n+import org.apache.beam.sdk.testing.FileChecksumMatcher;\n+import org.apache.beam.sdk.testing.SerializableMatcher;\n import org.apache.beam.sdk.testing.StreamingIT;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.testing.TestPipelineOptions;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n+import org.apache.beam.sdk.util.ExplicitShardedFile;\n+import org.apache.beam.sdk.util.FluentBackoff;\n import org.apache.beam.sdk.util.IOChannelUtils;\n+import org.apache.beam.sdk.util.ShardedFile;\n+import org.hamcrest.Description;\n+import org.hamcrest.TypeSafeMatcher;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n import org.junit.runner.RunWith;\n import org.junit.runners.JUnit4;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n-/**\n- * End-to-end integration test of {@link WindowedWordCount}.\n- */\n+/** End-to-end integration test of {@link WindowedWordCount}. */\n @RunWith(JUnit4.class)\n public class WindowedWordCountIT {\n \n   private static final String DEFAULT_INPUT =\n       \"gs://apache-beam-samples/shakespeare/winterstale-personae\";\n-  private static final String DEFAULT_OUTPUT_CHECKSUM = \"cd5b52939257e12428a9fa085c32a84dd209b180\";\n+  static final int MAX_READ_RETRIES = 4;\n+  static final Duration DEFAULT_SLEEP_DURATION = Duration.standardSeconds(10L);\n+  static final FluentBackoff BACK_OFF_FACTORY =\n+      FluentBackoff.DEFAULT\n+          .withInitialBackoff(DEFAULT_SLEEP_DURATION)\n+          .withMaxRetries(MAX_READ_RETRIES);\n \n-  /**\n-   * Options for the {@link WindowedWordCount} Integration Test.\n-   */\n+  /** Options for the {@link WindowedWordCount} Integration Test. */\n   public interface WindowedWordCountITOptions\n-      extends WindowedWordCount.Options, TestPipelineOptions, StreamingOptions {\n-  }\n+      extends WindowedWordCount.Options, TestPipelineOptions, StreamingOptions {}\n \n   @BeforeClass\n   public static void setUp() {\n     PipelineOptionsFactory.register(TestPipelineOptions.class);\n   }\n \n   @Test\n-  public void testWindowedWordCountInBatch() throws IOException {\n-    testWindowedWordCountPipeline(false /* isStreaming */);\n+  public void testWindowedWordCountInBatch() throws Exception {\n+    testWindowedWordCountPipeline(defaultOptions());\n   }\n \n   @Test\n   @Category(StreamingIT.class)\n-  public void testWindowedWordCountInStreaming() throws IOException {\n-    testWindowedWordCountPipeline(true /* isStreaming */);\n+  public void testWindowedWordCountInStreaming() throws Exception {\n+    testWindowedWordCountPipeline(streamingOptions());\n   }\n \n-  private void testWindowedWordCountPipeline(boolean isStreaming) throws IOException {\n+  private WindowedWordCountITOptions defaultOptions() throws Exception {\n     WindowedWordCountITOptions options =\n         TestPipeline.testingPipelineOptions().as(WindowedWordCountITOptions.class);\n-    options.setStreaming(isStreaming);\n     options.setInputFile(DEFAULT_INPUT);\n+    options.setTestTimeoutSeconds(1200L);\n+\n+    options.setMinTimestampMillis(0L);\n+    options.setMinTimestampMillis(Duration.standardHours(1).getMillis());\n+    options.setWindowSize(10);\n+\n+    options.setOutput(\n+        IOChannelUtils.resolve(\n+            options.getTempRoot(),\n+            String.format(\"WindowedWordCountIT-%tF-%<tH-%<tM-%<tS-%<tL\", new Date()),\n+            \"output\",\n+            \"results\"));\n+    return options;\n+  }\n+\n+  private WindowedWordCountITOptions streamingOptions() throws Exception {\n+    WindowedWordCountITOptions options = defaultOptions();\n+    options.setStreaming(true);\n+    return options;\n+  }\n+\n+  private WindowedWordCountITOptions batchOptions() throws Exception {\n+    WindowedWordCountITOptions options = defaultOptions();\n+    // This is the default value, but make it explicit\n+    options.setStreaming(false);\n+    return options;\n+  }\n+\n+  private void testWindowedWordCountPipeline(WindowedWordCountITOptions options) throws Exception {\n+\n+    String outputPrefix = options.getOutput();\n+\n+    List<String> expectedOutputFiles = Lists.newArrayListWithCapacity(6);\n+    for (int startMinute : ImmutableList.of(0, 10, 20, 30, 40, 50)) {\n+      Instant windowStart =\n+          new Instant(options.getMinTimestampMillis()).plus(Duration.standardMinutes(startMinute));\n+      expectedOutputFiles.add(\n+          WriteWindowedFilesDoFn.fileForWindow(\n+              outputPrefix,\n+              new IntervalWindow(windowStart, windowStart.plus(Duration.standardMinutes(10)))));\n+    }\n \n-    // Note: currently unused because the example writes to BigQuery, but WindowedWordCount.Options\n-    // are tightly coupled to WordCount.Options, where the option is required.\n-    options.setOutput(IOChannelUtils.resolve(\n-        options.getTempRoot(),\n-        String.format(\"WindowedWordCountIT-%tF-%<tH-%<tM-%<tS-%<tL\", new Date()),\n-        \"output\",\n-        \"results\"));\n+    ShardedFile inputFile =\n+        new ExplicitShardedFile(Collections.singleton(options.getInputFile()));\n+\n+    // For this integration test, input is tiny and we can build the expected counts\n+    SortedMap<String, Long> expectedWordCounts = new TreeMap<>();\n+    for (String line :\n+        inputFile.readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff())) {\n+      String[] words = line.split(\"[^a-zA-Z']+\");\n+\n+      for (String word : words) {\n+        if (!word.isEmpty()) {\n+          expectedWordCounts.put(word,\n+              MoreObjects.firstNonNull(expectedWordCounts.get(word), 0L) + 1L);\n+        }\n+      }\n+    }\n \n-    String query = String.format(\"SELECT word, SUM(count) FROM [%s:%s.%s] GROUP BY word\",\n-        options.getProject(), options.getBigQueryDataset(), options.getBigQueryTable());\n     options.setOnSuccessMatcher(\n-        new BigqueryMatcher(\n-            options.getAppName(), options.getProject(), query, DEFAULT_OUTPUT_CHECKSUM));\n+        new WordCountsMatcher(expectedWordCounts, new ExplicitShardedFile(expectedOutputFiles)));\n \n     WindowedWordCount.main(TestPipeline.convertToArgs(options));\n   }\n+\n+  /**\n+   * A matcher that bakes in expected word counts, so they can be read directly via some other\n+   * mechanism, and compares a sharded output file with the result.\n+   */\n+  private static class WordCountsMatcher extends TypeSafeMatcher<PipelineResult>\n+      implements SerializableMatcher<PipelineResult> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(FileChecksumMatcher.class);\n+\n+    private final SortedMap<String, Long> expectedWordCounts;\n+    private final ShardedFile outputFile;\n+    private SortedMap<String, Long> actualCounts;\n+\n+    public WordCountsMatcher(SortedMap<String, Long> expectedWordCounts, ShardedFile outputFile) {\n+      this.expectedWordCounts = expectedWordCounts;\n+      this.outputFile = outputFile;\n+    }\n+\n+    @Override\n+    public boolean matchesSafely(PipelineResult pipelineResult) {\n+      try {\n+        // Load output data\n+        List<String> lines =\n+            outputFile.readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff());\n+\n+        // Since the windowing is nondeterministic we only check the sums\n+        actualCounts = new TreeMap<>();\n+        for (String line : lines) {\n+          String[] splits = line.split(\": \");\n+          String word = splits[0];\n+          long count = Long.parseLong(splits[1]);\n+\n+          Long current = actualCounts.get(word);\n+          if (current == null) {\n+            actualCounts.put(word, count);\n+          } else {\n+            actualCounts.put(word, current + count);\n+          }\n+        }\n+\n+        return actualCounts.equals(expectedWordCounts);\n+      } catch (Exception e) {\n+        throw new RuntimeException(\n+            String.format(\"Failed to read from sharded output: %s\", outputFile));\n+      }\n+    }\n+\n+    @Override\n+    public void describeTo(Description description) {\n+      equalTo(expectedWordCounts).describeTo(description);\n+    }\n+\n+    @Override\n+    public void describeMismatchSafely(PipelineResult pResult, Description description) {\n+      equalTo(expectedWordCounts).describeMismatch(actualCounts, description);\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/test/java/org/apache/beam/examples/WindowedWordCountIT.java",
                "sha": "e4570acb9533c586a5e5f6cee086cda735d6deff",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/test/java/org/apache/beam/examples/complete/AutoCompleteTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java/src/test/java/org/apache/beam/examples/complete/AutoCompleteTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java/src/test/java/org/apache/beam/examples/complete/AutoCompleteTest.java",
                "patch": "@@ -168,7 +168,7 @@ public void testWindowedAutoComplete() {\n   private static class ReifyTimestamps<T>\n       extends PTransform<PCollection<TimestampedValue<T>>, PCollection<T>> {\n     @Override\n-    public PCollection<T> apply(PCollection<TimestampedValue<T>> input) {\n+    public PCollection<T> expand(PCollection<TimestampedValue<T>> input) {\n       return input.apply(ParDo.of(new DoFn<TimestampedValue<T>, T>() {\n         @ProcessElement\n         public void processElement(ProcessContext c) {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java/src/test/java/org/apache/beam/examples/complete/AutoCompleteTest.java",
                "sha": "d7d4dc6c37d96c13864b8fb283854fab42c3bf59",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java8/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-examples-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/pom.xml",
                "sha": "1abf6fdd5974cdbecefeb1e5bd991c807e52e167",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "patch": "@@ -110,7 +110,7 @@\n     private static final double SCORE_WEIGHT = 2.5;\n \n     @Override\n-    public PCollection<KV<String, Integer>> apply(PCollection<KV<String, Integer>> userScores) {\n+    public PCollection<KV<String, Integer>> expand(PCollection<KV<String, Integer>> userScores) {\n \n       // Get the sum of scores for each user.\n       PCollection<KV<String, Integer>> sumScores = userScores",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/GameStats.java",
                "sha": "6ad6a236d6aa231554bf6ae808c10b2f333a77cb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "patch": "@@ -234,7 +234,7 @@ public static void main(String[] args) throws Exception {\n     }\n \n     @Override\n-    public PCollection<KV<String, Integer>> apply(PCollection<GameActionInfo> infos) {\n+    public PCollection<KV<String, Integer>> expand(PCollection<GameActionInfo> infos) {\n       return infos.apply(\"LeaderboardTeamFixedWindows\",\n           Window.<GameActionInfo>into(FixedWindows.of(teamWindowDuration))\n               // We will get early (speculative) results as well as cumulative\n@@ -267,7 +267,7 @@ public static void main(String[] args) throws Exception {\n     }\n \n     @Override\n-    public PCollection<KV<String, Integer>> apply(PCollection<GameActionInfo> input) {\n+    public PCollection<KV<String, Integer>> expand(PCollection<GameActionInfo> input) {\n       return input.apply(\"LeaderboardUserGlobalWindow\",\n           Window.<GameActionInfo>into(new GlobalWindows())\n               // Get periodic results every ten minutes.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/LeaderBoard.java",
                "sha": "519bd5f53475cd52bbd76350bdb3ab63b39eb52c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "patch": "@@ -160,7 +160,7 @@ public void processElement(ProcessContext c) {\n     }\n \n     @Override\n-    public PCollection<KV<String, Integer>> apply(\n+    public PCollection<KV<String, Integer>> expand(\n         PCollection<GameActionInfo> gameInfo) {\n \n       return gameInfo",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/UserScore.java",
                "sha": "cb81a7e65428b10d4075420e12a4b21cd99e1819",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "patch": "@@ -118,7 +118,7 @@ protected TableSchema getSchema() {\n   }\n \n   @Override\n-  public PDone apply(PCollection<InputT> teamAndScore) {\n+  public PDone expand(PCollection<InputT> teamAndScore) {\n     return teamAndScore\n       .apply(\"ConvertToRow\", ParDo.of(new BuildRowFn()))\n       .apply(BigQueryIO.Write",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteToBigQuery.java",
                "sha": "1f33915b7a301ce74f2b7f0df3c8c10329ac916a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "patch": "@@ -23,7 +23,6 @@\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;\n import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;\n import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.OldDoFn.RequiresWindowAccess;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.values.PCollection;\n@@ -43,9 +42,7 @@ public WriteWindowedToBigQuery(String tableName,\n   }\n \n   /** Convert each key/score pair into a BigQuery TableRow. */\n-  protected class BuildRowFn extends DoFn<T, TableRow>\n-      implements RequiresWindowAccess {\n-\n+  protected class BuildRowFn extends DoFn<T, TableRow> {\n     @ProcessElement\n     public void processElement(ProcessContext c, BoundedWindow window) {\n \n@@ -60,7 +57,7 @@ public void processElement(ProcessContext c, BoundedWindow window) {\n   }\n \n   @Override\n-  public PDone apply(PCollection<T> teamAndScore) {\n+  public PDone expand(PCollection<T> teamAndScore) {\n     return teamAndScore\n       .apply(\"ConvertToRow\", ParDo.of(new BuildRowFn()))\n       .apply(BigQueryIO.Write",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/java8/src/main/java/org/apache/beam/examples/complete/game/utils/WriteWindowedToBigQuery.java",
                "sha": "7a4fb2c8c1957d8fb2209eac1d30b715171271e8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/examples/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "examples/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/examples/pom.xml",
                "sha": "3e0fbddfee1b43289b1fd4dc96aa4b2233fc7379",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/pom.xml",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "pom.xml",
                "patch": "@@ -34,7 +34,7 @@\n   <url>http://beam.incubator.apache.org</url>\n   <inceptionYear>2016</inceptionYear>\n \n-  <version>0.4.0-incubating-SNAPSHOT</version>\n+  <version>0.5.0-incubating-SNAPSHOT</version>\n \n   <licenses>\n     <license>\n@@ -101,6 +101,7 @@\n     <beamSurefireArgline />\n \n     <!-- If updating dependencies, please update any relevant javadoc offlineLinks -->\n+    <apex.kryo.version>2.24.0</apex.kryo.version>\n     <avro.version>1.8.1</avro.version>\n     <bigquery.version>v2-rev295-1.22.0</bigquery.version>\n     <cloudresourcemanager.version>v1-rev6-1.22.0</cloudresourcemanager.version>\n@@ -122,7 +123,7 @@\n     <jackson.version>2.7.2</jackson.version>\n     <findbugs.version>3.0.1</findbugs.version>\n     <joda.version>2.4</joda.version>\n-    <junit.version>4.11</junit.version>\n+    <junit.version>4.12</junit.version>\n     <mockito.version>1.9.5</mockito.version>\n     <netty.version>4.1.3.Final</netty.version>\n     <os-maven-plugin.version>1.4.0.Final</os-maven-plugin.version>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/pom.xml",
                "sha": "970547def704e5cfa78aa1bfd38c6d1565ecd0a2",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/pom.xml",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 12,
                "filename": "runners/apex/pom.xml",
                "patch": "@@ -15,16 +15,14 @@\n     See the License for the specific language governing permissions and\n     limitations under the License.\n -->\n-<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n-         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n-         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n \n   <modelVersion>4.0.0</modelVersion>\n \n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -35,7 +33,7 @@\n   <packaging>jar</packaging>\n \n   <properties>\n-    <apex.core.version>3.5.0-SNAPSHOT</apex.core.version>\n+    <apex.core.version>3.5.0</apex.core.version>\n     <apex.malhar.version>3.4.0</apex.malhar.version>\n     <skipIntegrationTests>true</skipIntegrationTests>\n     <!-- memory limit for embedded cluster -->\n@@ -75,7 +73,7 @@\n       <scope>runtime</scope>\n     </dependency>\n \n-    <!--- Beam -->\n+    <!-- Beam -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-core</artifactId>\n@@ -187,7 +185,9 @@\n               <groups>org.apache.beam.sdk.testing.RunnableOnService</groups>\n               <excludedGroups>\n                 org.apache.beam.sdk.testing.UsesStatefulParDo,\n-                org.apache.beam.sdk.testing.UsesSplittableParDo\n+                org.apache.beam.sdk.testing.UsesTimersInParDo,\n+                org.apache.beam.sdk.testing.UsesSplittableParDo,\n+                org.apache.beam.sdk.testing.UsesMetrics\n               </excludedGroups>\n               <parallel>none</parallel>\n               <failIfNoTests>true</failIfNoTests>\n@@ -217,22 +217,64 @@\n             </goals>\n             <configuration>\n               <ignoredUsedUndeclaredDependencies>\n-                <ignoredUsedUndeclaredDependency>org.apache.apex:apex-api:jar:3.5.0-SNAPSHOT</ignoredUsedUndeclaredDependency>\n+                <ignoredUsedUndeclaredDependency>org.apache.apex:apex-api:jar:3.5.0</ignoredUsedUndeclaredDependency>\n                 <ignoredUsedUndeclaredDependency>org.apache.commons:commons-lang3::3.1</ignoredUsedUndeclaredDependency>\n-                <ignoredUsedUndeclaredDependency>commons-io:commons-io:jar:</ignoredUsedUndeclaredDependency>\n-                <ignoredUsedUndeclaredDependency>com.esotericsoftware.kryo:kryo::2.24.0</ignoredUsedUndeclaredDependency>\n-                <ignoredUsedUndeclaredDependency>com.datatorrent:netlet::</ignoredUsedUndeclaredDependency>\n+                <ignoredUsedUndeclaredDependency>commons-io:commons-io:jar:2.4</ignoredUsedUndeclaredDependency>\n+                <ignoredUsedUndeclaredDependency>com.esotericsoftware.kryo:kryo::${apex.kryo.version}</ignoredUsedUndeclaredDependency>\n+                <ignoredUsedUndeclaredDependency>com.datatorrent:netlet::1.3.0</ignoredUsedUndeclaredDependency>\n                 <ignoredUsedUndeclaredDependency>org.slf4j:slf4j-api:jar:1.7.14</ignoredUsedUndeclaredDependency>\n-                <ignoredUsedUndeclaredDependency>org.apache.hadoop:hadoop-common:jar:</ignoredUsedUndeclaredDependency>\n+                <ignoredUsedUndeclaredDependency>org.apache.hadoop:hadoop-common:jar:2.6.0</ignoredUsedUndeclaredDependency>\n                 <ignoredUsedUndeclaredDependency>joda-time:joda-time:jar:2.4</ignoredUsedUndeclaredDependency>\n                 <ignoredUsedUndeclaredDependency>com.google.guava:guava:jar:19.0</ignoredUsedUndeclaredDependency>\n               </ignoredUsedUndeclaredDependencies>\n             </configuration>\n           </execution>\n+          <execution>\n+            <!--  used in ApexYarnLauncher to filter compile time Hadoop dependencies -->\n+            <id>dependency-tree</id>\n+            <phase>generate-test-resources</phase>\n+            <goals>\n+              <goal>tree</goal>\n+            </goals>\n+            <configuration>\n+              <outputFile>${project.build.directory}/classes/org/apache/beam/runners/apex/dependency-tree</outputFile>\n+            </configuration>\n+          </execution>\n         </executions>\n       </plugin>\n \n     </plugins>\n+\n+    <pluginManagement>\n+      <plugins>\n+        <!-- Eclipse has a problem with dependency:tree when it is not in package phase -->\n+        <!--This plugin's configuration is used to store Eclipse m2e settings only. It has no influence on the Maven build itself.-->\n+        <plugin>\n+          <groupId>org.eclipse.m2e</groupId>\n+          <artifactId>lifecycle-mapping</artifactId>\n+          <version>1.0.0</version>\n+          <configuration>\n+            <lifecycleMappingMetadata>\n+              <pluginExecutions>\n+                <pluginExecution>\n+                  <pluginExecutionFilter>\n+                    <groupId>org.apache.maven.plugins</groupId>\n+                    <artifactId>maven-dependency-plugin</artifactId>\n+                    <versionRange>[2.10,)</versionRange>\n+                    <goals>\n+                      <goal>tree</goal>\n+                    </goals>\n+                  </pluginExecutionFilter>\n+                  <action>\n+                    <ignore />\n+                  </action>\n+                </pluginExecution>\n+              </pluginExecutions>\n+            </lifecycleMappingMetadata>\n+          </configuration>\n+        </plugin>\n+      </plugins>\n+    </pluginManagement>\n   </build>\n \n </project>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/pom.xml",
                "sha": "d03964daaa7e0187ba4f1d8d9cbabce95e634c3c",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "changes": 111,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 72,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "patch": "@@ -17,21 +17,21 @@\n  */\n package org.apache.beam.runners.apex;\n \n-import static com.google.common.base.Preconditions.checkArgument;\n-\n+import com.datatorrent.api.Attribute;\n import com.datatorrent.api.Context.DAGContext;\n import com.datatorrent.api.DAG;\n-import com.datatorrent.api.LocalMode;\n import com.datatorrent.api.StreamingApplication;\n import com.google.common.base.Throwables;\n-\n+import java.io.IOException;\n import java.util.ArrayList;\n-import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.concurrent.atomic.AtomicReference;\n-\n+import org.apache.apex.api.EmbeddedAppLauncher;\n+import org.apache.apex.api.Launcher;\n+import org.apache.apex.api.Launcher.AppHandle;\n+import org.apache.apex.api.Launcher.LaunchMode;\n import org.apache.beam.runners.apex.translation.ApexPipelineTranslator;\n-import org.apache.beam.runners.core.AssignWindows;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderRegistry;\n@@ -42,13 +42,9 @@\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View;\n-import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.PCollectionViews;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollection;\n@@ -90,10 +86,7 @@ public static ApexRunner fromOptions(PipelineOptions options) {\n   public <OutputT extends POutput, InputT extends PInput> OutputT apply(\n       PTransform<InputT, OutputT> transform, InputT input) {\n \n-    if (Window.Bound.class.equals(transform.getClass())) {\n-      return (OutputT) ((PCollection) input).apply(\n-          new AssignWindowsAndSetStrategy((Window.Bound) transform));\n-    } else if (Create.Values.class.equals(transform.getClass())) {\n+    if (Create.Values.class.equals(transform.getClass())) {\n       return (OutputT) PCollection\n           .<OutputT>createPrimitiveOutputInternal(\n               input.getPipeline(),\n@@ -122,70 +115,44 @@ public static ApexRunner fromOptions(PipelineOptions options) {\n   public ApexRunnerResult run(final Pipeline pipeline) {\n \n     final ApexPipelineTranslator translator = new ApexPipelineTranslator(options);\n+    final AtomicReference<DAG> apexDAG = new AtomicReference<>();\n \n     StreamingApplication apexApp = new StreamingApplication() {\n       @Override\n       public void populateDAG(DAG dag, Configuration conf) {\n+        apexDAG.set(dag);\n         dag.setAttribute(DAGContext.APPLICATION_NAME, options.getApplicationName());\n         translator.translate(pipeline, dag);\n       }\n     };\n \n-    checkArgument(options.isEmbeddedExecution(),\n-        \"only embedded execution is supported at this time\");\n-    LocalMode lma = LocalMode.newInstance();\n-    Configuration conf = new Configuration(false);\n-    try {\n-      lma.prepareDAG(apexApp, conf);\n-      LocalMode.Controller lc = lma.getController();\n+    if (options.isEmbeddedExecution()) {\n+      Launcher<AppHandle> launcher = Launcher.getLauncher(LaunchMode.EMBEDDED);\n+      Attribute.AttributeMap launchAttributes = new Attribute.AttributeMap.DefaultAttributeMap();\n+      launchAttributes.put(EmbeddedAppLauncher.RUN_ASYNC, true);\n       if (options.isEmbeddedExecutionDebugMode()) {\n         // turns off timeout checking for operator progress\n-        lc.setHeartbeatMonitoringEnabled(false);\n+        launchAttributes.put(EmbeddedAppLauncher.HEARTBEAT_MONITORING, false);\n       }\n-      ApexRunner.ASSERTION_ERROR.set(null);\n-      lc.runAsync();\n-      return new ApexRunnerResult(lma.getDAG(), lc);\n-    } catch (Exception e) {\n-      Throwables.propagateIfPossible(e);\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n-  /**\n-   * copied from DirectPipelineRunner.\n-   * used to replace Window.Bound till equivalent function is added in Apex\n-   */\n-  private static class AssignWindowsAndSetStrategy<T, W extends BoundedWindow>\n-      extends PTransform<PCollection<T>, PCollection<T>> {\n-\n-    private final Window.Bound<T> wrapped;\n-\n-    public AssignWindowsAndSetStrategy(Window.Bound<T> wrapped) {\n-      this.wrapped = wrapped;\n-    }\n-\n-    @Override\n-    public PCollection<T> apply(PCollection<T> input) {\n-      WindowingStrategy<?, ?> outputStrategy =\n-          wrapped.getOutputStrategyInternal(input.getWindowingStrategy());\n-\n-      WindowFn<T, BoundedWindow> windowFn =\n-          (WindowFn<T, BoundedWindow>) outputStrategy.getWindowFn();\n-\n-      // If the Window.Bound transform only changed parts other than the WindowFn, then\n-      // we skip AssignWindows even though it should be harmless in a perfect world.\n-      // The world is not perfect, and a GBK may have set it to InvalidWindows to forcibly\n-      // crash if another GBK is performed without explicitly setting the WindowFn. So we skip\n-      // AssignWindows in this case.\n-      if (wrapped.getWindowFn() == null) {\n-        return input.apply(\"Identity\", ParDo.of(new IdentityFn<T>()))\n-            .setWindowingStrategyInternal(outputStrategy);\n-      } else {\n-        return input\n-            .apply(\"AssignWindows\", new AssignWindows<>(windowFn))\n-            .setWindowingStrategyInternal(outputStrategy);\n+      Configuration conf = new Configuration(false);\n+      try {\n+        ApexRunner.ASSERTION_ERROR.set(null);\n+        AppHandle apexAppResult = launcher.launchApp(apexApp, conf, launchAttributes);\n+        return new ApexRunnerResult(apexDAG.get(), apexAppResult);\n+      } catch (Exception e) {\n+        Throwables.propagateIfPossible(e);\n+        throw new RuntimeException(e);\n+      }\n+    } else {\n+      try {\n+        ApexYarnLauncher yarnLauncher = new ApexYarnLauncher();\n+        AppHandle apexAppResult = yarnLauncher.launchApp(apexApp);\n+        return new ApexRunnerResult(apexDAG.get(), apexAppResult);\n+      } catch (IOException e) {\n+        throw new RuntimeException(\"Failed to launch the application on YARN.\", e);\n       }\n     }\n+\n   }\n \n   private static class IdentityFn<T> extends DoFn<T, T> {\n@@ -226,15 +193,15 @@ private CreateApexPCollectionView(PCollectionView<ViewT> view) {\n     }\n \n     @Override\n-    public PCollectionView<ViewT> apply(PCollection<List<ElemT>> input) {\n+    public PCollectionView<ViewT> expand(PCollection<List<ElemT>> input) {\n       return view;\n     }\n   }\n \n-  private static class WrapAsList<T> extends OldDoFn<T, List<T>> {\n-    @Override\n+  private static class WrapAsList<T> extends DoFn<T, List<T>> {\n+    @ProcessElement\n     public void processElement(ProcessContext c) {\n-      c.output(Arrays.asList(c.element()));\n+      c.output(Collections.singletonList(c.element()));\n     }\n   }\n \n@@ -252,7 +219,7 @@ public StreamingCombineGloballyAsSingletonView(ApexRunner runner,\n     }\n \n     @Override\n-    public PCollectionView<OutputT> apply(PCollection<InputT> input) {\n+    public PCollectionView<OutputT> expand(PCollection<InputT> input) {\n       PCollection<OutputT> combined = input\n           .apply(Combine.globally(transform.getCombineFn())\n               .withoutDefaults().withFanout(transform.getFanout()));\n@@ -282,7 +249,7 @@ public StreamingViewAsSingleton(ApexRunner runner, View.AsSingleton<T> transform\n     }\n \n     @Override\n-    public PCollectionView<T> apply(PCollection<T> input) {\n+    public PCollectionView<T> expand(PCollection<T> input) {\n       Combine.Globally<T, T> combine = Combine\n           .globally(new SingletonCombine<>(transform.hasDefaultValue(), transform.defaultValue()));\n       if (!transform.hasDefaultValue()) {\n@@ -335,7 +302,7 @@ public StreamingViewAsIterable(ApexRunner runner, View.AsIterable<T> transform)\n     }\n \n     @Override\n-    public PCollectionView<Iterable<T>> apply(PCollection<T> input) {\n+    public PCollectionView<Iterable<T>> expand(PCollection<T> input) {\n       PCollectionView<Iterable<T>> view = PCollectionViews.iterableView(input.getPipeline(),\n           input.getWindowingStrategy(), input.getCoder());\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunner.java",
                "sha": "f12ebef7b6e74562bf1d43a67dd37bf37a4cd457",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunnerResult.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunnerResult.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 30,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunnerResult.java",
                "patch": "@@ -18,11 +18,11 @@\n package org.apache.beam.runners.apex;\n \n import com.datatorrent.api.DAG;\n-import com.datatorrent.api.LocalMode;\n \n import java.io.IOException;\n-import java.lang.reflect.Field;\n \n+import org.apache.apex.api.Launcher.AppHandle;\n+import org.apache.apex.api.Launcher.ShutdownMode;\n import org.apache.beam.sdk.AggregatorRetrievalException;\n import org.apache.beam.sdk.AggregatorValues;\n import org.apache.beam.sdk.Pipeline;\n@@ -36,12 +36,12 @@\n  */\n public class ApexRunnerResult implements PipelineResult {\n   private final DAG apexDAG;\n-  private final LocalMode.Controller ctrl;\n+  private final AppHandle apexApp;\n   private State state = State.UNKNOWN;\n \n-  public ApexRunnerResult(DAG dag, LocalMode.Controller ctrl) {\n+  public ApexRunnerResult(DAG dag, AppHandle apexApp) {\n     this.apexDAG = dag;\n-    this.ctrl = ctrl;\n+    this.apexApp = apexApp;\n   }\n \n   @Override\n@@ -57,19 +57,31 @@ public State getState() {\n \n   @Override\n   public State cancel() throws IOException {\n-    ctrl.shutdown();\n+    apexApp.shutdown(ShutdownMode.KILL);\n     state = State.CANCELLED;\n     return state;\n   }\n \n   @Override\n   public State waitUntilFinish(Duration duration) {\n-    return ApexRunnerResult.waitUntilFinished(ctrl, duration);\n+    long timeout = (duration == null || duration.getMillis() < 1) ? Long.MAX_VALUE\n+        : System.currentTimeMillis() + duration.getMillis();\n+    try {\n+      while (!apexApp.isFinished() && System.currentTimeMillis() < timeout) {\n+        if (ApexRunner.ASSERTION_ERROR.get() != null) {\n+          throw ApexRunner.ASSERTION_ERROR.get();\n+        }\n+        Thread.sleep(500);\n+      }\n+      return apexApp.isFinished() ? State.DONE : null;\n+    } catch (InterruptedException e) {\n+      throw new RuntimeException(e);\n+    }\n   }\n \n   @Override\n   public State waitUntilFinish() {\n-    return ApexRunnerResult.waitUntilFinished(ctrl, null);\n+    return waitUntilFinish(null);\n   }\n \n   @Override\n@@ -85,26 +97,4 @@ public DAG getApexDAG() {\n     return apexDAG;\n   }\n \n-  public static State waitUntilFinished(LocalMode.Controller ctrl, Duration duration) {\n-    // we need to rely on internal field for now\n-    // Apex should make it available through API in upcoming release.\n-    long timeout = (duration == null || duration.getMillis() < 1) ? Long.MAX_VALUE\n-        : System.currentTimeMillis() + duration.getMillis();\n-    Field appDoneField;\n-    try {\n-      appDoneField = ctrl.getClass().getDeclaredField(\"appDone\");\n-      appDoneField.setAccessible(true);\n-      while (!appDoneField.getBoolean(ctrl) && System.currentTimeMillis() < timeout) {\n-        if (ApexRunner.ASSERTION_ERROR.get() != null) {\n-          throw ApexRunner.ASSERTION_ERROR.get();\n-        }\n-        Thread.sleep(500);\n-      }\n-      return appDoneField.getBoolean(ctrl) ? State.DONE : null;\n-    } catch (NoSuchFieldException | SecurityException | IllegalArgumentException\n-        | IllegalAccessException | InterruptedException e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexRunnerResult.java",
                "sha": "85481945b6e504975709015c3037e8d80a508bd6",
                "status": "modified"
            },
            {
                "additions": 395,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "changes": 395,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "patch": "@@ -0,0 +1,395 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.apex;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+\n+import com.datatorrent.api.Attribute;\n+import com.datatorrent.api.Attribute.AttributeMap;\n+import com.datatorrent.api.DAG;\n+import com.datatorrent.api.StreamingApplication;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.Sets;\n+\n+import java.io.BufferedReader;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.io.Serializable;\n+import java.lang.reflect.AccessibleObject;\n+import java.lang.reflect.Field;\n+import java.net.URI;\n+import java.net.URL;\n+import java.net.URLClassLoader;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.FileVisitResult;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.BasicFileAttributes;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.jar.JarFile;\n+import java.util.jar.Manifest;\n+\n+import org.apache.apex.api.EmbeddedAppLauncher;\n+import org.apache.apex.api.Launcher;\n+import org.apache.apex.api.Launcher.AppHandle;\n+import org.apache.apex.api.Launcher.LaunchMode;\n+import org.apache.apex.api.Launcher.LauncherException;\n+import org.apache.apex.api.Launcher.ShutdownMode;\n+import org.apache.apex.api.YarnAppLauncher;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.IOUtils;\n+import org.apache.commons.lang3.SerializationUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Proxy to launch the YARN application through the hadoop script to run in the\n+ * pre-configured environment (class path, configuration, native libraries etc.).\n+ *\n+ * <p>The proxy takes the DAG and communicates with the Hadoop services to launch\n+ * it on the cluster.\n+ */\n+public class ApexYarnLauncher {\n+  private static final Logger LOG = LoggerFactory.getLogger(ApexYarnLauncher.class);\n+\n+  public AppHandle launchApp(StreamingApplication app) throws IOException {\n+\n+    List<File> jarsToShip = getYarnDeployDependencies();\n+    StringBuilder classpath = new StringBuilder();\n+    for (File path : jarsToShip) {\n+      if (path.isDirectory()) {\n+        File tmpJar = File.createTempFile(\"beam-runners-apex-\", \".jar\");\n+        createJar(path, tmpJar);\n+        tmpJar.deleteOnExit();\n+        path = tmpJar;\n+      }\n+      if (classpath.length() != 0) {\n+        classpath.append(':');\n+      }\n+      classpath.append(path.getAbsolutePath());\n+    }\n+\n+    EmbeddedAppLauncher<?> embeddedLauncher = Launcher.getLauncher(LaunchMode.EMBEDDED);\n+    DAG dag = embeddedLauncher.getDAG();\n+    app.populateDAG(dag, new Configuration(false));\n+\n+    Attribute.AttributeMap launchAttributes = new Attribute.AttributeMap.DefaultAttributeMap();\n+    launchAttributes.put(YarnAppLauncher.LIB_JARS, classpath.toString().replace(':', ','));\n+    LaunchParams lp = new LaunchParams(dag, launchAttributes);\n+    lp.cmd = \"hadoop \" + ApexYarnLauncher.class.getName();\n+    HashMap<String, String> env = new HashMap<>();\n+    env.put(\"HADOOP_USER_CLASSPATH_FIRST\", \"1\");\n+    env.put(\"HADOOP_CLASSPATH\", classpath.toString());\n+    lp.env = env;\n+    return launchApp(lp);\n+  }\n+\n+  protected AppHandle launchApp(LaunchParams params) throws IOException {\n+    File tmpFile = File.createTempFile(\"beam-runner-apex\", \"params\");\n+    tmpFile.deleteOnExit();\n+    try (FileOutputStream fos = new FileOutputStream(tmpFile)) {\n+      SerializationUtils.serialize(params, fos);\n+    }\n+    if (params.getCmd() == null) {\n+      ApexYarnLauncher.main(new String[] {tmpFile.getAbsolutePath()});\n+    } else {\n+      String cmd = params.getCmd() + \" \" + tmpFile.getAbsolutePath();\n+      ByteArrayOutputStream consoleOutput = new ByteArrayOutputStream();\n+      LOG.info(\"Executing: {} with {}\", cmd, params.getEnv());\n+\n+      ProcessBuilder pb = new ProcessBuilder(\"bash\", \"-c\", cmd);\n+      Map<String, String> env = pb.environment();\n+      env.putAll(params.getEnv());\n+      Process p = pb.start();\n+      ProcessWatcher pw = new ProcessWatcher(p);\n+      InputStream output = p.getInputStream();\n+      InputStream error = p.getErrorStream();\n+      while (!pw.isFinished()) {\n+        IOUtils.copy(output, consoleOutput);\n+        IOUtils.copy(error, consoleOutput);\n+      }\n+      if (pw.rc != 0) {\n+        String msg = \"The Beam Apex runner in non-embedded mode requires the Hadoop client\"\n+            + \" to be installed on the machine from which you launch the job\"\n+            + \" and the 'hadoop' script in $PATH\";\n+        LOG.error(msg);\n+        throw new RuntimeException(\"Failed to run: \" + cmd + \" (exit code \" + pw.rc + \")\" + \"\\n\"\n+            + consoleOutput.toString());\n+      }\n+    }\n+    return new AppHandle() {\n+      @Override\n+      public boolean isFinished() {\n+        // TODO (future PR): interaction with child process\n+        LOG.warn(\"YARN application runs asynchronously and status check not implemented.\");\n+        return true;\n+      }\n+      @Override\n+      public void shutdown(ShutdownMode arg0) throws LauncherException {\n+        // TODO (future PR): interaction with child process\n+        throw new UnsupportedOperationException();\n+      }\n+    };\n+  }\n+\n+  /**\n+   * From the current classpath, find the jar files that need to be deployed\n+   * with the application to run on YARN. Hadoop dependencies are provided\n+   * through the Hadoop installation and the application should not bundle them\n+   * to avoid conflicts. This is done by removing the Hadoop compile\n+   * dependencies (transitively) by parsing the Maven dependency tree.\n+   *\n+   * @return list of jar files to ship\n+   * @throws IOException when dependency information cannot be read\n+   */\n+  public static List<File> getYarnDeployDependencies() throws IOException {\n+    InputStream dependencyTree = ApexRunner.class.getResourceAsStream(\"dependency-tree\");\n+    BufferedReader br = new BufferedReader(new InputStreamReader(dependencyTree));\n+    String line = null;\n+    List<String> excludes = new ArrayList<>();\n+    int excludeLevel = Integer.MAX_VALUE;\n+    while ((line = br.readLine()) != null) {\n+      for (int i = 0; i < line.length(); i++) {\n+        char c = line.charAt(i);\n+        if (Character.isLetter(c)) {\n+          if (i > excludeLevel) {\n+            excludes.add(line.substring(i));\n+          } else {\n+            if (line.substring(i).startsWith(\"org.apache.hadoop\")) {\n+              excludeLevel = i;\n+              excludes.add(line.substring(i));\n+            } else {\n+              excludeLevel = Integer.MAX_VALUE;\n+            }\n+          }\n+          break;\n+        }\n+      }\n+    }\n+    br.close();\n+\n+    Set<String> excludeJarFileNames = Sets.newHashSet();\n+    for (String exclude : excludes) {\n+      String[] mvnc = exclude.split(\":\");\n+      String fileName = mvnc[1] + \"-\";\n+      if (mvnc.length == 6) {\n+        fileName += mvnc[4] + \"-\" + mvnc[3]; // with classifier\n+      } else {\n+        fileName += mvnc[3];\n+      }\n+      fileName += \".jar\";\n+      excludeJarFileNames.add(fileName);\n+    }\n+\n+    ClassLoader classLoader = ApexYarnLauncher.class.getClassLoader();\n+    URL[] urls = ((URLClassLoader) classLoader).getURLs();\n+    List<File> dependencyJars = new ArrayList<>();\n+    for (int i = 0; i < urls.length; i++) {\n+      File f = new File(urls[i].getFile());\n+      // dependencies can also be directories in the build reactor,\n+      // the Apex client will automatically create jar files for those.\n+      if (f.exists() && !excludeJarFileNames.contains(f.getName())) {\n+          dependencyJars.add(f);\n+      }\n+    }\n+    return dependencyJars;\n+  }\n+\n+  /**\n+   * Create a jar file from the given directory.\n+   * @param dir source directory\n+   * @param jarFile jar file name\n+   * @throws IOException when file cannot be created\n+   */\n+  public static void createJar(File dir, File jarFile) throws IOException {\n+\n+    final Map<String, ?> env = Collections.singletonMap(\"create\", \"true\");\n+    if (jarFile.exists() && !jarFile.delete()) {\n+      throw new RuntimeException(\"Failed to remove \" + jarFile);\n+    }\n+    URI uri = URI.create(\"jar:\" + jarFile.toURI());\n+    try (final FileSystem zipfs = FileSystems.newFileSystem(uri, env);) {\n+\n+      File manifestFile = new File(dir, JarFile.MANIFEST_NAME);\n+      Files.createDirectory(zipfs.getPath(\"META-INF\"));\n+      final OutputStream out = Files.newOutputStream(zipfs.getPath(JarFile.MANIFEST_NAME));\n+      if (!manifestFile.exists()) {\n+        new Manifest().write(out);\n+      } else {\n+        FileUtils.copyFile(manifestFile, out);\n+      }\n+      out.close();\n+\n+      final java.nio.file.Path root = dir.toPath();\n+      Files.walkFileTree(root, new java.nio.file.SimpleFileVisitor<Path>() {\n+        String relativePath;\n+\n+        @Override\n+        public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs)\n+            throws IOException {\n+          relativePath = root.relativize(dir).toString();\n+          if (!relativePath.isEmpty()) {\n+            if (!relativePath.endsWith(\"/\")) {\n+              relativePath += \"/\";\n+            }\n+            final Path dstDir = zipfs.getPath(relativePath);\n+            Files.createDirectory(dstDir);\n+          }\n+          return super.preVisitDirectory(dir, attrs);\n+        }\n+\n+        @Override\n+        public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n+          String name = relativePath + file.getFileName();\n+          if (!JarFile.MANIFEST_NAME.equals(name)) {\n+            final OutputStream out = Files.newOutputStream(zipfs.getPath(name));\n+            FileUtils.copyFile(file.toFile(), out);\n+            out.close();\n+          }\n+          return super.visitFile(file, attrs);\n+        }\n+\n+        @Override\n+        public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException {\n+          relativePath = root.relativize(dir.getParent()).toString();\n+          if (!relativePath.isEmpty() && !relativePath.endsWith(\"/\")) {\n+            relativePath += \"/\";\n+          }\n+          return super.postVisitDirectory(dir, exc);\n+        }\n+      });\n+    }\n+  }\n+\n+  /**\n+   * The main method expects the serialized DAG and will launch the YARN application.\n+   * @param args location of launch parameters\n+   * @throws IOException when parameters cannot be read\n+   */\n+  public static void main(String[] args) throws IOException {\n+    checkArgument(args.length == 1, \"exactly one argument expected\");\n+    File file = new File(args[0]);\n+    checkArgument(file.exists() && file.isFile(), \"invalid file path %s\", file);\n+    final LaunchParams params = (LaunchParams) SerializationUtils.deserialize(\n+        new FileInputStream(file));\n+    StreamingApplication apexApp = new StreamingApplication() {\n+      @Override\n+      public void populateDAG(DAG dag, Configuration conf) {\n+        copyShallow(params.dag, dag);\n+      }\n+    };\n+    Configuration conf = new Configuration(); // configuration from Hadoop client\n+    AppHandle appHandle = params.getApexLauncher().launchApp(apexApp, conf,\n+        params.launchAttributes);\n+    if (appHandle == null) {\n+      throw new AssertionError(\"Launch returns null handle.\");\n+    }\n+    // TODO (future PR)\n+    // At this point the application is running, but this process should remain active to\n+    // allow the parent to implement the runner result.\n+  }\n+\n+  /**\n+   * Launch parameters that will be serialized and passed to the child process.\n+   */\n+  @VisibleForTesting\n+  protected static class LaunchParams implements Serializable {\n+    private static final long serialVersionUID = 1L;\n+    private final DAG dag;\n+    private final Attribute.AttributeMap launchAttributes;\n+    private HashMap<String, String> env;\n+    private String cmd;\n+\n+    protected LaunchParams(DAG dag, AttributeMap launchAttributes) {\n+      this.dag = dag;\n+      this.launchAttributes = launchAttributes;\n+    }\n+\n+    protected Launcher<?> getApexLauncher() {\n+      return Launcher.getLauncher(LaunchMode.YARN);\n+    }\n+\n+    protected String getCmd() {\n+      return cmd;\n+    }\n+\n+    protected Map<String, String> getEnv() {\n+      return env;\n+    }\n+\n+  }\n+\n+  private static void copyShallow(DAG from, DAG to) {\n+    checkArgument(from.getClass() == to.getClass(), \"must be same class %s %s\",\n+        from.getClass(), to.getClass());\n+    Field[] fields = from.getClass().getDeclaredFields();\n+    AccessibleObject.setAccessible(fields, true);\n+    for (int i = 0; i < fields.length; i++) {\n+      Field field = fields[i];\n+      if (!java.lang.reflect.Modifier.isStatic(field.getModifiers())) {\n+        try {\n+          field.set(to,  field.get(from));\n+        } catch (IllegalArgumentException | IllegalAccessException e) {\n+          throw new RuntimeException(e);\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Starts a command and waits for it to complete.\n+   */\n+  public static class ProcessWatcher implements Runnable {\n+    private final Process p;\n+    private volatile boolean finished = false;\n+    private volatile int rc;\n+\n+    public ProcessWatcher(Process p) {\n+      this.p = p;\n+      new Thread(this).start();\n+    }\n+\n+    public boolean isFinished() {\n+      return finished;\n+    }\n+\n+    @Override\n+    public void run() {\n+      try {\n+        rc = p.waitFor();\n+      } catch (Exception e) {\n+        // ignore\n+      }\n+      finished = true;\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/ApexYarnLauncher.java",
                "sha": "0ae4cc7dee9097ab60ef59371d87328d37f26f52",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View.CreatePCollectionView;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PValue;\n import org.slf4j.Logger;\n@@ -70,6 +71,7 @@\n         new CreateApexPCollectionViewTranslator());\n     registerTransformTranslator(CreatePCollectionView.class,\n         new CreatePCollectionViewTranslator());\n+    registerTransformTranslator(Window.Bound.class, new WindowBoundTranslator());\n   }\n \n   public ApexPipelineTranslator(ApexPipelineOptions options) {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ApexPipelineTranslator.java",
                "sha": "c8e02908274c187a06b4815005ee60b870fdf91c",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundMultiTranslator.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundMultiTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundMultiTranslator.java",
                "patch": "@@ -31,8 +31,8 @@\n import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n import org.apache.beam.sdk.util.WindowedValue.WindowedValueCoder;\n@@ -53,8 +53,10 @@\n \n   @Override\n   public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationContext context) {\n-    DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-    if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n+    DoFn<InputT, OutputT> doFn = transform.getFn();\n+    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n+\n+    if (signature.stateDeclarations().size() > 0) {\n       throw new UnsupportedOperationException(\n           String.format(\n               \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n@@ -63,7 +65,17 @@ public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationCo\n               DoFn.class.getSimpleName(),\n               ApexRunner.class.getSimpleName()));\n     }\n-    OldDoFn<InputT, OutputT> oldDoFn = transform.getFn();\n+\n+    if (signature.timerDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n+              DoFn.TimerId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              ApexRunner.class.getSimpleName()));\n+    }\n+\n     PCollectionTuple output = context.getOutput();\n     PCollection<InputT> input = context.getInput();\n     List<PCollectionView<?>> sideInputs = transform.getSideInputs();\n@@ -75,7 +87,7 @@ public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationCo\n     ApexParDoOperator<InputT, OutputT> operator =\n         new ApexParDoOperator<>(\n             context.getPipelineOptions(),\n-            oldDoFn,\n+            doFn,\n             transform.getMainOutputTag(),\n             transform.getSideOutputTags().getAll(),\n             context.<PCollection<?>>getInput().getWindowingStrategy(),",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundMultiTranslator.java",
                "sha": "bff76528a2d676207366d34bc9dc5bec151622f7",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "patch": "@@ -23,8 +23,8 @@\n import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n import org.apache.beam.sdk.util.WindowedValue.WindowedValueCoder;\n@@ -40,8 +40,10 @@\n \n   @Override\n   public void translate(ParDo.Bound<InputT, OutputT> transform, TranslationContext context) {\n-    DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-    if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n+    DoFn<InputT, OutputT> doFn = transform.getFn();\n+    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n+\n+    if (signature.stateDeclarations().size() > 0) {\n       throw new UnsupportedOperationException(\n           String.format(\n               \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n@@ -50,7 +52,17 @@ public void translate(ParDo.Bound<InputT, OutputT> transform, TranslationContext\n               DoFn.class.getSimpleName(),\n               ApexRunner.class.getSimpleName()));\n     }\n-    OldDoFn<InputT, OutputT> oldDoFn = transform.getOldFn();\n+\n+    if (signature.timerDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n+              DoFn.TimerId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              ApexRunner.class.getSimpleName()));\n+    }\n+\n     PCollection<OutputT> output = context.getOutput();\n     PCollection<InputT> input = context.getInput();\n     List<PCollectionView<?>> sideInputs = transform.getSideInputs();\n@@ -62,7 +74,7 @@ public void translate(ParDo.Bound<InputT, OutputT> transform, TranslationContext\n     ApexParDoOperator<InputT, OutputT> operator =\n         new ApexParDoOperator<>(\n             context.getPipelineOptions(),\n-            oldDoFn,\n+            doFn,\n             new TupleTag<OutputT>(),\n             TupleTagList.empty().getAll() /*sideOutputTags*/,\n             output.getWindowingStrategy(),",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslator.java",
                "sha": "3b6eb6ea34e83d2f7b15d85837c61eadc2ee2864",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n-import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n import org.apache.beam.sdk.util.state.StateInternalsFactory;\n import org.apache.beam.sdk.values.PCollection;\n@@ -72,8 +71,7 @@ public void addView(PCollectionView<?> view) {\n   }\n \n   public void setCurrentTransform(TransformHierarchy.Node treeNode) {\n-    this.currentTransform = AppliedPTransform.of(treeNode.getFullName(),\n-        treeNode.getInput(), treeNode.getOutput(), (PTransform) treeNode.getTransform());\n+    this.currentTransform = treeNode.toAppliedPTransform();\n   }\n \n   public ApexPipelineOptions getPipelineOptions() {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/TranslationContext.java",
                "sha": "3bf01a87dc4d3c8d3fbf9d0278289273964b7aa3",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowBoundTranslator.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowBoundTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowBoundTranslator.java",
                "patch": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.apex.translation;\n+\n+import java.util.Collections;\n+import org.apache.beam.runners.apex.ApexPipelineOptions;\n+import org.apache.beam.runners.apex.translation.operators.ApexParDoOperator;\n+import org.apache.beam.runners.core.AssignWindowsDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFnAdapters;\n+import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.apache.beam.sdk.values.TupleTagList;\n+\n+/**\n+ * {@link Window.Bound} is translated to {link ApexParDoOperator} that wraps an {@link\n+ * AssignWindowsDoFn}.\n+ */\n+class WindowBoundTranslator<T> implements TransformTranslator<Window.Bound<T>> {\n+  private static final long serialVersionUID = 1L;\n+\n+  @Override\n+  public void translate(Window.Bound<T> transform, TranslationContext context) {\n+    PCollection<T> output = context.getOutput();\n+    PCollection<T> input = context.getInput();\n+    @SuppressWarnings(\"unchecked\")\n+    WindowingStrategy<T, BoundedWindow> windowingStrategy =\n+        (WindowingStrategy<T, BoundedWindow>) output.getWindowingStrategy();\n+\n+    OldDoFn<T, T> fn =\n+        (transform.getWindowFn() == null)\n+            ? DoFnAdapters.toOldDoFn(new IdentityFn<T>())\n+            : new AssignWindowsDoFn<>(transform.getWindowFn());\n+\n+    ApexParDoOperator<T, T> operator =\n+        new ApexParDoOperator<T, T>(\n+            context.getPipelineOptions().as(ApexPipelineOptions.class),\n+            fn,\n+            new TupleTag<T>(),\n+            TupleTagList.empty().getAll(),\n+            windowingStrategy,\n+            Collections.<PCollectionView<?>>emptyList(),\n+            WindowedValue.getFullCoder(\n+                input.getCoder(), windowingStrategy.getWindowFn().windowCoder()),\n+            context.<Void>stateInternalsFactory());\n+    context.addOperator(operator, operator.output);\n+    context.addStream(context.getInput(), operator.input);\n+  }\n+\n+  private static class IdentityFn<T> extends DoFn<T, T> {\n+    @ProcessElement\n+    public void processElement(ProcessContext c) {\n+      c.output(c.element());\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/WindowBoundTranslator.java",
                "sha": "33b9269f9c4341ed88841a1941e15387620c0b70",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "patch": "@@ -42,6 +42,8 @@\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.SerializablePipelineOptions;\n import org.apache.beam.runners.core.GroupAlsoByWindowViaWindowSetDoFn;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.core.SystemReduceFn;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderException;\n@@ -54,8 +56,6 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.util.CoderUtils;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowedValue;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexGroupByKeyOperator.java",
                "sha": "48ac177ead7ba066c6995df7409d91668fcc4a7f",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "patch": "@@ -28,17 +28,16 @@\n import com.google.common.base.Throwables;\n import com.google.common.collect.Iterables;\n import com.google.common.collect.Maps;\n-\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n-\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner;\n import org.apache.beam.runners.apex.translation.utils.ApexStreamTuple;\n import org.apache.beam.runners.apex.translation.utils.NoOpStepContext;\n import org.apache.beam.runners.apex.translation.utils.SerializablePipelineOptions;\n import org.apache.beam.runners.apex.translation.utils.ValueAndCoderKryoSerializable;\n+import org.apache.beam.runners.core.AggregatorFactory;\n import org.apache.beam.runners.core.DoFnRunner;\n import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n@@ -47,9 +46,9 @@\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.ListCoder;\n import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Aggregator.AggregatorFactory;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFnAdapters;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.util.ExecutionContext;\n import org.apache.beam.sdk.util.NullSideInputReader;\n@@ -95,6 +94,7 @@\n   private transient Map<TupleTag<?>, DefaultOutputPort<ApexStreamTuple<?>>> sideOutputPortMapping =\n       Maps.newHashMapWithExpectedSize(5);\n \n+  @Deprecated\n   public ApexParDoOperator(\n       ApexPipelineOptions pipelineOptions,\n       OldDoFn<InputT, OutputT> doFn,\n@@ -125,6 +125,27 @@ public ApexParDoOperator(\n \n   }\n \n+  public ApexParDoOperator(\n+      ApexPipelineOptions pipelineOptions,\n+      DoFn<InputT, OutputT> doFn,\n+      TupleTag<OutputT> mainOutputTag,\n+      List<TupleTag<?>> sideOutputTags,\n+      WindowingStrategy<?, ?> windowingStrategy,\n+      List<PCollectionView<?>> sideInputs,\n+      Coder<WindowedValue<InputT>> inputCoder,\n+      StateInternalsFactory<Void> stateInternalsFactory\n+      ) {\n+    this(\n+        pipelineOptions,\n+        DoFnAdapters.toOldDoFn(doFn),\n+        mainOutputTag,\n+        sideOutputTags,\n+        windowingStrategy,\n+        sideInputs,\n+        inputCoder,\n+        stateInternalsFactory);\n+  }\n+\n   @SuppressWarnings(\"unused\") // for Kryo\n   private ApexParDoOperator() {\n     this.pipelineOptions = null;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/main/java/org/apache/beam/runners/apex/translation/operators/ApexParDoOperator.java",
                "sha": "a3d3a97e2ce04c14049f72b5ecfe94a143d341b0",
                "status": "modified"
            },
            {
                "additions": 138,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/ApexYarnLauncherTest.java",
                "changes": 138,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/ApexYarnLauncherTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/ApexYarnLauncherTest.java",
                "patch": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.apex;\n+\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.not;\n+import static org.junit.Assert.assertThat;\n+\n+import com.datatorrent.api.Attribute;\n+import com.datatorrent.api.Attribute.AttributeMap;\n+import com.datatorrent.api.Context.DAGContext;\n+import com.datatorrent.api.DAG;\n+import com.datatorrent.api.StreamingApplication;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.Files;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.jar.JarFile;\n+\n+import org.apache.apex.api.EmbeddedAppLauncher;\n+import org.apache.apex.api.Launcher;\n+import org.apache.apex.api.Launcher.AppHandle;\n+import org.apache.apex.api.Launcher.LaunchMode;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Test for dependency resolution for pipeline execution on YARN.\n+ */\n+public class ApexYarnLauncherTest {\n+\n+  @Test\n+  public void testGetYarnDeployDependencies() throws Exception {\n+    List<File> deps = ApexYarnLauncher.getYarnDeployDependencies();\n+    String depsToString = deps.toString();\n+    // the beam dependencies are not present as jar when running within the Maven build reactor\n+    //assertThat(depsToString, containsString(\"beam-runners-core-\"));\n+    //assertThat(depsToString, containsString(\"beam-runners-apex-\"));\n+    assertThat(depsToString, containsString(\"apex-common-\"));\n+    assertThat(depsToString, not(containsString(\"hadoop-\")));\n+    assertThat(depsToString, not(containsString(\"zookeeper-\")));\n+  }\n+\n+  @Test\n+  public void testProxyLauncher() throws Exception {\n+    // use the embedded launcher to build the DAG only\n+    EmbeddedAppLauncher<?> embeddedLauncher = Launcher.getLauncher(LaunchMode.EMBEDDED);\n+\n+    StreamingApplication app = new StreamingApplication() {\n+      @Override\n+      public void populateDAG(DAG dag, Configuration conf) {\n+        dag.setAttribute(DAGContext.APPLICATION_NAME, \"DummyApp\");\n+      }\n+    };\n+\n+    Configuration conf = new Configuration(false);\n+    DAG dag = embeddedLauncher.prepareDAG(app, conf);\n+    Attribute.AttributeMap launchAttributes = new Attribute.AttributeMap.DefaultAttributeMap();\n+    ApexYarnLauncher launcher = new ApexYarnLauncher();\n+    launcher.launchApp(new MockApexYarnLauncherParams(dag, launchAttributes));\n+  }\n+\n+  private static class MockApexYarnLauncherParams extends  ApexYarnLauncher.LaunchParams {\n+    private static final long serialVersionUID = 1L;\n+\n+    public MockApexYarnLauncherParams(DAG dag, AttributeMap launchAttributes) {\n+      super(dag, launchAttributes);\n+    }\n+\n+    @Override\n+    protected Launcher<?> getApexLauncher() {\n+      return new Launcher<AppHandle>() {\n+        @Override\n+        public AppHandle launchApp(StreamingApplication application,\n+            Configuration configuration, AttributeMap launchParameters)\n+            throws org.apache.apex.api.Launcher.LauncherException {\n+          EmbeddedAppLauncher<?> embeddedLauncher = Launcher.getLauncher(LaunchMode.EMBEDDED);\n+          DAG dag = embeddedLauncher.getDAG();\n+          application.populateDAG(dag, new Configuration(false));\n+          String appName = dag.getValue(DAGContext.APPLICATION_NAME);\n+          Assert.assertEquals(\"DummyApp\", appName);\n+          return new AppHandle() {\n+            @Override\n+            public boolean isFinished() {\n+              return true;\n+            }\n+            @Override\n+            public void shutdown(org.apache.apex.api.Launcher.ShutdownMode arg0) {\n+            }\n+          };\n+        }\n+      };\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testCreateJar() throws Exception {\n+    File baseDir = new File(\"./target/testCreateJar\");\n+    File srcDir = new File(baseDir, \"src\");\n+    String file1 = \"file1\";\n+    FileUtils.forceMkdir(srcDir);\n+    FileUtils.write(new File(srcDir, file1), \"file1\");\n+\n+    File jarFile = new File(baseDir, \"test.jar\");\n+    ApexYarnLauncher.createJar(srcDir, jarFile);\n+    Assert.assertTrue(\"exists: \" + jarFile, jarFile.exists());\n+    URI uri = URI.create(\"jar:\" + jarFile.toURI());\n+    final Map<String, ?> env = Collections.singletonMap(\"create\", \"true\");\n+    try (final FileSystem zipfs = FileSystems.newFileSystem(uri, env);) {\n+      Assert.assertTrue(\"manifest\", Files.isRegularFile(zipfs.getPath(JarFile.MANIFEST_NAME)));\n+      Assert.assertTrue(\"file1\", Files.isRegularFile(zipfs.getPath(file1)));\n+    }\n+\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/ApexYarnLauncherTest.java",
                "sha": "986818bf8f1abeac8491820ec07d39cfa13bb301",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "patch": "@@ -19,21 +19,20 @@\n package org.apache.beam.runners.apex.translation;\n \n import com.google.common.collect.Sets;\n-\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Set;\n-\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner;\n import org.apache.beam.runners.apex.ApexRunnerResult;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n@@ -83,14 +82,10 @@ public void test() throws Exception {\n     Assert.assertEquals(expected, Sets.newHashSet(EmbeddedCollector.RESULTS));\n   }\n \n-  @SuppressWarnings(\"serial\")\n-  private static class EmbeddedCollector extends OldDoFn<Object, Void> {\n-    protected static final ArrayList<Object> RESULTS = new ArrayList<>();\n-\n-    public EmbeddedCollector() {\n-    }\n+  private static class EmbeddedCollector extends DoFn<Object, Void> {\n+    private static final List<Object> RESULTS = Collections.synchronizedList(new ArrayList<>());\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       RESULTS.add(c.element());\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/FlattenPCollectionTranslatorTest.java",
                "sha": "f5abc34a6eda11971624bc6d0976dd404270f5fc",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 14,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "patch": "@@ -20,17 +20,15 @@\n \n import com.google.common.collect.Lists;\n import com.google.common.collect.Sets;\n-\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.Collections;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.NoSuchElementException;\n-\n+import java.util.Set;\n import javax.annotation.Nullable;\n-\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner;\n import org.apache.beam.runners.apex.ApexRunnerResult;\n@@ -42,7 +40,7 @@\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n@@ -106,22 +104,17 @@ public void test() throws Exception {\n \n   }\n \n-  @SuppressWarnings(\"serial\")\n-  private static class EmbeddedCollector extends OldDoFn<Object, Void> {\n-    protected static final HashSet<Object> RESULTS = new HashSet<>();\n-\n-    public EmbeddedCollector() {\n-    }\n+  private static class EmbeddedCollector extends DoFn<Object, Void> {\n+    private static final Set<Object> RESULTS = Collections.synchronizedSet(new HashSet<>());\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       RESULTS.add(c.element());\n     }\n   }\n \n-  private static class KeyedByTimestamp<T> extends OldDoFn<T, KV<Instant, T>> {\n-\n-    @Override\n+  private static class KeyedByTimestamp<T> extends DoFn<T, KV<Instant, T>> {\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       c.output(KV.of(c.timestamp(), c.element()));\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/GroupByKeyTranslatorTest.java",
                "sha": "96963a0f6073b389321b0f0706df19a8b2587ce8",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslatorTest.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslatorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 19,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslatorTest.java",
                "patch": "@@ -26,14 +26,13 @@\n import com.datatorrent.lib.util.KryoCloneUtils;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Sets;\n-\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Set;\n import java.util.regex.Pattern;\n-\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner;\n import org.apache.beam.runners.apex.ApexRunnerResult;\n@@ -49,7 +48,7 @@\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.testing.PAssert;\n import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.View;\n@@ -113,8 +112,7 @@ public void test() throws Exception {\n     Assert.assertEquals(Sets.newHashSet(expected), EmbeddedCollector.RESULTS);\n   }\n \n-  @SuppressWarnings(\"serial\")\n-  private static class Add extends OldDoFn<Integer, Integer> {\n+  private static class Add extends DoFn<Integer, Integer> {\n     private Integer number;\n     private PCollectionView<Integer> sideInputView;\n \n@@ -126,7 +124,7 @@ private Add(PCollectionView<Integer> sideInputView) {\n       this.sideInputView = sideInputView;\n     }\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       if (sideInputView != null) {\n         number = c.sideInput(sideInputView);\n@@ -135,15 +133,14 @@ public void processElement(ProcessContext c) throws Exception {\n     }\n   }\n \n-  private static class EmbeddedCollector extends OldDoFn<Object, Void> {\n-    private static final long serialVersionUID = 1L;\n-    protected static final HashSet<Object> RESULTS = new HashSet<>();\n+  private static class EmbeddedCollector extends DoFn<Object, Void> {\n+    private static final Set<Object> RESULTS = Collections.synchronizedSet(new HashSet<>());\n \n     public EmbeddedCollector() {\n       RESULTS.clear();\n     }\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       RESULTS.add(c.element());\n     }\n@@ -207,13 +204,16 @@ public void testSerialization() throws Exception {\n     PCollectionView<Integer> singletonView = pipeline.apply(Create.of(1))\n             .apply(Sum.integersGlobally().asSingletonView());\n \n-    ApexParDoOperator<Integer, Integer> operator = new ApexParDoOperator<>(options,\n-        new Add(singletonView), new TupleTag<Integer>(), TupleTagList.empty().getAll(),\n-        WindowingStrategy.globalDefault(),\n-        Collections.<PCollectionView<?>>singletonList(singletonView),\n-        coder,\n-        new ApexStateInternals.ApexStateInternalsFactory<Void>()\n-        );\n+    ApexParDoOperator<Integer, Integer> operator =\n+        new ApexParDoOperator<>(\n+            options,\n+            new Add(singletonView),\n+            new TupleTag<Integer>(),\n+            TupleTagList.empty().getAll(),\n+            WindowingStrategy.globalDefault(),\n+            Collections.<PCollectionView<?>>singletonList(singletonView),\n+            coder,\n+            new ApexStateInternals.ApexStateInternalsFactory<Void>());\n     operator.setup(null);\n     operator.beginWindow(0);\n     WindowedValue<Integer> wv1 = WindowedValue.valueInGlobalWindow(1);\n@@ -303,7 +303,7 @@ public void testMultiOutputParDoWithSideInputs() throws Exception {\n      Assert.assertEquals(Sets.newHashSet(expected), EmbeddedCollector.RESULTS);\n   }\n \n-  private static class TestMultiOutputWithSideInputsFn extends OldDoFn<Integer, String> {\n+  private static class TestMultiOutputWithSideInputsFn extends DoFn<Integer, String> {\n     private static final long serialVersionUID = 1L;\n \n     final List<PCollectionView<Integer>> sideInputViews = new ArrayList<>();\n@@ -315,7 +315,7 @@ public TestMultiOutputWithSideInputsFn(List<PCollectionView<Integer>> sideInputV\n       this.sideOutputTupleTags.addAll(sideOutputTupleTags);\n     }\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       outputToAllWithSideInputs(c, \"processing: \" + c.element());\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ParDoBoundTranslatorTest.java",
                "sha": "fa94b2a442e997698cfb6279170e27bf60ffad96",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ReadUnboundTranslatorTest.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ReadUnboundTranslatorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ReadUnboundTranslatorTest.java",
                "patch": "@@ -24,11 +24,10 @@\n import com.google.common.collect.Lists;\n import com.google.common.collect.Range;\n import com.google.common.collect.Sets;\n-\n+import java.util.Collections;\n import java.util.HashSet;\n import java.util.List;\n import java.util.Set;\n-\n import org.apache.beam.runners.apex.ApexPipelineOptions;\n import org.apache.beam.runners.apex.ApexRunner;\n import org.apache.beam.runners.apex.ApexRunnerResult;\n@@ -39,7 +38,7 @@\n import org.apache.beam.sdk.io.CountingSource;\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -113,14 +112,10 @@ public void testReadBounded() throws Exception {\n     Assert.assertEquals(Sets.newHashSet(expected), EmbeddedCollector.RESULTS);\n   }\n \n-  @SuppressWarnings(\"serial\")\n-  private static class EmbeddedCollector extends OldDoFn<Object, Void> {\n-    protected static final HashSet<Object> RESULTS = new HashSet<>();\n-\n-    public EmbeddedCollector() {\n-    }\n+  private static class EmbeddedCollector extends DoFn<Object, Void> {\n+    private static final Set<Object> RESULTS = Collections.synchronizedSet(new HashSet<>());\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       RESULTS.add(c.element());\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/apex/src/test/java/org/apache/beam/runners/apex/translation/ReadUnboundTranslatorTest.java",
                "sha": "8e44bab18b0e51274cba67e3efd3f64d2978ea47",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/pom.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/core-java/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -90,11 +90,11 @@\n                      the second relocation. -->\n                 <relocation>\n                   <pattern>com.google.common</pattern>\n-                  <shadedPattern>org.apache.beam.sdk.repackaged.com.google.common</shadedPattern>\n+                  <shadedPattern>org.apache.beam.runners.core.repackaged.com.google.common</shadedPattern>\n                 </relocation>\n                 <relocation>\n                   <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>org.apache.beam.sdk.repackaged.com.google.thirdparty</shadedPattern>\n+                  <shadedPattern>org.apache.beam.runners.core.repackaged.com.google.thirdparty</shadedPattern>\n                 </relocation>\n               </relocations>\n             </configuration>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/pom.xml",
                "sha": "704aeaf91f06893127b6fcbd4acc12d6e4bf3f14",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/ActiveWindowSet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ActiveWindowSet.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ActiveWindowSet.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import com.google.common.annotations.VisibleForTesting;\n import java.util.Collection;",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/ActiveWindowSet.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/ActiveWindowSet.java",
                "sha": "79d1f3f9927e21f609b791a982048b8da429b7dd",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/b6e7bb659f33e346c00e66ca96e3c54dd7ef07da/runners/core-java/src/main/java/org/apache/beam/runners/core/AssignWindows.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/AssignWindows.java?ref=b6e7bb659f33e346c00e66ca96e3c54dd7ef07da",
                "deletions": 46,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/AssignWindows.java",
                "patch": "@@ -1,46 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.beam.runners.core;\n-\n-import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.values.PCollection;\n-\n-/**\n- * {@link PTransform} that uses privileged (non-user-facing) APIs to assign elements of a\n- * {@link PCollection} to windows according to the provided {@link WindowFn}.\n- *\n- * @param <T> Type of elements being windowed\n- * @param <W> Window type\n- */\n-public class AssignWindows<T, W extends BoundedWindow>\n-    extends PTransform<PCollection<T>, PCollection<T>> {\n-\n-  private WindowFn<? super T, W> fn;\n-\n-  public AssignWindows(WindowFn<? super T, W> fn) {\n-    this.fn = fn;\n-  }\n-\n-  @Override\n-  public PCollection<T> apply(PCollection<T> input) {\n-    return input.apply(\"AssignWindows\", ParDo.of(new AssignWindowsDoFn<>(fn)));\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/b6e7bb659f33e346c00e66ca96e3c54dd7ef07da/runners/core-java/src/main/java/org/apache/beam/runners/core/AssignWindows.java",
                "sha": "f2387f57ba393710a2a74221c5efe8294ec8f1ab",
                "status": "removed"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 8,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java",
                "patch": "@@ -18,30 +18,38 @@\n package org.apache.beam.runners.core;\n \n import org.apache.beam.sdk.transforms.Aggregator;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.OldDoFn;\n-import org.apache.beam.sdk.transforms.OldDoFn.ProcessContext;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.KV;\n+import org.joda.time.Instant;\n \n /**\n- * An wrapper interface that represents the execution of a {@link OldDoFn}.\n+ * An wrapper interface that represents the execution of a {@link DoFn}.\n  */\n public interface DoFnRunner<InputT, OutputT> {\n   /**\n-   * Prepares and calls {@link OldDoFn#startBundle}.\n+   * Prepares and calls a {@link DoFn DoFn's} {@link DoFn.StartBundle @StartBundle} method.\n    */\n   void startBundle();\n \n   /**\n-   * Calls {@link OldDoFn#processElement} with a {@link ProcessContext} containing the current\n-   * element.\n+   * Calls a {@link DoFn DoFn's} {@link DoFn.ProcessElement @ProcessElement} method with a\n+   * {@link DoFn.ProcessContext} containing the provided element.\n    */\n   void processElement(WindowedValue<InputT> elem);\n \n   /**\n-   * Calls {@link OldDoFn#finishBundle} and performs additional tasks, such as\n-   * flushing in-memory states.\n+   * Calls a {@link DoFn DoFn's} {@link DoFn.OnTimer @OnTimer} method for the given timer\n+   * in the given window.\n+   */\n+  void onTimer(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain);\n+\n+  /**\n+   * Calls a {@link DoFn DoFn's} {@link DoFn.FinishBundle @FinishBundle} method and performs\n+   * additional tasks, such as flushing in-memory states.\n    */\n   void finishBundle();\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java",
                "sha": "7c73a349155695a7ac2452e92d343093170ae865",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "patch": "@@ -22,12 +22,10 @@\n import org.apache.beam.runners.core.GroupByKeyViaGroupByKeyOnly.GroupAlsoByWindow;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Aggregator.AggregatorFactory;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.ExecutionContext.StepContext;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunners.java",
                "sha": "0e4bf75ba0b442ce7d67a8620084df2b0358fc18",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 8,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "patch": "@@ -24,10 +24,8 @@\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n import org.apache.beam.sdk.util.TimerInternals;\n-import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.state.StateInternals;\n import org.apache.beam.sdk.util.state.StateInternalsFactory;\n@@ -73,9 +71,9 @@ private GroupAlsoByWindowViaWindowSetDoFn(\n \n   @Override\n   public void processElement(ProcessContext c) throws Exception {\n-    KeyedWorkItem<K, InputT> element = c.element();\n+    KeyedWorkItem<K, InputT> keyedWorkItem = c.element();\n \n-    K key = c.element().key();\n+    K key = keyedWorkItem.key();\n     TimerInternals timerInternals = c.windowingInternals().timerInternals();\n     StateInternals<K> stateInternals = stateInternalsFactory.stateInternalsForKey(key);\n \n@@ -93,10 +91,8 @@ public void processElement(ProcessContext c) throws Exception {\n             reduceFn,\n             c.getPipelineOptions());\n \n-    reduceFnRunner.processElements(element.elementsIterable());\n-    for (TimerData timer : element.timersIterable()) {\n-      reduceFnRunner.onTimer(timer);\n-    }\n+    reduceFnRunner.processElements(keyedWorkItem.elementsIterable());\n+    reduceFnRunner.onTimers(keyedWorkItem.timersIterable());\n     reduceFnRunner.persist();\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowViaWindowSetDoFn.java",
                "sha": "14171b37a57a2ebc3f58db15e217e533e017095b",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowsViaOutputBufferDoFn.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowsViaOutputBufferDoFn.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowsViaOutputBufferDoFn.java",
                "patch": "@@ -18,17 +18,18 @@\n package org.apache.beam.runners.core;\n \n import com.google.common.collect.Iterables;\n+import java.util.ArrayList;\n import java.util.List;\n import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n+import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.state.InMemoryTimerInternals;\n import org.apache.beam.sdk.util.state.StateInternals;\n import org.apache.beam.sdk.util.state.StateInternalsFactory;\n-import org.apache.beam.sdk.util.state.TimerCallback;\n import org.joda.time.Instant;\n \n /**\n@@ -59,9 +60,8 @@ public void processElement(ProcessContext c) throws Exception {\n     // timer manager from the context because it doesn't exist. So we create one and emulate the\n     // watermark, knowing that we have all data and it is in timestamp order.\n     InMemoryTimerInternals timerInternals = new InMemoryTimerInternals();\n-    timerInternals.advanceProcessingTime(TimerCallback.NO_OP, Instant.now());\n-    timerInternals.advanceSynchronizedProcessingTime(\n-        TimerCallback.NO_OP, BoundedWindow.TIMESTAMP_MAX_VALUE);\n+    timerInternals.advanceProcessingTime(Instant.now());\n+    timerInternals.advanceSynchronizedProcessingTime(Instant.now());\n     StateInternals<K> stateInternals = stateInternalsFactory.stateInternalsForKey(key);\n \n     ReduceFnRunner<K, InputT, OutputT, W> reduceFnRunner =\n@@ -85,22 +85,50 @@ public void processElement(ProcessContext c) throws Exception {\n       reduceFnRunner.processElements(chunk);\n \n       // Then, since elements are sorted by their timestamp, advance the input watermark\n-      // to the first element, and fire any timers that may have been scheduled.\n-      timerInternals.advanceInputWatermark(reduceFnRunner, chunk.iterator().next().getTimestamp());\n+      // to the first element.\n+      timerInternals.advanceInputWatermark(chunk.iterator().next().getTimestamp());\n+      // Advance the processing times.\n+      timerInternals.advanceProcessingTime(Instant.now());\n+      timerInternals.advanceSynchronizedProcessingTime(Instant.now());\n \n-      // Fire any processing timers that need to fire\n-      timerInternals.advanceProcessingTime(reduceFnRunner, Instant.now());\n+      // Fire all the eligible timers.\n+      fireEligibleTimers(timerInternals, reduceFnRunner);\n \n       // Leave the output watermark undefined. Since there's no late data in batch mode\n       // there's really no need to track it as we do for streaming.\n     }\n \n     // Finish any pending windows by advancing the input watermark to infinity.\n-    timerInternals.advanceInputWatermark(reduceFnRunner, BoundedWindow.TIMESTAMP_MAX_VALUE);\n+    timerInternals.advanceInputWatermark(BoundedWindow.TIMESTAMP_MAX_VALUE);\n \n     // Finally, advance the processing time to infinity to fire any timers.\n-    timerInternals.advanceProcessingTime(reduceFnRunner, BoundedWindow.TIMESTAMP_MAX_VALUE);\n+    timerInternals.advanceProcessingTime(BoundedWindow.TIMESTAMP_MAX_VALUE);\n+    timerInternals.advanceSynchronizedProcessingTime(BoundedWindow.TIMESTAMP_MAX_VALUE);\n+\n+    fireEligibleTimers(timerInternals, reduceFnRunner);\n \n     reduceFnRunner.persist();\n   }\n+\n+  private void fireEligibleTimers(InMemoryTimerInternals timerInternals,\n+      ReduceFnRunner<K, InputT, OutputT, W> reduceFnRunner) throws Exception {\n+    List<TimerInternals.TimerData> timers = new ArrayList<>();\n+    while (true) {\n+        TimerInternals.TimerData timer;\n+        while ((timer = timerInternals.removeNextEventTimer()) != null) {\n+          timers.add(timer);\n+        }\n+        while ((timer = timerInternals.removeNextProcessingTimer()) != null) {\n+          timers.add(timer);\n+        }\n+        while ((timer = timerInternals.removeNextSynchronizedProcessingTimer()) != null) {\n+          timers.add(timer);\n+        }\n+        if (timers.isEmpty()) {\n+          break;\n+        }\n+        reduceFnRunner.onTimers(timers);\n+        timers.clear();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupAlsoByWindowsViaOutputBufferDoFn.java",
                "sha": "918919170ec69b0574bb0d4063fda45709fe4427",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupByKeyViaGroupByKeyOnly.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupByKeyViaGroupByKeyOnly.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/GroupByKeyViaGroupByKeyOnly.java",
                "patch": "@@ -77,7 +77,7 @@ public GroupByKeyViaGroupByKeyOnly(GroupByKey<K, V> originalTransform) {\n   }\n \n   @Override\n-  public PCollection<KV<K, Iterable<V>>> apply(PCollection<KV<K, V>> input) {\n+  public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n     WindowingStrategy<?, ?> windowingStrategy = input.getWindowingStrategy();\n \n     return input\n@@ -109,7 +109,7 @@ public GroupByKeyViaGroupByKeyOnly(GroupByKey<K, V> originalTransform) {\n \n     @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n     @Override\n-    public PCollection<KV<K, Iterable<WindowedValue<V>>>> apply(PCollection<KV<K, V>> input) {\n+    public PCollection<KV<K, Iterable<WindowedValue<V>>>> expand(PCollection<KV<K, V>> input) {\n       return PCollection.createPrimitiveOutputInternal(\n           input.getPipeline(), input.getWindowingStrategy(), input.isBounded());\n     }\n@@ -128,7 +128,7 @@ public GroupByKeyViaGroupByKeyOnly(GroupByKey<K, V> originalTransform) {\n           PCollection<KV<K, Iterable<WindowedValue<V>>>>,\n           PCollection<KV<K, Iterable<WindowedValue<V>>>>> {\n     @Override\n-    public PCollection<KV<K, Iterable<WindowedValue<V>>>> apply(\n+    public PCollection<KV<K, Iterable<WindowedValue<V>>>> expand(\n         PCollection<KV<K, Iterable<WindowedValue<V>>>> input) {\n       return input\n           .apply(\n@@ -225,7 +225,7 @@ public GroupAlsoByWindow(WindowingStrategy<?, ?> windowingStrategy) {\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> apply(\n+    public PCollection<KV<K, Iterable<V>>> expand(\n         PCollection<KV<K, Iterable<WindowedValue<V>>>> input) {\n       @SuppressWarnings(\"unchecked\")\n       KvCoder<K, Iterable<WindowedValue<V>>> inputKvCoder =",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/GroupByKeyViaGroupByKeyOnly.java",
                "sha": "694c5ebe056edd4b336b74d3b8d66b6da83d67cf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItem.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItem.java",
                "patch": "@@ -15,9 +15,10 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n+import org.apache.beam.sdk.util.WindowedValue;\n \n /**\n  * Interface that contains all the timers and elements associated with a specific work item.",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/KeyedWorkItem.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItem.java",
                "sha": "c75fc25457721c9a9eb76fd3eb6b875749de743c",
                "status": "renamed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItemCoder.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItemCoder.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItemCoder.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import static com.google.common.base.Preconditions.checkArgument;\n \n@@ -31,8 +31,10 @@\n import org.apache.beam.sdk.coders.IterableCoder;\n import org.apache.beam.sdk.coders.StandardCoder;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.PropertyNames;\n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.TimerInternals.TimerDataCoder;\n+import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowedValue.FullWindowedValueCoder;\n \n /**",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/KeyedWorkItemCoder.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItemCoder.java",
                "sha": "95be04732333d9b8468ef873bd6b534513256d22",
                "status": "renamed"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItems.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItems.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItems.java",
                "patch": "@@ -15,13 +15,14 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import com.google.common.base.MoreObjects;\n import com.google.common.collect.Iterables;\n import java.util.Collections;\n import java.util.Objects;\n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n+import org.apache.beam.sdk.util.WindowedValue;\n \n /**\n  * Static utility methods that provide {@link KeyedWorkItem} implementations.",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/KeyedWorkItems.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/KeyedWorkItems.java",
                "sha": "94c3bb63003a94c23eb3a2580bfe78afff8da5ef",
                "status": "renamed"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java",
                "patch": "@@ -24,8 +24,7 @@\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowTracing;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -74,6 +73,12 @@ public void processElement(WindowedValue<KeyedWorkItem<K, InputT>> elem) {\n     doFnRunner.processElement(elem.withValue(keyedWorkItem));\n   }\n \n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+      TimeDomain timeDomain) {\n+    doFnRunner.onTimer(timerId, window, timestamp, timeDomain);\n+  }\n+\n   @Override\n   public void finishBundle() {\n     doFnRunner.finishBundle();\n@@ -118,21 +123,30 @@ public LateDataFilter(\n                     }\n                   });\n             }});\n+      Iterable<WindowedValue<InputT>> concatElements = Iterables.concat(windowsExpandedElements);\n+\n+      // Bump the counter separately since we don't want multiple iterations to\n+      // increase it multiple times.\n+      for (WindowedValue<InputT> input : concatElements) {\n+        BoundedWindow window = Iterables.getOnlyElement(input.getWindows());\n+        if (canDropDueToExpiredWindow(window)) {\n+          // The element is too late for this window.\n+          droppedDueToLateness.addValue(1L);\n+          WindowTracing.debug(\n+              \"ReduceFnRunner.processElement: Dropping element at {} for key:{}; window:{} \"\n+              + \"since too far behind inputWatermark:{}; outputWatermark:{}\",\n+              input.getTimestamp(), key, window, timerInternals.currentInputWatermarkTime(),\n+              timerInternals.currentOutputWatermarkTime());\n+        }\n+      }\n \n       Iterable<WindowedValue<InputT>> nonLateElements = Iterables.filter(\n-          Iterables.concat(windowsExpandedElements),\n+          concatElements,\n           new Predicate<WindowedValue<InputT>>() {\n             @Override\n             public boolean apply(WindowedValue<InputT> input) {\n               BoundedWindow window = Iterables.getOnlyElement(input.getWindows());\n               if (canDropDueToExpiredWindow(window)) {\n-                // The element is too late for this window.\n-                droppedDueToLateness.addValue(1L);\n-                WindowTracing.debug(\n-                    \"ReduceFnRunner.processElement: Dropping element at {} for key:{}; window:{} \"\n-                    + \"since too far behind inputWatermark:{}; outputWatermark:{}\",\n-                    input.getTimestamp(), key, window, timerInternals.currentInputWatermarkTime(),\n-                    timerInternals.currentOutputWatermarkTime());\n                 return false;\n               } else {\n                 return true;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java",
                "sha": "290171ad22800375bb01ac7ecd180e0cbe58d272",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/MergingActiveWindowSet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/MergingActiveWindowSet.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/MergingActiveWindowSet.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import static com.google.common.base.Preconditions.checkNotNull;\n import static com.google.common.base.Preconditions.checkState;",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/MergingActiveWindowSet.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/MergingActiveWindowSet.java",
                "sha": "720377adacd996c08154c2da4f2b080936205441",
                "status": "renamed"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/NonMergingActiveWindowSet.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/NonMergingActiveWindowSet.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/NonMergingActiveWindowSet.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.collect.ImmutableSet;",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/NonMergingActiveWindowSet.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/NonMergingActiveWindowSet.java",
                "sha": "fec6c4531d3f2189ef28bfe95f7110ae76d043f5",
                "status": "renamed"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PaneInfoTracker.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/PaneInfoTracker.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/PaneInfoTracker.java",
                "patch": "@@ -54,6 +54,10 @@ public void clear(StateAccessor<?> state) {\n     state.access(PANE_INFO_TAG).clear();\n   }\n \n+  public void prefetchPaneInfo(ReduceFn<?, ?, ?, ?>.Context context) {\n+    context.state().access(PaneInfoTracker.PANE_INFO_TAG).readLater();\n+  }\n+\n   /**\n    * Return a ({@link ReadableState} for) the pane info appropriate for {@code context}. The pane\n    * info includes the timing for the pane, who's calculation is quite subtle.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PaneInfoTracker.java",
                "sha": "69a4cfd5b9ef00921281d8c078c875e6d5a796ab",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunner.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunner.java",
                "patch": "@@ -15,14 +15,15 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import java.io.Serializable;\n import java.util.Collection;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.CombineFnBase.PerKeyCombineFn;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.SideInputReader;\n \n /**\n  * An interface that runs a {@link PerKeyCombineFn} with unified APIs.",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/PerKeyCombineFnRunner.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunner.java",
                "sha": "a927ecd31312c6b1e37400f4d10f5b6003887735",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunners.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunners.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunners.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.CombineContextFactory;\n-import org.apache.beam.sdk.util.PerKeyCombineFnRunner;\n import org.apache.beam.sdk.util.SideInputReader;\n \n /**",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PerKeyCombineFnRunners.java",
                "sha": "34d711bc241dae0c16731e8f1140eb3e546293c5",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 16,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "patch": "@@ -25,8 +25,10 @@\n import java.util.Set;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollectionView;\n+import org.joda.time.Instant;\n \n /**\n  * A {@link DoFnRunner} that can refuse to process elements that are not ready, instead returning\n@@ -71,32 +73,23 @@ public void startBundle() {\n    */\n   public Iterable<WindowedValue<InputT>> processElementInReadyWindows(WindowedValue<InputT> elem) {\n     if (views.isEmpty()) {\n+      // When there are no side inputs, we can preserve the compressed representation.\n       processElement(elem);\n       return Collections.emptyList();\n     }\n-    ImmutableList.Builder<BoundedWindow> readyWindowsBuilder = ImmutableList.builder();\n-    ImmutableList.Builder<BoundedWindow> pushedBackWindowsBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<WindowedValue<InputT>> pushedBack = ImmutableList.builder();\n     for (WindowedValue<InputT> windowElem : elem.explodeWindows()) {\n       BoundedWindow mainInputWindow = Iterables.getOnlyElement(windowElem.getWindows());\n       if (isReady(mainInputWindow)) {\n-        readyWindowsBuilder.add(mainInputWindow);\n+        // When there are any side inputs, we have to process the element in each window\n+        // individually, to disambiguate access to per-window side inputs.\n+        processElement(windowElem);\n       } else {\n         notReadyWindows.add(mainInputWindow);\n-        pushedBackWindowsBuilder.add(mainInputWindow);\n+        pushedBack.add(windowElem);\n       }\n     }\n-    ImmutableList<BoundedWindow> readyWindows = readyWindowsBuilder.build();\n-    ImmutableList<BoundedWindow> pushedBackWindows = pushedBackWindowsBuilder.build();\n-    if (!readyWindows.isEmpty()) {\n-      processElement(\n-          WindowedValue.of(\n-              elem.getValue(), elem.getTimestamp(), readyWindows, elem.getPane()));\n-    }\n-    return pushedBackWindows.isEmpty()\n-        ? ImmutableList.<WindowedValue<InputT>>of()\n-        : ImmutableList.of(\n-            WindowedValue.of(\n-                elem.getValue(), elem.getTimestamp(), pushedBackWindows, elem.getPane()));\n+    return pushedBack.build();\n   }\n \n   private boolean isReady(BoundedWindow mainInputWindow) {\n@@ -118,6 +111,12 @@ public void processElement(WindowedValue<InputT> elem) {\n     underlying.processElement(elem);\n   }\n \n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+      TimeDomain timeDomain) {\n+    underlying.onTimer(timerId, window, timestamp, timeDomain);\n+  }\n+\n   /**\n    * Call the underlying {@link DoFnRunner#finishBundle()}.\n    */",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "sha": "2962832a5096cc729f098b1eda532ccec75f013b",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 15,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "patch": "@@ -28,16 +28,13 @@\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.util.ActiveWindowSet;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.Timers;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.state.MergingStateAccessor;\n-import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.State;\n import org.apache.beam.sdk.util.state.StateAccessor;\n import org.apache.beam.sdk.util.state.StateContext;\n@@ -99,11 +96,7 @@\n         activeWindows,\n         windowingStrategy.getWindowFn().windowCoder(),\n         stateInternals,\n-        stateContextFromComponents(\n-            options,\n-            sideInputReader,\n-            window,\n-            windowingStrategy.getWindowFn()),\n+        stateContextFromComponents(options, sideInputReader, window),\n         style);\n   }\n \n@@ -117,7 +110,7 @@\n   }\n \n   public ReduceFn<K, InputT, OutputT, W>.OnTriggerContext forTrigger(W window,\n-      ReadableState<PaneInfo> pane, StateStyle style, OnTriggerCallbacks<OutputT> callbacks) {\n+      PaneInfo pane, StateStyle style, OnTriggerCallbacks<OutputT> callbacks) {\n     return new OnTriggerContextImpl(stateAccessor(window, style), pane, callbacks);\n   }\n \n@@ -389,11 +382,11 @@ public Timers timers() {\n \n   private class OnTriggerContextImpl extends ReduceFn<K, InputT, OutputT, W>.OnTriggerContext {\n     private final StateAccessorImpl<K, W> state;\n-    private final ReadableState<PaneInfo> pane;\n+    private final PaneInfo pane;\n     private final OnTriggerCallbacks<OutputT> callbacks;\n     private final TimersImpl timers;\n \n-    private OnTriggerContextImpl(StateAccessorImpl<K, W> state, ReadableState<PaneInfo> pane,\n+    private OnTriggerContextImpl(StateAccessorImpl<K, W> state, PaneInfo pane,\n         OnTriggerCallbacks<OutputT> callbacks) {\n       reduceFn.super();\n       this.state = state;\n@@ -424,7 +417,7 @@ public W window() {\n \n     @Override\n     public PaneInfo paneInfo() {\n-      return pane.read();\n+      return pane;\n     }\n \n     @Override\n@@ -513,8 +506,7 @@ public Timers timers() {\n   private static <W extends BoundedWindow> StateContext<W> stateContextFromComponents(\n       @Nullable final PipelineOptions options,\n       final SideInputReader sideInputReader,\n-      final W mainInputWindow,\n-      final WindowFn<?, W> windowFn) {\n+      final W mainInputWindow) {\n     if (options == null) {\n       return StateContexts.nullContext();\n     } else {\n@@ -527,7 +519,11 @@ public PipelineOptions getPipelineOptions() {\n \n         @Override\n         public <T> T sideInput(PCollectionView<T> view) {\n-          return sideInputReader.get(view, windowFn.getSideInputWindow(mainInputWindow));\n+          return sideInputReader.get(\n+              view,\n+              view.getWindowingStrategyInternal()\n+                  .getWindowFn()\n+                  .getSideInputWindow(mainInputWindow));\n         }\n \n         @Override",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnContextFactory.java",
                "sha": "eae1a8b3b5470469d9bfd7693b87db8c8673f763",
                "status": "modified"
            },
            {
                "additions": 313,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnRunner.java",
                "changes": 496,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 183,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnRunner.java",
                "patch": "@@ -21,12 +21,15 @@\n import static com.google.common.base.Preconditions.checkState;\n \n import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.collect.ImmutableMap;\n+import com.google.common.base.Function;\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableSet;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n@@ -47,9 +50,6 @@\n import org.apache.beam.sdk.transforms.windowing.PaneInfo.Timing;\n import org.apache.beam.sdk.transforms.windowing.Window.ClosingBehavior;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.util.ActiveWindowSet;\n-import org.apache.beam.sdk.util.MergingActiveWindowSet;\n-import org.apache.beam.sdk.util.NonMergingActiveWindowSet;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n@@ -58,10 +58,8 @@\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.WindowingStrategy.AccumulationMode;\n-import org.apache.beam.sdk.util.state.ReadableState;\n import org.apache.beam.sdk.util.state.StateInternals;\n import org.apache.beam.sdk.util.state.StateNamespaces.WindowNamespace;\n-import org.apache.beam.sdk.util.state.TimerCallback;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.joda.time.Duration;\n@@ -91,7 +89,7 @@\n  * @param <OutputT> The output type that will be produced for each key.\n  * @param <W> The type of windows this operates on.\n  */\n-public class ReduceFnRunner<K, InputT, OutputT, W extends BoundedWindow> implements TimerCallback {\n+public class ReduceFnRunner<K, InputT, OutputT, W extends BoundedWindow> {\n \n   /**\n    * The {@link ReduceFnRunner} depends on most aspects of the {@link WindowingStrategy}.\n@@ -268,6 +266,32 @@ boolean hasNoActiveWindows() {\n     return activeWindows.getActiveAndNewWindows().isEmpty();\n   }\n \n+  private Set<W> openWindows(Collection<W> windows) {\n+    Set<W> result = new HashSet<>();\n+    for (W window : windows) {\n+      ReduceFn<K, InputT, OutputT, W>.Context directContext = contextFactory.base(\n+          window, StateStyle.DIRECT);\n+      if (!triggerRunner.isClosed(directContext.state())) {\n+        result.add(window);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private Collection<W> windowsThatShouldFire(Set<W> windows) throws Exception {\n+    Collection<W> result = new ArrayList<>();\n+    // Filter out timers that didn't trigger.\n+    for (W window : windows) {\n+      ReduceFn<K, InputT, OutputT, W>.Context directContext =\n+          contextFactory.base(window, StateStyle.DIRECT);\n+      if (triggerRunner.shouldFire(\n+          directContext.window(), directContext.timers(), directContext.state())) {\n+        result.add(window);\n+      }\n+    }\n+    return result;\n+  }\n+\n   /**\n    * Incorporate {@code values} into the underlying reduce function, and manage holds, timers,\n    * triggers, and window merging.\n@@ -293,25 +317,54 @@ boolean hasNoActiveWindows() {\n    * </ol>\n    */\n   public void processElements(Iterable<WindowedValue<InputT>> values) throws Exception {\n+    if (!values.iterator().hasNext()) {\n+      return;\n+    }\n+\n+    // Determine all the windows for elements.\n+    Set<W> windows = collectWindows(values);\n     // If an incoming element introduces a new window, attempt to merge it into an existing\n     // window eagerly.\n-    Map<W, W> windowToMergeResult = collectAndMergeWindows(values);\n+    Map<W, W> windowToMergeResult = mergeWindows(windows);\n+    if (!windowToMergeResult.isEmpty()) {\n+      // Update windows by removing all windows that were merged away and adding\n+      // the windows they were merged to. We add after completing all the\n+      // removals to avoid removing a window that was also added.\n+      List<W> addedWindows = new ArrayList<>(windowToMergeResult.size());\n+      for (Map.Entry<W, W> entry : windowToMergeResult.entrySet()) {\n+        windows.remove(entry.getKey());\n+        addedWindows.add(entry.getValue());\n+      }\n+      windows.addAll(addedWindows);\n+    }\n \n-    Set<W> windowsToConsider = new HashSet<>();\n+    prefetchWindowsForValues(windows);\n \n-    // Process each element, using the updated activeWindows determined by collectAndMergeWindows.\n+    // All windows that are open before element processing may need to fire.\n+    Set<W> windowsToConsider = openWindows(windows);\n+\n+    // Process each element, using the updated activeWindows determined by mergeWindows.\n     for (WindowedValue<InputT> value : values) {\n-      windowsToConsider.addAll(processElement(windowToMergeResult, value));\n+      processElement(windowToMergeResult, value);\n     }\n \n-    // Trigger output from any window for which the trigger is ready\n+    // Now that we've processed the elements, see if any of the windows need to fire.\n+    // Prefetch state necessary to determine if the triggers should fire.\n     for (W mergedWindow : windowsToConsider) {\n-      ReduceFn<K, InputT, OutputT, W>.Context directContext =\n-          contextFactory.base(mergedWindow, StateStyle.DIRECT);\n-      ReduceFn<K, InputT, OutputT, W>.Context renamedContext =\n-          contextFactory.base(mergedWindow, StateStyle.RENAMED);\n-      triggerRunner.prefetchShouldFire(mergedWindow, directContext.state());\n-      emitIfAppropriate(directContext, renamedContext);\n+      triggerRunner.prefetchShouldFire(\n+          mergedWindow, contextFactory.base(mergedWindow, StateStyle.DIRECT).state());\n+    }\n+    // Filter to windows that are firing.\n+    Collection<W> windowsToFire = windowsThatShouldFire(windowsToConsider);\n+    // Prefetch windows that are firing.\n+    for (W window : windowsToFire) {\n+      prefetchEmit(contextFactory.base(window, StateStyle.DIRECT),\n+          contextFactory.base(window, StateStyle.RENAMED));\n+    }\n+    // Trigger output from firing windows.\n+    for (W window : windowsToFire) {\n+      emit(contextFactory.base(window, StateStyle.DIRECT),\n+          contextFactory.base(window, StateStyle.RENAMED));\n     }\n \n     // We're all done with merging and emitting elements so can compress the activeWindow state.\n@@ -325,52 +378,61 @@ public void persist() {\n   }\n \n   /**\n-   * Extract the windows associated with the values, and invoke merge. Return a map\n-   * from windows to the merge result window. If a window is not in the domain of\n-   * the result map then it did not get merged into a different window.\n+   * Extract the windows associated with the values.\n    */\n-  private Map<W, W> collectAndMergeWindows(Iterable<WindowedValue<InputT>> values)\n-      throws Exception {\n-    // No-op if no merging can take place\n+  private Set<W> collectWindows(Iterable<WindowedValue<InputT>> values) throws Exception {\n+    Set<W> windows = new HashSet<>();\n+    for (WindowedValue<?> value : values) {\n+      for (BoundedWindow untypedWindow : value.getWindows()) {\n+        @SuppressWarnings(\"unchecked\")\n+          W window = (W) untypedWindow;\n+        windows.add(window);\n+      }\n+    }\n+    return windows;\n+  }\n+\n+  /**\n+   * Invoke merge for the given windows and return a map from windows to the\n+   * merge result window. Windows that were not merged are not present in the\n+   * map.\n+   */\n+  private Map<W, W> mergeWindows(Set<W> windows) throws Exception {\n     if (windowingStrategy.getWindowFn().isNonMerging()) {\n-      return ImmutableMap.of();\n+      // Return an empty map, indicating that every window is not merged.\n+      return Collections.emptyMap();\n     }\n \n+    Map<W, W> windowToMergeResult = new HashMap<>();\n     // Collect the windows from all elements (except those which are too late) and\n     // make sure they are already in the active window set or are added as NEW windows.\n-    for (WindowedValue<?> value : values) {\n-      for (BoundedWindow untypedWindow : value.getWindows()) {\n-        @SuppressWarnings(\"unchecked\")\n-        W window = (W) untypedWindow;\n-\n-        // For backwards compat with pre 1.4 only.\n-        // We may still have ACTIVE windows with multiple state addresses, representing\n-        // a window who's state has not yet been eagerly merged.\n-        // We'll go ahead and merge that state now so that we don't have to worry about\n-        // this legacy case anywhere else.\n-        if (activeWindows.isActive(window)) {\n-          Set<W> stateAddressWindows = activeWindows.readStateAddresses(window);\n-          if (stateAddressWindows.size() > 1) {\n-            // This is a legacy window who's state has not been eagerly merged.\n-            // Do that now.\n-            ReduceFn<K, InputT, OutputT, W>.OnMergeContext premergeContext =\n-                contextFactory.forPremerge(window);\n-            reduceFn.onMerge(premergeContext);\n-            watermarkHold.onMerge(premergeContext);\n-            activeWindows.merged(window);\n-          }\n+    for (W window : windows) {\n+      // For backwards compat with pre 1.4 only.\n+      // We may still have ACTIVE windows with multiple state addresses, representing\n+      // a window who's state has not yet been eagerly merged.\n+      // We'll go ahead and merge that state now so that we don't have to worry about\n+      // this legacy case anywhere else.\n+      if (activeWindows.isActive(window)) {\n+        Set<W> stateAddressWindows = activeWindows.readStateAddresses(window);\n+        if (stateAddressWindows.size() > 1) {\n+          // This is a legacy window who's state has not been eagerly merged.\n+          // Do that now.\n+          ReduceFn<K, InputT, OutputT, W>.OnMergeContext premergeContext =\n+              contextFactory.forPremerge(window);\n+          reduceFn.onMerge(premergeContext);\n+          watermarkHold.onMerge(premergeContext);\n+          activeWindows.merged(window);\n         }\n-\n-        // Add this window as NEW if it is not currently ACTIVE.\n-        // If we had already seen this window and closed its trigger, then the\n-        // window will not be currently ACTIVE. It will then be added as NEW here,\n-        // and fall into the merging logic as usual.\n-        activeWindows.ensureWindowExists(window);\n       }\n+\n+      // Add this window as NEW if it is not currently ACTIVE.\n+      // If we had already seen this window and closed its trigger, then the\n+      // window will not be currently ACTIVE. It will then be added as NEW here,\n+      // and fall into the merging logic as usual.\n+      activeWindows.ensureWindowExists(window);\n     }\n \n     // Merge all of the active windows and retain a mapping from source windows to result windows.\n-    Map<W, W> windowToMergeResult = new HashMap<>();\n     activeWindows.merge(new OnMergeCallback(windowToMergeResult));\n     return windowToMergeResult;\n   }\n@@ -472,38 +534,50 @@ public void onMerge(Collection<W> toBeMerged, W mergeResult) throws Exception {\n   }\n \n   /**\n-   * Process an element.\n-   *\n-   * @param value the value being processed\n-   * @return the set of windows in which the element was actually processed\n+   * Redirect element windows to the ACTIVE windows they have been merged into.\n+   * The compressed representation (value, {window1, window2, ...}) actually represents\n+   * distinct elements (value, window1), (value, window2), ...\n+   * so if window1 and window2 merge, the resulting window will contain both copies\n+   * of the value.\n    */\n-  private Collection<W> processElement(Map<W, W> windowToMergeResult, WindowedValue<InputT> value)\n-      throws Exception {\n-    // Redirect element windows to the ACTIVE windows they have been merged into.\n-    // The compressed representation (value, {window1, window2, ...}) actually represents\n-    // distinct elements (value, window1), (value, window2), ...\n-    // so if window1 and window2 merge, the resulting window will contain both copies\n-    // of the value.\n-    Collection<W> windows = new ArrayList<>();\n-    for (BoundedWindow untypedWindow : value.getWindows()) {\n-      @SuppressWarnings(\"unchecked\")\n-      W window = (W) untypedWindow;\n-      W mergeResult = windowToMergeResult.get(window);\n-      if (mergeResult == null) {\n-        mergeResult = window;\n-      }\n-      windows.add(mergeResult);\n-    }\n+  private ImmutableSet<W> toMergedWindows(final Map<W, W> windowToMergeResult,\n+      final Collection<? extends BoundedWindow> windows) {\n+    return ImmutableSet.copyOf(\n+        FluentIterable.from(windows).transform(\n+            new Function<BoundedWindow, W>() {\n+              @Override\n+              public W apply(BoundedWindow untypedWindow) {\n+                @SuppressWarnings(\"unchecked\")\n+                W window = (W) untypedWindow;\n+                W mergedWindow = windowToMergeResult.get(window);\n+                // If the element is not present in the map, the window is unmerged.\n+                return (mergedWindow == null) ? window : mergedWindow;\n+              }\n+            }\n+        ));\n+  }\n \n+  private void prefetchWindowsForValues(Collection<W> windows) {\n     // Prefetch in each of the windows if we're going to need to process triggers\n     for (W window : windows) {\n-      ReduceFn<K, InputT, OutputT, W>.ProcessValueContext directContext = contextFactory.forValue(\n-          window, value.getValue(), value.getTimestamp(), StateStyle.DIRECT);\n+      ReduceFn<K, InputT, OutputT, W>.Context directContext = contextFactory.base(\n+          window, StateStyle.DIRECT);\n       triggerRunner.prefetchForValue(window, directContext.state());\n     }\n+  }\n+\n+  /**\n+   * Process an element.\n+   *\n+   * @param windowToMergeResult map of windows to merged windows. If a window is\n+   * not present it is unmerged.\n+   * @param value the value being processed\n+   */\n+  private void processElement(Map<W, W> windowToMergeResult, WindowedValue<InputT> value)\n+      throws Exception {\n+    ImmutableSet<W> windows = toMergedWindows(windowToMergeResult, value.getWindows());\n \n     // Process the element for each (mergeResultWindow, not closed) window it belongs to.\n-    List<W> triggerableWindows = new ArrayList<>(windows.size());\n     for (W window : windows) {\n       ReduceFn<K, InputT, OutputT, W>.ProcessValueContext directContext = contextFactory.forValue(\n           window, value.getValue(), value.getTimestamp(), StateStyle.DIRECT);\n@@ -518,7 +592,6 @@ public void onMerge(Collection<W> toBeMerged, W mergeResult) throws Exception {\n         continue;\n       }\n \n-      triggerableWindows.add(window);\n       activeWindows.ensureWindowIsActive(window);\n       ReduceFn<K, InputT, OutputT, W>.ProcessValueContext renamedContext = contextFactory.forValue(\n           window, value.getValue(), value.getTimestamp(), StateStyle.RENAMED);\n@@ -562,102 +635,152 @@ public void onMerge(Collection<W> toBeMerged, W mergeResult) throws Exception {\n       // cannot take a trigger state from firing to non-firing.\n       // (We don't actually assert this since it is too slow.)\n     }\n-\n-    return triggerableWindows;\n   }\n \n   /**\n-   * Called when an end-of-window, garbage collection, or trigger-specific timer fires.\n+   * Enriches TimerData with state necessary for processing a timer as well as\n+   * common queries about a timer.\n    */\n-  public void onTimer(TimerData timer) throws Exception {\n-    // Which window is the timer for?\n-    checkArgument(timer.getNamespace() instanceof WindowNamespace,\n-        \"Expected timer to be in WindowNamespace, but was in %s\", timer.getNamespace());\n-    @SuppressWarnings(\"unchecked\")\n-    WindowNamespace<W> windowNamespace = (WindowNamespace<W>) timer.getNamespace();\n-    W window = windowNamespace.getWindow();\n-    ReduceFn<K, InputT, OutputT, W>.Context directContext =\n-        contextFactory.base(window, StateStyle.DIRECT);\n-    ReduceFn<K, InputT, OutputT, W>.Context renamedContext =\n-        contextFactory.base(window, StateStyle.RENAMED);\n+  private class EnrichedTimerData {\n+    public final Instant timestamp;\n+    public final ReduceFn<K, InputT, OutputT, W>.Context directContext;\n+    public final ReduceFn<K, InputT, OutputT, W>.Context renamedContext;\n+    // If this is an end-of-window timer then we may need to set a garbage collection timer\n+    // if allowed lateness is non-zero.\n+    public final boolean isEndOfWindow;\n+    // If this is a garbage collection timer then we should trigger and\n+    // garbage collect the window.  We'll consider any timer at or after the\n+    // end-of-window time to be a signal to garbage collect.\n+    public final boolean isGarbageCollection;\n+\n+    EnrichedTimerData(\n+        TimerData timer,\n+        ReduceFn<K, InputT, OutputT, W>.Context directContext,\n+        ReduceFn<K, InputT, OutputT, W>.Context renamedContext) {\n+      this.timestamp = timer.getTimestamp();\n+      this.directContext = directContext;\n+      this.renamedContext = renamedContext;\n+      W window = directContext.window();\n+      this.isEndOfWindow = TimeDomain.EVENT_TIME == timer.getDomain()\n+          && timer.getTimestamp().equals(window.maxTimestamp());\n+      Instant cleanupTime = garbageCollectionTime(window);\n+      this.isGarbageCollection = !timer.getTimestamp().isBefore(cleanupTime);\n+    }\n \n     // Has this window had its trigger finish?\n     // - The trigger may implement isClosed as constant false.\n     // - If the window function does not support windowing then all windows will be considered\n     // active.\n     // So we must take conjunction of activeWindows and triggerRunner state.\n-    boolean windowIsActiveAndOpen =\n-        activeWindows.isActive(window) && !triggerRunner.isClosed(directContext.state());\n+    public boolean windowIsActiveAndOpen() {\n+      return activeWindows.isActive(directContext.window())\n+          && !triggerRunner.isClosed(directContext.state());\n+    }\n+  }\n \n-    if (!windowIsActiveAndOpen) {\n-      WindowTracing.debug(\n-          \"ReduceFnRunner.onTimer: Note that timer {} is for non-ACTIVE window {}\", timer, window);\n+  public void onTimers(Iterable<TimerData> timers) throws Exception {\n+    if (!timers.iterator().hasNext()) {\n+      return;\n     }\n \n-    // If this is an end-of-window timer then we may need to set a garbage collection timer\n-    // if allowed lateness is non-zero.\n-    boolean isEndOfWindow = TimeDomain.EVENT_TIME == timer.getDomain()\n-        && timer.getTimestamp().equals(window.maxTimestamp());\n-\n-    // If this is a garbage collection timer then we should trigger and garbage collect the window.\n-    // We'll consider any timer at or after the end-of-window time to be a signal to garbage\n-    // collect.\n-    Instant cleanupTime = garbageCollectionTime(window);\n-    boolean isGarbageCollection = TimeDomain.EVENT_TIME == timer.getDomain()\n-        && !timer.getTimestamp().isBefore(cleanupTime);\n-\n-    if (isGarbageCollection) {\n-      WindowTracing.debug(\n-          \"ReduceFnRunner.onTimer: Cleaning up for key:{}; window:{} at {} with \"\n-          + \"inputWatermark:{}; outputWatermark:{}\",\n-          key, window, timer.getTimestamp(), timerInternals.currentInputWatermarkTime(),\n-          timerInternals.currentOutputWatermarkTime());\n-\n-      if (windowIsActiveAndOpen) {\n-        // We need to call onTrigger to emit the final pane if required.\n-        // The final pane *may* be ON_TIME if no prior ON_TIME pane has been emitted,\n-        // and the watermark has passed the end of the window.\n-        @Nullable Instant newHold =\n-            onTrigger(directContext, renamedContext, true/* isFinished */, isEndOfWindow);\n-        checkState(newHold == null,\n-            \"Hold placed at %s despite isFinished being true.\", newHold);\n+    // Create a reusable context for each timer and begin prefetching necessary\n+    // state.\n+    List<EnrichedTimerData> enrichedTimers = new LinkedList();\n+    for (TimerData timer : timers) {\n+      checkArgument(timer.getNamespace() instanceof WindowNamespace,\n+          \"Expected timer to be in WindowNamespace, but was in %s\", timer.getNamespace());\n+      @SuppressWarnings(\"unchecked\")\n+        WindowNamespace<W> windowNamespace = (WindowNamespace<W>) timer.getNamespace();\n+      W window = windowNamespace.getWindow();\n+      ReduceFn<K, InputT, OutputT, W>.Context directContext =\n+          contextFactory.base(window, StateStyle.DIRECT);\n+      ReduceFn<K, InputT, OutputT, W>.Context renamedContext =\n+          contextFactory.base(window, StateStyle.RENAMED);\n+      EnrichedTimerData enrichedTimer = new EnrichedTimerData(timer, directContext, renamedContext);\n+      enrichedTimers.add(enrichedTimer);\n+\n+      // Perform prefetching of state to determine if the trigger should fire.\n+      if (enrichedTimer.isGarbageCollection) {\n+        triggerRunner.prefetchIsClosed(directContext.state());\n+      } else {\n+        triggerRunner.prefetchShouldFire(directContext.window(), directContext.state());\n       }\n+    }\n \n-      // Cleanup flavor B: Clear all the remaining state for this window since we'll never\n-      // see elements for it again.\n-      clearAllState(directContext, renamedContext, windowIsActiveAndOpen);\n-    } else {\n-      WindowTracing.debug(\n-          \"ReduceFnRunner.onTimer: Triggering for key:{}; window:{} at {} with \"\n-          + \"inputWatermark:{}; outputWatermark:{}\",\n-          key, window, timer.getTimestamp(), timerInternals.currentInputWatermarkTime(),\n-          timerInternals.currentOutputWatermarkTime());\n-      if (windowIsActiveAndOpen) {\n-        emitIfAppropriate(directContext, renamedContext);\n+    // For those windows that are active and open, prefetch the triggering or emitting state.\n+    for (EnrichedTimerData timer : enrichedTimers) {\n+      if (timer.windowIsActiveAndOpen()) {\n+        ReduceFn<K, InputT, OutputT, W>.Context directContext = timer.directContext;\n+        if (timer.isGarbageCollection) {\n+          prefetchOnTrigger(directContext, timer.renamedContext);\n+        } else if (triggerRunner.shouldFire(\n+            directContext.window(), directContext.timers(), directContext.state())) {\n+          prefetchEmit(directContext, timer.renamedContext);\n+        }\n       }\n+    }\n \n-      if (isEndOfWindow) {\n-        // If the window strategy trigger includes a watermark trigger then at this point\n-        // there should be no data holds, either because we'd already cleared them on an\n-        // earlier onTrigger, or because we just cleared them on the above emitIfAppropriate.\n-        // We could assert this but it is very expensive.\n-\n-        // Since we are processing an on-time firing we should schedule the garbage collection\n-        // timer. (If getAllowedLateness is zero then the timer event will be considered a\n-        // cleanup event and handled by the above).\n-        // Note we must do this even if the trigger is finished so that we are sure to cleanup\n-        // any final trigger finished bits.\n-        checkState(\n-            windowingStrategy.getAllowedLateness().isLongerThan(Duration.ZERO),\n-            \"Unexpected zero getAllowedLateness\");\n-        WindowTracing.debug(\n-            \"ReduceFnRunner.onTimer: Scheduling cleanup timer for key:{}; window:{} at {} with \"\n-            + \"inputWatermark:{}; outputWatermark:{}\",\n-            key, directContext.window(), cleanupTime, timerInternals.currentInputWatermarkTime(),\n+    // Perform processing now that everything is prefetched.\n+    for (EnrichedTimerData timer : enrichedTimers) {\n+      ReduceFn<K, InputT, OutputT, W>.Context directContext = timer.directContext;\n+      ReduceFn<K, InputT, OutputT, W>.Context renamedContext = timer.renamedContext;\n+\n+      if (timer.isGarbageCollection) {\n+        WindowTracing.debug(\"ReduceFnRunner.onTimer: Cleaning up for key:{}; window:{} at {} with \"\n+                + \"inputWatermark:{}; outputWatermark:{}\",\n+            key, directContext.window(), timer.timestamp,\n+            timerInternals.currentInputWatermarkTime(),\n             timerInternals.currentOutputWatermarkTime());\n-        checkState(!cleanupTime.isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE),\n-                                 \"Cleanup time %s is beyond end-of-time\", cleanupTime);\n-        directContext.timers().setTimer(cleanupTime, TimeDomain.EVENT_TIME);\n+\n+        boolean windowIsActiveAndOpen = timer.windowIsActiveAndOpen();\n+        if (windowIsActiveAndOpen) {\n+          // We need to call onTrigger to emit the final pane if required.\n+          // The final pane *may* be ON_TIME if no prior ON_TIME pane has been emitted,\n+          // and the watermark has passed the end of the window.\n+          @Nullable\n+          Instant newHold = onTrigger(\n+              directContext, renamedContext, true /* isFinished */, timer.isEndOfWindow);\n+          checkState(newHold == null, \"Hold placed at %s despite isFinished being true.\", newHold);\n+        }\n+\n+        // Cleanup flavor B: Clear all the remaining state for this window since we'll never\n+        // see elements for it again.\n+        clearAllState(directContext, renamedContext, windowIsActiveAndOpen);\n+      } else {\n+        WindowTracing.debug(\"ReduceFnRunner.onTimer: Triggering for key:{}; window:{} at {} with \"\n+                + \"inputWatermark:{}; outputWatermark:{}\",\n+            key, directContext.window(), timer.timestamp,\n+            timerInternals.currentInputWatermarkTime(),\n+            timerInternals.currentOutputWatermarkTime());\n+        if (timer.windowIsActiveAndOpen()\n+            && triggerRunner.shouldFire(\n+                   directContext.window(), directContext.timers(), directContext.state())) {\n+          emit(directContext, renamedContext);\n+        }\n+\n+        if (timer.isEndOfWindow) {\n+          // If the window strategy trigger includes a watermark trigger then at this point\n+          // there should be no data holds, either because we'd already cleared them on an\n+          // earlier onTrigger, or because we just cleared them on the above emit.\n+          // We could assert this but it is very expensive.\n+\n+          // Since we are processing an on-time firing we should schedule the garbage collection\n+          // timer. (If getAllowedLateness is zero then the timer event will be considered a\n+          // cleanup event and handled by the above).\n+          // Note we must do this even if the trigger is finished so that we are sure to cleanup\n+          // any final trigger finished bits.\n+          checkState(windowingStrategy.getAllowedLateness().isLongerThan(Duration.ZERO),\n+              \"Unexpected zero getAllowedLateness\");\n+          Instant cleanupTime = garbageCollectionTime(directContext.window());\n+          WindowTracing.debug(\n+              \"ReduceFnRunner.onTimer: Scheduling cleanup timer for key:{}; window:{} at {} with \"\n+                  + \"inputWatermark:{}; outputWatermark:{}\",\n+              key, directContext.window(), cleanupTime, timerInternals.currentInputWatermarkTime(),\n+              timerInternals.currentOutputWatermarkTime());\n+          checkState(!cleanupTime.isAfter(BoundedWindow.TIMESTAMP_MAX_VALUE),\n+              \"Cleanup time %s is beyond end-of-time\", cleanupTime);\n+          directContext.timers().setTimer(cleanupTime, TimeDomain.EVENT_TIME);\n+        }\n       }\n     }\n   }\n@@ -666,7 +789,7 @@ public void onTimer(TimerData timer) throws Exception {\n    * Clear all the state associated with {@code context}'s window.\n    * Should only be invoked if we know all future elements for this window will be considered\n    * beyond allowed lateness.\n-   * This is a superset of the clearing done by {@link #emitIfAppropriate} below since:\n+   * This is a superset of the clearing done by {@link #emit} below since:\n    * <ol>\n    * <li>We can clear the trigger finished bits since we'll never need to ask if the trigger is\n    * closed again.\n@@ -692,10 +815,10 @@ private void clearAllState(\n     } else {\n       // If !windowIsActiveAndOpen then !activeWindows.isActive (1) or triggerRunner.isClosed (2).\n       // For (1), if !activeWindows.isActive then the window must be merging and has been\n-      // explicitly removed by emitIfAppropriate. But in that case the trigger must have fired\n+      // explicitly removed by emit. But in that case the trigger must have fired\n       // and been closed, so this case reduces to (2).\n       // For (2), if triggerRunner.isClosed then the trigger was fired and entered the\n-      // closed state. In that case emitIfAppropriate will have cleared all state in\n+      // closed state. In that case emit will have cleared all state in\n       // reduceFn, triggerRunner (except for finished bits), paneInfoTracker and activeWindows.\n       // We also know nonEmptyPanes must have been unconditionally cleared by the trigger.\n       // Since the trigger fired the existing watermark holds must have been cleared, and since\n@@ -737,17 +860,23 @@ private boolean shouldDiscardAfterFiring(boolean isFinished) {\n     return false;\n   }\n \n+  private void prefetchEmit(ReduceFn<K, InputT, OutputT, W>.Context directContext,\n+                                ReduceFn<K, InputT, OutputT, W>.Context renamedContext) {\n+    triggerRunner.prefetchShouldFire(directContext.window(), directContext.state());\n+    triggerRunner.prefetchOnFire(directContext.window(), directContext.state());\n+    triggerRunner.prefetchIsClosed(directContext.state());\n+    prefetchOnTrigger(directContext, renamedContext);\n+  }\n+\n   /**\n-   * Possibly emit a pane if a trigger is ready to fire or timers require it, and cleanup state.\n+   * Emit if a trigger is ready to fire or timers require it, and cleanup state.\n    */\n-  private void emitIfAppropriate(ReduceFn<K, InputT, OutputT, W>.Context directContext,\n+  private void emit(\n+      ReduceFn<K, InputT, OutputT, W>.Context directContext,\n       ReduceFn<K, InputT, OutputT, W>.Context renamedContext)\n       throws Exception {\n-    if (!triggerRunner.shouldFire(\n-        directContext.window(), directContext.timers(), directContext.state())) {\n-      // Ignore unless trigger is ready to fire\n-      return;\n-    }\n+    checkState(triggerRunner.shouldFire(\n+        directContext.window(), directContext.timers(), directContext.state()));\n \n     // Inform the trigger of the transition to see if it is finished\n     triggerRunner.onFire(directContext.window(), directContext.timers(), directContext.state());\n@@ -782,7 +911,7 @@ private void emitIfAppropriate(ReduceFn<K, InputT, OutputT, W>.Context directCon\n   }\n \n   /**\n-   * Do we need to emit a pane?\n+   * Do we need to emit?\n    */\n   private boolean needToEmit(boolean isEmpty, boolean isFinished, PaneInfo.Timing timing) {\n     if (!isEmpty) {\n@@ -800,6 +929,15 @@ private boolean needToEmit(boolean isEmpty, boolean isFinished, PaneInfo.Timing\n     return false;\n   }\n \n+  private void prefetchOnTrigger(\n+      final ReduceFn<K, InputT, OutputT, W>.Context directContext,\n+      ReduceFn<K, InputT, OutputT, W>.Context renamedContext) {\n+    paneInfoTracker.prefetchPaneInfo(directContext);\n+    watermarkHold.prefetchExtract(renamedContext);\n+    nonEmptyPanes.isEmpty(renamedContext.state()).readLater();\n+    reduceFn.prefetchOnTrigger(directContext.state());\n+  }\n+\n   /**\n    * Run the {@link ReduceFn#onTrigger} method and produce any necessary output.\n    *\n@@ -813,25 +951,17 @@ private Instant onTrigger(\n           throws Exception {\n     Instant inputWM = timerInternals.currentInputWatermarkTime();\n \n-    // Prefetch necessary states\n-    ReadableState<WatermarkHold.OldAndNewHolds> outputTimestampFuture =\n-        watermarkHold.extractAndRelease(renamedContext, isFinished).readLater();\n-    ReadableState<PaneInfo> paneFuture =\n-        paneInfoTracker.getNextPaneInfo(directContext, isFinished).readLater();\n-    ReadableState<Boolean> isEmptyFuture =\n-        nonEmptyPanes.isEmpty(renamedContext.state()).readLater();\n-\n-    reduceFn.prefetchOnTrigger(directContext.state());\n-    triggerRunner.prefetchOnFire(directContext.window(), directContext.state());\n-\n     // Calculate the pane info.\n-    final PaneInfo pane = paneFuture.read();\n-    // Extract the window hold, and as a side effect clear it.\n+    final PaneInfo pane = paneInfoTracker.getNextPaneInfo(directContext, isFinished).read();\n \n-    WatermarkHold.OldAndNewHolds pair = outputTimestampFuture.read();\n+    // Extract the window hold, and as a side effect clear it.\n+    final WatermarkHold.OldAndNewHolds pair =\n+        watermarkHold.extractAndRelease(renamedContext, isFinished).read();\n     final Instant outputTimestamp = pair.oldHold;\n     @Nullable Instant newHold = pair.newHold;\n \n+    final boolean isEmpty = nonEmptyPanes.isEmpty(renamedContext.state()).read();\n+\n     if (newHold != null) {\n       // We can't be finished yet.\n       checkState(\n@@ -863,11 +993,11 @@ private Instant onTrigger(\n     }\n \n     // Only emit a pane if it has data or empty panes are observable.\n-    if (needToEmit(isEmptyFuture.read(), isFinished, pane.getTiming())) {\n+    if (needToEmit(isEmpty, isFinished, pane.getTiming())) {\n       // Run reduceFn.onTrigger method.\n       final List<W> windows = Collections.singletonList(directContext.window());\n       ReduceFn<K, InputT, OutputT, W>.OnTriggerContext renamedTriggerContext =\n-          contextFactory.forTrigger(directContext.window(), paneFuture, StateStyle.RENAMED,\n+          contextFactory.forTrigger(directContext.window(), pane, StateStyle.RENAMED,\n               new OnTriggerCallbacks<OutputT>() {\n                 @Override\n                 public void output(OutputT toOutput) {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/ReduceFnRunner.java",
                "sha": "96e76b7559c9957d63c9cbd194034e1587418dac",
                "status": "modified"
            },
            {
                "additions": 249,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "changes": 252,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "patch": "@@ -30,11 +30,11 @@\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Aggregator.AggregatorFactory;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.DoFn.Context;\n import org.apache.beam.sdk.transforms.DoFn.InputProvider;\n+import org.apache.beam.sdk.transforms.DoFn.OnTimerContext;\n import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;\n@@ -50,8 +50,10 @@\n import org.apache.beam.sdk.util.ExecutionContext.StepContext;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.Timer;\n import org.apache.beam.sdk.util.TimerInternals;\n+import org.apache.beam.sdk.util.TimerSpec;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingInternals;\n@@ -64,6 +66,7 @@\n import org.apache.beam.sdk.util.state.StateTags;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n+import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.joda.time.format.PeriodFormat;\n \n@@ -113,7 +116,7 @@ public SimpleDoFnRunner(\n       WindowingStrategy<?, ?> windowingStrategy) {\n     this.fn = fn;\n     this.signature = DoFnSignatures.getSignature(fn.getClass());\n-    this.observesWindow = signature.processElement().observesWindow();\n+    this.observesWindow = signature.processElement().observesWindow() || !sideInputReader.isEmpty();\n     this.invoker = DoFnInvokers.invokerFor(fn);\n     this.outputManager = outputManager;\n     this.mainOutputTag = mainOutputTag;\n@@ -161,6 +164,35 @@ public void processElement(WindowedValue<InputT> compressedElem) {\n     }\n   }\n \n+  @Override\n+  public void onTimer(\n+      String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain) {\n+\n+    // The effective timestamp is when derived elements will have their timestamp set, if not\n+    // otherwise specified. If this is an event time timer, then they have the timestamp of the\n+    // timer itself. Otherwise, they are set to the input timestamp, which is by definition\n+    // non-late.\n+    Instant effectiveTimestamp;\n+    switch (timeDomain) {\n+      case EVENT_TIME:\n+        effectiveTimestamp = timestamp;\n+        break;\n+\n+      case PROCESSING_TIME:\n+      case SYNCHRONIZED_PROCESSING_TIME:\n+        effectiveTimestamp = context.stepContext.timerInternals().currentInputWatermarkTime();\n+        break;\n+\n+      default:\n+        throw new IllegalArgumentException(\n+            String.format(\"Unknown time domain: %s\", timeDomain));\n+    }\n+\n+    OnTimerArgumentProvider<InputT, OutputT> argumentProvider =\n+        new OnTimerArgumentProvider<>(fn, context, window, effectiveTimestamp, timeDomain);\n+    invoker.invokeOnTimer(timerId, argumentProvider);\n+  }\n+\n   private void invokeProcessElement(WindowedValue<InputT> elem) {\n     final DoFnProcessContext<InputT, OutputT> processContext = createProcessContext(elem);\n \n@@ -402,6 +434,12 @@ public ProcessContext processContext(DoFn<InputT, OutputT> doFn) {\n           \"Cannot access ProcessContext outside of @Processelement method.\");\n     }\n \n+    @Override\n+    public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\n+          \"Cannot access OnTimerContext outside of @OnTimer methods.\");\n+    }\n+\n     @Override\n     public InputProvider<InputT> inputProvider() {\n       throw new UnsupportedOperationException(\"InputProvider is for testing only.\");\n@@ -588,6 +626,12 @@ public BoundedWindow window() {\n       return this;\n     }\n \n+    @Override\n+    public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\n+          \"Cannot access OnTimerContext outside of @OnTimer methods.\");\n+    }\n+\n     @Override\n     public InputProvider<InputT> inputProvider() {\n       throw new UnsupportedOperationException(\"InputProvider parameters are not supported.\");\n@@ -618,7 +662,13 @@ public State state(String stateId) {\n \n     @Override\n     public Timer timer(String timerId) {\n-      throw new UnsupportedOperationException(\"Timer parameters are not supported.\");\n+      try {\n+        TimerSpec spec =\n+            (TimerSpec) signature.timerDeclarations().get(timerId).field().get(fn);\n+        return new TimerInternalsTimer(getNamespace(), timerId, spec, stepContext.timerInternals());\n+      } catch (IllegalAccessException e) {\n+        throw new RuntimeException(e);\n+      }\n     }\n \n     @Override\n@@ -670,5 +720,201 @@ public void outputWindowedValue(\n         }\n       };\n     }\n+\n+  }\n+\n+  /**\n+   * A concrete implementation of {@link DoFnInvoker.ArgumentProvider} used for running a {@link\n+   * DoFn} on a timer.\n+   *\n+   * @param <InputT> the type of the {@link DoFn} (main) input elements\n+   * @param <OutputT> the type of the {@link DoFn} (main) output elements\n+   */\n+  private class OnTimerArgumentProvider<InputT, OutputT>\n+      extends DoFn<InputT, OutputT>.OnTimerContext\n+      implements DoFnInvoker.ArgumentProvider<InputT, OutputT> {\n+\n+    final DoFn<InputT, OutputT> fn;\n+    final DoFnContext<InputT, OutputT> context;\n+    private final BoundedWindow window;\n+    private final Instant timestamp;\n+    private final TimeDomain timeDomain;\n+\n+    /** Lazily initialized; should only be accessed via {@link #getNamespace()}. */\n+    private StateNamespace namespace;\n+\n+    /**\n+     * The state namespace for this context.\n+     *\n+     * <p>Any call to {@link #getNamespace()} when more than one window is present will crash; this\n+     * represents a bug in the runner or the {@link DoFnSignature}, since values must be in exactly\n+     * one window when state or timers are relevant.\n+     */\n+    private StateNamespace getNamespace() {\n+      if (namespace == null) {\n+        namespace = StateNamespaces.window(windowCoder, window);\n+      }\n+      return namespace;\n+    }\n+\n+    private OnTimerArgumentProvider(\n+        DoFn<InputT, OutputT> fn,\n+        DoFnContext<InputT, OutputT> context,\n+        BoundedWindow window,\n+        Instant timestamp,\n+        TimeDomain timeDomain) {\n+      fn.super();\n+      this.fn = fn;\n+      this.context = context;\n+      this.window = window;\n+      this.timestamp = timestamp;\n+      this.timeDomain = timeDomain;\n+    }\n+\n+    @Override\n+    public Instant timestamp() {\n+      return timestamp;\n+    }\n+\n+    @Override\n+    public BoundedWindow window() {\n+      return window;\n+    }\n+\n+    @Override\n+    public TimeDomain timeDomain() {\n+      return timeDomain;\n+    }\n+\n+    @Override\n+    public Context context(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\"Context parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public ProcessContext processContext(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\"ProcessContext parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+      return this;\n+    }\n+\n+    @Override\n+    public InputProvider<InputT> inputProvider() {\n+      throw new UnsupportedOperationException(\"InputProvider parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public OutputReceiver<OutputT> outputReceiver() {\n+      throw new UnsupportedOperationException(\"OutputReceiver parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public <RestrictionT> RestrictionTracker<RestrictionT> restrictionTracker() {\n+      throw new UnsupportedOperationException(\"RestrictionTracker parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public State state(String stateId) {\n+      try {\n+        StateSpec<?, ?> spec =\n+            (StateSpec<?, ?>) signature.stateDeclarations().get(stateId).field().get(fn);\n+        return stepContext\n+            .stateInternals()\n+            .state(getNamespace(), StateTags.tagForSpec(stateId, (StateSpec) spec));\n+      } catch (IllegalAccessException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public Timer timer(String timerId) {\n+      try {\n+        TimerSpec spec =\n+            (TimerSpec) signature.timerDeclarations().get(timerId).field().get(fn);\n+        return new TimerInternalsTimer(getNamespace(), timerId, spec, stepContext.timerInternals());\n+      } catch (IllegalAccessException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public PipelineOptions getPipelineOptions() {\n+      return context.getPipelineOptions();\n+    }\n+\n+    @Override\n+    public void output(OutputT output) {\n+      context.outputWithTimestamp(output, timestamp);\n+    }\n+\n+    @Override\n+    public void outputWithTimestamp(OutputT output, Instant timestamp) {\n+      context.outputWithTimestamp(output, timestamp);\n+    }\n+\n+    @Override\n+    public <T> void sideOutput(TupleTag<T> tag, T output) {\n+      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    }\n+\n+    @Override\n+    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    }\n+\n+    @Override\n+    protected <AggInputT, AggOutputT> Aggregator<AggInputT, AggOutputT> createAggregator(\n+        String name,\n+        CombineFn<AggInputT, ?, AggOutputT> combiner) {\n+      throw new UnsupportedOperationException(\"Cannot createAggregator in @OnTimer method\");\n+    }\n+\n+    @Override\n+    public WindowingInternals<InputT, OutputT> windowingInternals() {\n+      throw new UnsupportedOperationException(\"WindowingInternals are unsupported.\");\n+    }\n+  }\n+\n+  private static class TimerInternalsTimer implements Timer {\n+    private final TimerInternals timerInternals;\n+    private final String timerId;\n+    private final TimerSpec spec;\n+    private final StateNamespace namespace;\n+\n+    public TimerInternalsTimer(\n+        StateNamespace namespace, String timerId, TimerSpec spec, TimerInternals timerInternals) {\n+      this.namespace = namespace;\n+      this.timerId = timerId;\n+      this.spec = spec;\n+      this.timerInternals = timerInternals;\n+    }\n+\n+    @Override\n+    public void setForNowPlus(Duration durationFromNow) {\n+      timerInternals.setTimer(\n+          namespace, timerId, getCurrentTime().plus(durationFromNow), spec.getTimeDomain());\n+    }\n+\n+    @Override\n+    public void cancel() {\n+      timerInternals.deleteTimer(namespace, timerId);\n+    }\n+\n+    private Instant getCurrentTime() {\n+      switch(spec.getTimeDomain()) {\n+        case EVENT_TIME:\n+          return timerInternals.currentInputWatermarkTime();\n+        case PROCESSING_TIME:\n+          return timerInternals.currentProcessingTime();\n+        case SYNCHRONIZED_PROCESSING_TIME:\n+          return timerInternals.currentSynchronizedProcessingTime();\n+        default:\n+          throw new IllegalStateException(\n+              String.format(\"Timer created for unknown time domain %s\", spec.getTimeDomain()));\n+      }\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "sha": "a7d82bf52ed87780b02e99cc4bd0a799da6ff674",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Aggregator.AggregatorFactory;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.OldDoFn.RequiresWindowAccess;\n@@ -40,6 +39,7 @@\n import org.apache.beam.sdk.util.ExecutionContext.StepContext;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -108,6 +108,13 @@ public void processElement(WindowedValue<InputT> elem) {\n     }\n   }\n \n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+      TimeDomain timeDomain) {\n+    throw new UnsupportedOperationException(\n+        String.format(\"Timers are not supported by %s\", OldDoFn.class.getSimpleName()));\n+  }\n+\n   private void invokeProcessElement(WindowedValue<InputT> elem) {\n     final OldDoFn<InputT, OutputT>.ProcessContext processContext = createProcessContext(elem);\n     // This can contain user code. Wrap it in case it throws an exception.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "sha": "342a4a8694583c3bf7eb0a6d058c2bc966e577ad",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "patch": "@@ -49,8 +49,6 @@\n import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItemCoder;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.Timer;\n import org.apache.beam.sdk.util.TimerInternals;\n@@ -107,17 +105,17 @@ public SplittableParDo(ParDo.BoundMulti<InputT, OutputT> parDo) {\n     checkNotNull(parDo, \"parDo must not be null\");\n     this.parDo = parDo;\n     checkArgument(\n-        DoFnSignatures.getSignature(parDo.getNewFn().getClass()).processElement().isSplittable(),\n+        DoFnSignatures.getSignature(parDo.getFn().getClass()).processElement().isSplittable(),\n         \"fn must be a splittable DoFn\");\n   }\n \n   @Override\n-  public PCollectionTuple apply(PCollection<InputT> input) {\n+  public PCollectionTuple expand(PCollection<InputT> input) {\n     return applyTyped(input);\n   }\n \n   private PCollectionTuple applyTyped(PCollection<InputT> input) {\n-    DoFn<InputT, OutputT> fn = parDo.getNewFn();\n+    DoFn<InputT, OutputT> fn = parDo.getFn();\n     Coder<RestrictionT> restrictionCoder =\n         DoFnInvokers.invokerFor(fn)\n             .invokeGetRestrictionCoder(input.getPipeline().getCoderRegistry());\n@@ -181,7 +179,7 @@ private PCollectionTuple applyTyped(PCollection<InputT> input) {\n   public static class GBKIntoKeyedWorkItems<KeyT, InputT>\n       extends PTransform<PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>> {\n     @Override\n-    public PCollection<KeyedWorkItem<KeyT, InputT>> apply(PCollection<KV<KeyT, InputT>> input) {\n+    public PCollection<KeyedWorkItem<KeyT, InputT>> expand(PCollection<KV<KeyT, InputT>> input) {\n       return PCollection.createPrimitiveOutputInternal(\n           input.getPipeline(), WindowingStrategy.globalDefault(), input.isBounded());\n     }\n@@ -249,7 +247,7 @@ public TupleTagList getSideOutputTags() {\n     }\n \n     @Override\n-    public PCollectionTuple apply(\n+    public PCollectionTuple expand(\n         PCollection<? extends KeyedWorkItem<String, ElementAndRestriction<InputT, RestrictionT>>>\n             input) {\n       DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());\n@@ -261,7 +259,7 @@ public PCollectionTuple apply(\n               input.isBounded().and(signature.isBoundedPerElement()));\n \n       // Set output type descriptor similarly to how ParDo.BoundMulti does it.\n-      outputs.get(mainOutputTag).setTypeDescriptorInternal(fn.getOutputTypeDescriptor());\n+      outputs.get(mainOutputTag).setTypeDescriptor(fn.getOutputTypeDescriptor());\n \n       return outputs;\n     }\n@@ -592,9 +590,14 @@ public void outputWithTimestamp(OutputT output, Instant timestamp) {\n         }\n \n         private void noteOutput() {\n-          if (++numOutputs >= MAX_OUTPUTS_PER_BUNDLE) {\n+          // Take the checkpoint only if it hasn't been taken yet, because:\n+          // 1) otherwise we'd lose the previous checkpoint stored in residualRestrictionHolder\n+          // 2) it's not allowed to checkpoint a RestrictionTracker twice, since the first call\n+          // by definition already maximally narrows its restriction, so a second checkpoint would\n+          // have produced a useless empty residual restriction anyway.\n+          if (++numOutputs >= MAX_OUTPUTS_PER_BUNDLE && residualRestrictionHolder[0] == null) {\n             // Request a checkpoint. The fn *may* produce more output, but hopefully not too much.\n-            residualRestrictionHolder[0] = tracker.checkpoint();\n+            residualRestrictionHolder[0] = checkNotNull(tracker.checkpoint());\n           }\n         }\n \n@@ -664,6 +667,11 @@ public BoundedWindow window() {\n         return processContext;\n       }\n \n+      @Override\n+      public DoFn.OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+        throw new IllegalStateException(\"Unexpected extra context access on a splittable DoFn\");\n+      }\n+\n       @Override\n       public DoFn.InputProvider<InputT> inputProvider() {\n         // DoFnSignatures should have verified that this DoFn doesn't access extra context.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/SplittableParDo.java",
                "sha": "e6a2466b2f1786dcce9c9d96f0b0fe041e78b465",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSource.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSource.java",
                "patch": "@@ -88,7 +88,7 @@ public UnboundedReadFromBoundedSource(BoundedSource<T> source) {\n   }\n \n   @Override\n-  public PCollection<T> apply(PBegin input) {\n+  public PCollection<T> expand(PBegin input) {\n     return input.getPipeline().apply(\n         Read.from(new BoundedToUnboundedSourceAdapter<>(source)));\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/UnboundedReadFromBoundedSource.java",
                "sha": "f3f93e1bccec24548eca02e2e793d7d5f4dce5e3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/WatermarkHold.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/WatermarkHold.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/WatermarkHold.java",
                "patch": "@@ -444,6 +444,11 @@ public OldAndNewHolds(Instant oldHold, @Nullable Instant newHold) {\n     }\n   }\n \n+  public void prefetchExtract(final ReduceFn<?, ?, ?, W>.Context context) {\n+    context.state().access(elementHoldTag).readLater();\n+    context.state().access(EXTRA_HOLD_TAG).readLater();\n+  }\n+\n   /**\n    * Return (a future for) the earliest hold for {@code context}. Clear all the holds after\n    * reading, but add/restore an end-of-window or garbage collection hold if required.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/WatermarkHold.java",
                "sha": "7f1afcc0a79cc2a872f9989e9ad47ba5a383d755",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineContextFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineContextFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineContextFactory.java",
                "patch": "@@ -24,12 +24,12 @@\n import java.util.Collection;\n import java.util.Map;\n import javax.annotation.Nullable;\n+import org.apache.beam.runners.core.ActiveWindowSet;\n import org.apache.beam.runners.core.triggers.TriggerStateMachine.MergingTriggerInfo;\n import org.apache.beam.runners.core.triggers.TriggerStateMachine.TriggerInfo;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.util.ActiveWindowSet;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.Timers;\n import org.apache.beam.sdk.util.state.MergingStateAccessor;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineContextFactory.java",
                "sha": "e3df4ee278d9bce9adc4080d47fa1f9c9896e30b",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineRunner.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 7,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineRunner.java",
                "patch": "@@ -99,25 +99,25 @@ public boolean isClosed(StateAccessor<?> state) {\n     return readFinishedBits(state.access(FINISHED_BITS_TAG)).isFinished(rootTrigger);\n   }\n \n-  public void prefetchForValue(W window, StateAccessor<?> state) {\n+  public void prefetchIsClosed(StateAccessor<?> state) {\n     if (isFinishedSetNeeded()) {\n       state.access(FINISHED_BITS_TAG).readLater();\n     }\n+  }\n+\n+  public void prefetchForValue(W window, StateAccessor<?> state) {\n+    prefetchIsClosed(state);\n     rootTrigger.getSpec().prefetchOnElement(\n         contextFactory.createStateAccessor(window, rootTrigger));\n   }\n \n   public void prefetchOnFire(W window, StateAccessor<?> state) {\n-    if (isFinishedSetNeeded()) {\n-      state.access(FINISHED_BITS_TAG).readLater();\n-    }\n+    prefetchIsClosed(state);\n     rootTrigger.getSpec().prefetchOnFire(contextFactory.createStateAccessor(window, rootTrigger));\n   }\n \n   public void prefetchShouldFire(W window, StateAccessor<?> state) {\n-    if (isFinishedSetNeeded()) {\n-      state.access(FINISHED_BITS_TAG).readLater();\n-    }\n+    prefetchIsClosed(state);\n     rootTrigger.getSpec().prefetchShouldFire(\n         contextFactory.createStateAccessor(window, rootTrigger));\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/main/java/org/apache/beam/runners/core/triggers/TriggerStateMachineRunner.java",
                "sha": "2f277eb5bce3a876a6d2a1c3e3f40f3dfb8b7e8c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/KeyedWorkItemCoderTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/KeyedWorkItemCoderTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/KeyedWorkItemCoderTest.java",
                "patch": "@@ -15,14 +15,16 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import com.google.common.collect.ImmutableList;\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VarIntCoder;\n import org.apache.beam.sdk.testing.CoderProperties;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n+import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.state.StateNamespaces;\n import org.joda.time.Instant;\n import org.junit.Test;",
                "previous_filename": "sdks/java/core/src/test/java/org/apache/beam/sdk/util/KeyedWorkItemCoderTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/KeyedWorkItemCoderTest.java",
                "sha": "37fabddca270e9f59474c025880915f51612a913",
                "status": "renamed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunnerTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunnerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunnerTest.java",
                "patch": "@@ -80,6 +80,9 @@ public void testLateDataFilter() throws Exception {\n         createDatum(18, 18L));\n     assertThat(expected, containsInAnyOrder(Iterables.toArray(actual, WindowedValue.class)));\n     assertEquals(1, droppedDueToLateness.sum);\n+    // Ensure that reiterating returns the same results and doesn't increment the counter again.\n+    assertThat(expected, containsInAnyOrder(Iterables.toArray(actual, WindowedValue.class)));\n+    assertEquals(1, droppedDueToLateness.sum);\n   }\n \n   private <T> WindowedValue<T> createDatum(T element, long timestampMillis) {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunnerTest.java",
                "sha": "3cd5d4abaddf4facba66df0e63f4a4806c756776",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/MergingActiveWindowSetTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/MergingActiveWindowSetTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/MergingActiveWindowSetTest.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util;\n+package org.apache.beam.runners.core;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;",
                "previous_filename": "sdks/java/core/src/test/java/org/apache/beam/sdk/util/MergingActiveWindowSetTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/MergingActiveWindowSetTest.java",
                "sha": "a4928e36d32a5af999ff8797f739c55b5e6ab476",
                "status": "renamed"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 7,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "patch": "@@ -20,6 +20,7 @@\n import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n import static org.hamcrest.Matchers.is;\n import static org.junit.Assert.assertThat;\n import static org.mockito.Mockito.when;\n@@ -37,7 +38,10 @@\n import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.util.IdentitySideInputWindowFn;\n import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.state.StateNamespaces;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.hamcrest.Matchers;\n@@ -130,7 +134,7 @@ public void processElementSideInputNotReadyMultipleWindows() {\n             PaneInfo.ON_TIME_AND_ONLY_FIRING);\n     Iterable<WindowedValue<Integer>> multiWindowPushback =\n         runner.processElementInReadyWindows(multiWindow);\n-    assertThat(multiWindowPushback, contains(multiWindow));\n+    assertThat(multiWindowPushback, equalTo(multiWindow.explodeWindows()));\n     assertThat(underlying.inputElems, Matchers.<WindowedValue<Integer>>emptyIterable());\n   }\n \n@@ -165,10 +169,8 @@ public void processElementSideInputNotReadySomeWindows() {\n         underlying.inputElems,\n         containsInAnyOrder(\n             WindowedValue.of(\n-                2,\n-                new Instant(-2),\n-                ImmutableList.of(littleWindow, bigWindow),\n-                PaneInfo.NO_FIRING)));\n+                2, new Instant(-2), ImmutableList.of(littleWindow), PaneInfo.NO_FIRING),\n+            WindowedValue.of(2, new Instant(-2), ImmutableList.of(bigWindow), PaneInfo.NO_FIRING)));\n   }\n \n   @Test\n@@ -191,8 +193,9 @@ public void processElementSideInputReadyAllWindows() {\n     Iterable<WindowedValue<Integer>> multiWindowPushback =\n         runner.processElementInReadyWindows(multiWindow);\n     assertThat(multiWindowPushback, emptyIterable());\n-    assertThat(underlying.inputElems,\n-        containsInAnyOrder(ImmutableList.of(multiWindow).toArray()));\n+    assertThat(\n+        underlying.inputElems,\n+        containsInAnyOrder(ImmutableList.copyOf(multiWindow.explodeWindows()).toArray()));\n   }\n \n   @Test\n@@ -212,25 +215,63 @@ public void processElementNoSideInputs() {\n     Iterable<WindowedValue<Integer>> multiWindowPushback =\n         runner.processElementInReadyWindows(multiWindow);\n     assertThat(multiWindowPushback, emptyIterable());\n+    // Should preserve the compressed representation when there's no side inputs.\n     assertThat(underlying.inputElems, containsInAnyOrder(multiWindow));\n   }\n \n+  /** Tests that a call to onTimer gets delegated. */\n+  @Test\n+  public void testOnTimerCalled() {\n+    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+        createRunner(ImmutableList.<PCollectionView<?>>of());\n+\n+    String timerId = \"fooTimer\";\n+    IntervalWindow window = new IntervalWindow(new Instant(4), new Instant(16));\n+    Instant timestamp = new Instant(72);\n+\n+    // Mocking is not easily compatible with annotation analysis, so we manually record\n+    // the method call.\n+    runner.onTimer(timerId, window, new Instant(timestamp), TimeDomain.EVENT_TIME);\n+\n+    assertThat(\n+        underlying.firedTimers,\n+        contains(\n+            TimerData.of(\n+                timerId,\n+                StateNamespaces.window(IntervalWindow.getCoder(), window),\n+                timestamp,\n+                TimeDomain.EVENT_TIME)));\n+  }\n+\n   private static class TestDoFnRunner<InputT, OutputT> implements DoFnRunner<InputT, OutputT> {\n     List<WindowedValue<InputT>> inputElems;\n+    List<TimerData> firedTimers;\n     private boolean started = false;\n     private boolean finished = false;\n \n     @Override\n     public void startBundle() {\n       started = true;\n       inputElems = new ArrayList<>();\n+      firedTimers = new ArrayList<>();\n     }\n \n     @Override\n     public void processElement(WindowedValue<InputT> elem) {\n       inputElems.add(elem);\n     }\n \n+    @Override\n+    public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+        TimeDomain timeDomain) {\n+      firedTimers.add(\n+          TimerData.of(\n+              timerId,\n+              StateNamespaces.window(IntervalWindow.getCoder(), (IntervalWindow) window),\n+              timestamp,\n+              timeDomain));\n+    }\n+\n     @Override\n     public void finishBundle() {\n       finished = true;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "sha": "a1cdbf6dce04c998773000cd8a17cb35bcae82a4",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "changes": 133,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 63,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "patch": "@@ -17,13 +17,14 @@\n  */\n package org.apache.beam.runners.core;\n \n-import static com.google.common.base.Preconditions.checkArgument;\n import static org.apache.beam.runners.core.WindowMatchers.isSingleWindowedValue;\n import static org.apache.beam.runners.core.WindowMatchers.isWindowedValue;\n+import static org.hamcrest.Matchers.anyOf;\n import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.emptyIterable;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertThat;\n@@ -36,7 +37,6 @@\n import static org.mockito.Mockito.withSettings;\n \n import com.google.common.collect.Iterables;\n-import java.util.Iterator;\n import java.util.List;\n import org.apache.beam.runners.core.triggers.TriggerStateMachine;\n import org.apache.beam.sdk.coders.VarIntCoder;\n@@ -348,49 +348,67 @@ public void testOnElementCombiningAccumulating() throws Exception {\n \n   @Test\n   public void testOnElementCombiningWithContext() throws Exception {\n-    Integer expectedValue = 5;\n-    WindowingStrategy<?, IntervalWindow> windowingStrategy = WindowingStrategy\n-        .of(FixedWindows.of(Duration.millis(10)))\n-        .withMode(AccumulationMode.DISCARDING_FIRED_PANES)\n-        .withOutputTimeFn(OutputTimeFns.outputAtEarliestInputTimestamp())\n-        .withAllowedLateness(Duration.millis(100));\n+    // Create values at timestamps 0 .. 8, windowed into fixed windows of 2.\n+    // Side input windowed into fixed windows of 4:\n+    // main: [ 0 1 ] [ 2 3 ] [ 4 5 ] [ 6 7 ]\n+    // side: [     100     ] [     104     ]\n+    // Combine using a CombineFn \"side input + sum(main inputs)\".\n+    final int firstWindowSideInput = 100;\n+    final int secondWindowSideInput = 104;\n+    final Integer expectedValue = firstWindowSideInput;\n+    WindowingStrategy<?, IntervalWindow> mainInputWindowingStrategy =\n+        WindowingStrategy.of(FixedWindows.of(Duration.millis(2)))\n+            .withMode(AccumulationMode.ACCUMULATING_FIRED_PANES);\n+\n+    WindowingStrategy<?, IntervalWindow> sideInputWindowingStrategy =\n+        WindowingStrategy.of(FixedWindows.of(Duration.millis(4)));\n \n     TestOptions options = PipelineOptionsFactory.as(TestOptions.class);\n-    options.setValue(5);\n+    options.setValue(expectedValue);\n \n     when(mockSideInputReader.contains(Matchers.<PCollectionView<Integer>>any())).thenReturn(true);\n     when(mockSideInputReader.get(\n-        Matchers.<PCollectionView<Integer>>any(), any(BoundedWindow.class))).thenReturn(5);\n+            Matchers.<PCollectionView<Integer>>any(), any(BoundedWindow.class)))\n+        .then(\n+            new Answer<Integer>() {\n+              @Override\n+              public Integer answer(InvocationOnMock invocation) throws Throwable {\n+                IntervalWindow sideInputWindow = (IntervalWindow) invocation.getArguments()[1];\n+                long startMs = sideInputWindow.start().getMillis();\n+                long endMs = sideInputWindow.end().getMillis();\n+                // Window should have been produced by sideInputWindowingStrategy.\n+                assertThat(startMs, anyOf(equalTo(0L), equalTo(4L)));\n+                assertThat(endMs - startMs, equalTo(4L));\n+                // If startMs == 4 (second window), equal to secondWindowSideInput.\n+                return firstWindowSideInput + (int) startMs;\n+              }\n+            });\n \n     @SuppressWarnings({\"rawtypes\", \"unchecked\", \"unused\"})\n     Object suppressWarningsVar = when(mockView.getWindowingStrategyInternal())\n-        .thenReturn((WindowingStrategy) windowingStrategy);\n+        .thenReturn((WindowingStrategy) sideInputWindowingStrategy);\n \n     SumAndVerifyContextFn combineFn = new SumAndVerifyContextFn(mockView, expectedValue);\n-    // Test basic execution of a trigger using a non-combining window set and discarding mode.\n     ReduceFnTester<Integer, Integer, IntervalWindow> tester = ReduceFnTester.combining(\n-        windowingStrategy, mockTriggerStateMachine, combineFn.<String>asKeyedFn(),\n+        mainInputWindowingStrategy, mockTriggerStateMachine, combineFn.<String>asKeyedFn(),\n         VarIntCoder.of(), options, mockSideInputReader);\n \n-    injectElement(tester, 2);\n-\n-    when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);\n-    injectElement(tester, 3);\n-\n     when(mockTriggerStateMachine.shouldFire(anyTriggerContext())).thenReturn(true);\n-    triggerShouldFinish(mockTriggerStateMachine);\n-    injectElement(tester, 4);\n-\n-    // This element shouldn't be seen, because the trigger has finished\n-    injectElement(tester, 6);\n+    for (int i = 0; i < 8; ++i) {\n+      injectElement(tester, i);\n+    }\n \n     assertThat(\n         tester.extractOutput(),\n         contains(\n-            isSingleWindowedValue(equalTo(5), 2, 0, 10),\n-            isSingleWindowedValue(equalTo(4), 4, 0, 10)));\n-    assertTrue(tester.isMarkedFinished(firstWindow));\n-    tester.assertHasOnlyGlobalAndFinishedSetsFor(firstWindow);\n+            isSingleWindowedValue(equalTo(0 + firstWindowSideInput), 1, 0, 2),\n+            isSingleWindowedValue(equalTo(0 + 1 + firstWindowSideInput), 1, 0, 2),\n+            isSingleWindowedValue(equalTo(2 + firstWindowSideInput), 3, 2, 4),\n+            isSingleWindowedValue(equalTo(2 + 3 + firstWindowSideInput), 3, 2, 4),\n+            isSingleWindowedValue(equalTo(4 + secondWindowSideInput), 5, 4, 6),\n+            isSingleWindowedValue(equalTo(4 + 5 + secondWindowSideInput), 5, 4, 6),\n+            isSingleWindowedValue(equalTo(6 + secondWindowSideInput), 7, 6, 8),\n+            isSingleWindowedValue(equalTo(6 + 7 + secondWindowSideInput), 7, 6, 8)));\n   }\n \n   @Test\n@@ -1424,7 +1442,8 @@ public void setGarbageCollectionHoldOnLateElements() throws Exception {\n     assertEquals(2, output.size());\n   }\n \n-  private static class SumAndVerifyContextFn extends CombineFnWithContext<Integer, int[], Integer> {\n+  private static class SumAndVerifyContextFn\n+      extends CombineFnWithContext<Integer, Integer, Integer> {\n \n     private final PCollectionView<Integer> view;\n     private final int expectedValue;\n@@ -1433,58 +1452,46 @@ private SumAndVerifyContextFn(PCollectionView<Integer> view, int expectedValue)\n       this.view = view;\n       this.expectedValue = expectedValue;\n     }\n-    @Override\n-    public int[] createAccumulator(Context c) {\n-      checkArgument(\n-          c.getPipelineOptions().as(TestOptions.class).getValue() == expectedValue);\n-      checkArgument(c.sideInput(view) == expectedValue);\n-      return wrap(0);\n+\n+    private void verifyContext(Context c) {\n+      assertThat(expectedValue, equalTo(c.getPipelineOptions().as(TestOptions.class).getValue()));\n+      assertThat(c.sideInput(view), greaterThanOrEqualTo(100));\n     }\n \n     @Override\n-    public int[] addInput(int[] accumulator, Integer input, Context c) {\n-      checkArgument(\n-          c.getPipelineOptions().as(TestOptions.class).getValue() == expectedValue);\n-      checkArgument(c.sideInput(view) == expectedValue);\n-      accumulator[0] += input.intValue();\n-      return accumulator;\n+    public Integer createAccumulator(Context c) {\n+      verifyContext(c);\n+      return 0;\n     }\n \n     @Override\n-    public int[] mergeAccumulators(Iterable<int[]> accumulators, Context c) {\n-      checkArgument(\n-          c.getPipelineOptions().as(TestOptions.class).getValue() == expectedValue);\n-      checkArgument(c.sideInput(view) == expectedValue);\n-      Iterator<int[]> iter = accumulators.iterator();\n-      if (!iter.hasNext()) {\n-        return createAccumulator(c);\n-      } else {\n-        int[] running = iter.next();\n-        while (iter.hasNext()) {\n-          running[0] += iter.next()[0];\n-        }\n-        return running;\n-      }\n+    public Integer addInput(Integer accumulator, Integer input, Context c) {\n+      verifyContext(c);\n+      return accumulator + input;\n     }\n \n     @Override\n-    public Integer extractOutput(int[] accumulator, Context c) {\n-      checkArgument(\n-          c.getPipelineOptions().as(TestOptions.class).getValue() == expectedValue);\n-      checkArgument(c.sideInput(view) == expectedValue);\n-      return accumulator[0];\n+    public Integer mergeAccumulators(Iterable<Integer> accumulators, Context c) {\n+      verifyContext(c);\n+      int res = 0;\n+      for (Integer accum : accumulators) {\n+        res += accum;\n+      }\n+      return res;\n     }\n \n-    private int[] wrap(int value) {\n-      return new int[] { value };\n+    @Override\n+    public Integer extractOutput(Integer accumulator, Context c) {\n+      verifyContext(c);\n+      return accumulator + c.sideInput(view);\n     }\n   }\n \n   /**\n    * A {@link PipelineOptions} to test combining with context.\n    */\n   public interface TestOptions extends PipelineOptions {\n-    Integer getValue();\n-    void setValue(Integer value);\n+    int getValue();\n+    void setValue(int value);\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnRunnerTest.java",
                "sha": "4abfc9ab1cc8bb658fc60a6d216d6ad71128c3d9",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 25,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "patch": "@@ -78,7 +78,6 @@\n import org.apache.beam.sdk.util.state.StateNamespaces;\n import org.apache.beam.sdk.util.state.StateTag;\n import org.apache.beam.sdk.util.state.TestInMemoryStateInternals;\n-import org.apache.beam.sdk.util.state.TimerCallback;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -100,7 +99,7 @@\n \n   private final TestInMemoryStateInternals<String> stateInternals =\n       new TestInMemoryStateInternals<>(KEY);\n-  private final TestTimerInternals timerInternals = new TestTimerInternals();\n+  private final InMemoryTimerInternals timerInternals = new InMemoryTimerInternals();\n \n   private final WindowFn<Object, W> windowFn;\n   private final TestOutputWindowedValue testOutputter;\n@@ -443,8 +442,29 @@ public int getOutputSize() {\n    * fire. Then advance the output watermark as far as possible.\n    */\n   public void advanceInputWatermark(Instant newInputWatermark) throws Exception {\n+    timerInternals.advanceInputWatermark(newInputWatermark);\n     ReduceFnRunner<String, InputT, OutputT, W> runner = createRunner();\n-    timerInternals.advanceInputWatermark(runner, newInputWatermark);\n+    while (true) {\n+      TimerData timer;\n+      List<TimerInternals.TimerData> timers = new ArrayList<>();\n+      while ((timer = timerInternals.removeNextEventTimer()) != null) {\n+        timers.add(timer);\n+      }\n+      if (timers.isEmpty()) {\n+        break;\n+      }\n+      runner.onTimers(timers);\n+    }\n+    if (autoAdvanceOutputWatermark) {\n+      Instant hold = stateInternals.earliestWatermarkHold();\n+      if (hold == null) {\n+        WindowTracing.trace(\n+            \"TestInMemoryTimerInternals.advanceInputWatermark: no holds, \"\n+                + \"so output watermark = input watermark\");\n+        hold = timerInternals.currentInputWatermarkTime();\n+      }\n+      advanceOutputWatermark(hold);\n+    }\n     runner.persist();\n   }\n \n@@ -458,18 +478,41 @@ public void advanceOutputWatermark(Instant newOutputWatermark) throws Exception\n \n   /** Advance the processing time to the specified time, firing any timers that should fire. */\n   public void advanceProcessingTime(Instant newProcessingTime) throws Exception {\n+    timerInternals.advanceProcessingTime(newProcessingTime);\n     ReduceFnRunner<String, InputT, OutputT, W> runner = createRunner();\n-    timerInternals.advanceProcessingTime(runner, newProcessingTime);\n+    while (true) {\n+      TimerData timer;\n+      List<TimerInternals.TimerData> timers = new ArrayList<>();\n+      while ((timer = timerInternals.removeNextProcessingTimer()) != null) {\n+        timers.add(timer);\n+      }\n+      if (timers.isEmpty()) {\n+        break;\n+      }\n+      runner.onTimers(timers);\n+    }\n     runner.persist();\n   }\n \n   /**\n    * Advance the synchronized processing time to the specified time,\n    * firing any timers that should fire.\n    */\n-  public void advanceSynchronizedProcessingTime(Instant newProcessingTime) throws Exception {\n+  public void advanceSynchronizedProcessingTime(\n+      Instant newSynchronizedProcessingTime) throws Exception {\n+    timerInternals.advanceSynchronizedProcessingTime(newSynchronizedProcessingTime);\n     ReduceFnRunner<String, InputT, OutputT, W> runner = createRunner();\n-    timerInternals.advanceSynchronizedProcessingTime(runner, newProcessingTime);\n+    while (true) {\n+      TimerData timer;\n+      List<TimerInternals.TimerData> timers = new ArrayList<>();\n+      while ((timer = timerInternals.removeNextSynchronizedProcessingTimer()) != null) {\n+        timers.add(timer);\n+      }\n+      if (timers.isEmpty()) {\n+        break;\n+      }\n+      runner.onTimers(timers);\n+    }\n     runner.persist();\n   }\n \n@@ -509,8 +552,10 @@ public final void injectElements(TimestampedValue<InputT>... values) throws Exce\n \n   public void fireTimer(W window, Instant timestamp, TimeDomain domain) throws Exception {\n     ReduceFnRunner<String, InputT, OutputT, W> runner = createRunner();\n-    runner.onTimer(\n+    ArrayList timers = new ArrayList(1);\n+    timers.add(\n         TimerData.of(StateNamespaces.window(windowFn.windowCoder(), window), timestamp, domain));\n+    runner.onTimers(timers);\n     runner.persist();\n   }\n \n@@ -601,22 +646,4 @@ public long getSum() {\n       return sum;\n     }\n   }\n-\n-  private class TestTimerInternals extends InMemoryTimerInternals {\n-    @Override\n-    public void advanceInputWatermark(TimerCallback timerCallback, Instant newInputWatermark)\n-        throws Exception {\n-      super.advanceInputWatermark(timerCallback, newInputWatermark);\n-      if (autoAdvanceOutputWatermark) {\n-        Instant hold = stateInternals.earliestWatermarkHold();\n-        if (hold == null) {\n-          WindowTracing.trace(\n-              \"TestInMemoryTimerInternals.advanceInputWatermark: no holds, \"\n-                  + \"so output watermark = input watermark\");\n-          hold = currentInputWatermarkTime();\n-        }\n-        advanceOutputWatermark(hold);\n-      }\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/ReduceFnTester.java",
                "sha": "db0cf9186a7d702cc8a49219b0eeed37be8d3540",
                "status": "modified"
            },
            {
                "additions": 301,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "changes": 301,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "patch": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.core;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertThat;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.BaseExecutionContext.StepContext;\n+import org.apache.beam.sdk.util.NullSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.Timer;\n+import org.apache.beam.sdk.util.TimerInternals;\n+import org.apache.beam.sdk.util.TimerInternals.TimerData;\n+import org.apache.beam.sdk.util.TimerSpec;\n+import org.apache.beam.sdk.util.TimerSpecs;\n+import org.apache.beam.sdk.util.UserCodeException;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.util.state.StateNamespaces;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.mockito.Mock;\n+import org.mockito.MockitoAnnotations;\n+\n+/** Tests for {@link SimpleDoFnRunner}. */\n+@RunWith(JUnit4.class)\n+public class SimpleDoFnRunnerTest {\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  @Mock StepContext mockStepContext;\n+\n+  @Mock TimerInternals mockTimerInternals;\n+\n+  @Before\n+  public void setup() {\n+    MockitoAnnotations.initMocks(this);\n+    when(mockStepContext.timerInternals()).thenReturn(mockTimerInternals);\n+  }\n+\n+  @Test\n+  public void testProcessElementExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.processElement(WindowedValue.valueInGlobalWindow(\"anyValue\"));\n+  }\n+\n+  @Test\n+  public void testOnTimerExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.onTimer(\n+        ThrowingDoFn.TIMER_ID,\n+        GlobalWindow.INSTANCE,\n+        new Instant(0),\n+        TimeDomain.EVENT_TIME);\n+  }\n+\n+  /**\n+   * Tests that a users call to set a timer gets properly dispatched to the timer internals. From\n+   * there on, it is the duty of the runner & step context to set it in whatever way is right for\n+   * that runner.\n+   */\n+  @Test\n+  public void testTimerSet() {\n+    WindowFn<?, ?> windowFn = new GlobalWindows();\n+    DoFnWithTimers<GlobalWindow> fn = new DoFnWithTimers(windowFn.windowCoder());\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    // Setting the timer needs the current time, as it is set relative\n+    Instant currentTime = new Instant(42);\n+    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(currentTime);\n+\n+    runner.processElement(WindowedValue.valueInGlobalWindow(\"anyValue\"));\n+\n+    verify(mockTimerInternals)\n+        .setTimer(\n+            StateNamespaces.window(new GlobalWindows().windowCoder(), GlobalWindow.INSTANCE),\n+            DoFnWithTimers.TIMER_ID,\n+            currentTime.plus(DoFnWithTimers.TIMER_OFFSET),\n+            TimeDomain.EVENT_TIME);\n+  }\n+\n+  @Test\n+  public void testStartBundleExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.startBundle();\n+  }\n+\n+  @Test\n+  public void testFinishBundleExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.finishBundle();\n+  }\n+\n+\n+  /**\n+   * Tests that {@link SimpleDoFnRunner#onTimer} properly dispatches to the underlying\n+   * {@link DoFn}.\n+   */\n+  @Test\n+  public void testOnTimerCalled() {\n+    WindowFn<?, GlobalWindow> windowFn = new GlobalWindows();\n+    DoFnWithTimers<GlobalWindow> fn = new DoFnWithTimers(windowFn.windowCoder());\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(windowFn));\n+\n+    Instant currentTime = new Instant(42);\n+    Duration offset = Duration.millis(37);\n+\n+    // Mocking is not easily compatible with annotation analysis, so we manually record\n+    // the method call.\n+    runner.onTimer(\n+        DoFnWithTimers.TIMER_ID,\n+        GlobalWindow.INSTANCE,\n+        currentTime.plus(offset),\n+        TimeDomain.EVENT_TIME);\n+\n+    assertThat(\n+        fn.onTimerInvocations,\n+        contains(\n+            TimerData.of(\n+                DoFnWithTimers.TIMER_ID,\n+                StateNamespaces.window(windowFn.windowCoder(), GlobalWindow.INSTANCE),\n+                currentTime.plus(offset),\n+                TimeDomain.EVENT_TIME)));\n+  }\n+\n+  static class ThrowingDoFn extends DoFn<String, String> {\n+    final Exception exceptionToThrow = new UnsupportedOperationException(\"Expected exception\");\n+\n+    static final String TIMER_ID = \"throwingTimerId\";\n+\n+    @TimerId(TIMER_ID)\n+    private static final TimerSpec timer = TimerSpecs.timer(TimeDomain.EVENT_TIME);\n+\n+    @StartBundle\n+    public void startBundle(Context c) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+\n+    @FinishBundle\n+    public void finishBundle(Context c) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+\n+    @ProcessElement\n+    public void processElement(ProcessContext c) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+\n+    @OnTimer(TIMER_ID)\n+    public void onTimer(OnTimerContext context) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+  }\n+\n+  private static class DoFnWithTimers<W extends BoundedWindow> extends DoFn<String, String> {\n+    static final String TIMER_ID = \"testTimerId\";\n+\n+    static final Duration TIMER_OFFSET = Duration.millis(100);\n+\n+    private final Coder<W> windowCoder;\n+\n+    // Mutable\n+    List<TimerData> onTimerInvocations;\n+\n+    DoFnWithTimers(Coder<W> windowCoder) {\n+      this.windowCoder = windowCoder;\n+      this.onTimerInvocations = new ArrayList<>();\n+    }\n+\n+    @TimerId(TIMER_ID)\n+    private static final TimerSpec timer = TimerSpecs.timer(TimeDomain.EVENT_TIME);\n+\n+    @ProcessElement\n+    public void process(ProcessContext context, @TimerId(TIMER_ID) Timer timer) {\n+      timer.setForNowPlus(TIMER_OFFSET);\n+    }\n+\n+    @OnTimer(TIMER_ID)\n+    public void onTimer(OnTimerContext context) {\n+      onTimerInvocations.add(\n+          TimerData.of(\n+              DoFnWithTimers.TIMER_ID,\n+              StateNamespaces.window(windowCoder, (W) context.window()),\n+              context.timestamp(),\n+              context.timeDomain()));\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "sha": "ec5d375117e907d7a3ad2b2a854fef515aa36a01",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "patch": "@@ -46,8 +46,6 @@\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.state.StateInternals;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/SplittableParDoTest.java",
                "sha": "cf96b660bea66f12eae0cc33a3a803a1707e2265",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/triggers/TriggerStateMachineTester.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/triggers/TriggerStateMachineTester.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 9,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/triggers/TriggerStateMachineTester.java",
                "patch": "@@ -32,14 +32,14 @@\n import java.util.Map;\n import java.util.Set;\n import javax.annotation.Nullable;\n+import org.apache.beam.runners.core.ActiveWindowSet;\n+import org.apache.beam.runners.core.ActiveWindowSet.MergeCallback;\n+import org.apache.beam.runners.core.MergingActiveWindowSet;\n+import org.apache.beam.runners.core.NonMergingActiveWindowSet;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.util.ActiveWindowSet;\n-import org.apache.beam.sdk.util.ActiveWindowSet.MergeCallback;\n-import org.apache.beam.sdk.util.MergingActiveWindowSet;\n-import org.apache.beam.sdk.util.NonMergingActiveWindowSet;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.Timers;\n@@ -53,7 +53,6 @@\n import org.apache.beam.sdk.util.state.StateNamespaces.WindowAndTriggerNamespace;\n import org.apache.beam.sdk.util.state.StateNamespaces.WindowNamespace;\n import org.apache.beam.sdk.util.state.TestInMemoryStateInternals;\n-import org.apache.beam.sdk.util.state.TimerCallback;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.joda.time.Duration;\n import org.joda.time.Instant;\n@@ -221,14 +220,22 @@ private StateNamespace windowNamespace(W window) {\n    * possible.\n    */\n   public void advanceInputWatermark(Instant newInputWatermark) throws Exception {\n-    // TODO: Should test timer firings: see https://issues.apache.org/jira/browse/BEAM-694\n-    timerInternals.advanceInputWatermark(TimerCallback.NO_OP, newInputWatermark);\n+    timerInternals.advanceInputWatermark(newInputWatermark);\n+    while (timerInternals.removeNextEventTimer() != null) {\n+      // TODO: Should test timer firings: see https://issues.apache.org/jira/browse/BEAM-694\n+    }\n   }\n \n   /** Advance the processing time to the specified time. */\n   public void advanceProcessingTime(Instant newProcessingTime) throws Exception {\n-    // TODO: Should test timer firings: see https://issues.apache.org/jira/browse/BEAM-694\n-    timerInternals.advanceProcessingTime(TimerCallback.NO_OP, newProcessingTime);\n+    timerInternals.advanceProcessingTime(newProcessingTime);\n+    while (timerInternals.removeNextProcessingTimer() != null) {\n+      // TODO: Should test timer firings: see https://issues.apache.org/jira/browse/BEAM-694\n+    }\n+    timerInternals.advanceSynchronizedProcessingTime(newProcessingTime);\n+    while (timerInternals.removeNextSynchronizedProcessingTimer() != null) {\n+      // TODO: Should test timer firings: see https://issues.apache.org/jira/browse/BEAM-694\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/core-java/src/test/java/org/apache/beam/runners/core/triggers/TriggerStateMachineTester.java",
                "sha": "be63c0644e27b6ebd22b0184ddbfdf94ae19148f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/pom.xml",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -68,6 +68,7 @@\n             </goals>\n             <configuration>\n               <groups>org.apache.beam.sdk.testing.NeedsRunner</groups>\n+              <excludedGroups>org.apache.beam.sdk.testing.UsesTimersInParDo</excludedGroups>\n               <parallel>none</parallel>\n               <failIfNoTests>true</failIfNoTests>\n               <dependenciesToScan>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/pom.xml",
                "sha": "5da569259bdff1ab837a145800cadd95e7cfba4b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/AggregatorContainer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/AggregatorContainer.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/AggregatorContainer.java",
                "patch": "@@ -27,8 +27,8 @@\n import java.util.concurrent.ConcurrentMap;\n import javax.annotation.Nullable;\n import javax.annotation.concurrent.GuardedBy;\n+import org.apache.beam.runners.core.AggregatorFactory;\n import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Aggregator.AggregatorFactory;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.util.ExecutionContext;\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/AggregatorContainer.java",
                "sha": "c7fa4df7cd8a23d260cb70f238b5c023156b32d8",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 16,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util.state;\n+package org.apache.beam.runners.direct;\n \n import static com.google.common.base.Preconditions.checkState;\n \n@@ -32,8 +32,24 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.util.CombineFnUtil;\n+import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.InMemoryStateInternals.InMemoryBag;\n+import org.apache.beam.sdk.util.state.InMemoryStateInternals.InMemoryCombiningValue;\n import org.apache.beam.sdk.util.state.InMemoryStateInternals.InMemoryState;\n+import org.apache.beam.sdk.util.state.InMemoryStateInternals.InMemoryStateBinder;\n+import org.apache.beam.sdk.util.state.InMemoryStateInternals.InMemoryValue;\n+import org.apache.beam.sdk.util.state.InMemoryStateInternals.InMemoryWatermarkHold;\n+import org.apache.beam.sdk.util.state.State;\n+import org.apache.beam.sdk.util.state.StateContext;\n+import org.apache.beam.sdk.util.state.StateContexts;\n+import org.apache.beam.sdk.util.state.StateInternals;\n+import org.apache.beam.sdk.util.state.StateNamespace;\n+import org.apache.beam.sdk.util.state.StateTable;\n+import org.apache.beam.sdk.util.state.StateTag;\n import org.apache.beam.sdk.util.state.StateTag.StateBinder;\n+import org.apache.beam.sdk.util.state.ValueState;\n+import org.apache.beam.sdk.util.state.WatermarkHoldState;\n import org.joda.time.Instant;\n \n /**\n@@ -262,11 +278,11 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n             if (containedInUnderlying(namespace, address)) {\n               @SuppressWarnings(\"unchecked\")\n               InMemoryState<? extends WatermarkHoldState<W>> existingState =\n-                  (InMemoryStateInternals.InMemoryState<? extends WatermarkHoldState<W>>)\n+                  (InMemoryState<? extends WatermarkHoldState<W>>)\n                   underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryStateInternals.InMemoryWatermarkHold<>(\n+              return new InMemoryWatermarkHold<>(\n                   outputTimeFn);\n             }\n           }\n@@ -277,11 +293,11 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n             if (containedInUnderlying(namespace, address)) {\n               @SuppressWarnings(\"unchecked\")\n               InMemoryState<? extends ValueState<T>> existingState =\n-                  (InMemoryStateInternals.InMemoryState<? extends ValueState<T>>)\n+                  (InMemoryState<? extends ValueState<T>>)\n                   underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryStateInternals.InMemoryValue<>();\n+              return new InMemoryValue<>();\n             }\n           }\n \n@@ -294,12 +310,11 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n               @SuppressWarnings(\"unchecked\")\n               InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT, OutputT>>\n                   existingState = (\n-                      InMemoryStateInternals\n-                          .InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n-                          OutputT>>) underlying.get().get(namespace, address, c);\n+                  InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n+                                            OutputT>>) underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryStateInternals.InMemoryCombiningValue<>(\n+              return new InMemoryCombiningValue<>(\n                   key, combineFn.asKeyedFn());\n             }\n           }\n@@ -310,11 +325,11 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n             if (containedInUnderlying(namespace, address)) {\n               @SuppressWarnings(\"unchecked\")\n               InMemoryState<? extends BagState<T>> existingState =\n-                  (InMemoryStateInternals.InMemoryState<? extends BagState<T>>)\n+                  (InMemoryState<? extends BagState<T>>)\n                   underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryStateInternals.InMemoryBag<>();\n+              return new InMemoryBag<>();\n             }\n           }\n \n@@ -328,12 +343,11 @@ private boolean containedInUnderlying(StateNamespace namespace, StateTag<? super\n               @SuppressWarnings(\"unchecked\")\n               InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT, OutputT>>\n                   existingState = (\n-                      InMemoryStateInternals\n-                          .InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n-                          OutputT>>) underlying.get().get(namespace, address, c);\n+                  InMemoryState<? extends AccumulatorCombiningState<InputT, AccumT,\n+                                            OutputT>>) underlying.get().get(namespace, address, c);\n               return existingState.copy();\n             } else {\n-              return new InMemoryStateInternals.InMemoryCombiningValue<>(key, combineFn);\n+              return new InMemoryCombiningValue<>(key, combineFn);\n             }\n           }\n \n@@ -446,7 +460,7 @@ public InMemoryStateBinderFactory(K key) {\n \n       @Override\n       public StateBinder<K> forNamespace(StateNamespace namespace, StateContext<?> c) {\n-        return new InMemoryStateInternals.InMemoryStateBinder<>(key, c);\n+        return new InMemoryStateBinder<>(key, c);\n       }\n     }\n   }",
                "previous_filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/util/state/CopyOnAccessInMemoryStateInternals.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternals.java",
                "sha": "e486a754a4ffb4c70f87a4c1d3790e03cb92e14d",
                "status": "renamed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectExecutionContext.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectExecutionContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectExecutionContext.java",
                "patch": "@@ -23,7 +23,6 @@\n import org.apache.beam.sdk.util.BaseExecutionContext;\n import org.apache.beam.sdk.util.ExecutionContext;\n import org.apache.beam.sdk.util.TimerInternals;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n \n /**\n  * Execution Context for the {@link DirectRunner}.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectExecutionContext.java",
                "sha": "c6051f08dfb15cf2007164a00f05a0171dcf7add",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "patch": "@@ -17,9 +17,10 @@\n  */\n package org.apache.beam.runners.direct;\n \n-import org.apache.beam.runners.core.SplittableParDo;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.SplittableParDo.GBKIntoKeyedWorkItems;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n \n@@ -30,10 +31,10 @@\n class DirectGBKIntoKeyedWorkItemsOverrideFactory<KeyT, InputT>\n     implements PTransformOverrideFactory<\n         PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>,\n-        SplittableParDo.GBKIntoKeyedWorkItems<KeyT, InputT>> {\n+        GBKIntoKeyedWorkItems<KeyT, InputT>> {\n   @Override\n   public PTransform<PCollection<KV<KeyT, InputT>>, PCollection<KeyedWorkItem<KeyT, InputT>>>\n-      override(SplittableParDo.GBKIntoKeyedWorkItems<KeyT, InputT> transform) {\n+      getReplacementTransform(GBKIntoKeyedWorkItems<KeyT, InputT> transform) {\n     return new DirectGroupByKey.DirectGroupByKeyOnly<>();\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGBKIntoKeyedWorkItemsOverrideFactory.java",
                "sha": "ab4c114965c743890c9f11febde4a80849015ae5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 13,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "patch": "@@ -79,13 +79,13 @@ public void leaveCompositeTransform(TransformHierarchy.Node node) {\n \n   @Override\n   public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n-    toFinalize.removeAll(node.getInput().expand());\n+    toFinalize.removeAll(node.getInputs());\n     AppliedPTransform<?, ?, ?> appliedTransform = getAppliedTransform(node);\n     stepNames.put(appliedTransform, genStepName());\n-    if (node.getInput().expand().isEmpty()) {\n+    if (node.getInputs().isEmpty()) {\n       rootTransforms.add(appliedTransform);\n     } else {\n-      for (PValue value : node.getInput().expand()) {\n+      for (PValue value : node.getInputs()) {\n         primitiveConsumers.put(value, appliedTransform);\n       }\n     }\n@@ -99,20 +99,17 @@ public void visitValue(PValue value, TransformHierarchy.Node producer) {\n     if (!producers.containsKey(value)) {\n       producers.put(value, appliedTransform);\n     }\n-    for (PValue expandedValue : value.expand()) {\n-      if (expandedValue instanceof PCollectionView) {\n-        views.add((PCollectionView<?>) expandedValue);\n-      }\n-      if (!producers.containsKey(expandedValue)) {\n-        producers.put(value, appliedTransform);\n-      }\n-    }\n+   if (value instanceof PCollectionView) {\n+     views.add((PCollectionView<?>) value);\n+   }\n+   if (!producers.containsKey(value)) {\n+     producers.put(value, appliedTransform);\n+   }\n   }\n \n   private AppliedPTransform<?, ?, ?> getAppliedTransform(TransformHierarchy.Node node) {\n     @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n-    AppliedPTransform<?, ?, ?> application = AppliedPTransform.of(\n-        node.getFullName(), node.getInput(), node.getOutput(), (PTransform) node.getTransform());\n+    AppliedPTransform<?, ?, ?> application = node.toAppliedPTransform();\n     return application;\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGraphVisitor.java",
                "sha": "0283d0352240dfec969f2b5603aea7b269ca81e8",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 21,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "patch": "@@ -20,13 +20,14 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n \n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItemCoder;\n+import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.IterableCoder;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItemCoder;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -45,10 +46,7 @@\n   }\n \n   @Override\n-  public PCollection<KV<K, Iterable<V>>> apply(PCollection<KV<K, V>> input) {\n-    @SuppressWarnings(\"unchecked\")\n-    KvCoder<K, V> inputCoder = (KvCoder<K, V>) input.getCoder();\n-\n+  public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n     // This operation groups by the combination of key and window,\n     // merging windows as needed, using the windows assigned to the\n     // key/value input elements and the window merge operation of the\n@@ -61,30 +59,32 @@\n     // By default, implement GroupByKey via a series of lower-level operations.\n     return input\n         .apply(new DirectGroupByKeyOnly<K, V>())\n-        .setCoder(\n-            KeyedWorkItemCoder.of(\n-                inputCoder.getKeyCoder(),\n-                inputCoder.getValueCoder(),\n-                inputWindowingStrategy.getWindowFn().windowCoder()))\n \n         // Group each key's values by window, merging windows as needed.\n         .apply(\n             \"GroupAlsoByWindow\",\n-            new DirectGroupAlsoByWindow<K, V>(inputWindowingStrategy, outputWindowingStrategy))\n-\n-        .setCoder(\n-            KvCoder.of(inputCoder.getKeyCoder(), IterableCoder.of(inputCoder.getValueCoder())));\n+            new DirectGroupAlsoByWindow<K, V>(inputWindowingStrategy, outputWindowingStrategy));\n   }\n \n   static final class DirectGroupByKeyOnly<K, V>\n       extends PTransform<PCollection<KV<K, V>>, PCollection<KeyedWorkItem<K, V>>> {\n     @Override\n-    public PCollection<KeyedWorkItem<K, V>> apply(PCollection<KV<K, V>> input) {\n+    public PCollection<KeyedWorkItem<K, V>> expand(PCollection<KV<K, V>> input) {\n       return PCollection.createPrimitiveOutputInternal(\n           input.getPipeline(), WindowingStrategy.globalDefault(), input.isBounded());\n     }\n \n     DirectGroupByKeyOnly() {}\n+\n+    @Override\n+    protected Coder<?> getDefaultOutputCoder(\n+        @SuppressWarnings(\"unused\") PCollection<KV<K, V>> input)\n+        throws CannotProvideCoderException {\n+      return KeyedWorkItemCoder.of(\n+          GroupByKey.getKeyCoder(input.getCoder()),\n+          GroupByKey.getInputValueCoder(input.getCoder()),\n+          input.getWindowingStrategy().getWindowFn().windowCoder());\n+    }\n   }\n \n   static final class DirectGroupAlsoByWindow<K, V>\n@@ -117,16 +117,20 @@ public DirectGroupAlsoByWindow(\n       return kvCoder;\n     }\n \n-    public Coder<K> getKeyCoder(Coder<KeyedWorkItem<K, V>> inputCoder) {\n-      return getKeyedWorkItemCoder(inputCoder).getKeyCoder();\n-    }\n-\n     public Coder<V> getValueCoder(Coder<KeyedWorkItem<K, V>> inputCoder) {\n       return getKeyedWorkItemCoder(inputCoder).getElementCoder();\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> apply(PCollection<KeyedWorkItem<K, V>> input) {\n+    protected Coder<?> getDefaultOutputCoder(\n+        @SuppressWarnings(\"unused\") PCollection<KeyedWorkItem<K, V>> input)\n+        throws CannotProvideCoderException {\n+      KeyedWorkItemCoder<K, V> inputCoder = getKeyedWorkItemCoder(input.getCoder());\n+      return KvCoder.of(inputCoder.getKeyCoder(), IterableCoder.of(inputCoder.getElementCoder()));\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KeyedWorkItem<K, V>> input) {\n       return PCollection.createPrimitiveOutputInternal(\n           input.getPipeline(), outputWindowingStrategy, input.isBounded());\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKey.java",
                "sha": "6c10bd249b42e079e2a243c2b32ef8a558ba1023",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.KV;\n@@ -27,7 +28,7 @@\n     implements PTransformOverrideFactory<\n         PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupByKey<K, V>> {\n   @Override\n-  public PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> override(\n+  public PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> getReplacementTransform(\n       GroupByKey<K, V> transform) {\n     return new DirectGroupByKey<>(transform);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectGroupByKeyOverrideFactory.java",
                "sha": "7cf325686a20e4085f4a8b0e8b33988971fc73c3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "patch": "@@ -46,6 +46,7 @@\n import org.apache.beam.sdk.metrics.MetricResults;\n import org.apache.beam.sdk.metrics.MetricsEnvironment;\n import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.transforms.Aggregator;\n@@ -284,9 +285,11 @@ void setClockSupplier(Supplier<Clock> supplier) {\n   @Override\n   public <OutputT extends POutput, InputT extends PInput> OutputT apply(\n       PTransform<InputT, OutputT> transform, InputT input) {\n-    PTransformOverrideFactory overrideFactory = defaultTransformOverrides.get(transform.getClass());\n+    PTransformOverrideFactory<InputT, OutputT, PTransform<InputT, OutputT>> overrideFactory =\n+        defaultTransformOverrides.get(transform.getClass());\n     if (overrideFactory != null) {\n-      PTransform<InputT, OutputT> customTransform = overrideFactory.override(transform);\n+      PTransform<InputT, OutputT> customTransform =\n+          overrideFactory.getReplacementTransform(transform);\n       if (customTransform != transform) {\n         return Pipeline.applyTransform(transform.getName(), input, customTransform);\n       }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/DirectRunner.java",
                "sha": "78163c0d7e23ac6553bcda3f241c5881f0cced61",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "patch": "@@ -48,7 +48,6 @@\n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollection.IsBounded;\n import org.apache.beam.sdk.values.PCollectionView;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/EvaluationContext.java",
                "sha": "230d91b52dff1a898a277c2370060c5986135fe2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "patch": "@@ -43,13 +43,13 @@\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.concurrent.atomic.AtomicReference;\n import javax.annotation.Nullable;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.WatermarkManager.FiredTimers;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ExecutorServiceParallelExecutor.java",
                "sha": "a30829520d9f252b711d2b35fbf18d097d8f2fea",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ForwardingPTransform.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ForwardingPTransform.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ForwardingPTransform.java",
                "patch": "@@ -28,15 +28,15 @@\n /**\n  * A base class for implementing {@link PTransform} overrides, which behave identically to the\n  * delegate transform but with overridden methods. Implementors are required to implement\n- * {@link #delegate()}, which returns the object to forward calls to, and {@link #apply(PInput)}.\n+ * {@link #delegate()}, which returns the object to forward calls to, and {@link #expand(PInput)}.\n  */\n public abstract class ForwardingPTransform<InputT extends PInput, OutputT extends POutput>\n     extends PTransform<InputT, OutputT> {\n   protected abstract PTransform<InputT, OutputT> delegate();\n \n   @Override\n-  public OutputT apply(InputT input) {\n-    return delegate().apply(input);\n+  public OutputT expand(InputT input) {\n+    return delegate().expand(input);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ForwardingPTransform.java",
                "sha": "97c09838a1a5c20a6fce8863cc1c6caf6afb1675",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 6,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.beam.runners.core.GroupByKeyViaGroupByKeyOnly;\n import org.apache.beam.runners.core.GroupByKeyViaGroupByKeyOnly.GroupAlsoByWindow;\n import org.apache.beam.runners.core.GroupByKeyViaGroupByKeyOnly.GroupByKeyOnly;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.OutputWindowedValue;\n import org.apache.beam.runners.core.ReduceFnRunner;\n import org.apache.beam.runners.core.SystemReduceFn;\n@@ -44,14 +45,11 @@\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.TimerInternals;\n-import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.WindowTracing;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n@@ -201,9 +199,7 @@ public boolean isEmpty() {\n       // Drop any elements within expired windows\n       reduceFnRunner.processElements(\n           dropExpiredWindows(key, workItem.elementsIterable(), timerInternals));\n-      for (TimerData timer : workItem.timersIterable()) {\n-        reduceFnRunner.onTimer(timer);\n-      }\n+      reduceFnRunner.onTimers(workItem.timersIterable());\n       reduceFnRunner.persist();\n     }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupAlsoByWindowEvaluatorFactory.java",
                "sha": "bb11923fd1cd93a23d59823a1a735556619d4afb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "patch": "@@ -27,6 +27,8 @@\n import java.util.Map;\n import org.apache.beam.runners.core.GroupByKeyViaGroupByKeyOnly;\n import org.apache.beam.runners.core.GroupByKeyViaGroupByKeyOnly.GroupByKeyOnly;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.direct.DirectGroupByKey.DirectGroupByKeyOnly;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n@@ -36,8 +38,6 @@\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactory.java",
                "sha": "20d619fbc592efdba57bba01411a971448d1f15c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "patch": "@@ -74,7 +74,7 @@ public void leaveCompositeTransform(TransformHierarchy.Node node) {\n     if (node.isRootNode()) {\n       finalized = true;\n     } else if (producesKeyedOutputs.contains(node.getTransform().getClass())) {\n-      keyedValues.addAll(node.getOutput().expand());\n+      keyedValues.addAll(node.getOutputs());\n     }\n   }\n \n@@ -84,7 +84,7 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {}\n   @Override\n   public void visitValue(PValue value, TransformHierarchy.Node producer) {\n     if (producesKeyedOutputs.contains(producer.getTransform().getClass())) {\n-      keyedValues.addAll(value.expand());\n+      keyedValues.add(value);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitor.java",
                "sha": "7f85169478a1cef4c690424c8c5798ac85ba6832",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluator.java",
                "sha": "a915cf0bc9900a06892f903e55a07ad83dc8caa6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "patch": "@@ -65,7 +65,7 @@ public DoFnLifecycleManager load(DoFn<?, ?> key) throws Exception {\n                 application;\n \n     ParDo.BoundMulti<InputT, OutputT> transform = parDoApplication.getTransform();\n-    final DoFn<InputT, OutputT> doFn = transform.getNewFn();\n+    final DoFn<InputT, OutputT> doFn = transform.getFn();\n \n     @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n     TransformEvaluator<T> evaluator =",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoEvaluatorFactory.java",
                "sha": "b4684e34b26680005bb8f63bb02febb30ca05dab",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "patch": "@@ -20,10 +20,12 @@\n import org.apache.beam.runners.core.SplittableParDo;\n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.ParDo.BoundMulti;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.values.KV;\n@@ -39,20 +41,27 @@\n  */\n class ParDoMultiOverrideFactory<InputT, OutputT>\n     implements PTransformOverrideFactory<\n-        PCollection<? extends InputT>, PCollectionTuple, ParDo.BoundMulti<InputT, OutputT>> {\n-\n+        PCollection<? extends InputT>, PCollectionTuple, BoundMulti<InputT, OutputT>> {\n   @Override\n   @SuppressWarnings(\"unchecked\")\n-  public PTransform<PCollection<? extends InputT>, PCollectionTuple> override(\n-      ParDo.BoundMulti<InputT, OutputT> transform) {\n+  public PTransform<PCollection<? extends InputT>, PCollectionTuple> getReplacementTransform(\n+      BoundMulti<InputT, OutputT> transform) {\n \n-    DoFn<InputT, OutputT> fn = transform.getNewFn();\n+    DoFn<InputT, OutputT> fn = transform.getFn();\n     DoFnSignature signature = DoFnSignatures.getSignature(fn.getClass());\n     if (signature.processElement().isSplittable()) {\n       return new SplittableParDo(transform);\n-    } else if (signature.stateDeclarations().size() > 0\n-        || signature.timerDeclarations().size() > 0) {\n-\n+    } else if (signature.timerDeclarations().size() > 0) {\n+      // Temporarily actually reject timers\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n+              DoFn.TimerId.class.getSimpleName(),\n+              fn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              DirectRunner.class.getSimpleName()));\n+\n+    } else if (signature.stateDeclarations().size() > 0) {\n       // Based on the fact that the signature is stateful, DoFnSignatures ensures\n       // that it is also keyed\n       ParDo.BoundMulti<KV<?, ?>, OutputT> keyedTransform =\n@@ -73,7 +82,7 @@ public GbkThenStatefulParDo(ParDo.BoundMulti<KV<K, InputT>, OutputT> underlyingP\n     }\n \n     @Override\n-    public PCollectionTuple apply(PCollection<KV<K, InputT>> input) {\n+    public PCollectionTuple expand(PCollection<KV<K, InputT>> input) {\n \n       PCollectionTuple outputs = input\n           .apply(\"Group by key\", GroupByKey.<K, InputT>create())\n@@ -106,7 +115,7 @@ public StatefulParDo(\n       return underlyingParDo.getDefaultOutputCoder(originalInput, output);\n     }\n \n-    public PCollectionTuple apply(PCollection<? extends KV<K, Iterable<InputT>>> input) {\n+    public PCollectionTuple expand(PCollection<? extends KV<K, Iterable<InputT>>> input) {\n \n       PCollectionTuple outputs = PCollectionTuple.ofPrimitiveOutputsInternal(\n           input.getPipeline(),",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoMultiOverrideFactory.java",
                "sha": "c5bc0698cad7cbab19ffa13509f9e0f9f6e7b9ff",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 8,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "patch": "@@ -17,8 +17,10 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.ParDo.Bound;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionTuple;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -30,12 +32,11 @@\n  */\n class ParDoSingleViaMultiOverrideFactory<InputT, OutputT>\n     implements PTransformOverrideFactory<\n-        PCollection<? extends InputT>, PCollection<OutputT>, ParDo.Bound<InputT, OutputT>> {\n+        PCollection<? extends InputT>, PCollection<OutputT>, Bound<InputT, OutputT>>{\n   @Override\n-  @SuppressWarnings(\"unchecked\")\n-  public PTransform<PCollection<? extends InputT>, PCollection<OutputT>> override(\n-      ParDo.Bound<InputT, OutputT> transform) {\n-    return new ParDoSingleViaMulti(transform);\n+  public PTransform<PCollection<? extends InputT>, PCollection<OutputT>> getReplacementTransform(\n+      Bound<InputT, OutputT> transform) {\n+    return new ParDoSingleViaMulti<>(transform);\n   }\n \n   static class ParDoSingleViaMulti<InputT, OutputT>\n@@ -49,19 +50,19 @@ public ParDoSingleViaMulti(ParDo.Bound<InputT, OutputT> underlyingParDo) {\n     }\n \n     @Override\n-    public PCollection<OutputT> apply(PCollection<? extends InputT> input) {\n+    public PCollection<OutputT> expand(PCollection<? extends InputT> input) {\n \n       // Output tags for ParDo need only be unique up to applied transform\n       TupleTag<OutputT> mainOutputTag = new TupleTag<OutputT>(MAIN_OUTPUT_TAG);\n \n       PCollectionTuple outputs =\n           input.apply(\n-              ParDo.of(underlyingParDo.getNewFn())\n+              ParDo.of(underlyingParDo.getFn())\n                   .withSideInputs(underlyingParDo.getSideInputs())\n                   .withOutputTags(mainOutputTag, TupleTagList.empty()));\n       PCollection<OutputT> output = outputs.get(mainOutputTag);\n \n-      output.setTypeDescriptorInternal(underlyingParDo.getNewFn().getOutputTypeDescriptor());\n+      output.setTypeDescriptor(underlyingParDo.getFn().getOutputTypeDescriptor());\n       return output;\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ParDoSingleViaMultiOverrideFactory.java",
                "sha": "3ae338285e56b4bc42221f3f6954152378166cd7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "patch": "@@ -20,13 +20,13 @@\n import java.util.Collection;\n import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n import org.apache.beam.runners.core.ElementAndRestriction;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.runners.core.OutputWindowedValue;\n import org.apache.beam.runners.core.SplittableParDo;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.state.StateInternals;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/SplittableProcessElementsEvaluatorFactory.java",
                "sha": "aae1149f99e1531bd9223c9f9ce5a16150156cf1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "patch": "@@ -86,7 +86,7 @@ public void cleanup() throws Exception {\n       throws Exception {\n \n     final DoFn<KV<K, InputT>, OutputT> doFn =\n-        application.getTransform().getUnderlyingParDo().getNewFn();\n+        application.getTransform().getUnderlyingParDo().getFn();\n     final DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n \n     // If the DoFn is stateful, schedule state clearing.\n@@ -141,7 +141,7 @@ public Runnable load(\n       WindowingStrategy<?, ?> windowingStrategy = pc.getWindowingStrategy();\n       BoundedWindow window = transformOutputWindow.getWindow();\n       final DoFn<?, ?> doFn =\n-          transformOutputWindow.getTransform().getTransform().getUnderlyingParDo().getNewFn();\n+          transformOutputWindow.getTransform().getTransform().getUnderlyingParDo().getFn();\n       final DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n \n       final DirectStepContext stepContext =",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactory.java",
                "sha": "1f64d9ac705983f757fe71dea703578188e678c4",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StepTransformResult.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StepTransformResult.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/StepTransformResult.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n import org.joda.time.Instant;\n \n /**",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/StepTransformResult.java",
                "sha": "01b2a7261f0727c61b47c804d20ee6b7576866b3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "patch": "@@ -30,6 +30,7 @@\n import javax.annotation.Nullable;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.testing.TestStream;\n import org.apache.beam.sdk.testing.TestStream.ElementEvent;\n@@ -157,8 +158,10 @@ public Clock get() {\n \n   static class DirectTestStreamFactory<T>\n       implements PTransformOverrideFactory<PBegin, PCollection<T>, TestStream<T>> {\n+\n     @Override\n-    public PTransform<PBegin, PCollection<T>> override(TestStream<T> transform) {\n+    public PTransform<PBegin, PCollection<T>> getReplacementTransform(\n+        TestStream<T> transform) {\n       return new DirectTestStream<>(transform);\n     }\n \n@@ -170,7 +173,7 @@ private DirectTestStream(TestStream<T> transform) {\n       }\n \n       @Override\n-      public PCollection<T> apply(PBegin input) {\n+      public PCollection<T> expand(PBegin input) {\n         PipelineRunner<?> runner = input.getPipeline().getRunner();\n         checkState(\n             runner instanceof DirectRunner,",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactory.java",
                "sha": "6ba65bf008e791b0e31b6990c3ec8a8297248e5b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformResult.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformResult.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformResult.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n import org.joda.time.Instant;\n \n /**",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/TransformResult.java",
                "sha": "8bb5f9355860239cbcd38b61244c7bdd2941d790",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.beam.runners.direct.StepTransformResult.Builder;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.runners.PTransformOverrideFactory;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.PTransform;\n@@ -96,8 +97,9 @@ public void processElement(WindowedValue<Iterable<InT>> element) {\n   public static class ViewOverrideFactory<ElemT, ViewT>\n       implements PTransformOverrideFactory<\n           PCollection<ElemT>, PCollectionView<ViewT>, CreatePCollectionView<ElemT, ViewT>> {\n+\n     @Override\n-    public PTransform<PCollection<ElemT>, PCollectionView<ViewT>> override(\n+    public PTransform<PCollection<ElemT>, PCollectionView<ViewT>> getReplacementTransform(\n         CreatePCollectionView<ElemT, ViewT> transform) {\n       return new DirectCreatePCollectionView<>(transform);\n     }\n@@ -115,7 +117,7 @@ private DirectCreatePCollectionView(CreatePCollectionView<ElemT, ViewT> og) {\n     }\n \n     @Override\n-    public PCollectionView<ViewT> apply(PCollection<ElemT> input) {\n+    public PCollectionView<ViewT> expand(PCollection<ElemT> input) {\n       return input.apply(WithKeys.<Void, ElemT>of((Void) null))\n           .setCoder(KvCoder.of(VoidCoder.of(), input.getCoder()))\n           .apply(GroupByKey.<Void, ElemT>create())\n@@ -145,7 +147,7 @@ private DirectCreatePCollectionView(CreatePCollectionView<ElemT, ViewT> og) {\n     }\n \n     @Override\n-    public PCollectionView<ViewT> apply(PCollection<Iterable<ElemT>> input) {\n+    public PCollectionView<ViewT> expand(PCollection<Iterable<ElemT>> input) {\n       return og.getView();\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/ViewEvaluatorFactory.java",
                "sha": "96a18d77f46861cfe2931606af5c418472f6734d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "patch": "@@ -47,11 +47,13 @@\n  * of shards is the log base 10 of the number of input records, with up to 2 additional shards.\n  */\n class WriteWithShardingFactory<InputT>\n-    implements PTransformOverrideFactory<PCollection<InputT>, PDone, Write.Bound<InputT>> {\n+    implements org.apache.beam.sdk.runners.PTransformOverrideFactory<\n+        PCollection<InputT>, PDone, Write.Bound<InputT>> {\n   static final int MAX_RANDOM_EXTRA_SHARDS = 3;\n \n   @Override\n-  public PTransform<PCollection<InputT>, PDone> override(Write.Bound<InputT> transform) {\n+  public PTransform<PCollection<InputT>, PDone> getReplacementTransform(\n+      Bound<InputT> transform) {\n     if (transform.getNumShards() == 0) {\n       return new DynamicallyReshardedWrite<>(transform);\n     }\n@@ -66,7 +68,7 @@ private DynamicallyReshardedWrite(Bound<T> original) {\n     }\n \n     @Override\n-    public PDone apply(PCollection<T> input) {\n+    public PDone expand(PCollection<T> input) {\n       checkArgument(IsBounded.BOUNDED == input.isBounded(),\n           \"%s can only be applied to a Bounded PCollection\",\n           getClass().getSimpleName());\n@@ -92,7 +94,7 @@ public PDone apply(PCollection<T> input) {\n       // without adding a new Write Transform Node, which would be overwritten the same way, leading\n       // to an infinite recursion. We cannot modify the number of shards, because that is determined\n       // at runtime.\n-      return original.apply(resharded);\n+      return original.expand(resharded);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/main/java/org/apache/beam/runners/direct/WriteWithShardingFactory.java",
                "sha": "fd1c1751e54ce4a8c48eb93800aab9faff519b2d",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 8,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "patch": "@@ -80,6 +80,7 @@\n   private BoundedReadEvaluatorFactory factory;\n   @Mock private EvaluationContext context;\n   private BundleFactory bundleFactory;\n+  private AppliedPTransform<?, ?, ?> longsProducer;\n \n   @Before\n   public void setup() {\n@@ -92,6 +93,7 @@ public void setup() {\n         new BoundedReadEvaluatorFactory(\n             context, Long.MAX_VALUE /* minimum size for dynamic splits */);\n     bundleFactory = ImmutableListBundleFactory.create();\n+    longsProducer = DirectGraphs.getProducer(longs);\n   }\n \n   @Test\n@@ -102,11 +104,11 @@ public void boundedSourceInMemoryTransformEvaluatorProducesElements() throws Exc\n \n     Collection<CommittedBundle<?>> initialInputs =\n         new BoundedReadEvaluatorFactory.InputProvider(context)\n-            .getInitialInputs(longs.getProducingTransformInternal(), 1);\n+            .getInitialInputs(longsProducer, 1);\n     List<WindowedValue<?>> outputs = new ArrayList<>();\n     for (CommittedBundle<?> shardBundle : initialInputs) {\n       TransformEvaluator<?> evaluator =\n-          factory.forApplication(longs.getProducingTransformInternal(), null);\n+          factory.forApplication(longsProducer, null);\n       for (WindowedValue<?> shard : shardBundle.getElements()) {\n         evaluator.processElement((WindowedValue) shard);\n       }\n@@ -141,7 +143,7 @@ public void boundedSourceEvaluatorProducesDynamicSplits() throws Exception {\n     }\n     PCollection<Long> read =\n         TestPipeline.create().apply(Read.from(new TestSource<>(VarLongCoder.of(), 5, elems)));\n-    AppliedPTransform<?, ?, ?> transform = read.getProducingTransformInternal();\n+    AppliedPTransform<?, ?, ?> transform = DirectGraphs.getProducer(read);\n     Collection<CommittedBundle<?>> unreadInputs =\n         new BoundedReadEvaluatorFactory.InputProvider(context).getInitialInputs(transform, 1);\n \n@@ -191,7 +193,7 @@ public void boundedSourceEvaluatorDynamicSplitsUnsplittable() throws Exception {\n     PCollection<Long> read =\n         TestPipeline.create()\n             .apply(Read.from(SourceTestUtils.toUnsplittableSource(CountingSource.upTo(10L))));\n-    AppliedPTransform<?, ?, ?> transform = read.getProducingTransformInternal();\n+    AppliedPTransform<?, ?, ?> transform = DirectGraphs.getProducer(read);\n \n     when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n     when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n@@ -238,7 +240,7 @@ public void getInitialInputsSplitsIntoBundles() throws Exception {\n             });\n     Collection<CommittedBundle<?>> initialInputs =\n         new BoundedReadEvaluatorFactory.InputProvider(context)\n-            .getInitialInputs(longs.getProducingTransformInternal(), 3);\n+            .getInitialInputs(longsProducer, 3);\n \n     assertThat(initialInputs, hasSize(allOf(greaterThanOrEqualTo(3), lessThanOrEqualTo(4))));\n \n@@ -271,7 +273,7 @@ public void boundedSourceInMemoryTransformEvaluatorShardsOfSource() throws Excep\n     CommittedBundle<BoundedSourceShard<Long>> shards = rootBundle.commit(Instant.now());\n \n     TransformEvaluator<BoundedSourceShard<Long>> evaluator =\n-        factory.forApplication(longs.getProducingTransformInternal(), shards);\n+        factory.forApplication(longsProducer, shards);\n     for (WindowedValue<BoundedSourceShard<Long>> shard : shards.getElements()) {\n       UncommittedBundle<Long> outputBundle = bundleFactory.createBundle(longs);\n       when(context.createBundle(longs)).thenReturn(outputBundle);\n@@ -299,7 +301,7 @@ public void boundedSourceEvaluatorClosesReader() throws Exception {\n \n     TestPipeline p = TestPipeline.create();\n     PCollection<Long> pcollection = p.apply(Read.from(source));\n-    AppliedPTransform<?, ?, ?> sourceTransform = pcollection.getProducingTransformInternal();\n+    AppliedPTransform<?, ?, ?> sourceTransform = DirectGraphs.getProducer(pcollection);\n \n     UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);\n     when(context.createBundle(pcollection)).thenReturn(output);\n@@ -320,7 +322,7 @@ public void boundedSourceEvaluatorNoElementsClosesReader() throws Exception {\n \n     TestPipeline p = TestPipeline.create();\n     PCollection<Long> pcollection = p.apply(Read.from(source));\n-    AppliedPTransform<?, ?, ?> sourceTransform = pcollection.getProducingTransformInternal();\n+    AppliedPTransform<?, ?, ?> sourceTransform = DirectGraphs.getProducer(pcollection);\n \n     UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);\n     when(context.createBundle(pcollection)).thenReturn(output);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/BoundedReadEvaluatorFactoryTest.java",
                "sha": "acb14441b4073cfc1fc1119e90c0e2c305d1f769",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CommittedResultTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CommittedResultTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/CommittedResultTest.java",
                "patch": "@@ -52,7 +52,7 @@\n   private transient AppliedPTransform<?, ?, ?> transform =\n       AppliedPTransform.of(\"foo\", p.begin(), PDone.in(p), new PTransform<PBegin, PDone>() {\n         @Override\n-        public PDone apply(PBegin begin) {\n+        public PDone expand(PBegin begin) {\n           throw new IllegalArgumentException(\"Should never be applied\");\n         }\n       });",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CommittedResultTest.java",
                "sha": "c6986c0e6774050320eecc0f2882316c52bc21b6",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "patch": "@@ -15,7 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-package org.apache.beam.sdk.util.state;\n+package org.apache.beam.runners.direct;\n \n import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.emptyIterable;\n@@ -39,6 +39,16 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFns;\n+import org.apache.beam.sdk.util.state.AccumulatorCombiningState;\n+import org.apache.beam.sdk.util.state.BagState;\n+import org.apache.beam.sdk.util.state.CombiningState;\n+import org.apache.beam.sdk.util.state.StateNamespace;\n+import org.apache.beam.sdk.util.state.StateNamespaceForTest;\n+import org.apache.beam.sdk.util.state.StateNamespaces;\n+import org.apache.beam.sdk.util.state.StateTag;\n+import org.apache.beam.sdk.util.state.StateTags;\n+import org.apache.beam.sdk.util.state.ValueState;\n+import org.apache.beam.sdk.util.state.WatermarkHoldState;\n import org.joda.time.Instant;\n import org.junit.Rule;\n import org.junit.Test;",
                "previous_filename": "sdks/java/core/src/test/java/org/apache/beam/sdk/util/state/CopyOnAccessInMemoryStateInternalsTest.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/CopyOnAccessInMemoryStateInternalsTest.java",
                "sha": "deefc68489dc4d56683ca6cbc8202d9f29fe7739",
                "status": "renamed"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 23,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "patch": "@@ -19,25 +19,34 @@\n \n import static org.hamcrest.Matchers.emptyIterable;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n import static org.hamcrest.Matchers.is;\n import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertTrue;\n \n+import com.google.common.collect.Iterables;\n import java.io.Serializable;\n+import java.util.ArrayList;\n import java.util.List;\n import org.apache.beam.sdk.io.CountingInput;\n+import org.apache.beam.sdk.io.CountingSource;\n+import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n+import org.apache.beam.sdk.transforms.Flatten.FlattenPCollectionList;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View;\n+import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PDone;\n import org.apache.beam.sdk.values.PInput;\n+import org.apache.beam.sdk.values.POutput;\n import org.hamcrest.Matchers;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -80,26 +89,36 @@ public void processElement(DoFn<String, String>.ProcessContext c)\n   @Test\n   public void getRootTransformsContainsPBegins() {\n     PCollection<String> created = p.apply(Create.of(\"foo\", \"bar\"));\n-    PCollection<Long> counted = p.apply(CountingInput.upTo(1234L));\n+    PCollection<Long> counted = p.apply(Read.from(CountingSource.upTo(1234L)));\n     PCollection<Long> unCounted = p.apply(CountingInput.unbounded());\n     p.traverseTopologically(visitor);\n+    DirectGraph graph = visitor.getGraph();\n+    assertThat(graph.getRootTransforms(), hasSize(3));\n+    List<PTransform<?, ?>> unapplied = new ArrayList<>();\n     assertThat(\n-        visitor.getGraph().getRootTransforms(),\n+        graph.getRootTransforms(),\n         Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(\n-            created.getProducingTransformInternal(),\n-            counted.getProducingTransformInternal(),\n-            unCounted.getProducingTransformInternal()));\n+            graph.getProducer(created), graph.getProducer(counted), graph.getProducer(unCounted)));\n+    for (AppliedPTransform<?, ?, ?> root : graph.getRootTransforms())  {\n+      assertTrue(root.getInput() instanceof PBegin);\n+      assertThat(root.getOutput(), Matchers.<POutput>isOneOf(created, counted, unCounted));\n+    }\n   }\n \n   @Test\n   public void getRootTransformsContainsEmptyFlatten() {\n-    PCollection<String> empty =\n-        PCollectionList.<String>empty(p).apply(Flatten.<String>pCollections());\n+    FlattenPCollectionList<String> flatten = Flatten.pCollections();\n+    PCollectionList<String> emptyList = PCollectionList.empty(p);\n+    PCollection<String> empty = emptyList.apply(flatten);\n     p.traverseTopologically(visitor);\n+    DirectGraph graph = visitor.getGraph();\n     assertThat(\n-        visitor.getGraph().getRootTransforms(),\n-        Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(\n-            empty.getProducingTransformInternal()));\n+        graph.getRootTransforms(),\n+        Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(graph.getProducer(empty)));\n+    AppliedPTransform<?, ?, ?> onlyRoot = Iterables.getOnlyElement(graph.getRootTransforms());\n+    assertThat(onlyRoot.getTransform(), Matchers.<PTransform<?, ?>>equalTo(flatten));\n+    assertThat(onlyRoot.getInput(), Matchers.<PInput>equalTo(emptyList));\n+    assertThat(onlyRoot.getOutput(), Matchers.<POutput>equalTo(empty));\n   }\n \n   @Test\n@@ -121,16 +140,20 @@ public void processElement(DoFn<String, String>.ProcessContext c)\n \n     p.traverseTopologically(visitor);\n \n+    DirectGraph graph = visitor.getGraph();\n+    AppliedPTransform<?, ?, ?> transformedProducer =\n+        graph.getProducer(transformed);\n+    AppliedPTransform<?, ?, ?> flattenedProducer =\n+        graph.getProducer(flattened);\n+\n     assertThat(\n-        visitor.getGraph().getPrimitiveConsumers(created),\n+        graph.getPrimitiveConsumers(created),\n         Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(\n-            transformed.getProducingTransformInternal(),\n-            flattened.getProducingTransformInternal()));\n+            transformedProducer, flattenedProducer));\n     assertThat(\n-        visitor.getGraph().getPrimitiveConsumers(transformed),\n-        Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(\n-            flattened.getProducingTransformInternal()));\n-    assertThat(visitor.getGraph().getPrimitiveConsumers(flattened), emptyIterable());\n+        graph.getPrimitiveConsumers(transformed),\n+        Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(flattenedProducer));\n+    assertThat(graph.getPrimitiveConsumers(flattened), emptyIterable());\n   }\n \n   @Test\n@@ -142,12 +165,14 @@ public void getValueToConsumersWithDuplicateInputSucceeds() {\n \n     p.traverseTopologically(visitor);\n \n+    DirectGraph graph = visitor.getGraph();\n+    AppliedPTransform<?, ?, ?> flattenedProducer = graph.getProducer(flattened);\n+\n     assertThat(\n-        visitor.getGraph().getPrimitiveConsumers(created),\n-        Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(\n-            flattened.getProducingTransformInternal(),\n-            flattened.getProducingTransformInternal()));\n-    assertThat(visitor.getGraph().getPrimitiveConsumers(flattened), emptyIterable());\n+        graph.getPrimitiveConsumers(created),\n+        Matchers.<AppliedPTransform<?, ?, ?>>containsInAnyOrder(flattenedProducer,\n+            flattenedProducer));\n+    assertThat(graph.getPrimitiveConsumers(flattened), emptyIterable());\n   }\n \n   @Test\n@@ -188,7 +213,7 @@ public void processElement(DoFn<String, String>.ProcessContext c)\n         transformed.apply(\n             new PTransform<PInput, PDone>() {\n               @Override\n-              public PDone apply(PInput input) {\n+              public PDone expand(PInput input) {\n                 return PDone.in(input.getPipeline());\n               }\n             });",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphVisitorTest.java",
                "sha": "b88c9a4775f79376ce52f2d472b8f539a8f053f3",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphs.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphs.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphs.java",
                "patch": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.direct;\n+\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n+import org.apache.beam.sdk.values.PValue;\n+\n+/** Test utilities for the {@link DirectRunner}. */\n+final class DirectGraphs {\n+  public static DirectGraph getGraph(Pipeline p) {\n+    DirectGraphVisitor visitor = new DirectGraphVisitor();\n+    p.traverseTopologically(visitor);\n+    return visitor.getGraph();\n+  }\n+\n+  public static AppliedPTransform<?, ?, ?> getProducer(PValue value) {\n+    return getGraph(value.getPipeline()).getProducer(value);\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectGraphs.java",
                "sha": "73ada196104f108f2bda6a172087589f1f487499",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 36,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "patch": "@@ -18,8 +18,6 @@\n package org.apache.beam.runners.direct;\n \n import static com.google.common.base.Preconditions.checkState;\n-import static org.apache.beam.sdk.metrics.MetricMatchers.metricResult;\n-import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.equalTo;\n import static org.hamcrest.Matchers.is;\n import static org.hamcrest.Matchers.isA;\n@@ -37,7 +35,6 @@\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.beam.runners.direct.DirectRunner.DirectPipelineResult;\n import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.coders.AtomicCoder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderException;\n@@ -48,13 +45,6 @@\n import org.apache.beam.sdk.io.CountingInput;\n import org.apache.beam.sdk.io.CountingSource;\n import org.apache.beam.sdk.io.Read;\n-import org.apache.beam.sdk.metrics.Counter;\n-import org.apache.beam.sdk.metrics.Distribution;\n-import org.apache.beam.sdk.metrics.DistributionResult;\n-import org.apache.beam.sdk.metrics.MetricNameFilter;\n-import org.apache.beam.sdk.metrics.MetricQueryResults;\n-import org.apache.beam.sdk.metrics.Metrics;\n-import org.apache.beam.sdk.metrics.MetricsFilter;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.runners.PipelineRunner;\n@@ -467,32 +457,6 @@ public Long decode(InputStream inStream, Context context) throws IOException {\n     }\n   }\n \n-  public void testMetrics() throws Exception {\n-    Pipeline pipeline = getPipeline();\n-    pipeline\n-        .apply(Create.of(5, 8, 13))\n-        .apply(\"MyStep\", ParDo.of(new DoFn<Integer, Void>() {\n-          @ProcessElement\n-          public void processElement(ProcessContext c) {\n-            Counter count = Metrics.counter(DirectRunnerTest.class, \"count\");\n-            Distribution values = Metrics.distribution(DirectRunnerTest.class, \"input\");\n-\n-            count.inc();\n-            values.update(c.element());\n-          }\n-        }));\n-    PipelineResult result = pipeline.run();\n-    MetricQueryResults metrics = result.metrics().queryMetrics(MetricsFilter.builder()\n-        .addNameFilter(MetricNameFilter.inNamespace(DirectRunnerTest.class))\n-        .build());\n-    assertThat(metrics.counters(), contains(\n-        metricResult(DirectRunnerTest.class.getName(), \"count\", \"MyStep\", 3L, 3L)));\n-    assertThat(metrics.distributions(), contains(\n-        metricResult(DirectRunnerTest.class.getName(), \"input\", \"MyStep\",\n-            DistributionResult.create(26L, 3L, 5L, 13L),\n-            DistributionResult.create(26L, 3L, 5L, 13L))));\n-  }\n-\n   private static class MustSplitSource<T> extends BoundedSource<T>{\n     public static <T> BoundedSource<T> of(BoundedSource<T> underlying) {\n       return new MustSplitSource<>(underlying);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/DirectRunnerTest.java",
                "sha": "eafb788d9c968de3fed4c11ddccf1b199392e9dd",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 39,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "patch": "@@ -58,7 +58,6 @@\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.util.state.BagState;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n import org.apache.beam.sdk.util.state.StateNamespaces;\n import org.apache.beam.sdk.util.state.StateTag;\n import org.apache.beam.sdk.util.state.StateTags;\n@@ -86,9 +85,13 @@\n   private PCollectionView<Iterable<Integer>> view;\n   private PCollection<Long> unbounded;\n \n-  private BundleFactory bundleFactory;\n   private DirectGraph graph;\n \n+  private AppliedPTransform<?, ?, ?> createdProducer;\n+  private AppliedPTransform<?, ?, ?> downstreamProducer;\n+  private AppliedPTransform<?, ?, ?> viewProducer;\n+  private AppliedPTransform<?, ?, ?> unboundedProducer;\n+\n   @Before\n   public void setup() {\n     DirectRunner runner =\n@@ -101,14 +104,16 @@ public void setup() {\n     view = created.apply(View.<Integer>asIterable());\n     unbounded = p.apply(CountingInput.unbounded());\n \n-    DirectGraphVisitor graphVisitor = new DirectGraphVisitor();\n-    p.traverseTopologically(graphVisitor);\n-\n-    bundleFactory = ImmutableListBundleFactory.create();\n-    graph = graphVisitor.getGraph();\n+    BundleFactory bundleFactory = ImmutableListBundleFactory.create();\n+    graph = DirectGraphs.getGraph(p);\n     context =\n         EvaluationContext.create(\n             runner.getPipelineOptions(), NanosOffsetClock.create(), bundleFactory, graph);\n+\n+    createdProducer = graph.getProducer(created);\n+    downstreamProducer = graph.getProducer(downstream);\n+    viewProducer = graph.getProducer(view);\n+    unboundedProducer = graph.getProducer(unbounded);\n   }\n \n   @Test\n@@ -146,7 +151,7 @@ public void writeToViewWriterThenReadReads() {\n   @Test\n   public void getExecutionContextSameStepSameKeyState() {\n     DirectExecutionContext fooContext =\n-        context.getExecutionContext(created.getProducingTransformInternal(),\n+        context.getExecutionContext(createdProducer,\n             StructuralKey.of(\"foo\", StringUtf8Coder.of()));\n \n     StateTag<Object, BagState<Integer>> intBag = StateTags.bag(\"myBag\", VarIntCoder.of());\n@@ -159,12 +164,12 @@ public void getExecutionContextSameStepSameKeyState() {\n             .createKeyedBundle(StructuralKey.of(\"foo\", StringUtf8Coder.of()), created)\n             .commit(Instant.now()),\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal())\n+        StepTransformResult.withoutHold(createdProducer)\n             .withState(stepContext.commitState())\n             .build());\n \n     DirectExecutionContext secondFooContext =\n-        context.getExecutionContext(created.getProducingTransformInternal(),\n+        context.getExecutionContext(createdProducer,\n             StructuralKey.of(\"foo\", StringUtf8Coder.of()));\n     assertThat(\n         secondFooContext\n@@ -179,7 +184,7 @@ public void getExecutionContextSameStepSameKeyState() {\n   @Test\n   public void getExecutionContextDifferentKeysIndependentState() {\n     DirectExecutionContext fooContext =\n-        context.getExecutionContext(created.getProducingTransformInternal(),\n+        context.getExecutionContext(createdProducer,\n             StructuralKey.of(\"foo\", StringUtf8Coder.of()));\n \n     StateTag<Object, BagState<Integer>> intBag = StateTags.bag(\"myBag\", VarIntCoder.of());\n@@ -191,7 +196,7 @@ public void getExecutionContextDifferentKeysIndependentState() {\n         .add(1);\n \n     DirectExecutionContext barContext =\n-        context.getExecutionContext(created.getProducingTransformInternal(),\n+        context.getExecutionContext(createdProducer,\n             StructuralKey.of(\"bar\", StringUtf8Coder.of()));\n     assertThat(barContext, not(equalTo(fooContext)));\n     assertThat(\n@@ -207,7 +212,7 @@ public void getExecutionContextDifferentKeysIndependentState() {\n   public void getExecutionContextDifferentStepsIndependentState() {\n     StructuralKey<?> myKey = StructuralKey.of(\"foo\", StringUtf8Coder.of());\n     DirectExecutionContext fooContext =\n-        context.getExecutionContext(created.getProducingTransformInternal(), myKey);\n+        context.getExecutionContext(createdProducer, myKey);\n \n     StateTag<Object, BagState<Integer>> intBag = StateTags.bag(\"myBag\", VarIntCoder.of());\n \n@@ -218,7 +223,7 @@ public void getExecutionContextDifferentStepsIndependentState() {\n         .add(1);\n \n     DirectExecutionContext barContext =\n-        context.getExecutionContext(downstream.getProducingTransformInternal(), myKey);\n+        context.getExecutionContext(downstreamProducer, myKey);\n     assertThat(\n         barContext\n             .getOrCreateStepContext(\"s1\", \"s1\")\n@@ -232,15 +237,15 @@ public void getExecutionContextDifferentStepsIndependentState() {\n   public void handleResultCommitsAggregators() {\n     Class<?> fn = getClass();\n     DirectExecutionContext fooContext =\n-        context.getExecutionContext(created.getProducingTransformInternal(), null);\n+        context.getExecutionContext(createdProducer, null);\n     DirectExecutionContext.StepContext stepContext = fooContext.createStepContext(\n-        \"STEP\", created.getProducingTransformInternal().getTransform().getName());\n+        \"STEP\", createdProducer.getTransform().getName());\n     AggregatorContainer container = context.getAggregatorContainer();\n     AggregatorContainer.Mutator mutator = container.createMutator();\n     mutator.createAggregatorForDoFn(fn, stepContext, \"foo\", new SumLongFn()).addValue(4L);\n \n     TransformResult<?> result =\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal())\n+        StepTransformResult.withoutHold(createdProducer)\n             .withAggregatorChanges(mutator)\n             .build();\n     context.handleResult(null, ImmutableList.<TimerData>of(), result);\n@@ -250,7 +255,7 @@ public void handleResultCommitsAggregators() {\n     mutatorAgain.createAggregatorForDoFn(fn, stepContext, \"foo\", new SumLongFn()).addValue(12L);\n \n     TransformResult<?> secondResult =\n-        StepTransformResult.withoutHold(downstream.getProducingTransformInternal())\n+        StepTransformResult.withoutHold(downstreamProducer)\n             .withAggregatorChanges(mutatorAgain)\n             .build();\n     context.handleResult(\n@@ -264,7 +269,7 @@ public void handleResultCommitsAggregators() {\n   public void handleResultStoresState() {\n     StructuralKey<?> myKey = StructuralKey.of(\"foo\".getBytes(), ByteArrayCoder.of());\n     DirectExecutionContext fooContext =\n-        context.getExecutionContext(downstream.getProducingTransformInternal(), myKey);\n+        context.getExecutionContext(downstreamProducer, myKey);\n \n     StateTag<Object, BagState<Integer>> intBag = StateTags.bag(\"myBag\", VarIntCoder.of());\n \n@@ -276,7 +281,7 @@ public void handleResultStoresState() {\n     bag.add(4);\n \n     TransformResult<?> stateResult =\n-        StepTransformResult.withoutHold(downstream.getProducingTransformInternal())\n+        StepTransformResult.withoutHold(downstreamProducer)\n             .withState(state)\n             .build();\n \n@@ -286,7 +291,7 @@ public void handleResultStoresState() {\n         stateResult);\n \n     DirectExecutionContext afterResultContext =\n-        context.getExecutionContext(downstream.getProducingTransformInternal(), myKey);\n+        context.getExecutionContext(downstreamProducer, myKey);\n \n     CopyOnAccessInMemoryStateInternals<Object> afterResultState =\n         afterResultContext.getOrCreateStepContext(\"s1\", \"s1\").stateInternals();\n@@ -309,7 +314,7 @@ public void run() {\n         downstream, GlobalWindow.INSTANCE, WindowingStrategy.globalDefault(), callback);\n \n     TransformResult<?> result =\n-        StepTransformResult.withHold(created.getProducingTransformInternal(), new Instant(0))\n+        StepTransformResult.withHold(createdProducer, new Instant(0))\n             .build();\n \n     context.handleResult(null, ImmutableList.<TimerData>of(), result);\n@@ -318,7 +323,7 @@ public void run() {\n     assertThat(callLatch.await(500L, TimeUnit.MILLISECONDS), is(false));\n \n     TransformResult<?> finishedResult =\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal()).build();\n+        StepTransformResult.withoutHold(createdProducer).build();\n     context.handleResult(null, ImmutableList.<TimerData>of(), finishedResult);\n     context.forceRefresh();\n     // Obtain the value via blocking call\n@@ -328,7 +333,7 @@ public void run() {\n   @Test\n   public void callAfterOutputMustHaveBeenProducedAlreadyAfterCallsImmediately() throws Exception {\n     TransformResult<?> finishedResult =\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal()).build();\n+        StepTransformResult.withoutHold(createdProducer).build();\n     context.handleResult(null, ImmutableList.<TimerData>of(), finishedResult);\n \n     final CountDownLatch callLatch = new CountDownLatch(1);\n@@ -348,15 +353,15 @@ public void run() {\n   @Test\n   public void extractFiredTimersExtractsTimers() {\n     TransformResult<?> holdResult =\n-        StepTransformResult.withHold(created.getProducingTransformInternal(), new Instant(0))\n+        StepTransformResult.withHold(createdProducer, new Instant(0))\n             .build();\n     context.handleResult(null, ImmutableList.<TimerData>of(), holdResult);\n \n     StructuralKey<?> key = StructuralKey.of(\"foo\".length(), VarIntCoder.of());\n     TimerData toFire =\n         TimerData.of(StateNamespaces.global(), new Instant(100L), TimeDomain.EVENT_TIME);\n     TransformResult<?> timerResult =\n-        StepTransformResult.withoutHold(downstream.getProducingTransformInternal())\n+        StepTransformResult.withoutHold(downstreamProducer)\n             .withState(CopyOnAccessInMemoryStateInternals.withUnderlying(key, null))\n             .withTimerUpdate(TimerUpdate.builder(key).setTimer(toFire).build())\n             .build();\n@@ -372,7 +377,7 @@ public void extractFiredTimersExtractsTimers() {\n     assertThat(context.extractFiredTimers(), emptyIterable());\n \n     TransformResult<?> advanceResult =\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal()).build();\n+        StepTransformResult.withoutHold(createdProducer).build();\n     // Should cause the downstream timer to fire\n     context.handleResult(null, ImmutableList.<TimerData>of(), advanceResult);\n \n@@ -403,14 +408,14 @@ public void createKeyedBundleKeyed() {\n   @Test\n   public void isDoneWithUnboundedPCollectionAndShutdown() {\n     context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(true);\n-    assertThat(context.isDone(unbounded.getProducingTransformInternal()), is(false));\n+    assertThat(context.isDone(unboundedProducer), is(false));\n \n     context.handleResult(\n         null,\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(unbounded.getProducingTransformInternal()).build());\n+        StepTransformResult.withoutHold(unboundedProducer).build());\n     context.extractFiredTimers();\n-    assertThat(context.isDone(unbounded.getProducingTransformInternal()), is(true));\n+    assertThat(context.isDone(unboundedProducer), is(true));\n   }\n \n   @Test\n@@ -428,14 +433,14 @@ public void isDoneWithUnboundedPCollectionAndNotShutdown() {\n   @Test\n   public void isDoneWithOnlyBoundedPCollections() {\n     context.getPipelineOptions().setShutdownUnboundedProducersWithMaxWatermark(false);\n-    assertThat(context.isDone(created.getProducingTransformInternal()), is(false));\n+    assertThat(context.isDone(createdProducer), is(false));\n \n     context.handleResult(\n         null,\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal()).build());\n+        StepTransformResult.withoutHold(createdProducer).build());\n     context.extractFiredTimers();\n-    assertThat(context.isDone(created.getProducingTransformInternal()), is(true));\n+    assertThat(context.isDone(createdProducer), is(true));\n   }\n \n   @Test\n@@ -449,7 +454,7 @@ public void isDoneWithPartiallyDone() {\n         context.handleResult(\n             null,\n             ImmutableList.<TimerData>of(),\n-            StepTransformResult.<Integer>withoutHold(created.getProducingTransformInternal())\n+            StepTransformResult.<Integer>withoutHold(createdProducer)\n                 .addOutput(rootBundle)\n                 .build());\n     @SuppressWarnings(\"unchecked\")\n@@ -458,7 +463,7 @@ public void isDoneWithPartiallyDone() {\n     context.handleResult(\n         null,\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(unbounded.getProducingTransformInternal()).build());\n+        StepTransformResult.withoutHold(unboundedProducer).build());\n     assertThat(context.isDone(), is(false));\n \n     for (AppliedPTransform<?, ?, ?> consumers : graph.getPrimitiveConsumers(created)) {\n@@ -479,22 +484,22 @@ public void isDoneWithUnboundedAndNotShutdown() {\n     context.handleResult(\n         null,\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal()).build());\n+        StepTransformResult.withoutHold(createdProducer).build());\n     context.handleResult(\n         null,\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(unbounded.getProducingTransformInternal()).build());\n+        StepTransformResult.withoutHold(unboundedProducer).build());\n     context.handleResult(\n         context.createBundle(created).commit(Instant.now()),\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(downstream.getProducingTransformInternal()).build());\n+        StepTransformResult.withoutHold(downstreamProducer).build());\n     context.extractFiredTimers();\n     assertThat(context.isDone(), is(false));\n \n     context.handleResult(\n         context.createBundle(created).commit(Instant.now()),\n         ImmutableList.<TimerData>of(),\n-        StepTransformResult.withoutHold(view.getProducingTransformInternal()).build());\n+        StepTransformResult.withoutHold(viewProducer).build());\n     context.extractFiredTimers();\n     assertThat(context.isDone(), is(false));\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/EvaluationContextTest.java",
                "sha": "f11c370ac9d2f3e323062beb8cbb3f7797f8eaf5",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/FlattenEvaluatorFactoryTest.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/FlattenEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 6,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/FlattenEvaluatorFactoryTest.java",
                "patch": "@@ -47,6 +47,7 @@\n @RunWith(JUnit4.class)\n public class FlattenEvaluatorFactoryTest {\n   private BundleFactory bundleFactory = ImmutableListBundleFactory.create();\n+\n   @Test\n   public void testFlattenInMemoryEvaluator() throws Exception {\n     TestPipeline p = TestPipeline.create();\n@@ -69,10 +70,11 @@ public void testFlattenInMemoryEvaluator() throws Exception {\n     when(context.createBundle(flattened)).thenReturn(flattenedLeftBundle, flattenedRightBundle);\n \n     FlattenEvaluatorFactory factory = new FlattenEvaluatorFactory(context);\n+    AppliedPTransform<?, ?, ?> flattenedProducer = DirectGraphs.getProducer(flattened);\n     TransformEvaluator<Integer> leftSideEvaluator =\n-        factory.forApplication(flattened.getProducingTransformInternal(), leftBundle);\n+        factory.forApplication(flattenedProducer, leftBundle);\n     TransformEvaluator<Integer> rightSideEvaluator =\n-        factory.forApplication(flattened.getProducingTransformInternal(), rightBundle);\n+        factory.forApplication(flattenedProducer, rightBundle);\n \n     leftSideEvaluator.processElement(WindowedValue.valueInGlobalWindow(1));\n     rightSideEvaluator.processElement(WindowedValue.valueInGlobalWindow(-1));\n@@ -92,13 +94,13 @@ public void testFlattenInMemoryEvaluator() throws Exception {\n         Matchers.<UncommittedBundle<?>>contains(flattenedRightBundle));\n     assertThat(\n         rightSideResult.getTransform(),\n-        Matchers.<AppliedPTransform<?, ?, ?>>equalTo(flattened.getProducingTransformInternal()));\n+        Matchers.<AppliedPTransform<?, ?, ?>>equalTo(flattenedProducer));\n     assertThat(\n         leftSideResult.getOutputBundles(),\n         Matchers.<UncommittedBundle<?>>contains(flattenedLeftBundle));\n     assertThat(\n         leftSideResult.getTransform(),\n-        Matchers.<AppliedPTransform<?, ?, ?>>equalTo(flattened.getProducingTransformInternal()));\n+        Matchers.<AppliedPTransform<?, ?, ?>>equalTo(flattenedProducer));\n \n     assertThat(\n         flattenedLeftBundle.commit(Instant.now()).getElements(),\n@@ -126,9 +128,10 @@ public void testFlattenInMemoryEvaluatorWithEmptyPCollectionList() throws Except\n         .thenReturn(bundleFactory.createBundle(flattened));\n \n     FlattenEvaluatorFactory factory = new FlattenEvaluatorFactory(evaluationContext);\n+    AppliedPTransform<?, ?, ?> flattendProducer = DirectGraphs.getProducer(flattened);\n     TransformEvaluator<Integer> emptyEvaluator =\n         factory.forApplication(\n-            flattened.getProducingTransformInternal(),\n+            flattendProducer,\n             bundleFactory.createRootBundle().commit(BoundedWindow.TIMESTAMP_MAX_VALUE));\n \n     TransformResult<Integer> leftSideResult = emptyEvaluator.finishBundle();\n@@ -138,7 +141,7 @@ public void testFlattenInMemoryEvaluatorWithEmptyPCollectionList() throws Except\n     assertThat(outputBundle.getElements(), emptyIterable());\n     assertThat(\n         leftSideResult.getTransform(),\n-        Matchers.<AppliedPTransform<?, ?, ?>>equalTo(flattened.getProducingTransformInternal()));\n+        Matchers.<AppliedPTransform<?, ?, ?>>equalTo(flattendProducer));\n   }\n \n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/FlattenEvaluatorFactoryTest.java",
                "sha": "9e22c362802b713881c6a74d6ba1f58a2c49a7a7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ForwardingPTransformTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ForwardingPTransformTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ForwardingPTransformTest.java",
                "patch": "@@ -66,8 +66,8 @@ public void applyDelegates() {\n     PCollection<Integer> collection = mock(PCollection.class);\n     @SuppressWarnings(\"unchecked\")\n     PCollection<String> output = mock(PCollection.class);\n-    when(delegate.apply(collection)).thenReturn(output);\n-    PCollection<String> result = forwarding.apply(collection);\n+    when(delegate.expand(collection)).thenReturn(output);\n+    PCollection<String> result = forwarding.expand(collection);\n     assertThat(result, equalTo(output));\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ForwardingPTransformTest.java",
                "sha": "6860a58f9993483384068d087b6a9783d33be745",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyEvaluatorFactoryTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyEvaluatorFactoryTest.java",
                "patch": "@@ -25,6 +25,8 @@\n import com.google.common.collect.HashMultiset;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Multiset;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.direct.DirectGroupByKey.DirectGroupByKeyOnly;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n@@ -33,8 +35,6 @@\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -97,7 +97,7 @@ public void testInMemoryEvaluator() throws Exception {\n         ((KvCoder<String, Integer>) values.getCoder()).getKeyCoder();\n     TransformEvaluator<KV<String, Integer>> evaluator =\n         new GroupByKeyOnlyEvaluatorFactory(evaluationContext)\n-            .forApplication(groupedKvs.getProducingTransformInternal(), inputBundle);\n+            .forApplication(DirectGraphs.getProducer(groupedKvs), inputBundle);\n \n     evaluator.processElement(WindowedValue.valueInGlobalWindow(firstFoo));\n     evaluator.processElement(WindowedValue.valueInGlobalWindow(secondFoo));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyEvaluatorFactoryTest.java",
                "sha": "f0b29f07ab62e06abe3f33569887163c99be4fa3",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactoryTest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactoryTest.java",
                "patch": "@@ -25,6 +25,8 @@\n import com.google.common.collect.HashMultiset;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Multiset;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.direct.DirectGroupByKey.DirectGroupByKeyOnly;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n@@ -33,8 +35,6 @@\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n@@ -90,8 +90,7 @@ public void testInMemoryEvaluator() throws Exception {\n         ((KvCoder<String, Integer>) values.getCoder()).getKeyCoder();\n     TransformEvaluator<KV<String, Integer>> evaluator =\n         new GroupByKeyOnlyEvaluatorFactory(evaluationContext)\n-            .forApplication(\n-                groupedKvs.getProducingTransformInternal(), inputBundle);\n+            .forApplication(DirectGraphs.getProducer(groupedKvs), inputBundle);\n \n     evaluator.processElement(WindowedValue.valueInGlobalWindow(firstFoo));\n     evaluator.processElement(WindowedValue.valueInGlobalWindow(secondFoo));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/GroupByKeyOnlyEvaluatorFactoryTest.java",
                "sha": "7efdb3d2b875695305aa4ddb6d0f0432367a7b21",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ImmutabilityEnforcementFactoryTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ImmutabilityEnforcementFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ImmutabilityEnforcementFactoryTest.java",
                "patch": "@@ -64,7 +64,7 @@ public void processElement(ProcessContext c)\n                         c.element()[0] = 'b';\n                       }\n                     }));\n-    consumer = pcollection.apply(Count.<byte[]>globally()).getProducingTransformInternal();\n+    consumer = DirectGraphs.getProducer(pcollection.apply(Count.<byte[]>globally()));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ImmutabilityEnforcementFactoryTest.java",
                "sha": "1ad6ba6af30c90d2d675c1db268b32663d017826",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitorTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitorTest.java",
                "patch": "@@ -163,7 +163,7 @@ public void getKeyedPValuesBeforeTraverseThrows() {\n \n   private static class PrimitiveKeyer<K> extends PTransform<PCollection<K>, PCollection<K>> {\n     @Override\n-    public PCollection<K> apply(PCollection<K> input) {\n+    public PCollection<K> expand(PCollection<K> input) {\n       return PCollection.<K>createPrimitiveOutputInternal(\n               input.getPipeline(), input.getWindowingStrategy(), input.isBounded())\n           .setCoder(input.getCoder());\n@@ -172,7 +172,7 @@ public void getKeyedPValuesBeforeTraverseThrows() {\n \n   private static class CompositeKeyer<K> extends PTransform<PCollection<K>, PCollection<K>> {\n     @Override\n-    public PCollection<K> apply(PCollection<K> input) {\n+    public PCollection<K> expand(PCollection<K> input) {\n       return input.apply(new PrimitiveKeyer<K>()).apply(ParDo.of(new IdentityFn<K>()));\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/KeyedPValueTrackingVisitorTest.java",
                "sha": "0852cd3f544d23c95ff0bd67f1b6cfc25ed17834",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "patch": "@@ -152,8 +152,9 @@ public void sideInputsNotReadyResultHasUnprocessedElements() {\n     when(evaluationContext.getAggregatorContainer()).thenReturn(container);\n     when(evaluationContext.getAggregatorMutator()).thenReturn(mutator);\n \n+    @SuppressWarnings(\"unchecked\")\n     AppliedPTransform<PCollection<Integer>, ?, ?> transform =\n-        (AppliedPTransform<PCollection<Integer>, ?, ?>) output.getProducingTransformInternal();\n+        (AppliedPTransform<PCollection<Integer>, ?, ?>) DirectGraphs.getProducer(output);\n     return ParDoEvaluator.create(\n         evaluationContext,\n         stepContext,",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ParDoEvaluatorTest.java",
                "sha": "d48ac142ad6d10994134f89e5f8b7e56f0aec843",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "patch": "@@ -31,6 +31,7 @@\n import com.google.common.collect.Lists;\n import java.io.Serializable;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import org.apache.beam.runners.direct.DirectRunner.CommittedBundle;\n import org.apache.beam.runners.direct.DirectRunner.UncommittedBundle;\n@@ -51,7 +52,6 @@\n import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n-import org.apache.beam.sdk.util.state.CopyOnAccessInMemoryStateInternals;\n import org.apache.beam.sdk.util.state.StateInternals;\n import org.apache.beam.sdk.util.state.StateNamespace;\n import org.apache.beam.sdk.util.state.StateNamespaces;\n@@ -95,6 +95,11 @@\n   public void setup() {\n     MockitoAnnotations.initMocks(this);\n     when((StateInternals<Object>) mockStepContext.stateInternals()).thenReturn(stateInternals);\n+    when(mockEvaluationContext.createSideInputReader(anyList()))\n+        .thenReturn(\n+            SideInputContainer.create(\n+                    mockEvaluationContext, Collections.<PCollectionView<?>>emptyList())\n+                .createReaderForViews(Collections.<PCollectionView<?>>emptyList()));\n   }\n \n   @Test\n@@ -129,7 +134,7 @@ public void process(ProcessContext c) {}\n     AppliedPTransform<\n             PCollection<? extends KV<String, Iterable<Integer>>>, PCollectionTuple,\n             StatefulParDo<String, Integer, Integer>>\n-        producingTransform = (AppliedPTransform) produced.getProducingTransformInternal();\n+        producingTransform = (AppliedPTransform) DirectGraphs.getProducer(produced);\n \n     // Then there will be a digging down to the step context to get the state internals\n     when(mockEvaluationContext.getExecutionContext(\n@@ -239,7 +244,7 @@ public void process(ProcessContext c) {}\n     AppliedPTransform<\n             PCollection<KV<String, Iterable<Integer>>>, PCollectionTuple,\n             StatefulParDo<String, Integer, Integer>>\n-        producingTransform = (AppliedPTransform) produced.getProducingTransformInternal();\n+        producingTransform = (AppliedPTransform) DirectGraphs.getProducer(produced);\n \n     // Then there will be a digging down to the step context to get the state internals\n     when(mockEvaluationContext.getExecutionContext(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StatefulParDoEvaluatorFactoryTest.java",
                "sha": "326310bf6db98b3caeeb08478a79d5d752f0dad7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StepTransformResultTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StepTransformResultTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/StepTransformResultTest.java",
                "patch": "@@ -48,7 +48,7 @@\n   public void setup() {\n     TestPipeline p = TestPipeline.create();\n     pc = p.apply(Create.of(1, 2, 3));\n-    transform = pc.getProducingTransformInternal();\n+    transform = DirectGraphs.getGraph(p).getProducer(pc);\n \n     bundleFactory = ImmutableListBundleFactory.create();\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/StepTransformResultTest.java",
                "sha": "d3a2cca844446f9f05f117e1ea23071caa011044",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 6,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.beam.sdk.coders.VarIntCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n import org.apache.beam.sdk.testing.TestStream;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollection;\n@@ -80,15 +81,16 @@ public void producesElementsInSequence() throws Exception {\n     when(context.createBundle(streamVals))\n         .thenReturn(bundleFactory.createBundle(streamVals), bundleFactory.createBundle(streamVals));\n \n+    AppliedPTransform<?, ?, ?> streamProducer = DirectGraphs.getProducer(streamVals);\n     Collection<CommittedBundle<?>> initialInputs =\n         new TestStreamEvaluatorFactory.InputProvider(context)\n-            .getInitialInputs(streamVals.getProducingTransformInternal(), 1);\n+            .getInitialInputs(streamProducer, 1);\n     @SuppressWarnings(\"unchecked\")\n     CommittedBundle<TestStreamIndex<Integer>> initialBundle =\n         (CommittedBundle<TestStreamIndex<Integer>>) Iterables.getOnlyElement(initialInputs);\n \n     TransformEvaluator<TestStreamIndex<Integer>> firstEvaluator =\n-        factory.forApplication(streamVals.getProducingTransformInternal(), initialBundle);\n+        factory.forApplication(streamProducer, initialBundle);\n     firstEvaluator.processElement(Iterables.getOnlyElement(initialBundle.getElements()));\n     TransformResult<TestStreamIndex<Integer>> firstResult = firstEvaluator.finishBundle();\n \n@@ -101,7 +103,7 @@ public void producesElementsInSequence() throws Exception {\n     CommittedBundle<TestStreamIndex<Integer>> secondBundle =\n         initialBundle.withElements(Collections.singleton(firstResidual));\n     TransformEvaluator<TestStreamIndex<Integer>> secondEvaluator =\n-        factory.forApplication(streamVals.getProducingTransformInternal(), secondBundle);\n+        factory.forApplication(streamProducer, secondBundle);\n     secondEvaluator.processElement(firstResidual);\n     TransformResult<TestStreamIndex<Integer>> secondResult = secondEvaluator.finishBundle();\n \n@@ -114,7 +116,7 @@ public void producesElementsInSequence() throws Exception {\n     CommittedBundle<TestStreamIndex<Integer>> thirdBundle =\n         secondBundle.withElements(Collections.singleton(secondResidual));\n     TransformEvaluator<TestStreamIndex<Integer>> thirdEvaluator =\n-        factory.forApplication(streamVals.getProducingTransformInternal(), thirdBundle);\n+        factory.forApplication(streamProducer, thirdBundle);\n     thirdEvaluator.processElement(secondResidual);\n     TransformResult<TestStreamIndex<Integer>> thirdResult = thirdEvaluator.finishBundle();\n \n@@ -128,7 +130,7 @@ public void producesElementsInSequence() throws Exception {\n     CommittedBundle<TestStreamIndex<Integer>> fourthBundle =\n         thirdBundle.withElements(Collections.singleton(thirdResidual));\n     TransformEvaluator<TestStreamIndex<Integer>> fourthEvaluator =\n-        factory.forApplication(streamVals.getProducingTransformInternal(), fourthBundle);\n+        factory.forApplication(streamProducer, fourthBundle);\n     fourthEvaluator.processElement(thirdResidual);\n     TransformResult<TestStreamIndex<Integer>> fourthResult = fourthEvaluator.finishBundle();\n \n@@ -142,7 +144,7 @@ public void producesElementsInSequence() throws Exception {\n     CommittedBundle<TestStreamIndex<Integer>> fifthBundle =\n         thirdBundle.withElements(Collections.singleton(fourthResidual));\n     TransformEvaluator<TestStreamIndex<Integer>> fifthEvaluator =\n-        factory.forApplication(streamVals.getProducingTransformInternal(), fifthBundle);\n+        factory.forApplication(streamProducer, fifthBundle);\n     fifthEvaluator.processElement(fourthResidual);\n     TransformResult<TestStreamIndex<Integer>> fifthResult = fifthEvaluator.finishBundle();\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TestStreamEvaluatorFactoryTest.java",
                "sha": "6bb86233a0dc272fcc8345cb5dbf4a99d56722f6",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorTest.java",
                "patch": "@@ -89,8 +89,9 @@ public void setup() {\n     created = p.apply(Create.of(\"foo\", \"spam\", \"third\"));\n     PCollection<KV<Integer, String>> downstream = created.apply(WithKeys.<Integer, String>of(3));\n \n-    createdProducer = created.getProducingTransformInternal();\n-    downstreamProducer = downstream.getProducingTransformInternal();\n+    DirectGraph graph = DirectGraphs.getGraph(p);\n+    createdProducer = graph.getProducer(created);\n+    downstreamProducer = graph.getProducer(downstream);\n \n     when(evaluationContext.getMetrics()).thenReturn(metrics);\n   }\n@@ -317,7 +318,7 @@ public void processElement(WindowedValue<Object> element) throws Exception {}\n   @Test\n   public void callWithEnforcementThrowsOnFinishPropagates() throws Exception {\n     final TransformResult<Object> result =\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal()).build();\n+        StepTransformResult.withoutHold(createdProducer).build();\n \n     TransformEvaluator<Object> evaluator =\n         new TransformEvaluator<Object>() {\n@@ -356,7 +357,7 @@ public void processElement(WindowedValue<Object> element) throws Exception {}\n   @Test\n   public void callWithEnforcementThrowsOnElementPropagates() throws Exception {\n     final TransformResult<Object> result =\n-        StepTransformResult.withoutHold(created.getProducingTransformInternal()).build();\n+        StepTransformResult.withoutHold(createdProducer).build();\n \n     TransformEvaluator<Object> evaluator =\n         new TransformEvaluator<Object>() {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/TransformExecutorTest.java",
                "sha": "4ad22bc3af49bc6d9b3fe0d0468ac3e5accb6574",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.direct;\n \n+import static org.apache.beam.runners.direct.DirectGraphs.getProducer;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.equalTo;\n import static org.hamcrest.Matchers.hasSize;\n@@ -90,6 +91,7 @@\n   private BundleFactory bundleFactory = ImmutableListBundleFactory.create();\n \n   private UnboundedSource<Long, ?> source;\n+  private DirectGraph graph;\n \n   @Before\n   public void setup() {\n@@ -100,6 +102,7 @@ public void setup() {\n     context = mock(EvaluationContext.class);\n     factory = new UnboundedReadEvaluatorFactory(context);\n     output = bundleFactory.createBundle(longs);\n+    graph = DirectGraphs.getGraph(p);\n     when(context.createBundle(longs)).thenReturn(output);\n   }\n \n@@ -115,7 +118,7 @@ public void generatesInitialSplits() throws Exception {\n     int numSplits = 5;\n     Collection<CommittedBundle<?>> initialInputs =\n         new UnboundedReadEvaluatorFactory.InputProvider(context)\n-            .getInitialInputs(longs.getProducingTransformInternal(), numSplits);\n+            .getInitialInputs(graph.getProducer(longs), numSplits);\n     // CountingSource.unbounded has very good splitting behavior\n     assertThat(initialInputs, hasSize(numSplits));\n \n@@ -148,15 +151,14 @@ public void unboundedSourceInMemoryTransformEvaluatorProducesElements() throws E\n \n     Collection<CommittedBundle<?>> initialInputs =\n         new UnboundedReadEvaluatorFactory.InputProvider(context)\n-            .getInitialInputs(longs.getProducingTransformInternal(), 1);\n+            .getInitialInputs(graph.getProducer(longs), 1);\n \n     CommittedBundle<?> inputShards = Iterables.getOnlyElement(initialInputs);\n     UnboundedSourceShard<Long, ?> inputShard =\n         (UnboundedSourceShard<Long, ?>)\n             Iterables.getOnlyElement(inputShards.getElements()).getValue();\n     TransformEvaluator<? super UnboundedSourceShard<Long, ?>> evaluator =\n-        factory.forApplication(\n-            longs.getProducingTransformInternal(), inputShards);\n+        factory.forApplication(graph.getProducer(longs), inputShards);\n \n     evaluator.processElement((WindowedValue) Iterables.getOnlyElement(inputShards.getElements()));\n     TransformResult<? super UnboundedSourceShard<Long, ?>> result = evaluator.finishBundle();\n@@ -190,7 +192,7 @@ public void unboundedSourceWithDuplicatesMultipleCalls() throws Exception {\n \n     TestPipeline p = TestPipeline.create();\n     PCollection<Long> pcollection = p.apply(Read.from(source));\n-    AppliedPTransform<?, ?, ?> sourceTransform = pcollection.getProducingTransformInternal();\n+    AppliedPTransform<?, ?, ?> sourceTransform = getProducer(pcollection);\n \n     when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n     Collection<CommittedBundle<?>> initialInputs =\n@@ -233,7 +235,7 @@ public void noElementsAvailableReaderIncludedInResidual() throws Exception {\n     // Read with a very slow rate so by the second read there are no more elements\n     PCollection<Long> pcollection =\n         p.apply(Read.from(new TestUnboundedSource<>(VarLongCoder.of(), 1L)));\n-    AppliedPTransform<?, ?, ?> sourceTransform = pcollection.getProducingTransformInternal();\n+    AppliedPTransform<?, ?, ?> sourceTransform = DirectGraphs.getProducer(pcollection);\n \n     when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n     Collection<CommittedBundle<?>> initialInputs =\n@@ -291,7 +293,9 @@ public void evaluatorReusesReader() throws Exception {\n \n     TestPipeline p = TestPipeline.create();\n     PCollection<Long> pcollection = p.apply(Read.from(source));\n-    AppliedPTransform<?, ?, ?> sourceTransform = pcollection.getProducingTransformInternal();\n+    DirectGraph graph = DirectGraphs.getGraph(p);\n+    AppliedPTransform<?, ?, ?> sourceTransform =\n+        graph.getProducer(pcollection);\n \n     when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n     UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);\n@@ -307,8 +311,7 @@ public void evaluatorReusesReader() throws Exception {\n             .commit(Instant.now());\n     UnboundedReadEvaluatorFactory factory =\n         new UnboundedReadEvaluatorFactory(context, 1.0 /* Always reuse */);\n-    new UnboundedReadEvaluatorFactory.InputProvider(context)\n-        .getInitialInputs(pcollection.getProducingTransformInternal(), 1);\n+    new UnboundedReadEvaluatorFactory.InputProvider(context).getInitialInputs(sourceTransform, 1);\n     TransformEvaluator<UnboundedSourceShard<Long, TestCheckpointMark>> evaluator =\n         factory.forApplication(sourceTransform, inputBundle);\n     evaluator.processElement(shard);\n@@ -336,7 +339,8 @@ public void evaluatorClosesReaderAndResumesFromCheckpoint() throws Exception {\n \n     TestPipeline p = TestPipeline.create();\n     PCollection<Long> pcollection = p.apply(Read.from(source));\n-    AppliedPTransform<?, ?, ?> sourceTransform = pcollection.getProducingTransformInternal();\n+    AppliedPTransform<?, ?, ?> sourceTransform =\n+        DirectGraphs.getGraph(p).getProducer(pcollection);\n \n     when(context.createRootBundle()).thenReturn(bundleFactory.createRootBundle());\n     UncommittedBundle<Long> output = bundleFactory.createBundle(pcollection);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactoryTest.java",
                "sha": "dd36a2f8c1f794a400aff215a1c5da3efaedb04e",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.coders.VoidCoder;\n import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Create;\n import org.apache.beam.sdk.transforms.GroupByKey;\n import org.apache.beam.sdk.transforms.Values;\n@@ -73,9 +74,10 @@ public void testInMemoryEvaluator() throws Exception {\n \n     CommittedBundle<String> inputBundle =\n         bundleFactory.createBundle(input).commit(Instant.now());\n+    AppliedPTransform<?, ?, ?> producer = DirectGraphs.getProducer(view);\n     TransformEvaluator<Iterable<String>> evaluator =\n         new ViewEvaluatorFactory(context)\n-            .forApplication(view.getProducingTransformInternal(), inputBundle);\n+            .forApplication(producer, inputBundle);\n \n     evaluator.processElement(\n         WindowedValue.<Iterable<String>>valueInGlobalWindow(ImmutableList.of(\"foo\", \"bar\")));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/ViewEvaluatorFactoryTest.java",
                "sha": "7c080099675a4f4d61e22ef25037fc205907f599",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkCallbackExecutorTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkCallbackExecutorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkCallbackExecutorTest.java",
                "patch": "@@ -55,8 +55,10 @@\n   public void setup() {\n     TestPipeline p = TestPipeline.create();\n     PCollection<Integer> created = p.apply(Create.of(1, 2, 3));\n-    create = created.getProducingTransformInternal();\n-    sum = created.apply(Sum.integersGlobally()).getProducingTransformInternal();\n+    PCollection<Integer> summed = created.apply(Sum.integersGlobally());\n+    DirectGraph graph = DirectGraphs.getGraph(p);\n+    create = graph.getProducer(created);\n+    sum = graph.getProducer(summed);\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkCallbackExecutorTest.java",
                "sha": "acdabb6b26bd62baa00c4a663680a1ce40b65754",
                "status": "modified"
            },
            {
                "additions": 105,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkManagerTest.java",
                "changes": 237,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkManagerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 132,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkManagerTest.java",
                "patch": "@@ -31,7 +31,6 @@\n import java.util.Collection;\n import java.util.Collections;\n import java.util.EnumSet;\n-import java.util.HashMap;\n import java.util.Map;\n import javax.annotation.Nullable;\n import org.apache.beam.runners.direct.CommittedResult.OutputType;\n@@ -63,7 +62,6 @@\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionList;\n-import org.apache.beam.sdk.values.PValue;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.hamcrest.BaseMatcher;\n import org.hamcrest.Description;\n@@ -116,33 +114,8 @@ public void processElement(ProcessContext c) throws Exception {\n     PCollectionList<Integer> preFlatten = PCollectionList.of(createdInts).and(intsToFlatten);\n     flattened = preFlatten.apply(\"flattened\", Flatten.<Integer>pCollections());\n \n-    Collection<AppliedPTransform<?, ?, ?>> rootTransforms =\n-        ImmutableList.<AppliedPTransform<?, ?, ?>>of(\n-            createdInts.getProducingTransformInternal(),\n-            intsToFlatten.getProducingTransformInternal());\n-\n-    Map<PValue, Collection<AppliedPTransform<?, ?, ?>>> consumers = new HashMap<>();\n-    consumers.put(\n-        createdInts,\n-        ImmutableList.<AppliedPTransform<?, ?, ?>>of(filtered.getProducingTransformInternal(),\n-            keyed.getProducingTransformInternal(), flattened.getProducingTransformInternal()));\n-    consumers.put(\n-        filtered,\n-        Collections.<AppliedPTransform<?, ?, ?>>singleton(\n-            filteredTimesTwo.getProducingTransformInternal()));\n-    consumers.put(filteredTimesTwo, Collections.<AppliedPTransform<?, ?, ?>>emptyList());\n-    consumers.put(keyed, Collections.<AppliedPTransform<?, ?, ?>>emptyList());\n-\n-    consumers.put(\n-        intsToFlatten,\n-        Collections.<AppliedPTransform<?, ?, ?>>singleton(\n-            flattened.getProducingTransformInternal()));\n-    consumers.put(flattened, Collections.<AppliedPTransform<?, ?, ?>>emptyList());\n-\n     clock = MockClock.fromInstant(new Instant(1000));\n-    DirectGraphVisitor visitor = new DirectGraphVisitor();\n-    p.traverseTopologically(visitor);\n-    graph = visitor.getGraph();\n+    graph = DirectGraphs.getGraph(p);\n \n     manager = WatermarkManager.create(clock, graph);\n     bundleFactory = ImmutableListBundleFactory.create();\n@@ -155,7 +128,7 @@ public void processElement(ProcessContext c) throws Exception {\n   @Test\n   public void getWatermarkForUntouchedTransform() {\n     TransformWatermarks watermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n \n     assertThat(watermarks.getInputWatermark(), equalTo(BoundedWindow.TIMESTAMP_MIN_VALUE));\n     assertThat(watermarks.getOutputWatermark(), equalTo(BoundedWindow.TIMESTAMP_MIN_VALUE));\n@@ -170,13 +143,13 @@ public void getWatermarkForUpdatedSourceTransform() {\n     CommittedBundle<Integer> output = multiWindowedBundle(createdInts, 1);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(output)),\n         new Instant(8000L));\n     manager.refreshAll();\n     TransformWatermarks updatedSourceWatermark =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n \n     assertThat(updatedSourceWatermark.getOutputWatermark(), equalTo(new Instant(8000L)));\n   }\n@@ -191,30 +164,30 @@ public void getWatermarkForMultiInputTransform() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(intsToFlatten.getProducingTransformInternal(),\n+        result(graph.getProducer(intsToFlatten),\n             null,\n             Collections.<CommittedBundle<?>>singleton(secondPcollectionBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n \n     // We didn't do anything for the first source, so we shouldn't have progressed the watermark\n     TransformWatermarks firstSourceWatermark =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(\n         firstSourceWatermark.getOutputWatermark(),\n         not(laterThan(BoundedWindow.TIMESTAMP_MIN_VALUE)));\n \n     // the Second Source output all of the elements so it should be done (with a watermark at the\n     // end of time).\n     TransformWatermarks secondSourceWatermark =\n-        manager.getWatermarks(intsToFlatten.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(intsToFlatten));\n     assertThat(\n         secondSourceWatermark.getOutputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n \n     // We haven't consumed anything yet, so our watermark should be at the beginning of time\n     TransformWatermarks transformWatermark =\n-        manager.getWatermarks(flattened.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(flattened));\n     assertThat(\n         transformWatermark.getInputWatermark(), not(laterThan(BoundedWindow.TIMESTAMP_MIN_VALUE)));\n     assertThat(\n@@ -225,15 +198,15 @@ public void getWatermarkForMultiInputTransform() {\n     // anything from the first PCollection yet; so our watermark shouldn't advance\n     manager.updateWatermarks(secondPcollectionBundle,\n         TimerUpdate.empty(),\n-        result(flattened.getProducingTransformInternal(),\n+        result(graph.getProducer(flattened),\n             secondPcollectionBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(flattenedBundleSecondCreate)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     TransformWatermarks transformAfterProcessing =\n-        manager.getWatermarks(flattened.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(flattened));\n     manager.updateWatermarks(secondPcollectionBundle,\n         TimerUpdate.empty(),\n-        result(flattened.getProducingTransformInternal(),\n+        result(graph.getProducer(flattened),\n             secondPcollectionBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(flattenedBundleSecondCreate)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -252,21 +225,21 @@ public void getWatermarkForMultiInputTransform() {\n     // past the end of the global window\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(firstPcollectionBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks firstSourceWatermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(\n         firstSourceWatermarks.getOutputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n \n     // We still haven't consumed any of the first source's input, so the watermark should still not\n     // progress\n     TransformWatermarks flattenAfterSourcesProduced =\n-        manager.getWatermarks(flattened.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(flattened));\n     assertThat(\n         flattenAfterSourcesProduced.getInputWatermark(), not(laterThan(firstCollectionTimestamp)));\n     assertThat(\n@@ -276,21 +249,21 @@ public void getWatermarkForMultiInputTransform() {\n     // end of the global window), we should have a watermark equal to the min among buffered\n     // elements\n     TransformWatermarks withBufferedElements =\n-        manager.getWatermarks(flattened.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(flattened));\n     assertThat(withBufferedElements.getInputWatermark(), equalTo(firstCollectionTimestamp));\n     assertThat(withBufferedElements.getOutputWatermark(), equalTo(firstCollectionTimestamp));\n \n     CommittedBundle<?> completedFlattenBundle =\n         bundleFactory.createBundle(flattened).commit(BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.updateWatermarks(firstPcollectionBundle,\n         TimerUpdate.empty(),\n-        result(flattened.getProducingTransformInternal(),\n+        result(graph.getProducer(flattened),\n             firstPcollectionBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(completedFlattenBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks afterConsumingAllInput =\n-        manager.getWatermarks(flattened.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(flattened));\n     assertThat(\n         afterConsumingAllInput.getInputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n@@ -329,16 +302,16 @@ public void getWatermarkMultiIdenticalInput() {\n \n     Map<AppliedPTransform<?, ?, ?>, Collection<CommittedBundle<?>>> initialInputs =\n         ImmutableMap.<AppliedPTransform<?, ?, ?>, Collection<CommittedBundle<?>>>builder()\n-            .put(\n-                created.getProducingTransformInternal(),\n+            .put(graph.getProducer(\n+                created),\n                 Collections.<CommittedBundle<?>>singleton(root))\n             .build();\n     tstMgr.initialize(initialInputs);\n     tstMgr.updateWatermarks(\n         root,\n         TimerUpdate.empty(),\n         CommittedResult.create(\n-            StepTransformResult.withoutHold(created.getProducingTransformInternal()).build(),\n+            StepTransformResult.withoutHold(graph.getProducer(created)).build(),\n             root.withElements(Collections.<WindowedValue<Void>>emptyList()),\n             Collections.singleton(createBundle),\n             EnumSet.allOf(OutputType.class)),\n@@ -385,13 +358,13 @@ public void getWatermarkForMultiConsumedCollection() {\n         TimestampedValue.of(3, new Instant(-1000L)));\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createdBundle)),\n         new Instant(Long.MAX_VALUE));\n     manager.refreshAll();\n     TransformWatermarks createdAfterProducing =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(\n         createdAfterProducing.getOutputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n@@ -402,34 +375,34 @@ public void getWatermarkForMultiConsumedCollection() {\n             TimestampedValue.of(KV.of(\"MyKey\", 3), new Instant(-1000L)));\n     manager.updateWatermarks(createdBundle,\n         TimerUpdate.empty(),\n-        result(keyed.getProducingTransformInternal(),\n+        result(graph.getProducer(keyed),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(keyBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks keyedWatermarks =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     assertThat(\n         keyedWatermarks.getInputWatermark(), not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n     assertThat(\n         keyedWatermarks.getOutputWatermark(), not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n \n     TransformWatermarks filteredWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(filteredWatermarks.getInputWatermark(), not(laterThan(new Instant(-1000L))));\n     assertThat(filteredWatermarks.getOutputWatermark(), not(laterThan(new Instant(-1000L))));\n \n     CommittedBundle<Integer> filteredBundle =\n         timestampedBundle(filtered, TimestampedValue.of(2, new Instant(1234L)));\n     manager.updateWatermarks(createdBundle,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(filteredBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks filteredProcessedWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(\n         filteredProcessedWatermarks.getInputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n@@ -450,7 +423,7 @@ public void updateWatermarkWithWatermarkHolds() {\n         TimestampedValue.of(3, new Instant(-1000L)));\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createdBundle)),\n         new Instant(Long.MAX_VALUE));\n@@ -461,13 +434,13 @@ public void updateWatermarkWithWatermarkHolds() {\n         TimestampedValue.of(KV.of(\"MyKey\", 3), new Instant(-1000L)));\n     manager.updateWatermarks(createdBundle,\n         TimerUpdate.empty(),\n-        result(keyed.getProducingTransformInternal(),\n+        result(graph.getProducer(keyed),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(keyBundle)),\n         new Instant(500L));\n     manager.refreshAll();\n     TransformWatermarks keyedWatermarks =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     assertThat(\n         keyedWatermarks.getInputWatermark(), not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n     assertThat(keyedWatermarks.getOutputWatermark(), not(laterThan(new Instant(500L))));\n@@ -494,27 +467,27 @@ public void updateWatermarkWithKeyedWatermarkHolds() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             ImmutableList.of(firstKeyBundle, secondKeyBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n \n     manager.updateWatermarks(firstKeyBundle,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             firstKeyBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         new Instant(-1000L));\n     manager.updateWatermarks(secondKeyBundle,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             secondKeyBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         new Instant(1234L));\n     manager.refreshAll();\n \n     TransformWatermarks filteredWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(filteredWatermarks.getInputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n     assertThat(filteredWatermarks.getOutputWatermark(), not(laterThan(new Instant(-1000L))));\n@@ -524,7 +497,7 @@ public void updateWatermarkWithKeyedWatermarkHolds() {\n         createdInts).commit(clock.now());\n     manager.updateWatermarks(fauxFirstKeyTimerBundle,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             fauxFirstKeyTimerBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -537,7 +510,7 @@ public void updateWatermarkWithKeyedWatermarkHolds() {\n         createdInts).commit(clock.now());\n     manager.updateWatermarks(fauxSecondKeyTimerBundle,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             fauxSecondKeyTimerBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         new Instant(5678L));\n@@ -546,7 +519,7 @@ public void updateWatermarkWithKeyedWatermarkHolds() {\n \n     manager.updateWatermarks(fauxSecondKeyTimerBundle,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             fauxSecondKeyTimerBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -564,26 +537,26 @@ public void updateOutputWatermarkShouldBeMonotonic() {\n     CommittedBundle<?> firstInput =\n         bundleFactory.createBundle(createdInts).commit(BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.updateWatermarks(null,  TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(firstInput)),\n         new Instant(0L));\n     manager.refreshAll();\n     TransformWatermarks firstWatermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(firstWatermarks.getOutputWatermark(), equalTo(new Instant(0L)));\n \n     CommittedBundle<?> secondInput =\n         bundleFactory.createBundle(createdInts).commit(BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(secondInput)),\n         new Instant(-250L));\n     manager.refreshAll();\n     TransformWatermarks secondWatermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(secondWatermarks.getOutputWatermark(), not(earlierThan(new Instant(0L))));\n   }\n \n@@ -599,7 +572,7 @@ public void updateWatermarkWithHoldsShouldBeMonotonic() {\n         TimestampedValue.of(3, new Instant(-1000L)));\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createdBundle)),\n         new Instant(Long.MAX_VALUE));\n@@ -610,20 +583,20 @@ public void updateWatermarkWithHoldsShouldBeMonotonic() {\n             TimestampedValue.of(KV.of(\"MyKey\", 3), new Instant(-1000L)));\n     manager.updateWatermarks(createdBundle,\n         TimerUpdate.empty(),\n-        result(keyed.getProducingTransformInternal(),\n+        result(graph.getProducer(keyed),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(keyBundle)),\n         new Instant(500L));\n     manager.refreshAll();\n     TransformWatermarks keyedWatermarks =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     assertThat(\n         keyedWatermarks.getInputWatermark(), not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n     assertThat(keyedWatermarks.getOutputWatermark(), not(laterThan(new Instant(500L))));\n     Instant oldOutputWatermark = keyedWatermarks.getOutputWatermark();\n \n     TransformWatermarks updatedWatermarks =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     assertThat(\n         updatedWatermarks.getInputWatermark(), not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n     // We added a hold prior to the old watermark; we shouldn't progress (due to the earlier hold)\n@@ -646,7 +619,7 @@ public void updateWatermarkWithUnprocessedElements() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createdBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -655,12 +628,12 @@ public void updateWatermarkWithUnprocessedElements() {\n         TimestampedValue.of(KV.of(\"MyKey\", 1), BoundedWindow.TIMESTAMP_MIN_VALUE));\n     manager.updateWatermarks(createdBundle,\n         TimerUpdate.empty(),\n-        result(keyed.getProducingTransformInternal(),\n+        result(graph.getProducer(keyed),\n             createdBundle.withElements(ImmutableList.of(second, third)),\n             Collections.<CommittedBundle<?>>singleton(keyBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     TransformWatermarks keyedWatermarks =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     // the unprocessed second and third are readded to pending\n     assertThat(\n         keyedWatermarks.getInputWatermark(), not(laterThan(new Instant(-1000L))));\n@@ -681,23 +654,23 @@ public void updateWatermarkWithCompletedElementsNotPending() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createdBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n \n     manager.updateWatermarks(\n         neverCreatedBundle,\n         TimerUpdate.empty(),\n-        result(\n-            filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(\n+            filtered),\n             neverCreatedBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n \n     manager.refreshAll();\n     TransformWatermarks filteredWms =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(filteredWms.getInputWatermark(), equalTo(new Instant(22L)));\n   }\n \n@@ -712,7 +685,7 @@ public void updateWatermarkWithLateData() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createdBundle)),\n         sourceWatermark);\n@@ -724,13 +697,13 @@ public void updateWatermarkWithLateData() {\n     // Finish processing the on-time data. The watermarks should progress to be equal to the source\n     manager.updateWatermarks(createdBundle,\n         TimerUpdate.empty(),\n-        result(keyed.getProducingTransformInternal(),\n+        result(graph.getProducer(keyed),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(keyBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks onTimeWatermarks =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     assertThat(onTimeWatermarks.getInputWatermark(), equalTo(sourceWatermark));\n     assertThat(onTimeWatermarks.getOutputWatermark(), equalTo(sourceWatermark));\n \n@@ -740,27 +713,27 @@ public void updateWatermarkWithLateData() {\n     // we don't advance the watermark past the current watermark until we've consumed the late data\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(lateDataBundle)),\n         new Instant(2_000_000L));\n     manager.refreshAll();\n     TransformWatermarks bufferedLateWm =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(bufferedLateWm.getOutputWatermark(), equalTo(new Instant(2_000_000L)));\n \n     // The input watermark should be held to its previous value (not advanced due to late data; not\n     // moved backwards in the presence of watermarks due to monotonicity).\n     TransformWatermarks lateDataBufferedWatermark =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     assertThat(lateDataBufferedWatermark.getInputWatermark(), not(earlierThan(sourceWatermark)));\n     assertThat(lateDataBufferedWatermark.getOutputWatermark(), not(earlierThan(sourceWatermark)));\n \n     CommittedBundle<KV<String, Integer>> lateKeyedBundle =\n         timestampedBundle(keyed, TimestampedValue.of(KV.of(\"MyKey\", 3), new Instant(-1000L)));\n     manager.updateWatermarks(lateDataBundle,\n         TimerUpdate.empty(),\n-        result(keyed.getProducingTransformInternal(),\n+        result(graph.getProducer(keyed),\n             lateDataBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(lateKeyedBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -770,7 +743,7 @@ public void updateWatermarkWithLateData() {\n   public void updateWatermarkWithDifferentWindowedValueInstances() {\n     manager.updateWatermarks(\n         null,\n-        TimerUpdate.empty(), result(createdInts.getProducingTransformInternal(), null,\n+        TimerUpdate.empty(), result(graph.getProducer(createdInts), null,\n         Collections.<CommittedBundle<?>>singleton(\n             bundleFactory\n                 .createBundle(createdInts)\n@@ -783,13 +756,13 @@ public void updateWatermarkWithDifferentWindowedValueInstances() {\n         .commit(Instant.now());\n     manager.updateWatermarks(createdBundle,\n         TimerUpdate.empty(),\n-        result(keyed.getProducingTransformInternal(),\n+        result(graph.getProducer(keyed),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         null);\n     manager.refreshAll();\n     TransformWatermarks onTimeWatermarks =\n-        manager.getWatermarks(keyed.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(keyed));\n     assertThat(onTimeWatermarks.getInputWatermark(), equalTo(BoundedWindow.TIMESTAMP_MAX_VALUE));\n   }\n \n@@ -802,20 +775,20 @@ public void getWatermarksAfterOnlyEmptyOutput() {\n     CommittedBundle<Integer> emptyCreateOutput = multiWindowedBundle(createdInts);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(emptyCreateOutput)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks updatedSourceWatermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n \n     assertThat(\n         updatedSourceWatermarks.getOutputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n \n     TransformWatermarks finishedFilterWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(\n         finishedFilterWatermarks.getInputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n@@ -833,41 +806,41 @@ public void getWatermarksAfterHoldAndEmptyOutput() {\n     CommittedBundle<Integer> firstCreateOutput = multiWindowedBundle(createdInts, 1, 2);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(firstCreateOutput)),\n         new Instant(12_000L));\n \n     CommittedBundle<Integer> firstFilterOutput = multiWindowedBundle(filtered);\n     manager.updateWatermarks(firstCreateOutput,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             firstCreateOutput.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(firstFilterOutput)),\n         new Instant(10_000L));\n     manager.refreshAll();\n     TransformWatermarks firstFilterWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(firstFilterWatermarks.getInputWatermark(), not(earlierThan(new Instant(12_000L))));\n     assertThat(firstFilterWatermarks.getOutputWatermark(), not(laterThan(new Instant(10_000L))));\n \n     CommittedBundle<Integer> emptyCreateOutput = multiWindowedBundle(createdInts);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(emptyCreateOutput)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks updatedSourceWatermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n \n     assertThat(\n         updatedSourceWatermarks.getOutputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n \n     TransformWatermarks finishedFilterWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(\n         finishedFilterWatermarks.getInputWatermark(),\n         not(earlierThan(BoundedWindow.TIMESTAMP_MAX_VALUE)));\n@@ -877,14 +850,14 @@ public void getWatermarksAfterHoldAndEmptyOutput() {\n   @Test\n   public void getSynchronizedProcessingTimeInputWatermarksHeldToPendingBundles() {\n     TransformWatermarks watermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(watermarks.getSynchronizedProcessingInputTime(), equalTo(clock.now()));\n     assertThat(\n         watermarks.getSynchronizedProcessingOutputTime(),\n         equalTo(BoundedWindow.TIMESTAMP_MIN_VALUE));\n \n     TransformWatermarks filteredWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     // Non-root processing watermarks don't progress until data has been processed\n     assertThat(\n         filteredWatermarks.getSynchronizedProcessingInputTime(),\n@@ -898,18 +871,18 @@ public void getSynchronizedProcessingTimeInputWatermarksHeldToPendingBundles() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createOutput)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks createAfterUpdate =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(createAfterUpdate.getSynchronizedProcessingInputTime(), equalTo(clock.now()));\n     assertThat(createAfterUpdate.getSynchronizedProcessingOutputTime(), equalTo(clock.now()));\n \n     TransformWatermarks filterAfterProduced =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(\n         filterAfterProduced.getSynchronizedProcessingInputTime(), not(laterThan(clock.now())));\n     assertThat(\n@@ -929,13 +902,13 @@ public void getSynchronizedProcessingTimeInputWatermarksHeldToPendingBundles() {\n         bundleFactory.createBundle(intsToFlatten).commit(new Instant(1250L));\n     manager.updateWatermarks(createOutput,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             createOutput.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(filterOutputBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks filterAfterConsumed =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     assertThat(\n         filterAfterConsumed.getSynchronizedProcessingInputTime(),\n         not(laterThan(createAfterUpdate.getSynchronizedProcessingOutputTime())));\n@@ -955,16 +928,16 @@ public void getSynchronizedProcessingTimeOutputHeldToPendingTimers() {\n     CommittedBundle<Integer> createdBundle = multiWindowedBundle(createdInts, 1, 2, 4, 8);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createdBundle)),\n         new Instant(1248L));\n     manager.refreshAll();\n \n     TransformWatermarks filteredWms =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     TransformWatermarks filteredDoubledWms =\n-        manager.getWatermarks(filteredTimesTwo.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filteredTimesTwo));\n     Instant initialFilteredWm = filteredWms.getSynchronizedProcessingOutputTime();\n     Instant initialFilteredDoubledWm = filteredDoubledWms.getSynchronizedProcessingOutputTime();\n \n@@ -978,7 +951,7 @@ public void getSynchronizedProcessingTimeOutputHeldToPendingTimers() {\n         TimerUpdate.builder(key).setTimer(pastTimer).setTimer(futureTimer).build();\n     manager.updateWatermarks(createdBundle,\n         timers,\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(filteredBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -1014,7 +987,7 @@ public void getSynchronizedProcessingTimeOutputHeldToPendingTimers() {\n     manager.updateWatermarks(filteredTimerBundle,\n         TimerUpdate.builder(key)\n             .withCompletedTimers(Collections.<TimerData>singleton(pastTimer)).build(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             filteredTimerBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(filteredTimerResult)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -1029,7 +1002,7 @@ public void getSynchronizedProcessingTimeOutputHeldToPendingTimers() {\n \n     manager.updateWatermarks(filteredTimerResult,\n         TimerUpdate.empty(),\n-        result(filteredTimesTwo.getProducingTransformInternal(),\n+        result(graph.getProducer(filteredTimesTwo),\n             filteredTimerResult.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -1050,11 +1023,11 @@ public void getSynchronizedProcessingTimeOutputHeldToPendingTimers() {\n   public void getSynchronizedProcessingTimeOutputTimeIsMonotonic() {\n     Instant startTime = clock.now();\n     TransformWatermarks watermarks =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(watermarks.getSynchronizedProcessingInputTime(), equalTo(startTime));\n \n     TransformWatermarks filteredWatermarks =\n-        manager.getWatermarks(filtered.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filtered));\n     // Non-root processing watermarks don't progress until data has been processed\n     assertThat(\n         filteredWatermarks.getSynchronizedProcessingInputTime(),\n@@ -1068,13 +1041,13 @@ public void getSynchronizedProcessingTimeOutputTimeIsMonotonic() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createOutput)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n     TransformWatermarks createAfterUpdate =\n-        manager.getWatermarks(createdInts.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(createdInts));\n     assertThat(createAfterUpdate.getSynchronizedProcessingInputTime(), not(laterThan(clock.now())));\n     assertThat(createAfterUpdate.getSynchronizedProcessingOutputTime(),\n         not(laterThan(clock.now())));\n@@ -1083,7 +1056,7 @@ public void getSynchronizedProcessingTimeOutputTimeIsMonotonic() {\n         bundleFactory.createBundle(createdInts).commit(new Instant(750L));\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(createSecondOutput)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -1097,7 +1070,7 @@ public void synchronizedProcessingInputTimeIsHeldToUpstreamProcessingTimeTimers(\n     CommittedBundle<Integer> created = multiWindowedBundle(createdInts, 1, 2, 3);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(created)),\n         new Instant(40_900L));\n@@ -1111,14 +1084,14 @@ public void synchronizedProcessingInputTimeIsHeldToUpstreamProcessingTimeTimers(\n         TimerUpdate.builder(StructuralKey.of(\"key\", StringUtf8Coder.of()))\n             .setTimer(upstreamProcessingTimer)\n             .build(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             created.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(filteredBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n \n     TransformWatermarks downstreamWms =\n-        manager.getWatermarks(filteredTimesTwo.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filteredTimesTwo));\n     assertThat(downstreamWms.getSynchronizedProcessingInputTime(), equalTo(clock.now()));\n \n     clock.set(BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -1133,7 +1106,7 @@ public void synchronizedProcessingInputTimeIsHeldToUpstreamProcessingTimeTimers(\n     manager.updateWatermarks(otherCreated,\n         TimerUpdate.builder(StructuralKey.of(\"key\", StringUtf8Coder.of()))\n             .withCompletedTimers(Collections.singleton(upstreamProcessingTimer)).build(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             otherCreated.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>emptyList()),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -1148,7 +1121,7 @@ public void synchronizedProcessingInputTimeIsHeldToPendingBundleTimes() {\n     manager.updateWatermarks(\n         null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>singleton(created)),\n         new Instant(29_919_235L));\n@@ -1160,14 +1133,14 @@ public void synchronizedProcessingInputTimeIsHeldToPendingBundleTimes() {\n     manager.updateWatermarks(\n         created,\n         TimerUpdate.empty(),\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             created.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(filteredBundle)),\n         BoundedWindow.TIMESTAMP_MAX_VALUE);\n     manager.refreshAll();\n \n     TransformWatermarks downstreamWms =\n-        manager.getWatermarks(filteredTimesTwo.getProducingTransformInternal());\n+        manager.getWatermarks(graph.getProducer(filteredTimesTwo));\n     assertThat(downstreamWms.getSynchronizedProcessingInputTime(), equalTo(clock.now()));\n \n     clock.set(BoundedWindow.TIMESTAMP_MAX_VALUE);\n@@ -1184,7 +1157,7 @@ public void extractFiredTimersReturnsFiredEventTimeTimers() {\n     CommittedBundle<Integer> createdBundle = multiWindowedBundle(filtered);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.singleton(createdBundle)),\n         new Instant(1500L));\n@@ -1206,7 +1179,7 @@ public void extractFiredTimersReturnsFiredEventTimeTimers() {\n \n     manager.updateWatermarks(createdBundle,\n         update,\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten))),\n         new Instant(1000L));\n@@ -1220,7 +1193,7 @@ public void extractFiredTimersReturnsFiredEventTimeTimers() {\n \n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>emptyList()),\n         new Instant(50_000L));\n@@ -1242,7 +1215,7 @@ public void extractFiredTimersReturnsFiredProcessingTimeTimers() {\n     CommittedBundle<Integer> createdBundle = multiWindowedBundle(filtered);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.singleton(createdBundle)),\n         new Instant(1500L));\n@@ -1264,7 +1237,7 @@ public void extractFiredTimersReturnsFiredProcessingTimeTimers() {\n     manager.updateWatermarks(\n         createdBundle,\n         update,\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten))),\n         new Instant(1000L));\n@@ -1278,7 +1251,7 @@ public void extractFiredTimersReturnsFiredProcessingTimeTimers() {\n     clock.set(new Instant(50_000L));\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>emptyList()),\n         new Instant(50_000L));\n@@ -1301,7 +1274,7 @@ public void extractFiredTimersReturnsFiredSynchronizedProcessingTimeTimers() {\n     CommittedBundle<Integer> createdBundle = multiWindowedBundle(filtered);\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.singleton(createdBundle)),\n         new Instant(1500L));\n@@ -1323,7 +1296,7 @@ public void extractFiredTimersReturnsFiredSynchronizedProcessingTimeTimers() {\n     manager.updateWatermarks(\n         createdBundle,\n         update,\n-        result(filtered.getProducingTransformInternal(),\n+        result(graph.getProducer(filtered),\n             createdBundle.withElements(Collections.<WindowedValue<Integer>>emptyList()),\n             Collections.<CommittedBundle<?>>singleton(multiWindowedBundle(intsToFlatten))),\n         new Instant(1000L));\n@@ -1338,7 +1311,7 @@ public void extractFiredTimersReturnsFiredSynchronizedProcessingTimeTimers() {\n     clock.set(new Instant(50_000L));\n     manager.updateWatermarks(null,\n         TimerUpdate.empty(),\n-        result(createdInts.getProducingTransformInternal(),\n+        result(graph.getProducer(createdInts),\n             null,\n             Collections.<CommittedBundle<?>>emptyList()),\n         new Instant(50_000L));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WatermarkManagerTest.java",
                "sha": "eb4d0cdc996621c9949171426887412e02d391e0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "patch": "@@ -121,13 +121,13 @@ public void dynamicallyReshardedWrite() throws Exception {\n   public void withShardingSpecifiesOriginalTransform() {\n     Write.Bound<Object> original = Write.to(new TestSink()).withNumShards(3);\n \n-    assertThat(factory.override(original), equalTo((Object) original));\n+    assertThat(factory.getReplacementTransform(original), equalTo((Object) original));\n   }\n \n   @Test\n   public void withNoShardingSpecifiedReturnsNewTransform() {\n     Write.Bound<Object> original = Write.to(new TestSink());\n-    assertThat(factory.override(original), not(equalTo((Object) original)));\n+    assertThat(factory.getReplacementTransform(original), not(equalTo((Object) original)));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/direct-java/src/test/java/org/apache/beam/runners/direct/WriteWithShardingFactoryTest.java",
                "sha": "aeb75ed88968015d26cd4fb9dd6f61920ad6a4b7",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/pom.xml",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 7,
                "filename": "runners/flink/examples/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-flink-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -34,9 +34,9 @@\n \n   <properties>\n     <!-- Default parameters for mvn exec:java -->\n-    <input>kinglear.txt</input>\n-    <output>wordcounts.txt</output>\n-    <parallelism>-1</parallelism>\n+    <flink.examples.input>kinglear.txt</flink.examples.input>\n+    <flink.examples.output>wordcounts.txt</flink.examples.output>\n+    <flink.examples.parallelism>-1</flink.examples.parallelism>\n   </properties>\n \n   <profiles>\n@@ -112,9 +112,9 @@\n           <executable>java</executable>\n           <arguments>\n             <argument>--runner=org.apache.beam.runners.flink.FlinkRunner</argument>\n-            <argument>--parallelism=${parallelism}</argument>\n-            <argument>--input=${input}</argument>\n-            <argument>--output=${output}</argument>\n+            <argument>--parallelism=${flink.examples.parallelism}</argument>\n+            <argument>--input=${flink.examples.input}</argument>\n+            <argument>--output=${flink.examples.output}</argument>\n           </arguments>\n         </configuration>\n       </plugin>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/pom.xml",
                "sha": "c50a2937c0dd4885d34cf6a38ff03242f7caa66e",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "patch": "@@ -176,7 +176,7 @@ public ReadDocuments(Iterable<URI> uris) {\n     }\n \n     @Override\n-    public PCollection<KV<URI, String>> apply(PBegin input) {\n+    public PCollection<KV<URI, String>> expand(PBegin input) {\n       Pipeline pipeline = input.getPipeline();\n \n       // Create one TextIO.Read transform for each document\n@@ -219,7 +219,7 @@ public ReadDocuments(Iterable<URI> uris) {\n     public ComputeTfIdf() { }\n \n     @Override\n-    public PCollection<KV<String, KV<URI, Double>>> apply(\n+    public PCollection<KV<String, KV<URI, Double>>> expand(\n         PCollection<KV<URI, String>> uriToContent) {\n \n       // Compute the total number of documents, and\n@@ -419,7 +419,7 @@ public WriteTfIdf(String output) {\n     }\n \n     @Override\n-    public PDone apply(PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf) {\n+    public PDone expand(PCollection<KV<String, KV<URI, Double>>> wordToUriAndTfIdf) {\n       return wordToUriAndTfIdf\n           .apply(\"Format\", ParDo.of(new DoFn<KV<String, KV<URI, Double>>, String>() {\n             private static final long serialVersionUID = 0;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/TFIDF.java",
                "sha": "89e261b52b3066faadf79eb25540463f9cc7713d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "patch": "@@ -72,7 +72,7 @@ public void processElement(ProcessContext c) {\n   public static class CountWords extends PTransform<PCollection<String>,\n                     PCollection<KV<String, Long>>> {\n     @Override\n-    public PCollection<KV<String, Long>> apply(PCollection<String> lines) {\n+    public PCollection<KV<String, Long>> expand(PCollection<String> lines) {\n \n       // Convert lines of text into individual words.\n       PCollection<String> words = lines.apply(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/WordCount.java",
                "sha": "b6b3c1a02ebe395e4c54d3a646996d3a35b643a4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "patch": "@@ -82,7 +82,7 @@ public static ComputeTopCompletions top(int candidatesPerPrefix, boolean recursi\n     }\n \n     @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> apply(PCollection<String> input) {\n+    public PCollection<KV<String, List<CompletionCandidate>>> expand(PCollection<String> input) {\n       PCollection<CompletionCandidate> candidates = input\n         // First count how often each token appears.\n         .apply(new Count.PerElement<String>())\n@@ -129,7 +129,7 @@ public ComputeTopFlat(int candidatesPerPrefix, int minPrefix) {\n     }\n \n     @Override\n-    public PCollection<KV<String, List<CompletionCandidate>>> apply(\n+    public PCollection<KV<String, List<CompletionCandidate>>> expand(\n         PCollection<CompletionCandidate> input) {\n       return input\n         // For each completion candidate, map it to all prefixes.\n@@ -192,7 +192,7 @@ public void processElement(ProcessContext c) {\n     }\n \n     @Override\n-    public PCollectionList<KV<String, List<CompletionCandidate>>> apply(\n+    public PCollectionList<KV<String, List<CompletionCandidate>>> expand(\n           PCollection<CompletionCandidate> input) {\n         if (minPrefix > 10) {\n           // Base case, partitioning to return the output in the expected format.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/examples/src/main/java/org/apache/beam/runners/flink/examples/streaming/AutoComplete.java",
                "sha": "3405981d6716c081b42cf597456646136fa86c61",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/pom.xml",
                "sha": "8d76ab0495f0841cf7d9d5b8d7ef483dce77bd40",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/pom.xml",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/flink/runner/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-flink-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -55,7 +55,9 @@\n                   <groups>org.apache.beam.sdk.testing.RunnableOnService</groups>\n                   <excludedGroups>\n                     org.apache.beam.sdk.testing.UsesStatefulParDo,\n-                    org.apache.beam.sdk.testing.UsesSplittableParDo\n+                    org.apache.beam.sdk.testing.UsesTimersInParDo,\n+                    org.apache.beam.sdk.testing.UsesSplittableParDo,\n+                    org.apache.beam.sdk.testing.UsesMetrics\n                   </excludedGroups>\n                   <parallel>none</parallel>\n                   <failIfNoTests>true</failIfNoTests>\n@@ -84,7 +86,9 @@\n                   <groups>org.apache.beam.sdk.testing.RunnableOnService</groups>\n                   <excludedGroups>\n                     org.apache.beam.sdk.testing.UsesStatefulParDo,\n-                    org.apache.beam.sdk.testing.UsesSplittableParDo\n+                    org.apache.beam.sdk.testing.UsesTimersInParDo,\n+                    org.apache.beam.sdk.testing.UsesSplittableParDo,\n+                    org.apache.beam.sdk.testing.UsesMetrics\n                   </excludedGroups>\n                   <parallel>none</parallel>\n                   <failIfNoTests>true</failIfNoTests>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/pom.xml",
                "sha": "7f49372aff17fe0fb1643c8dad75b3fd9d3654c7",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "patch": "@@ -159,6 +159,12 @@ private ExecutionEnvironment createBatchExecutionEnvironment() {\n     // set parallelism in the options (required by some execution code)\n     options.setParallelism(flinkBatchEnv.getParallelism());\n \n+    if (options.getObjectReuse()) {\n+      flinkBatchEnv.getConfig().enableObjectReuse();\n+    } else {\n+      flinkBatchEnv.getConfig().disableObjectReuse();\n+    }\n+\n     return flinkBatchEnv;\n   }\n \n@@ -197,6 +203,12 @@ private StreamExecutionEnvironment createStreamExecutionEnvironment() {\n     // set parallelism in the options (required by some execution code)\n     options.setParallelism(flinkStreamEnv.getParallelism());\n \n+    if (options.getObjectReuse()) {\n+      flinkStreamEnv.getConfig().enableObjectReuse();\n+    } else {\n+      flinkStreamEnv.getConfig().disableObjectReuse();\n+    }\n+\n     // default to event time\n     flinkStreamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineExecutionEnvironment.java",
                "sha": "69dcd5e86132c11badc94d9cd89832a600fe198e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "patch": "@@ -83,6 +83,11 @@\n   Long getExecutionRetryDelay();\n   void setExecutionRetryDelay(Long delay);\n \n+  @Description(\"Sets the behavior of reusing objects.\")\n+  @Default.Boolean(false)\n+  Boolean getObjectReuse();\n+  void setObjectReuse(Boolean reuse);\n+\n   /**\n    * Sets a state backend to store Beam's state during computation.\n    * Note: Only applicable when executing in streaming mode.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java",
                "sha": "3bb358e6b71e825cd35c0d021bc8a831cbe41780",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 12,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "patch": "@@ -24,7 +24,7 @@\n import java.net.URL;\n import java.net.URLClassLoader;\n import java.util.ArrayList;\n-import java.util.Arrays;\n+import java.util.Collections;\n import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n@@ -42,7 +42,7 @@\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n import org.apache.beam.sdk.transforms.Combine;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View;\n@@ -307,7 +307,7 @@ public StreamingViewAsMap(FlinkRunner runner, View.AsMap<K, V> transform) {\n     }\n \n     @Override\n-    public PCollectionView<Map<K, V>> apply(PCollection<KV<K, V>> input) {\n+    public PCollectionView<Map<K, V>> expand(PCollection<KV<K, V>> input) {\n       PCollectionView<Map<K, V>> view =\n           PCollectionViews.mapView(\n               input.getPipeline(),\n@@ -352,7 +352,7 @@ public StreamingViewAsMultimap(FlinkRunner runner, View.AsMultimap<K, V> transfo\n     }\n \n     @Override\n-    public PCollectionView<Map<K, Iterable<V>>> apply(PCollection<KV<K, V>> input) {\n+    public PCollectionView<Map<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n       PCollectionView<Map<K, Iterable<V>>> view =\n           PCollectionViews.multimapView(\n               input.getPipeline(),\n@@ -392,7 +392,7 @@ protected String getKindString() {\n     public StreamingViewAsList(FlinkRunner runner, View.AsList<T> transform) {}\n \n     @Override\n-    public PCollectionView<List<T>> apply(PCollection<T> input) {\n+    public PCollectionView<List<T>> expand(PCollection<T> input) {\n       PCollectionView<List<T>> view =\n           PCollectionViews.listView(\n               input.getPipeline(),\n@@ -423,7 +423,7 @@ protected String getKindString() {\n     public StreamingViewAsIterable(FlinkRunner runner, View.AsIterable<T> transform) { }\n \n     @Override\n-    public PCollectionView<Iterable<T>> apply(PCollection<T> input) {\n+    public PCollectionView<Iterable<T>> expand(PCollection<T> input) {\n       PCollectionView<Iterable<T>> view =\n           PCollectionViews.iterableView(\n               input.getPipeline(),\n@@ -440,10 +440,10 @@ protected String getKindString() {\n     }\n   }\n \n-  private static class WrapAsList<T> extends OldDoFn<T, List<T>> {\n-    @Override\n+  private static class WrapAsList<T> extends DoFn<T, List<T>> {\n+    @ProcessElement\n     public void processElement(ProcessContext c) {\n-      c.output(Arrays.asList(c.element()));\n+      c.output(Collections.singletonList(c.element()));\n     }\n   }\n \n@@ -465,7 +465,7 @@ public StreamingViewAsSingleton(FlinkRunner runner, View.AsSingleton<T> transfor\n     }\n \n     @Override\n-    public PCollectionView<T> apply(PCollection<T> input) {\n+    public PCollectionView<T> expand(PCollection<T> input) {\n       Combine.Globally<T, T> combine = Combine.globally(\n           new SingletonCombine<>(transform.hasDefaultValue(), transform.defaultValue()));\n       if (!transform.hasDefaultValue()) {\n@@ -523,7 +523,7 @@ public StreamingCombineGloballyAsSingletonView(\n     }\n \n     @Override\n-    public PCollectionView<OutputT> apply(PCollection<InputT> input) {\n+    public PCollectionView<OutputT> expand(PCollection<InputT> input) {\n       PCollection<OutputT> combined =\n           input.apply(Combine.globally(transform.getCombineFn())\n               .withoutDefaults()\n@@ -620,7 +620,7 @@ private CreateFlinkPCollectionView(PCollectionView<ViewT> view) {\n     }\n \n     @Override\n-    public PCollectionView<ViewT> apply(PCollection<List<ElemT>> input) {\n+    public PCollectionView<ViewT> expand(PCollection<List<ElemT>> input) {\n       return view;\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/FlinkRunner.java",
                "sha": "5f92378f5d5cabdb859de5f6fa70ddba98aa3050",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchPipelineTranslator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchPipelineTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchPipelineTranslator.java",
                "patch": "@@ -20,7 +20,6 @@\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n-import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.flink.api.java.DataSet;\n import org.apache.flink.api.java.ExecutionEnvironment;\n@@ -113,8 +112,7 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n     BatchTransformTranslator<T> typedTranslator = (BatchTransformTranslator<T>) translator;\n \n     // create the applied PTransform on the batchContext\n-    batchContext.setCurrentTransform(AppliedPTransform.of(\n-        node.getFullName(), node.getInput(), node.getOutput(), (PTransform) transform));\n+    batchContext.setCurrentTransform(node.toAppliedPTransform());\n     typedTranslator.translateNode(typedTransform, batchContext);\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchPipelineTranslator.java",
                "sha": "209be69c5e48a044c02eeb13d6d0ead39335f18c",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchTransformTranslators.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchTransformTranslators.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 28,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchTransformTranslators.java",
                "patch": "@@ -50,12 +50,12 @@\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.transforms.GroupByKey;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View;\n import org.apache.beam.sdk.transforms.join.RawUnionValue;\n import org.apache.beam.sdk.transforms.join.UnionCoder;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n@@ -483,6 +483,30 @@ public void translateNode(\n     }\n   }\n \n+  private static void rejectStateAndTimers(DoFn<?, ?> doFn) {\n+    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n+\n+    if (signature.stateDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n+              DoFn.StateId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              FlinkRunner.class.getSimpleName()));\n+    }\n+\n+    if (signature.timerDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n+              DoFn.TimerId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              FlinkRunner.class.getSimpleName()));\n+    }\n+  }\n+\n   private static class ParDoBoundTranslatorBatch<InputT, OutputT>\n       implements FlinkBatchPipelineTranslator.BatchTransformTranslator<\n           ParDo.Bound<InputT, OutputT>> {\n@@ -492,22 +516,12 @@ public void translateNode(\n         ParDo.Bound<InputT, OutputT> transform,\n \n         FlinkBatchTranslationContext context) {\n-      DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-      if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n-        throw new UnsupportedOperationException(\n-            String.format(\n-                \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-                DoFn.StateId.class.getSimpleName(),\n-                doFn.getClass().getName(),\n-                DoFn.class.getSimpleName(),\n-                FlinkRunner.class.getSimpleName()));\n-      }\n+      DoFn<InputT, OutputT> doFn = transform.getFn();\n+      rejectStateAndTimers(doFn);\n \n       DataSet<WindowedValue<InputT>> inputDataSet =\n           context.getInputDataSet(context.getInput(transform));\n \n-      final OldDoFn<InputT, OutputT> oldDoFn = transform.getFn();\n-\n       TypeInformation<WindowedValue<OutputT>> typeInformation =\n           context.getTypeInfo(context.getOutput(transform));\n \n@@ -522,7 +536,7 @@ public void translateNode(\n \n       FlinkDoFnFunction<InputT, OutputT> doFnWrapper =\n           new FlinkDoFnFunction<>(\n-              oldDoFn,\n+              doFn,\n               context.getOutput(transform).getWindowingStrategy(),\n               sideInputStrategies,\n               context.getPipelineOptions());\n@@ -548,22 +562,11 @@ public void translateNode(\n     public void translateNode(\n         ParDo.BoundMulti<InputT, OutputT> transform,\n         FlinkBatchTranslationContext context) {\n-      DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-      if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n-        throw new UnsupportedOperationException(\n-            String.format(\n-                \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-                DoFn.StateId.class.getSimpleName(),\n-                doFn.getClass().getName(),\n-                DoFn.class.getSimpleName(),\n-                FlinkRunner.class.getSimpleName()));\n-      }\n-\n+      DoFn<InputT, OutputT> doFn = transform.getFn();\n+      rejectStateAndTimers(doFn);\n       DataSet<WindowedValue<InputT>> inputDataSet =\n           context.getInputDataSet(context.getInput(transform));\n \n-      final OldDoFn<InputT, OutputT> oldDoFn = transform.getFn();\n-\n       Map<TupleTag<?>, PCollection<?>> outputs = context.getOutput(transform).getAll();\n \n       Map<TupleTag<?>, Integer> outputMap = Maps.newHashMap();\n@@ -610,7 +613,7 @@ public void translateNode(\n       @SuppressWarnings(\"unchecked\")\n       FlinkMultiOutputDoFnFunction<InputT, OutputT> doFnWrapper =\n           new FlinkMultiOutputDoFnFunction(\n-              oldDoFn,\n+              doFn,\n               windowingStrategy,\n               sideInputStrategies,\n               context.getPipelineOptions(),",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkBatchTransformTranslators.java",
                "sha": "eb625b2a4be3654c72940bd606dc6df5af0b316d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingPipelineTranslator.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingPipelineTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingPipelineTranslator.java",
                "patch": "@@ -19,7 +19,6 @@\n \n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n-import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PValue;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n@@ -109,8 +108,7 @@ public void visitValue(PValue value, TransformHierarchy.Node producer) {\n     StreamTransformTranslator<T> typedTranslator = (StreamTransformTranslator<T>) translator;\n \n     // create the applied PTransform on the streamingContext\n-    streamingContext.setCurrentTransform(AppliedPTransform.of(\n-        node.getFullName(), node.getInput(), node.getOutput(), (PTransform) transform));\n+    streamingContext.setCurrentTransform(node.toAppliedPTransform());\n     typedTranslator.translateNode(typedTransform, streamingContext);\n   }\n \n@@ -125,8 +123,7 @@ public void visitValue(PValue value, TransformHierarchy.Node producer) {\n     @SuppressWarnings(\"unchecked\")\n     StreamTransformTranslator<T> typedTranslator = (StreamTransformTranslator<T>) translator;\n \n-    streamingContext.setCurrentTransform(AppliedPTransform.of(\n-        node.getFullName(), node.getInput(), node.getOutput(), (PTransform) transform));\n+    streamingContext.setCurrentTransform(node.toAppliedPTransform());\n \n     return typedTranslator.canTranslate(typedTransform, streamingContext);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingPipelineTranslator.java",
                "sha": "23f4d34da46050509a1ed4d364d61082884995d8",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingTransformTranslators.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingTransformTranslators.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 30,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingTransformTranslators.java",
                "patch": "@@ -57,6 +57,7 @@\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.join.RawUnionValue;\n import org.apache.beam.sdk.transforms.join.UnionCoder;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n@@ -78,11 +79,13 @@\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.core.fs.FileSystem;\n+import org.apache.flink.streaming.api.collector.selector.OutputSelector;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.datastream.KeyedStream;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n+import org.apache.flink.streaming.api.datastream.SplitStream;\n import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n import org.apache.flink.streaming.api.operators.TwoInputStreamOperator;\n@@ -302,6 +305,30 @@ public void translateNode(\n     }\n   }\n \n+  private static void rejectStateAndTimers(DoFn<?, ?> doFn) {\n+    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n+\n+    if (signature.stateDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n+              DoFn.StateId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              FlinkRunner.class.getSimpleName()));\n+    }\n+\n+    if (signature.timerDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n+              DoFn.TimerId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              FlinkRunner.class.getSimpleName()));\n+    }\n+  }\n+\n   private static class ParDoBoundStreamingTranslator<InputT, OutputT>\n       extends FlinkStreamingPipelineTranslator.StreamTransformTranslator<\n         ParDo.Bound<InputT, OutputT>> {\n@@ -311,16 +338,8 @@ public void translateNode(\n         ParDo.Bound<InputT, OutputT> transform,\n         FlinkStreamingTranslationContext context) {\n \n-      DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-      if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n-        throw new UnsupportedOperationException(\n-            String.format(\n-                \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-                DoFn.StateId.class.getSimpleName(),\n-                doFn.getClass().getName(),\n-                DoFn.class.getSimpleName(),\n-                FlinkRunner.class.getSimpleName()));\n-      }\n+      DoFn<InputT, OutputT> doFn = transform.getFn();\n+      rejectStateAndTimers(doFn);\n \n       WindowingStrategy<?, ?> windowingStrategy =\n           context.getOutput(transform).getWindowingStrategy();\n@@ -471,16 +490,8 @@ public void translateNode(\n         ParDo.BoundMulti<InputT, OutputT> transform,\n         FlinkStreamingTranslationContext context) {\n \n-      DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-      if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n-        throw new UnsupportedOperationException(\n-            String.format(\n-                \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-                DoFn.StateId.class.getSimpleName(),\n-                doFn.getClass().getName(),\n-                DoFn.class.getSimpleName(),\n-                FlinkRunner.class.getSimpleName()));\n-      }\n+      DoFn<InputT, OutputT> doFn = transform.getFn();\n+      rejectStateAndTimers(doFn);\n \n       // we assume that the transformation does not change the windowing strategy.\n       WindowingStrategy<?, ?> windowingStrategy =\n@@ -554,24 +565,30 @@ public void translateNode(\n             .transform(transform.getName(), outputUnionTypeInformation, doFnOperator);\n       }\n \n+      SplitStream<RawUnionValue> splitStream = unionOutputStream\n+              .split(new OutputSelector<RawUnionValue>() {\n+                @Override\n+                public Iterable<String> select(RawUnionValue value) {\n+                  return Collections.singletonList(Integer.toString(value.getUnionTag()));\n+                }\n+              });\n+\n       for (Map.Entry<TupleTag<?>, PCollection<?>> output : outputs.entrySet()) {\n         final int outputTag = tagsToLabels.get(output.getKey());\n \n         TypeInformation outputTypeInfo =\n             context.getTypeInfo(output.getValue());\n \n         @SuppressWarnings(\"unchecked\")\n-        DataStream filtered =\n-            unionOutputStream.flatMap(new FlatMapFunction<RawUnionValue, Object>() {\n-              @Override\n-              public void flatMap(RawUnionValue value, Collector<Object> out) throws Exception {\n-                if (value.getUnionTag() == outputTag) {\n-                  out.collect(value.getValue());\n-                }\n-              }\n-            }).returns(outputTypeInfo);\n+        DataStream unwrapped = splitStream.select(String.valueOf(outputTag))\n+          .flatMap(new FlatMapFunction<RawUnionValue, Object>() {\n+            @Override\n+            public void flatMap(RawUnionValue value, Collector<Object> out) throws Exception {\n+              out.collect(value.getValue());\n+            }\n+          }).returns(outputTypeInfo);\n \n-        context.setOutputDataStream(output.getValue(), filtered);\n+        context.setOutputDataStream(output.getValue(), unwrapped);\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/FlinkStreamingTransformTranslators.java",
                "sha": "ffa6d166385ec8167aacea27af726c22d30a7202",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "patch": "@@ -20,7 +20,10 @@\n import java.util.Map;\n import org.apache.beam.runners.flink.translation.utils.SerializedPipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFnAdapters;\n import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollectionView;\n@@ -46,16 +49,17 @@\n   private final WindowingStrategy<?, ?> windowingStrategy;\n \n   public FlinkDoFnFunction(\n-      OldDoFn<InputT, OutputT> doFn,\n+      DoFn<InputT, OutputT> doFn,\n       WindowingStrategy<?, ?> windowingStrategy,\n       Map<PCollectionView<?>, WindowingStrategy<?, ?>> sideInputs,\n       PipelineOptions options) {\n-    this.doFn = doFn;\n+    this.doFn = DoFnAdapters.toOldDoFn(doFn);\n     this.sideInputs = sideInputs;\n     this.serializedOptions = new SerializedPipelineOptions(options);\n     this.windowingStrategy = windowingStrategy;\n \n-    this.requiresWindowAccess = doFn instanceof OldDoFn.RequiresWindowAccess;\n+    this.requiresWindowAccess =\n+        DoFnSignatures.signatureForDoFn(doFn).processElement().observesWindow();\n     this.hasSideInputs = !sideInputs.isEmpty();\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkDoFnFunction.java",
                "sha": "ed200d58aa57acd27ff243ab1f01585c5ae1de31",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "patch": "@@ -24,6 +24,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.runners.core.PerKeyCombineFnRunner;\n import org.apache.beam.runners.core.PerKeyCombineFnRunners;\n import org.apache.beam.runners.flink.translation.utils.SerializedPipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptions;\n@@ -33,7 +34,6 @@\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.PerKeyCombineFnRunner;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingNonShuffleReduceFunction.java",
                "sha": "041d0e8684cf995e09883ea850efa84ab26e1c4c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "patch": "@@ -24,14 +24,14 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.runners.core.PerKeyCombineFnRunner;\n import org.apache.beam.runners.core.PerKeyCombineFnRunners;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.CombineFnBase;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.PerKeyCombineFnRunner;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingPartialReduceFunction.java",
                "sha": "fef7921098bffed14f406f8594146fb94f0e66e6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "patch": "@@ -26,14 +26,14 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.runners.core.PerKeyCombineFnRunner;\n import org.apache.beam.runners.core.PerKeyCombineFnRunners;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.CombineFnBase;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.PerKeyCombineFnRunner;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMergingReduceFunction.java",
                "sha": "59163e9614307337c469d4bd938411bb84db93b7",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputDoFnFunction.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputDoFnFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputDoFnFunction.java",
                "patch": "@@ -20,8 +20,11 @@\n import java.util.Map;\n import org.apache.beam.runners.flink.translation.utils.SerializedPipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFnAdapters;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.PCollectionView;\n@@ -54,16 +57,17 @@\n   private final WindowingStrategy<?, ?> windowingStrategy;\n \n   public FlinkMultiOutputDoFnFunction(\n-      OldDoFn<InputT, OutputT> doFn,\n+      DoFn<InputT, OutputT> doFn,\n       WindowingStrategy<?, ?> windowingStrategy,\n       Map<PCollectionView<?>, WindowingStrategy<?, ?>> sideInputs,\n       PipelineOptions options,\n       Map<TupleTag<?>, Integer> outputMap) {\n-    this.doFn = doFn;\n+    this.doFn = DoFnAdapters.toOldDoFn(doFn);\n     this.serializedOptions = new SerializedPipelineOptions(options);\n     this.outputMap = outputMap;\n \n-    this.requiresWindowAccess = doFn instanceof OldDoFn.RequiresWindowAccess;\n+    this.requiresWindowAccess =\n+        DoFnSignatures.signatureForDoFn(doFn).processElement().observesWindow();\n     this.hasSideInputs = !sideInputs.isEmpty();\n     this.windowingStrategy = windowingStrategy;\n     this.sideInputs = sideInputs;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkMultiOutputDoFnFunction.java",
                "sha": "7f6a4369ad85ca99effa8ee2322b4bc8817ce2de",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "patch": "@@ -24,6 +24,7 @@\n import java.util.Comparator;\n import java.util.Iterator;\n import java.util.Map;\n+import org.apache.beam.runners.core.PerKeyCombineFnRunner;\n import org.apache.beam.runners.core.PerKeyCombineFnRunners;\n import org.apache.beam.runners.flink.translation.utils.SerializedPipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptions;\n@@ -32,7 +33,6 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.PerKeyCombineFnRunner;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkPartialReduceFunction.java",
                "sha": "8b6ec3a0ea1cf5fe6e564c0a4e5181c31dfdcdad",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkProcessContextBase.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkProcessContextBase.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 12,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkProcessContextBase.java",
                "patch": "@@ -20,7 +20,6 @@\n import static com.google.common.base.Preconditions.checkNotNull;\n \n import com.google.common.collect.Iterables;\n-import java.io.Serializable;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.Iterator;\n@@ -39,7 +38,6 @@\n import org.apache.beam.sdk.util.state.StateInternals;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.flink.api.common.accumulators.Accumulator;\n import org.apache.flink.api.common.functions.RuntimeContext;\n import org.joda.time.Instant;\n \n@@ -256,15 +254,14 @@ protected abstract void outputWithTimestampAndWindow(\n   @Override\n   protected <AggInputT, AggOutputT> Aggregator<AggInputT, AggOutputT>\n   createAggregatorInternal(String name, Combine.CombineFn<AggInputT, ?, AggOutputT> combiner) {\n-    SerializableFnAggregatorWrapper<AggInputT, AggOutputT> wrapper =\n-        new SerializableFnAggregatorWrapper<>(combiner);\n-    Accumulator<?, ?> existingAccum =\n-        (Accumulator<AggInputT, Serializable>) runtimeContext.getAccumulator(name);\n-    if (existingAccum != null) {\n-      return wrapper;\n-    } else {\n-      runtimeContext.addAccumulator(name, wrapper);\n+    @SuppressWarnings(\"unchecked\")\n+    SerializableFnAggregatorWrapper<AggInputT, AggOutputT> result =\n+        (SerializableFnAggregatorWrapper<AggInputT, AggOutputT>)\n+            runtimeContext.getAccumulator(name);\n+\n+    if (result == null) {\n+      result = new SerializableFnAggregatorWrapper<>(combiner);\n+      runtimeContext.addAccumulator(name, result);\n     }\n-    return wrapper;\n-  }\n+    return result;  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkProcessContextBase.java",
                "sha": "6afca38dcce4b708559848948cdeb1219d216866",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.runners.core.PerKeyCombineFnRunner;\n import org.apache.beam.runners.core.PerKeyCombineFnRunners;\n import org.apache.beam.runners.flink.translation.utils.SerializedPipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptions;\n@@ -34,7 +35,6 @@\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.OutputTimeFn;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.util.PerKeyCombineFnRunner;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkReduceFunction.java",
                "sha": "fb5c90cb85b1f0e050460c3827273b521fd92c39",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "patch": "@@ -28,6 +28,7 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.runners.core.AggregatorFactory;\n import org.apache.beam.runners.core.DoFnRunner;\n import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.PushbackSideInputDoFnRunner;\n@@ -40,6 +41,7 @@\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFnAdapters;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.join.RawUnionValue;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n@@ -88,7 +90,8 @@\n     implements OneInputStreamOperator<WindowedValue<InputT>, OutputT>,\n       TwoInputStreamOperator<WindowedValue<InputT>, RawUnionValue, OutputT> {\n \n-  protected OldDoFn<InputT, FnOutputT> doFn;\n+  protected OldDoFn<InputT, FnOutputT> oldDoFn;\n+\n   protected final SerializedPipelineOptions serializedOptions;\n \n   protected final TupleTag<FnOutputT> mainOutputTag;\n@@ -117,8 +120,9 @@\n \n   private transient Map<String, KvStateSnapshot<?, ?, ?, ?, ?>> restoredSideInputState;\n \n+  @Deprecated\n   public DoFnOperator(\n-      OldDoFn<InputT, FnOutputT> doFn,\n+      OldDoFn<InputT, FnOutputT> oldDoFn,\n       TypeInformation<WindowedValue<InputT>> inputType,\n       TupleTag<FnOutputT> mainOutputTag,\n       List<TupleTag<?>> sideOutputTags,\n@@ -127,7 +131,7 @@ public DoFnOperator(\n       Map<Integer, PCollectionView<?>> sideInputTagMapping,\n       Collection<PCollectionView<?>> sideInputs,\n       PipelineOptions options) {\n-    this.doFn = doFn;\n+    this.oldDoFn = oldDoFn;\n     this.mainOutputTag = mainOutputTag;\n     this.sideOutputTags = sideOutputTags;\n     this.sideInputTagMapping = sideInputTagMapping;\n@@ -148,26 +152,48 @@ public DoFnOperator(\n     setChainingStrategy(ChainingStrategy.ALWAYS);\n   }\n \n+  public DoFnOperator(\n+      DoFn<InputT, FnOutputT> doFn,\n+      TypeInformation<WindowedValue<InputT>> inputType,\n+      TupleTag<FnOutputT> mainOutputTag,\n+      List<TupleTag<?>> sideOutputTags,\n+      OutputManagerFactory<OutputT> outputManagerFactory,\n+      WindowingStrategy<?, ?> windowingStrategy,\n+      Map<Integer, PCollectionView<?>> sideInputTagMapping,\n+      Collection<PCollectionView<?>> sideInputs,\n+      PipelineOptions options) {\n+    this(\n+        DoFnAdapters.toOldDoFn(doFn),\n+        inputType,\n+        mainOutputTag,\n+        sideOutputTags,\n+        outputManagerFactory,\n+        windowingStrategy,\n+        sideInputTagMapping,\n+        sideInputs,\n+        options);\n+  }\n+\n   protected ExecutionContext.StepContext createStepContext() {\n     return new StepContext();\n   }\n \n   // allow overriding this in WindowDoFnOperator because this one dynamically creates\n   // the DoFn\n-  protected OldDoFn<InputT, FnOutputT> getDoFn() {\n-    return doFn;\n+  protected OldDoFn<InputT, FnOutputT> getOldDoFn() {\n+    return oldDoFn;\n   }\n \n   @Override\n   public void open() throws Exception {\n     super.open();\n \n-    this.doFn = getDoFn();\n+    this.oldDoFn = getOldDoFn();\n \n     currentInputWatermark = Long.MIN_VALUE;\n     currentOutputWatermark = currentInputWatermark;\n \n-    Aggregator.AggregatorFactory aggregatorFactory = new Aggregator.AggregatorFactory() {\n+    AggregatorFactory aggregatorFactory = new AggregatorFactory() {\n       @Override\n       public <InputT, AccumT, OutputT> Aggregator<InputT, OutputT> createAggregatorForDoFn(\n           Class<?> fnClass,\n@@ -220,7 +246,7 @@ public void open() throws Exception {\n \n     DoFnRunner<InputT, FnOutputT> doFnRunner = DoFnRunners.createDefault(\n         serializedOptions.getPipelineOptions(),\n-        doFn,\n+        oldDoFn,\n         sideInputReader,\n         outputManagerFactory.create(output),\n         mainOutputTag,\n@@ -232,13 +258,13 @@ public void open() throws Exception {\n     pushbackDoFnRunner =\n         PushbackSideInputDoFnRunner.create(doFnRunner, sideInputs, sideInputHandler);\n \n-    doFn.setup();\n+    oldDoFn.setup();\n   }\n \n   @Override\n   public void close() throws Exception {\n     super.close();\n-    doFn.teardown();\n+    oldDoFn.teardown();\n   }\n \n   protected final long getPushbackWatermarkHold() {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "sha": "870430844a7c072314e624cb968f0d7f00b46b29",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItem.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItem.java",
                "patch": "@@ -18,7 +18,7 @@\n package org.apache.beam.runners.flink.translation.wrappers.streaming;\n \n import java.util.Collections;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowedValue;\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItem.java",
                "sha": "b53658e5fd7ddc5cd65e2e358fbf79a85c30c3e0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItemCoder.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItemCoder.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItemCoder.java",
                "patch": "@@ -26,12 +26,12 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.util.List;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItemCoder;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.coders.CoderException;\n import org.apache.beam.sdk.coders.StandardCoder;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItemCoder;\n import org.apache.beam.sdk.util.PropertyNames;\n import org.apache.beam.sdk.util.WindowedValue;\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/SingletonKeyedWorkItemCoder.java",
                "sha": "ad306886b856e06133fb83fe811f98d7003d15ab",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WindowDoFnOperator.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WindowDoFnOperator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 8,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WindowDoFnOperator.java",
                "patch": "@@ -39,6 +39,8 @@\n import java.util.concurrent.ScheduledFuture;\n import javax.annotation.Nullable;\n import org.apache.beam.runners.core.GroupAlsoByWindowViaWindowSetDoFn;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n+import org.apache.beam.runners.core.KeyedWorkItems;\n import org.apache.beam.runners.core.SystemReduceFn;\n import org.apache.beam.runners.flink.translation.wrappers.DataInputViewWrapper;\n import org.apache.beam.sdk.coders.Coder;\n@@ -47,8 +49,6 @@\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.ExecutionContext;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n-import org.apache.beam.sdk.util.KeyedWorkItems;\n import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -59,8 +59,6 @@\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n-\n-\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.tuple.Tuple2;\n import org.apache.flink.core.memory.DataInputView;\n@@ -108,7 +106,7 @@ public WindowDoFnOperator(\n       PipelineOptions options,\n       Coder<K> keyCoder) {\n     super(\n-        null,\n+        (OldDoFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>>) null,\n         inputType,\n         mainOutputTag,\n         sideOutputTags,\n@@ -126,7 +124,7 @@ public WindowDoFnOperator(\n   }\n \n   @Override\n-  protected OldDoFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>> getDoFn() {\n+  protected OldDoFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>> getOldDoFn() {\n     StateInternalsFactory<K> stateInternalsFactory = new StateInternalsFactory<K>() {\n       @Override\n       public StateInternals<K> stateInternalsForKey(K key) {\n@@ -140,10 +138,10 @@ public WindowDoFnOperator(\n     // has the window type as generic parameter while WindowingStrategy is almost always\n     // untyped.\n     @SuppressWarnings(\"unchecked\")\n-    OldDoFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>> doFn =\n+    OldDoFn<KeyedWorkItem<K, InputT>, KV<K, OutputT>> oldDoFn =\n         GroupAlsoByWindowViaWindowSetDoFn.create(\n             windowingStrategy, stateInternalsFactory, (SystemReduceFn) systemReduceFn);\n-    return doFn;\n+    return oldDoFn;\n   }\n \n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WindowDoFnOperator.java",
                "sha": "9cea5296d12b031ca80ecf691ed8329164f27e78",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java",
                "patch": "@@ -18,9 +18,9 @@\n package org.apache.beam.runners.flink.translation.wrappers.streaming;\n \n import java.nio.ByteBuffer;\n+import org.apache.beam.runners.core.KeyedWorkItem;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.util.CoderUtils;\n-import org.apache.beam.sdk.util.KeyedWorkItem;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.functions.KeySelector;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java",
                "sha": "1dff36772f2f86bacbac6ce8d728654a3d577dfa",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/PipelineOptionsTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/PipelineOptionsTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/flink/runner/src/test/java/org/apache/beam/runners/flink/PipelineOptionsTest.java",
                "patch": "@@ -29,7 +29,7 @@\n import org.apache.beam.sdk.options.Description;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -151,9 +151,9 @@ public void parDoBaseClassPipelineOptionsSerializationTest() throws Exception {\n   }\n \n \n-  private static class TestDoFn extends OldDoFn<Object, Object> {\n+  private static class TestDoFn extends DoFn<Object, Object> {\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       Assert.assertNotNull(c.getPipelineOptions());\n       Assert.assertEquals(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/PipelineOptionsTest.java",
                "sha": "4c97cc78230d622176f78bc30ad59c133d0a3fe3",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/DoFnOperatorTest.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/DoFnOperatorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 7,
                "filename": "runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/DoFnOperatorTest.java",
                "patch": "@@ -25,7 +25,6 @@\n import com.google.common.collect.FluentIterable;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableMap;\n-\n import java.util.Collections;\n import java.util.HashMap;\n import javax.annotation.Nullable;\n@@ -35,7 +34,7 @@\n import org.apache.beam.sdk.coders.StringUtf8Coder;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.testing.PCollectionViewTesting;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.join.RawUnionValue;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n@@ -280,7 +279,7 @@ public RawUnionValue apply(@Nullable Object o) {\n     });\n   }\n \n-  private static class MultiOutputDoFn extends OldDoFn<String, String> {\n+  private static class MultiOutputDoFn extends DoFn<String, String> {\n     private TupleTag<String> sideOutput1;\n     private TupleTag<String> sideOutput2;\n \n@@ -289,7 +288,7 @@ public MultiOutputDoFn(TupleTag<String> sideOutput1, TupleTag<String> sideOutput\n       this.sideOutput2 = sideOutput2;\n     }\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       if (c.element().equals(\"one\")) {\n         c.sideOutput(sideOutput1, \"side: one\");\n@@ -303,9 +302,9 @@ public void processElement(ProcessContext c) throws Exception {\n     }\n   }\n \n-  private static class IdentityDoFn<T> extends OldDoFn<T, T> {\n-    @Override\n-    public void processElement(OldDoFn<T, T>.ProcessContext c) throws Exception {\n+  private static class IdentityDoFn<T> extends DoFn<T, T> {\n+    @ProcessElement\n+    public void processElement(ProcessContext c) throws Exception {\n       c.output(c.element());\n     }\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/DoFnOperatorTest.java",
                "sha": "113802ddabd3e17db220120634a3cd1d14c14bdd",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/GroupByNullKeyTest.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/GroupByNullKeyTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/GroupByNullKeyTest.java",
                "patch": "@@ -24,8 +24,8 @@\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.io.TextIO;\n import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.GroupByKey;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.AfterWatermark;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n@@ -64,10 +64,8 @@ protected void postSubmit() throws Exception {\n   /**\n    * DoFn extracting user and timestamp.\n    */\n-  public static class ExtractUserAndTimestamp extends OldDoFn<KV<Integer, String>, String> {\n-    private static final long serialVersionUID = 0;\n-\n-    @Override\n+  private static class ExtractUserAndTimestamp extends DoFn<KV<Integer, String>, String> {\n+    @ProcessElement\n     public void processElement(ProcessContext c) {\n       KV<Integer, String> record = c.element();\n       int timestamp = record.getKey();\n@@ -100,16 +98,16 @@ protected void testProgram() throws Exception {\n               .withAllowedLateness(Duration.ZERO)\n               .discardingFiredPanes())\n \n-          .apply(ParDo.of(new OldDoFn<String, KV<Void, String>>() {\n-            @Override\n+          .apply(ParDo.of(new DoFn<String, KV<Void, String>>() {\n+            @ProcessElement\n             public void processElement(ProcessContext c) throws Exception {\n               String elem = c.element();\n-              c.output(KV.<Void, String>of((Void) null, elem));\n+              c.output(KV.<Void, String>of(null, elem));\n             }\n           }))\n           .apply(GroupByKey.<Void, String>create())\n-          .apply(ParDo.of(new OldDoFn<KV<Void, Iterable<String>>, String>() {\n-            @Override\n+          .apply(ParDo.of(new DoFn<KV<Void, Iterable<String>>, String>() {\n+            @ProcessElement\n             public void processElement(ProcessContext c) throws Exception {\n               KV<Void, Iterable<String>> elem = c.element();\n               StringBuilder str = new StringBuilder();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/GroupByNullKeyTest.java",
                "sha": "663b910e3e80a7384e89665852a09d8777de673f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/TopWikipediaSessionsITCase.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/TopWikipediaSessionsITCase.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/TopWikipediaSessionsITCase.java",
                "patch": "@@ -26,7 +26,7 @@\n import org.apache.beam.sdk.io.TextIO;\n import org.apache.beam.sdk.transforms.Count;\n import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.Sessions;\n import org.apache.beam.sdk.transforms.windowing.Window;\n@@ -100,8 +100,8 @@ protected void testProgram() throws Exception {\n \n \n \n-      .apply(ParDo.of(new OldDoFn<TableRow, String>() {\n-        @Override\n+      .apply(ParDo.of(new DoFn<TableRow, String>() {\n+        @ProcessElement\n         public void processElement(ProcessContext c) throws Exception {\n           TableRow row = c.element();\n           long timestamp = (Integer) row.get(\"timestamp\");\n@@ -117,8 +117,8 @@ public void processElement(ProcessContext c) throws Exception {\n \n       .apply(Count.<String>perElement());\n \n-    PCollection<String> format = output.apply(ParDo.of(new OldDoFn<KV<String, Long>, String>() {\n-      @Override\n+    PCollection<String> format = output.apply(ParDo.of(new DoFn<KV<String, Long>, String>() {\n+      @ProcessElement\n       public void processElement(ProcessContext c) throws Exception {\n         KV<String, Long> el = c.element();\n         String out = \"user: \" + el.getKey() + \" value:\" + el.getValue();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/flink/runner/src/test/java/org/apache/beam/runners/flink/streaming/TopWikipediaSessionsITCase.java",
                "sha": "9e6bba8fe0cbf7480b8a9acbe7c980a0626504a9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/gearpump/pom.xml",
                "patch": "@@ -23,7 +23,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/pom.xml",
                "sha": "bb35ad71e6f3da6648f7697e494e28183dae432f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/GearpumpRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/GearpumpRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/GearpumpRunner.java",
                "patch": "@@ -107,7 +107,7 @@ public GearpumpPipelineResult run(Pipeline pipeline) {\n     TranslationContext translationContext = new TranslationContext(streamApp, options);\n     GearpumpPipelineTranslator translator = new GearpumpPipelineTranslator(translationContext);\n     translator.translate(pipeline);\n-    streamApp.run();\n+    streamApp.submit();\n \n     return null;\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/GearpumpRunner.java",
                "sha": "4083922a72c72537b17f57ebe86df5b1b65a3bff",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/examples/StreamingWordCount.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/examples/StreamingWordCount.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 7,
                "filename": "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/examples/StreamingWordCount.java",
                "patch": "@@ -24,7 +24,7 @@\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.transforms.Count;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n import org.apache.beam.sdk.transforms.windowing.Window;\n@@ -42,10 +42,10 @@\n  */\n public class StreamingWordCount {\n \n-  static class ExtractWordsFn extends OldDoFn<String, String> {\n+  static class ExtractWordsFn extends DoFn<String, String> {\n \n-    @Override\n-    public void processElement(ProcessContext c) {\n+    @ProcessElement\n+    public void process(ProcessContext c) {\n       // Split the line into words.\n       String[] words = c.element().split(\"[^a-zA-Z']+\");\n \n@@ -58,11 +58,11 @@ public void processElement(ProcessContext c) {\n     }\n   }\n \n-  static class FormatAsStringFn extends OldDoFn<KV<String, Long>, String> {\n+  static class FormatAsStringFn extends DoFn<KV<String, Long>, String> {\n     private static final Logger LOG = LoggerFactory.getLogger(FormatAsStringFn.class);\n \n-    @Override\n-    public void processElement(ProcessContext c) {\n+    @ProcessElement\n+    public void process(ProcessContext c) {\n       String row = c.element().getKey()\n           + \" - \" + c.element().getValue()\n           + \" @ \" + c.timestamp().toString();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/examples/StreamingWordCount.java",
                "sha": "b2d762a200eefad35e558a8d4a2321a64ede4911",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundMultiTranslator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundMultiTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundMultiTranslator.java",
                "patch": "@@ -64,7 +64,7 @@ public void translate(ParDo.BoundMulti<InputT, OutputT> transform, TranslationCo\n     JavaStream<WindowedValue<KV<TupleTag<OutputT>, OutputT>>> outputStream = inputStream.flatMap(\n         new DoFnMultiFunction<>(\n             context.getPipelineOptions(),\n-            transform.getNewFn(),\n+            transform.getFn(),\n             transform.getMainOutputTag(),\n             transform.getSideOutputTags(),\n             inputT.getWindowingStrategy(),",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundMultiTranslator.java",
                "sha": "24f973421ebb0f6d5162c796ec7b5d3b0cd12685",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundTranslator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundTranslator.java",
                "patch": "@@ -38,7 +38,7 @@\n \n   @Override\n   public void translate(ParDo.Bound<InputT, OutputT> transform, TranslationContext context) {\n-    DoFn<InputT, OutputT> doFn = transform.getNewFn();\n+    DoFn<InputT, OutputT> doFn = transform.getFn();\n     PCollection<OutputT> output = context.getOutput(transform);\n     WindowingStrategy<?, ?> windowingStrategy = output.getWindowingStrategy();\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/ParDoBoundTranslator.java",
                "sha": "689bc08e0f47ebdefce5455a6fc40d19f641fc28",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/TranslationContext.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/TranslationContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/TranslationContext.java",
                "patch": "@@ -54,8 +54,7 @@ public TranslationContext(JavaStreamApp streamApp, GearpumpPipelineOptions pipel\n   }\n \n   public void setCurrentTransform(TransformHierarchy.Node treeNode) {\n-    this.currentTransform = AppliedPTransform.of(treeNode.getFullName(),\n-        treeNode.getInput(), treeNode.getOutput(), (PTransform) treeNode.getTransform());\n+    this.currentTransform = treeNode.toAppliedPTransform();\n   }\n \n   public GearpumpPipelineOptions getPipelineOptions() {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/TranslationContext.java",
                "sha": "63fb619ce0c1a59c37e14a2d31a72bdb97f9549a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/DoFnRunnerFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/DoFnRunnerFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/DoFnRunnerFactory.java",
                "patch": "@@ -21,12 +21,12 @@\n import java.io.Serializable;\n import java.util.List;\n \n+import org.apache.beam.runners.core.AggregatorFactory;\n import org.apache.beam.runners.core.DoFnRunner;\n import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.core.SimpleDoFnRunner;\n import org.apache.beam.runners.gearpump.GearpumpPipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.transforms.Aggregator.AggregatorFactory;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.util.ExecutionContext;\n import org.apache.beam.sdk.util.SideInputReader;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/DoFnRunnerFactory.java",
                "sha": "7e1402f4ebeb8a2d4d5851328e8a0a6460e7bc48",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/NoOpAggregatorFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/NoOpAggregatorFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/NoOpAggregatorFactory.java",
                "patch": "@@ -20,8 +20,8 @@\n \n import java.io.Serializable;\n \n+import org.apache.beam.runners.core.AggregatorFactory;\n import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Aggregator.AggregatorFactory;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.util.ExecutionContext;\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/gearpump/src/main/java/org/apache/beam/runners/gearpump/translators/utils/NoOpAggregatorFactory.java",
                "sha": "22ffc4d926b0953b821cedf33a8128fe05b3e0dd",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/pom.xml",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "runners/google-cloud-dataflow-java/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -79,7 +79,9 @@\n             <configuration>\n               <excludedGroups>\n                 org.apache.beam.sdk.testing.UsesStatefulParDo,\n-                org.apache.beam.sdk.testing.UsesSplittableParDo\n+                org.apache.beam.sdk.testing.UsesTimersInParDo,\n+                org.apache.beam.sdk.testing.UsesSplittableParDo,\n+                org.apache.beam.sdk.testing.UsesMetrics\n               </excludedGroups>\n               <excludes>\n                 <exclude>org.apache.beam.sdk.transforms.FlattenTest</exclude>\n@@ -132,15 +134,15 @@\n                     <!-- com.google.common is too generic, need to exclude guava-testlib -->\n                     <exclude>com.google.common.**.testing.*</exclude>\n                   </excludes>\n-                  <shadedPattern>org.apache.beam.sdk.repackaged.com.google.common</shadedPattern>\n+                  <shadedPattern>org.apache.beam.runners.dataflow.repackaged.com.google.common</shadedPattern>\n                 </relocation>\n                 <relocation>\n                   <pattern>com.google.thirdparty</pattern>\n-                  <shadedPattern>org.apache.beam.sdk.repackaged.com.google.thirdparty</shadedPattern>\n+                  <shadedPattern>org.apache.beam.runners.dataflow.repackaged.com.google.thirdparty</shadedPattern>\n                 </relocation>\n                 <relocation>\n                   <pattern>com.google.cloud.bigtable</pattern>\n-                  <shadedPattern>org.apache.beam.sdk.repackaged.com.google.cloud.bigtable</shadedPattern>\n+                  <shadedPattern>org.apache.beam.runners.dataflow.repackaged.com.google.cloud.bigtable</shadedPattern>\n                   <excludes>\n                     <exclude>com.google.cloud.bigtable.config.BigtableOptions*</exclude>\n                     <exclude>com.google.cloud.bigtable.config.CredentialOptions*</exclude>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/pom.xml",
                "sha": "0094791f0d30e0a80aa1ec1cb05a3bfad7a4124f",
                "status": "modified"
            },
            {
                "additions": 140,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowClient.java",
                "changes": 140,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowClient.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowClient.java",
                "patch": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.dataflow;\n+\n+import static com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.api.services.dataflow.Dataflow;\n+import com.google.api.services.dataflow.Dataflow.Projects.Jobs;\n+import com.google.api.services.dataflow.model.Job;\n+import com.google.api.services.dataflow.model.JobMetrics;\n+import com.google.api.services.dataflow.model.LeaseWorkItemRequest;\n+import com.google.api.services.dataflow.model.LeaseWorkItemResponse;\n+import com.google.api.services.dataflow.model.ListJobMessagesResponse;\n+import com.google.api.services.dataflow.model.ListJobsResponse;\n+import com.google.api.services.dataflow.model.ReportWorkItemStatusRequest;\n+import com.google.api.services.dataflow.model.ReportWorkItemStatusResponse;\n+import java.io.IOException;\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+\n+/**\n+ * Wrapper around the generated {@link Dataflow} client to provide common functionality.\n+ */\n+public class DataflowClient {\n+\n+  public static DataflowClient create(DataflowPipelineOptions options) {\n+    return new DataflowClient(options.getDataflowClient(), options.getProject());\n+  }\n+\n+  private final Dataflow dataflow;\n+  private final String projectId;\n+\n+  private DataflowClient(Dataflow dataflow, String projectId) {\n+    this.dataflow = checkNotNull(dataflow, \"dataflow\");\n+    this.projectId = checkNotNull(projectId, \"options\");\n+  }\n+\n+  /**\n+   * Creates the Dataflow {@link Job}.\n+   */\n+  public Job createJob(@Nonnull Job job) throws IOException {\n+    checkNotNull(job, \"job\");\n+    Jobs.Create jobsCreate = dataflow.projects().jobs().create(projectId, job);\n+    return jobsCreate.execute();\n+  }\n+\n+  /**\n+   * Lists Dataflow {@link Job Jobs} in the project associated with\n+   * the {@link DataflowPipelineOptions}.\n+   */\n+  public ListJobsResponse listJobs(@Nullable String pageToken) throws IOException {\n+    Jobs.List jobsList = dataflow.projects().jobs()\n+        .list(projectId)\n+        .setPageToken(pageToken);\n+    return jobsList.execute();\n+  }\n+\n+  /**\n+   * Updates the Dataflow {@link Job} with the given {@code jobId}.\n+   */\n+  public Job updateJob(@Nonnull String jobId, @Nonnull Job content) throws IOException {\n+    checkNotNull(jobId, \"jobId\");\n+    checkNotNull(content, \"content\");\n+    Jobs.Update jobsUpdate = dataflow.projects().jobs()\n+        .update(projectId, jobId, content);\n+    return jobsUpdate.execute();\n+  }\n+\n+  /**\n+   * Gets the Dataflow {@link Job} with the given {@code jobId}.\n+   */\n+  public Job getJob(@Nonnull String jobId) throws IOException {\n+    checkNotNull(jobId, \"jobId\");\n+    Jobs.Get jobsGet = dataflow.projects().jobs()\n+        .get(projectId, jobId);\n+    return jobsGet.execute();\n+  }\n+\n+  /**\n+   * Gets the {@link JobMetrics} with the given {@code jobId}.\n+   */\n+  public JobMetrics getJobMetrics(@Nonnull String jobId) throws IOException {\n+    checkNotNull(jobId, \"jobId\");\n+    Jobs.GetMetrics jobsGetMetrics = dataflow.projects().jobs()\n+        .getMetrics(projectId, jobId);\n+    return jobsGetMetrics.execute();\n+  }\n+\n+  /**\n+   * Lists job messages with the given {@code jobId}.\n+   */\n+  public ListJobMessagesResponse listJobMessages(\n+      @Nonnull String jobId, @Nullable String pageToken) throws IOException {\n+    checkNotNull(jobId, \"jobId\");\n+    Jobs.Messages.List jobMessagesList = dataflow.projects().jobs().messages()\n+        .list(projectId, jobId)\n+        .setPageToken(pageToken);\n+    return jobMessagesList.execute();\n+  }\n+\n+  /**\n+   * Leases the work item for {@code jobId}.\n+   */\n+  public LeaseWorkItemResponse leaseWorkItem(\n+      @Nonnull String jobId, @Nonnull LeaseWorkItemRequest request) throws IOException {\n+    checkNotNull(jobId, \"jobId\");\n+    checkNotNull(request, \"request\");\n+    Jobs.WorkItems.Lease jobWorkItemsLease = dataflow.projects().jobs().workItems()\n+        .lease(projectId, jobId, request);\n+    return jobWorkItemsLease.execute();\n+  }\n+\n+  /**\n+   * Reports the status of the work item for {@code jobId}.\n+   */\n+  public ReportWorkItemStatusResponse reportWorkItemStatus(\n+      @Nonnull String jobId, @Nonnull ReportWorkItemStatusRequest request) throws IOException {\n+    checkNotNull(jobId, \"jobId\");\n+    checkNotNull(request, \"request\");\n+    Jobs.WorkItems.ReportStatus jobWorkItemsReportStatus = dataflow.projects().jobs().workItems()\n+        .reportStatus(projectId, jobId, request);\n+    return jobWorkItemsReportStatus.execute();\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowClient.java",
                "sha": "3536d7272c3a8738130ef07c649cbe8637939c6d",
                "status": "added"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 20,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java",
                "patch": "@@ -62,15 +62,15 @@\n   private String jobId;\n \n   /**\n-   * Google cloud project to associate this pipeline with.\n+   * The {@link DataflowPipelineOptions} for the job.\n    */\n-  private String projectId;\n+  private final DataflowPipelineOptions dataflowOptions;\n \n   /**\n    * Client for the Dataflow service. This can be used to query the service\n    * for information about the job.\n    */\n-  private DataflowPipelineOptions dataflowOptions;\n+  private final DataflowClient dataflowClient;\n \n   /**\n    * The state the job terminated in or {@code null} if the job has not terminated.\n@@ -119,19 +119,17 @@\n   /**\n    * Constructs the job.\n    *\n-   * @param projectId the project id\n    * @param jobId the job id\n    * @param dataflowOptions used to configure the client for the Dataflow Service\n    * @param aggregatorTransforms a mapping from aggregators to PTransforms\n    */\n   public DataflowPipelineJob(\n-      String projectId,\n       String jobId,\n       DataflowPipelineOptions dataflowOptions,\n       DataflowAggregatorTransforms aggregatorTransforms) {\n-    this.projectId = projectId;\n     this.jobId = jobId;\n     this.dataflowOptions = dataflowOptions;\n+    this.dataflowClient = (dataflowOptions == null ? null : DataflowClient.create(dataflowOptions));\n     this.aggregatorTransforms = aggregatorTransforms;\n   }\n \n@@ -146,7 +144,7 @@ public String getJobId() {\n    * Get the project this job exists in.\n    */\n   public String getProjectId() {\n-    return projectId;\n+    return dataflowOptions.getProject();\n   }\n \n   /**\n@@ -249,7 +247,7 @@ State waitUntilFinish(\n       MonitoringUtil.JobMessagesHandler messageHandler,\n       Sleeper sleeper,\n       NanoClock nanoClock) throws IOException, InterruptedException {\n-    MonitoringUtil monitor = new MonitoringUtil(projectId, dataflowOptions.getDataflowClient());\n+    MonitoringUtil monitor = new MonitoringUtil(dataflowClient);\n \n     long lastTimestamp = 0;\n     BackOff backoff;\n@@ -338,13 +336,11 @@ State waitUntilFinish(\n   @Override\n   public State cancel() throws IOException {\n     Job content = new Job();\n-    content.setProjectId(projectId);\n+    content.setProjectId(getProjectId());\n     content.setId(jobId);\n     content.setRequestedState(\"JOB_STATE_CANCELLED\");\n     try {\n-      dataflowOptions.getDataflowClient().projects().jobs()\n-          .update(projectId, jobId, content)\n-          .execute();\n+      dataflowClient.updateJob(jobId, content);\n       return State.CANCELLED;\n     } catch (IOException e) {\n       State state = getState();\n@@ -409,16 +405,12 @@ private Job getJobWithRetries(BackOff backoff, Sleeper sleeper) throws IOExcepti\n     // Retry loop ends in return or throw\n     while (true) {\n       try {\n-        Job job = dataflowOptions.getDataflowClient()\n-            .projects()\n-            .jobs()\n-            .get(projectId, jobId)\n-            .execute();\n+        Job job = dataflowClient.getJob(jobId);\n         State currentState = MonitoringUtil.toState(job.getCurrentState());\n         if (currentState.isTerminal()) {\n           terminalState = currentState;\n           replacedByJob = new DataflowPipelineJob(\n-              getProjectId(), job.getReplacedByJobId(), dataflowOptions, aggregatorTransforms);\n+              job.getReplacedByJobId(), dataflowOptions, aggregatorTransforms);\n         }\n         return job;\n       } catch (IOException exn) {\n@@ -484,8 +476,7 @@ public MetricResults metrics() {\n         metricUpdates = terminalMetricUpdates;\n       } else {\n         boolean terminal = getState().isTerminal();\n-        JobMetrics jobMetrics = dataflowOptions.getDataflowClient()\n-            .projects().jobs().getMetrics(projectId, jobId).execute();\n+        JobMetrics jobMetrics = dataflowClient.getJobMetrics(jobId);\n         metricUpdates = jobMetrics.getMetrics();\n         if (terminal && jobMetrics.getMetrics() != null) {\n           terminalMetricUpdates = metricUpdates;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineJob.java",
                "sha": "00c88f994958ea9414fc362c437f1c0f4e8a0cc9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslator.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 10,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslator.java",
                "patch": "@@ -423,10 +423,6 @@ public Job translate(List<DataflowPackage> packages) {\n \n       WorkerPool workerPool = new WorkerPool();\n \n-      if (options.getTeardownPolicy() != null) {\n-        workerPool.setTeardownPolicy(options.getTeardownPolicy().getTeardownPolicyName());\n-      }\n-\n       if (options.isStreaming()) {\n         job.setType(\"JOB_TYPE_STREAMING\");\n       } else {\n@@ -530,8 +526,7 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n             \"no translator registered for \" + transform);\n       }\n       LOG.debug(\"Translating {}\", transform);\n-      currentTransform = AppliedPTransform.of(\n-          node.getFullName(), node.getInput(), node.getOutput(), (PTransform) transform);\n+      currentTransform = node.toAppliedPTransform();\n       translator.translate(transform, this);\n       currentTransform = null;\n     }\n@@ -960,14 +955,14 @@ public void translate(\n           private <InputT, OutputT> void translateMultiHelper(\n               ParDo.BoundMulti<InputT, OutputT> transform,\n               TranslationContext context) {\n-            rejectStatefulDoFn(transform.getNewFn());\n+            rejectStatefulDoFn(transform.getFn());\n \n             context.addStep(transform, \"ParallelDo\");\n             translateInputs(context.getInput(transform), transform.getSideInputs(), context);\n             BiMap<Long, TupleTag<?>> outputMap =\n                 translateOutputs(context.getOutput(transform), context);\n             translateFn(\n-                transform.getNewFn(),\n+                transform.getFn(),\n                 context.getInput(transform).getWindowingStrategy(),\n                 transform.getSideInputs(),\n                 context.getInput(transform).getCoder(),\n@@ -990,13 +985,13 @@ public void translate(\n           private <InputT, OutputT> void translateSingleHelper(\n               ParDo.Bound<InputT, OutputT> transform,\n               TranslationContext context) {\n-            rejectStatefulDoFn(transform.getNewFn());\n+            rejectStatefulDoFn(transform.getFn());\n \n             context.addStep(transform, \"ParallelDo\");\n             translateInputs(context.getInput(transform), transform.getSideInputs(), context);\n             long mainOutput = context.addOutput(context.getOutput(transform));\n             translateFn(\n-                transform.getNewFn(),\n+                transform.getFn(),\n                 context.getInput(transform).getWindowingStrategy(),\n                 transform.getSideInputs(),\n                 context.getInput(transform).getCoder(),",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslator.java",
                "sha": "8d2b0763be10bc4e73e9ee7ff4d6ed95dee63452",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java",
                "changes": 136,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 61,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java",
                "patch": "@@ -32,7 +32,6 @@\n import com.google.api.services.clouddebugger.v2.model.Debuggee;\n import com.google.api.services.clouddebugger.v2.model.RegisterDebuggeeRequest;\n import com.google.api.services.clouddebugger.v2.model.RegisterDebuggeeResponse;\n-import com.google.api.services.dataflow.Dataflow;\n import com.google.api.services.dataflow.model.DataflowPackage;\n import com.google.api.services.dataflow.model.Job;\n import com.google.api.services.dataflow.model.ListJobsResponse;\n@@ -41,6 +40,7 @@\n import com.google.common.base.Function;\n import com.google.common.base.Joiner;\n import com.google.common.base.Optional;\n+import com.google.common.base.Strings;\n import com.google.common.base.Utf8;\n import com.google.common.collect.ForwardingMap;\n import com.google.common.collect.HashMultimap;\n@@ -118,6 +118,7 @@\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsValidator;\n import org.apache.beam.sdk.options.StreamingOptions;\n+import org.apache.beam.sdk.options.ValueProvider.NestedValueProvider;\n import org.apache.beam.sdk.runners.PipelineRunner;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n import org.apache.beam.sdk.transforms.Aggregator;\n@@ -193,7 +194,7 @@\n   private final DataflowPipelineOptions options;\n \n   /** Client for the Dataflow service. This is used to actually submit jobs. */\n-  private final Dataflow dataflowClient;\n+  private final DataflowClient dataflowClient;\n \n   /** Translator for this DataflowRunner, based on options. */\n   private final DataflowPipelineTranslator translator;\n@@ -204,21 +205,11 @@\n   /** A set of user defined functions to invoke at different points in execution. */\n   private DataflowRunnerHooks hooks;\n \n-  // Environment version information.\n-  private static final String ENVIRONMENT_MAJOR_VERSION = \"5\";\n-\n-  // Default Docker container images that execute Dataflow worker harness, residing in Google\n-  // Container Registry, separately for Batch and Streaming.\n-  public static final String BATCH_WORKER_HARNESS_CONTAINER_IMAGE =\n-      \"dataflow.gcr.io/v1beta3/beam-java-batch:beam-master-20161129\";\n-  public static final String STREAMING_WORKER_HARNESS_CONTAINER_IMAGE =\n-      \"dataflow.gcr.io/v1beta3/beam-java-streaming:beam-master-20161129\";\n-\n   // The limit of CreateJob request size.\n   private static final int CREATE_JOB_REQUEST_LIMIT_BYTES = 10 * 1024 * 1024;\n \n   @VisibleForTesting\n-  static final int GCS_UPLOAD_BUFFER_SIZE_BYTES_DEFAULT = 1 * 1024 * 1024;\n+  static final int GCS_UPLOAD_BUFFER_SIZE_BYTES_DEFAULT = 1024 * 1024;\n \n   private final Set<PCollection<?>> pcollectionsRequiringIndexedFormat;\n \n@@ -262,6 +253,10 @@ public static DataflowRunner fromOptions(PipelineOptions options) {\n         \"DataflowRunner requires stagingLocation, and it is missing in PipelineOptions.\");\n     validator.validateOutputFilePrefixSupported(dataflowOptions.getStagingLocation());\n \n+    if (!Strings.isNullOrEmpty(dataflowOptions.getSaveProfilesToGcs())) {\n+      validator.validateOutputFilePrefixSupported(dataflowOptions.getSaveProfilesToGcs());\n+    }\n+\n     if (dataflowOptions.getFilesToStage() == null) {\n       dataflowOptions.setFilesToStage(detectClassPathResourcesToStage(\n           DataflowRunner.class.getClassLoader()));\n@@ -320,7 +315,7 @@ public static DataflowRunner fromOptions(PipelineOptions options) {\n \n   @VisibleForTesting protected DataflowRunner(DataflowPipelineOptions options) {\n     this.options = options;\n-    this.dataflowClient = options.getDataflowClient();\n+    this.dataflowClient = DataflowClient.create(options);\n     this.translator = DataflowPipelineTranslator.fromOptions(options);\n     this.pcollectionsRequiringIndexedFormat = new HashSet<>();\n     this.ptransformViewsWithNonDeterministicKeyCoders = new HashSet<>();\n@@ -541,7 +536,9 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n \n     // Requirements about the service.\n     Map<String, Object> environmentVersion = new HashMap<>();\n-    environmentVersion.put(PropertyNames.ENVIRONMENT_VERSION_MAJOR_KEY, ENVIRONMENT_MAJOR_VERSION);\n+    environmentVersion.put(\n+        PropertyNames.ENVIRONMENT_VERSION_MAJOR_KEY,\n+        DataflowRunnerInfo.getDataflowRunnerInfo().getEnvironmentMajorVersion());\n     newJob.getEnvironment().setVersion(environmentVersion);\n     // Default jobType is JAVA_BATCH_AUTOSCALING: A Java job with workers that the job can\n     // autoscale if specified.\n@@ -596,11 +593,7 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n     }\n     Job jobResult;\n     try {\n-      jobResult = dataflowClient\n-              .projects()\n-              .jobs()\n-              .create(options.getProject(), newJob)\n-              .execute();\n+      jobResult = dataflowClient.createJob(newJob);\n     } catch (GoogleJsonResponseException e) {\n       String errorMessages = \"Unexpected errors\";\n       if (e.getDetails() != null) {\n@@ -628,8 +621,8 @@ public DataflowPipelineJob run(Pipeline pipeline) {\n \n     // Use a raw client for post-launch monitoring, as status calls may fail\n     // regularly and need not be retried automatically.\n-    DataflowPipelineJob dataflowPipelineJob = new DataflowPipelineJob(\n-        options.getProject(), jobResult.getId(), options, aggregatorTransforms);\n+    DataflowPipelineJob dataflowPipelineJob =\n+        new DataflowPipelineJob(jobResult.getId(), options, aggregatorTransforms);\n \n     // If the service returned client request id, the SDK needs to compare it\n     // with the original id generated in the request, if they are not the same\n@@ -760,7 +753,7 @@ private GroupByKeyAndSortValuesOnly() {\n     }\n \n     @Override\n-    public PCollection<KV<K1, Iterable<KV<K2, V>>>> apply(PCollection<KV<K1, KV<K2, V>>> input) {\n+    public PCollection<KV<K1, Iterable<KV<K2, V>>>> expand(PCollection<KV<K1, KV<K2, V>>> input) {\n       PCollection<KV<K1, Iterable<KV<K2, V>>>> rval =\n           PCollection.<KV<K1, Iterable<KV<K2, V>>>>createPrimitiveOutputInternal(\n           input.getPipeline(),\n@@ -818,7 +811,8 @@ private GroupByWindowHashAsKeyAndWindowAsSortKey(IsmRecordCoder<?> ismCoderForHa\n     }\n \n     @Override\n-    public PCollection<KV<Integer, Iterable<KV<W, WindowedValue<T>>>>> apply(PCollection<T> input) {\n+    public PCollection<KV<Integer, Iterable<KV<W, WindowedValue<T>>>>> expand(\n+        PCollection<T> input) {\n       @SuppressWarnings(\"unchecked\")\n       Coder<W> windowCoder = (Coder<W>)\n           input.getWindowingStrategy().getWindowFn().windowCoder();\n@@ -906,7 +900,7 @@ public BatchViewAsSingleton(DataflowRunner runner, View.AsSingleton<T> transform\n     }\n \n     @Override\n-    public PCollectionView<T> apply(PCollection<T> input) {\n+    public PCollectionView<T> expand(PCollection<T> input) {\n       @SuppressWarnings(\"unchecked\")\n       Coder<BoundedWindow> windowCoder = (Coder<BoundedWindow>)\n           input.getWindowingStrategy().getWindowFn().windowCoder();\n@@ -997,7 +991,7 @@ public BatchViewAsIterable(DataflowRunner runner, View.AsIterable<T> transform)\n     }\n \n     @Override\n-    public PCollectionView<Iterable<T>> apply(PCollection<T> input) {\n+    public PCollectionView<Iterable<T>> expand(PCollection<T> input) {\n       PCollectionView<Iterable<T>> view = PCollectionViews.iterableView(\n           input.getPipeline(), input.getWindowingStrategy(), input.getCoder());\n       return BatchViewAsList.applyForIterableLike(runner, input, view);\n@@ -1101,7 +1095,7 @@ public BatchViewAsList(DataflowRunner runner, View.AsList<T> transform) {\n     }\n \n     @Override\n-    public PCollectionView<List<T>> apply(PCollection<T> input) {\n+    public PCollectionView<List<T>> expand(PCollection<T> input) {\n       PCollectionView<List<T>> view = PCollectionViews.listView(\n           input.getPipeline(), input.getWindowingStrategy(), input.getCoder());\n       return applyForIterableLike(runner, input, view);\n@@ -1269,7 +1263,7 @@ public BatchViewAsMap(DataflowRunner runner, View.AsMap<K, V> transform) {\n     }\n \n     @Override\n-    public PCollectionView<Map<K, V>> apply(PCollection<KV<K, V>> input) {\n+    public PCollectionView<Map<K, V>> expand(PCollection<KV<K, V>> input) {\n       return this.<BoundedWindow>applyInternal(input);\n     }\n \n@@ -1410,7 +1404,7 @@ public GroupByKeyHashAndSortByKeyAndWindow(IsmRecordCoder<?> coder) {\n \n       @Override\n       public PCollection<KV<Integer, Iterable<KV<KV<K, W>, WindowedValue<V>>>>>\n-          apply(PCollection<KV<K, V>> input) {\n+      expand(PCollection<KV<K, V>> input) {\n \n         @SuppressWarnings(\"unchecked\")\n         Coder<W> windowCoder = (Coder<W>)\n@@ -1758,7 +1752,7 @@ public BatchViewAsMultimap(DataflowRunner runner, View.AsMultimap<K, V> transfor\n     }\n \n     @Override\n-    public PCollectionView<Map<K, Iterable<V>>> apply(PCollection<KV<K, V>> input) {\n+    public PCollectionView<Map<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n       return this.<BoundedWindow>applyInternal(input);\n     }\n \n@@ -2060,13 +2054,16 @@ public BatchWrite(DataflowRunner runner, Write.Bound<T> transform) {\n     }\n \n     @Override\n-    public PDone apply(PCollection<T> input) {\n+    public PDone expand(PCollection<T> input) {\n       if (transform.getSink() instanceof FileBasedSink) {\n         FileBasedSink<?> sink = (FileBasedSink<?>) transform.getSink();\n-        PathValidator validator = runner.options.getPathValidator();\n-        validator.validateOutputFilePrefixSupported(sink.getBaseOutputFilename());\n+        if (sink.getBaseOutputFilenameProvider().isAccessible()) {\n+          PathValidator validator = runner.options.getPathValidator();\n+          validator.validateOutputFilePrefixSupported(\n+              sink.getBaseOutputFilenameProvider().get());\n+        }\n       }\n-      return transform.apply(input);\n+      return transform.expand(input);\n     }\n   }\n \n@@ -2075,7 +2072,7 @@ public PDone apply(PCollection<T> input) {\n   // ================================================================================\n \n   /**\n-   * Suppress application of {@link PubsubUnboundedSource#apply} in streaming mode so that we\n+   * Suppress application of {@link PubsubUnboundedSource#expand} in streaming mode so that we\n    * can instead defer to Windmill's implementation.\n    */\n   private static class StreamingPubsubIORead<T> extends PTransform<PBegin, PCollection<T>> {\n@@ -2094,7 +2091,7 @@ public StreamingPubsubIORead(\n     }\n \n     @Override\n-    public PCollection<T> apply(PBegin input) {\n+    public PCollection<T> expand(PBegin input) {\n       return PCollection.<T>createPrimitiveOutputInternal(\n           input.getPipeline(), WindowingStrategy.globalDefault(), IsBounded.UNBOUNDED)\n           .setCoder(transform.getElementCoder());\n@@ -2125,14 +2122,27 @@ public void translate(\n       PubsubUnboundedSource<T> overriddenTransform = transform.getOverriddenTransform();\n       context.addStep(transform, \"ParallelRead\");\n       context.addInput(PropertyNames.FORMAT, \"pubsub\");\n-      if (overriddenTransform.getTopic() != null) {\n-        context.addInput(PropertyNames.PUBSUB_TOPIC,\n-                         overriddenTransform.getTopic().getV1Beta1Path());\n+      if (overriddenTransform.getTopicProvider() != null) {\n+        if (overriddenTransform.getTopicProvider().isAccessible()) {\n+          context.addInput(\n+              PropertyNames.PUBSUB_TOPIC, overriddenTransform.getTopic().getV1Beta1Path());\n+        } else {\n+          context.addInput(\n+              PropertyNames.PUBSUB_TOPIC_OVERRIDE,\n+              ((NestedValueProvider) overriddenTransform.getTopicProvider()).propertyName());\n+        }\n       }\n-      if (overriddenTransform.getSubscription() != null) {\n-        context.addInput(\n-            PropertyNames.PUBSUB_SUBSCRIPTION,\n-            overriddenTransform.getSubscription().getV1Beta1Path());\n+      if (overriddenTransform.getSubscriptionProvider() != null) {\n+        if (overriddenTransform.getSubscriptionProvider().isAccessible()) {\n+          context.addInput(\n+              PropertyNames.PUBSUB_SUBSCRIPTION,\n+              overriddenTransform.getSubscription().getV1Beta1Path());\n+        } else {\n+          context.addInput(\n+              PropertyNames.PUBSUB_SUBSCRIPTION_OVERRIDE,\n+              ((NestedValueProvider) overriddenTransform.getSubscriptionProvider())\n+              .propertyName());\n+        }\n       }\n       if (overriddenTransform.getTimestampLabel() != null) {\n         context.addInput(PropertyNames.PUBSUB_TIMESTAMP_LABEL,\n@@ -2146,7 +2156,7 @@ public void translate(\n   }\n \n   /**\n-   * Suppress application of {@link PubsubUnboundedSink#apply} in streaming mode so that we\n+   * Suppress application of {@link PubsubUnboundedSink#expand} in streaming mode so that we\n    * can instead defer to Windmill's implementation.\n    */\n   private static class StreamingPubsubIOWrite<T> extends PTransform<PCollection<T>, PDone> {\n@@ -2165,7 +2175,7 @@ public StreamingPubsubIOWrite(\n     }\n \n     @Override\n-    public PDone apply(PCollection<T> input) {\n+    public PDone expand(PCollection<T> input) {\n       return PDone.in(input.getPipeline());\n     }\n \n@@ -2195,7 +2205,14 @@ public void translate(\n       PubsubUnboundedSink<T> overriddenTransform = transform.getOverriddenTransform();\n       context.addStep(transform, \"ParallelWrite\");\n       context.addInput(PropertyNames.FORMAT, \"pubsub\");\n-      context.addInput(PropertyNames.PUBSUB_TOPIC, overriddenTransform.getTopic().getV1Beta1Path());\n+      if (overriddenTransform.getTopicProvider().isAccessible()) {\n+        context.addInput(\n+            PropertyNames.PUBSUB_TOPIC, overriddenTransform.getTopic().getV1Beta1Path());\n+      } else {\n+        context.addInput(\n+            PropertyNames.PUBSUB_TOPIC_OVERRIDE,\n+            ((NestedValueProvider) overriddenTransform.getTopicProvider()).propertyName());\n+      }\n       if (overriddenTransform.getTimestampLabel() != null) {\n         context.addInput(PropertyNames.PUBSUB_TIMESTAMP_LABEL,\n                          overriddenTransform.getTimestampLabel());\n@@ -2236,7 +2253,7 @@ public StreamingUnboundedRead(DataflowRunner runner, Read.Unbounded<T> transform\n     }\n \n     @Override\n-    public final PCollection<T> apply(PInput input) {\n+    public final PCollection<T> expand(PInput input) {\n       source.validate();\n \n       if (source.requiresDeduping()) {\n@@ -2261,7 +2278,7 @@ private ReadWithIds(UnboundedSource<T, ?> source) {\n       }\n \n       @Override\n-      public final PCollection<ValueWithRecordId<T>> apply(PInput input) {\n+      public final PCollection<ValueWithRecordId<T>> expand(PInput input) {\n         return PCollection.<ValueWithRecordId<T>>createPrimitiveOutputInternal(\n             input.getPipeline(), WindowingStrategy.globalDefault(), IsBounded.UNBOUNDED);\n       }\n@@ -2311,7 +2328,7 @@ public void translate(ReadWithIds<?> transform,\n     // more per-key overhead.\n     private static final int NUM_RESHARD_KEYS = 10000;\n     @Override\n-    public PCollection<T> apply(PCollection<ValueWithRecordId<T>> input) {\n+    public PCollection<T> expand(PCollection<ValueWithRecordId<T>> input) {\n       return input\n           .apply(WithKeys.of(new SerializableFunction<ValueWithRecordId<T>, Integer>() {\n                     @Override\n@@ -2351,7 +2368,7 @@ public StreamingBoundedRead(DataflowRunner runner, Read.Bounded<T> transform) {\n     }\n \n     @Override\n-    public final PCollection<T> apply(PBegin input) {\n+    public final PCollection<T> expand(PBegin input) {\n       source.validate();\n \n       return Pipeline.applyTransform(input, new DataflowUnboundedReadFromBoundedSource<>(source))\n@@ -2409,7 +2426,7 @@ public StreamingViewAsMap(DataflowRunner runner, View.AsMap<K, V> transform) {\n     }\n \n     @Override\n-    public PCollectionView<Map<K, V>> apply(PCollection<KV<K, V>> input) {\n+    public PCollectionView<Map<K, V>> expand(PCollection<KV<K, V>> input) {\n       PCollectionView<Map<K, V>> view =\n           PCollectionViews.mapView(\n               input.getPipeline(),\n@@ -2454,7 +2471,7 @@ public StreamingViewAsMultimap(DataflowRunner runner, View.AsMultimap<K, V> tran\n     }\n \n     @Override\n-    public PCollectionView<Map<K, Iterable<V>>> apply(PCollection<KV<K, V>> input) {\n+    public PCollectionView<Map<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n       PCollectionView<Map<K, Iterable<V>>> view =\n           PCollectionViews.multimapView(\n               input.getPipeline(),\n@@ -2495,7 +2512,7 @@ protected String getKindString() {\n     public StreamingViewAsList(DataflowRunner runner, View.AsList<T> transform) {}\n \n     @Override\n-    public PCollectionView<List<T>> apply(PCollection<T> input) {\n+    public PCollectionView<List<T>> expand(PCollection<T> input) {\n       PCollectionView<List<T>> view =\n           PCollectionViews.listView(\n               input.getPipeline(),\n@@ -2527,7 +2544,7 @@ protected String getKindString() {\n     public StreamingViewAsIterable(DataflowRunner runner, View.AsIterable<T> transform) { }\n \n     @Override\n-    public PCollectionView<Iterable<T>> apply(PCollection<T> input) {\n+    public PCollectionView<Iterable<T>> expand(PCollection<T> input) {\n       PCollectionView<Iterable<T>> view =\n           PCollectionViews.iterableView(\n               input.getPipeline(),\n@@ -2570,7 +2587,7 @@ public StreamingViewAsSingleton(DataflowRunner runner, View.AsSingleton<T> trans\n     }\n \n     @Override\n-    public PCollectionView<T> apply(PCollection<T> input) {\n+    public PCollectionView<T> expand(PCollection<T> input) {\n       Combine.Globally<T, T> combine = Combine.globally(\n           new SingletonCombine<>(transform.hasDefaultValue(), transform.defaultValue()));\n       if (!transform.hasDefaultValue()) {\n@@ -2628,7 +2645,7 @@ public StreamingCombineGloballyAsSingletonView(\n     }\n \n     @Override\n-    public PCollectionView<OutputT> apply(PCollection<InputT> input) {\n+    public PCollectionView<OutputT> expand(PCollection<InputT> input) {\n       PCollection<OutputT> combined =\n           input.apply(Combine.<InputT, OutputT>globally(transform.getCombineFn())\n               .withoutDefaults()\n@@ -2754,7 +2771,7 @@ public UnsupportedIO(DataflowRunner runner, PubsubUnboundedSink<?> transform) {\n \n \n     @Override\n-    public OutputT apply(InputT input) {\n+    public OutputT expand(InputT input) {\n       String mode = input.getPipeline().getOptions().as(StreamingOptions.class).isStreaming()\n           ? \"streaming\" : \"batch\";\n       String name =\n@@ -2809,10 +2826,7 @@ private String getJobIdFromName(String jobName) {\n       ListJobsResponse listResult;\n       String token = null;\n       do {\n-        listResult = dataflowClient.projects().jobs()\n-            .list(options.getProject())\n-            .setPageToken(token)\n-            .execute();\n+        listResult = dataflowClient.listJobs(token);\n         token = listResult.getNextPageToken();\n         for (Job job : listResult.getJobs()) {\n           if (job.getName().equals(jobName)",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java",
                "sha": "711b1b0d0555370860a795958f89f74e6a790aa7",
                "status": "modified"
            },
            {
                "additions": 92,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunnerInfo.java",
                "changes": 92,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunnerInfo.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunnerInfo.java",
                "patch": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.dataflow;\n+\n+import static com.google.common.base.Preconditions.checkState;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Properties;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Populates versioning and other information for {@link DataflowRunner}.\n+ */\n+public final class DataflowRunnerInfo {\n+  private static final Logger LOG = LoggerFactory.getLogger(DataflowRunnerInfo.class);\n+\n+  private static final String PROPERTIES_PATH =\n+      \"/org/apache/beam/runners/dataflow/dataflow.properties\";\n+\n+  private static class LazyInit {\n+    private static final DataflowRunnerInfo INSTANCE = new DataflowRunnerInfo(PROPERTIES_PATH);\n+  }\n+\n+  /**\n+   * Returns an instance of {@link DataflowRunnerInfo}.\n+   */\n+  public static DataflowRunnerInfo getDataflowRunnerInfo() {\n+    return LazyInit.INSTANCE;\n+  }\n+\n+  private Properties properties;\n+\n+  private static final String ENVIRONMENT_MAJOR_VERSION_KEY = \"environment.major.version\";\n+  private static final String BATCH_WORKER_HARNESS_CONTAINER_IMAGE_KEY = \"worker.image.batch\";\n+  private static final String STREAMING_WORKER_HARNESS_CONTAINER_IMAGE_KEY =\n+      \"worker.image.streaming\";\n+\n+  /** Provides the environment's major version number. */\n+  public String getEnvironmentMajorVersion() {\n+    checkState(\n+        properties.containsKey(ENVIRONMENT_MAJOR_VERSION_KEY), \"Unknown environment major version\");\n+    return properties.getProperty(ENVIRONMENT_MAJOR_VERSION_KEY);\n+  }\n+\n+  /** Provides the batch worker harness container image name. */\n+  public String getBatchWorkerHarnessContainerImage() {\n+    checkState(\n+        properties.containsKey(BATCH_WORKER_HARNESS_CONTAINER_IMAGE_KEY),\n+        \"Unknown batch worker harness container image\");\n+    return properties.getProperty(BATCH_WORKER_HARNESS_CONTAINER_IMAGE_KEY);\n+  }\n+\n+  /** Provides the streaming worker harness container image name. */\n+  public String getStreamingWorkerHarnessContainerImage() {\n+    checkState(\n+        properties.containsKey(STREAMING_WORKER_HARNESS_CONTAINER_IMAGE_KEY),\n+        \"Unknown streaming worker harness container image\");\n+    return properties.getProperty(STREAMING_WORKER_HARNESS_CONTAINER_IMAGE_KEY);\n+  }\n+\n+  private DataflowRunnerInfo(String resourcePath) {\n+    properties = new Properties();\n+\n+    try (InputStream in = DataflowRunnerInfo.class.getResourceAsStream(PROPERTIES_PATH)) {\n+      if (in == null) {\n+        LOG.warn(\"Dataflow runner properties resource not found: {}\", resourcePath);\n+        return;\n+      }\n+\n+      properties.load(in);\n+    } catch (IOException e) {\n+      LOG.warn(\"Error loading Dataflow runner properties resource: \", e);\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunnerInfo.java",
                "sha": "59cb8a49de26067097a429681abcd9f625235668",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/AssignWindows.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/AssignWindows.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/AssignWindows.java",
                "patch": "@@ -18,7 +18,7 @@\n package org.apache.beam.runners.dataflow.internal;\n \n import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.Window;\n@@ -53,7 +53,7 @@ public AssignWindows(Window.Bound<T> transform) {\n   }\n \n   @Override\n-  public PCollection<T> apply(PCollection<T> input) {\n+  public PCollection<T> expand(PCollection<T> input) {\n     WindowingStrategy<?, ?> outputStrategy =\n         transform.getOutputStrategyInternal(input.getWindowingStrategy());\n     if (transform.getWindowFn() != null) {\n@@ -63,9 +63,9 @@ public AssignWindows(Window.Bound<T> transform) {\n     } else {\n       // If the windowFn didn't change, we just run a pass-through transform and then set the\n       // new windowing strategy.\n-      return input.apply(\"Identity\", ParDo.of(new OldDoFn<T, T>() {\n-        @Override\n-        public void processElement(OldDoFn<T, T>.ProcessContext c) throws Exception {\n+      return input.apply(\"Identity\", ParDo.of(new DoFn<T, T>() {\n+        @ProcessElement\n+        public void processElement(ProcessContext c) throws Exception {\n           c.output(c.element());\n         }\n       })).setWindowingStrategyInternal(outputStrategy);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/AssignWindows.java",
                "sha": "27fe13d76b1dfba0f551ab5dcdbe80925bf8ffcd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/DataflowUnboundedReadFromBoundedSource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/DataflowUnboundedReadFromBoundedSource.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/DataflowUnboundedReadFromBoundedSource.java",
                "patch": "@@ -93,7 +93,7 @@ public DataflowUnboundedReadFromBoundedSource(BoundedSource<T> source) {\n   }\n \n   @Override\n-  public PCollection<T> apply(PBegin input) {\n+  public PCollection<T> expand(PBegin input) {\n     return input.getPipeline().apply(\n         Read.from(new BoundedToUnboundedSourceAdapter<>(source)));\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/internal/DataflowUnboundedReadFromBoundedSource.java",
                "sha": "e1eedd8ae7749547ce59303822b423af0616eadc",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowPipelineWorkerPoolOptions.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowPipelineWorkerPoolOptions.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 51,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowPipelineWorkerPoolOptions.java",
                "patch": "@@ -20,7 +20,7 @@\n import com.fasterxml.jackson.annotation.JsonIgnore;\n import java.util.List;\n import javax.annotation.Nullable;\n-import org.apache.beam.runners.dataflow.DataflowRunner;\n+import org.apache.beam.runners.dataflow.DataflowRunnerInfo;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.options.Default;\n import org.apache.beam.sdk.options.DefaultValueFactory;\n@@ -130,9 +130,9 @@ public String getAlgorithm() {\n     public String create(PipelineOptions options) {\n       DataflowPipelineOptions dataflowOptions = options.as(DataflowPipelineOptions.class);\n       if (dataflowOptions.isStreaming()) {\n-        return DataflowRunner.STREAMING_WORKER_HARNESS_CONTAINER_IMAGE;\n+        return DataflowRunnerInfo.getDataflowRunnerInfo().getStreamingWorkerHarnessContainerImage();\n       } else {\n-        return DataflowRunner.BATCH_WORKER_HARNESS_CONTAINER_IMAGE;\n+        return DataflowRunnerInfo.getDataflowRunnerInfo().getBatchWorkerHarnessContainerImage();\n       }\n     }\n   }\n@@ -154,9 +154,9 @@ public String create(PipelineOptions options) {\n    * workers.\n    *\n    * <p>Default is up to the Dataflow service. Expected format is\n-   * regions/REGION/subnetworks/SUBNETWORK.\n-   *\n-   * <p>You may also need to specify network option.\n+   * regions/REGION/subnetworks/SUBNETWORK or the fully qualified subnetwork name, beginning with\n+   * https://..., e.g. https://www.googleapis.com/compute/alpha/projects/PROJECT/\n+   *   regions/REGION/subnetworks/SUBNETWORK\n    */\n   @Description(\"GCE subnetwork for launching workers. For more information, see the reference \"\n       + \"documentation https://cloud.google.com/compute/docs/networking. \"\n@@ -190,51 +190,6 @@ public String create(PipelineOptions options) {\n   String getWorkerMachineType();\n   void setWorkerMachineType(String value);\n \n-  /**\n-   * The policy for tearing down the workers spun up by the service.\n-   *\n-   * @deprecated Dataflow Service will only support TEARDOWN_ALWAYS policy in the future.\n-   */\n-  @Deprecated\n-  enum TeardownPolicy {\n-    /**\n-     * All VMs created for a Dataflow job are deleted when the job finishes, regardless of whether\n-     * it fails or succeeds.\n-     */\n-    TEARDOWN_ALWAYS(\"TEARDOWN_ALWAYS\"),\n-    /**\n-     * All VMs created for a Dataflow job are left running when the job finishes, regardless of\n-     * whether it fails or succeeds.\n-     */\n-    TEARDOWN_NEVER(\"TEARDOWN_NEVER\"),\n-    /**\n-     * All VMs created for a Dataflow job are deleted when the job succeeds, but are left running\n-     * when it fails. (This is typically used for debugging failing jobs by SSHing into the\n-     * workers.)\n-     */\n-    TEARDOWN_ON_SUCCESS(\"TEARDOWN_ON_SUCCESS\");\n-\n-    private final String teardownPolicy;\n-\n-    TeardownPolicy(String teardownPolicy) {\n-      this.teardownPolicy = teardownPolicy;\n-    }\n-\n-    public String getTeardownPolicyName() {\n-      return this.teardownPolicy;\n-    }\n-  }\n-\n-  /**\n-   * The teardown policy for the VMs.\n-   *\n-   * <p>If unset, the Dataflow service will choose a reasonable default.\n-   */\n-  @Description(\"The teardown policy for the VMs. If unset, the Dataflow service will \"\n-      + \"choose a reasonable default.\")\n-  TeardownPolicy getTeardownPolicy();\n-  void setTeardownPolicy(TeardownPolicy value);\n-\n   /**\n    * List of local files to make available to workers.\n    *",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowPipelineWorkerPoolOptions.java",
                "sha": "05086b0d8901989f90571546b7e870f2fed2f128",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptions.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptions.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptions.java",
                "patch": "@@ -30,10 +30,10 @@\n @Hidden\n public interface DataflowProfilingOptions {\n \n-  @Description(\"Whether to periodically dump profiling information to local disk.\\n\"\n-      + \"WARNING: Enabling this option may fill local disk with profiling information.\")\n-  boolean getEnableProfilingAgent();\n-  void setEnableProfilingAgent(boolean enabled);\n+  @Description(\"When set to a non-empty value, enables recording profiles and saving them to GCS.\\n\"\n+      + \"Profiles will continue until the pipeline is stopped or updated without this option.\\n\")\n+  String getSaveProfilesToGcs();\n+  void setSaveProfilesToGcs(String gcsPath);\n \n   @Description(\n       \"[INTERNAL] Additional configuration for the profiling agent. Not typically necessary.\")",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptions.java",
                "sha": "a87d688f05eab14b632b465d32a142f994eb689c",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunner.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 7,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunner.java",
                "patch": "@@ -34,6 +34,7 @@\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n import javax.annotation.Nullable;\n+import org.apache.beam.runners.dataflow.DataflowClient;\n import org.apache.beam.runners.dataflow.DataflowPipelineJob;\n import org.apache.beam.runners.dataflow.DataflowRunner;\n import org.apache.beam.runners.dataflow.util.MonitoringUtil;\n@@ -60,16 +61,23 @@\n  */\n public class TestDataflowRunner extends PipelineRunner<DataflowPipelineJob> {\n   private static final String TENTATIVE_COUNTER = \"tentative\";\n-  private static final String WATERMARK_METRIC_SUFFIX = \"windmill-data-watermark\";\n+  // See https://issues.apache.org/jira/browse/BEAM-1170\n+  // we need to either fix the API or pipe the DRAINED signal through\n+  @VisibleForTesting\n+  static final String LEGACY_WATERMARK_METRIC_SUFFIX = \"windmill-data-watermark\";\n+  @VisibleForTesting\n+  static final String WATERMARK_METRIC_SUFFIX = \"DataWatermark\";\n   private static final long MAX_WATERMARK_VALUE = -2L;\n   private static final Logger LOG = LoggerFactory.getLogger(TestDataflowRunner.class);\n \n   private final TestDataflowPipelineOptions options;\n+  private final DataflowClient dataflowClient;\n   private final DataflowRunner runner;\n   private int expectedNumberOfAssertions = 0;\n \n   TestDataflowRunner(TestDataflowPipelineOptions options) {\n     this.options = options;\n+    this.dataflowClient = DataflowClient.create(options);\n     this.runner = DataflowRunner.fromOptions(options);\n   }\n \n@@ -244,6 +252,23 @@ DataflowPipelineJob run(Pipeline pipeline, DataflowRunner runner) {\n     return Optional.absent();\n   }\n \n+  /**\n+   * Checks wether a metric is a streaming watermark.\n+   *\n+   * @return true if the metric is a watermark.\n+   */\n+  boolean isWatermark(MetricUpdate metric) {\n+    if (metric.getName() == null || metric.getName().getName() == null) {\n+      return false; // no name -> shouldn't happen, not the watermark\n+    }\n+    if (metric.getScalar() == null) {\n+      return false; // no scalar value -> not the watermark\n+    }\n+    String name = metric.getName().getName();\n+    return name.endsWith(LEGACY_WATERMARK_METRIC_SUFFIX)\n+        || name.endsWith(WATERMARK_METRIC_SUFFIX);\n+  }\n+\n   /**\n    * Check watermarks of the streaming job. At least one watermark metric must exist.\n    *\n@@ -253,10 +278,7 @@ DataflowPipelineJob run(Pipeline pipeline, DataflowRunner runner) {\n   boolean atMaxWatermark(DataflowPipelineJob job, JobMetrics metrics) {\n     boolean hasMaxWatermark = false;\n     for (MetricUpdate metric : metrics.getMetrics()) {\n-      if (metric.getName() == null\n-          || metric.getName().getName() == null\n-          || !metric.getName().getName().endsWith(WATERMARK_METRIC_SUFFIX)\n-          || metric.getScalar() == null) {\n+      if (!isWatermark(metric)) {\n         continue;\n       }\n       BigDecimal watermark = (BigDecimal) metric.getScalar();\n@@ -279,8 +301,7 @@ boolean atMaxWatermark(DataflowPipelineJob job, JobMetrics metrics) {\n   JobMetrics getJobMetrics(DataflowPipelineJob job) {\n     JobMetrics metrics = null;\n     try {\n-      metrics = options.getDataflowClient().projects().jobs()\n-          .getMetrics(job.getProjectId(), job.getJobId()).execute();\n+      metrics = dataflowClient.getJobMetrics(job.getJobId());\n     } catch (IOException e) {\n       LOG.warn(\"Failed to get job metrics: \", e);\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunner.java",
                "sha": "056444877734d1566be2cfa2aa4201d13427cf78",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/DataflowTemplateJob.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/DataflowTemplateJob.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/DataflowTemplateJob.java",
                "patch": "@@ -30,7 +30,7 @@\n       \"The result of template creation should not be used.\";\n \n   public DataflowTemplateJob() {\n-    super(null, null, null, null);\n+    super(null, null, null);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/DataflowTemplateJob.java",
                "sha": "1a44963e4e781d21cd171687fc486b29be4ebe02",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/MonitoringUtil.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/MonitoringUtil.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 17,
                "filename": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/MonitoringUtil.java",
                "patch": "@@ -20,7 +20,6 @@\n import static org.apache.beam.runners.dataflow.util.TimeUtil.fromCloudTime;\n \n import com.google.api.services.dataflow.Dataflow;\n-import com.google.api.services.dataflow.Dataflow.Projects.Jobs.Messages;\n import com.google.api.services.dataflow.model.JobMessage;\n import com.google.api.services.dataflow.model.ListJobMessagesResponse;\n import com.google.common.base.MoreObjects;\n@@ -35,6 +34,7 @@\n import java.util.List;\n import java.util.Map;\n import javax.annotation.Nullable;\n+import org.apache.beam.runners.dataflow.DataflowClient;\n import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n import org.apache.beam.sdk.PipelineResult.State;\n import org.joda.time.Instant;\n@@ -67,8 +67,7 @@\n   private static final String JOB_MESSAGE_DETAILED = \"JOB_MESSAGE_DETAILED\";\n   private static final String JOB_MESSAGE_DEBUG = \"JOB_MESSAGE_DEBUG\";\n \n-  private String projectId;\n-  private Messages messagesClient;\n+  private final DataflowClient dataflowClient;\n \n   /**\n    * An interface that can be used for defining callbacks to receive a list\n@@ -115,14 +114,8 @@ public void process(List<JobMessage> messages) {\n   }\n \n   /** Construct a helper for monitoring. */\n-  public MonitoringUtil(String projectId, Dataflow dataflow) {\n-    this(projectId, dataflow.projects().jobs().messages());\n-  }\n-\n-  // @VisibleForTesting\n-  MonitoringUtil(String projectId, Messages messagesClient) {\n-    this.projectId = projectId;\n-    this.messagesClient = messagesClient;\n+  public MonitoringUtil(DataflowClient dataflowClient) {\n+    this.dataflowClient = dataflowClient;\n   }\n \n   /**\n@@ -157,12 +150,7 @@ public int compare(JobMessage o1, JobMessage o2) {\n     ArrayList<JobMessage> allMessages = new ArrayList<>();\n     String pageToken = null;\n     while (true) {\n-      Messages.List listRequest = messagesClient.list(projectId, jobId);\n-      if (pageToken != null) {\n-        listRequest.setPageToken(pageToken);\n-      }\n-      ListJobMessagesResponse response = listRequest.execute();\n-\n+      ListJobMessagesResponse response = dataflowClient.listJobMessages(jobId, pageToken);\n       if (response == null || response.getJobMessages() == null) {\n         return allMessages;\n       }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/util/MonitoringUtil.java",
                "sha": "d0a24bf0ec302215e2d69c634d145b6aa65c36e5",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/resources/org/apache/beam/runners/dataflow/dataflow.properties",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/main/resources/org/apache/beam/runners/dataflow/dataflow.properties?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/google-cloud-dataflow-java/src/main/resources/org/apache/beam/runners/dataflow/dataflow.properties",
                "patch": "@@ -0,0 +1,23 @@\n+#\n+#    Licensed to the Apache Software Foundation (ASF) under one or more\n+#    contributor license agreements.  See the NOTICE file distributed with\n+#    this work for additional information regarding copyright ownership.\n+#    The ASF licenses this file to You under the Apache License, Version 2.0\n+#    (the \"License\"); you may not use this file except in compliance with\n+#    the License.  You may obtain a copy of the License at\n+#\n+#       http://www.apache.org/licenses/LICENSE-2.0\n+#\n+#    Unless required by applicable law or agreed to in writing, software\n+#    distributed under the License is distributed on an \"AS IS\" BASIS,\n+#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+#    See the License for the specific language governing permissions and\n+#    limitations under the License.\n+#\n+# Dataflow runtime properties\n+\n+environment.major.version=6\n+\n+worker.image.batch=dataflow.gcr.io/v1beta3/beam-java-batch:beam-master-20161216\n+\n+worker.image.streaming=dataflow.gcr.io/v1beta3/beam-java-streaming:beam-master-20161216",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/main/resources/org/apache/beam/runners/dataflow/dataflow.properties",
                "sha": "27a518f02713a2c323699b541867238062141531",
                "status": "added"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineJobTest.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineJobTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 25,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineJobTest.java",
                "patch": "@@ -115,6 +115,7 @@ public void setup() {\n \n     options = PipelineOptionsFactory.as(TestDataflowPipelineOptions.class);\n     options.setDataflowClient(mockWorkflowClient);\n+    options.setProject(PROJECT_ID);\n   }\n \n   /**\n@@ -156,12 +157,13 @@ public void testWaitToFinishMessagesFail() throws Exception {\n     Messages.List listRequest = mock(Dataflow.Projects.Jobs.Messages.List.class);\n     when(mockJobs.messages()).thenReturn(mockMessages);\n     when(mockMessages.list(eq(PROJECT_ID), eq(JOB_ID))).thenReturn(listRequest);\n+    when(listRequest.setPageToken(eq((String) null))).thenReturn(listRequest);\n     when(listRequest.execute()).thenThrow(SocketTimeoutException.class);\n     DataflowAggregatorTransforms dataflowAggregatorTransforms =\n         mock(DataflowAggregatorTransforms.class);\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(\n-        PROJECT_ID, JOB_ID, options, dataflowAggregatorTransforms);\n+    DataflowPipelineJob job =\n+        new DataflowPipelineJob(JOB_ID, options, dataflowAggregatorTransforms);\n \n     State state = job.waitUntilFinish(\n         Duration.standardMinutes(5), jobHandler, fastClock, fastClock);\n@@ -182,8 +184,8 @@ public State mockWaitToFinishInState(State state) throws Exception {\n     DataflowAggregatorTransforms dataflowAggregatorTransforms =\n         mock(DataflowAggregatorTransforms.class);\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(\n-        PROJECT_ID, JOB_ID, options, dataflowAggregatorTransforms);\n+    DataflowPipelineJob job =\n+        new DataflowPipelineJob(JOB_ID, options, dataflowAggregatorTransforms);\n \n     return job.waitUntilFinish(Duration.standardMinutes(1), null, fastClock, fastClock);\n   }\n@@ -249,8 +251,8 @@ public void testWaitToFinishFail() throws Exception {\n     DataflowAggregatorTransforms dataflowAggregatorTransforms =\n         mock(DataflowAggregatorTransforms.class);\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(\n-        PROJECT_ID, JOB_ID, options, dataflowAggregatorTransforms);\n+    DataflowPipelineJob job =\n+        new DataflowPipelineJob(JOB_ID, options, dataflowAggregatorTransforms);\n \n     long startTime = fastClock.nanoTime();\n     State state = job.waitUntilFinish(Duration.standardMinutes(5), null, fastClock, fastClock);\n@@ -269,8 +271,8 @@ public void testWaitToFinishTimeFail() throws Exception {\n     DataflowAggregatorTransforms dataflowAggregatorTransforms =\n         mock(DataflowAggregatorTransforms.class);\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(\n-        PROJECT_ID, JOB_ID, options, dataflowAggregatorTransforms);\n+    DataflowPipelineJob job =\n+        new DataflowPipelineJob(JOB_ID, options, dataflowAggregatorTransforms);\n     long startTime = fastClock.nanoTime();\n     State state = job.waitUntilFinish(Duration.millis(4), null, fastClock, fastClock);\n     assertEquals(null, state);\n@@ -294,7 +296,7 @@ public void testCumulativeTimeOverflow() throws Exception {\n     FastNanoClockAndFuzzySleeper clock = new FastNanoClockAndFuzzySleeper();\n \n     DataflowPipelineJob job = new DataflowPipelineJob(\n-        PROJECT_ID, JOB_ID, options, dataflowAggregatorTransforms);\n+        JOB_ID, options, dataflowAggregatorTransforms);\n     long startTime = clock.nanoTime();\n     State state = job.waitUntilFinish(Duration.millis(4), null, clock, clock);\n     assertEquals(null, state);\n@@ -317,7 +319,7 @@ public void testGetStateReturnsServiceState() throws Exception {\n         mock(DataflowAggregatorTransforms.class);\n \n     DataflowPipelineJob job = new DataflowPipelineJob(\n-        PROJECT_ID, JOB_ID, options, dataflowAggregatorTransforms);\n+        JOB_ID, options, dataflowAggregatorTransforms);\n \n     assertEquals(\n         State.RUNNING,\n@@ -333,8 +335,8 @@ public void testGetStateWithExceptionReturnsUnknown() throws Exception {\n     DataflowAggregatorTransforms dataflowAggregatorTransforms =\n         mock(DataflowAggregatorTransforms.class);\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(\n-        PROJECT_ID, JOB_ID, options, dataflowAggregatorTransforms);\n+    DataflowPipelineJob job =\n+        new DataflowPipelineJob(JOB_ID, options, dataflowAggregatorTransforms);\n \n     long startTime = fastClock.nanoTime();\n     assertEquals(\n@@ -373,7 +375,7 @@ public void testGetAggregatorValuesWithNoMetricUpdatesReturnsEmptyValue()\n     modelJob.setCurrentState(State.RUNNING.toString());\n \n     DataflowPipelineJob job =\n-        new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, aggregatorTransforms);\n+        new DataflowPipelineJob(JOB_ID, options, aggregatorTransforms);\n \n     AggregatorValues<?> values = job.getAggregatorValues(aggregator);\n \n@@ -408,7 +410,7 @@ public void testGetAggregatorValuesWithNullMetricUpdatesReturnsEmptyValue()\n     modelJob.setCurrentState(State.RUNNING.toString());\n \n     DataflowPipelineJob job =\n-        new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, aggregatorTransforms);\n+        new DataflowPipelineJob(JOB_ID, options, aggregatorTransforms);\n \n     AggregatorValues<?> values = job.getAggregatorValues(aggregator);\n \n@@ -453,8 +455,7 @@ public void testGetAggregatorValuesWithSingleMetricUpdateReturnsSingletonCollect\n     when(getState.execute()).thenReturn(modelJob);\n     modelJob.setCurrentState(State.RUNNING.toString());\n \n-    DataflowPipelineJob job =\n-        new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, aggregatorTransforms);\n+    DataflowPipelineJob job = new DataflowPipelineJob(JOB_ID, options, aggregatorTransforms);\n \n     AggregatorValues<Long> values = job.getAggregatorValues(aggregator);\n \n@@ -521,8 +522,7 @@ public void testGetAggregatorValuesWithMultipleMetricUpdatesReturnsCollection()\n     when(getState.execute()).thenReturn(modelJob);\n     modelJob.setCurrentState(State.RUNNING.toString());\n \n-    DataflowPipelineJob job =\n-        new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, aggregatorTransforms);\n+    DataflowPipelineJob job = new DataflowPipelineJob(JOB_ID, options, aggregatorTransforms);\n \n     AggregatorValues<Long> values = job.getAggregatorValues(aggregator);\n \n@@ -571,7 +571,7 @@ public void testGetAggregatorValuesWithUnrelatedMetricUpdateIgnoresUpdate()\n     modelJob.setCurrentState(State.RUNNING.toString());\n \n     DataflowPipelineJob job =\n-        new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, aggregatorTransforms);\n+        new DataflowPipelineJob(JOB_ID, options, aggregatorTransforms);\n \n     AggregatorValues<Long> values = job.getAggregatorValues(aggregator);\n \n@@ -589,7 +589,7 @@ public void testGetAggregatorValuesWithUnusedAggregatorThrowsException()\n         ImmutableMap.<AppliedPTransform<?, ?, ?>, String>of());\n \n     DataflowPipelineJob job =\n-        new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, aggregatorTransforms);\n+        new DataflowPipelineJob(JOB_ID, options, aggregatorTransforms);\n \n     thrown.expect(IllegalArgumentException.class);\n     thrown.expectMessage(\"not used in this pipeline\");\n@@ -624,8 +624,7 @@ public void testGetAggregatorValuesWhenClientThrowsExceptionThrowsAggregatorRetr\n     when(getState.execute()).thenReturn(modelJob);\n     modelJob.setCurrentState(State.RUNNING.toString());\n \n-    DataflowPipelineJob job =\n-        new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, aggregatorTransforms);\n+    DataflowPipelineJob job = new DataflowPipelineJob(JOB_ID, options, aggregatorTransforms);\n \n     thrown.expect(AggregatorRetrievalException.class);\n     thrown.expectCause(is(cause));\n@@ -690,7 +689,7 @@ public void testCancelUnterminatedJobThatSucceeds() throws IOException {\n     when(mockJobs.update(anyString(), anyString(), any(Job.class))).thenReturn(update);\n     when(update.execute()).thenReturn(new Job());\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, null);\n+    DataflowPipelineJob job = new DataflowPipelineJob(JOB_ID, options, null);\n \n     assertEquals(State.CANCELLED, job.cancel());\n     Job content = new Job();\n@@ -714,7 +713,7 @@ public void testCancelUnterminatedJobThatFails() throws IOException {\n     when(mockJobs.update(anyString(), anyString(), any(Job.class))).thenReturn(update);\n     when(update.execute()).thenThrow(new IOException());\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, null);\n+    DataflowPipelineJob job = new DataflowPipelineJob(JOB_ID, options, null);\n \n     thrown.expect(IOException.class);\n     thrown.expectMessage(\"Failed to cancel the job, \"\n@@ -742,7 +741,7 @@ public void testCancelTerminatedJob() throws IOException {\n     when(mockJobs.update(anyString(), anyString(), any(Job.class))).thenReturn(update);\n     when(update.execute()).thenThrow(new IOException());\n \n-    DataflowPipelineJob job = new DataflowPipelineJob(PROJECT_ID, JOB_ID, options, null);\n+    DataflowPipelineJob job = new DataflowPipelineJob(JOB_ID, options, null);\n \n     assertEquals(State.FAILED, job.cancel());\n     Job content = new Job();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineJobTest.java",
                "sha": "1890da1cb58d1cc162031c07423c113b60c19301",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslatorTest.java",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslatorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 32,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslatorTest.java",
                "patch": "@@ -22,6 +22,7 @@\n import static org.apache.beam.sdk.util.Structs.getString;\n import static org.hamcrest.Matchers.allOf;\n import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.hasEntry;\n import static org.hamcrest.Matchers.hasKey;\n import static org.hamcrest.core.IsInstanceOf.instanceOf;\n import static org.junit.Assert.assertEquals;\n@@ -49,7 +50,6 @@\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n@@ -70,12 +70,13 @@\n import org.apache.beam.sdk.options.ValueProvider;\n import org.apache.beam.sdk.transforms.Count;\n import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.View;\n import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.util.GcsPathValidator;\n import org.apache.beam.sdk.util.GcsUtil;\n import org.apache.beam.sdk.util.PropertyNames;\n import org.apache.beam.sdk.util.Structs;\n@@ -188,27 +189,22 @@ public void testSettingOfSdkPipelineOptions() throws IOException {\n                 p, (DataflowRunner) p.getRunner(), Collections.<DataflowPackage>emptyList())\n             .getJob();\n \n-    // Note that the contents of this materialized map may be changed by the act of reading an\n-    // option, which will cause the default to get materialized whereas it would otherwise be\n-    // left absent. It is permissible to simply alter this test to reflect current behavior.\n-    Map<String, Object> settings = new HashMap<>();\n-    settings.put(\"appName\", \"DataflowPipelineTranslatorTest\");\n-    settings.put(\"project\", \"some-project\");\n-    settings.put(\"pathValidatorClass\",\n-        \"org.apache.beam.sdk.util.GcsPathValidator\");\n-    settings.put(\"runner\", \"org.apache.beam.runners.dataflow.DataflowRunner\");\n-    settings.put(\"jobName\", \"some-job-name\");\n-    settings.put(\"tempLocation\", \"gs://somebucket/some/path\");\n-    settings.put(\"gcpTempLocation\", \"gs://somebucket/some/path\");\n-    settings.put(\"stagingLocation\", \"gs://somebucket/some/path/staging\");\n-    settings.put(\"stableUniqueNames\", \"WARNING\");\n-    settings.put(\"streaming\", false);\n-    settings.put(\"numberOfWorkerHarnessThreads\", 0);\n-    settings.put(\"experiments\", null);\n-\n     Map<String, Object> sdkPipelineOptions = job.getEnvironment().getSdkPipelineOptions();\n     assertThat(sdkPipelineOptions, hasKey(\"options\"));\n-    assertEquals(settings, sdkPipelineOptions.get(\"options\"));\n+    Map<String, Object> optionsMap = (Map<String, Object>) sdkPipelineOptions.get(\"options\");\n+\n+    assertThat(optionsMap, hasEntry(\"appName\", (Object) \"DataflowPipelineTranslatorTest\"));\n+    assertThat(optionsMap, hasEntry(\"project\", (Object) \"some-project\"));\n+    assertThat(optionsMap,\n+        hasEntry(\"pathValidatorClass\", (Object) GcsPathValidator.class.getName()));\n+    assertThat(optionsMap, hasEntry(\"runner\", (Object) DataflowRunner.class.getName()));\n+    assertThat(optionsMap, hasEntry(\"jobName\", (Object) \"some-job-name\"));\n+    assertThat(optionsMap, hasEntry(\"tempLocation\", (Object) \"gs://somebucket/some/path\"));\n+    assertThat(optionsMap,\n+        hasEntry(\"stagingLocation\", (Object) \"gs://somebucket/some/path/staging\"));\n+    assertThat(optionsMap, hasEntry(\"stableUniqueNames\", (Object) \"WARNING\"));\n+    assertThat(optionsMap, hasEntry(\"streaming\", (Object) false));\n+    assertThat(optionsMap, hasEntry(\"numberOfWorkerHarnessThreads\", (Object) 0));\n   }\n \n   @Test\n@@ -508,7 +504,7 @@ private static OutputReference getOutputPortReference(Step step) throws Exceptio\n   }\n \n   /**\n-   * Returns a Step for a OldDoFn by creating and translating a pipeline.\n+   * Returns a Step for a {@link DoFn} by creating and translating a pipeline.\n    */\n   private static Step createPredefinedStep() throws Exception {\n     DataflowPipelineOptions options = buildPipelineOptions();\n@@ -533,8 +529,9 @@ private static Step createPredefinedStep() throws Exception {\n     return step;\n   }\n \n-  private static class NoOpFn extends OldDoFn<String, String> {\n-    @Override public void processElement(ProcessContext c) throws Exception {\n+  private static class NoOpFn extends DoFn<String, String> {\n+    @ProcessElement\n+    public void processElement(ProcessContext c) throws Exception {\n       c.output(c.element());\n     }\n   }\n@@ -551,7 +548,7 @@ public EmbeddedTransform(Step step) {\n     }\n \n     @Override\n-    public PCollection<String> apply(PCollection<String> input) {\n+    public PCollection<String> expand(PCollection<String> input) {\n       return PCollection.createPrimitiveOutputInternal(\n           input.getPipeline(),\n           WindowingStrategy.globalDefault(),\n@@ -585,7 +582,7 @@ public EmbeddedTransform(Step step) {\n       extends PTransform<PCollection<Integer>, PCollection<Integer>> {\n \n     @Override\n-    public PCollection<Integer> apply(PCollection<Integer> input) {\n+    public PCollection<Integer> expand(PCollection<Integer> input) {\n       // Apply an operation so that this is a composite transform.\n       input.apply(Count.<Integer>perElement());\n \n@@ -606,7 +603,7 @@ public EmbeddedTransform(Step step) {\n       extends PTransform<PCollection<Integer>, PDone> {\n \n     @Override\n-    public PDone apply(PCollection<Integer> input) {\n+    public PDone expand(PCollection<Integer> input) {\n       // Apply an operation so that this is a composite transform.\n       input.apply(Count.<Integer>perElement());\n \n@@ -631,7 +628,7 @@ public PDone apply(PCollection<Integer> input) {\n     public final TupleTag<Void> doneTag = new TupleTag<>(\"done\");\n \n     @Override\n-    public PCollectionTuple apply(PCollection<Integer> input) {\n+    public PCollectionTuple expand(PCollection<Integer> input) {\n       PCollection<Integer> sum = input.apply(Sum.integersGlobally());\n \n       // Fails here when attempting to construct a tuple with an unbound object.\n@@ -899,8 +896,8 @@ public void testStepDisplayData() throws Exception {\n     DataflowPipelineTranslator translator = DataflowPipelineTranslator.fromOptions(options);\n     Pipeline pipeline = Pipeline.create(options);\n \n-    OldDoFn<Integer, Integer> fn1 = new OldDoFn<Integer, Integer>() {\n-      @Override\n+    DoFn<Integer, Integer> fn1 = new DoFn<Integer, Integer>() {\n+      @ProcessElement\n       public void processElement(ProcessContext c) throws Exception {\n         c.output(c.element());\n       }\n@@ -915,8 +912,8 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       }\n     };\n \n-    OldDoFn<Integer, Integer> fn2 = new OldDoFn<Integer, Integer>() {\n-      @Override\n+    DoFn<Integer, Integer> fn2 = new DoFn<Integer, Integer>() {\n+      @ProcessElement\n       public void processElement(ProcessContext c) throws Exception {\n         c.output(c.element());\n       }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowPipelineTranslatorTest.java",
                "sha": "ab82941f02a9f97da3e5e1a9f97c56624291ad13",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerInfoTest.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerInfoTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerInfoTest.java",
                "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.dataflow;\n+\n+import static org.hamcrest.Matchers.containsString;\n+import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.junit.Test;\n+\n+/**\n+ * Tests for {@link DataflowRunnerInfo}.\n+ */\n+public class DataflowRunnerInfoTest {\n+\n+  @Test\n+  public void getDataflowRunnerInfo() throws Exception {\n+    DataflowRunnerInfo info = DataflowRunnerInfo.getDataflowRunnerInfo();\n+\n+    String version = info.getEnvironmentMajorVersion();\n+    // Validate major version is a number\n+    assertTrue(\n+        String.format(\"Environment major version number %s is not a number\", version),\n+        version.matches(\"\\\\d+\"));\n+\n+    // Validate container images contain gcr.io\n+    assertThat(\n+        \"batch worker harness container image invalid\",\n+        info.getBatchWorkerHarnessContainerImage(),\n+        containsString(\"gcr.io\"));\n+    assertThat(\n+        \"streaming worker harness container image invalid\",\n+        info.getStreamingWorkerHarnessContainerImage(),\n+        containsString(\"gcr.io\"));\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerInfoTest.java",
                "sha": "9b5b3749736737ff256106125c8c0527947d5516",
                "status": "added"
            },
            {
                "additions": 182,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java",
                "changes": 285,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 103,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java",
                "patch": "@@ -34,6 +34,7 @@\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyString;\n import static org.mockito.Matchers.eq;\n+import static org.mockito.Matchers.isA;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n@@ -81,6 +82,7 @@\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptions.CheckEnabled;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.options.ValueProvider;\n import org.apache.beam.sdk.runners.TransformHierarchy;\n import org.apache.beam.sdk.runners.dataflow.TestCountingSource;\n import org.apache.beam.sdk.testing.ExpectedLogs;\n@@ -110,6 +112,7 @@\n import org.hamcrest.Matchers;\n import org.hamcrest.TypeSafeMatcher;\n import org.joda.time.Instant;\n+import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.internal.matchers.ThrowableMessageMatcher;\n@@ -128,6 +131,11 @@\n @RunWith(JUnit4.class)\n public class DataflowRunnerTest {\n \n+  private static final String VALID_STAGING_BUCKET = \"gs://valid-bucket/staging\";\n+  private static final String VALID_TEMP_BUCKET = \"gs://valid-bucket/temp\";\n+  private static final String VALID_PROFILE_BUCKET = \"gs://valid-bucket/profiles\";\n+  private static final String NON_EXISTENT_BUCKET = \"gs://non-existent-bucket/location\";\n+\n   private static final String PROJECT_ID = \"some-project\";\n \n   @Rule\n@@ -137,13 +145,48 @@\n   @Rule\n   public ExpectedLogs expectedLogs = ExpectedLogs.none(DataflowRunner.class);\n \n+  private Dataflow.Projects.Jobs mockJobs;\n+  private GcsUtil mockGcsUtil;\n+\n   // Asserts that the given Job has all expected fields set.\n   private static void assertValidJob(Job job) {\n     assertNull(job.getId());\n     assertNull(job.getCurrentState());\n     assertTrue(Pattern.matches(\"[a-z]([-a-z0-9]*[a-z0-9])?\", job.getName()));\n   }\n \n+  @Before\n+  public void setUp() throws IOException {\n+    this.mockGcsUtil = mock(GcsUtil.class);\n+    when(mockGcsUtil.create(any(GcsPath.class), anyString()))\n+        .then(new Answer<SeekableByteChannel>() {\n+          @Override\n+          public SeekableByteChannel answer(InvocationOnMock invocation) throws Throwable {\n+            return FileChannel.open(\n+                Files.createTempFile(\"channel-\", \".tmp\"),\n+                StandardOpenOption.CREATE, StandardOpenOption.DELETE_ON_CLOSE);\n+          }\n+        });\n+    when(mockGcsUtil.isGcsPatternSupported(anyString())).thenReturn(true);\n+    when(mockGcsUtil.expand(any(GcsPath.class))).then(new Answer<List<GcsPath>>() {\n+      @Override\n+      public List<GcsPath> answer(InvocationOnMock invocation) throws Throwable {\n+        return ImmutableList.of((GcsPath) invocation.getArguments()[0]);\n+      }\n+    });\n+    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_STAGING_BUCKET))).thenReturn(true);\n+    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_TEMP_BUCKET))).thenReturn(true);\n+    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_TEMP_BUCKET + \"/staging\"))).\n+        thenReturn(true);\n+    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(VALID_PROFILE_BUCKET))).thenReturn(true);\n+    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(NON_EXISTENT_BUCKET))).thenReturn(false);\n+\n+    // The dataflow pipeline attempts to output to this location.\n+    when(mockGcsUtil.bucketAccessible(GcsPath.fromUri(\"gs://bucket/object\"))).thenReturn(true);\n+\n+    mockJobs = mock(Dataflow.Projects.Jobs.class);\n+  }\n+\n   private Pipeline buildDataflowPipeline(DataflowPipelineOptions options) {\n     options.setStableUniqueNames(CheckEnabled.ERROR);\n     options.setRunner(DataflowRunner.class);\n@@ -155,19 +198,16 @@ private Pipeline buildDataflowPipeline(DataflowPipelineOptions options) {\n     return p;\n   }\n \n-  private static Dataflow buildMockDataflow(\n-      final ArgumentCaptor<Job> jobCaptor) throws IOException {\n+  private Dataflow buildMockDataflow() throws IOException {\n     Dataflow mockDataflowClient = mock(Dataflow.class);\n     Dataflow.Projects mockProjects = mock(Dataflow.Projects.class);\n-    Dataflow.Projects.Jobs mockJobs = mock(Dataflow.Projects.Jobs.class);\n     Dataflow.Projects.Jobs.Create mockRequest =\n         mock(Dataflow.Projects.Jobs.Create.class);\n     Dataflow.Projects.Jobs.List mockList = mock(Dataflow.Projects.Jobs.List.class);\n \n     when(mockDataflowClient.projects()).thenReturn(mockProjects);\n     when(mockProjects.jobs()).thenReturn(mockJobs);\n-    when(mockJobs.create(eq(PROJECT_ID), jobCaptor.capture()))\n-        .thenReturn(mockRequest);\n+    when(mockJobs.create(eq(PROJECT_ID), isA(Job.class))).thenReturn(mockRequest);\n     when(mockJobs.list(eq(PROJECT_ID))).thenReturn(mockList);\n     when(mockList.setPageToken(anyString())).thenReturn(mockList);\n     when(mockList.execute())\n@@ -186,52 +226,36 @@ private static Dataflow buildMockDataflow(\n     return mockDataflowClient;\n   }\n \n-  /**\n-   * Build a mock {@link GcsUtil} with return values.\n-   *\n-   * @param bucketExist first return value\n-   * @param bucketAccessible next return values\n-   */\n-  private GcsUtil buildMockGcsUtil(Boolean bucketExist, Boolean... bucketAccessible)\n-      throws IOException {\n+  private GcsUtil buildMockGcsUtil() throws IOException {\n     GcsUtil mockGcsUtil = mock(GcsUtil.class);\n     when(mockGcsUtil.create(any(GcsPath.class), anyString()))\n         .then(new Answer<SeekableByteChannel>() {\n-              @Override\n-              public SeekableByteChannel answer(InvocationOnMock invocation) throws Throwable {\n-                return FileChannel.open(\n-                    Files.createTempFile(\"channel-\", \".tmp\"),\n-                    StandardOpenOption.CREATE, StandardOpenOption.DELETE_ON_CLOSE);\n-              }\n-            });\n-\n+          @Override\n+          public SeekableByteChannel answer(InvocationOnMock invocation) throws Throwable {\n+            return FileChannel.open(\n+                Files.createTempFile(\"channel-\", \".tmp\"),\n+                StandardOpenOption.CREATE, StandardOpenOption.DELETE_ON_CLOSE);\n+          }\n+        });\n     when(mockGcsUtil.isGcsPatternSupported(anyString())).thenReturn(true);\n     when(mockGcsUtil.expand(any(GcsPath.class))).then(new Answer<List<GcsPath>>() {\n       @Override\n       public List<GcsPath> answer(InvocationOnMock invocation) throws Throwable {\n         return ImmutableList.of((GcsPath) invocation.getArguments()[0]);\n       }\n     });\n-    when(mockGcsUtil.bucketAccessible(any(GcsPath.class)))\n-        .thenReturn(bucketExist, bucketAccessible);\n     return mockGcsUtil;\n   }\n \n   private DataflowPipelineOptions buildPipelineOptions() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-    return buildPipelineOptions(jobCaptor);\n-  }\n-\n-  private DataflowPipelineOptions buildPipelineOptions(\n-      ArgumentCaptor<Job> jobCaptor) throws IOException {\n     DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);\n     options.setRunner(DataflowRunner.class);\n     options.setProject(PROJECT_ID);\n-    options.setTempLocation(\"gs://somebucket/some/path\");\n+    options.setTempLocation(VALID_TEMP_BUCKET);\n     // Set FILES_PROPERTY to empty to prevent a default value calculated from classpath.\n     options.setFilesToStage(new LinkedList<String>());\n-    options.setDataflowClient(buildMockDataflow(jobCaptor));\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setDataflowClient(buildMockDataflow());\n+    options.setGcsUtil(mockGcsUtil);\n     options.setGcpCredential(new TestCredential());\n     return options;\n   }\n@@ -271,25 +295,44 @@ public void testPathValidatorOverride() {\n   @Test\n   public void testFromOptionsWithUppercaseConvertsToLowercase() throws Exception {\n     String mixedCase = \"ThisJobNameHasMixedCase\";\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n+    DataflowPipelineOptions options = buildPipelineOptions();\n     options.setJobName(mixedCase);\n \n-    DataflowRunner runner = DataflowRunner.fromOptions(options);\n+    DataflowRunner.fromOptions(options);\n     assertThat(options.getJobName(), equalTo(mixedCase.toLowerCase()));\n   }\n \n   @Test\n   public void testRun() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n+    DataflowPipelineOptions options = buildPipelineOptions();\n     Pipeline p = buildDataflowPipeline(options);\n     DataflowPipelineJob job = (DataflowPipelineJob) p.run();\n     assertEquals(\"newid\", job.getJobId());\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n   }\n \n+  /** Options for testing. */\n+  public interface RuntimeTestOptions extends PipelineOptions {\n+    ValueProvider<String> getInput();\n+    void setInput(ValueProvider<String> value);\n+\n+    ValueProvider<String> getOutput();\n+    void setOutput(ValueProvider<String> value);\n+  }\n+\n+  @Test\n+  public void testTextIOWithRuntimeParameters() throws IOException {\n+    DataflowPipelineOptions dataflowOptions = buildPipelineOptions();\n+    RuntimeTestOptions options = dataflowOptions.as(RuntimeTestOptions.class);\n+    Pipeline p = buildDataflowPipeline(dataflowOptions);\n+    p\n+        .apply(TextIO.Read.from(options.getInput()).withoutValidation())\n+        .apply(TextIO.Write.to(options.getOutput()).withoutValidation());\n+  }\n+\n   @Test\n   public void testRunReturnDifferentRequestId() throws IOException {\n     DataflowPipelineOptions options = buildPipelineOptions();\n@@ -317,14 +360,15 @@ public void testRunReturnDifferentRequestId() throws IOException {\n \n   @Test\n   public void testUpdate() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n+    DataflowPipelineOptions options = buildPipelineOptions();\n     options.setUpdate(true);\n     options.setJobName(\"oldJobName\");\n     Pipeline p = buildDataflowPipeline(options);\n     DataflowPipelineJob job = (DataflowPipelineJob) p.run();\n     assertEquals(\"newid\", job.getJobId());\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n   }\n \n@@ -378,9 +422,6 @@ protected boolean matchesSafely(DataflowJobAlreadyUpdatedException item) {\n   public void testRunWithFiles() throws IOException {\n     // Test that the function DataflowRunner.stageFiles works as\n     // expected.\n-    GcsUtil mockGcsUtil = buildMockGcsUtil(true /* bucket exists */);\n-    final String gcsStaging = \"gs://somebucket/some/path\";\n-    final String gcsTemp = \"gs://somebucket/some/temp/path\";\n     final String cloudDataflowDataset = \"somedataset\";\n \n     // Create some temporary files.\n@@ -391,17 +432,16 @@ public void testRunWithFiles() throws IOException {\n \n     String overridePackageName = \"alias.txt\";\n \n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n     DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);\n     options.setFilesToStage(ImmutableList.of(\n         temp1.getAbsolutePath(),\n         overridePackageName + \"=\" + temp2.getAbsolutePath()));\n-    options.setStagingLocation(gcsStaging);\n-    options.setTempLocation(gcsTemp);\n+    options.setStagingLocation(VALID_STAGING_BUCKET);\n+    options.setTempLocation(VALID_TEMP_BUCKET);\n     options.setTempDatasetId(cloudDataflowDataset);\n     options.setProject(PROJECT_ID);\n     options.setJobName(\"job\");\n-    options.setDataflowClient(buildMockDataflow(jobCaptor));\n+    options.setDataflowClient(buildMockDataflow());\n     options.setGcsUtil(mockGcsUtil);\n     options.setGcpCredential(new TestCredential());\n \n@@ -410,6 +450,8 @@ public void testRunWithFiles() throws IOException {\n     DataflowPipelineJob job = (DataflowPipelineJob) p.run();\n     assertEquals(\"newid\", job.getJobId());\n \n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     Job workflowJob = jobCaptor.getValue();\n     assertValidJob(workflowJob);\n \n@@ -424,7 +466,7 @@ public void testRunWithFiles() throws IOException {\n     assertEquals(overridePackageName, workflowPackage2.getName());\n \n     assertEquals(\n-        \"storage.googleapis.com/somebucket/some/temp/path\",\n+        GcsPath.fromUri(VALID_TEMP_BUCKET).toResourceName(),\n         workflowJob.getEnvironment().getTempStoragePrefix());\n     assertEquals(\n         cloudDataflowDataset,\n@@ -481,15 +523,12 @@ public void detectClassPathResourceWithNonFileResources() throws Exception {\n \n   @Test\n   public void testGcsStagingLocationInitialization() throws Exception {\n-    // Test that the staging location is initialized correctly.\n-    String gcsTemp = \"gs://somebucket/some/temp/path\";\n-\n     // Set temp location (required), and check that staging location is set.\n     DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);\n-    options.setTempLocation(gcsTemp);\n+    options.setTempLocation(VALID_TEMP_BUCKET);\n     options.setProject(PROJECT_ID);\n     options.setGcpCredential(new TestCredential());\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setGcsUtil(mockGcsUtil);\n     options.setRunner(DataflowRunner.class);\n \n     DataflowRunner.fromOptions(options);\n@@ -499,22 +538,24 @@ public void testGcsStagingLocationInitialization() throws Exception {\n \n   @Test\n   public void testNonGcsFilePathInReadFailure() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-\n-    Pipeline p = buildDataflowPipeline(buildPipelineOptions(jobCaptor));\n+    Pipeline p = buildDataflowPipeline(buildPipelineOptions());\n     p.apply(\"ReadMyNonGcsFile\", TextIO.Read.from(tmpFolder.newFile().getPath()));\n \n     thrown.expectCause(Matchers.allOf(\n         instanceOf(IllegalArgumentException.class),\n         ThrowableMessageMatcher.hasMessage(\n             containsString(\"expected a valid 'gs://' path but was given\"))));\n     p.run();\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n   }\n \n   @Test\n   public void testNonGcsFilePathInWriteFailure() throws IOException {\n     Pipeline p = buildDataflowPipeline(buildPipelineOptions());\n+\n     PCollection<String> pc = p.apply(\"ReadMyGcsFile\", TextIO.Read.from(\"gs://bucket/object\"));\n \n     thrown.expect(IllegalArgumentException.class);\n@@ -524,15 +565,16 @@ public void testNonGcsFilePathInWriteFailure() throws IOException {\n \n   @Test\n   public void testMultiSlashGcsFileReadPath() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-\n-    Pipeline p = buildDataflowPipeline(buildPipelineOptions(jobCaptor));\n+    Pipeline p = buildDataflowPipeline(buildPipelineOptions());\n     p.apply(\"ReadInvalidGcsFile\", TextIO.Read.from(\"gs://bucket/tmp//file\"));\n \n     thrown.expectCause(Matchers.allOf(\n         instanceOf(IllegalArgumentException.class),\n         ThrowableMessageMatcher.hasMessage(containsString(\"consecutive slashes\"))));\n     p.run();\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n   }\n \n@@ -548,22 +590,21 @@ public void testMultiSlashGcsFileWritePath() throws IOException {\n \n   @Test\n   public void testInvalidGcpTempLocation() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n+    DataflowPipelineOptions options = buildPipelineOptions();\n     options.setGcpTempLocation(\"file://temp/location\");\n \n     thrown.expect(IllegalArgumentException.class);\n     thrown.expectMessage(containsString(\"expected a valid 'gs://' path but was given\"));\n     DataflowRunner.fromOptions(options);\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n   }\n \n   @Test\n   public void testNonGcsTempLocation() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n+    DataflowPipelineOptions options = buildPipelineOptions();\n     options.setTempLocation(\"file://temp/location\");\n \n     thrown.expect(IllegalArgumentException.class);\n@@ -592,39 +633,68 @@ public void testInvalidStagingLocation() throws IOException {\n   }\n \n   @Test\n-  public void testNonExistentTempLocation() throws IOException {\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+  public void testInvalidProfileLocation() throws IOException {\n+    DataflowPipelineOptions options = buildPipelineOptions();\n+    options.setSaveProfilesToGcs(\"file://my/staging/location\");\n+    try {\n+      DataflowRunner.fromOptions(options);\n+      fail(\"fromOptions should have failed\");\n+    } catch (IllegalArgumentException e) {\n+      assertThat(e.getMessage(), containsString(\"expected a valid 'gs://' path but was given\"));\n+    }\n+    options.setSaveProfilesToGcs(\"my/staging/location\");\n+    try {\n+      DataflowRunner.fromOptions(options);\n+      fail(\"fromOptions should have failed\");\n+    } catch (IllegalArgumentException e) {\n+      assertThat(e.getMessage(), containsString(\"expected a valid 'gs://' path but was given\"));\n+    }\n+  }\n \n-    GcsUtil mockGcsUtil =\n-        buildMockGcsUtil(false /* temp bucket exists */, true /* staging bucket exists */);\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n-    options.setGcsUtil(mockGcsUtil);\n-    options.setGcpTempLocation(\"gs://non-existent-bucket/location\");\n+  @Test\n+  public void testNonExistentTempLocation() throws IOException {\n+    DataflowPipelineOptions options = buildPipelineOptions();\n+    options.setGcpTempLocation(NON_EXISTENT_BUCKET);\n \n     thrown.expect(IllegalArgumentException.class);\n     thrown.expectMessage(containsString(\n-        \"Output path does not exist or is not writeable: gs://non-existent-bucket/location\"));\n+        \"Output path does not exist or is not writeable: \" + NON_EXISTENT_BUCKET));\n     DataflowRunner.fromOptions(options);\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n   }\n \n   @Test\n   public void testNonExistentStagingLocation() throws IOException {\n+    DataflowPipelineOptions options = buildPipelineOptions();\n+    options.setStagingLocation(NON_EXISTENT_BUCKET);\n+\n+    thrown.expect(IllegalArgumentException.class);\n+    thrown.expectMessage(containsString(\n+        \"Output path does not exist or is not writeable: \" + NON_EXISTENT_BUCKET));\n+    DataflowRunner.fromOptions(options);\n+\n     ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n+    assertValidJob(jobCaptor.getValue());\n+  }\n \n-    GcsUtil mockGcsUtil =\n-        buildMockGcsUtil(true /* temp bucket exists */, false /* staging bucket exists */);\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n-    options.setGcpTempLocation(options.getTempLocation()); // bypass validation for GcpTempLocation\n-    options.setGcsUtil(mockGcsUtil);\n-    options.setStagingLocation(\"gs://non-existent-bucket/location\");\n+  @Test\n+  public void testNonExistentProfileLocation() throws IOException {\n+    DataflowPipelineOptions options = buildPipelineOptions();\n+    options.setSaveProfilesToGcs(NON_EXISTENT_BUCKET);\n \n     thrown.expect(IllegalArgumentException.class);\n     thrown.expectMessage(containsString(\n-        \"Output path does not exist or is not writeable: gs://non-existent-bucket/location\"));\n+        \"Output path does not exist or is not writeable: \" + NON_EXISTENT_BUCKET));\n     DataflowRunner.fromOptions(options);\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n-  }\n+   }\n \n   @Test\n   public void testNoProjectFails() {\n@@ -648,8 +718,8 @@ public void testProjectId() throws IOException {\n     options.setRunner(DataflowRunner.class);\n     options.setProject(\"foo-12345\");\n \n-    options.setGcpTempLocation(\"gs://spam/ham/eggs\");\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setGcpTempLocation(VALID_TEMP_BUCKET);\n+    options.setGcsUtil(mockGcsUtil);\n     options.setGcpCredential(new TestCredential());\n \n     DataflowRunner.fromOptions(options);\n@@ -661,8 +731,8 @@ public void testProjectPrefix() throws IOException {\n     options.setRunner(DataflowRunner.class);\n     options.setProject(\"google.com:some-project-12345\");\n \n-    options.setGcpTempLocation(\"gs://spam/ham/eggs\");\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setGcpTempLocation(VALID_TEMP_BUCKET);\n+    options.setGcsUtil(mockGcsUtil);\n     options.setGcpCredential(new TestCredential());\n \n     DataflowRunner.fromOptions(options);\n@@ -674,8 +744,8 @@ public void testProjectNumber() throws IOException {\n     options.setRunner(DataflowRunner.class);\n     options.setProject(\"12345\");\n \n-    options.setGcpTempLocation(\"gs://spam/ham/eggs\");\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setGcpTempLocation(VALID_TEMP_BUCKET);\n+    options.setGcsUtil(mockGcsUtil);\n \n     thrown.expect(IllegalArgumentException.class);\n     thrown.expectMessage(\"Project ID\");\n@@ -690,8 +760,8 @@ public void testProjectDescription() throws IOException {\n     options.setRunner(DataflowRunner.class);\n     options.setProject(\"some project\");\n \n-    options.setGcpTempLocation(\"gs://spam/ham/eggs\");\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setGcpTempLocation(VALID_TEMP_BUCKET);\n+    options.setGcsUtil(mockGcsUtil);\n \n     thrown.expect(IllegalArgumentException.class);\n     thrown.expectMessage(\"Project ID\");\n@@ -706,8 +776,8 @@ public void testInvalidNumberOfWorkerHarnessThreads() throws IOException {\n     options.setRunner(DataflowRunner.class);\n     options.setProject(\"foo-12345\");\n \n-    options.setTempLocation(\"gs://spam/ham/eggs\");\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setGcpTempLocation(VALID_TEMP_BUCKET);\n+    options.setGcsUtil(mockGcsUtil);\n \n     options.as(DataflowPipelineDebugOptions.class).setNumberOfWorkerHarnessThreads(-1);\n \n@@ -731,25 +801,34 @@ public void testNoStagingLocationAndNoTempLocationFails() {\n   }\n \n   @Test\n-  public void testStagingLocationAndNoTempLocationSucceeds() throws Exception {\n+  public void testGcpTempAndNoTempLocationSucceeds() throws Exception {\n     DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);\n     options.setRunner(DataflowRunner.class);\n     options.setGcpCredential(new TestCredential());\n     options.setProject(\"foo-project\");\n-    options.setGcpTempLocation(\"gs://spam/ham/eggs\");\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setGcpTempLocation(VALID_TEMP_BUCKET);\n+    options.setGcsUtil(mockGcsUtil);\n \n     DataflowRunner.fromOptions(options);\n   }\n \n   @Test\n-  public void testTempLocationAndNoStagingLocationSucceeds() throws Exception {\n+  public void testTempLocationAndNoGcpTempLocationSucceeds() throws Exception {\n     DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);\n     options.setRunner(DataflowRunner.class);\n     options.setGcpCredential(new TestCredential());\n     options.setProject(\"foo-project\");\n-    options.setTempLocation(\"gs://spam/ham/eggs\");\n-    options.setGcsUtil(buildMockGcsUtil(true /* bucket exists */));\n+    options.setTempLocation(VALID_TEMP_BUCKET);\n+    options.setGcsUtil(mockGcsUtil);\n+\n+    DataflowRunner.fromOptions(options);\n+  }\n+\n+\n+  @Test\n+  public void testValidProfileLocation() throws IOException {\n+    DataflowPipelineOptions options = buildPipelineOptions();\n+    options.setSaveProfilesToGcs(VALID_PROFILE_BUCKET);\n \n     DataflowRunner.fromOptions(options);\n   }\n@@ -840,7 +919,7 @@ public void testGcsUploadBufferSizeUnchangedWhenNotDefault() throws IOException\n     public boolean translated = false;\n \n     @Override\n-    public PCollection<Integer> apply(PCollection<Integer> input) {\n+    public PCollection<Integer> expand(PCollection<Integer> input) {\n       return PCollection.<Integer>createPrimitiveOutputInternal(\n           input.getPipeline(),\n           WindowingStrategy.globalDefault(),\n@@ -855,10 +934,7 @@ public void testGcsUploadBufferSizeUnchangedWhenNotDefault() throws IOException\n \n   @Test\n   public void testTransformTranslatorMissing() throws IOException {\n-    // Test that we throw if we don't provide a translation.\n-    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n-\n-    DataflowPipelineOptions options = buildPipelineOptions(jobCaptor);\n+    DataflowPipelineOptions options = buildPipelineOptions();\n     Pipeline p = Pipeline.create(options);\n \n     p.apply(Create.of(Arrays.asList(1, 2, 3)))\n@@ -869,6 +945,9 @@ public void testTransformTranslatorMissing() throws IOException {\n     DataflowPipelineTranslator.fromOptions(options)\n         .translate(\n             p, (DataflowRunner) p.getRunner(), Collections.<DataflowPackage>emptyList());\n+\n+    ArgumentCaptor<Job> jobCaptor = ArgumentCaptor.forClass(Job.class);\n+    Mockito.verify(mockJobs).create(eq(PROJECT_ID), jobCaptor.capture());\n     assertValidJob(jobCaptor.getValue());\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/DataflowRunnerTest.java",
                "sha": "4159b611e0b46f42fe6b0c339cf4598a236d2380",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptionsTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptionsTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptionsTest.java",
                "patch": "@@ -17,8 +17,8 @@\n  */\n package org.apache.beam.runners.dataflow.options;\n \n+import static org.hamcrest.Matchers.equalTo;\n import static org.junit.Assert.assertThat;\n-import static org.junit.Assert.assertTrue;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n@@ -38,9 +38,9 @@\n   @Test\n   public void testOptionsObject() throws Exception {\n     DataflowPipelineOptions options = PipelineOptionsFactory.fromArgs(new String[] {\n-        \"--enableProfilingAgent\", \"--profilingAgentConfiguration={\\\"interval\\\": 21}\"})\n+        \"--saveProfilesToGcs=path\", \"--profilingAgentConfiguration={\\\"interval\\\": 21}\"})\n         .as(DataflowPipelineOptions.class);\n-    assertTrue(options.getEnableProfilingAgent());\n+    assertThat(options.getSaveProfilesToGcs(), equalTo(\"path\"));\n \n     String json = MAPPER.writeValueAsString(options);\n     assertThat(json, Matchers.containsString(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/options/DataflowProfilingOptionsTest.java",
                "sha": "299f3c85652d4885f979ac7d9df8147dec0338e8",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunnerTest.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunnerTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 25,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunnerTest.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.beam.runners.dataflow.testing;\n \n+import static org.apache.beam.runners.dataflow.testing.TestDataflowRunner.LEGACY_WATERMARK_METRIC_SUFFIX;\n+import static org.apache.beam.runners.dataflow.testing.TestDataflowRunner.WATERMARK_METRIC_SUFFIX;\n import static org.hamcrest.Matchers.containsString;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n@@ -95,7 +97,6 @@\n   @Mock private MockLowLevelHttpRequest request;\n   @Mock private GcsUtil mockGcsUtil;\n \n-  private static final String WATERMARK_METRIC_SUFFIX = \"windmill-data-watermark\";\n   private static final BigDecimal DEFAULT_MAX_WATERMARK = new BigDecimal(-2);\n \n   private TestDataflowPipelineOptions options;\n@@ -344,8 +345,7 @@ private JobMetrics buildJobMetrics(List<MetricUpdate> metricList) {\n \n   @Test\n   public void testCheckingForSuccessWhenPAssertSucceeds() throws Exception {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));\n     PAssert.that(pc).containsInAnyOrder(1, 2, 3);\n@@ -359,8 +359,7 @@ public void testCheckingForSuccessWhenPAssertSucceeds() throws Exception {\n \n   @Test\n   public void testCheckingForSuccessWhenPAssertFails() throws Exception {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));\n     PAssert.that(pc).containsInAnyOrder(1, 2, 3);\n@@ -374,8 +373,7 @@ public void testCheckingForSuccessWhenPAssertFails() throws Exception {\n \n   @Test\n   public void testCheckingForSuccessSkipsNonTentativeMetrics() throws Exception {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));\n     PAssert.that(pc).containsInAnyOrder(1, 2, 3);\n@@ -389,8 +387,7 @@ public void testCheckingForSuccessSkipsNonTentativeMetrics() throws Exception {\n \n   @Test\n   public void testCheckMaxWatermarkWithNoWatermarkMetric() throws IOException {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n \n@@ -403,8 +400,7 @@ public void testCheckMaxWatermarkWithNoWatermarkMetric() throws IOException {\n \n   @Test\n   public void testCheckMaxWatermarkWithSingleWatermarkAtMax() throws IOException {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n \n@@ -415,10 +411,22 @@ public void testCheckMaxWatermarkWithSingleWatermarkAtMax() throws IOException {\n     assertTrue(runner.atMaxWatermark(job, metrics));\n   }\n \n+  @Test\n+  public void testCheckMaxWatermarkWithLegacyWatermarkAtMax() throws IOException {\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n+    Pipeline p = TestPipeline.create(options);\n+    p.apply(Create.of(1, 2, 3));\n+\n+    TestDataflowRunner runner = (TestDataflowRunner) p.getRunner();\n+    JobMetrics metrics = buildJobMetrics(generateMockStreamingMetrics(\n+        ImmutableMap.of(LEGACY_WATERMARK_METRIC_SUFFIX, DEFAULT_MAX_WATERMARK)));\n+    doReturn(State.RUNNING).when(job).getState();\n+    assertTrue(runner.atMaxWatermark(job, metrics));\n+  }\n+\n   @Test\n   public void testCheckMaxWatermarkWithSingleWatermarkNotAtMax() throws IOException {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n \n@@ -431,8 +439,7 @@ public void testCheckMaxWatermarkWithSingleWatermarkNotAtMax() throws IOExceptio\n \n   @Test\n   public void testCheckMaxWatermarkWithMultipleWatermarksAtMax() throws IOException {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n \n@@ -446,8 +453,7 @@ public void testCheckMaxWatermarkWithMultipleWatermarksAtMax() throws IOExceptio\n \n   @Test\n   public void testCheckMaxWatermarkWithMultipleMaxAndNotMaxWatermarks() throws IOException {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n \n@@ -461,8 +467,7 @@ public void testCheckMaxWatermarkWithMultipleMaxAndNotMaxWatermarks() throws IOE\n \n   @Test\n   public void testCheckMaxWatermarkIgnoresUnrelatedMatrics() throws IOException {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n \n@@ -476,8 +481,7 @@ public void testCheckMaxWatermarkIgnoresUnrelatedMatrics() throws IOException {\n \n   @Test\n   public void testStreamingPipelineFailsIfServiceFails() throws Exception {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     PCollection<Integer> pc = p.apply(Create.of(1, 2, 3));\n     PAssert.that(pc).containsInAnyOrder(1, 2, 3);\n@@ -532,8 +536,7 @@ public State answer(InvocationOnMock invocation) {\n \n   @Test\n   public void testGetJobMetricsThatSucceeds() throws Exception {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n \n@@ -549,8 +552,7 @@ public void testGetJobMetricsThatSucceeds() throws Exception {\n \n   @Test\n   public void testGetJobMetricsThatFailsForException() throws Exception {\n-    DataflowPipelineJob job =\n-        spy(new DataflowPipelineJob(\"test-project\", \"test-job\", options, null));\n+    DataflowPipelineJob job = spy(new DataflowPipelineJob(\"test-job\", options, null));\n     Pipeline p = TestPipeline.create(options);\n     p.apply(Create.of(1, 2, 3));\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/testing/TestDataflowRunnerTest.java",
                "sha": "da5630b487b2b7542e31d7b3e03b0d480935c223",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowGroupByKeyTest.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowGroupByKeyTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowGroupByKeyTest.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.dataflow.transforms;\n \n+import com.google.api.services.dataflow.Dataflow;\n import java.util.Arrays;\n import java.util.List;\n import org.apache.beam.runners.dataflow.DataflowRunner;\n@@ -38,18 +39,29 @@\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.TypeDescriptor;\n import org.joda.time.Duration;\n+import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.rules.ExpectedException;\n import org.junit.runner.RunWith;\n import org.junit.runners.JUnit4;\n+import org.mockito.Mock;\n+import org.mockito.MockitoAnnotations;\n \n /** Tests for {@link GroupByKey} for the {@link DataflowRunner}. */\n @RunWith(JUnit4.class)\n public class DataflowGroupByKeyTest {\n   @Rule\n   public ExpectedException thrown = ExpectedException.none();\n \n+  @Mock\n+  private Dataflow dataflow;\n+\n+  @Before\n+  public void setUp() {\n+    MockitoAnnotations.initMocks(this);\n+  }\n+\n   /**\n    * Create a test pipeline that uses the {@link DataflowRunner} so that {@link GroupByKey}\n    * is not expanded. This is used for verifying that even without expansion the proper errors show\n@@ -61,7 +73,7 @@ private Pipeline createTestServiceRunner() {\n     options.setProject(\"someproject\");\n     options.setGcpTempLocation(\"gs://staging\");\n     options.setPathValidatorClass(NoopPathValidator.class);\n-    options.setDataflowClient(null);\n+    options.setDataflowClient(dataflow);\n     return Pipeline.create(options);\n   }\n \n@@ -92,12 +104,12 @@ public void testGroupByKeyServiceUnbounded() {\n         p.apply(\n             new PTransform<PBegin, PCollection<KV<String, Integer>>>() {\n               @Override\n-              public PCollection<KV<String, Integer>> apply(PBegin input) {\n+              public PCollection<KV<String, Integer>> expand(PBegin input) {\n                 return PCollection.<KV<String, Integer>>createPrimitiveOutputInternal(\n                         input.getPipeline(),\n                         WindowingStrategy.globalDefault(),\n                         PCollection.IsBounded.UNBOUNDED)\n-                    .setTypeDescriptorInternal(new TypeDescriptor<KV<String, Integer>>() {});\n+                    .setTypeDescriptor(new TypeDescriptor<KV<String, Integer>>() {});\n               }\n             });\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowGroupByKeyTest.java",
                "sha": "c9c7806084b964b45a01cadfa0af860c110f3983",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowViewTest.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowViewTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowViewTest.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.dataflow.transforms;\n \n+import com.google.api.services.dataflow.Dataflow;\n import org.apache.beam.runners.dataflow.DataflowRunner;\n import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n import org.apache.beam.sdk.Pipeline;\n@@ -36,26 +37,37 @@\n import org.apache.beam.sdk.values.TypeDescriptor;\n import org.hamcrest.Matchers;\n import org.joda.time.Duration;\n+import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n import org.junit.internal.matchers.ThrowableMessageMatcher;\n import org.junit.rules.ExpectedException;\n import org.junit.runner.RunWith;\n import org.junit.runners.JUnit4;\n+import org.mockito.Mock;\n+import org.mockito.MockitoAnnotations;\n \n /** Tests for {@link View} for a {@link DataflowRunner}. */\n @RunWith(JUnit4.class)\n public class DataflowViewTest {\n   @Rule\n   public transient ExpectedException thrown = ExpectedException.none();\n \n+  @Mock\n+  private Dataflow dataflow;\n+\n+  @Before\n+  public void setUp() {\n+    MockitoAnnotations.initMocks(this);\n+  }\n+\n   private Pipeline createTestBatchRunner() {\n     DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);\n     options.setRunner(DataflowRunner.class);\n     options.setProject(\"someproject\");\n     options.setGcpTempLocation(\"gs://staging\");\n     options.setPathValidatorClass(NoopPathValidator.class);\n-    options.setDataflowClient(null);\n+    options.setDataflowClient(dataflow);\n     return Pipeline.create(options);\n   }\n \n@@ -66,7 +78,7 @@ private Pipeline createTestStreamingRunner() {\n     options.setProject(\"someproject\");\n     options.setGcpTempLocation(\"gs://staging\");\n     options.setPathValidatorClass(NoopPathValidator.class);\n-    options.setDataflowClient(null);\n+    options.setDataflowClient(dataflow);\n     return Pipeline.create(options);\n   }\n \n@@ -81,12 +93,12 @@ private void testViewUnbounded(\n         .apply(\n             new PTransform<PBegin, PCollection<KV<String, Integer>>>() {\n               @Override\n-              public PCollection<KV<String, Integer>> apply(PBegin input) {\n+              public PCollection<KV<String, Integer>> expand(PBegin input) {\n                 return PCollection.<KV<String, Integer>>createPrimitiveOutputInternal(\n                         input.getPipeline(),\n                         WindowingStrategy.globalDefault(),\n                         PCollection.IsBounded.UNBOUNDED)\n-                    .setTypeDescriptorInternal(new TypeDescriptor<KV<String, Integer>>() {});\n+                    .setTypeDescriptor(new TypeDescriptor<KV<String, Integer>>() {});\n               }\n             })\n         .apply(view);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/transforms/DataflowViewTest.java",
                "sha": "455868367ff7ae0d24d375ef8028aac7ae35a1df",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/util/MonitoringUtilTest.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/util/MonitoringUtilTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 16,
                "filename": "runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/util/MonitoringUtilTest.java",
                "patch": "@@ -19,16 +19,15 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n-import com.google.api.services.dataflow.Dataflow;\n import com.google.api.services.dataflow.model.JobMessage;\n import com.google.api.services.dataflow.model.ListJobMessagesResponse;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n+import org.apache.beam.runners.dataflow.DataflowClient;\n import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n import org.apache.beam.runners.dataflow.util.MonitoringUtil.LoggingHandler;\n import org.apache.beam.sdk.PipelineResult.State;\n@@ -57,15 +56,7 @@\n \n   @Test\n   public void testGetJobMessages() throws IOException {\n-    Dataflow.Projects.Jobs.Messages mockMessages = mock(Dataflow.Projects.Jobs.Messages.class);\n-\n-    // Two requests are needed to get all the messages.\n-    Dataflow.Projects.Jobs.Messages.List firstRequest =\n-        mock(Dataflow.Projects.Jobs.Messages.List.class);\n-    Dataflow.Projects.Jobs.Messages.List secondRequest =\n-        mock(Dataflow.Projects.Jobs.Messages.List.class);\n-\n-    when(mockMessages.list(PROJECT_ID, JOB_ID)).thenReturn(firstRequest).thenReturn(secondRequest);\n+    DataflowClient dataflowClient = mock(DataflowClient.class);\n \n     ListJobMessagesResponse firstResponse = new ListJobMessagesResponse();\n     firstResponse.setJobMessages(new ArrayList<JobMessage>());\n@@ -87,15 +78,13 @@ public void testGetJobMessages() throws IOException {\n       secondResponse.getJobMessages().add(message);\n     }\n \n-    when(firstRequest.execute()).thenReturn(firstResponse);\n-    when(secondRequest.execute()).thenReturn(secondResponse);\n+    when(dataflowClient.listJobMessages(JOB_ID, null)).thenReturn(firstResponse);\n+    when(dataflowClient.listJobMessages(JOB_ID, pageToken)).thenReturn(secondResponse);\n \n-    MonitoringUtil util = new MonitoringUtil(PROJECT_ID, mockMessages);\n+    MonitoringUtil util = new MonitoringUtil(dataflowClient);\n \n     List<JobMessage> messages = util.getJobMessages(JOB_ID, -1);\n \n-    verify(secondRequest).setPageToken(pageToken);\n-\n     assertEquals(150, messages.size());\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/google-cloud-dataflow-java/src/test/java/org/apache/beam/runners/dataflow/util/MonitoringUtilTest.java",
                "sha": "23ed26f2d7cba7535cf0ef455e4d6621672996bb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/pom.xml",
                "sha": "0efd41f8b7951fbdf92a706178719c3da1602b15",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/README.md",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/README.md?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 40,
                "filename": "runners/spark/README.md",
                "patch": "@@ -38,32 +38,25 @@ with Apache Spark. This runner allows to execute both batch and streaming pipeli\n - Side inputs/outputs\n - Encoding\n \n-### Sources and Sinks\n-\n-- Text\n-- Hadoop\n-- Avro\n-- Kafka\n-\n ### Fault-Tolerance\n \n The Spark runner fault-tolerance guarantees the same guarantees as [Apache Spark](http://spark.apache.org/).\n \n ### Monitoring\n \n-The Spark runner supports monitoring via Beam Aggregators implemented on top of Spark's [Accumulators](http://spark.apache.org/docs/latest/programming-guide.html#accumulators).  \n-Spark also provides a web UI for monitoring, more details [here](http://spark.apache.org/docs/latest/monitoring.html).\n+The Spark runner supports user-defined counters via Beam Aggregators implemented on top of Spark's [Accumulators](http://spark.apache.org/docs/1.6.3/programming-guide.html#accumulators).  \n+The Aggregators (defined by the pipeline author) and Spark's internal metrics are reported using Spark's [metrics system](http://spark.apache.org/docs/1.6.3/monitoring.html#metrics).  \n+Spark also provides a web UI for monitoring, more details [here](http://spark.apache.org/docs/1.6.3/monitoring.html).\n \n ## Beam Model support\n \n ### Batch\n \n-The Spark runner provides support for batch processing of Beam bounded PCollections as Spark [RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)s.\n+The Spark runner provides full support for the Beam Model in batch processing via Spark [RDD](http://spark.apache.org/docs/1.6.3/programming-guide.html#resilient-distributed-datasets-rdds)s.\n \n ### Streaming\n \n-The Spark runner currently provides partial support for stream processing of Beam unbounded PCollections as Spark [DStream](http://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams)s.  \n-Currently, both *FixedWindows* and *SlidingWindows* are supported, but only with processing-time triggers and discarding pane.  \n+Providing full support for the Beam Model in streaming pipelines is under development. To follow-up you can subscribe to our [mailing list](http://beam.incubator.apache.org/get-started/support/).\n \n ### issue tracking\n \n@@ -84,19 +77,21 @@ Then switch to the newly created directory and run Maven to build the Apache Bea\n \n Now Apache Beam and the Spark Runner are installed in your local maven repository.\n \n-If we wanted to run a Beam pipeline with the default options of a single threaded Spark\n-instance in local mode, we would do the following:\n+If we wanted to run a Beam pipeline with the default options of a Spark instance in local mode, \n+we would do the following:\n \n     Pipeline p = <logic for pipeline creation >\n-    EvaluationResult result = SparkRunner.create().run(p);\n+    PipelineResult result = p.run();\n+    result.waitUntilFinish();\n \n To create a pipeline runner to run against a different Spark cluster, with a custom master url we\n would do the following:\n \n-    Pipeline p = <logic for pipeline creation >\n-    SparkPipelineOptions options = SparkPipelineOptionsFactory.create();\n+    SparkPipelineOptions options = PipelineOptionsFactory.as(SparkPipelineOptions.class);\n     options.setSparkMaster(\"spark://host:port\");\n-    EvaluationResult result = SparkRunner.create(options).run(p);\n+    Pipeline p = <logic for pipeline creation >\n+    PipelineResult result = p.run();\n+    result.waitUntilFinish();\n \n ## Word Count Example\n \n@@ -108,12 +103,11 @@ Switch to the Spark runner directory:\n \n     cd runners/spark\n     \n-Then run the [word count example][wc] from the SDK using a single threaded Spark instance\n-in local mode:\n+Then run the [word count example][wc] from the SDK using a Spark instance in local mode:\n \n-    mvn exec:exec -DmainClass=org.apache.beam.examples.WordCount \\\n-      -Dinput=/tmp/kinglear.txt -Doutput=/tmp/out -Drunner=SparkRunner \\\n-      -DsparkMaster=local\n+    mvn exec:exec -DmainClass=org.apache.beam.runners.spark.examples.WordCount \\\n+          -Dinput=/tmp/kinglear.txt -Doutput=/tmp/out -Drunner=SparkRunner \\\n+          -DsparkMaster=local\n \n Check the output by running:\n \n@@ -122,24 +116,9 @@ Check the output by running:\n __Note: running examples using `mvn exec:exec` only works for Spark local mode at the\n moment. See the next section for how to run on a cluster.__\n \n-[wc]: https://github.com/apache/incubator-beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WordCount.java\n+[wc]: https://github.com/apache/incubator-beam/blob/master/runners/spark/src/main/java/org/apache/beam/runners/spark/examples/WordCount.java\n ## Running on a Cluster\n \n Spark Beam pipelines can be run on a cluster using the `spark-submit` command.\n \n-First copy a text document to HDFS:\n-\n-    curl http://www.gutenberg.org/cache/epub/1128/pg1128.txt | hadoop fs -put - kinglear.txt\n-\n-Then run the word count example using Spark submit with the `yarn-client` master\n-(`yarn-cluster` works just as well):\n-\n-    spark-submit \\\n-      --class org.apache.beam.examples.WordCount \\\n-      --master yarn-client \\\n-      target/spark-runner-*-spark-app.jar \\\n-        --inputFile=kinglear.txt --output=out --runner=SparkRunner --sparkMaster=yarn-client\n-\n-Check the output by running:\n-\n-    hadoop fs -tail out-00000-of-00002\n+TBD pending native HDFS support (currently blocked by [BEAM-59](https://issues.apache.org/jira/browse/BEAM-59)).",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/README.md",
                "sha": "aad65b3a914a9430362af7cadf8e79c28b99d3b5",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/pom.xml",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 33,
                "filename": "runners/spark/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-runners-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n \n@@ -54,7 +54,7 @@\n     </profile>\n \n     <profile>\n-      <!-- This profile adds execution of RunnableOnService integration tests \n+      <!-- This profile adds execution of RunnableOnService integration tests\n            against a local Spark endpoint. -->\n       <id>local-runnable-on-service-tests</id>\n       <activation><activeByDefault>false</activeByDefault></activation>\n@@ -74,7 +74,9 @@\n                   <groups>org.apache.beam.sdk.testing.RunnableOnService</groups>\n                   <excludedGroups>\n                     org.apache.beam.sdk.testing.UsesStatefulParDo,\n-                    org.apache.beam.sdk.testing.UsesSplittableParDo\n+                    org.apache.beam.sdk.testing.UsesTimersInParDo,\n+                    org.apache.beam.sdk.testing.UsesSplittableParDo,\n+                    org.apache.beam.sdk.testing.UsesMetrics\n                   </excludedGroups>\n                   <forkCount>1</forkCount>\n                   <reuseForks>false</reuseForks>\n@@ -133,33 +135,18 @@\n       <version>${hadoop.version}</version>\n       <scope>provided</scope>\n     </dependency>\n+    <!-- Kryo bugfix version needed due to a state re-use issue in Kryo version 2.21 used in Spark 1.x\n+    See: https://issues.apache.org/jira/browse/SPARK-7708\n+    See: https://github.com/EsotericSoftware/kryo/issues/312\n+    -->\n     <dependency>\n       <groupId>com.esotericsoftware.kryo</groupId>\n       <artifactId>kryo</artifactId>\n-      <version>2.21</version>\n-      <scope>provided</scope>\n-    </dependency>\n-    <dependency>\n-      <groupId>de.javakaffee</groupId>\n-      <artifactId>kryo-serializers</artifactId>\n-      <version>0.39</version>\n-      <exclusions>\n-        <!-- Use Spark's Kryo -->\n-        <exclusion>\n-          <groupId>com.esotericsoftware</groupId>\n-          <artifactId>kryo</artifactId>\n-        </exclusion>\n-        <!-- We only really need the serializer implementations -->\n-        <exclusion>\n-          <groupId>com.google.protobuf</groupId>\n-          <artifactId>protobuf-java</artifactId>\n-        </exclusion>\n-      </exclusions>\n+      <version>2.21.1</version>\n     </dependency>\n     <dependency>\n       <groupId>com.google.code.findbugs</groupId>\n       <artifactId>jsr305</artifactId>\n-      <version>1.3.9</version>\n     </dependency>\n     <dependency>\n       <groupId>com.google.guava</groupId>\n@@ -263,23 +250,30 @@\n       <artifactId>metrics-core</artifactId>\n       <version>${dropwizard.metrics.version}</version>\n     </dependency>\n+    <dependency>\n+      <groupId>org.reflections</groupId>\n+      <artifactId>reflections</artifactId>\n+      <version>0.9.10</version>\n+    </dependency>\n \n     <!-- KafkaIO -->\n     <dependency>\n       <groupId>org.apache.beam</groupId>\n       <artifactId>beam-sdks-java-io-kafka</artifactId>\n+      <scope>test</scope>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.kafka</groupId>\n       <artifactId>kafka-clients</artifactId>\n       <version>0.9.0.1</version>\n+      <scope>test</scope>\n     </dependency>\n \n     <!-- test dependencies -->\n     <dependency>\n       <groupId>junit</groupId>\n       <artifactId>junit</artifactId>\n-      <scope>provided</scope>\n+      <scope>test</scope>\n       <exclusions>\n         <exclusion>\n           <artifactId>hamcrest-core</artifactId>\n@@ -323,15 +317,6 @@\n   <build>\n     <pluginManagement>\n       <plugins>\n-        <!-- BEAM-931 -->\n-        <plugin>\n-          <groupId>org.codehaus.mojo</groupId>\n-          <artifactId>findbugs-maven-plugin</artifactId>\n-          <configuration>\n-            <skip>true</skip>\n-          </configuration>\n-        </plugin>\n-\n         <plugin>\n           <groupId>org.apache.maven.plugins</groupId>\n           <artifactId>maven-surefire-plugin</artifactId>\n@@ -402,6 +387,10 @@\n                     <pattern>com.google.thirdparty</pattern>\n                     <shadedPattern>org.apache.beam.spark.relocated.com.google.thirdparty</shadedPattern>\n                   </relocation>\n+                  <relocation>\n+                    <pattern>com.esotericsoftware.kryo</pattern>\n+                    <shadedPattern>org.apache.beam.spark.relocated.com.esotericsoftware.kryo</shadedPattern>\n+                  </relocation>\n                 </relocations>\n                 <shadedArtifactAttached>true</shadedArtifactAttached>\n                 <shadedClassifierName>spark-app</shadedClassifierName>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/pom.xml",
                "sha": "309e1ffbef3fbec51316c035541df353470d98bf",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/b6e7bb659f33e346c00e66ca96e3c54dd7ef07da/runners/spark/src/main/java/org/apache/beam/runners/spark/EvaluationResult.java",
                "changes": 67,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/EvaluationResult.java?ref=b6e7bb659f33e346c00e66ca96e3c54dd7ef07da",
                "deletions": 67,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/EvaluationResult.java",
                "patch": "@@ -1,67 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.beam.runners.spark;\n-\n-import org.apache.beam.sdk.PipelineResult;\n-import org.apache.beam.sdk.values.PCollection;\n-import org.apache.beam.sdk.values.PValue;\n-\n-/**\n- * Interface for retrieving the result(s) of running a pipeline. Allows us to translate between\n- * {@code PObject<T>}s or {@code PCollection<T>}s and Ts or collections of Ts.\n- */\n-public interface EvaluationResult extends PipelineResult {\n-  /**\n-   * Retrieves an iterable of results associated with the PCollection passed in.\n-   *\n-   * @param pcollection Collection we wish to translate.\n-   * @param <T>         Type of elements contained in collection.\n-   * @return Natively types result associated with collection.\n-   */\n-  <T> Iterable<T> get(PCollection<T> pcollection);\n-\n-  /**\n-   * Retrieve an object of Type T associated with the PValue passed in.\n-   *\n-   * @param pval PValue to retrieve associated data for.\n-   * @param <T>  Type of object to return.\n-   * @return Native object.\n-   */\n-  <T> T get(PValue pval);\n-\n-  /**\n-   * Retrieves the final value of the aggregator.\n-   *\n-   * @param aggName    name of aggregator.\n-   * @param resultType Class of final result of aggregation.\n-   * @param <T>        Type of final result of aggregation.\n-   * @return Result of aggregation associated with specified name.\n-   */\n-  <T> T getAggregatorValue(String aggName, Class<T> resultType);\n-\n-  /**\n-   * Releases any runtime resources, including distributed-execution contexts currently held by\n-   * this EvaluationResult; once close() has been called,\n-   * {@link EvaluationResult#get(PCollection)} might\n-   * not work for subsequent calls.\n-   *\n-   * @param gracefully true if Spark should finish all ongoing work before closing.\n-   */\n-  void close(boolean gracefully);\n-}",
                "raw_url": "https://github.com/apache/beam/raw/b6e7bb659f33e346c00e66ca96e3c54dd7ef07da/runners/spark/src/main/java/org/apache/beam/runners/spark/EvaluationResult.java",
                "sha": "52606a35927c384c562c6e32406d0d62b5232381",
                "status": "removed"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java",
                "patch": "@@ -54,6 +54,11 @@\n   Long getMinReadTimeMillis();\n   void setMinReadTimeMillis(Long minReadTimeMillis);\n \n+  @Description(\"Max records per micro-batch. For streaming sources only.\")\n+  @Default.Long(-1)\n+  Long getMaxRecordsPerBatch();\n+  void setMaxRecordsPerBatch(Long maxRecordsPerBatch);\n+\n   @Description(\"A value between 0-1 to describe the percentage of a micro-batch dedicated \"\n       + \"to reading from UnboundedSource.\")\n   @Default.Double(0.1)\n@@ -95,4 +100,9 @@ public String create(PipelineOptions options) {\n   @Default.Boolean(false)\n   boolean getUsesProvidedSparkContext();\n   void setUsesProvidedSparkContext(boolean value);\n+\n+  @Description(\"A special flag that forces streaming in tests.\")\n+  @Default.Boolean(false)\n+  boolean isForceStreaming();\n+  void setForceStreaming(boolean forceStreaming);\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineOptions.java",
                "sha": "04c559e86aef779ef9298688f75af9fd9b775445",
                "status": "modified"
            },
            {
                "additions": 193,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineResult.java",
                "changes": 193,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineResult.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineResult.java",
                "patch": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.spark;\n+\n+import java.io.IOException;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import org.apache.beam.runners.spark.aggregators.SparkAggregators;\n+import org.apache.beam.runners.spark.translation.SparkContextFactory;\n+import org.apache.beam.sdk.AggregatorRetrievalException;\n+import org.apache.beam.sdk.AggregatorValues;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.metrics.MetricResults;\n+import org.apache.beam.sdk.transforms.Aggregator;\n+import org.apache.beam.sdk.util.UserCodeException;\n+import org.apache.spark.SparkException;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.joda.time.Duration;\n+\n+/**\n+ * Represents a Spark pipeline execution result.\n+ */\n+public abstract class SparkPipelineResult implements PipelineResult {\n+\n+  protected final Future pipelineExecution;\n+  protected JavaSparkContext javaSparkContext;\n+\n+  protected PipelineResult.State state;\n+\n+  SparkPipelineResult(final Future<?> pipelineExecution,\n+                      final JavaSparkContext javaSparkContext) {\n+    this.pipelineExecution = pipelineExecution;\n+    this.javaSparkContext = javaSparkContext;\n+    // pipelineExecution is expected to have started executing eagerly.\n+    state = State.RUNNING;\n+  }\n+\n+  private RuntimeException runtimeExceptionFrom(final Throwable e) {\n+    return (e instanceof RuntimeException) ? (RuntimeException) e : new RuntimeException(e);\n+  }\n+\n+  private RuntimeException beamExceptionFrom(final Throwable e) {\n+    // Scala doesn't declare checked exceptions in the bytecode, and the Java compiler\n+    // won't let you catch something that is not declared, so we can't catch\n+    // SparkException directly, instead we do an instanceof check.\n+\n+    if (e instanceof SparkException) {\n+      if (e.getCause() != null && e.getCause() instanceof UserCodeException) {\n+        UserCodeException userException = (UserCodeException) e.getCause();\n+        return new Pipeline.PipelineExecutionException(userException.getCause());\n+      } else if (e.getCause() != null) {\n+        return new Pipeline.PipelineExecutionException(e.getCause());\n+      }\n+    }\n+\n+    return runtimeExceptionFrom(e);\n+  }\n+\n+  protected abstract void stop();\n+\n+  protected abstract State awaitTermination(Duration duration)\n+      throws TimeoutException, ExecutionException, InterruptedException;\n+\n+  public <T> T getAggregatorValue(final String name, final Class<T> resultType) {\n+    return SparkAggregators.valueOf(name, resultType, javaSparkContext);\n+  }\n+\n+  @Override\n+  public <T> AggregatorValues<T> getAggregatorValues(final Aggregator<?, T> aggregator)\n+      throws AggregatorRetrievalException {\n+    return SparkAggregators.valueOf(aggregator, javaSparkContext);\n+  }\n+\n+  @Override\n+  public PipelineResult.State getState() {\n+    return state;\n+  }\n+\n+  @Override\n+  public PipelineResult.State waitUntilFinish() {\n+    return waitUntilFinish(Duration.millis(Long.MAX_VALUE));\n+  }\n+\n+  @Override\n+  public State waitUntilFinish(final Duration duration) {\n+    try {\n+      state = awaitTermination(duration);\n+    } catch (final TimeoutException e) {\n+      state = null;\n+    } catch (final ExecutionException e) {\n+      state = PipelineResult.State.FAILED;\n+      throw beamExceptionFrom(e.getCause());\n+    } catch (final Exception e) {\n+      state = PipelineResult.State.FAILED;\n+      throw beamExceptionFrom(e);\n+    } finally {\n+      stop();\n+    }\n+\n+    return state;\n+  }\n+\n+  @Override\n+  public MetricResults metrics() {\n+    throw new UnsupportedOperationException(\"The SparkRunner does not currently support metrics.\");\n+  }\n+\n+  @Override\n+  public PipelineResult.State cancel() throws IOException {\n+    if (state != null && !state.isTerminal()) {\n+      stop();\n+      state = PipelineResult.State.CANCELLED;\n+    }\n+\n+    return state;\n+  }\n+\n+  /**\n+   * Represents the result of running a batch pipeline.\n+   */\n+  static class BatchMode extends SparkPipelineResult {\n+\n+    BatchMode(final Future<?> pipelineExecution,\n+              final JavaSparkContext javaSparkContext) {\n+      super(pipelineExecution, javaSparkContext);\n+    }\n+\n+    @Override\n+    protected void stop() {\n+      SparkContextFactory.stopSparkContext(javaSparkContext);\n+    }\n+\n+    @Override\n+    protected State awaitTermination(final Duration duration)\n+        throws TimeoutException, ExecutionException, InterruptedException {\n+      pipelineExecution.get(duration.getMillis(), TimeUnit.MILLISECONDS);\n+      return PipelineResult.State.DONE;\n+    }\n+  }\n+\n+  /**\n+   * Represents a streaming Spark pipeline result.\n+   */\n+  static class StreamingMode extends SparkPipelineResult {\n+\n+    private final JavaStreamingContext javaStreamingContext;\n+\n+    StreamingMode(final Future<?> pipelineExecution,\n+                  final JavaStreamingContext javaStreamingContext) {\n+      super(pipelineExecution, javaStreamingContext.sparkContext());\n+      this.javaStreamingContext = javaStreamingContext;\n+    }\n+\n+    @Override\n+    protected void stop() {\n+      javaStreamingContext.stop(false, true);\n+      SparkContextFactory.stopSparkContext(javaSparkContext);\n+    }\n+\n+    @Override\n+    protected State awaitTermination(final Duration duration) throws TimeoutException,\n+        ExecutionException, InterruptedException {\n+      pipelineExecution.get(duration.getMillis(), TimeUnit.MILLISECONDS);\n+      if (javaStreamingContext.awaitTerminationOrTimeout(duration.getMillis())) {\n+        return State.DONE;\n+      } else {\n+        return null;\n+      }\n+    }\n+\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineResult.java",
                "sha": "b1027a6e96f7f7292c5858fbca469de1ae84cb3c",
                "status": "added"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java",
                "changes": 130,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 53,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java",
                "patch": "@@ -18,8 +18,15 @@\n \n package org.apache.beam.runners.spark;\n \n+import com.google.common.collect.Iterables;\n import java.util.Collection;\n import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n+import org.apache.beam.runners.spark.aggregators.SparkAggregators;\n+import org.apache.beam.runners.spark.aggregators.metrics.AggregatorMetricSource;\n import org.apache.beam.runners.spark.translation.EvaluationContext;\n import org.apache.beam.runners.spark.translation.SparkContextFactory;\n import org.apache.beam.runners.spark.translation.SparkPipelineTranslator;\n@@ -36,15 +43,15 @@\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.PTransform;\n-import org.apache.beam.sdk.util.UserCodeException;\n-import org.apache.beam.sdk.values.PBegin;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n import org.apache.beam.sdk.values.PValue;\n-import org.apache.spark.SparkException;\n+import org.apache.spark.Accumulator;\n+import org.apache.spark.SparkEnv$;\n import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.metrics.MetricsSystem;\n import org.apache.spark.streaming.api.java.JavaStreamingContext;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -58,7 +65,7 @@\n  *\n  * {@code\n  * Pipeline p = [logic for pipeline creation]\n- * EvaluationResult result = (EvaluationResult) p.run();\n+ * SparkPipelineResult result = (SparkPipelineResult) p.run();\n  * }\n  *\n  * <p>To create a pipeline runner to run against a different spark cluster, with a custom master url\n@@ -68,10 +75,10 @@\n  * Pipeline p = [logic for pipeline creation]\n  * SparkPipelineOptions options = SparkPipelineOptionsFactory.create();\n  * options.setSparkMaster(\"spark://host:port\");\n- * EvaluationResult result = (EvaluationResult) p.run();\n+ * SparkPipelineResult result = (SparkPipelineResult) p.run();\n  * }\n  */\n-public final class SparkRunner extends PipelineRunner<EvaluationResult> {\n+public final class SparkRunner extends PipelineRunner<SparkPipelineResult> {\n \n   private static final Logger LOG = LoggerFactory.getLogger(SparkRunner.class);\n   /**\n@@ -121,51 +128,67 @@ private SparkRunner(SparkPipelineOptions options) {\n     mOptions = options;\n   }\n \n+  private void registerMetrics(final SparkPipelineOptions opts, final JavaSparkContext jsc) {\n+    final Accumulator<NamedAggregators> accum = SparkAggregators.getNamedAggregators(jsc);\n+    final NamedAggregators initialValue = accum.value();\n+\n+    if (opts.getEnableSparkMetricSinks()) {\n+      final MetricsSystem metricsSystem = SparkEnv$.MODULE$.get().metricsSystem();\n+      final AggregatorMetricSource aggregatorMetricSource =\n+          new AggregatorMetricSource(opts.getAppName(), initialValue);\n+      // re-register the metrics in case of context re-use\n+      metricsSystem.removeSource(aggregatorMetricSource);\n+      metricsSystem.registerSource(aggregatorMetricSource);\n+    }\n+  }\n+\n   @Override\n-  public EvaluationResult run(Pipeline pipeline) {\n-    try {\n-      LOG.info(\"Executing pipeline using the SparkRunner.\");\n-\n-      detectTranslationMode(pipeline);\n-      if (mOptions.isStreaming()) {\n-        SparkRunnerStreamingContextFactory contextFactory =\n-            new SparkRunnerStreamingContextFactory(pipeline, mOptions);\n-        JavaStreamingContext jssc = JavaStreamingContext.getOrCreate(mOptions.getCheckpointDir(),\n-            contextFactory);\n-\n-        LOG.info(\"Starting streaming pipeline execution.\");\n-        jssc.start();\n-\n-        // if recovering from checkpoint, we have to reconstruct the EvaluationResult instance.\n-        return contextFactory.getCtxt() == null ? new EvaluationContext(jssc.sparkContext(),\n-            pipeline, jssc) : contextFactory.getCtxt();\n-      } else {\n-        JavaSparkContext jsc = SparkContextFactory.getSparkContext(mOptions);\n-        EvaluationContext ctxt = new EvaluationContext(jsc, pipeline);\n-        SparkPipelineTranslator translator = new TransformTranslator.Translator();\n-        pipeline.traverseTopologically(new Evaluator(translator, ctxt));\n-        ctxt.computeOutputs();\n+  public SparkPipelineResult run(final Pipeline pipeline) {\n+    LOG.info(\"Executing pipeline using the SparkRunner.\");\n \n-        LOG.info(\"Pipeline execution complete.\");\n+    final SparkPipelineResult result;\n+    final Future<?> startPipeline;\n+    final ExecutorService executorService = Executors.newSingleThreadExecutor();\n \n-        return ctxt;\n-      }\n-    } catch (Exception e) {\n-      // Scala doesn't declare checked exceptions in the bytecode, and the Java compiler\n-      // won't let you catch something that is not declared, so we can't catch\n-      // SparkException here. Instead we do an instanceof check.\n-      // Then we find the cause by seeing if it's a user exception (wrapped by Beam's\n-      // UserCodeException), or just use the SparkException cause.\n-      if (e instanceof SparkException && e.getCause() != null) {\n-        if (e.getCause() instanceof UserCodeException && e.getCause().getCause() != null) {\n-          throw UserCodeException.wrap(e.getCause().getCause());\n-        } else {\n-          throw new RuntimeException(e.getCause());\n+    detectTranslationMode(pipeline);\n+\n+    if (mOptions.isStreaming()) {\n+      final SparkRunnerStreamingContextFactory contextFactory =\n+          new SparkRunnerStreamingContextFactory(pipeline, mOptions);\n+      final JavaStreamingContext jssc =\n+          JavaStreamingContext.getOrCreate(mOptions.getCheckpointDir(), contextFactory);\n+\n+      startPipeline = executorService.submit(new Runnable() {\n+\n+        @Override\n+        public void run() {\n+          registerMetrics(mOptions, jssc.sparkContext());\n+          LOG.info(\"Starting streaming pipeline execution.\");\n+          jssc.start();\n         }\n-      }\n-      // otherwise just wrap in a RuntimeException\n-      throw new RuntimeException(e);\n+      });\n+\n+      result = new SparkPipelineResult.StreamingMode(startPipeline, jssc);\n+    } else {\n+      final JavaSparkContext jsc = SparkContextFactory.getSparkContext(mOptions);\n+      final EvaluationContext evaluationContext = new EvaluationContext(jsc, pipeline);\n+\n+      startPipeline = executorService.submit(new Runnable() {\n+\n+        @Override\n+        public void run() {\n+          registerMetrics(mOptions, jsc);\n+          pipeline.traverseTopologically(new Evaluator(new TransformTranslator.Translator(),\n+                                                       evaluationContext));\n+          evaluationContext.computeOutputs();\n+          LOG.info(\"Batch pipeline execution complete.\");\n+        }\n+      });\n+\n+      result = new SparkPipelineResult.BatchMode(startPipeline, jsc);\n     }\n+\n+    return result;\n   }\n \n   /**\n@@ -255,8 +278,11 @@ public CompositeBehavior enterCompositeTransform(TransformHierarchy.Node node) {\n     }\n \n     private boolean shouldDefer(TransformHierarchy.Node node) {\n-      PInput input = node.getInput();\n       // if the input is not a PCollection, or it is but with non merging windows, don't defer.\n+      if (node.getInputs().size() != 1) {\n+        return false;\n+      }\n+      PValue input = Iterables.getOnlyElement(node.getInputs());\n       if (!(input instanceof PCollection)\n           || ((PCollection) input).getWindowingStrategy().getWindowFn().isNonMerging()) {\n         return false;\n@@ -296,8 +322,7 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n       @SuppressWarnings(\"unchecked\") TransformEvaluator<TransformT> evaluator =\n           translate(node, transform, transformClass);\n       LOG.info(\"Evaluating {}\", transform);\n-      AppliedPTransform<PInput, POutput, TransformT> appliedTransform =\n-          AppliedPTransform.of(node.getFullName(), node.getInput(), node.getOutput(), transform);\n+      AppliedPTransform<?, ?, ?> appliedTransform = node.toAppliedPTransform();\n       ctxt.setCurrentTransform(appliedTransform);\n       evaluator.evaluate(transform, ctxt);\n       ctxt.setCurrentTransform(null);\n@@ -314,12 +339,11 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n       // usually, the input determines if the PCollection to apply the next transformation to\n       // is BOUNDED or UNBOUNDED, meaning RDD/DStream.\n       Collection<? extends PValue> pValues;\n-      PInput pInput = node.getInput();\n-      if (pInput instanceof PBegin) {\n+      if (node.getInputs().isEmpty()) {\n         // in case of a PBegin, it's the output.\n-        pValues = node.getOutput().expand();\n+        pValues = node.getOutputs();\n       } else {\n-        pValues = pInput.expand();\n+        pValues = node.getInputs();\n       }\n       PCollection.IsBounded isNodeBounded = isBoundedCollection(pValues);\n       // translate accordingly.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkRunner.java",
                "sha": "3d98b87a351fed62c5a09269304696801bd27f9f",
                "status": "modified"
            },
            {
                "additions": 82,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java",
                "changes": 91,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 9,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java",
                "patch": "@@ -19,16 +19,26 @@\n package org.apache.beam.runners.spark;\n \n import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n \n+import org.apache.beam.runners.core.UnboundedReadFromBoundedSource;\n import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult.State;\n+import org.apache.beam.sdk.io.BoundedReadFromUnboundedSource;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.PipelineOptionsValidator;\n import org.apache.beam.sdk.runners.PipelineRunner;\n+import org.apache.beam.sdk.testing.PAssert;\n import org.apache.beam.sdk.testing.TestPipelineOptions;\n import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.util.ValueWithRecordId;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n \n+\n /**\n  * The SparkRunner translate operations defined on a pipeline to a representation executable\n  * by Spark, and then submitting the job to Spark to be executed. If we wanted to run a Beam\n@@ -37,7 +47,7 @@\n  *\n  * {@code\n  * Pipeline p = [logic for pipeline creation]\n- * EvaluationResult result = (EvaluationResult) p.run();\n+ * SparkPipelineResult result = (SparkPipelineResult) p.run();\n  * }\n  *\n  * <p>To create a pipeline runner to run against a different spark cluster, with a custom master url\n@@ -47,15 +57,18 @@\n  * Pipeline p = [logic for pipeline creation]\n  * SparkPipelineOptions options = SparkPipelineOptionsFactory.create();\n  * options.setSparkMaster(\"spark://host:port\");\n- * EvaluationResult result = (EvaluationResult) p.run();\n+ * SparkPipelineResult result = (SparkPipelineResult) p.run();\n  * }\n  */\n-public final class TestSparkRunner extends PipelineRunner<EvaluationResult> {\n+public final class TestSparkRunner extends PipelineRunner<SparkPipelineResult> {\n \n   private SparkRunner delegate;\n+  private boolean isForceStreaming;\n+  private int expectedNumberOfAssertions = 0;\n \n   private TestSparkRunner(SparkPipelineOptions options) {\n     this.delegate = SparkRunner.fromOptions(options);\n+    this.isForceStreaming = options.isForceStreaming();\n   }\n \n   public static TestSparkRunner fromOptions(PipelineOptions options) {\n@@ -65,18 +78,78 @@ public static TestSparkRunner fromOptions(PipelineOptions options) {\n     return new TestSparkRunner(sparkOptions);\n   }\n \n+  /**\n+   * Overrides for the test runner.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n   @Override\n-  public <OutputT extends POutput, InputT extends PInput>\n-      OutputT apply(PTransform<InputT, OutputT> transform, InputT input) {\n-    return delegate.apply(transform, input);\n-  };\n+  public <OutputT extends POutput, InputT extends PInput> OutputT apply(\n+          PTransform<InputT, OutputT> transform, InputT input) {\n+    // if the pipeline forces execution as a streaming pipeline,\n+    // and the source is an adapted unbounded source (as bounded),\n+    // read it as unbounded source via UnboundedReadFromBoundedSource.\n+    if (isForceStreaming && transform instanceof BoundedReadFromUnboundedSource) {\n+      return (OutputT) delegate.apply(new AdaptedBoundedAsUnbounded(\n+          (BoundedReadFromUnboundedSource) transform), input);\n+    } else {\n+      // no actual override, simply counts asserting transforms in the pipeline.\n+      if (transform instanceof PAssert.OneSideInputAssert\n+          || transform instanceof PAssert.GroupThenAssert\n+          || transform instanceof PAssert.GroupThenAssertForSingleton) {\n+        expectedNumberOfAssertions += 1;\n+      }\n+\n+      return delegate.apply(transform, input);\n+    }\n+  }\n \n   @Override\n-  public EvaluationResult run(Pipeline pipeline) {\n+  public SparkPipelineResult run(Pipeline pipeline) {\n     TestPipelineOptions testPipelineOptions = pipeline.getOptions().as(TestPipelineOptions.class);\n-    EvaluationResult result = delegate.run(pipeline);\n+    SparkPipelineResult result = delegate.run(pipeline);\n+    result.waitUntilFinish();\n+\n+    // make sure the test pipeline finished successfully.\n+    State resultState = result.getState();\n+    assertThat(\n+        String.format(\"Test pipeline result state was %s instead of %s\", resultState, State.DONE),\n+        resultState,\n+        is(State.DONE));\n     assertThat(result, testPipelineOptions.getOnCreateMatcher());\n     assertThat(result, testPipelineOptions.getOnSuccessMatcher());\n+\n+    // if the pipeline was executed in streaming mode, validate aggregators.\n+    if (isForceStreaming) {\n+      // validate assertion succeeded (at least once).\n+      int success = result.getAggregatorValue(PAssert.SUCCESS_COUNTER, Integer.class);\n+      assertThat(\n+          String.format(\n+              \"Expected %d successful assertions, but found %d.\",\n+              expectedNumberOfAssertions, success),\n+          success,\n+          is(expectedNumberOfAssertions));\n+      // validate assertion didn't fail.\n+      int failure = result.getAggregatorValue(PAssert.FAILURE_COUNTER, Integer.class);\n+      assertThat(\"Failure aggregator should be zero.\", failure, is(0));\n+    }\n     return result;\n   }\n+\n+  private static class AdaptedBoundedAsUnbounded<T> extends PTransform<PBegin, PCollection<T>> {\n+    private final BoundedReadFromUnboundedSource<T> source;\n+\n+    AdaptedBoundedAsUnbounded(BoundedReadFromUnboundedSource<T> source) {\n+      this.source = source;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      PTransform<PBegin, ? extends PCollection<ValueWithRecordId<T>>> replacingTransform =\n+          new UnboundedReadFromBoundedSource<>(source.getAdaptedSource());\n+      return (PCollection<T>) input.apply(replacingTransform)\n+          .apply(\"StripIds\", ParDo.of(new ValueWithRecordId.StripIdsDoFn()));\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/TestSparkRunner.java",
                "sha": "798ca47798045d581f9eb75f43d8c82b64a61522",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/AccumulatorSingleton.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/AccumulatorSingleton.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/AccumulatorSingleton.java",
                "patch": "@@ -26,11 +26,11 @@\n  * For resilience, {@link Accumulator}s are required to be wrapped in a Singleton.\n  * @see <a href=\"https://spark.apache.org/docs/1.6.3/streaming-programming-guide.html#accumulators-and-broadcast-variables\">accumulators</a>\n  */\n-public class AccumulatorSingleton {\n+class AccumulatorSingleton {\n \n   private static volatile Accumulator<NamedAggregators> instance = null;\n \n-  public static Accumulator<NamedAggregators> getInstance(JavaSparkContext jsc) {\n+  static Accumulator<NamedAggregators> getInstance(JavaSparkContext jsc) {\n     if (instance == null) {\n       synchronized (AccumulatorSingleton.class) {\n         if (instance == null) {\n@@ -45,7 +45,7 @@\n   }\n \n   @VisibleForTesting\n-  public static void clear() {\n+  static void clear() {\n     synchronized (AccumulatorSingleton.class) {\n       instance = null;\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/AccumulatorSingleton.java",
                "sha": "883830e5ce8e831895be0f946a94160b73f6a4e2",
                "status": "modified"
            },
            {
                "additions": 126,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/SparkAggregators.java",
                "changes": 126,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/SparkAggregators.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/SparkAggregators.java",
                "patch": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.spark.aggregators;\n+\n+import com.google.common.collect.ImmutableList;\n+import java.util.Collection;\n+import java.util.Map;\n+import org.apache.beam.runners.core.AggregatorFactory;\n+import org.apache.beam.runners.spark.translation.SparkRuntimeContext;\n+import org.apache.beam.sdk.AggregatorValues;\n+import org.apache.beam.sdk.transforms.Aggregator;\n+import org.apache.beam.sdk.transforms.Combine;\n+import org.apache.beam.sdk.util.ExecutionContext;\n+import org.apache.spark.Accumulator;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+/**\n+ * A utility class for handling Beam {@link Aggregator}s.\n+ */\n+public class SparkAggregators {\n+\n+  private static <T> AggregatorValues<T> valueOf(final Accumulator<NamedAggregators> accum,\n+                                                 final Aggregator<?, T> aggregator) {\n+    @SuppressWarnings(\"unchecked\")\n+    Class<T> valueType = (Class<T>) aggregator.getCombineFn().getOutputType().getRawType();\n+    final T value = valueOf(accum, aggregator.getName(), valueType);\n+\n+    return new AggregatorValues<T>() {\n+\n+      @Override\n+      public Collection<T> getValues() {\n+        return ImmutableList.of(value);\n+      }\n+\n+      @Override\n+      public Map<String, T> getValuesAtSteps() {\n+        throw new UnsupportedOperationException(\"getValuesAtSteps is not supported.\");\n+      }\n+    };\n+  }\n+\n+  private static <T> T valueOf(final Accumulator<NamedAggregators> accum,\n+                               final String aggregatorName,\n+                               final Class<T> typeClass) {\n+    return accum.value().getValue(aggregatorName, typeClass);\n+  }\n+\n+  /**\n+   * Retrieves the {@link NamedAggregators} instance using the provided Spark context.\n+   *\n+   * @param jsc a Spark context to be used in order to retrieve the name\n+   * {@link NamedAggregators} instance\n+   * @return a {@link NamedAggregators} instance\n+   */\n+  public static Accumulator<NamedAggregators> getNamedAggregators(JavaSparkContext jsc) {\n+    return AccumulatorSingleton.getInstance(jsc);\n+  }\n+\n+  /**\n+   * Retrieves the value of an aggregator from a SparkContext instance.\n+   *\n+   * @param aggregator The aggregator whose value to retrieve\n+   * @param javaSparkContext The SparkContext instance\n+   * @param <T> The type of the aggregator's output\n+   * @return The value of the aggregator\n+   */\n+  public static <T> AggregatorValues<T> valueOf(final Aggregator<?, T> aggregator,\n+                                                final JavaSparkContext javaSparkContext) {\n+    return valueOf(getNamedAggregators(javaSparkContext), aggregator);\n+  }\n+\n+  /**\n+   * Retrieves the value of an aggregator from a SparkContext instance.\n+   *\n+   * @param name Name of the aggregator to retrieve the value of.\n+   * @param typeClass      Type class of value to be retrieved.\n+   * @param <T>            Type of object to be returned.\n+   * @return The value of the aggregator.\n+   */\n+  public static <T> T valueOf(final String name,\n+                              final Class<T> typeClass,\n+                              final JavaSparkContext javaSparkContext) {\n+    return valueOf(getNamedAggregators(javaSparkContext), name, typeClass);\n+  }\n+\n+  /**\n+   * An implementation of {@link AggregatorFactory} for the SparkRunner.\n+   */\n+  public static class Factory implements AggregatorFactory {\n+\n+    private final SparkRuntimeContext runtimeContext;\n+    private final Accumulator<NamedAggregators> accumulator;\n+\n+    public Factory(SparkRuntimeContext runtimeContext, Accumulator<NamedAggregators> accumulator) {\n+      this.runtimeContext = runtimeContext;\n+      this.accumulator = accumulator;\n+    }\n+\n+    @Override\n+    public <InputT, AccumT, OutputT> Aggregator<InputT, OutputT> createAggregatorForDoFn(\n+        Class<?> fnClass,\n+        ExecutionContext.StepContext stepContext,\n+        String aggregatorName,\n+        Combine.CombineFn<InputT, AccumT, OutputT> combine) {\n+\n+      return runtimeContext.createAggregator(accumulator, aggregatorName, combine);\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/SparkAggregators.java",
                "sha": "17d584481bc263c974f610478312ec4db7e2b800",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/metrics/WithNamedAggregatorsSupport.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/metrics/WithNamedAggregatorsSupport.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/metrics/WithNamedAggregatorsSupport.java",
                "patch": "@@ -121,8 +121,8 @@ public static WithNamedAggregatorsSupport forRegistry(final MetricRegistry metri\n         final String parentName = entry.getKey();\n         final Map<String, Gauge> gaugeMap = Maps.transformEntries(agg.renderAll(), toGauge());\n         final Map<String, Gauge> fullNameGaugeMap = Maps.newLinkedHashMap();\n-        for (String shortName : gaugeMap.keySet()) {\n-          fullNameGaugeMap.put(parentName + \".\" + shortName, gaugeMap.get(shortName));\n+        for (Map.Entry<String, Gauge> gaugeEntry : gaugeMap.entrySet()) {\n+          fullNameGaugeMap.put(parentName + \".\" + gaugeEntry.getKey(), gaugeEntry.getValue());\n         }\n         return Maps.filterValues(fullNameGaugeMap, Predicates.notNull());\n       }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/aggregators/metrics/WithNamedAggregatorsSupport.java",
                "sha": "5e712803224cbae8497e0c71151944f3db61fdc0",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 14,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java",
                "patch": "@@ -19,28 +19,60 @@\n package org.apache.beam.runners.spark.coders;\n \n import com.esotericsoftware.kryo.Kryo;\n-import de.javakaffee.kryoserializers.UnmodifiableCollectionsSerializer;\n-import de.javakaffee.kryoserializers.guava.ImmutableListSerializer;\n-import de.javakaffee.kryoserializers.guava.ImmutableMapSerializer;\n-import de.javakaffee.kryoserializers.guava.ImmutableMultimapSerializer;\n-import de.javakaffee.kryoserializers.guava.ImmutableSetSerializer;\n-import de.javakaffee.kryoserializers.guava.ReverseListSerializer;\n+import com.esotericsoftware.kryo.serializers.JavaSerializer;\n+import com.google.common.base.Function;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.util.Arrays;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.Source;\n import org.apache.spark.serializer.KryoRegistrator;\n+import org.reflections.Reflections;\n \n \n /**\n- * Custom {@link com.esotericsoftware.kryo.Serializer}s for Beam's Spark runner needs.\n+ * Custom {@link KryoRegistrator}s for Beam's Spark runner needs.\n  */\n public class BeamSparkRunnerRegistrator implements KryoRegistrator {\n \n   @Override\n   public void registerClasses(Kryo kryo) {\n-    UnmodifiableCollectionsSerializer.registerSerializers(kryo);\n-    // Guava\n-    ImmutableListSerializer.registerSerializers(kryo);\n-    ImmutableSetSerializer.registerSerializers(kryo);\n-    ImmutableMapSerializer.registerSerializers(kryo);\n-    ImmutableMultimapSerializer.registerSerializers(kryo);\n-    ReverseListSerializer.registerSerializers(kryo);\n+    for (Class<?> clazz : ClassesForJavaSerialization.getClasses()) {\n+      kryo.register(clazz, new JavaSerializer());\n+    }\n+  }\n+\n+  /**\n+   * Register coders and sources with {@link JavaSerializer} since they aren't guaranteed to be\n+   * Kryo-serializable.\n+   */\n+  private static class ClassesForJavaSerialization {\n+    private static final Class<?>[] CLASSES_FOR_JAVA_SERIALIZATION = new Class<?>[]{\n+        Coder.class, Source.class\n+    };\n+\n+    private static final Iterable<Class<?>> INSTANCE;\n+\n+    /**\n+     * Find all subclasses of ${@link CLASSES_FOR_JAVA_SERIALIZATION}\n+     */\n+    static {\n+      final Reflections reflections = new Reflections();\n+      INSTANCE = Iterables.concat(Lists.transform(Arrays.asList(CLASSES_FOR_JAVA_SERIALIZATION),\n+          new Function<Class, Set<Class<?>>>() {\n+            @SuppressWarnings({\"unchecked\", \"ConstantConditions\"})\n+            @Nullable\n+            @Override\n+            public Set<Class<?>> apply(@Nullable Class clazz) {\n+              return reflections.getSubTypesOf(clazz);\n+            }\n+          }));\n+    }\n+\n+    static Iterable<Class<?>> getClasses() {\n+      return INSTANCE;\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistrator.java",
                "sha": "41b0a0198f6c59b2326639b012f48d478aea6a20",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/examples/WordCount.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/examples/WordCount.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 8,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/examples/WordCount.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.Count;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.MapElements;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.SimpleFunction;\n@@ -35,7 +35,7 @@\n import org.apache.beam.sdk.values.PCollection;\n \n /**\n- * Duplicated to avoid dependency on beam-examples.\n+ * Duplicated from beam-examples-java to avoid dependency.\n  */\n public class WordCount {\n \n@@ -44,11 +44,11 @@\n    * of-line. This DoFn tokenizes lines of text into individual words; we pass it to a ParDo in the\n    * pipeline.\n    */\n-  static class ExtractWordsFn extends OldDoFn<String, String> {\n+  static class ExtractWordsFn extends DoFn<String, String> {\n     private final Aggregator<Long, Long> emptyLines =\n         createAggregator(\"emptyLines\", new Sum.SumLongFn());\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) {\n       if (c.element().trim().isEmpty()) {\n         emptyLines.addValue(1L);\n@@ -85,7 +85,7 @@ public String apply(KV<String, Long> input) {\n   public static class CountWords extends PTransform<PCollection<String>,\n       PCollection<KV<String, Long>>> {\n     @Override\n-    public PCollection<KV<String, Long>> apply(PCollection<String> lines) {\n+    public PCollection<KV<String, Long>> expand(PCollection<String> lines) {\n \n       // Convert lines of text into individual words.\n       PCollection<String> words = lines.apply(\n@@ -126,12 +126,11 @@ public static void main(String[] args) {\n \n     // Concepts #2 and #3: Our pipeline applies the composite CountWords transform, and passes the\n     // static FormatAsTextFn() to the ParDo transform.\n-    //TODO: remove withoutValidation once possible\n-    p.apply(\"ReadLines\", TextIO.Read.from(options.getInputFile()).withoutValidation())\n+    p.apply(\"ReadLines\", TextIO.Read.from(options.getInputFile()))\n      .apply(new CountWords())\n      .apply(MapElements.via(new FormatAsTextFn()))\n      .apply(\"WriteCounts\", TextIO.Write.to(options.getOutput()));\n \n-    p.run();\n+    p.run().waitUntilFinish();\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/examples/WordCount.java",
                "sha": "1252d1249b69b490882fd786afc8d6f58427fcf7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/ConsoleIO.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/io/ConsoleIO.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/io/ConsoleIO.java",
                "patch": "@@ -62,7 +62,7 @@ public int getNum() {\n       }\n \n       @Override\n-      public PDone apply(PCollection<T> input) {\n+      public PDone expand(PCollection<T> input) {\n         return PDone.in(input.getPipeline());\n       }\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/ConsoleIO.java",
                "sha": "0a566331a45fc5fc370c1afff63ec897b37051eb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/CreateStream.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/io/CreateStream.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/io/CreateStream.java",
                "patch": "@@ -63,7 +63,7 @@ private CreateStream() {\n     }\n \n     @Override\n-    public PCollection<T> apply(PBegin input) {\n+    public PCollection<T> expand(PBegin input) {\n       // Spark streaming micro batches are bounded by default\n       return PCollection.createPrimitiveOutputInternal(input.getPipeline(),\n           WindowingStrategy.globalDefault(), PCollection.IsBounded.UNBOUNDED);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/CreateStream.java",
                "sha": "7ebba90b518fb987653388cf315570f563c95c6a",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SourceDStream.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SourceDStream.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 5,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/io/SourceDStream.java",
                "patch": "@@ -53,7 +53,7 @@\n  * {@link SparkPipelineOptions#getMinReadTimeMillis()}.\n  * Records bound is controlled by the {@link RateController} mechanism.\n  */\n-public class SourceDStream<T, CheckpointMarkT extends UnboundedSource.CheckpointMark>\n+class SourceDStream<T, CheckpointMarkT extends UnboundedSource.CheckpointMark>\n       extends InputDStream<Tuple2<Source<T>, CheckpointMarkT>> {\n   private static final Logger LOG = LoggerFactory.getLogger(SourceDStream.class);\n \n@@ -64,10 +64,16 @@\n   // in case of resuming/recovering from checkpoint, the DStream will be reconstructed and this\n   // property should not be reset.\n   private final int initialParallelism;\n+  // the bound on max records is optional.\n+  // in case it is set explicitly via PipelineOptions, it takes precedence\n+  // otherwise it could be activated via RateController.\n+  private Long boundMaxRecords = null;\n+\n+  SourceDStream(\n+      StreamingContext ssc,\n+      UnboundedSource<T, CheckpointMarkT> unboundedSource,\n+      SparkRuntimeContext runtimeContext) {\n \n-  public SourceDStream(StreamingContext ssc,\n-                       UnboundedSource<T, CheckpointMarkT> unboundedSource,\n-                       SparkRuntimeContext runtimeContext) {\n     super(ssc, JavaSparkContext$.MODULE$.<scala.Tuple2<Source<T>, CheckpointMarkT>>fakeClassTag());\n     this.unboundedSource = unboundedSource;\n     this.runtimeContext = runtimeContext;\n@@ -80,10 +86,15 @@ public SourceDStream(StreamingContext ssc,\n     checkArgument(this.initialParallelism > 0, \"Number of partitions must be greater than zero.\");\n   }\n \n+  public void setMaxRecordsPerBatch(long maxRecordsPerBatch) {\n+    boundMaxRecords = maxRecordsPerBatch;\n+  }\n+\n   @Override\n   public scala.Option<RDD<Tuple2<Source<T>, CheckpointMarkT>>> compute(Time validTime) {\n+    long maxNumRecords = boundMaxRecords != null ? boundMaxRecords : rateControlledMaxRecords();\n     MicrobatchSource<T, CheckpointMarkT> microbatchSource = new MicrobatchSource<>(\n-        unboundedSource, boundReadDuration, initialParallelism, rateControlledMaxRecords(), -1,\n+        unboundedSource, boundReadDuration, initialParallelism, maxNumRecords, -1,\n         id());\n     RDD<scala.Tuple2<Source<T>, CheckpointMarkT>> rdd = new SourceRDD.Unbounded<>(\n         ssc().sc(), runtimeContext, microbatchSource);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SourceDStream.java",
                "sha": "8a0763b7052b3752544ddf3aa024a240b12b903e",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 6,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java",
                "patch": "@@ -61,19 +61,25 @@\n   JavaDStream<WindowedValue<T>> read(JavaStreamingContext jssc,\n                                      SparkRuntimeContext rc,\n                                      UnboundedSource<T, CheckpointMarkT> source) {\n+    SparkPipelineOptions options = rc.getPipelineOptions().as(SparkPipelineOptions.class);\n+    Long maxRecordsPerBatch = options.getMaxRecordsPerBatch();\n+    SourceDStream<T, CheckpointMarkT> sourceDStream = new SourceDStream<>(jssc.ssc(), source, rc);\n+    // if max records per batch was set by the user.\n+    if (maxRecordsPerBatch > 0) {\n+      sourceDStream.setMaxRecordsPerBatch(maxRecordsPerBatch);\n+    }\n     JavaPairInputDStream<Source<T>, CheckpointMarkT> inputDStream =\n-        JavaPairInputDStream$.MODULE$.fromInputDStream(new SourceDStream<>(jssc.ssc(), source, rc),\n+        JavaPairInputDStream$.MODULE$.fromInputDStream(sourceDStream,\n             JavaSparkContext$.MODULE$.<Source<T>>fakeClassTag(),\n                 JavaSparkContext$.MODULE$.<CheckpointMarkT>fakeClassTag());\n \n     // call mapWithState to read from a checkpointable sources.\n-    //TODO: consider broadcasting the rc instead of re-sending every batch.\n     JavaMapWithStateDStream<Source<T>, CheckpointMarkT, byte[],\n         Iterator<WindowedValue<T>>> mapWithStateDStream = inputDStream.mapWithState(\n             StateSpec.function(StateSpecFunctions.<T, CheckpointMarkT>mapSourceFunction(rc)));\n \n     // set checkpoint duration for read stream, if set.\n-    checkpointStream(mapWithStateDStream, rc);\n+    checkpointStream(mapWithStateDStream, options);\n     // flatmap and report read elements. Use the inputDStream's id to tie between the reported\n     // info and the inputDStream it originated from.\n     int id = inputDStream.inputDStream().id();\n@@ -97,9 +103,8 @@\n   }\n \n   private static void checkpointStream(JavaDStream<?> dStream,\n-                                       SparkRuntimeContext rc) {\n-    long checkpointDurationMillis = rc.getPipelineOptions().as(SparkPipelineOptions.class)\n-        .getCheckpointDurationMillis();\n+                                       SparkPipelineOptions options) {\n+    long checkpointDurationMillis = options.getCheckpointDurationMillis();\n     if (checkpointDurationMillis > 0) {\n       dStream.checkpoint(new Duration(checkpointDurationMillis));\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/SparkUnboundedSource.java",
                "sha": "394b02373436a44300df2ec4a19d4d7885c02cb4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/hadoop/HadoopIO.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/io/hadoop/HadoopIO.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/io/hadoop/HadoopIO.java",
                "patch": "@@ -94,7 +94,7 @@ public String getFilepattern() {\n       }\n \n       @Override\n-      public PCollection<KV<K, V>> apply(PBegin input) {\n+      public PCollection<KV<K, V>> expand(PBegin input) {\n         return PCollection.createPrimitiveOutputInternal(input.getPipeline(),\n             WindowingStrategy.globalDefault(), PCollection.IsBounded.BOUNDED);\n       }\n@@ -197,7 +197,7 @@ public String getFilenameSuffix() {\n       }\n \n       @Override\n-      public PDone apply(PCollection<KV<K, V>> input) {\n+      public PDone expand(PCollection<KV<K, V>> input) {\n         checkNotNull(\n             filenamePrefix, \"need to set the filename prefix of an HadoopIO.Write transform\");\n         checkNotNull(formatClass, \"need to set the format class of an HadoopIO.Write transform\");",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/io/hadoop/HadoopIO.java",
                "sha": "f2457ced1e08a2391c7193ce8a63a9ba5cfd2b2f",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/DoFnFunction.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/DoFnFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 53,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/DoFnFunction.java",
                "patch": "@@ -18,14 +18,18 @@\n \n package org.apache.beam.runners.spark.translation;\n \n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.runners.core.DoFnRunner;\n+import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n+import org.apache.beam.runners.spark.aggregators.SparkAggregators;\n import org.apache.beam.runners.spark.util.BroadcastHelper;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.runners.spark.util.SparkSideInputReader;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n@@ -37,80 +41,80 @@\n /**\n  * Beam's Do functions correspond to Spark's FlatMap functions.\n  *\n- * @param <InputT> Input element type.\n+ * @param <InputT>  Input element type.\n  * @param <OutputT> Output element type.\n  */\n public class DoFnFunction<InputT, OutputT>\n     implements FlatMapFunction<Iterator<WindowedValue<InputT>>, WindowedValue<OutputT>> {\n-  private final Accumulator<NamedAggregators> accum;\n-  private final OldDoFn<InputT, OutputT> mFunction;\n-  private final SparkRuntimeContext mRuntimeContext;\n-  private final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> mSideInputs;\n-  private final WindowFn<Object, ?> windowFn;\n+\n+  private final Accumulator<NamedAggregators> accumulator;\n+  private final DoFn<InputT, OutputT> doFn;\n+  private final SparkRuntimeContext runtimeContext;\n+  private final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs;\n+  private final WindowingStrategy<?, ?> windowingStrategy;\n \n   /**\n-   * @param accum             The Spark Accumulator that handles the Beam Aggregators.\n-   * @param fn                DoFunction to be wrapped.\n-   * @param runtime           Runtime to apply function in.\n-   * @param sideInputs        Side inputs used in DoFunction.\n-   * @param windowFn          Input {@link WindowFn}.\n+   * @param accumulator       The Spark {@link Accumulator} that backs the Beam Aggregators.\n+   * @param doFn              The {@link DoFn} to be wrapped.\n+   * @param runtimeContext    The {@link SparkRuntimeContext}.\n+   * @param sideInputs        Side inputs used in this {@link DoFn}.\n+   * @param windowingStrategy Input {@link WindowingStrategy}.\n    */\n-  public DoFnFunction(Accumulator<NamedAggregators> accum,\n-                      OldDoFn<InputT, OutputT> fn,\n-                      SparkRuntimeContext runtime,\n-                      Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs,\n-                      WindowFn<Object, ?> windowFn) {\n-    this.accum = accum;\n-    this.mFunction = fn;\n-    this.mRuntimeContext = runtime;\n-    this.mSideInputs = sideInputs;\n-    this.windowFn = windowFn;\n+  public DoFnFunction(\n+      Accumulator<NamedAggregators> accumulator,\n+      DoFn<InputT, OutputT> doFn,\n+      SparkRuntimeContext runtimeContext,\n+      Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs,\n+      WindowingStrategy<?, ?> windowingStrategy) {\n+\n+    this.accumulator = accumulator;\n+    this.doFn = doFn;\n+    this.runtimeContext = runtimeContext;\n+    this.sideInputs = sideInputs;\n+    this.windowingStrategy = windowingStrategy;\n   }\n \n \n   @Override\n-  public Iterable<WindowedValue<OutputT>> call(Iterator<WindowedValue<InputT>> iter) throws\n-      Exception {\n-    return new ProcCtxt(mFunction, mRuntimeContext, mSideInputs, windowFn)\n-        .callWithCtxt(iter);\n+  public Iterable<WindowedValue<OutputT>> call(\n+      Iterator<WindowedValue<InputT>> iter) throws Exception {\n+\n+    DoFnOutputManager outputManager = new DoFnOutputManager();\n+    DoFnRunner<InputT, OutputT> doFnRunner =\n+        DoFnRunners.createDefault(\n+            runtimeContext.getPipelineOptions(),\n+            doFn,\n+            new SparkSideInputReader(sideInputs),\n+            outputManager,\n+            new TupleTag<OutputT>() {},\n+            Collections.<TupleTag<?>>emptyList(),\n+            new SparkProcessContext.NoOpStepContext(),\n+            new SparkAggregators.Factory(runtimeContext, accumulator),\n+            windowingStrategy\n+        );\n+\n+    return new SparkProcessContext<>(doFn, doFnRunner, outputManager).processPartition(iter);\n   }\n \n-  private class ProcCtxt extends SparkProcessContext<InputT, OutputT, WindowedValue<OutputT>> {\n+  private class DoFnOutputManager\n+      implements SparkProcessContext.SparkOutputManager<WindowedValue<OutputT>> {\n \n     private final List<WindowedValue<OutputT>> outputs = new LinkedList<>();\n \n-    ProcCtxt(OldDoFn<InputT, OutputT> fn,\n-             SparkRuntimeContext runtimeContext,\n-             Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs,\n-             WindowFn<Object, ?> windowFn) {\n-      super(fn, runtimeContext, sideInputs, windowFn);\n-    }\n-\n     @Override\n-    protected synchronized void outputWindowedValue(WindowedValue<OutputT> o) {\n-      outputs.add(o);\n-    }\n-\n-    @Override\n-    protected <T> void sideOutputWindowedValue(TupleTag<T> tag, WindowedValue<T> output) {\n-      throw new UnsupportedOperationException(\n-          \"sideOutput is an unsupported operation for doFunctions, use a \"\n-              + \"MultiDoFunction instead.\");\n-    }\n-\n-    @Override\n-    public Accumulator<NamedAggregators> getAccumulator() {\n-      return accum;\n+    public void clear() {\n+      outputs.clear();\n     }\n \n     @Override\n-    protected void clearOutput() {\n-      outputs.clear();\n+    public Iterator<WindowedValue<OutputT>> iterator() {\n+      return outputs.iterator();\n     }\n \n     @Override\n-    protected Iterator<WindowedValue<OutputT>> getOutputIterator() {\n-      return outputs.iterator();\n+    @SuppressWarnings(\"unchecked\")\n+    public synchronized <T> void output(TupleTag<T> tag, WindowedValue<T> output) {\n+      outputs.add((WindowedValue<OutputT>) output);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/DoFnFunction.java",
                "sha": "6a641b5c87f4a290d6a72ad3a3c6c510768b867f",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java",
                "changes": 131,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 111,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java",
                "patch": "@@ -21,21 +21,14 @@\n import static com.google.common.base.Preconditions.checkArgument;\n \n import com.google.common.collect.Iterables;\n-import java.io.IOException;\n import java.util.LinkedHashMap;\n import java.util.LinkedHashSet;\n import java.util.Map;\n import java.util.Set;\n-import org.apache.beam.runners.spark.EvaluationResult;\n import org.apache.beam.runners.spark.SparkPipelineOptions;\n-import org.apache.beam.runners.spark.aggregators.AccumulatorSingleton;\n import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n-import org.apache.beam.sdk.AggregatorRetrievalException;\n-import org.apache.beam.sdk.AggregatorValues;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.Coder;\n-import org.apache.beam.sdk.metrics.MetricResults;\n-import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.AppliedPTransform;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -46,15 +39,13 @@\n import org.apache.beam.sdk.values.PValue;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.streaming.StreamingContextState;\n import org.apache.spark.streaming.api.java.JavaStreamingContext;\n-import org.joda.time.Duration;\n-\n \n /**\n- * Evaluation context allows us to define how pipeline instructions.\n+ * The EvaluationContext allows us to define pipeline instructions and translate between\n+ * {@code PObject<T>}s or {@code PCollection<T>}s and Ts or DStreams/RDDs of Ts.\n  */\n-public class EvaluationContext implements EvaluationResult {\n+public class EvaluationContext {\n   private final JavaSparkContext jsc;\n   private JavaStreamingContext jssc;\n   private final SparkRuntimeContext runtime;\n@@ -66,24 +57,19 @@\n   private final Map<PValue, Object> pobjects = new LinkedHashMap<>();\n   private final Map<PValue, Iterable<? extends WindowedValue<?>>> pview = new LinkedHashMap<>();\n   private AppliedPTransform<?, ?, ?> currentTransform;\n-  private State state;\n \n   public EvaluationContext(JavaSparkContext jsc, Pipeline pipeline) {\n     this.jsc = jsc;\n     this.pipeline = pipeline;\n-    this.runtime = new SparkRuntimeContext(pipeline, jsc);\n-    // A batch pipeline is blocking by nature\n-    this.state = State.DONE;\n+    this.runtime = new SparkRuntimeContext(pipeline);\n   }\n \n-  public EvaluationContext(JavaSparkContext jsc, Pipeline pipeline,\n-                           JavaStreamingContext jssc) {\n+  public EvaluationContext(JavaSparkContext jsc, Pipeline pipeline, JavaStreamingContext jssc) {\n     this(jsc, pipeline);\n     this.jssc = jssc;\n-    this.state = State.RUNNING;\n   }\n \n-  JavaSparkContext getSparkContext() {\n+  public JavaSparkContext getSparkContext() {\n     return jsc;\n   }\n \n@@ -179,8 +165,14 @@ public void computeOutputs() {\n     }\n   }\n \n+  /**\n+   * Retrieve an object of Type T associated with the PValue passed in.\n+   *\n+   * @param value PValue to retrieve associated data for.\n+   * @param <T>  Type of object to return.\n+   * @return Native object.\n+   */\n   @SuppressWarnings(\"unchecked\")\n-  @Override\n   public <T> T get(PValue value) {\n     if (pobjects.containsKey(value)) {\n       T result = (T) pobjects.get(value);\n@@ -195,23 +187,13 @@ public void computeOutputs() {\n     throw new IllegalStateException(\"Cannot resolve un-known PObject: \" + value);\n   }\n \n-  @Override\n-  public <T> T getAggregatorValue(String named, Class<T> resultType) {\n-    return runtime.getAggregatorValue(AccumulatorSingleton.getInstance(jsc), named, resultType);\n-  }\n-\n-  @Override\n-  public <T> AggregatorValues<T> getAggregatorValues(Aggregator<?, T> aggregator)\n-      throws AggregatorRetrievalException {\n-    return runtime.getAggregatorValues(AccumulatorSingleton.getInstance(jsc), aggregator);\n-  }\n-\n-  @Override\n-  public MetricResults metrics() {\n-    throw new UnsupportedOperationException(\"The SparkRunner does not currently support metrics.\");\n-  }\n-\n-  @Override\n+  /**\n+   * Retrieves an iterable of results associated with the PCollection passed in.\n+   *\n+   * @param pcollection Collection we wish to translate.\n+   * @param <T>         Type of elements contained in collection.\n+   * @return Natively types result associated with collection.\n+   */\n   public <T> Iterable<T> get(PCollection<T> pcollection) {\n     @SuppressWarnings(\"unchecked\")\n     BoundedDataset<T> boundedDataset = (BoundedDataset<T>) datasets.get(pcollection);\n@@ -225,79 +207,6 @@ public MetricResults metrics() {\n     return boundedDataset.getValues(pcollection);\n   }\n \n-  @Override\n-  public void close(boolean gracefully) {\n-    // Stopping streaming job if running\n-    if (isStreamingPipeline() && !state.isTerminal()) {\n-      try {\n-        cancel(gracefully);\n-      } catch (IOException e) {\n-        throw new RuntimeException(\"Failed to cancel streaming job\", e);\n-      }\n-    }\n-    SparkContextFactory.stopSparkContext(jsc);\n-  }\n-\n-  @Override\n-  public State getState() {\n-    return state;\n-  }\n-\n-  @Override\n-  public State cancel() throws IOException {\n-    return cancel(true);\n-  }\n-\n-  private State cancel(boolean gracefully) throws IOException {\n-    if (isStreamingPipeline()) {\n-      if (!state.isTerminal()) {\n-        jssc.stop(false, gracefully);\n-        state = State.CANCELLED;\n-      }\n-      return state;\n-    } else {\n-      // Batch is currently blocking so\n-      // there is no way to cancel a batch job\n-      // will be handled at BEAM-1000\n-      throw new UnsupportedOperationException(\n-          \"Spark runner EvaluationContext does not support cancel.\");\n-    }\n-  }\n-\n-  @Override\n-  public State waitUntilFinish() {\n-    return waitUntilFinish(Duration.ZERO);\n-  }\n-\n-  @Override\n-  public State waitUntilFinish(Duration duration) {\n-    if (isStreamingPipeline()) {\n-      // According to PipelineResult: Provide a value less than 1 ms for an infinite wait\n-      if (duration.getMillis() < 1L) {\n-        jssc.awaitTermination();\n-        state = State.DONE;\n-      } else {\n-        jssc.awaitTermination(duration.getMillis());\n-        // According to PipelineResult: The final state of the pipeline or null on timeout\n-        if (jssc.getState().equals(StreamingContextState.STOPPED)) {\n-          state = State.DONE;\n-        } else {\n-          return null;\n-        }\n-      }\n-      return state;\n-    } else {\n-      // This is no-op, since Spark runner in batch is blocking.\n-      // It needs to be updated once SparkRunner supports non-blocking execution:\n-      // https://issues.apache.org/jira/browse/BEAM-595\n-      return State.DONE;\n-    }\n-  }\n-\n-  private boolean isStreamingPipeline() {\n-    return jssc != null;\n-  }\n-\n   private String storageLevel() {\n     return runtime.getPipelineOptions().as(SparkPipelineOptions.class).getStorageLevel();\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/EvaluationContext.java",
                "sha": "a412e313b8578f22c6029d6f3b7fe1c9f2db8ed7",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/GroupCombineFunctions.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/GroupCombineFunctions.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 14,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/GroupCombineFunctions.java",
                "patch": "@@ -18,11 +18,9 @@\n \n package org.apache.beam.runners.spark.translation;\n \n-\n import com.google.common.collect.Lists;\n import java.util.Collections;\n import java.util.Map;\n-import org.apache.beam.runners.core.GroupAlsoByWindowsViaOutputBufferDoFn;\n import org.apache.beam.runners.core.SystemReduceFn;\n import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n import org.apache.beam.runners.spark.coders.CoderHelpers;\n@@ -33,9 +31,7 @@\n import org.apache.beam.sdk.coders.IterableCoder;\n import org.apache.beam.sdk.coders.KvCoder;\n import org.apache.beam.sdk.transforms.CombineWithContext;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n@@ -59,7 +55,7 @@\n   /**\n    * Apply {@link org.apache.beam.sdk.transforms.GroupByKey} to a Spark RDD.\n    */\n-  public static <K, V,  W extends BoundedWindow> JavaRDD<WindowedValue<KV<K,\n+  public static <K, V, W extends BoundedWindow> JavaRDD<WindowedValue<KV<K,\n       Iterable<V>>>> groupByKey(JavaRDD<WindowedValue<KV<K, V>>> rdd,\n                                 Accumulator<NamedAggregators> accum,\n                                 KvCoder<K, V> coder,\n@@ -86,15 +82,14 @@\n             .map(WindowingHelpers.<KV<K, Iterable<WindowedValue<V>>>>windowFunction());\n \n     //--- now group also by window.\n-    @SuppressWarnings(\"unchecked\")\n-    WindowFn<Object, W> windowFn = (WindowFn<Object, W>) windowingStrategy.getWindowFn();\n-    // GroupAlsoByWindow current uses a dummy in-memory StateInternals\n-    OldDoFn<KV<K, Iterable<WindowedValue<V>>>, KV<K, Iterable<V>>> gabwDoFn =\n-        new GroupAlsoByWindowsViaOutputBufferDoFn<K, V, Iterable<V>, W>(\n-            windowingStrategy, new TranslationUtils.InMemoryStateInternalsFactory<K>(),\n-                SystemReduceFn.<K, V, W>buffering(valueCoder));\n-    return groupedByKey.mapPartitions(new DoFnFunction<>(accum, gabwDoFn, runtimeContext, null,\n-        windowFn));\n+    // GroupAlsoByWindow currently uses a dummy in-memory StateInternals\n+    return groupedByKey.flatMap(\n+        new SparkGroupAlsoByWindowFn<>(\n+            windowingStrategy,\n+            new TranslationUtils.InMemoryStateInternalsFactory<K>(),\n+            SystemReduceFn.<K, V, W>buffering(valueCoder),\n+            runtimeContext,\n+            accum));\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/GroupCombineFunctions.java",
                "sha": "4875b0cd30c94631683a5f4dd15f520ee01e4b22",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/MultiDoFnFunction.java",
                "changes": 135,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/MultiDoFnFunction.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 64,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/MultiDoFnFunction.java",
                "patch": "@@ -22,20 +22,26 @@\n import com.google.common.collect.Iterators;\n import com.google.common.collect.LinkedListMultimap;\n import com.google.common.collect.Multimap;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.Map;\n+import org.apache.beam.runners.core.DoFnRunner;\n+import org.apache.beam.runners.core.DoFnRunners;\n import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n+import org.apache.beam.runners.spark.aggregators.SparkAggregators;\n import org.apache.beam.runners.spark.util.BroadcastHelper;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.runners.spark.util.SparkSideInputReader;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.TupleTag;\n import org.apache.spark.Accumulator;\n import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n import scala.Tuple2;\n \n+\n /**\n  * DoFunctions ignore side outputs. MultiDoFunctions deal with side outputs by enriching the\n  * underlying data with multiple TupleTags.\n@@ -44,89 +50,90 @@\n  * @param <OutputT> Output type for DoFunction.\n  */\n public class MultiDoFnFunction<InputT, OutputT>\n-    implements PairFlatMapFunction<Iterator<WindowedValue<InputT>>, TupleTag<?>,\n-        WindowedValue<?>> {\n-  private final Accumulator<NamedAggregators> accum;\n-  private final OldDoFn<InputT, OutputT> mFunction;\n-  private final SparkRuntimeContext mRuntimeContext;\n-  private final TupleTag<OutputT> mMainOutputTag;\n-  private final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> mSideInputs;\n-  private final WindowFn<Object, ?> windowFn;\n+    implements PairFlatMapFunction<Iterator<WindowedValue<InputT>>, TupleTag<?>, WindowedValue<?>> {\n+\n+  private final Accumulator<NamedAggregators> accumulator;\n+  private final DoFn<InputT, OutputT> doFn;\n+  private final SparkRuntimeContext runtimeContext;\n+  private final TupleTag<OutputT> mainOutputTag;\n+  private final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs;\n+  private final WindowingStrategy<?, ?> windowingStrategy;\n \n   /**\n-   * @param accum             The Spark Accumulator that handles the Beam Aggregators.\n-   * @param fn                DoFunction to be wrapped.\n-   * @param runtimeContext    Runtime to apply function in.\n+   * @param accumulator       The Spark {@link Accumulator} that backs the Beam Aggregators.\n+   * @param doFn              The {@link DoFn} to be wrapped.\n+   * @param runtimeContext    The {@link SparkRuntimeContext}.\n    * @param mainOutputTag     The main output {@link TupleTag}.\n-   * @param sideInputs        Side inputs used in DoFunction.\n-   * @param windowFn          Input {@link WindowFn}.\n+   * @param sideInputs        Side inputs used in this {@link DoFn}.\n+   * @param windowingStrategy Input {@link WindowingStrategy}.\n    */\n-  public MultiDoFnFunction(Accumulator<NamedAggregators> accum,\n-                           OldDoFn<InputT, OutputT> fn,\n-                           SparkRuntimeContext runtimeContext,\n-                           TupleTag<OutputT> mainOutputTag,\n-                           Map<TupleTag<?>, KV<WindowingStrategy<?, ?>,\n-                               BroadcastHelper<?>>> sideInputs,\n-                           WindowFn<Object, ?> windowFn) {\n-    this.accum = accum;\n-    this.mFunction = fn;\n-    this.mRuntimeContext = runtimeContext;\n-    this.mMainOutputTag = mainOutputTag;\n-    this.mSideInputs = sideInputs;\n-    this.windowFn = windowFn;\n+  public MultiDoFnFunction(\n+      Accumulator<NamedAggregators> accumulator,\n+      DoFn<InputT, OutputT> doFn,\n+      SparkRuntimeContext runtimeContext,\n+      TupleTag<OutputT> mainOutputTag,\n+      Map<TupleTag<?>, KV<WindowingStrategy<?, ?>,\n+      BroadcastHelper<?>>> sideInputs,\n+      WindowingStrategy<?, ?> windowingStrategy) {\n+\n+    this.accumulator = accumulator;\n+    this.doFn = doFn;\n+    this.runtimeContext = runtimeContext;\n+    this.mainOutputTag = mainOutputTag;\n+    this.sideInputs = sideInputs;\n+    this.windowingStrategy = windowingStrategy;\n   }\n \n   @Override\n-  public Iterable<Tuple2<TupleTag<?>, WindowedValue<?>>>\n-      call(Iterator<WindowedValue<InputT>> iter) throws Exception {\n-    return new ProcCtxt(mFunction, mRuntimeContext, mSideInputs, windowFn)\n-        .callWithCtxt(iter);\n-  }\n+  public Iterable<Tuple2<TupleTag<?>, WindowedValue<?>>> call(\n+      Iterator<WindowedValue<InputT>> iter) throws Exception {\n \n-  private class ProcCtxt\n-      extends SparkProcessContext<InputT, OutputT, Tuple2<TupleTag<?>, WindowedValue<?>>> {\n+    DoFnOutputManager outputManager = new DoFnOutputManager();\n+    DoFnRunner<InputT, OutputT> doFnRunner =\n+        DoFnRunners.createDefault(\n+            runtimeContext.getPipelineOptions(),\n+            doFn,\n+            new SparkSideInputReader(sideInputs),\n+            outputManager,\n+            mainOutputTag,\n+            Collections.<TupleTag<?>>emptyList(),\n+            new SparkProcessContext.NoOpStepContext(),\n+            new SparkAggregators.Factory(runtimeContext, accumulator),\n+            windowingStrategy\n+        );\n \n-    private final Multimap<TupleTag<?>, WindowedValue<?>> outputs = LinkedListMultimap.create();\n+    return new SparkProcessContext<>(doFn, doFnRunner, outputManager).processPartition(iter);\n+  }\n \n-    ProcCtxt(OldDoFn<InputT, OutputT> fn,\n-             SparkRuntimeContext runtimeContext,\n-             Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs,\n-             WindowFn<Object, ?> windowFn) {\n-      super(fn, runtimeContext, sideInputs, windowFn);\n-    }\n+  private class DoFnOutputManager\n+      implements SparkProcessContext.SparkOutputManager<Tuple2<TupleTag<?>, WindowedValue<?>>> {\n \n-    @Override\n-    protected synchronized void outputWindowedValue(WindowedValue<OutputT> o) {\n-      outputs.put(mMainOutputTag, o);\n-    }\n+    private final Multimap<TupleTag<?>, WindowedValue<?>> outputs = LinkedListMultimap.create();;\n \n     @Override\n-    protected <T> void sideOutputWindowedValue(TupleTag<T> tag, WindowedValue<T> output) {\n-      outputs.put(tag, output);\n-    }\n-\n-    @Override\n-    public Accumulator<NamedAggregators> getAccumulator() {\n-      return accum;\n+    public void clear() {\n+      outputs.clear();\n     }\n \n     @Override\n-    protected void clearOutput() {\n-      outputs.clear();\n+    public Iterator<Tuple2<TupleTag<?>, WindowedValue<?>>> iterator() {\n+      Iterator<Map.Entry<TupleTag<?>, WindowedValue<?>>> entryIter = outputs.entries().iterator();\n+      return Iterators.transform(entryIter, this.<TupleTag<?>, WindowedValue<?>>entryToTupleFn());\n     }\n \n-    @Override\n-    protected Iterator<Tuple2<TupleTag<?>, WindowedValue<?>>> getOutputIterator() {\n-      return Iterators.transform(outputs.entries().iterator(),\n-          new Function<Map.Entry<TupleTag<?>, WindowedValue<?>>,\n-              Tuple2<TupleTag<?>, WindowedValue<?>>>() {\n+    private <K, V> Function<Map.Entry<K, V>, Tuple2<K, V>> entryToTupleFn() {\n+      return new Function<Map.Entry<K, V>, Tuple2<K, V>>() {\n         @Override\n-        public Tuple2<TupleTag<?>, WindowedValue<?>> apply(Map.Entry<TupleTag<?>,\n-            WindowedValue<?>> input) {\n-          return new Tuple2<TupleTag<?>, WindowedValue<?>>(input.getKey(), input.getValue());\n+        public Tuple2<K, V> apply(Map.Entry<K, V> en) {\n+          return new Tuple2<>(en.getKey(), en.getValue());\n         }\n-      });\n+      };\n     }\n \n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public synchronized <T> void output(TupleTag<T> tag, WindowedValue<T> output) {\n+      outputs.put(tag, output);\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/MultiDoFnFunction.java",
                "sha": "8a5536905c96d428b8fb97e34040ec44935eabfd",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkAssignWindowFn.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkAssignWindowFn.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkAssignWindowFn.java",
                "patch": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.spark.translation;\n+\n+import com.google.common.collect.Iterables;\n+import java.util.Collection;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.spark.api.java.function.Function;\n+import org.joda.time.Instant;\n+\n+\n+/**\n+ * An implementation of {@link org.apache.beam.runners.core.AssignWindowsDoFn} for the Spark runner.\n+ */\n+public class SparkAssignWindowFn<T, W extends BoundedWindow>\n+    implements Function<WindowedValue<T>, WindowedValue<T>> {\n+\n+  private WindowFn<? super T, W> fn;\n+\n+  public SparkAssignWindowFn(WindowFn<? super T, W> fn) {\n+    this.fn = fn;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public WindowedValue<T> call(WindowedValue<T> windowedValue) throws Exception {\n+    final BoundedWindow boundedWindow = Iterables.getOnlyElement(windowedValue.getWindows());\n+    final T element = windowedValue.getValue();\n+    final Instant timestamp = windowedValue.getTimestamp();\n+    Collection<W> windows =\n+        ((WindowFn<T, W>) fn).assignWindows(\n+            ((WindowFn<T, W>) fn).new AssignContext() {\n+                @Override\n+                public T element() {\n+                  return element;\n+                }\n+\n+                @Override\n+                public Instant timestamp() {\n+                  return timestamp;\n+                }\n+\n+                @Override\n+                public BoundedWindow window() {\n+                  return boundedWindow;\n+                }\n+              });\n+    return WindowedValue.of(element, timestamp, windows, PaneInfo.NO_FIRING);\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkAssignWindowFn.java",
                "sha": "18a3dd89990c09a281801a47f6cd5950b81e42a6",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkContextFactory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkContextFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkContextFactory.java",
                "patch": "@@ -66,7 +66,7 @@ public static synchronized JavaSparkContext getSparkContext(SparkPipelineOptions\n     }\n   }\n \n-  static synchronized void stopSparkContext(JavaSparkContext context) {\n+  public static synchronized void stopSparkContext(JavaSparkContext context) {\n     if (!Boolean.getBoolean(TEST_REUSE_SPARK_CONTEXT)) {\n       context.stop();\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkContextFactory.java",
                "sha": "67839a80824366be130629bce5c91da75491167b",
                "status": "modified"
            },
            {
                "additions": 214,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkGroupAlsoByWindowFn.java",
                "changes": 214,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkGroupAlsoByWindowFn.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkGroupAlsoByWindowFn.java",
                "patch": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.spark.translation;\n+\n+import com.google.common.collect.Iterables;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import org.apache.beam.runners.core.GroupAlsoByWindowsDoFn;\n+import org.apache.beam.runners.core.OutputWindowedValue;\n+import org.apache.beam.runners.core.ReduceFnRunner;\n+import org.apache.beam.runners.core.SystemReduceFn;\n+import org.apache.beam.runners.core.triggers.ExecutableTriggerStateMachine;\n+import org.apache.beam.runners.core.triggers.TriggerStateMachines;\n+import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n+import org.apache.beam.sdk.transforms.Aggregator;\n+import org.apache.beam.sdk.transforms.Sum;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.util.SideInputReader;\n+import org.apache.beam.sdk.util.TimerInternals;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.util.state.InMemoryTimerInternals;\n+import org.apache.beam.sdk.util.state.StateInternals;\n+import org.apache.beam.sdk.util.state.StateInternalsFactory;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollectionView;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.apache.spark.Accumulator;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.joda.time.Instant;\n+\n+\n+\n+/**\n+ * An implementation of {@link org.apache.beam.runners.core.GroupAlsoByWindowsViaOutputBufferDoFn}\n+ * for the Spark runner.\n+ */\n+public class SparkGroupAlsoByWindowFn<K, InputT, W extends BoundedWindow>\n+    implements FlatMapFunction<WindowedValue<KV<K, Iterable<WindowedValue<InputT>>>>,\n+        WindowedValue<KV<K, Iterable<InputT>>>> {\n+\n+  private final WindowingStrategy<?, W> windowingStrategy;\n+  private final StateInternalsFactory<K> stateInternalsFactory;\n+  private final SystemReduceFn<K, InputT, Iterable<InputT>, Iterable<InputT>, W> reduceFn;\n+  private final SparkRuntimeContext runtimeContext;\n+  private final Aggregator<Long, Long> droppedDueToClosedWindow;\n+\n+\n+  public SparkGroupAlsoByWindowFn(\n+      WindowingStrategy<?, W> windowingStrategy,\n+      StateInternalsFactory<K> stateInternalsFactory,\n+      SystemReduceFn<K, InputT, Iterable<InputT>, Iterable<InputT>, W> reduceFn,\n+      SparkRuntimeContext runtimeContext,\n+      Accumulator<NamedAggregators> accumulator) {\n+    this.windowingStrategy = windowingStrategy;\n+    this.stateInternalsFactory = stateInternalsFactory;\n+    this.reduceFn = reduceFn;\n+    this.runtimeContext = runtimeContext;\n+\n+    droppedDueToClosedWindow = runtimeContext.createAggregator(\n+        accumulator,\n+        GroupAlsoByWindowsDoFn.DROPPED_DUE_TO_CLOSED_WINDOW_COUNTER,\n+        new Sum.SumLongFn());\n+  }\n+\n+  @Override\n+  public Iterable<WindowedValue<KV<K, Iterable<InputT>>>> call(\n+      WindowedValue<KV<K, Iterable<WindowedValue<InputT>>>> windowedValue) throws Exception {\n+    K key = windowedValue.getValue().getKey();\n+    Iterable<WindowedValue<InputT>> inputs = windowedValue.getValue().getValue();\n+\n+    //------ based on GroupAlsoByWindowsViaOutputBufferDoFn ------//\n+\n+    // Used with Batch, we know that all the data is available for this key. We can't use the\n+    // timer manager from the context because it doesn't exist. So we create one and emulate the\n+    // watermark, knowing that we have all data and it is in timestamp order.\n+    InMemoryTimerInternals timerInternals = new InMemoryTimerInternals();\n+    timerInternals.advanceProcessingTime(Instant.now());\n+    timerInternals.advanceSynchronizedProcessingTime(Instant.now());\n+    StateInternals<K> stateInternals = stateInternalsFactory.stateInternalsForKey(key);\n+    GABWOutputWindowedValue<K, InputT> outputter = new GABWOutputWindowedValue<>();\n+\n+    ReduceFnRunner<K, InputT, Iterable<InputT>, W> reduceFnRunner =\n+        new ReduceFnRunner<>(\n+            key,\n+            windowingStrategy,\n+            ExecutableTriggerStateMachine.create(\n+                TriggerStateMachines.stateMachineForTrigger(windowingStrategy.getTrigger())),\n+            stateInternals,\n+            timerInternals,\n+            outputter,\n+            new SideInputReader() {\n+                @Override\n+                public <T> T get(PCollectionView<T> view, BoundedWindow sideInputWindow) {\n+                  throw new UnsupportedOperationException(\n+                      \"GroupAlsoByWindow must not have side inputs\");\n+                }\n+\n+                @Override\n+                public <T> boolean contains(PCollectionView<T> view) {\n+                  throw new UnsupportedOperationException(\n+                      \"GroupAlsoByWindow must not have side inputs\");\n+                }\n+\n+                @Override\n+                public boolean isEmpty() {\n+                  throw new UnsupportedOperationException(\n+                      \"GroupAlsoByWindow must not have side inputs\");\n+                }\n+              },\n+            droppedDueToClosedWindow,\n+            reduceFn,\n+            runtimeContext.getPipelineOptions());\n+\n+    Iterable<List<WindowedValue<InputT>>> chunks = Iterables.partition(inputs, 1000);\n+    for (Iterable<WindowedValue<InputT>> chunk : chunks) {\n+      // Process the chunk of elements.\n+      reduceFnRunner.processElements(chunk);\n+\n+      // Then, since elements are sorted by their timestamp, advance the input watermark\n+      // to the first element.\n+      timerInternals.advanceInputWatermark(chunk.iterator().next().getTimestamp());\n+      // Advance the processing times.\n+      timerInternals.advanceProcessingTime(Instant.now());\n+      timerInternals.advanceSynchronizedProcessingTime(Instant.now());\n+\n+      // Fire all the eligible timers.\n+      fireEligibleTimers(timerInternals, reduceFnRunner);\n+\n+      // Leave the output watermark undefined. Since there's no late data in batch mode\n+      // there's really no need to track it as we do for streaming.\n+    }\n+\n+    // Finish any pending windows by advancing the input watermark to infinity.\n+    timerInternals.advanceInputWatermark(BoundedWindow.TIMESTAMP_MAX_VALUE);\n+\n+    // Finally, advance the processing time to infinity to fire any timers.\n+    timerInternals.advanceProcessingTime(BoundedWindow.TIMESTAMP_MAX_VALUE);\n+    timerInternals.advanceSynchronizedProcessingTime(BoundedWindow.TIMESTAMP_MAX_VALUE);\n+\n+    fireEligibleTimers(timerInternals, reduceFnRunner);\n+\n+    reduceFnRunner.persist();\n+\n+    return outputter.getOutputs();\n+  }\n+\n+  private void fireEligibleTimers(InMemoryTimerInternals timerInternals,\n+      ReduceFnRunner<K, InputT, Iterable<InputT>, W> reduceFnRunner) throws Exception {\n+    List<TimerInternals.TimerData> timers = new ArrayList<>();\n+    while (true) {\n+        TimerInternals.TimerData timer;\n+        while ((timer = timerInternals.removeNextEventTimer()) != null) {\n+          timers.add(timer);\n+        }\n+        while ((timer = timerInternals.removeNextProcessingTimer()) != null) {\n+          timers.add(timer);\n+        }\n+        while ((timer = timerInternals.removeNextSynchronizedProcessingTimer()) != null) {\n+          timers.add(timer);\n+        }\n+        if (timers.isEmpty()) {\n+          break;\n+        }\n+        reduceFnRunner.onTimers(timers);\n+        timers.clear();\n+    }\n+  }\n+\n+  private static class GABWOutputWindowedValue<K, V>\n+      implements OutputWindowedValue<KV<K, Iterable<V>>> {\n+    private final List<WindowedValue<KV<K, Iterable<V>>>> outputs = new ArrayList<>();\n+\n+    @Override\n+    public void outputWindowedValue(\n+        KV<K, Iterable<V>> output,\n+        Instant timestamp,\n+        Collection<? extends BoundedWindow> windows,\n+        PaneInfo pane) {\n+      outputs.add(WindowedValue.of(output, timestamp, windows, pane));\n+    }\n+\n+    @Override\n+    public <SideOutputT> void sideOutputWindowedValue(\n+        TupleTag<SideOutputT> tag,\n+        SideOutputT output,\n+        Instant timestamp,\n+        Collection<? extends BoundedWindow> windows, PaneInfo pane) {\n+      throw new UnsupportedOperationException(\"GroupAlsoByWindow should not use side outputs.\");\n+    }\n+\n+    Iterable<WindowedValue<KV<K, Iterable<V>>>> getOutputs() {\n+      return outputs;\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkGroupAlsoByWindowFn.java",
                "sha": "87d3f505903991644aef1ce7558518fded14e175",
                "status": "added"
            },
            {
                "additions": 84,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkProcessContext.java",
                "changes": 385,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkProcessContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 301,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkProcessContext.java",
                "patch": "@@ -18,304 +18,137 @@\n \n package org.apache.beam.runners.spark.translation;\n \n-import static com.google.common.base.Preconditions.checkState;\n-\n import com.google.common.collect.AbstractIterator;\n-import com.google.common.collect.Iterables;\n import com.google.common.collect.Lists;\n-import java.util.Collection;\n+import java.io.IOException;\n import java.util.Iterator;\n-import java.util.Map;\n-import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n-import org.apache.beam.runners.spark.util.BroadcastHelper;\n-import org.apache.beam.runners.spark.util.SparkSideInputReader;\n-import org.apache.beam.sdk.options.PipelineOptions;\n-import org.apache.beam.sdk.transforms.Aggregator;\n-import org.apache.beam.sdk.transforms.Combine;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n-import org.apache.beam.sdk.transforms.OldDoFn.RequiresWindowAccess;\n+import org.apache.beam.runners.core.DoFnRunner;\n+import org.apache.beam.runners.core.DoFnRunners.OutputManager;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.reflect.DoFnInvokers;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n-import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n-import org.apache.beam.sdk.util.SideInputReader;\n-import org.apache.beam.sdk.util.SystemDoFnInternal;\n+import org.apache.beam.sdk.util.ExecutionContext.StepContext;\n import org.apache.beam.sdk.util.TimerInternals;\n-import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n-import org.apache.beam.sdk.util.WindowingInternals;\n-import org.apache.beam.sdk.util.WindowingStrategy;\n-import org.apache.beam.sdk.util.state.InMemoryStateInternals;\n import org.apache.beam.sdk.util.state.StateInternals;\n-import org.apache.beam.sdk.values.KV;\n-import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n-import org.apache.spark.Accumulator;\n-import org.joda.time.Instant;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n \n \n /**\n- * Spark runner process context.\n+ * Spark runner process context processes Spark partitions using Beam's {@link DoFnRunner}.\n  */\n-public abstract class SparkProcessContext<InputT, OutputT, ValueT>\n-    extends OldDoFn<InputT, OutputT>.ProcessContext {\n-  private static final Logger LOG = LoggerFactory.getLogger(SparkProcessContext.class);\n+class SparkProcessContext<FnInputT, FnOutputT, OutputT> {\n \n-  private final OldDoFn<InputT, OutputT> fn;\n-  private final SparkRuntimeContext mRuntimeContext;\n-  private final SideInputReader sideInputReader;\n-  private final WindowFn<Object, ?> windowFn;\n+  private final DoFn<FnInputT, FnOutputT> doFn;\n+  private final DoFnRunner<FnInputT, FnOutputT> doFnRunner;\n+  private final SparkOutputManager<OutputT> outputManager;\n \n-  WindowedValue<InputT> windowedValue;\n+  SparkProcessContext(\n+      DoFn<FnInputT, FnOutputT> doFn,\n+      DoFnRunner<FnInputT, FnOutputT> doFnRunner,\n+      SparkOutputManager<OutputT> outputManager) {\n \n-  SparkProcessContext(OldDoFn<InputT, OutputT> fn,\n-                      SparkRuntimeContext runtime,\n-                      Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs,\n-                      WindowFn<Object, ?> windowFn) {\n-    fn.super();\n-    this.fn = fn;\n-    this.mRuntimeContext = runtime;\n-    this.sideInputReader = new SparkSideInputReader(sideInputs);\n-    this.windowFn = windowFn;\n+    this.doFn = doFn;\n+    this.doFnRunner = doFnRunner;\n+    this.outputManager = outputManager;\n   }\n \n-  void setup() {\n-    setupDelegateAggregators();\n-  }\n+  Iterable<OutputT> processPartition(\n+      Iterator<WindowedValue<FnInputT>> partition) throws Exception {\n \n-  Iterable<ValueT> callWithCtxt(Iterator<WindowedValue<InputT>> iter) throws Exception{\n-    this.setup();\n-    // skip if bundle is empty.\n-    if (!iter.hasNext()) {\n+    // setup DoFn.\n+    DoFnInvokers.invokerFor(doFn).invokeSetup();\n+\n+    // skip if partition is empty.\n+    if (!partition.hasNext()) {\n       return Lists.newArrayList();\n     }\n-    try {\n-      fn.setup();\n-      fn.startBundle(this);\n-      return this.getOutputIterable(iter, fn);\n-    } catch (Exception e) {\n-      try {\n-        // this teardown handles exceptions encountered in setup() and startBundle(). teardown\n-        // after execution or due to exceptions in process element is called in the iterator\n-        // produced by ctxt.getOutputIterable returned from this method.\n-        fn.teardown();\n-      } catch (Exception teardownException) {\n-        LOG.error(\n-            \"Suppressing exception while tearing down Function {}\", fn, teardownException);\n-        e.addSuppressed(teardownException);\n-      }\n-      throw wrapUserCodeException(e);\n-    }\n-  }\n-\n-  @Override\n-  public PipelineOptions getPipelineOptions() {\n-    return mRuntimeContext.getPipelineOptions();\n+    // call startBundle() before beginning to process the partition.\n+    doFnRunner.startBundle();\n+    // process the partition; finishBundle() is called from within the output iterator.\n+    return this.getOutputIterable(partition, doFnRunner);\n   }\n \n-  @Override\n-  public <T> T sideInput(PCollectionView<T> view) {\n-    //validate element window.\n-    final Collection<? extends BoundedWindow> elementWindows = windowedValue.getWindows();\n-    checkState(elementWindows.size() == 1, \"sideInput can only be called when the main \"\n-        + \"input element is in exactly one window\");\n-    return sideInputReader.get(view, elementWindows.iterator().next());\n+  private void clearOutput() {\n+    outputManager.clear();\n   }\n \n-  @Override\n-  public <AggregatorInputT, AggregatorOutputT>\n-  Aggregator<AggregatorInputT, AggregatorOutputT> createAggregatorInternal(\n-      String named,\n-      Combine.CombineFn<AggregatorInputT, ?, AggregatorOutputT> combineFn) {\n-    return mRuntimeContext.createAggregator(getAccumulator(), named, combineFn);\n+  private Iterator<OutputT> getOutputIterator() {\n+    return outputManager.iterator();\n   }\n \n-  public abstract Accumulator<NamedAggregators> getAccumulator();\n+  private Iterable<OutputT> getOutputIterable(\n+      final Iterator<WindowedValue<FnInputT>> iter,\n+      final DoFnRunner<FnInputT, FnOutputT> doFnRunner) {\n \n-  @Override\n-  public InputT element() {\n-    return windowedValue.getValue();\n+    return new Iterable<OutputT>() {\n+      @Override\n+      public Iterator<OutputT> iterator() {\n+        return new ProcCtxtIterator(iter, doFnRunner);\n+      }\n+    };\n   }\n \n-  @Override\n-  public void output(OutputT output) {\n-    outputWithTimestamp(output, windowedValue != null ? windowedValue.getTimestamp() : null);\n-  }\n+  interface SparkOutputManager<T> extends OutputManager, Iterable<T> {\n \n-  @Override\n-  public void outputWithTimestamp(OutputT output, Instant timestamp) {\n-    if (windowedValue == null) {\n-      // this is start/finishBundle.\n-      outputWindowedValue(noElementWindowedValue(output, timestamp, windowFn));\n-    } else {\n-      outputWindowedValue(WindowedValue.of(output, timestamp, windowedValue.getWindows(),\n-          windowedValue.getPane()));\n-    }\n-  }\n+    void clear();\n \n-  @Override\n-  public <T> void sideOutput(TupleTag<T> tag, T output) {\n-    sideOutputWithTimestamp(\n-        tag, output, windowedValue != null ? windowedValue.getTimestamp() : null);\n   }\n \n-  @Override\n-  public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n-    if (windowedValue == null) {\n-      // this is start/finishBundle.\n-      sideOutputWindowedValue(tag, noElementWindowedValue(output, timestamp, windowFn));\n-    } else {\n-      sideOutputWindowedValue(tag, WindowedValue.of(output, timestamp, windowedValue.getWindows(),\n-          windowedValue.getPane()));\n+  static class NoOpStepContext implements StepContext {\n+    @Override\n+    public String getStepName() {\n+      return null;\n     }\n-  }\n-\n-  protected abstract void outputWindowedValue(WindowedValue<OutputT> output);\n \n-  protected abstract <T> void sideOutputWindowedValue(TupleTag<T> tag, WindowedValue<T> output);\n+    @Override\n+    public String getTransformName() {\n+      return null;\n+    }\n \n-  static <T, W extends BoundedWindow> WindowedValue<T> noElementWindowedValue(\n-      final T output, final Instant timestamp, WindowFn<Object, W> windowFn) {\n-    WindowFn<Object, W>.AssignContext assignContext =\n-        windowFn.new AssignContext() {\n+    @Override\n+    public void noteOutput(WindowedValue<?> output) { }\n \n-          @Override\n-          public Object element() {\n-            return output;\n-          }\n+    @Override\n+    public void noteSideOutput(TupleTag<?> tag, WindowedValue<?> output) { }\n \n-          @Override\n-          public Instant timestamp() {\n-            if (timestamp != null) {\n-              return timestamp;\n-            }\n-            throw new UnsupportedOperationException(\n-                \"outputWithTimestamp was called with \" + \"null timestamp.\");\n-          }\n+    @Override\n+    public <T, W extends BoundedWindow> void writePCollectionViewData(\n+        TupleTag<?> tag,\n+        Iterable<WindowedValue<T>> data,\n+        Coder<Iterable<WindowedValue<T>>> dataCoder,\n+        W window,\n+        Coder<W> windowCoder) throws IOException { }\n \n-          @Override\n-          public BoundedWindow window() {\n-            throw new UnsupportedOperationException(\n-                \"Window not available for \" + \"start/finishBundle output.\");\n-          }\n-        };\n-    try {\n-      @SuppressWarnings(\"unchecked\")\n-      Collection<? extends BoundedWindow> windows = windowFn.assignWindows(assignContext);\n-      Instant outputTimestamp = timestamp != null ? timestamp : BoundedWindow.TIMESTAMP_MIN_VALUE;\n-      return WindowedValue.of(output, outputTimestamp, windows, PaneInfo.NO_FIRING);\n-    } catch (Exception e) {\n-      throw new RuntimeException(\"Failed to assign windows at start/finishBundle.\", e);\n+    @Override\n+    public StateInternals<?> stateInternals() {\n+      return null;\n     }\n-  }\n \n-  @Override\n-  public Instant timestamp() {\n-    return windowedValue.getTimestamp();\n-  }\n-\n-  @Override\n-  public BoundedWindow window() {\n-    if (!(fn instanceof OldDoFn.RequiresWindowAccess)) {\n-      throw new UnsupportedOperationException(\n-          \"window() is only available in the context of a OldDoFn marked as RequiresWindowAccess.\");\n+    @Override\n+    public TimerInternals timerInternals() {\n+      return null;\n     }\n-    return Iterables.getOnlyElement(windowedValue.getWindows());\n   }\n \n-  @Override\n-  public PaneInfo pane() {\n-    return windowedValue.getPane();\n-  }\n-\n-  @Override\n-  public WindowingInternals<InputT, OutputT> windowingInternals() {\n-    return new WindowingInternals<InputT, OutputT>() {\n-\n-      @Override\n-      public Collection<? extends BoundedWindow> windows() {\n-        return windowedValue.getWindows();\n-      }\n+  private class ProcCtxtIterator extends AbstractIterator<OutputT> {\n \n-      @Override\n-      public void outputWindowedValue(\n-          OutputT output,\n-          Instant timestamp,\n-          Collection<? extends BoundedWindow> windows,\n-          PaneInfo paneInfo) {\n-        SparkProcessContext.this.outputWindowedValue(\n-            WindowedValue.of(output, timestamp, windows, paneInfo));\n-      }\n-\n-      @Override\n-      public <SideOutputT> void sideOutputWindowedValue(\n-          TupleTag<SideOutputT> tag,\n-          SideOutputT output,\n-          Instant timestamp,\n-          Collection<? extends BoundedWindow> windows,\n-          PaneInfo paneInfo) {\n-        SparkProcessContext.this.sideOutputWindowedValue(\n-            tag, WindowedValue.of(output, timestamp, windows, paneInfo));\n-      }\n-\n-      @Override\n-      public StateInternals stateInternals() {\n-        //TODO: implement state internals.\n-        // This is a temporary placeholder to get the TfIdfTest\n-        // working for the initial Beam code drop.\n-        return InMemoryStateInternals.forKey(\"DUMMY\");\n-      }\n-\n-      @Override\n-      public TimerInternals timerInternals() {\n-        throw new UnsupportedOperationException(\n-            \"WindowingInternals#timerInternals() is not yet supported.\");\n-      }\n-\n-      @Override\n-      public PaneInfo pane() {\n-        return windowedValue.getPane();\n-      }\n-\n-      @Override\n-      public <T> T sideInput(PCollectionView<T> view, BoundedWindow sideInputWindow) {\n-        throw new UnsupportedOperationException(\n-            \"WindowingInternals#sideInput() is not yet supported.\");\n-      }\n-    };\n-  }\n-\n-  protected abstract void clearOutput();\n-\n-  protected abstract Iterator<ValueT> getOutputIterator();\n-\n-  protected Iterable<ValueT> getOutputIterable(final Iterator<WindowedValue<InputT>> iter,\n-                                               final OldDoFn<InputT, OutputT> doFn) {\n-    return new Iterable<ValueT>() {\n-      @Override\n-      public Iterator<ValueT> iterator() {\n-        return new ProcCtxtIterator(iter, doFn);\n-      }\n-    };\n-  }\n-\n-  private class ProcCtxtIterator extends AbstractIterator<ValueT> {\n-\n-    private final Iterator<WindowedValue<InputT>> inputIterator;\n-    private final OldDoFn<InputT, OutputT> doFn;\n-    private Iterator<ValueT> outputIterator;\n+    private final Iterator<WindowedValue<FnInputT>> inputIterator;\n+    private final DoFnRunner<FnInputT, FnOutputT> doFnRunner;\n+    private Iterator<OutputT> outputIterator;\n     private boolean calledFinish;\n \n-    ProcCtxtIterator(Iterator<WindowedValue<InputT>> iterator, OldDoFn<InputT, OutputT> doFn) {\n+    ProcCtxtIterator(\n+        Iterator<WindowedValue<FnInputT>> iterator,\n+        DoFnRunner<FnInputT, FnOutputT> doFnRunner) {\n       this.inputIterator = iterator;\n-      this.doFn = doFn;\n+      this.doFnRunner = doFnRunner;\n       this.outputIterator = getOutputIterator();\n     }\n \n     @Override\n-    protected ValueT computeNext() {\n+    protected OutputT computeNext() {\n       // Process each element from the (input) iterator, which produces, zero, one or more\n       // output elements (of type V) in the output iterator. Note that the output\n       // collection (and iterator) is reset between each call to processElement, so the\n@@ -327,72 +160,22 @@ protected ValueT computeNext() {\n         } else if (inputIterator.hasNext()) {\n           clearOutput();\n           // grab the next element and process it.\n-          windowedValue = inputIterator.next();\n-          if (windowedValue.getWindows().size() <= 1\n-              || (!RequiresWindowAccess.class.isAssignableFrom(doFn.getClass())\n-                  && sideInputReader.isEmpty())) {\n-            // if there's no reason to explode, process compacted.\n-            invokeProcessElement();\n-          } else {\n-            // explode and process the element in each of it's assigned windows.\n-            for (WindowedValue<InputT> wv: windowedValue.explodeWindows()) {\n-              windowedValue = wv;\n-              invokeProcessElement();\n-            }\n-          }\n+          doFnRunner.processElement(inputIterator.next());\n           outputIterator = getOutputIterator();\n         } else {\n           // no more input to consume, but finishBundle can produce more output\n           if (!calledFinish) {\n-            windowedValue = null; // clear the last element processed\n             clearOutput();\n-            try {\n-              calledFinish = true;\n-              doFn.finishBundle(SparkProcessContext.this);\n-            } catch (Exception e) {\n-              handleProcessingException(e);\n-              throw wrapUserCodeException(e);\n-            }\n+            calledFinish = true;\n+            doFnRunner.finishBundle();\n+            // teardown DoFn.\n+            DoFnInvokers.invokerFor(doFn).invokeTeardown();\n             outputIterator = getOutputIterator();\n             continue; // try to consume outputIterator from start of loop\n           }\n-          try {\n-            doFn.teardown();\n-          } catch (Exception e) {\n-            LOG.error(\n-                \"Suppressing teardown exception that occurred after processing entire input\", e);\n-          }\n           return endOfData();\n         }\n       }\n     }\n-\n-    private void invokeProcessElement() {\n-      try {\n-        doFn.processElement(SparkProcessContext.this);\n-      } catch (Exception e) {\n-        handleProcessingException(e);\n-        throw wrapUserCodeException(e);\n-      }\n-    }\n-\n-    private void handleProcessingException(Exception e) {\n-      try {\n-        doFn.teardown();\n-      } catch (Exception e1) {\n-        LOG.error(\"Exception while cleaning up DoFn\", e1);\n-        e.addSuppressed(e1);\n-      }\n-    }\n   }\n-\n-\n-  private RuntimeException wrapUserCodeException(Throwable t) {\n-    throw UserCodeException.wrapIf(!isSystemDoFn(), t);\n-  }\n-\n-  private boolean isSystemDoFn() {\n-    return fn.getClass().isAnnotationPresent(SystemDoFnInternal.class);\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkProcessContext.java",
                "sha": "3a31caed94bcc323ad8352791d0793fe78439437",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkRuntimeContext.java",
                "changes": 62,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkRuntimeContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 60,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkRuntimeContext.java",
                "patch": "@@ -20,17 +20,11 @@\n \n import com.fasterxml.jackson.core.JsonProcessingException;\n import com.fasterxml.jackson.databind.ObjectMapper;\n-import com.google.common.collect.ImmutableList;\n import java.io.IOException;\n import java.io.Serializable;\n-import java.util.Collection;\n import java.util.HashMap;\n import java.util.Map;\n-import org.apache.beam.runners.spark.SparkPipelineOptions;\n-import org.apache.beam.runners.spark.aggregators.AccumulatorSingleton;\n import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n-import org.apache.beam.runners.spark.aggregators.metrics.AggregatorMetricSource;\n-import org.apache.beam.sdk.AggregatorValues;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.coders.CannotProvideCoderException;\n import org.apache.beam.sdk.coders.Coder;\n@@ -43,10 +37,6 @@\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.values.TypeDescriptor;\n import org.apache.spark.Accumulator;\n-import org.apache.spark.SparkEnv$;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.metrics.MetricsSystem;\n-\n \n /**\n  * The SparkRuntimeContext allows us to define useful features on the client side before our\n@@ -61,12 +51,11 @@\n   private final Map<String, Aggregator<?, ?>> aggregators = new HashMap<>();\n   private transient CoderRegistry coderRegistry;\n \n-  SparkRuntimeContext(Pipeline pipeline, JavaSparkContext jsc) {\n+  SparkRuntimeContext(Pipeline pipeline) {\n     this.serializedPipelineOptions = serializePipelineOptions(pipeline.getOptions());\n-    registerMetrics(pipeline.getOptions().as(SparkPipelineOptions.class), jsc);\n   }\n \n-  private static String serializePipelineOptions(PipelineOptions pipelineOptions) {\n+  private String serializePipelineOptions(PipelineOptions pipelineOptions) {\n     try {\n       return new ObjectMapper().writeValueAsString(pipelineOptions);\n     } catch (JsonProcessingException e) {\n@@ -82,53 +71,6 @@ private static PipelineOptions deserializePipelineOptions(String serializedPipel\n     }\n   }\n \n-  private void registerMetrics(final SparkPipelineOptions opts, final JavaSparkContext jsc) {\n-    final Accumulator<NamedAggregators> accum = AccumulatorSingleton.getInstance(jsc);\n-    final NamedAggregators initialValue = accum.value();\n-\n-    if (opts.getEnableSparkMetricSinks()) {\n-      final MetricsSystem metricsSystem = SparkEnv$.MODULE$.get().metricsSystem();\n-      final AggregatorMetricSource aggregatorMetricSource =\n-          new AggregatorMetricSource(opts.getAppName(), initialValue);\n-      // re-register the metrics in case of context re-use\n-      metricsSystem.removeSource(aggregatorMetricSource);\n-      metricsSystem.registerSource(aggregatorMetricSource);\n-    }\n-  }\n-\n-  /**\n-   * Retrieves corresponding value of an aggregator.\n-   *\n-   * @param accum          The Spark Accumulator holding all Aggregators.\n-   * @param aggregatorName Name of the aggregator to retrieve the value of.\n-   * @param typeClass      Type class of value to be retrieved.\n-   * @param <T>            Type of object to be returned.\n-   * @return The value of the aggregator.\n-   */\n-  public <T> T getAggregatorValue(Accumulator<NamedAggregators> accum,\n-                                  String aggregatorName,\n-                                  Class<T> typeClass) {\n-    return accum.value().getValue(aggregatorName, typeClass);\n-  }\n-\n-  public <T> AggregatorValues<T> getAggregatorValues(Accumulator<NamedAggregators> accum,\n-                                                     Aggregator<?, T> aggregator) {\n-    @SuppressWarnings(\"unchecked\")\n-    Class<T> aggValueClass = (Class<T>) aggregator.getCombineFn().getOutputType().getRawType();\n-    final T aggregatorValue = getAggregatorValue(accum, aggregator.getName(), aggValueClass);\n-    return new AggregatorValues<T>() {\n-      @Override\n-      public Collection<T> getValues() {\n-        return ImmutableList.of(aggregatorValue);\n-      }\n-\n-      @Override\n-      public Map<String, T> getValuesAtSteps() {\n-        throw new UnsupportedOperationException(\"getValuesAtSteps is not supported.\");\n-      }\n-    };\n-  }\n-\n   public synchronized PipelineOptions getPipelineOptions() {\n     return deserializePipelineOptions(serializedPipelineOptions);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkRuntimeContext.java",
                "sha": "01b6b547a04bff6a17f8c43f21f8a306c9191648",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/StorageLevelPTransform.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/StorageLevelPTransform.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/StorageLevelPTransform.java",
                "patch": "@@ -29,7 +29,7 @@\n public final class StorageLevelPTransform extends PTransform<PCollection<?>, PCollection<String>> {\n \n   @Override\n-  public PCollection<String> apply(PCollection<?> input) {\n+  public PCollection<String> expand(PCollection<?> input) {\n     return PCollection.createPrimitiveOutputInternal(input.getPipeline(),\n         WindowingStrategy.globalDefault(),\n         PCollection.IsBounded.BOUNDED);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/StorageLevelPTransform.java",
                "sha": "30b51e66e4df96e76afeb17a825245b611d0fe95",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java",
                "changes": 67,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 48,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java",
                "patch": "@@ -23,6 +23,7 @@\n import static org.apache.beam.runners.spark.io.hadoop.ShardNameBuilder.getOutputFilePrefix;\n import static org.apache.beam.runners.spark.io.hadoop.ShardNameBuilder.getOutputFileTemplate;\n import static org.apache.beam.runners.spark.io.hadoop.ShardNameBuilder.replaceShardCount;\n+import static org.apache.beam.runners.spark.translation.TranslationUtils.rejectStateAndTimers;\n \n import com.google.common.collect.Maps;\n import java.io.IOException;\n@@ -31,10 +32,8 @@\n import org.apache.avro.mapred.AvroKey;\n import org.apache.avro.mapreduce.AvroJob;\n import org.apache.avro.mapreduce.AvroKeyInputFormat;\n-import org.apache.beam.runners.core.AssignWindowsDoFn;\n-import org.apache.beam.runners.spark.SparkRunner;\n-import org.apache.beam.runners.spark.aggregators.AccumulatorSingleton;\n import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n+import org.apache.beam.runners.spark.aggregators.SparkAggregators;\n import org.apache.beam.runners.spark.coders.CoderHelpers;\n import org.apache.beam.runners.spark.io.SourceRDD;\n import org.apache.beam.runners.spark.io.hadoop.HadoopIO;\n@@ -54,14 +53,11 @@\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.transforms.GroupByKey;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.View;\n-import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.Window;\n-import org.apache.beam.sdk.transforms.windowing.WindowFn;\n import org.apache.beam.sdk.util.CombineFnUtil;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingStrategy;\n@@ -81,7 +77,6 @@\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.api.java.function.Function;\n import org.apache.spark.api.java.function.PairFunction;\n-\n import scala.Tuple2;\n \n \n@@ -126,7 +121,7 @@ public void evaluate(GroupByKey<K, V> transform, EvaluationContext context) {\n         final KvCoder<K, V> coder = (KvCoder<K, V>) context.getInput(transform).getCoder();\n \n         final Accumulator<NamedAggregators> accum =\n-                AccumulatorSingleton.getInstance(context.getSparkContext());\n+            SparkAggregators.getNamedAggregators(context.getSparkContext());\n \n         context.putDataset(transform,\n             new BoundedDataset<>(GroupCombineFunctions.groupByKey(inRDD, accum, coder,\n@@ -232,29 +227,20 @@ public void evaluate(Combine.PerKey<K, InputT, OutputT> transform,\n     return new TransformEvaluator<ParDo.Bound<InputT, OutputT>>() {\n       @Override\n       public void evaluate(ParDo.Bound<InputT, OutputT> transform, EvaluationContext context) {\n-        DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-        if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n-          throw new UnsupportedOperationException(\n-              String.format(\n-                  \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-                  DoFn.StateId.class.getSimpleName(),\n-                  doFn.getClass().getName(),\n-                  DoFn.class.getSimpleName(),\n-                  SparkRunner.class.getSimpleName()));\n-        }\n+        DoFn<InputT, OutputT> doFn = transform.getFn();\n+        rejectStateAndTimers(doFn);\n         @SuppressWarnings(\"unchecked\")\n         JavaRDD<WindowedValue<InputT>> inRDD =\n             ((BoundedDataset<InputT>) context.borrowDataset(transform)).getRDD();\n-        @SuppressWarnings(\"unchecked\")\n-        final WindowFn<Object, ?> windowFn =\n-            (WindowFn<Object, ?>) context.getInput(transform).getWindowingStrategy().getWindowFn();\n+        WindowingStrategy<?, ?> windowingStrategy =\n+            context.getInput(transform).getWindowingStrategy();\n         Accumulator<NamedAggregators> accum =\n-            AccumulatorSingleton.getInstance(context.getSparkContext());\n+            SparkAggregators.getNamedAggregators(context.getSparkContext());\n         Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs =\n             TranslationUtils.getSideInputs(transform.getSideInputs(), context);\n         context.putDataset(transform,\n-            new BoundedDataset<>(inRDD.mapPartitions(new DoFnFunction<>(accum, transform.getFn(),\n-                context.getRuntimeContext(), sideInputs, windowFn))));\n+            new BoundedDataset<>(inRDD.mapPartitions(new DoFnFunction<>(accum, doFn,\n+                context.getRuntimeContext(), sideInputs, windowingStrategy))));\n       }\n     };\n   }\n@@ -264,29 +250,20 @@ public void evaluate(ParDo.Bound<InputT, OutputT> transform, EvaluationContext c\n     return new TransformEvaluator<ParDo.BoundMulti<InputT, OutputT>>() {\n       @Override\n       public void evaluate(ParDo.BoundMulti<InputT, OutputT> transform, EvaluationContext context) {\n-        DoFn<InputT, OutputT> doFn = transform.getNewFn();\n-        if (DoFnSignatures.getSignature(doFn.getClass()).stateDeclarations().size() > 0) {\n-          throw new UnsupportedOperationException(\n-              String.format(\n-                  \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n-                  DoFn.StateId.class.getSimpleName(),\n-                  doFn.getClass().getName(),\n-                  DoFn.class.getSimpleName(),\n-                  SparkRunner.class.getSimpleName()));\n-        }\n+        DoFn<InputT, OutputT> doFn = transform.getFn();\n+        rejectStateAndTimers(doFn);\n         @SuppressWarnings(\"unchecked\")\n         JavaRDD<WindowedValue<InputT>> inRDD =\n             ((BoundedDataset<InputT>) context.borrowDataset(transform)).getRDD();\n-        @SuppressWarnings(\"unchecked\")\n-        final WindowFn<Object, ?> windowFn =\n-            (WindowFn<Object, ?>) context.getInput(transform).getWindowingStrategy().getWindowFn();\n+        WindowingStrategy<?, ?> windowingStrategy =\n+            context.getInput(transform).getWindowingStrategy();\n         Accumulator<NamedAggregators> accum =\n-            AccumulatorSingleton.getInstance(context.getSparkContext());\n+            SparkAggregators.getNamedAggregators(context.getSparkContext());\n         JavaPairRDD<TupleTag<?>, WindowedValue<?>> all = inRDD\n             .mapPartitionsToPair(\n-                new MultiDoFnFunction<>(accum, transform.getFn(), context.getRuntimeContext(),\n+                new MultiDoFnFunction<>(accum, doFn, context.getRuntimeContext(),\n                 transform.getMainOutputTag(), TranslationUtils.getSideInputs(\n-                    transform.getSideInputs(), context), windowFn)).cache();\n+                    transform.getSideInputs(), context), windowingStrategy)).cache();\n         PCollectionTuple pct = context.getOutput(transform);\n         for (Map.Entry<TupleTag<?>, PCollection<?>> e : pct.getAll().entrySet()) {\n           @SuppressWarnings(\"unchecked\")\n@@ -526,14 +503,8 @@ public void evaluate(Window.Bound<T> transform, EvaluationContext context) {\n         if (TranslationUtils.skipAssignWindows(transform, context)) {\n           context.putDataset(transform, new BoundedDataset<>(inRDD));\n         } else {\n-          @SuppressWarnings(\"unchecked\")\n-          WindowFn<? super T, W> windowFn = (WindowFn<? super T, W>) transform.getWindowFn();\n-          OldDoFn<T, T> addWindowsDoFn = new AssignWindowsDoFn<>(windowFn);\n-          Accumulator<NamedAggregators> accum =\n-              AccumulatorSingleton.getInstance(context.getSparkContext());\n-          context.putDataset(transform,\n-              new BoundedDataset<>(inRDD.mapPartitions(new DoFnFunction<>(accum, addWindowsDoFn,\n-                  context.getRuntimeContext(), null, null))));\n+          context.putDataset(transform, new BoundedDataset<>(\n+              inRDD.map(new SparkAssignWindowFn<>(transform.getWindowFn()))));\n         }\n       }\n     };",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TransformTranslator.java",
                "sha": "5dd6beb1ae45ac65bf8d69a7aa89da81259b1fd5",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TranslationUtils.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TranslationUtils.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TranslationUtils.java",
                "patch": "@@ -24,8 +24,12 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import org.apache.beam.runners.spark.SparkRunner;\n import org.apache.beam.runners.spark.util.BroadcastHelper;\n import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n+import org.apache.beam.sdk.transforms.reflect.DoFnSignatures;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n import org.apache.beam.sdk.transforms.windowing.Window;\n@@ -211,4 +215,33 @@ public Boolean call(Tuple2<TupleTag<V>, WindowedValue<?>> input) {\n     }\n   }\n \n+  /**\n+   * Reject state and timers {@link DoFn}.\n+   *\n+   * @param doFn the {@link DoFn} to possibly reject.\n+   */\n+  public static void rejectStateAndTimers(DoFn<?, ?> doFn) {\n+    DoFnSignature signature = DoFnSignatures.getSignature(doFn.getClass());\n+\n+    if (signature.stateDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with state in the %s.\",\n+              DoFn.StateId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              SparkRunner.class.getSimpleName()));\n+    }\n+\n+    if (signature.timerDeclarations().size() > 0) {\n+      throw new UnsupportedOperationException(\n+          String.format(\n+              \"Found %s annotations on %s, but %s cannot yet be used with timers in the %s.\",\n+              DoFn.TimerId.class.getSimpleName(),\n+              doFn.getClass().getName(),\n+              DoFn.class.getSimpleName(),\n+              SparkRunner.class.getSimpleName()));\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/TranslationUtils.java",
                "sha": "eddc771efd865949ce1f55a3d5eb46f4ef8109de",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/WindowingHelpers.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/WindowingHelpers.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/WindowingHelpers.java",
                "patch": "@@ -18,6 +18,7 @@\n \n package org.apache.beam.runners.spark.translation;\n \n+import javax.annotation.Nonnull;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.spark.api.java.function.Function;\n \n@@ -84,7 +85,7 @@ public T call(WindowedValue<T> t) {\n   public static <T> com.google.common.base.Function<WindowedValue<T>, T> unwindowValueFunction() {\n     return new com.google.common.base.Function<WindowedValue<T>, T>() {\n       @Override\n-      public T apply(WindowedValue<T> t) {\n+      public T apply(@Nonnull WindowedValue<T> t) {\n         return t.getValue();\n       }\n     };",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/WindowingHelpers.java",
                "sha": "0acff71e673bbbc8de0bf9b9be0af16e011b0a76",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 23,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java",
                "patch": "@@ -18,14 +18,14 @@\n package org.apache.beam.runners.spark.translation.streaming;\n \n import static com.google.common.base.Preconditions.checkState;\n+import static org.apache.beam.runners.spark.translation.TranslationUtils.rejectStateAndTimers;\n \n import com.google.common.collect.Maps;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n-import org.apache.beam.runners.core.AssignWindowsDoFn;\n-import org.apache.beam.runners.spark.aggregators.AccumulatorSingleton;\n import org.apache.beam.runners.spark.aggregators.NamedAggregators;\n+import org.apache.beam.runners.spark.aggregators.SparkAggregators;\n import org.apache.beam.runners.spark.io.ConsoleIO;\n import org.apache.beam.runners.spark.io.CreateStream;\n import org.apache.beam.runners.spark.io.SparkUnboundedSource;\n@@ -35,6 +35,7 @@\n import org.apache.beam.runners.spark.translation.EvaluationContext;\n import org.apache.beam.runners.spark.translation.GroupCombineFunctions;\n import org.apache.beam.runners.spark.translation.MultiDoFnFunction;\n+import org.apache.beam.runners.spark.translation.SparkAssignWindowFn;\n import org.apache.beam.runners.spark.translation.SparkKeyedCombineFn;\n import org.apache.beam.runners.spark.translation.SparkPipelineTranslator;\n import org.apache.beam.runners.spark.translation.SparkRuntimeContext;\n@@ -47,9 +48,9 @@\n import org.apache.beam.sdk.io.Read;\n import org.apache.beam.sdk.transforms.Combine;\n import org.apache.beam.sdk.transforms.CombineWithContext;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.Flatten;\n import org.apache.beam.sdk.transforms.GroupByKey;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n@@ -161,7 +162,7 @@ public void evaluate(Flatten.FlattenPCollectionList<T> transform, EvaluationCont\n   private static <T, W extends BoundedWindow> TransformEvaluator<Window.Bound<T>> window() {\n     return new TransformEvaluator<Window.Bound<T>>() {\n       @Override\n-      public void evaluate(Window.Bound<T> transform, EvaluationContext context) {\n+      public void evaluate(final Window.Bound<T> transform, EvaluationContext context) {\n         @SuppressWarnings(\"unchecked\")\n         WindowFn<? super T, W> windowFn = (WindowFn<? super T, W>) transform.getWindowFn();\n         @SuppressWarnings(\"unchecked\")\n@@ -187,16 +188,11 @@ public void evaluate(Window.Bound<T> transform, EvaluationContext context) {\n         if (TranslationUtils.skipAssignWindows(transform, context)) {\n           context.putDataset(transform, new UnboundedDataset<>(windowedDStream));\n         } else {\n-          final OldDoFn<T, T> addWindowsDoFn = new AssignWindowsDoFn<>(windowFn);\n-          final SparkRuntimeContext runtimeContext = context.getRuntimeContext();\n           JavaDStream<WindowedValue<T>> outStream = windowedDStream.transform(\n               new Function<JavaRDD<WindowedValue<T>>, JavaRDD<WindowedValue<T>>>() {\n             @Override\n             public JavaRDD<WindowedValue<T>> call(JavaRDD<WindowedValue<T>> rdd) throws Exception {\n-              final Accumulator<NamedAggregators> accum =\n-                AccumulatorSingleton.getInstance(new JavaSparkContext(rdd.context()));\n-              return rdd.mapPartitions(\n-                new DoFnFunction<>(accum, addWindowsDoFn, runtimeContext, null, null));\n+              return rdd.map(new SparkAssignWindowFn<>(transform.getWindowFn()));\n             }\n           });\n           context.putDataset(transform, new UnboundedDataset<>(outStream));\n@@ -227,7 +223,7 @@ public void evaluate(GroupByKey<K, V> transform, EvaluationContext context) {\n           public JavaRDD<WindowedValue<KV<K, Iterable<V>>>> call(\n               JavaRDD<WindowedValue<KV<K, V>>> rdd) throws Exception {\n             final Accumulator<NamedAggregators> accum =\n-                AccumulatorSingleton.getInstance(new JavaSparkContext(rdd.context()));\n+                SparkAggregators.getNamedAggregators(new JavaSparkContext(rdd.context()));\n             return GroupCombineFunctions.groupByKey(rdd, accum, coder, runtimeContext,\n                 windowingStrategy);\n           }\n@@ -348,11 +344,13 @@ public void evaluate(final Combine.PerKey<K, InputT, OutputT> transform,\n       @Override\n       public void evaluate(final ParDo.Bound<InputT, OutputT> transform,\n                            final EvaluationContext context) {\n+        final DoFn<InputT, OutputT> doFn = transform.getFn();\n+        rejectStateAndTimers(doFn);\n         final SparkRuntimeContext runtimeContext = context.getRuntimeContext();\n         final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs =\n             TranslationUtils.getSideInputs(transform.getSideInputs(), context);\n-        final WindowFn<Object, ?> windowFn =\n-            (WindowFn<Object, ?>) context.getInput(transform).getWindowingStrategy().getWindowFn();\n+        final WindowingStrategy<?, ?> windowingStrategy =\n+            context.getInput(transform).getWindowingStrategy();\n         JavaDStream<WindowedValue<InputT>> dStream =\n             ((UnboundedDataset<InputT>) context.borrowDataset(transform)).getDStream();\n \n@@ -363,9 +361,9 @@ public void evaluate(final ParDo.Bound<InputT, OutputT> transform,\n           public JavaRDD<WindowedValue<OutputT>> call(JavaRDD<WindowedValue<InputT>> rdd) throws\n               Exception {\n             final Accumulator<NamedAggregators> accum =\n-                AccumulatorSingleton.getInstance(new JavaSparkContext(rdd.context()));\n+                SparkAggregators.getNamedAggregators(new JavaSparkContext(rdd.context()));\n             return rdd.mapPartitions(\n-                new DoFnFunction<>(accum, transform.getFn(), runtimeContext, sideInputs, windowFn));\n+                new DoFnFunction<>(accum, doFn, runtimeContext, sideInputs, windowingStrategy));\n           }\n         });\n \n@@ -380,12 +378,13 @@ public void evaluate(final ParDo.Bound<InputT, OutputT> transform,\n       @Override\n       public void evaluate(final ParDo.BoundMulti<InputT, OutputT> transform,\n                            final EvaluationContext context) {\n+        final DoFn<InputT, OutputT> doFn = transform.getFn();\n+        rejectStateAndTimers(doFn);\n         final SparkRuntimeContext runtimeContext = context.getRuntimeContext();\n         final Map<TupleTag<?>, KV<WindowingStrategy<?, ?>, BroadcastHelper<?>>> sideInputs =\n             TranslationUtils.getSideInputs(transform.getSideInputs(), context);\n-        @SuppressWarnings(\"unchecked\")\n-        final WindowFn<Object, ?> windowFn =\n-            (WindowFn<Object, ?>) context.getInput(transform).getWindowingStrategy().getWindowFn();\n+        final WindowingStrategy<?, ?> windowingStrategy =\n+            context.getInput(transform).getWindowingStrategy();\n         @SuppressWarnings(\"unchecked\")\n         JavaDStream<WindowedValue<InputT>> dStream =\n             ((UnboundedDataset<InputT>) context.borrowDataset(transform)).getDStream();\n@@ -396,9 +395,9 @@ public void evaluate(final ParDo.BoundMulti<InputT, OutputT> transform,\n           public JavaPairRDD<TupleTag<?>, WindowedValue<?>> call(\n               JavaRDD<WindowedValue<InputT>> rdd) throws Exception {\n             final Accumulator<NamedAggregators> accum =\n-                AccumulatorSingleton.getInstance(new JavaSparkContext(rdd.context()));\n-            return rdd.mapPartitionsToPair(new MultiDoFnFunction<>(accum, transform.getFn(),\n-                runtimeContext, transform.getMainOutputTag(), sideInputs, windowFn));\n+                SparkAggregators.getNamedAggregators(new JavaSparkContext(rdd.context()));\n+            return rdd.mapPartitionsToPair(new MultiDoFnFunction<>(accum, doFn,\n+                runtimeContext, transform.getMainOutputTag(), sideInputs, windowingStrategy));\n           }\n         }).cache();\n         PCollectionTuple pct = context.getOutput(transform);\n@@ -417,8 +416,8 @@ public void evaluate(final ParDo.BoundMulti<InputT, OutputT> transform,\n     };\n   }\n \n-  private static final Map<Class<? extends PTransform>, TransformEvaluator<?>> EVALUATORS = Maps\n-      .newHashMap();\n+  private static final Map<Class<? extends PTransform>, TransformEvaluator<?>> EVALUATORS =\n+      Maps.newHashMap();\n \n   static {\n     EVALUATORS.put(Read.Unbounded.class, readUnbounded());",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/translation/streaming/StreamingTransformTranslator.java",
                "sha": "070ccbb3c4b2ab9568f2157e548d222542237717",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/util/SinglePrimitiveOutputPTransform.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/main/java/org/apache/beam/runners/spark/util/SinglePrimitiveOutputPTransform.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/main/java/org/apache/beam/runners/spark/util/SinglePrimitiveOutputPTransform.java",
                "patch": "@@ -35,7 +35,7 @@ public SinglePrimitiveOutputPTransform(PTransform<PInput, PCollection<T>> transf\n   }\n \n   @Override\n-  public PCollection<T> apply(PInput input) {\n+  public PCollection<T> expand(PInput input) {\n     try {\n       PCollection<T> collection = PCollection.<T>createPrimitiveOutputInternal(\n               input.getPipeline(), WindowingStrategy.globalDefault(), IsBounded.BOUNDED);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/main/java/org/apache/beam/runners/spark/util/SinglePrimitiveOutputPTransform.java",
                "sha": "7580da77a1385c25c27e144d073e109df4c8a05d",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/ForceStreamingTest.java",
                "changes": 123,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/ForceStreamingTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/ForceStreamingTest.java",
                "patch": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.spark;\n+\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.BoundedReadFromUnboundedSource;\n+import org.apache.beam.sdk.io.Read;\n+import org.apache.beam.sdk.io.UnboundedSource;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.runners.TransformHierarchy;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.junit.Test;\n+\n+\n+/**\n+ * Test that we can \"force streaming\" on pipelines with {@link BoundedReadFromUnboundedSource}\n+ * inputs using the {@link TestSparkRunner}.\n+ *\n+ * <p>The test validates that when a pipeline reads from a {@link BoundedReadFromUnboundedSource},\n+ * with {@link SparkPipelineOptions#setStreaming(boolean)} true\n+ * and using the {@link TestSparkRunner}; the {@link Read.Bounded} transform\n+ * is replaced by an {@link Read.Unbounded} transform.\n+ *\n+ * <p>This test does not execute a pipeline.\n+ */\n+public class ForceStreamingTest {\n+\n+  @Test\n+  public void test() throws IOException {\n+    SparkPipelineOptions options = PipelineOptionsFactory.create().as(SparkPipelineOptions.class);\n+    options.setRunner(TestSparkRunner.class);\n+    // force streaming.\n+    options.setForceStreaming(true);\n+\n+    Pipeline pipeline = Pipeline.create(options);\n+\n+    // apply the BoundedReadFromUnboundedSource.\n+    @SuppressWarnings(\"unchecked\")\n+    BoundedReadFromUnboundedSource boundedRead =\n+        Read.from(new FakeUnboundedSource()).withMaxNumRecords(-1);\n+    //noinspection unchecked\n+    pipeline.apply(boundedRead);\n+\n+    UnboundedReadDetector unboundedReadDetector = new UnboundedReadDetector();\n+    pipeline.traverseTopologically(unboundedReadDetector);\n+\n+    // assert that the applied BoundedReadFromUnboundedSource\n+    // is being treated as an unbounded read.\n+    assertThat(\"Expected to have an unbounded read.\", unboundedReadDetector.isUnbounded);\n+  }\n+\n+  /**\n+   * Traverses the Pipeline to check if the input is indeed a {@link Read.Unbounded}.\n+   */\n+  private class UnboundedReadDetector extends Pipeline.PipelineVisitor.Defaults {\n+    private boolean isUnbounded = false;\n+\n+    @Override\n+    public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n+      Class<? extends PTransform> transformClass = node.getTransform().getClass();\n+      if (transformClass == Read.Unbounded.class) {\n+        isUnbounded = true;\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * A fake {@link UnboundedSource} to satisfy the compiler.\n+   */\n+  private static class FakeUnboundedSource extends UnboundedSource {\n+\n+    @Override\n+    public List<? extends UnboundedSource> generateInitialSplits(\n+        int desiredNumSplits,\n+        PipelineOptions options) throws Exception {\n+      return null;\n+    }\n+\n+    @Override\n+    public UnboundedReader createReader(\n+        PipelineOptions options,\n+        CheckpointMark checkpointMark) throws IOException {\n+      return null;\n+    }\n+\n+    @Override\n+    public Coder getCheckpointMarkCoder() {\n+      return null;\n+    }\n+\n+    @Override\n+    public void validate() { }\n+\n+    @Override\n+    public Coder getDefaultOutputCoder() {\n+      return null;\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/ForceStreamingTest.java",
                "sha": "eb17eea9112f9484eae4857bbc0e4b89fb5ea437",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/ProvidedSparkContextTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/ProvidedSparkContextTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/ProvidedSparkContextTest.java",
                "patch": "@@ -76,7 +76,7 @@ public void testWithProvidedContext() throws Exception {\n         PAssert.that(output).containsInAnyOrder(EXPECTED_COUNT_SET);\n \n         // Run test from pipeline\n-        p.run();\n+        p.run().waitUntilFinish();\n \n         jsc.stop();\n     }\n@@ -100,7 +100,7 @@ public void testWithNullContext() throws Exception {\n         PAssert.that(output).containsInAnyOrder(EXPECTED_COUNT_SET);\n \n         try {\n-            p.run();\n+            p.run().waitUntilFinish();\n             fail(\"Should throw an exception when The provided Spark context is null\");\n         } catch (RuntimeException e){\n             assert(e.getMessage().contains(PROVIDED_CONTEXT_EXCEPTION));\n@@ -128,7 +128,7 @@ public void testWithStoppedProvidedContext() throws Exception {\n         PAssert.that(output).containsInAnyOrder(EXPECTED_COUNT_SET);\n \n         try {\n-            p.run();\n+            p.run().waitUntilFinish();\n             fail(\"Should throw an exception when The provided Spark context is stopped\");\n         } catch (RuntimeException e){\n             assert(e.getMessage().contains(PROVIDED_CONTEXT_EXCEPTION));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/ProvidedSparkContextTest.java",
                "sha": "298284453c392b37ff3e3f16b3e5cb640b4b8750",
                "status": "modified"
            },
            {
                "additions": 217,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/SparkPipelineStateTest.java",
                "changes": 217,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/SparkPipelineStateTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/SparkPipelineStateTest.java",
                "patch": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.spark;\n+\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.fail;\n+\n+import com.google.common.collect.Lists;\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.beam.runners.spark.io.CreateStream;\n+import org.apache.beam.runners.spark.translation.streaming.utils.SparkTestPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SimpleFunction;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.joda.time.Duration;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TestName;\n+\n+/**\n+ * This suite tests that various scenarios result in proper states of the pipeline.\n+ */\n+public class SparkPipelineStateTest implements Serializable {\n+\n+  private static class MyCustomException extends RuntimeException {\n+\n+    MyCustomException(final String message) {\n+      super(message);\n+    }\n+  }\n+\n+  @Rule\n+  public transient SparkTestPipelineOptions commonOptions = new SparkTestPipelineOptions();\n+\n+  @Rule\n+  public transient TestName testName = new TestName();\n+\n+  private static final String FAILED_THE_BATCH_INTENTIONALLY = \"Failed the batch intentionally\";\n+\n+  private static final List<String> BATCH_WORDS = Arrays.asList(\"one\", \"two\");\n+\n+  private static final List<Iterable<String>> STREAMING_WORDS =\n+      Lists.<Iterable<String>>newArrayList(BATCH_WORDS);\n+\n+  private ParDo.Bound<String, String> printParDo(final String prefix) {\n+    return ParDo.of(new DoFn<String, String>() {\n+\n+      @ProcessElement\n+      public void processElement(final ProcessContext c) {\n+        System.out.println(prefix + \" \" + c.element());\n+      }\n+    });\n+  }\n+\n+  private PTransform<PBegin, PCollection<String>> getValues(final SparkPipelineOptions options) {\n+    return options.isStreaming()\n+        ? CreateStream.fromQueue(STREAMING_WORDS)\n+        : Create.of(BATCH_WORDS);\n+  }\n+\n+  private SparkPipelineOptions getStreamingOptions() {\n+    final SparkPipelineOptions options = commonOptions.getOptions();\n+    options.setStreaming(true);\n+    return options;\n+  }\n+\n+  private SparkPipelineOptions getBatchOptions() {\n+    return commonOptions.getOptions();\n+  }\n+\n+  private Pipeline getPipeline(final SparkPipelineOptions options) {\n+\n+    final Pipeline pipeline = Pipeline.create(options);\n+    final String name = testName.getMethodName() + \"(isStreaming=\" + options.isStreaming() + \")\";\n+\n+    pipeline\n+        .apply(getValues(options)).setCoder(StringUtf8Coder.of())\n+        .apply(printParDo(name));\n+\n+    return pipeline;\n+  }\n+\n+  private void testFailedPipeline(final SparkPipelineOptions options) throws Exception {\n+\n+    SparkPipelineResult result = null;\n+\n+    try {\n+      final Pipeline pipeline = Pipeline.create(options);\n+      pipeline\n+          .apply(getValues(options)).setCoder(StringUtf8Coder.of())\n+          .apply(MapElements.via(new SimpleFunction<String, String>() {\n+\n+            @Override\n+            public String apply(final String input) {\n+              throw new MyCustomException(FAILED_THE_BATCH_INTENTIONALLY);\n+            }\n+          }));\n+\n+      result = (SparkPipelineResult) pipeline.run();\n+      result.waitUntilFinish();\n+    } catch (final Exception e) {\n+      assertThat(e, instanceOf(Pipeline.PipelineExecutionException.class));\n+      assertThat(e.getCause(), instanceOf(MyCustomException.class));\n+      assertThat(e.getCause().getMessage(), is(FAILED_THE_BATCH_INTENTIONALLY));\n+      assertThat(result.getState(), is(PipelineResult.State.FAILED));\n+      result.cancel();\n+      return;\n+    }\n+\n+    fail(\"An injected failure did not affect the pipeline as expected.\");\n+  }\n+\n+  private void testTimeoutPipeline(final SparkPipelineOptions options) throws Exception {\n+\n+    final Pipeline pipeline = getPipeline(options);\n+\n+    final SparkPipelineResult result = (SparkPipelineResult) pipeline.run();\n+\n+    result.waitUntilFinish(Duration.millis(1));\n+\n+    assertThat(result.getState(), nullValue());\n+\n+    result.cancel();\n+  }\n+\n+  private void testCanceledPipeline(final SparkPipelineOptions options) throws Exception {\n+\n+    final Pipeline pipeline = getPipeline(options);\n+\n+    final SparkPipelineResult result = (SparkPipelineResult) pipeline.run();\n+\n+    result.cancel();\n+\n+    assertThat(result.getState(), is(PipelineResult.State.CANCELLED));\n+  }\n+\n+  private void testRunningPipeline(final SparkPipelineOptions options) throws Exception {\n+\n+    final Pipeline pipeline = getPipeline(options);\n+\n+    final SparkPipelineResult result = (SparkPipelineResult) pipeline.run();\n+\n+    assertThat(result.getState(), is(PipelineResult.State.RUNNING));\n+\n+    result.cancel();\n+  }\n+\n+  @Test\n+  public void testStreamingPipelineRunningState() throws Exception {\n+    testRunningPipeline(getStreamingOptions());\n+  }\n+\n+  @Test\n+  public void testBatchPipelineRunningState() throws Exception {\n+    testRunningPipeline(getBatchOptions());\n+  }\n+\n+  @Test\n+  public void testStreamingPipelineCanceledState() throws Exception {\n+    testCanceledPipeline(getStreamingOptions());\n+  }\n+\n+  @Test\n+  public void testBatchPipelineCanceledState() throws Exception {\n+    testCanceledPipeline(getBatchOptions());\n+  }\n+\n+  @Test\n+  public void testStreamingPipelineFailedState() throws Exception {\n+    testFailedPipeline(getStreamingOptions());\n+  }\n+\n+  @Test\n+  public void testBatchPipelineFailedState() throws Exception {\n+    testFailedPipeline(getBatchOptions());\n+  }\n+\n+  @Test\n+  public void testStreamingPipelineTimeoutState() throws Exception {\n+    testTimeoutPipeline(getStreamingOptions());\n+  }\n+\n+  @Test\n+  public void testBatchPipelineTimeoutState() throws Exception {\n+    testTimeoutPipeline(getBatchOptions());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/SparkPipelineStateTest.java",
                "sha": "54e210d96b1b15a0f8ff165b348896d1229e58f0",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/ClearAggregatorsRule.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/ClearAggregatorsRule.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/ClearAggregatorsRule.java",
                "patch": "@@ -16,18 +16,22 @@\n  * limitations under the License.\n  */\n \n-package org.apache.beam.runners.spark.aggregators.metrics.sink;\n+package org.apache.beam.runners.spark.aggregators;\n \n-import org.apache.beam.runners.spark.aggregators.AccumulatorSingleton;\n import org.junit.rules.ExternalResource;\n \n /**\n  * A rule that clears the {@link org.apache.beam.runners.spark.aggregators.AccumulatorSingleton}\n  * which represents the Beam {@link org.apache.beam.sdk.transforms.Aggregator}s.\n  */\n-class ClearAggregatorsRule extends ExternalResource {\n+public class ClearAggregatorsRule extends ExternalResource {\n+\n   @Override\n   protected void before() throws Throwable {\n+    clearNamedAggregators();\n+  }\n+\n+  public void clearNamedAggregators() {\n     AccumulatorSingleton.clear();\n   }\n }",
                "previous_filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/metrics/sink/ClearAggregatorsRule.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/ClearAggregatorsRule.java",
                "sha": "4e91d15fe5920288080a9fe426d1c15762feecab",
                "status": "renamed"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/metrics/sink/NamedAggregatorsTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/metrics/sink/NamedAggregatorsTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/metrics/sink/NamedAggregatorsTest.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.List;\n import java.util.Set;\n import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.aggregators.ClearAggregatorsRule;\n import org.apache.beam.runners.spark.examples.WordCount;\n import org.apache.beam.runners.spark.translation.streaming.utils.SparkTestPipelineOptions;\n import org.apache.beam.sdk.Pipeline;\n@@ -78,7 +79,7 @@ private void runPipeline() {\n \n     PAssert.that(output).containsInAnyOrder(expectedCounts);\n \n-    pipeline.run();\n+    pipeline.run().waitUntilFinish();\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/aggregators/metrics/sink/NamedAggregatorsTest.java",
                "sha": "3b5dd21dacf2c21fde814eb4249e5ea59f7dfc3a",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistratorTest.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistratorTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistratorTest.java",
                "patch": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.runners.spark.coders;\n+\n+import com.esotericsoftware.kryo.Kryo;\n+import com.esotericsoftware.kryo.serializers.JavaSerializer;\n+import com.google.common.collect.Iterables;\n+import java.io.Serializable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.Source;\n+import org.hamcrest.Matchers;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.reflections.Reflections;\n+\n+\n+/**\n+ * BeamSparkRunnerRegistrator Test.\n+ */\n+public class BeamSparkRunnerRegistratorTest {\n+  @Test\n+  public void testCodersAndSourcesRegistration() {\n+    BeamSparkRunnerRegistrator registrator = new BeamSparkRunnerRegistrator();\n+\n+    Reflections reflections = new Reflections();\n+    Iterable<Class<? extends Serializable>> classesForJavaSerialization =\n+        Iterables.concat(reflections.getSubTypesOf(Coder.class),\n+            reflections.getSubTypesOf(Source.class));\n+\n+    Kryo kryo = new Kryo();\n+\n+    registrator.registerClasses(kryo);\n+\n+    for (Class<?> clazz : classesForJavaSerialization) {\n+      Assert.assertThat(\"Registered serializer for class \" + clazz.getName()\n+              + \" was not an instance of \" + JavaSerializer.class.getName(),\n+          kryo.getSerializer(clazz),\n+          Matchers.instanceOf(JavaSerializer.class));\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/coders/BeamSparkRunnerRegistratorTest.java",
                "sha": "e35301750cf61616d048f03cfdf1c9ebd31c9e25",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java",
                "patch": "@@ -76,7 +76,7 @@ public void testGeneric() throws Exception {\n     PCollection<GenericRecord> input = p.apply(\n         AvroIO.Read.from(inputFile.getAbsolutePath()).withSchema(schema));\n     input.apply(AvroIO.Write.to(outputDir.getAbsolutePath()).withSchema(schema));\n-    p.run();\n+    p.run().waitUntilFinish();\n \n     List<GenericRecord> records = readGenericFile();\n     assertEquals(Lists.newArrayList(savedRecord), records);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/io/AvroPipelineTest.java",
                "sha": "c5bb583abc40ab9c880333b2d06ac2b5b044d1d0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java",
                "patch": "@@ -74,7 +74,7 @@ public void testText() throws Exception {\n     PCollection<String> output = inputWords.apply(new WordCount.CountWords())\n         .apply(MapElements.via(new WordCount.FormatAsTextFn()));\n     output.apply(TextIO.Write.to(outputDir.getAbsolutePath()).withNumShards(3).withSuffix(\".txt\"));\n-    p.run();\n+    p.run().waitUntilFinish();\n \n     int count = 0;\n     Set<String> expected = Sets.newHashSet(\"hi: 5\", \"there: 1\", \"sue: 2\", \"bob: 2\");",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/io/NumShardsTest.java",
                "sha": "34d68189f6fc722681db00bb8e20e92e563b16c3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java",
                "patch": "@@ -88,7 +88,7 @@ public void testSequenceFile() throws Exception {\n     HadoopIO.Write.Bound<IntWritable, Text> write = HadoopIO.Write.to(outputFile.getAbsolutePath(),\n         outputFormatClass, IntWritable.class, Text.class);\n     input.apply(write.withoutSharding());\n-    p.run();\n+    p.run().waitUntilFinish();\n \n     IntWritable key = new IntWritable();\n     Text value = new Text();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/io/hadoop/HadoopFileFormatPipelineTest.java",
                "sha": "9efc670cf4fab6c7b959a494a1737c4c42c7d4d1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/beam/blob/b6e7bb659f33e346c00e66ca96e3c54dd7ef07da/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java?ref=b6e7bb659f33e346c00e66ca96e3c54dd7ef07da",
                "deletions": 59,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java",
                "patch": "@@ -1,59 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.beam.runners.spark.translation;\n-\n-import static org.hamcrest.core.Is.isA;\n-\n-import java.io.Serializable;\n-import org.apache.beam.runners.spark.translation.streaming.utils.SparkTestPipelineOptions;\n-import org.apache.beam.sdk.Pipeline;\n-import org.apache.beam.sdk.transforms.Create;\n-import org.apache.beam.sdk.transforms.DoFn;\n-import org.apache.beam.sdk.transforms.ParDo;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.ExpectedException;\n-\n-/**\n- * Side effects test.\n- */\n-public class SideEffectsTest implements Serializable {\n-  private static class UserException extends RuntimeException {\n-  }\n-\n-  @Rule\n-  public final transient SparkTestPipelineOptions pipelineOptions = new SparkTestPipelineOptions();\n-  @Rule\n-  public final transient ExpectedException expectedException = ExpectedException.none();\n-\n-  @Test\n-  public void test() throws Exception {\n-    Pipeline p = Pipeline.create(pipelineOptions.getOptions());\n-\n-    p.apply(Create.of(\"a\")).apply(ParDo.of(new DoFn<String, String>() {\n-      @ProcessElement\n-      public void processElement(ProcessContext c) throws Exception {\n-        throw new UserException();\n-      }\n-    }));\n-\n-    expectedException.expectCause(isA(UserException.class));\n-    p.run();\n-  }\n-}",
                "raw_url": "https://github.com/apache/beam/raw/b6e7bb659f33e346c00e66ca96e3c54dd7ef07da/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SideEffectsTest.java",
                "sha": "3b79d036c217e0a1701e0c1e192420b9c259732d",
                "status": "removed"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/EmptyStreamAssertionTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/EmptyStreamAssertionTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/EmptyStreamAssertionTest.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.io.Serializable;\n import java.util.Collections;\n import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.aggregators.ClearAggregatorsRule;\n import org.apache.beam.runners.spark.io.CreateStream;\n import org.apache.beam.runners.spark.translation.streaming.utils.PAssertStreaming;\n import org.apache.beam.runners.spark.translation.streaming.utils.SparkTestPipelineOptionsForStreaming;\n@@ -54,6 +55,9 @@\n   public SparkTestPipelineOptionsForStreaming commonOptions =\n       new SparkTestPipelineOptionsForStreaming();\n \n+  @Rule\n+  public ClearAggregatorsRule clearAggregatorsRule = new ClearAggregatorsRule();\n+\n   @Test\n   public void testAssertion() throws Exception {\n     SparkPipelineOptions options = commonOptions.withTmpCheckpointDir(checkpointParentDir);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/EmptyStreamAssertionTest.java",
                "sha": "e48294526038b97f9ae9e5ff4d9a3eb77740c48c",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 13,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java",
                "patch": "@@ -76,10 +76,15 @@ public static void init() throws IOException {\n \n   @Test\n   public void testEarliest2Topics() throws Exception {\n+    Duration batchIntervalDuration = Duration.standardSeconds(5);\n     SparkPipelineOptions options = commonOptions.withTmpCheckpointDir(checkpointParentDir);\n-    // It seems that the consumer's first \"position\" lookup (in unit test) takes +200 msec,\n-    // so to be on the safe side we'll set to 750 msec.\n-    options.setMinReadTimeMillis(750L);\n+    // provide a generous enough batch-interval to have everything fit in one micro-batch.\n+    options.setBatchIntervalMillis(batchIntervalDuration.getMillis());\n+    // provide a very generous read time bound, we rely on num records bound here.\n+    options.setMinReadTimeMillis(batchIntervalDuration.minus(1).getMillis());\n+    // bound the read on the number of messages - 2 topics of 4 messages each.\n+    options.setMaxRecordsPerBatch(8L);\n+\n     //--- setup\n     // two topics.\n     final String topic1 = \"topic1\";\n@@ -90,8 +95,6 @@ public void testEarliest2Topics() throws Exception {\n     );\n     // expected.\n     final String[] expected = {\"k1,v1\", \"k2,v2\", \"k3,v3\", \"k4,v4\"};\n-    // batch and window duration.\n-    final Duration batchAndWindowDuration = Duration.standardSeconds(1);\n \n     // write to both topics ahead.\n     produce(topic1, messages);\n@@ -114,17 +117,27 @@ public void testEarliest2Topics() throws Exception {\n     PCollection<String> deduped =\n         p.apply(read.withoutMetadata()).setCoder(\n             KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()))\n-        .apply(Window.<KV<String, String>>into(FixedWindows.of(batchAndWindowDuration)))\n+        .apply(Window.<KV<String, String>>into(FixedWindows.of(batchIntervalDuration)))\n         .apply(ParDo.of(new FormatKVFn()))\n         .apply(Distinct.<String>create());\n \n-    PAssertStreaming.runAndAssertContents(p, deduped, expected, Duration.standardSeconds(1L));\n+    // graceful shutdown will make sure first batch (at least) will finish.\n+    Duration timeout = Duration.standardSeconds(1L);\n+    PAssertStreaming.runAndAssertContents(p, deduped, expected, timeout);\n   }\n \n   @Test\n   public void testLatest() throws Exception {\n+    Duration batchIntervalDuration = Duration.standardSeconds(5);\n     SparkContextOptions options =\n         commonOptions.withTmpCheckpointDir(checkpointParentDir).as(SparkContextOptions.class);\n+    // provide a generous enough batch-interval to have everything fit in one micro-batch.\n+    options.setBatchIntervalMillis(batchIntervalDuration.getMillis());\n+    // provide a very generous read time bound, we rely on num records bound here.\n+    options.setMinReadTimeMillis(batchIntervalDuration.minus(1).getMillis());\n+    // bound the read on the number of messages - 1 topics of 4 messages.\n+    options.setMaxRecordsPerBatch(4L);\n+\n     //--- setup\n     final String topic = \"topic\";\n     // messages.\n@@ -133,16 +146,11 @@ public void testLatest() throws Exception {\n     );\n     // expected.\n     final String[] expected = {\"k1,v1\", \"k2,v2\", \"k3,v3\", \"k4,v4\"};\n-    // batch and window duration.\n-    final Duration batchAndWindowDuration = Duration.standardSeconds(1);\n \n     // write once first batch completes, this will guarantee latest-like behaviour.\n     options.setListeners(Collections.<JavaStreamingListener>singletonList(\n         KafkaWriteOnBatchCompleted.once(messages, Collections.singletonList(topic),\n             EMBEDDED_KAFKA_CLUSTER.getProps(), EMBEDDED_KAFKA_CLUSTER.getBrokerList())));\n-    // It seems that the consumer's first \"position\" lookup (in unit test) takes +200 msec,\n-    // so to be on the safe side we'll set to 750 msec.\n-    options.setMinReadTimeMillis(750L);\n \n     //------- test: read and format.\n     Pipeline p = Pipeline.create(options);\n@@ -161,7 +169,7 @@ public void testLatest() throws Exception {\n     PCollection<String> formatted =\n         p.apply(read.withoutMetadata()).setCoder(\n             KvCoder.of(StringUtf8Coder.of(), StringUtf8Coder.of()))\n-        .apply(Window.<KV<String, String>>into(FixedWindows.of(batchAndWindowDuration)))\n+        .apply(Window.<KV<String, String>>into(FixedWindows.of(batchIntervalDuration)))\n         .apply(ParDo.of(new FormatKVFn()));\n \n     // run for more than 1 batch interval, so that reading of latest is attempted in the",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/KafkaStreamingTest.java",
                "sha": "6be92d050b6f43d7024679afe6ac1fad00a2ff12",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/ResumeFromCheckpointStreamingTest.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/ResumeFromCheckpointStreamingTest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 13,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/ResumeFromCheckpointStreamingTest.java",
                "patch": "@@ -27,9 +27,9 @@\n import java.util.Map;\n import java.util.Properties;\n import java.util.concurrent.TimeUnit;\n-import org.apache.beam.runners.spark.EvaluationResult;\n import org.apache.beam.runners.spark.SparkPipelineOptions;\n-import org.apache.beam.runners.spark.aggregators.AccumulatorSingleton;\n+import org.apache.beam.runners.spark.SparkPipelineResult;\n+import org.apache.beam.runners.spark.aggregators.ClearAggregatorsRule;\n import org.apache.beam.runners.spark.translation.streaming.utils.EmbeddedKafkaCluster;\n import org.apache.beam.runners.spark.translation.streaming.utils.PAssertStreaming;\n import org.apache.beam.runners.spark.translation.streaming.utils.SparkTestPipelineOptionsForStreaming;\n@@ -83,6 +83,9 @@\n   public SparkTestPipelineOptionsForStreaming commonOptions =\n       new SparkTestPipelineOptionsForStreaming();\n \n+  @Rule\n+  public ClearAggregatorsRule clearAggregatorsRule = new ClearAggregatorsRule();\n+\n   @BeforeClass\n   public static void init() throws IOException {\n     EMBEDDED_ZOOKEEPER.startup();\n@@ -109,16 +112,20 @@ private static void produce() {\n \n   @Test\n   public void testRun() throws Exception {\n+    Duration batchIntervalDuration = Duration.standardSeconds(5);\n     SparkPipelineOptions options = commonOptions.withTmpCheckpointDir(checkpointParentDir);\n-    // It seems that the consumer's first \"position\" lookup (in unit test) takes +200 msec,\n-    // so to be on the safe side we'll set to 750 msec.\n-    options.setMinReadTimeMillis(750L);\n+    // provide a generous enough batch-interval to have everything fit in one micro-batch.\n+    options.setBatchIntervalMillis(batchIntervalDuration.getMillis());\n+    // provide a very generous read time bound, we rely on num records bound here.\n+    options.setMinReadTimeMillis(batchIntervalDuration.minus(1).getMillis());\n+    // bound the read on the number of messages - 1 topic of 4 messages.\n+    options.setMaxRecordsPerBatch(4L);\n \n     // checkpoint after first (and only) interval.\n     options.setCheckpointDurationMillis(options.getBatchIntervalMillis());\n \n     // first run will read from Kafka backlog - \"auto.offset.reset=smallest\"\n-    EvaluationResult res = run(options);\n+    SparkPipelineResult res = run(options);\n     long processedMessages1 = res.getAggregatorValue(\"processedMessages\", Long.class);\n     assertThat(String.format(\"Expected %d processed messages count but \"\n         + \"found %d\", EXPECTED_AGG_FIRST, processedMessages1), processedMessages1,\n@@ -132,14 +139,14 @@ public void testRun() throws Exception {\n             equalTo(EXPECTED_AGG_FIRST));\n   }\n \n-  private static EvaluationResult runAgain(SparkPipelineOptions options) {\n-    AccumulatorSingleton.clear();\n+  private SparkPipelineResult runAgain(SparkPipelineOptions options) {\n+    clearAggregatorsRule.clearNamedAggregators();\n     // sleep before next run.\n     Uninterruptibles.sleepUninterruptibly(100, TimeUnit.MILLISECONDS);\n     return run(options);\n   }\n \n-  private static EvaluationResult run(SparkPipelineOptions options) {\n+  private static SparkPipelineResult run(SparkPipelineOptions options) {\n     // write to Kafka\n     produce();\n     Map<String, Object> consumerProps = ImmutableMap.<String, Object>of(\n@@ -161,10 +168,9 @@ private static EvaluationResult run(SparkPipelineOptions options) {\n         .apply(Window.<KV<String, String>>into(FixedWindows.of(windowDuration)))\n         .apply(ParDo.of(new FormatAsText()));\n \n-    // requires a graceful stop so that checkpointing of the first run would finish successfully\n-    // before stopping and attempting to resume.\n-    return PAssertStreaming.runAndAssertContents(p, formattedKV, EXPECTED,\n-            Duration.standardSeconds(1L));\n+    // graceful shutdown will make sure first batch (at least) will finish.\n+    Duration timeout = Duration.standardSeconds(1L);\n+    return PAssertStreaming.runAndAssertContents(p, formattedKV, EXPECTED, timeout);\n   }\n \n   @AfterClass",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/ResumeFromCheckpointStreamingTest.java",
                "sha": "2718b5f7c9f216e5e72934e3c2c659ae55b21506",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/PAssertStreaming.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/PAssertStreaming.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 15,
                "filename": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/PAssertStreaming.java",
                "patch": "@@ -23,12 +23,12 @@\n import static org.junit.Assert.assertThat;\n \n import java.io.Serializable;\n-import org.apache.beam.runners.spark.EvaluationResult;\n+import org.apache.beam.runners.spark.SparkPipelineResult;\n import org.apache.beam.sdk.Pipeline;\n import org.apache.beam.sdk.testing.PAssert;\n import org.apache.beam.sdk.transforms.Aggregator;\n+import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.GroupByKey;\n-import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.ParDo;\n import org.apache.beam.sdk.transforms.Sum;\n import org.apache.beam.sdk.transforms.Values;\n@@ -55,11 +55,12 @@ private PAssertStreaming() {\n    * Note that it is oblivious to windowing, so the assertion will apply indiscriminately to all\n    * windows.\n    */\n-  public static <T> EvaluationResult runAndAssertContents(Pipeline p,\n-                                                          PCollection<T> actual,\n-                                                          T[] expected,\n-                                                          Duration timeout,\n-                                                          boolean stopGracefully) {\n+  public static <T> SparkPipelineResult runAndAssertContents(\n+      Pipeline p,\n+      PCollection<T> actual,\n+      T[] expected,\n+      Duration timeout,\n+      boolean stopGracefully) {\n     // Because PAssert does not support non-global windowing, but all our data is in one window,\n     // we set up the assertion directly.\n     actual\n@@ -69,9 +70,8 @@ private PAssertStreaming() {\n         .apply(ParDo.of(new AssertDoFn<>(expected)));\n \n     // run the pipeline.\n-    EvaluationResult res = (EvaluationResult) p.run();\n+    SparkPipelineResult res = (SparkPipelineResult) p.run();\n     res.waitUntilFinish(timeout);\n-    res.close(stopGracefully);\n     // validate assertion succeeded (at least once).\n     int success = res.getAggregatorValue(PAssert.SUCCESS_COUNTER, Integer.class);\n     Assert.assertThat(\"Success aggregator should be greater than zero.\", success, not(0));\n@@ -87,14 +87,15 @@ private PAssertStreaming() {\n    * Default to stop gracefully so that tests will finish processing even if slower for reasons\n    * such as a slow runtime environment.\n    */\n-  public static <T> EvaluationResult runAndAssertContents(Pipeline p,\n-                                                          PCollection<T> actual,\n-                                                          T[] expected,\n-                                                          Duration timeout) {\n+  public static <T> SparkPipelineResult runAndAssertContents(\n+      Pipeline p,\n+      PCollection<T> actual,\n+      T[] expected,\n+      Duration timeout) {\n     return runAndAssertContents(p, actual, expected, timeout, true);\n   }\n \n-  private static class AssertDoFn<T> extends OldDoFn<Iterable<T>, Void> {\n+  private static class AssertDoFn<T> extends DoFn<Iterable<T>, Void> {\n     private final Aggregator<Integer, Integer> success =\n         createAggregator(PAssert.SUCCESS_COUNTER, new Sum.SumIntegerFn());\n     private final Aggregator<Integer, Integer> failure =\n@@ -105,7 +106,7 @@ private PAssertStreaming() {\n       this.expected = expected;\n     }\n \n-    @Override\n+    @ProcessElement\n     public void processElement(ProcessContext c) throws Exception {\n       try {\n         assertThat(c.element(), containsInAnyOrder(expected));",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/utils/PAssertStreaming.java",
                "sha": "0284b3d190fd6d6093d90b9fe0cbd2f75e68c160",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/build-tools/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/build-tools/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/build-tools/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../../../pom.xml</relativePath>\n   </parent>\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/build-tools/pom.xml",
                "sha": "bf89b7d0768d066ac73a6a724dac1f7d48ba942d",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/build-tools/src/main/resources/beam/findbugs-filter.xml",
                "changes": 68,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/build-tools/src/main/resources/beam/findbugs-filter.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 6,
                "filename": "sdks/java/build-tools/src/main/resources/beam/findbugs-filter.xml",
                "patch": "@@ -43,6 +43,25 @@\n     -->\n   </Match>\n \n+  <Match>\n+    <Class name=\"org.apache.beam.sdk.io.jms.JmsRecord\"/>\n+    <Field name=\"jmsDestination\"/>\n+    <Bug pattern=\"SE_BAD_FIELD\"/>\n+    <!--\n+    JMS destination is serializable according to the JMS spec even if it doesn't implement\n+    Serializable.\n+     -->\n+  </Match>\n+  <Match>\n+    <Class name=\"org.apache.beam.sdk.io.jms.JmsRecord\"/>\n+    <Field name=\"jmsReplyTo\"/>\n+    <Bug pattern=\"SE_BAD_FIELD\"/>\n+    <!--\n+    JMS ReplyTo destination is serializable according to the JMS spec even if it doesn't implement\n+    Serializable.\n+     -->\n+  </Match>\n+\n   <Match>\n     <Class name=\"org.apache.beam.sdk.coders.InstantCoder$LexicographicLongConverter\"/>\n     <Bug pattern=\"HE_INHERITS_EQUALS_USE_HASHCODE\"/>\n@@ -105,6 +124,44 @@\n     <!-- Takes ownership of input buffer -->\n   </Match>\n \n+  <Match>\n+    <Class name=\"org.apache.beam.runners.spark.util.BroadcastHelper$CodedBroadcastHelper\"/>\n+    <Or>\n+      <Field name=\"bcast\" />\n+      <Field name=\"value\" />\n+    </Or>\n+    <Bug pattern=\"IS2_INCONSISTENT_SYNC\"/>\n+    <!--\n+      Spark's Broadcast variables are a distributed and cached objects\n+      and should not be treated as \"normal\" objects.\n+    -->\n+  </Match>\n+\n+  <Match>\n+    <Class name=\"org.apache.beam.runners.spark.util.BroadcastHelper$DirectBroadcastHelper\"/>\n+    <Or>\n+      <Field name=\"bcast\" />\n+      <Field name=\"value\" />\n+    </Or>\n+    <Bug pattern=\"IS2_INCONSISTENT_SYNC\"/>\n+    <!--\n+      Spark's Broadcast variables are a distributed and cached objects\n+      and should not be treated as \"normal\" objects.\n+    -->\n+  </Match>\n+\n+  <Match>\n+    <Class name=\"org.apache.beam.runners.spark.aggregators.metrics.sink.CsvSink\"/>\n+    <Bug pattern=\"NM_SAME_SIMPLE_NAME_AS_SUPERCLASS\"/>\n+    <!-- Intentionally overriding parent name because inheritors should replace the parent. -->\n+  </Match>\n+\n+  <Match>\n+    <Class name=\"org.apache.beam.runners.spark.aggregators.metrics.sink.GraphiteSink\"/>\n+    <Bug pattern=\"NM_SAME_SIMPLE_NAME_AS_SUPERCLASS\"/>\n+    <!-- Intentionally overriding parent name because inheritors should replace the parent. -->\n+  </Match>\n+\n   <Match>\n     <Class name=\"org.apache.beam.sdk.util.ZipFiles\"/>\n     <Method name=\"zipDirectory\" />\n@@ -183,18 +240,17 @@\n     <!-- Called via reflection -->\n   </Match>\n \n-\n   <!--\n     Baseline issues. No new issues should be added below this line and all existing issues should\n     have an associated JIRA\n   -->\n \n   <Match>\n-  <Class name=\"org.apache.beam.sdk.coders.JAXBCoder\"/>\n-  <Method name=\"getContext\"/>\n-  <Bug pattern=\"DC_DOUBLECHECK\"/>\n-  <!--[BEAM-398] Possible double check of field-->\n-</Match>\n+    <Class name=\"org.apache.beam.sdk.coders.JAXBCoder\"/>\n+    <Method name=\"getContext\"/>\n+    <Bug pattern=\"DC_DOUBLECHECK\"/>\n+    <!--[BEAM-398] Possible double check of field-->\n+  </Match>\n   <Match>\n     <Class name=\"org.apache.beam.sdk.io.range.OffsetRangeTracker\"/>\n     <Field name=\"done\"/>",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/build-tools/src/main/resources/beam/findbugs-filter.xml",
                "sha": "bfb498887309c3d377430fc5ecdc6674a07f5758",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/pom.xml?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/pom.xml",
                "patch": "@@ -22,7 +22,7 @@\n   <parent>\n     <groupId>org.apache.beam</groupId>\n     <artifactId>beam-sdks-java-parent</artifactId>\n-    <version>0.4.0-incubating-SNAPSHOT</version>\n+    <version>0.5.0-incubating-SNAPSHOT</version>\n     <relativePath>../pom.xml</relativePath>\n   </parent>\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/pom.xml",
                "sha": "d8d2c2670c9c3069590548ebdcdd350b4000f748",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/AggregatorPipelineExtractor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/AggregatorPipelineExtractor.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/AggregatorPipelineExtractor.java",
                "patch": "@@ -72,7 +72,8 @@ public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n         if (transform instanceof ParDo.Bound) {\n           return AggregatorRetriever.getAggregators(((ParDo.Bound<?, ?>) transform).getFn());\n         } else if (transform instanceof ParDo.BoundMulti) {\n-          return AggregatorRetriever.getAggregators(((ParDo.BoundMulti<?, ?>) transform).getFn());\n+          return AggregatorRetriever.getAggregators(\n+              ((ParDo.BoundMulti<?, ?>) transform).getFn());\n         }\n       }\n       return Collections.emptyList();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/AggregatorPipelineExtractor.java",
                "sha": "c79f779d81bfbe86b3d7885cfeb6e6434b36a00d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/annotations/Experimental.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/annotations/Experimental.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/annotations/Experimental.java",
                "patch": "@@ -85,6 +85,9 @@\n     SPLITTABLE_DO_FN,\n \n     /** Metrics-related experimental APIs. */\n-    METRICS\n+    METRICS,\n+\n+    /** Experimental runner APIs. Should not be used by pipeline authors. */\n+    CORE_RUNNERS_ONLY\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/annotations/Experimental.java",
                "sha": "265965967889bd8337d39d5ce093196c8c33025b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/coders/CoderRegistry.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/coders/CoderRegistry.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/coders/CoderRegistry.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.beam.sdk.coders.CannotProvideCoderException.ReasonCode;\n import org.apache.beam.sdk.coders.protobuf.ProtoCoder;\n import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.IntervalWindow;\n import org.apache.beam.sdk.util.CoderUtils;\n import org.apache.beam.sdk.values.KV;\n import org.apache.beam.sdk.values.TimestampedValue;\n@@ -107,6 +108,7 @@ public void registerStandardCoders() {\n     registerCoder(TimestampedValue.class, TimestampedValue.TimestampedValueCoder.class);\n     registerCoder(Void.class, VoidCoder.class);\n     registerCoder(byte[].class, ByteArrayCoder.class);\n+    registerCoder(IntervalWindow.class, IntervalWindow.getCoder());\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/coders/CoderRegistry.java",
                "sha": "65f4209acfb165c02d77dc73e90af86d23b58ee3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroIO.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroIO.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroIO.java",
                "patch": "@@ -281,7 +281,7 @@\n       }\n \n       @Override\n-      public PCollection<T> apply(PBegin input) {\n+      public PCollection<T> expand(PBegin input) {\n         if (filepattern == null) {\n           throw new IllegalStateException(\n               \"need to set the filepattern of an AvroIO.Read transform\");\n@@ -795,7 +795,7 @@ private Read() {}\n       }\n \n       @Override\n-      public PDone apply(PCollection<T> input) {\n+      public PDone expand(PCollection<T> input) {\n         if (filenamePrefix == null) {\n           throw new IllegalStateException(\n               \"need to set the filename prefix of an AvroIO.Write transform\");",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/AvroIO.java",
                "sha": "01a4cba00d4f24937b8c6ab678793fda05c0b31c",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BoundedReadFromUnboundedSource.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BoundedReadFromUnboundedSource.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/BoundedReadFromUnboundedSource.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.NoSuchElementException;\n import java.util.concurrent.TimeUnit;\n import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.coders.Coder;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.Distinct;\n@@ -50,6 +51,7 @@\n   private final UnboundedSource<T, ?> source;\n   private final long maxNumRecords;\n   private final Duration maxReadTime;\n+  private final BoundedSource<ValueWithRecordId<T>> adaptedSource;\n   private static final FluentBackoff BACKOFF_FACTORY =\n       FluentBackoff.DEFAULT\n           .withInitialBackoff(Duration.millis(10))\n@@ -81,12 +83,22 @@\n     this.source = source;\n     this.maxNumRecords = maxNumRecords;\n     this.maxReadTime = maxReadTime;\n+    this.adaptedSource = new UnboundedToBoundedSourceAdapter<>(source, maxNumRecords, maxReadTime);\n+  }\n+\n+  /**\n+   * Returns an adapted {@link BoundedSource} wrapping the underlying {@link UnboundedSource},\n+   * with the specified bounds on number of records and read time.\n+   */\n+  @Experimental\n+  public BoundedSource<ValueWithRecordId<T>> getAdaptedSource() {\n+    return adaptedSource;\n   }\n \n   @Override\n-  public PCollection<T> apply(PBegin input) {\n+  public PCollection<T> expand(PBegin input) {\n     PCollection<ValueWithRecordId<T>> read = Pipeline.applyTransform(input,\n-        Read.from(new UnboundedToBoundedSourceAdapter<>(source, maxNumRecords, maxReadTime)));\n+        Read.from(getAdaptedSource()));\n     if (source.requiresDeduping()) {\n       read = read.apply(Distinct.withRepresentativeValueFn(\n           new SerializableFunction<ValueWithRecordId<T>, byte[]>() {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/BoundedReadFromUnboundedSource.java",
                "sha": "84e3044337849924e137150de80db0550df270fe",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingInput.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingInput.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 11,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingInput.java",
                "patch": "@@ -35,22 +35,26 @@\n /**\n  * A {@link PTransform} that produces longs. When used to produce a\n  * {@link IsBounded#BOUNDED bounded} {@link PCollection}, {@link CountingInput} starts at {@code 0}\n- * and counts up to a specified maximum. When used to produce an\n+ * or starting value, and counts up to a specified maximum. When used to produce an\n  * {@link IsBounded#UNBOUNDED unbounded} {@link PCollection}, it counts up to {@link Long#MAX_VALUE}\n  * and then never produces more output. (In practice, this limit should never be reached.)\n  *\n  * <p>The bounded {@link CountingInput} is implemented based on {@link OffsetBasedSource} and\n  * {@link OffsetBasedSource.OffsetBasedReader}, so it performs efficient initial splitting and it\n  * supports dynamic work rebalancing.\n  *\n- * <p>To produce a bounded {@code PCollection<Long>}, use {@link CountingInput#upTo(long)}:\n+ * <p>To produce a bounded {@code PCollection<Long>} starting from {@code 0},\n+ * use {@link CountingInput#upTo(long)}:\n  *\n  * <pre>{@code\n  * Pipeline p = ...\n  * PTransform<PBegin, PCollection<Long>> producer = CountingInput.upTo(1000);\n  * PCollection<Long> bounded = p.apply(producer);\n  * }</pre>\n  *\n+ * <p>To produce a bounded {@code PCollection<Long>} starting from {@code startOffset},\n+ * use {@link CountingInput#forSubrange(long, long)} instead.\n+ *\n  * <p>To produce an unbounded {@code PCollection<Long>}, use {@link CountingInput#unbounded()},\n  * calling {@link UnboundedCountingInput#withTimestampFn(SerializableFunction)} to provide values\n  * with timestamps other than {@link Instant#now}.\n@@ -71,10 +75,24 @@\n    * from {@code 0} to {@code numElements - 1}.\n    */\n   public static BoundedCountingInput upTo(long numElements) {\n-    checkArgument(numElements > 0, \"numElements (%s) must be greater than 0\", numElements);\n+    checkArgument(numElements >= 0,\n+        \"numElements (%s) must be greater than or equal to 0\",\n+        numElements);\n     return new BoundedCountingInput(numElements);\n   }\n \n+  /**\n+   * Creates a {@link BoundedCountingInput} that will produce elements\n+   * starting from {@code startIndex} (inclusive) to {@code endIndex} (exclusive).\n+   * If {@code startIndex == endIndex}, then no elements will be produced.\n+   */\n+  public static BoundedCountingInput forSubrange(long startIndex, long endIndex) {\n+    checkArgument(endIndex >= startIndex,\n+        \"endIndex (%s) must be greater than or equal to startIndex (%s)\",\n+        endIndex, startIndex);\n+    return new BoundedCountingInput(startIndex, endIndex);\n+  }\n+\n   /**\n    * Creates an {@link UnboundedCountingInput} that will produce numbers starting from {@code 0} up\n    * to {@link Long#MAX_VALUE}.\n@@ -102,23 +120,35 @@ public static UnboundedCountingInput unbounded() {\n    * 0.\n    */\n   public static class BoundedCountingInput extends PTransform<PBegin, PCollection<Long>> {\n-    private final long numElements;\n+    private final long startIndex;\n+    private final long endIndex;\n \n     private BoundedCountingInput(long numElements) {\n-      this.numElements = numElements;\n+      this.endIndex = numElements;\n+      this.startIndex = 0;\n+    }\n+\n+    private BoundedCountingInput(long startIndex, long endIndex) {\n+      this.endIndex = endIndex;\n+      this.startIndex = startIndex;\n     }\n \n-    @SuppressWarnings(\"deprecation\")\n     @Override\n-    public PCollection<Long> apply(PBegin begin) {\n-      return begin.apply(Read.from(CountingSource.upTo(numElements)));\n+    public PCollection<Long> expand(PBegin begin) {\n+      return begin.apply(Read.from(CountingSource.createSourceForSubrange(startIndex, endIndex)));\n     }\n \n     @Override\n     public void populateDisplayData(DisplayData.Builder builder) {\n       super.populateDisplayData(builder);\n-      builder.add(DisplayData.item(\"upTo\", numElements)\n-        .withLabel(\"Count Up To\"));\n+\n+      if (startIndex == 0) {\n+            builder.add(DisplayData.item(\"upTo\", endIndex)\n+                .withLabel(\"Count Up To\"));\n+      } else {\n+            builder.add(DisplayData.item(\"startAt\", startIndex).withLabel(\"Count Starting At\"))\n+                    .add(DisplayData.item(\"upTo\", endIndex).withLabel(\"Count Up To\"));\n+        }\n     }\n   }\n \n@@ -210,7 +240,7 @@ public UnboundedCountingInput withMaxReadTime(Duration readTime) {\n \n     @SuppressWarnings(\"deprecation\")\n     @Override\n-    public PCollection<Long> apply(PBegin begin) {\n+    public PCollection<Long> expand(PBegin begin) {\n       Unbounded<Long> read =\n           Read.from(\n               CountingSource.createUnbounded()",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingInput.java",
                "sha": "ac70aca3418e33808b45fdd61c2dac05116e6877",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingSource.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingSource.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingSource.java",
                "patch": "@@ -78,10 +78,25 @@\n    */\n   @Deprecated\n   public static BoundedSource<Long> upTo(long numElements) {\n-    checkArgument(numElements > 0, \"numElements (%s) must be greater than 0\", numElements);\n+    checkArgument(numElements >= 0,\n+        \"numElements (%s) must be greater than or equal to 0\",\n+        numElements);\n     return new BoundedCountingSource(0, numElements);\n   }\n \n+  /**\n+   * Creates a {@link BoundedSource} that will produce elements\n+   * starting from {@code startIndex} (inclusive) to {@code endIndex} (exclusive).\n+   * If {@code startIndex == endIndex}, then no elements will be produced.\n+   */\n+  static BoundedSource<Long> createSourceForSubrange(long startIndex, long endIndex) {\n+    checkArgument(endIndex >= startIndex,\n+        \"endIndex (%s) must be greater than or equal to startIndex (%s)\",\n+        endIndex, startIndex);\n+\n+    return new BoundedCountingSource(startIndex, endIndex);\n+  }\n+\n   /**\n    * Create a new {@link UnboundedCountingSource}.\n    */",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/CountingSource.java",
                "sha": "9752dba84f21b6758c48ba4ba4f74ec3afed5a3c",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 25,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java",
                "patch": "@@ -30,7 +30,6 @@\n import java.nio.channels.WritableByteChannel;\n import java.nio.file.Path;\n import java.util.ArrayList;\n-import java.util.Collection;\n import java.util.HashSet;\n import java.util.List;\n import java.util.Set;\n@@ -42,7 +41,9 @@\n import org.apache.beam.sdk.coders.SerializableCoder;\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.options.ValueProvider.NestedValueProvider;\n import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n import org.apache.beam.sdk.transforms.display.DisplayData;\n import org.apache.beam.sdk.util.IOChannelFactory;\n import org.apache.beam.sdk.util.IOChannelUtils;\n@@ -201,8 +202,8 @@ public FileBasedSink(ValueProvider<String> baseOutputFilename, String extension,\n   /**\n    * Returns the base output filename for this file based sink.\n    */\n-  public String getBaseOutputFilename() {\n-    return baseOutputFilename.get();\n+  public ValueProvider<String> getBaseOutputFilenameProvider() {\n+    return baseOutputFilename;\n   }\n \n   @Override\n@@ -290,7 +291,7 @@ private static String getFileExtension(String usersExtension) {\n     protected final FileBasedSink<T> sink;\n \n     /** Directory for temporary output files. */\n-    protected final String tempDirectory;\n+    protected final ValueProvider<String> tempDirectory;\n \n     /** Constructs a temporary file path given the temporary directory and a filename. */\n     protected static String buildTemporaryFilename(String tempDirectory, String filename)\n@@ -308,22 +309,31 @@ protected static String buildTemporaryFilename(String tempDirectory, String file\n      * @param sink the FileBasedSink that will be used to configure this write operation.\n      */\n     public FileBasedWriteOperation(FileBasedSink<T> sink) {\n-      this(sink, buildTemporaryDirectoryName(sink.getBaseOutputFilename()));\n+      this(sink, NestedValueProvider.of(\n+          sink.getBaseOutputFilenameProvider(), new TemporaryDirectoryBuilder()));\n     }\n \n-    private static String buildTemporaryDirectoryName(String baseOutputFilename) {\n-      try {\n-        IOChannelFactory factory = IOChannelUtils.getFactory(baseOutputFilename);\n-        Path baseOutputPath = factory.toPath(baseOutputFilename);\n-        return baseOutputPath\n-            .resolveSibling(\n-                \"temp-beam-\"\n-                    + baseOutputPath.getFileName()\n-                    + \"-\"\n-                    + Instant.now().toString(DateTimeFormat.forPattern(\"yyyy-MM-DD_HH-mm-ss\")))\n-            .toString();\n-      } catch (IOException e) {\n-        throw new RuntimeException(e);\n+    private static class TemporaryDirectoryBuilder\n+        implements SerializableFunction<String, String> {\n+      // The intent of the code is to have a consistent value of tempDirectory across\n+      // all workers, which wouldn't happen if now() was called inline.\n+      Instant now = Instant.now();\n+\n+      @Override\n+      public String apply(String baseOutputFilename) {\n+        try {\n+          IOChannelFactory factory = IOChannelUtils.getFactory(baseOutputFilename);\n+          Path baseOutputPath = factory.toPath(baseOutputFilename);\n+          return baseOutputPath\n+              .resolveSibling(\n+                  \"temp-beam-\"\n+                  + baseOutputPath.getFileName()\n+                  + \"-\"\n+                  + now.toString(DateTimeFormat.forPattern(\"yyyy-MM-DD_HH-mm-ss\")))\n+              .toString();\n+        } catch (IOException e) {\n+          throw new RuntimeException(e);\n+        }\n       }\n     }\n \n@@ -334,6 +344,10 @@ private static String buildTemporaryDirectoryName(String baseOutputFilename) {\n      * @param tempDirectory the base directory to be used for temporary output files.\n      */\n     public FileBasedWriteOperation(FileBasedSink<T> sink, String tempDirectory) {\n+      this(sink, StaticValueProvider.of(tempDirectory));\n+    }\n+\n+    private FileBasedWriteOperation(FileBasedSink<T> sink, ValueProvider<String> tempDirectory) {\n       this.sink = sink;\n       this.tempDirectory = tempDirectory;\n     }\n@@ -452,26 +466,39 @@ public void finalize(Iterable<FileResult> writerResults, PipelineOptions options\n      */\n     protected final void removeTemporaryFiles(List<String> knownFiles, PipelineOptions options)\n         throws IOException {\n-      LOG.debug(\"Removing temporary bundle output files in {}.\", tempDirectory);\n-      IOChannelFactory factory = IOChannelUtils.getFactory(tempDirectory);\n+      String tempDir = tempDirectory.get();\n+      LOG.debug(\"Removing temporary bundle output files in {}.\", tempDir);\n+      IOChannelFactory factory = IOChannelUtils.getFactory(tempDir);\n \n       // To partially mitigate the effects of filesystems with eventually-consistent\n       // directory matching APIs, we remove not only files that the filesystem says exist\n       // in the directory (which may be incomplete), but also files that are known to exist\n       // (produced by successfully completed bundles).\n       // This may still fail to remove temporary outputs of some failed bundles, but at least\n       // the common case (where all bundles succeed) is guaranteed to be fully addressed.\n-      Collection<String> matches = factory.match(factory.resolve(tempDirectory, \"*\"));\n+      Set<String> matches = new HashSet<>();\n+      // TODO: Windows OS cannot resolves and matches '*' in the path,\n+      // ignore the exception for now to avoid failing the pipeline.\n+      try {\n+        matches.addAll(factory.match(factory.resolve(tempDir, \"*\")));\n+      } catch (Exception e) {\n+        LOG.warn(\"Failed to match temporary files under: [{}].\", tempDir);\n+      }\n       Set<String> allMatches = new HashSet<>(matches);\n       allMatches.addAll(knownFiles);\n       LOG.debug(\n           \"Removing {} temporary files found under {} ({} matched glob, {} known files)\",\n           allMatches.size(),\n-          tempDirectory,\n+          tempDir,\n           matches.size(),\n           allMatches.size() - matches.size());\n-      factory.remove(allMatches);\n-      factory.remove(ImmutableList.of(tempDirectory));\n+      // Deletion of the temporary directory might fail, if not all temporary files are removed.\n+      try {\n+        factory.remove(allMatches);\n+        factory.remove(ImmutableList.of(tempDir));\n+      } catch (Exception e) {\n+        LOG.warn(\"Failed to remove temporary directory: [{}].\", tempDir);\n+      }\n     }\n \n     /**\n@@ -569,7 +596,7 @@ protected void writeFooter() throws Exception {}\n     public final void open(String uId) throws Exception {\n       this.id = uId;\n       filename = FileBasedWriteOperation.buildTemporaryFilename(\n-          getWriteOperation().tempDirectory, uId);\n+          getWriteOperation().tempDirectory.get(), uId);\n       LOG.debug(\"Opening {}.\", filename);\n       final WritableByteChannelFactory factory =\n           getWriteOperation().getSink().writableByteChannelFactory;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSink.java",
                "sha": "32b8b4f11a5e5a5e3dded83fb8fdb61e73c267a0",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java",
                "patch": "@@ -331,7 +331,11 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       try {\n         checkState(fileOrPatternSpec.isAccessible(),\n                    \"Bundle splitting should only happen at execution time.\");\n-        for (final String file : FileBasedSource.expandFilePattern(fileOrPatternSpec.get())) {\n+        Collection<String> expandedFiles =\n+            FileBasedSource.expandFilePattern(fileOrPatternSpec.get());\n+        checkArgument(!expandedFiles.isEmpty(),\n+            \"Unable to find any files matching %s\", fileOrPatternSpec.get());\n+        for (final String file : expandedFiles) {\n           futures.add(createFutureForFileSplit(file, desiredBundleSizeBytes, options, service));\n         }\n         List<? extends FileBasedSource<T>> splitResults =\n@@ -411,11 +415,13 @@ protected boolean isSplittable() throws Exception {\n \n   @Override\n   public String toString() {\n+    String fileString = fileOrPatternSpec.isAccessible()\n+        ? fileOrPatternSpec.get() : fileOrPatternSpec.toString();\n     switch (mode) {\n       case FILEPATTERN:\n-        return fileOrPatternSpec.toString();\n+        return fileString;\n       case SINGLE_FILE_OR_SUBRANGE:\n-        return fileOrPatternSpec.toString() + \" range \" + super.toString();\n+        return fileString + \" range \" + super.toString();\n       default:\n         throw new IllegalStateException(\"Unexpected mode: \" + mode);\n     }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java",
                "sha": "5659d5b7d15412defd0230056ab1decf274b54aa",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystem.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystem.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystem.java",
                "patch": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+/**\n+ * File system interface in Beam.\n+ *\n+ * <p>It defines APIs for writing file systems agnostic code.\n+ *\n+ * <p>All methods are protected, and they are for file system providers to implement.\n+ * Clients should use {@link FileSystems} utility.\n+ */\n+public abstract class FileSystem {\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystem.java",
                "sha": "d99040346a954dc9a36355405a829848533cf8f2",
                "status": "added"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystemRegistrar.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystemRegistrar.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystemRegistrar.java",
                "patch": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import com.google.auto.service.AutoService;\n+import java.util.ServiceLoader;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+\n+/**\n+ * A registrar that creates {@link FileSystem} instances from {@link PipelineOptions}.\n+ *\n+ * <p>{@link FileSystem} creators have the ability to provide a registrar by creating\n+ * a {@link ServiceLoader} entry and a concrete implementation of this interface.\n+ *\n+ * <p>It is optional but recommended to use one of the many build time tools such as\n+ * {@link AutoService} to generate the necessary META-INF files automatically.\n+ */\n+public interface FileSystemRegistrar {\n+  /**\n+   * Create a {@link FileSystem} from the given {@link PipelineOptions}.\n+   */\n+  FileSystem fromOptions(@Nullable PipelineOptions options);\n+\n+  /**\n+   * Get the URI scheme which defines the namespace of the {@link FileSystemRegistrar}.\n+   *\n+   * <p>The scheme is required to be unique among all\n+   * {@link FileSystemRegistrar FileSystemRegistrars}.\n+   *\n+   * @see <a href=\"https://www.ietf.org/rfc/rfc2396.txt\">RFC 2396</a>\n+   */\n+  String getScheme();\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystemRegistrar.java",
                "sha": "1d81c1e36428143ac67cbd2afc8d1a417ed5f3cc",
                "status": "added"
            },
            {
                "additions": 155,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystems.java",
                "changes": 155,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystems.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystems.java",
                "patch": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Function;\n+import com.google.common.base.Joiner;\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Multimap;\n+import com.google.common.collect.Ordering;\n+import com.google.common.collect.Sets;\n+import com.google.common.collect.TreeMultimap;\n+import java.net.URI;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.ServiceLoader;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.regex.Pattern;\n+import javax.annotation.Nonnull;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.util.common.ReflectHelpers;\n+\n+/**\n+ * Clients facing {@link FileSystem} utility.\n+ */\n+public class FileSystems {\n+\n+  public static final String DEFAULT_SCHEME = \"default\";\n+\n+  private static final Pattern URI_SCHEME_PATTERN = Pattern.compile(\"^[a-zA-Z][-a-zA-Z0-9+.]*$\");\n+\n+  private static final Map<String, FileSystemRegistrar> SCHEME_TO_REGISTRAR =\n+      new ConcurrentHashMap<>();\n+\n+  private static final Map<String, PipelineOptions> SCHEME_TO_DEFAULT_CONFIG =\n+      new ConcurrentHashMap<>();\n+\n+  static {\n+    loadFileSystemRegistrars();\n+  }\n+\n+  /**\n+   * Loads available {@link FileSystemRegistrar} services.\n+   */\n+  private static void loadFileSystemRegistrars() {\n+    SCHEME_TO_REGISTRAR.clear();\n+    Set<FileSystemRegistrar> registrars =\n+        Sets.newTreeSet(ReflectHelpers.ObjectsClassComparator.INSTANCE);\n+    registrars.addAll(Lists.newArrayList(\n+        ServiceLoader.load(FileSystemRegistrar.class, ReflectHelpers.findClassLoader())));\n+\n+    verifySchemesAreUnique(registrars);\n+\n+    for (FileSystemRegistrar registrar : registrars) {\n+      SCHEME_TO_REGISTRAR.put(registrar.getScheme().toLowerCase(), registrar);\n+    }\n+  }\n+\n+  /**\n+   * Sets the default configuration to be used with a {@link FileSystemRegistrar} for the provided\n+   * {@code scheme}.\n+   *\n+   * <p>Syntax: <pre>scheme = alpha *( alpha | digit | \"+\" | \"-\" | \".\" )</pre>\n+   * Upper case letters are treated as the same as lower case letters.\n+   */\n+  public static void setDefaultConfig(String scheme, PipelineOptions options) {\n+    String lowerCaseScheme = checkNotNull(scheme, \"scheme\").toLowerCase();\n+    checkArgument(\n+        URI_SCHEME_PATTERN.matcher(lowerCaseScheme).matches(),\n+        String.format(\"Scheme: [%s] doesn't match URI syntax: %s\",\n+            lowerCaseScheme, URI_SCHEME_PATTERN.pattern()));\n+    checkArgument(\n+        SCHEME_TO_REGISTRAR.containsKey(lowerCaseScheme),\n+        String.format(\"No FileSystemRegistrar found for scheme: [%s].\", lowerCaseScheme));\n+    SCHEME_TO_DEFAULT_CONFIG.put(lowerCaseScheme, checkNotNull(options, \"options\"));\n+  }\n+\n+  @VisibleForTesting\n+  static PipelineOptions getDefaultConfig(String scheme) {\n+    return SCHEME_TO_DEFAULT_CONFIG.get(scheme.toLowerCase());\n+  }\n+\n+  /**\n+   * Internal method to get {@link FileSystem} for {@code spec}.\n+   */\n+  @VisibleForTesting\n+  static FileSystem getFileSystemInternal(URI uri) {\n+    String lowerCaseScheme = (uri.getScheme() != null\n+        ? uri.getScheme().toLowerCase() : LocalFileSystemRegistrar.LOCAL_FILE_SCHEME);\n+    return getRegistrarInternal(lowerCaseScheme).fromOptions(getDefaultConfig(lowerCaseScheme));\n+  }\n+\n+  /**\n+   * Internal method to get {@link FileSystemRegistrar} for {@code scheme}.\n+   */\n+  @VisibleForTesting\n+  static FileSystemRegistrar getRegistrarInternal(String scheme) {\n+    String lowerCaseScheme = scheme.toLowerCase();\n+    if (SCHEME_TO_REGISTRAR.containsKey(lowerCaseScheme)) {\n+      return SCHEME_TO_REGISTRAR.get(lowerCaseScheme);\n+    } else if (SCHEME_TO_REGISTRAR.containsKey(DEFAULT_SCHEME)) {\n+      return SCHEME_TO_REGISTRAR.get(DEFAULT_SCHEME);\n+    } else {\n+      throw new IllegalStateException(\"Unable to find registrar for \" + scheme);\n+    }\n+  }\n+\n+  @VisibleForTesting\n+  static void verifySchemesAreUnique(Set<FileSystemRegistrar> registrars) {\n+    Multimap<String, FileSystemRegistrar> registrarsBySchemes =\n+        TreeMultimap.create(Ordering.<String>natural(), Ordering.arbitrary());\n+\n+    for (FileSystemRegistrar registrar : registrars) {\n+      registrarsBySchemes.put(registrar.getScheme().toLowerCase(), registrar);\n+    }\n+    for (Entry<String, Collection<FileSystemRegistrar>> entry\n+        : registrarsBySchemes.asMap().entrySet()) {\n+      if (entry.getValue().size() > 1) {\n+        String conflictingRegistrars = Joiner.on(\", \").join(\n+            FluentIterable.from(entry.getValue())\n+                .transform(new Function<FileSystemRegistrar, String>() {\n+                  @Override\n+                  public String apply(@Nonnull FileSystemRegistrar input) {\n+                    return input.getClass().getName();\n+                  }})\n+                .toSortedList(Ordering.<String>natural()));\n+        throw new IllegalStateException(String.format(\n+            \"Scheme: [%s] has conflicting registrars: [%s]\",\n+            entry.getKey(),\n+            conflictingRegistrars));\n+      }\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileSystems.java",
                "sha": "d086ec62a7a79268ebcde02a1cc952d7b6f16971",
                "status": "added"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java",
                "patch": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+/**\n+ * {@link FileSystem} implementation for local files.\n+ */\n+class LocalFileSystem extends FileSystem {\n+\n+  LocalFileSystem() {\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystem.java",
                "sha": "23c2a920431fa41ffb801faa6972c52f52d2940a",
                "status": "added"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystemRegistrar.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystemRegistrar.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystemRegistrar.java",
                "patch": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io;\n+\n+import com.google.auto.service.AutoService;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+\n+/**\n+ * {@link AutoService} registrar for the {@link FileSystem}.\n+ */\n+@AutoService(FileSystemRegistrar.class)\n+public class LocalFileSystemRegistrar implements FileSystemRegistrar {\n+\n+  static final String LOCAL_FILE_SCHEME = \"file\";\n+\n+  @Override\n+  public FileSystem fromOptions(@Nullable PipelineOptions options) {\n+    return new LocalFileSystem();\n+  }\n+\n+  @Override\n+  public String getScheme() {\n+    return LOCAL_FILE_SCHEME;\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/LocalFileSystemRegistrar.java",
                "sha": "75a38e8c922eb568d668e1edd2e3681e3ada8d9b",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubIO.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubIO.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubIO.java",
                "patch": "@@ -700,7 +700,7 @@ private Bound(String name, ValueProvider<PubsubSubscription> subscription,\n       }\n \n       @Override\n-      public PCollection<T> apply(PBegin input) {\n+      public PCollection<T> expand(PBegin input) {\n         if (topic == null && subscription == null) {\n           throw new IllegalStateException(\"Need to set either the topic or the subscription for \"\n               + \"a PubsubIO.Read transform\");\n@@ -1057,7 +1057,7 @@ private Bound(\n       }\n \n       @Override\n-      public PDone apply(PCollection<T> input) {\n+      public PDone expand(PCollection<T> input) {\n         if (topic == null) {\n           throw new IllegalStateException(\"need to set the topic of a PubsubIO.Write transform\");\n         }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubIO.java",
                "sha": "9a6b5348e5031f2030bcf583f0f3e9889be5f663",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSink.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSink.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSink.java",
                "patch": "@@ -405,6 +405,10 @@ public TopicPath getTopic() {\n     return topic.get();\n   }\n \n+  public ValueProvider<TopicPath> getTopicProvider() {\n+    return topic;\n+  }\n+\n   @Nullable\n   public String getTimestampLabel() {\n     return timestampLabel;\n@@ -420,7 +424,7 @@ public String getIdLabel() {\n   }\n \n   @Override\n-  public PDone apply(PCollection<T> input) {\n+  public PDone expand(PCollection<T> input) {\n     input.apply(\"PubsubUnboundedSink.Window\", Window.<T>into(new GlobalWindows())\n         .triggering(\n             Repeatedly.forever(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSink.java",
                "sha": "1992cb8f535a70c5ff1ed156e96cd85025358fdf",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSource.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSource.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSource.java",
                "patch": "@@ -1313,11 +1313,21 @@ public TopicPath getTopic() {\n     return topic == null ? null : topic.get();\n   }\n \n+  @Nullable\n+  public ValueProvider<TopicPath> getTopicProvider() {\n+    return topic;\n+  }\n+\n   @Nullable\n   public SubscriptionPath getSubscription() {\n     return subscription == null ? null : subscription.get();\n   }\n \n+  @Nullable\n+  public ValueProvider<SubscriptionPath> getSubscriptionProvider() {\n+    return subscription;\n+  }\n+\n   @Nullable\n   public String getTimestampLabel() {\n     return timestampLabel;\n@@ -1329,7 +1339,7 @@ public String getIdLabel() {\n   }\n \n   @Override\n-  public PCollection<T> apply(PBegin input) {\n+  public PCollection<T> expand(PBegin input) {\n     return input.getPipeline().begin()\n                 .apply(Read.from(new PubsubSource<T>(this)))\n                 .apply(\"PubsubUnboundedSource.Stats\",",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/PubsubUnboundedSource.java",
                "sha": "da3b4375ae71b509b9cf7fdcef54fef9710723d9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java",
                "patch": "@@ -101,7 +101,7 @@ private Bounded(@Nullable String name, BoundedSource<T> source) {\n     }\n \n     @Override\n-    public final PCollection<T> apply(PBegin input) {\n+    public final PCollection<T> expand(PBegin input) {\n       source.validate();\n \n       return PCollection.<T>createPrimitiveOutputInternal(input.getPipeline(),\n@@ -169,7 +169,7 @@ private Unbounded(@Nullable String name, UnboundedSource<T, ?> source) {\n     }\n \n     @Override\n-    public final PCollection<T> apply(PBegin input) {\n+    public final PCollection<T> expand(PBegin input) {\n       source.validate();\n \n       return PCollection.<T>createPrimitiveOutputInternal(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java",
                "sha": "7ec3b0edd5ff29bd8f04f8a67ced292e2e1ac6d3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java",
                "patch": "@@ -292,7 +292,7 @@ private Bound(@Nullable String name, @Nullable ValueProvider<String> filepattern\n       }\n \n       @Override\n-      public PCollection<T> apply(PBegin input) {\n+      public PCollection<T> expand(PBegin input) {\n         if (filepattern == null) {\n           throw new IllegalStateException(\"need to set the filepattern of a TextIO.Read transform\");\n         }\n@@ -742,7 +742,7 @@ private Bound(String name, ValueProvider<String> filenamePrefix, String filename\n       }\n \n       @Override\n-      public PDone apply(PCollection<T> input) {\n+      public PDone expand(PCollection<T> input) {\n         if (filenamePrefix == null) {\n           throw new IllegalStateException(\n               \"need to set the filename prefix of a TextIO.Write transform\");",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java",
                "sha": "54e73d5cceb90f69b0cc24f7061ca98867157416",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Write.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Write.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/Write.java",
                "patch": "@@ -104,7 +104,7 @@ private Bound(Sink<T> sink, int numShards) {\n     }\n \n     @Override\n-    public PDone apply(PCollection<T> input) {\n+    public PDone expand(PCollection<T> input) {\n       checkArgument(IsBounded.BOUNDED == input.isBounded(),\n           \"%s can only be applied to a Bounded PCollection\",\n           Write.class.getSimpleName());",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Write.java",
                "sha": "bc651d8d2769920ac3be15509a0afe1574ce96f7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRange.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRange.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRange.java",
                "patch": "@@ -69,7 +69,7 @@\n  * @see ByteKey\n  */\n public final class ByteKeyRange implements Serializable {\n-  private static final Logger logger = LoggerFactory.getLogger(ByteKeyRange.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(ByteKeyRange.class);\n \n   /** The range of all keys, with empty start and end keys. */\n   public static final ByteKeyRange ALL_KEYS = ByteKeyRange.of(ByteKey.EMPTY, ByteKey.EMPTY);\n@@ -191,7 +191,7 @@ public double estimateFractionForKey(ByteKey key) {\n     // Keys are equal subject to padding by 0.\n     BigInteger range = rangeEndInt.subtract(rangeStartInt);\n     if (range.equals(BigInteger.ZERO)) {\n-      logger.warn(\n+      LOG.warn(\n           \"Using 0.0 as the default fraction for this near-empty range {} where start and end keys\"\n               + \" differ only by trailing zeros.\",\n           this);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRange.java",
                "sha": "0212e8a3a20f1ac5da8d445794b98bd89406cd11",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRangeTracker.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRangeTracker.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRangeTracker.java",
                "patch": "@@ -32,7 +32,7 @@\n  * @see ByteKeyRange\n  */\n public final class ByteKeyRangeTracker implements RangeTracker<ByteKey> {\n-  private static final Logger logger = LoggerFactory.getLogger(ByteKeyRangeTracker.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(ByteKeyRangeTracker.class);\n \n   /** Instantiates a new {@link ByteKeyRangeTracker} with the specified range. */\n   public static ByteKeyRangeTracker of(ByteKeyRange range) {\n@@ -89,7 +89,7 @@ public synchronized boolean tryReturnRecordAt(boolean isAtSplitPoint, ByteKey re\n   public synchronized boolean trySplitAtPosition(ByteKey splitPosition) {\n     // Unstarted.\n     if (position == null) {\n-      logger.warn(\n+      LOG.warn(\n           \"{}: Rejecting split request at {} because no records have been returned.\",\n           this,\n           splitPosition);\n@@ -98,7 +98,7 @@ public synchronized boolean trySplitAtPosition(ByteKey splitPosition) {\n \n     // Started, but not after current position.\n     if (splitPosition.compareTo(position) <= 0) {\n-      logger.warn(\n+      LOG.warn(\n           \"{}: Rejecting split request at {} because it is not after current position {}.\",\n           this,\n           splitPosition,\n@@ -108,7 +108,7 @@ public synchronized boolean trySplitAtPosition(ByteKey splitPosition) {\n \n     // Sanity check.\n     if (!range.containsKey(splitPosition)) {\n-      logger.warn(\n+      LOG.warn(\n           \"{}: Rejecting split request at {} because it is not within the range.\",\n           this,\n           splitPosition);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/io/range/ByteKeyRangeTracker.java",
                "sha": "99717a4bffa9be62bc81d160bb260758848aeb09",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/CounterCell.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/CounterCell.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/CounterCell.java",
                "patch": "@@ -26,13 +26,21 @@\n  *\n  * <p>This class generally shouldn't be used directly. The only exception is within a runner where\n  * a counter is being reported for a specific step (rather than the counter in the current context).\n+ * In that case retrieving the underlying cell and reporting directly to it avoids a step of\n+ * indirection.\n  */\n @Experimental(Kind.METRICS)\n-class CounterCell implements MetricCell<Counter, Long>, Counter {\n+public class CounterCell implements MetricCell<Counter, Long>, Counter {\n \n   private final DirtyState dirty = new DirtyState();\n   private final AtomicLong value = new AtomicLong();\n \n+  /**\n+   * Package-visibility because all {@link CounterCell CounterCells} should be created by\n+   * {@link MetricsContainer#getCounter(MetricName)}.\n+   */\n+  CounterCell() {}\n+\n   /** Increment the counter by the given amount. */\n   private void add(long n) {\n     value.addAndGet(n);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/CounterCell.java",
                "sha": "93700e6efe50149cf98143fbb8edc753dc929c66",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/DistributionCell.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/DistributionCell.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/DistributionCell.java",
                "patch": "@@ -23,14 +23,25 @@\n \n /**\n  * Tracks the current value (and delta) for a Distribution metric.\n+ *\n+ * <p>This class generally shouldn't be used directly. The only exception is within a runner where\n+ * a distribution is being reported for a specific step (rather than the distribution in the current\n+ * context). In that case retrieving the underlying cell and reporting directly to it avoids a step\n+ * of indirection.\n  */\n @Experimental(Kind.METRICS)\n-class DistributionCell implements MetricCell<Distribution, DistributionData>, Distribution {\n+public class DistributionCell implements MetricCell<Distribution, DistributionData>, Distribution {\n \n   private final DirtyState dirty = new DirtyState();\n   private final AtomicReference<DistributionData> value =\n       new AtomicReference<DistributionData>(DistributionData.EMPTY);\n \n+  /**\n+   * Package-visibility because all {@link DistributionCell DistributionCells} should be created by\n+   * {@link MetricsContainer#getDistribution(MetricName)}.\n+   */\n+  DistributionCell() {}\n+\n   /** Increment the counter by the given amount. */\n   @Override\n   public void update(long n) {\n@@ -56,3 +67,4 @@ public Distribution getInterface() {\n     return this;\n   }\n }\n+",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/DistributionCell.java",
                "sha": "7f684a86b405b1869e1566edb6729a4f4a517a71",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricCell.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricCell.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricCell.java",
                "patch": "@@ -28,7 +28,7 @@\n  * @param <DataT> The type of metric data stored (and extracted) from this cell.\n  */\n @Experimental(Kind.METRICS)\n-interface MetricCell<UserT extends Metric, DataT> {\n+public interface MetricCell<UserT extends Metric, DataT> {\n \n   /**\n    * Return the {@link DirtyState} tracking whether this metric cell contains uncommitted changes.",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricCell.java",
                "sha": "7cf9710c577e4334f664558c4f325e609f4742c6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricName.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricName.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricName.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.beam.sdk.metrics;\n \n import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.annotations.Experimental.Kind;\n \n@@ -28,7 +29,7 @@\n  */\n @Experimental(Kind.METRICS)\n @AutoValue\n-public abstract class MetricName {\n+public abstract class MetricName implements Serializable {\n \n   /** The namespace associated with this metric. */\n   public abstract String namespace();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricName.java",
                "sha": "3c7704336ed8945270088581d633b1a6737a2168",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/Metrics.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/Metrics.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/Metrics.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.sdk.metrics;\n \n+import java.io.Serializable;\n import org.apache.beam.sdk.annotations.Experimental;\n import org.apache.beam.sdk.annotations.Experimental.Kind;\n \n@@ -58,7 +59,7 @@ public static Distribution distribution(Class<?> namespace, String name) {\n   }\n \n   /** Implementation of {@link Counter} that delegates to the instance for the current context. */\n-  private static class DelegatingCounter implements Counter {\n+  private static class DelegatingCounter implements Counter, Serializable {\n     private final MetricName name;\n \n     private DelegatingCounter(MetricName name) {\n@@ -92,7 +93,7 @@ private DelegatingCounter(MetricName name) {\n   /**\n    * Implementation of {@link Distribution} that delegates to the instance for the current context.\n    */\n-  private static class DelegatingDistribution implements Distribution {\n+  private static class DelegatingDistribution implements Distribution, Serializable {\n     private final MetricName name;\n \n     private DelegatingDistribution(MetricName name) {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/Metrics.java",
                "sha": "045e076b2a784ac036f249ce7970795aeddf3ccf",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricsEnvironment.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricsEnvironment.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 4,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricsEnvironment.java",
                "patch": "@@ -41,7 +41,7 @@\n  */\n public class MetricsEnvironment {\n \n-  private static final Logger LOGGER = LoggerFactory.getLogger(MetricsContainer.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(MetricsContainer.class);\n \n   private static final AtomicBoolean METRICS_SUPPORTED = new AtomicBoolean(false);\n   private static final AtomicBoolean REPORTED_MISSING_CONTAINER = new AtomicBoolean(false);\n@@ -56,7 +56,7 @@\n    */\n   @Nullable\n   public static MetricsContainer setCurrentContainer(@Nullable MetricsContainer container) {\n-    MetricsContainer previous = getCurrentContainer();\n+    MetricsContainer previous = CONTAINER_FOR_THREAD.get();\n     if (container == null) {\n       CONTAINER_FOR_THREAD.remove();\n     } else {\n@@ -107,11 +107,11 @@ public static MetricsContainer getCurrentContainer() {\n     MetricsContainer container = CONTAINER_FOR_THREAD.get();\n     if (container == null && REPORTED_MISSING_CONTAINER.compareAndSet(false, true)) {\n       if (METRICS_SUPPORTED.get()) {\n-        LOGGER.error(\n+        LOG.error(\n             \"Unable to update metrics on the current thread. \"\n                 + \"Most likely caused by using metrics outside the managed work-execution thread.\");\n       } else {\n-        LOGGER.warn(\"Reporting metrics are not supported in the current execution environment.\");\n+        LOG.warn(\"Reporting metrics are not supported in the current execution environment.\");\n       }\n     }\n     return container;",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/metrics/MetricsEnvironment.java",
                "sha": "2942578b1e95e278010c324bae0b6c7e060034fc",
                "status": "modified"
            },
            {
                "additions": 83,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/options/PipelineOptionsFactory.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/options/PipelineOptionsFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 27,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/options/PipelineOptionsFactory.java",
                "patch": "@@ -55,7 +55,6 @@\n import java.lang.annotation.Annotation;\n import java.lang.reflect.Method;\n import java.lang.reflect.Modifier;\n-import java.lang.reflect.ParameterizedType;\n import java.lang.reflect.Proxy;\n import java.lang.reflect.Type;\n import java.util.ArrayList;\n@@ -1610,13 +1609,7 @@ public boolean apply(@Nonnull String input) {\n               throw new IllegalArgumentException(msg, e);\n             }\n           }\n-        } else if ((returnType.isArray() && (SIMPLE_TYPES.contains(returnType.getComponentType())\n-                   || returnType.getComponentType().isEnum()))\n-                   || Collection.class.isAssignableFrom(returnType)\n-                   || (returnType.equals(ValueProvider.class)\n-                       && MAPPER.getTypeFactory().constructType(\n-                         ((ParameterizedType) method.getGenericReturnType())\n-                         .getActualTypeArguments()[0]).isCollectionLikeType())) {\n+        } else if (isCollectionOrArrayOfAllowedTypes(returnType, type)) {\n           // Split any strings with \",\"\n           List<String> values = FluentIterable.from(entry.getValue())\n               .transformAndConcat(new Function<String, Iterable<String>>() {\n@@ -1626,31 +1619,21 @@ public boolean apply(@Nonnull String input) {\n                 }\n           }).toList();\n \n-          if (returnType.isArray() && !returnType.getComponentType().equals(String.class)\n-              || Collection.class.isAssignableFrom(returnType)\n-              || returnType.equals(ValueProvider.class)) {\n-            for (String value : values) {\n-              checkArgument(!value.isEmpty(),\n-                  \"Empty argument value is only allowed for String, String Array, \"\n-                            + \"and Collections of Strings, but received: %s\",\n-                            method.getGenericReturnType());\n-            }\n+          if (values.contains(\"\")) {\n+            checkEmptyStringAllowed(returnType, type, method.getGenericReturnType().toString());\n           }\n           convertedOptions.put(entry.getKey(), MAPPER.convertValue(values, type));\n-        } else if (SIMPLE_TYPES.contains(returnType) || returnType.isEnum()\n-                   || returnType.equals(ValueProvider.class)) {\n+        } else if (isSimpleType(returnType, type)) {\n           String value = Iterables.getOnlyElement(entry.getValue());\n-          checkArgument(returnType.equals(String.class) || !value.isEmpty(),\n-               \"Empty argument value is only allowed for String, String Array, \"\n-                        + \"and Collections of Strings, but received: %s\",\n-                        method.getGenericReturnType());\n+          if (value.isEmpty()) {\n+            checkEmptyStringAllowed(returnType, type, method.getGenericReturnType().toString());\n+          }\n           convertedOptions.put(entry.getKey(), MAPPER.convertValue(value, type));\n         } else {\n           String value = Iterables.getOnlyElement(entry.getValue());\n-          checkArgument(returnType.equals(String.class) || !value.isEmpty(),\n-                \"Empty argument value is only allowed for String, String Array, \"\n-                        + \"and Collections of Strings, but received: %s\",\n-                        method.getGenericReturnType());\n+          if (value.isEmpty()) {\n+            checkEmptyStringAllowed(returnType, type, method.getGenericReturnType().toString());\n+          }\n           try {\n             convertedOptions.put(entry.getKey(), MAPPER.readValue(value, type));\n           } catch (IOException e) {\n@@ -1669,6 +1652,79 @@ public boolean apply(@Nonnull String input) {\n     return convertedOptions;\n   }\n \n+\n+  /**\n+   * Returns true if the given type is one of {@code SIMPLE_TYPES} or an enum, or if the given type\n+   * is a {@link ValueProvider ValueProvider&lt;T&gt;} and {@code T} is one of {@code SIMPLE_TYPES}\n+   * or an enum.\n+   */\n+  private static boolean isSimpleType(Class<?> type, JavaType genericType) {\n+    Class<?> unwrappedType = type.equals(ValueProvider.class)\n+        ? genericType.containedType(0).getRawClass() : type;\n+    return SIMPLE_TYPES.contains(unwrappedType) || unwrappedType.isEnum();\n+  }\n+\n+  /**\n+   * Returns true if the given type is an array or {@link Collection} of {@code SIMPLE_TYPES} or\n+   * enums, or if the given type is a {@link ValueProvider ValueProvider&lt;T&gt;} and {@code T} is\n+   * an array or {@link Collection} of {@code SIMPLE_TYPES} or enums.\n+   */\n+  private static boolean isCollectionOrArrayOfAllowedTypes(Class<?> type, JavaType genericType) {\n+    JavaType containerType = type.equals(ValueProvider.class)\n+        ? genericType.containedType(0) : genericType;\n+\n+    // Check if it is an array of simple types or enum.\n+    if (containerType.getRawClass().isArray()\n+        && (SIMPLE_TYPES.contains(containerType.getRawClass().getComponentType())\n+            || containerType.getRawClass().getComponentType().isEnum())) {\n+        return true;\n+    }\n+    // Check if it is Collection of simple types or enum.\n+    if (Collection.class.isAssignableFrom(containerType.getRawClass())) {\n+      JavaType innerType = containerType.containedType(0);\n+      // Note that raw types are allowed, hence the null check.\n+      if (innerType == null || SIMPLE_TYPES.contains(innerType.getRawClass())\n+          || innerType.getRawClass().isEnum()) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * Ensures that empty string value is allowed for a given type.\n+   *\n+   * <p>Empty strings are only allowed for {@link String}, {@link String String[]},\n+   * {@link Collection Collection&lt;String&gt;}, or {@link ValueProvider ValueProvider&lt;T&gt;}\n+   * and {@code T} is of type {@link String}, {@link String String[]},\n+   * {@link Collection Collection&lt;String&gt;}.\n+   *\n+   * @param type class object for the type under check.\n+   * @param genericType complete type information for the type under check.\n+   * @param genericTypeName a string representation of the complete type information.\n+   */\n+  private static void checkEmptyStringAllowed(Class<?> type, JavaType genericType,\n+      String genericTypeName) {\n+    JavaType unwrappedType = type.equals(ValueProvider.class)\n+        ? genericType.containedType(0) : genericType;\n+\n+    Class<?> containedType = unwrappedType.getRawClass();\n+    if (unwrappedType.getRawClass().isArray()) {\n+      containedType = unwrappedType.getRawClass().getComponentType();\n+    } else if (Collection.class.isAssignableFrom(unwrappedType.getRawClass())) {\n+      JavaType innerType = unwrappedType.containedType(0);\n+      // Note that raw types are allowed, hence the null check.\n+      containedType = innerType == null ? String.class : innerType.getRawClass();\n+    }\n+    if (!containedType.equals(String.class)) {\n+      String msg = String.format(\"Empty argument value is only allowed for String, String Array, \"\n+              + \"Collections of Strings or any of these types in a parameterized ValueProvider, \"\n+              + \"but received: %s\",\n+          genericTypeName);\n+      throw new IllegalArgumentException(msg);\n+    }\n+  }\n+\n   @VisibleForTesting\n   static Set<String> getSupportedRunners() {\n     ImmutableSortedSet.Builder<String> supportedRunners = ImmutableSortedSet.naturalOrder();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/options/PipelineOptionsFactory.java",
                "sha": "42e1092ef4ed993053ccdaaf1c9e17c30f234c7e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/options/StreamingOptions.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/options/StreamingOptions.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/options/StreamingOptions.java",
                "patch": "@@ -21,7 +21,7 @@\n  * Options used to configure streaming.\n  */\n public interface StreamingOptions extends\n-    ApplicationNameOptions, GcpOptions, PipelineOptions {\n+    ApplicationNameOptions, PipelineOptions {\n   /**\n    * Set to true if running a streaming pipeline.\n    */",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/options/StreamingOptions.java",
                "sha": "99ecd8fbdb44e04dacd3c1754f30de6184a2bb7a",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/options/ValueProvider.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/options/ValueProvider.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 3,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/options/ValueProvider.java",
                "patch": "@@ -17,7 +17,6 @@\n  */\n package org.apache.beam.sdk.options;\n \n-import static com.google.common.base.MoreObjects.firstNonNull;\n import static com.google.common.base.Preconditions.checkNotNull;\n \n import com.fasterxml.jackson.core.JsonGenerator;\n@@ -134,6 +133,20 @@ public boolean isAccessible() {\n       return value.isAccessible();\n     }\n \n+    /**\n+     * Returns the property name associated with this provider.\n+     */\n+    public String propertyName() {\n+      if (value instanceof RuntimeValueProvider) {\n+        return ((RuntimeValueProvider) value).propertyName();\n+      } else if (value instanceof NestedValueProvider) {\n+        return ((NestedValueProvider) value).propertyName();\n+      } else {\n+        throw new RuntimeException(\"Only a RuntimeValueProvider or a NestedValueProvider can supply\"\n+            + \" a property name.\");\n+      }\n+    }\n+\n     @Override\n     public String toString() {\n       return MoreObjects.toStringHelper(this)\n@@ -208,8 +221,16 @@ public T get() {\n         Method method = klass.getMethod(methodName);\n         PipelineOptions methodOptions = options.as(klass);\n         InvocationHandler handler = Proxy.getInvocationHandler(methodOptions);\n-        T value = ((ValueProvider<T>) handler.invoke(methodOptions, method, null)).get();\n-        return firstNonNull(value, defaultValue);\n+        ValueProvider<T> result =\n+            (ValueProvider<T>) handler.invoke(methodOptions, method, null);\n+        // Two cases: If we have deserialized a new value from JSON, it will\n+        // be wrapped in a StaticValueProvider, which we can provide here.  If\n+        // not, there was no JSON value, and we return the default, whether or\n+        // not it is null.\n+        if (result instanceof StaticValueProvider) {\n+          return result.get();\n+        }\n+        return defaultValue;\n       } catch (Throwable e) {\n         throw new RuntimeException(\"Unable to load runtime value.\", e);\n       }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/options/ValueProvider.java",
                "sha": "93fcaf89804873d2d79f15770e50b7e1ecfc3993",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PTransformOverrideFactory.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PTransformOverrideFactory.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 8,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PTransformOverrideFactory.java",
                "patch": "@@ -14,22 +14,28 @@\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n+ *\n  */\n-package org.apache.beam.runners.direct;\n \n+package org.apache.beam.sdk.runners;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n import org.apache.beam.sdk.transforms.PTransform;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n \n-interface PTransformOverrideFactory<\n+/**\n+ * Produces {@link PipelineRunner}-specific overrides of {@link PTransform PTransforms}, and\n+ * provides mappings between original and replacement outputs.\n+ */\n+@Experimental(Kind.CORE_RUNNERS_ONLY)\n+public interface PTransformOverrideFactory<\n     InputT extends PInput,\n     OutputT extends POutput,\n-    TransformT extends PTransform<InputT, OutputT>> {\n+    TransformT extends PTransform<? super InputT, OutputT>> {\n   /**\n-   * Create a {@link PTransform} override for the provided {@link PTransform} if applicable.\n-   * Otherwise, return the input {@link PTransform}.\n-   *\n-   * <p>The returned PTransform must be semantically equivalent to the input {@link PTransform}.\n+   * Returns a {@link PTransform} that produces equivalent output to the provided transform.\n    */\n-  PTransform<InputT, OutputT> override(TransformT transform);\n+  PTransform<InputT, OutputT> getReplacementTransform(TransformT transform);\n }",
                "previous_filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/PTransformOverrideFactory.java",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PTransformOverrideFactory.java",
                "sha": "f6e90e203042657ec7ffdaf632670bdcfa25f3b2",
                "status": "renamed"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PipelineRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PipelineRunner.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PipelineRunner.java",
                "patch": "@@ -73,6 +73,6 @@\n    */\n   public <OutputT extends POutput, InputT extends PInput> OutputT apply(\n       PTransform<InputT, OutputT> transform, InputT input) {\n-    return transform.apply(input);\n+    return transform.expand(input);\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/PipelineRunner.java",
                "sha": "8604dbc65c4826340f3e74676c00c18290a68db2",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/TransformHierarchy.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/TransformHierarchy.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 12,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/runners/TransformHierarchy.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.annotations.VisibleForTesting;\n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -83,8 +84,8 @@ public Node pushNode(String name, PInput input, PTransform<?, ?> transform) {\n    */\n   public void finishSpecifyingInput() {\n     // Inputs must be completely specified before they are consumed by a transform.\n-    current.getInput().finishSpecifying();\n-    for (PValue inputValue : current.getInput().expand()) {\n+    for (PValue inputValue : current.getInputs()) {\n+      inputValue.finishSpecifying();\n       checkState(producers.get(inputValue) != null, \"Producer unknown for input %s\", inputValue);\n       inputValue.finishSpecifying();\n     }\n@@ -101,6 +102,7 @@ public void finishSpecifyingInput() {\n    * nodes.\n    */\n   public void setOutput(POutput output) {\n+    output.finishSpecifyingOutput();\n     for (PValue value : output.expand()) {\n       if (!producers.containsKey(value)) {\n         producers.put(value, current);\n@@ -253,11 +255,9 @@ public String getFullName() {\n       return fullName;\n     }\n \n-    /**\n-     * Returns the transform input, in unexpanded form.\n-     */\n-    public PInput getInput() {\n-      return input;\n+    /** Returns the transform input, in unexpanded form. */\n+    public Collection<? extends PValue> getInputs() {\n+      return input == null ? Collections.<PValue>emptyList() : input.expand();\n     }\n \n     /**\n@@ -296,13 +296,15 @@ private void setOutput(POutput output) {\n     }\n \n     /** Returns the transform output, in unexpanded form. */\n-    public POutput getOutput() {\n-      return output;\n+    public Collection<? extends PValue> getOutputs() {\n+      return output == null ? Collections.<PValue>emptyList() : output.expand();\n     }\n \n-    AppliedPTransform<?, ?, ?> toAppliedPTransform() {\n-      return AppliedPTransform.of(\n-          getFullName(), getInput(), getOutput(), (PTransform) getTransform());\n+    /**\n+     * Returns the {@link AppliedPTransform} representing this {@link Node}.\n+     */\n+    public AppliedPTransform<?, ?, ?> toAppliedPTransform() {\n+      return AppliedPTransform.of(getFullName(), input, output, (PTransform) getTransform());\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/runners/TransformHierarchy.java",
                "sha": "33d5231ac24fa83bb647dd64c2fa80bf4b85ee29",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/BigqueryMatcher.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/BigqueryMatcher.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 16,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/BigqueryMatcher.java",
                "patch": "@@ -39,6 +39,7 @@\n import com.google.common.hash.HashCode;\n import com.google.common.hash.Hashing;\n import java.io.IOException;\n+import java.io.InterruptedIOException;\n import java.nio.charset.StandardCharsets;\n import java.util.Collection;\n import java.util.Collections;\n@@ -117,20 +118,23 @@ protected boolean matchesSafely(PipelineResult pipelineResult) {\n \n       response = queryWithRetries(\n           bigqueryClient, queryContent, Sleeper.DEFAULT, BACKOFF_FACTORY.backoff());\n-    } catch (Exception e) {\n+    } catch (IOException | InterruptedException e) {\n+      if (e instanceof InterruptedIOException) {\n+        Thread.currentThread().interrupt();\n+      }\n       throw new RuntimeException(\"Failed to fetch BigQuery data.\", e);\n     }\n \n-    // validate BigQuery response\n-    if (response == null || response.getRows() == null || response.getRows().isEmpty()) {\n+    if (!response.getJobComplete()) {\n+      // query job not complete, verification failed\n       return false;\n-    }\n-\n-    // compute checksum\n-    actualChecksum = generateHash(response.getRows());\n-    LOG.debug(\"Generated a SHA1 checksum based on queried data: {}\", actualChecksum);\n+    } else {\n+      // compute checksum\n+      actualChecksum = generateHash(response.getRows());\n+      LOG.debug(\"Generated a SHA1 checksum based on queried data: {}\", actualChecksum);\n \n-    return expectedChecksum.equals(actualChecksum);\n+      return expectedChecksum.equals(actualChecksum);\n+    }\n   }\n \n   @VisibleForTesting\n@@ -144,23 +148,35 @@ Bigquery newBigqueryClient(String applicationName) {\n         .build();\n   }\n \n+  @Nonnull\n   @VisibleForTesting\n   QueryResponse queryWithRetries(Bigquery bigqueryClient, QueryRequest queryContent,\n                                  Sleeper sleeper, BackOff backOff)\n       throws IOException, InterruptedException {\n     IOException lastException = null;\n     do {\n+      if (lastException != null) {\n+        LOG.warn(\"Retrying query ({}) after exception\", queryContent.getQuery(), lastException);\n+      }\n       try {\n-        return bigqueryClient.jobs().query(projectId, queryContent).execute();\n+        QueryResponse response = bigqueryClient.jobs().query(projectId, queryContent).execute();\n+        if (response != null) {\n+          return response;\n+        } else {\n+          lastException =\n+              new IOException(\"Expected valid response from query job, but received null.\");\n+        }\n       } catch (IOException e) {\n         // ignore and retry\n-        LOG.warn(\"Ignore the error and retry the query.\");\n         lastException = e;\n       }\n     } while(BackOffUtils.next(sleeper, backOff));\n-    throw new IOException(\n+\n+    throw new RuntimeException(\n         String.format(\n-            \"Unable to get BigQuery response after retrying %d times\", MAX_QUERY_RETRIES),\n+            \"Unable to get BigQuery response after retrying %d times using query (%s)\",\n+            MAX_QUERY_RETRIES,\n+            queryContent.getQuery()),\n         lastException);\n   }\n \n@@ -210,9 +226,9 @@ public void describeTo(Description description) {\n   @Override\n   public void describeMismatchSafely(PipelineResult pResult, Description description) {\n     String info;\n-    if (response == null || response.getRows() == null || response.getRows().isEmpty()) {\n-      // invalid query response\n-      info = String.format(\"Invalid BigQuery response: %s\", Objects.toString(response));\n+    if (!response.getJobComplete()) {\n+      // query job not complete\n+      info = String.format(\"The query job hasn't completed. Got response: %s\", response);\n     } else {\n       // checksum mismatch\n       info = String.format(\"was (%s).%n\"",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/BigqueryMatcher.java",
                "sha": "8f752c0412e52a3048d0541103242a63269a502f",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/FileChecksumMatcher.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/FileChecksumMatcher.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 96,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/FileChecksumMatcher.java",
                "patch": "@@ -21,31 +21,19 @@\n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkNotNull;\n \n-import com.google.api.client.util.BackOff;\n-import com.google.api.client.util.BackOffUtils;\n import com.google.api.client.util.Sleeper;\n-import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Strings;\n-import com.google.common.collect.Lists;\n import com.google.common.hash.HashCode;\n import com.google.common.hash.Hashing;\n-import com.google.common.io.CharStreams;\n-import java.io.IOException;\n-import java.io.Reader;\n-import java.nio.channels.Channels;\n import java.nio.charset.StandardCharsets;\n-import java.nio.file.Path;\n-import java.nio.file.Paths;\n import java.util.ArrayList;\n-import java.util.Collection;\n import java.util.List;\n-import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n import javax.annotation.Nonnull;\n import org.apache.beam.sdk.PipelineResult;\n import org.apache.beam.sdk.util.FluentBackoff;\n-import org.apache.beam.sdk.util.IOChannelFactory;\n-import org.apache.beam.sdk.util.IOChannelUtils;\n+import org.apache.beam.sdk.util.NumberedShardedFile;\n+import org.apache.beam.sdk.util.ShardedFile;\n import org.hamcrest.Description;\n import org.hamcrest.TypeSafeMatcher;\n import org.joda.time.Duration;\n@@ -83,9 +71,8 @@\n       Pattern.compile(\"(?x) \\\\S* (?<shardnum> \\\\d+) -of- (?<numshards> \\\\d+)\");\n \n   private final String expectedChecksum;\n-  private final String filePath;\n-  private final Pattern shardTemplate;\n   private String actualChecksum;\n+  private final ShardedFile shardedFile;\n \n   /**\n    * Constructor that uses default shard template.\n@@ -98,7 +85,7 @@ public FileChecksumMatcher(String checksum, String filePath) {\n   }\n \n   /**\n-   * Constructor.\n+   * Constructor using a custom shard template.\n    *\n    * @param checksum expected checksum string used to verify file content.\n    * @param filePath path of files that's to be verified.\n@@ -121,18 +108,28 @@ public FileChecksumMatcher(String checksum, String filePath, Pattern shardTempla\n         DEFAULT_SHARD_TEMPLATE);\n \n     this.expectedChecksum = checksum;\n-    this.filePath = filePath;\n-    this.shardTemplate = shardTemplate;\n+    this.shardedFile = new NumberedShardedFile(filePath, shardTemplate);\n+  }\n+\n+  /**\n+   * Constructor using an entirely custom {@link ShardedFile} implementation.\n+   *\n+   * <p>For internal use only.\n+   */\n+  public FileChecksumMatcher(String expectedChecksum, ShardedFile shardedFile) {\n+    this.expectedChecksum = expectedChecksum;\n+    this.shardedFile = shardedFile;\n   }\n \n   @Override\n   public boolean matchesSafely(PipelineResult pipelineResult) {\n     // Load output data\n     List<String> outputs;\n     try {\n-      outputs = readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff());\n+      outputs = shardedFile.readFilesWithRetries(Sleeper.DEFAULT, BACK_OFF_FACTORY.backoff());\n     } catch (Exception e) {\n-      throw new RuntimeException(String.format(\"Failed to read from: %s\", filePath), e);\n+      throw new RuntimeException(\n+          String.format(\"Failed to read from: %s\", shardedFile), e);\n     }\n \n     // Verify outputs. Checksum is computed using SHA-1 algorithm\n@@ -142,81 +139,6 @@ public boolean matchesSafely(PipelineResult pipelineResult) {\n     return actualChecksum.equals(expectedChecksum);\n   }\n \n-  @VisibleForTesting\n-  List<String> readFilesWithRetries(Sleeper sleeper, BackOff backOff)\n-      throws IOException, InterruptedException {\n-    IOChannelFactory factory = IOChannelUtils.getFactory(filePath);\n-    IOException lastException = null;\n-\n-    do {\n-      try {\n-        // Match inputPath which may contains glob\n-        Collection<String> files = factory.match(filePath);\n-        LOG.debug(\"Found {} file(s) by matching the path: {}\", files.size(), filePath);\n-\n-        if (files.isEmpty() || !checkTotalNumOfFiles(files)) {\n-          continue;\n-        }\n-\n-        // Read data from file paths\n-        return readLines(files, factory);\n-      } catch (IOException e) {\n-        // Ignore and retry\n-        lastException = e;\n-        LOG.warn(\"Error in file reading. Ignore and retry.\");\n-      }\n-    } while(BackOffUtils.next(sleeper, backOff));\n-    // Failed after max retries\n-    throw new IOException(\n-        String.format(\"Unable to read file(s) after retrying %d times\", MAX_READ_RETRIES),\n-        lastException);\n-  }\n-\n-  @VisibleForTesting\n-  List<String> readLines(Collection<String> files, IOChannelFactory factory) throws IOException {\n-    List<String> allLines = Lists.newArrayList();\n-    int i = 1;\n-    for (String file : files) {\n-      try (Reader reader =\n-               Channels.newReader(factory.open(file), StandardCharsets.UTF_8.name())) {\n-        List<String> lines = CharStreams.readLines(reader);\n-        allLines.addAll(lines);\n-        LOG.debug(\n-            \"[{} of {}] Read {} lines from file: {}\", i, files.size(), lines.size(), file);\n-      }\n-      i++;\n-    }\n-    return allLines;\n-  }\n-\n-  /**\n-   * Check if total number of files is correct by comparing with the number that\n-   * is parsed from shard name using a name template. If no template is specified,\n-   * \"SSSS-of-NNNN\" will be used as default, and \"NNNN\" will be the expected total\n-   * number of files.\n-   *\n-   * @return {@code true} if at least one shard name matches template and total number\n-   * of given files equals the number that is parsed from shard name.\n-   */\n-  @VisibleForTesting\n-  boolean checkTotalNumOfFiles(Collection<String> files) {\n-    for (String filePath : files) {\n-      Path fileName = Paths.get(filePath).getFileName();\n-      if (fileName == null) {\n-        // this path has zero elements\n-        continue;\n-      }\n-      Matcher matcher = shardTemplate.matcher(fileName.toString());\n-      if (!matcher.matches()) {\n-        // shard name doesn't match the pattern, check with the next shard\n-        continue;\n-      }\n-      // once match, extract total number of shards and compare to file list\n-      return files.size() == Integer.parseInt(matcher.group(\"numshards\"));\n-    }\n-    return false;\n-  }\n-\n   private String computeHash(@Nonnull List<String> strs) {\n     if (strs.isEmpty()) {\n       return Hashing.sha1().hashString(\"\", StandardCharsets.UTF_8).toString();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/FileChecksumMatcher.java",
                "sha": "82a6b71176369c52fc8ae79c7598d5be2dfee4ea",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/GatherAllPanes.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/GatherAllPanes.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/GatherAllPanes.java",
                "patch": "@@ -55,7 +55,7 @@\n   private GatherAllPanes() {}\n \n   @Override\n-  public PCollection<Iterable<ValueInSingleWindow<T>>> apply(PCollection<T> input) {\n+  public PCollection<Iterable<ValueInSingleWindow<T>>> expand(PCollection<T> input) {\n     WindowFn<?, ?> originalWindowFn = input.getWindowingStrategy().getWindowFn();\n \n     return input",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/GatherAllPanes.java",
                "sha": "bf2cd0b3c503f6062175725f023c96bf4de14c63",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/PAssert.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/PAssert.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 6,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/PAssert.java",
                "patch": "@@ -763,7 +763,7 @@ private CreateActual(\n     }\n \n     @Override\n-    public PCollectionView<ActualT> apply(PBegin input) {\n+    public PCollectionView<ActualT> expand(PBegin input) {\n       final Coder<T> coder = actual.getCoder();\n       return actual\n           .apply(\"FilterActuals\", rewindowActuals.<T>prepareActuals())\n@@ -833,7 +833,7 @@ public GroupGlobally(AssertionWindows rewindowingStrategy) {\n     }\n \n     @Override\n-    public PCollection<Iterable<ValueInSingleWindow<T>>> apply(PCollection<T> input) {\n+    public PCollection<Iterable<ValueInSingleWindow<T>>> expand(PCollection<T> input) {\n       final int combinedKey = 42;\n \n       // Remove the triggering on both\n@@ -925,7 +925,7 @@ private GroupThenAssert(\n     }\n \n     @Override\n-    public PDone apply(PCollection<T> input) {\n+    public PDone expand(PCollection<T> input) {\n       input\n           .apply(\"GroupGlobally\", new GroupGlobally<T>(rewindowingStrategy))\n           .apply(\"GetPane\", MapElements.via(paneExtractor))\n@@ -958,7 +958,7 @@ private GroupThenAssertForSingleton(\n     }\n \n     @Override\n-    public PDone apply(PCollection<Iterable<T>> input) {\n+    public PDone expand(PCollection<Iterable<T>> input) {\n       input\n           .apply(\"GroupGlobally\", new GroupGlobally<Iterable<T>>(rewindowingStrategy))\n           .apply(\"GetPane\", MapElements.via(paneExtractor))\n@@ -995,7 +995,7 @@ private OneSideInputAssert(\n     }\n \n     @Override\n-    public PDone apply(PBegin input) {\n+    public PDone expand(PBegin input) {\n       final PCollectionView<ActualT> actual = input.apply(\"CreateActual\", createActual);\n \n       input\n@@ -1321,7 +1321,7 @@ public FilterWindows(StaticWindows windows) {\n     }\n \n     @Override\n-    public PCollection<T> apply(PCollection<T> input) {\n+    public PCollection<T> expand(PCollection<T> input) {\n       return input.apply(\"FilterWindows\", ParDo.of(new Fn()));\n     }\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/PAssert.java",
                "sha": "b23f4f3e69fd7f3d153009b3e0c6a7e22e3b4fab",
                "status": "modified"
            },
            {
                "additions": 198,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestPipeline.java",
                "changes": 225,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestPipeline.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 27,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestPipeline.java",
                "patch": "@@ -23,12 +23,17 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.node.ObjectNode;\n import com.google.common.base.Optional;\n+import com.google.common.base.Predicate;\n+import com.google.common.base.Predicates;\n import com.google.common.base.Strings;\n+import com.google.common.collect.FluentIterable;\n import com.google.common.collect.Iterators;\n import java.io.IOException;\n import java.lang.reflect.Method;\n import java.util.ArrayList;\n import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map.Entry;\n import javax.annotation.Nullable;\n import org.apache.beam.sdk.Pipeline;\n@@ -39,34 +44,39 @@\n import org.apache.beam.sdk.options.PipelineOptions.CheckEnabled;\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.runners.PipelineRunner;\n+import org.apache.beam.sdk.runners.TransformHierarchy;\n import org.apache.beam.sdk.util.IOChannelUtils;\n import org.apache.beam.sdk.util.TestCredential;\n import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestRule;\n+import org.junit.runner.Description;\n+import org.junit.runners.model.Statement;\n \n /**\n- * A creator of test pipelines that can be used inside of tests that can be\n- * configured to run locally or against a remote pipeline runner.\n+ * A creator of test pipelines that can be used inside of tests that can be configured to run\n+ * locally or against a remote pipeline runner.\n  *\n- * <p>It is recommended to tag hand-selected tests for this purpose using the\n- * {@link RunnableOnService} {@link Category} annotation, as each test run against a pipeline runner\n- * will utilize resources of that pipeline runner.\n+ * <p>It is recommended to tag hand-selected tests for this purpose using the {@link\n+ * RunnableOnService} {@link Category} annotation, as each test run against a pipeline runner will\n+ * utilize resources of that pipeline runner.\n  *\n  * <p>In order to run tests on a pipeline runner, the following conditions must be met:\n+ *\n  * <ul>\n- *   <li>System property \"beamTestPipelineOptions\" must contain a JSON delimited list of pipeline\n- *   options. For example:\n- *   <pre>{@code [\n+ * <li>System property \"beamTestPipelineOptions\" must contain a JSON delimited list of pipeline\n+ *     options. For example:\n+ *     <pre>{@code [\n  *     \"--runner=org.apache.beam.runners.dataflow.testing.TestDataflowRunner\",\n  *     \"--project=mygcpproject\",\n  *     \"--stagingLocation=gs://mygcsbucket/path\"\n  *     ]}</pre>\n  *     Note that the set of pipeline options required is pipeline runner specific.\n- *   </li>\n- *   <li>Jars containing the SDK and test classes must be available on the classpath.</li>\n+ * <li>Jars containing the SDK and test classes must be available on the classpath.\n  * </ul>\n  *\n  * <p>Use {@link PAssert} for tests, as it integrates with this test harness in both direct and\n  * remote execution modes. For example:\n+ *\n  * <pre>{@code\n  * Pipeline p = TestPipeline.create();\n  * PCollection<Integer> output = ...\n@@ -76,19 +86,141 @@\n  * p.run();\n  * }</pre>\n  *\n- * <p>For pipeline runners, it is required that they must throw an {@link AssertionError}\n- * containing the message from the {@link PAssert} that failed.\n+ * <p>For pipeline runners, it is required that they must throw an {@link AssertionError} containing\n+ * the message from the {@link PAssert} that failed.\n  */\n-public class TestPipeline extends Pipeline {\n+public class TestPipeline extends Pipeline implements TestRule {\n+\n+  private static class PipelineRunEnforcement {\n+\n+    protected boolean enableAutoRunIfMissing;\n+    protected final Pipeline pipeline;\n+    private boolean runInvoked;\n+\n+    private PipelineRunEnforcement(final Pipeline pipeline) {\n+      this.pipeline = pipeline;\n+    }\n+\n+    private void enableAutoRunIfMissing(final boolean enable) {\n+      enableAutoRunIfMissing = enable;\n+    }\n+\n+    protected void beforePipelineExecution() {\n+      runInvoked = true;\n+    }\n+\n+    protected void afterTestCompletion() {\n+      if (!runInvoked && enableAutoRunIfMissing) {\n+        pipeline.run().waitUntilFinish();\n+      }\n+    }\n+  }\n+\n+  private static class PipelineAbandonedNodeEnforcement extends PipelineRunEnforcement {\n+\n+    private List<TransformHierarchy.Node> runVisitedNodes;\n+\n+    private final Predicate<TransformHierarchy.Node> isPAssertNode =\n+        new Predicate<TransformHierarchy.Node>() {\n+\n+          @Override\n+          public boolean apply(final TransformHierarchy.Node node) {\n+            return node.getTransform() instanceof PAssert.GroupThenAssert\n+                || node.getTransform() instanceof PAssert.GroupThenAssertForSingleton\n+                || node.getTransform() instanceof PAssert.OneSideInputAssert;\n+          }\n+        };\n+\n+    private static class NodeRecorder extends PipelineVisitor.Defaults {\n+\n+      private final List<TransformHierarchy.Node> visited = new LinkedList<>();\n+\n+      @Override\n+      public void leaveCompositeTransform(final TransformHierarchy.Node node) {\n+        visited.add(node);\n+      }\n+\n+      @Override\n+      public void visitPrimitiveTransform(final TransformHierarchy.Node node) {\n+        visited.add(node);\n+      }\n+    }\n+\n+    private PipelineAbandonedNodeEnforcement(final TestPipeline pipeline) {\n+      super(pipeline);\n+    }\n+\n+    private List<TransformHierarchy.Node> recordPipelineNodes(final Pipeline pipeline) {\n+      final NodeRecorder nodeRecorder = new NodeRecorder();\n+      pipeline.traverseTopologically(nodeRecorder);\n+      return nodeRecorder.visited;\n+    }\n+\n+    private void verifyPipelineExecution() {\n+      final List<TransformHierarchy.Node> pipelineNodes = recordPipelineNodes(pipeline);\n+      if (runVisitedNodes != null && !runVisitedNodes.equals(pipelineNodes)) {\n+        final boolean hasDanglingPAssert =\n+            FluentIterable.from(pipelineNodes)\n+                .filter(Predicates.not(Predicates.in(runVisitedNodes)))\n+                .anyMatch(isPAssertNode);\n+        if (hasDanglingPAssert) {\n+          throw new AbandonedNodeException(\"The pipeline contains abandoned PAssert(s).\");\n+        } else {\n+          throw new AbandonedNodeException(\"The pipeline contains abandoned PTransform(s).\");\n+        }\n+      } else if (runVisitedNodes == null && !enableAutoRunIfMissing) {\n+        IsEmptyVisitor isEmptyVisitor = new IsEmptyVisitor();\n+        pipeline.traverseTopologically(isEmptyVisitor);\n+\n+        if (!isEmptyVisitor.isEmpty()) {\n+          throw new PipelineRunMissingException(\"The pipeline has not been run.\");\n+        }\n+      }\n+    }\n+\n+    @Override\n+    protected void beforePipelineExecution() {\n+      super.beforePipelineExecution();\n+      runVisitedNodes = recordPipelineNodes(pipeline);\n+    }\n+\n+    @Override\n+    protected void afterTestCompletion() {\n+      super.afterTestCompletion();\n+      verifyPipelineExecution();\n+    }\n+  }\n+\n+  /**\n+   * An exception thrown in case an abandoned {@link org.apache.beam.sdk.transforms.PTransform} is\n+   * detected, that is, a {@link org.apache.beam.sdk.transforms.PTransform} that has not been run.\n+   */\n+  public static class AbandonedNodeException extends RuntimeException {\n+\n+    AbandonedNodeException(final String msg) {\n+      super(msg);\n+    }\n+  }\n+\n+  /** An exception thrown in case a test finishes without invoking {@link Pipeline#run()}. */\n+  public static class PipelineRunMissingException extends RuntimeException {\n+\n+    PipelineRunMissingException(final String msg) {\n+      super(msg);\n+    }\n+  }\n+\n   static final String PROPERTY_BEAM_TEST_PIPELINE_OPTIONS = \"beamTestPipelineOptions\";\n   static final String PROPERTY_USE_DEFAULT_DUMMY_RUNNER = \"beamUseDummyRunner\";\n   private static final ObjectMapper MAPPER = new ObjectMapper();\n \n+  private PipelineRunEnforcement enforcement = new PipelineAbandonedNodeEnforcement(this);\n+\n   /**\n    * Creates and returns a new test pipeline.\n    *\n-   * <p>Use {@link PAssert} to add tests, then call\n-   * {@link Pipeline#run} to execute the pipeline and check the tests.\n+   * <p>Use {@link PAssert} to add tests, then call {@link Pipeline#run} to execute the pipeline and\n+   * check the tests.\n    */\n   public static TestPipeline create() {\n     return fromOptions(testingPipelineOptions());\n@@ -98,16 +230,30 @@ public static TestPipeline fromOptions(PipelineOptions options) {\n     return new TestPipeline(PipelineRunner.fromOptions(options), options);\n   }\n \n-  private TestPipeline(PipelineRunner<? extends PipelineResult> runner, PipelineOptions options) {\n+  private TestPipeline(\n+      final PipelineRunner<? extends PipelineResult> runner, final PipelineOptions options) {\n     super(runner, options);\n   }\n \n+  @Override\n+  public Statement apply(final Statement statement, final Description description) {\n+    return new Statement() {\n+\n+      @Override\n+      public void evaluate() throws Throwable {\n+        statement.evaluate();\n+        enforcement.afterTestCompletion();\n+      }\n+    };\n+  }\n+\n   /**\n-   * Runs this {@link TestPipeline}, unwrapping any {@code AssertionError}\n-   * that is raised during testing.\n+   * Runs this {@link TestPipeline}, unwrapping any {@code AssertionError} that is raised during\n+   * testing.\n    */\n   @Override\n   public PipelineResult run() {\n+    enforcement.beforePipelineExecution();\n     try {\n       return super.run();\n     } catch (RuntimeException exc) {\n@@ -120,18 +266,28 @@ public PipelineResult run() {\n     }\n   }\n \n+  public TestPipeline enableAbandonedNodeEnforcement(final boolean enable) {\n+    enforcement =\n+        enable ? new PipelineAbandonedNodeEnforcement(this) : new PipelineRunEnforcement(this);\n+\n+    return this;\n+  }\n+\n+  public TestPipeline enableAutoRunIfMissing(final boolean enable) {\n+    enforcement.enableAutoRunIfMissing(enable);\n+    return this;\n+  }\n+\n   @Override\n   public String toString() {\n     return \"TestPipeline#\" + getOptions().as(ApplicationNameOptions.class).getAppName();\n   }\n \n-  /**\n-   * Creates {@link PipelineOptions} for testing.\n-   */\n+  /** Creates {@link PipelineOptions} for testing. */\n   public static PipelineOptions testingPipelineOptions() {\n     try {\n-      @Nullable String beamTestPipelineOptions =\n-          System.getProperty(PROPERTY_BEAM_TEST_PIPELINE_OPTIONS);\n+      @Nullable\n+      String beamTestPipelineOptions = System.getProperty(PROPERTY_BEAM_TEST_PIPELINE_OPTIONS);\n \n       PipelineOptions options =\n           Strings.isNullOrEmpty(beamTestPipelineOptions)\n@@ -155,13 +311,15 @@ public static PipelineOptions testingPipelineOptions() {\n       IOChannelUtils.registerIOFactoriesAllowOverride(options);\n       return options;\n     } catch (IOException e) {\n-      throw new RuntimeException(\"Unable to instantiate test options from system property \"\n-          + PROPERTY_BEAM_TEST_PIPELINE_OPTIONS + \":\"\n-          + System.getProperty(PROPERTY_BEAM_TEST_PIPELINE_OPTIONS), e);\n+      throw new RuntimeException(\n+          \"Unable to instantiate test options from system property \"\n+              + PROPERTY_BEAM_TEST_PIPELINE_OPTIONS\n+              + \":\"\n+              + System.getProperty(PROPERTY_BEAM_TEST_PIPELINE_OPTIONS),\n+          e);\n     }\n   }\n \n-\n   public static String[] convertToArgs(PipelineOptions options) {\n     try {\n       byte[] opts = MAPPER.writeValueAsBytes(options);\n@@ -236,4 +394,17 @@ private static String getAppName() {\n     }\n     return firstInstanceAfterTestPipeline;\n   }\n+\n+  private static class IsEmptyVisitor extends PipelineVisitor.Defaults {\n+    private boolean empty = true;\n+\n+    public boolean isEmpty() {\n+      return empty;\n+    }\n+\n+    @Override\n+    public void visitPrimitiveTransform(TransformHierarchy.Node node) {\n+      empty = false;\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestPipeline.java",
                "sha": "b707a81deec57fef436e74e7aa3f5220a6ccede1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestStream.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestStream.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestStream.java",
                "patch": "@@ -252,7 +252,7 @@ private Builder(Coder<T> coder, ImmutableList<Event<T>> events, Instant currentW\n   }\n \n   @Override\n-  public PCollection<T> apply(PBegin input) {\n+  public PCollection<T> expand(PBegin input) {\n     throw new IllegalStateException(\n         String.format(\n             \"Pipeline Runner %s does not provide a required override for %s\",",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/TestStream.java",
                "sha": "da93cdc9736b815a93a7a271050051e3a084a9cf",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesMetrics.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesMetrics.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesMetrics.java",
                "patch": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.beam.sdk.testing;\n+\n+/**\n+ * Category tag for validation tests which utilize {@link org.apache.beam.sdk.metrics.Metrics}.\n+ */\n+public interface UsesMetrics {}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesMetrics.java",
                "sha": "261354c240d3289cb361504779c230753bb82e95",
                "status": "added"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesTimersInParDo.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesTimersInParDo.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesTimersInParDo.java",
                "patch": "@@ -0,0 +1,25 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.testing;\n+\n+import org.apache.beam.sdk.transforms.ParDo;\n+\n+/**\n+ * Category tag for validation tests which utilize timers in {@link ParDo}.\n+ */\n+public interface UsesTimersInParDo {}",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/testing/UsesTimersInParDo.java",
                "sha": "14123ed6a8d3e9e95ceea8c36ddc8ddce954329d",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AggregatorRetriever.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AggregatorRetriever.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AggregatorRetriever.java",
                "patch": "@@ -30,7 +30,7 @@ private AggregatorRetriever() {\n   /**\n    * Returns the {@link Aggregator Aggregators} created by the provided {@link OldDoFn}.\n    */\n-  public static Collection<Aggregator<?, ?>> getAggregators(OldDoFn<?, ?> fn) {\n+  public static Collection<Aggregator<?, ?>> getAggregators(DoFn<?, ?> fn) {\n     return fn.getAggregators();\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AggregatorRetriever.java",
                "sha": "ce47e22fda064d6f936e937b3ec8eadd7565b61f",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AppliedPTransform.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AppliedPTransform.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 60,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AppliedPTransform.java",
                "patch": "@@ -17,8 +17,7 @@\n  */\n package org.apache.beam.sdk.transforms;\n \n-import com.google.common.base.MoreObjects;\n-import com.google.common.base.Objects;\n+import com.google.auto.value.AutoValue;\n import org.apache.beam.sdk.values.PInput;\n import org.apache.beam.sdk.values.POutput;\n \n@@ -32,70 +31,26 @@\n  * @param <OutputT> transform output type\n  * @param <TransformT> transform type\n  */\n-public class AppliedPTransform\n+@AutoValue\n+public abstract class AppliedPTransform\n     <InputT extends PInput, OutputT extends POutput,\n      TransformT extends PTransform<? super InputT, OutputT>> {\n \n-  private final String fullName;\n-  private final InputT input;\n-  private final OutputT output;\n-  private final TransformT transform;\n-\n-  private AppliedPTransform(String fullName, InputT input, OutputT output, TransformT transform) {\n-    this.input = input;\n-    this.output = output;\n-    this.transform = transform;\n-    this.fullName = fullName;\n+  public static <\n+          InputT extends PInput,\n+          OutputT extends POutput,\n+          TransformT extends PTransform<? super InputT, OutputT>>\n+      AppliedPTransform<InputT, OutputT, TransformT> of(\n+          String fullName, InputT input, OutputT output, TransformT transform) {\n+    return new AutoValue_AppliedPTransform<InputT, OutputT, TransformT>(\n+        fullName, input, output, transform);\n   }\n \n-  public static <InputT extends PInput, OutputT extends POutput,\n-                 TransformT extends PTransform<? super InputT, OutputT>>\n-  AppliedPTransform<InputT, OutputT, TransformT> of(\n-      String fullName, InputT input, OutputT output, TransformT transform) {\n-    return new AppliedPTransform<InputT, OutputT, TransformT>(fullName, input, output, transform);\n-  }\n+  public abstract String getFullName();\n \n-  public String getFullName() {\n-    return fullName;\n-  }\n+  public abstract InputT getInput();\n \n-  public InputT getInput() {\n-    return input;\n-  }\n+  public abstract OutputT getOutput();\n \n-  public OutputT getOutput() {\n-    return output;\n-  }\n-\n-  public TransformT getTransform() {\n-    return transform;\n-  }\n-\n-  @Override\n-  public int hashCode() {\n-    return Objects.hashCode(getFullName(), getInput(), getOutput(), getTransform());\n-  }\n-\n-  @Override\n-  public boolean equals(Object other) {\n-    if (other instanceof AppliedPTransform) {\n-      AppliedPTransform<?, ?, ?> that = (AppliedPTransform<?, ?, ?>) other;\n-      return Objects.equal(this.getFullName(), that.getFullName())\n-          && Objects.equal(this.getInput(), that.getInput())\n-          && Objects.equal(this.getOutput(), that.getOutput())\n-          && Objects.equal(this.getTransform(), that.getTransform());\n-    } else {\n-      return false;\n-    }\n-  }\n-\n-  @Override\n-  public String toString() {\n-    return MoreObjects.toStringHelper(getClass())\n-        .add(\"fullName\", getFullName())\n-        .add(\"input\", getInput())\n-        .add(\"output\", getOutput())\n-        .add(\"transform\", getTransform())\n-        .toString();\n-  }\n+  public abstract TransformT getTransform();\n }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/AppliedPTransform.java",
                "sha": "77de54a1bd576024a52e63980039fef669107605",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/ApproximateUnique.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/ApproximateUnique.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/ApproximateUnique.java",
                "patch": "@@ -204,7 +204,7 @@ public Globally(double maximumEstimationError) {\n     }\n \n     @Override\n-    public PCollection<Long> apply(PCollection<T> input) {\n+    public PCollection<Long> expand(PCollection<T> input) {\n       Coder<T> coder = input.getCoder();\n       return input.apply(\n           Combine.globally(\n@@ -271,7 +271,7 @@ public PerKey(double estimationError) {\n     }\n \n     @Override\n-    public PCollection<KV<K, Long>> apply(PCollection<KV<K, V>> input) {\n+    public PCollection<KV<K, Long>> expand(PCollection<KV<K, V>> input) {\n       Coder<KV<K, V>> inputCoder = input.getCoder();\n       if (!(inputCoder instanceof KvCoder)) {\n         throw new IllegalStateException(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/ApproximateUnique.java",
                "sha": "33820e05d5406745444b7e7bb725ca13f45eaced",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Combine.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Combine.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 7,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Combine.java",
                "patch": "@@ -1431,7 +1431,7 @@ public boolean isInsertDefault() {\n     }\n \n     @Override\n-    public PCollection<OutputT> apply(PCollection<InputT> input) {\n+    public PCollection<OutputT> expand(PCollection<InputT> input) {\n       PCollection<KV<Void, InputT>> withKeys = input\n           .apply(WithKeys.<Void, InputT>of((Void) null))\n           .setCoder(KvCoder.of(VoidCoder.of(), input.getCoder()));\n@@ -1569,7 +1569,7 @@ private GloballyAsSingletonView(\n     }\n \n     @Override\n-    public PCollectionView<OutputT> apply(PCollection<InputT> input) {\n+    public PCollectionView<OutputT> expand(PCollection<InputT> input) {\n       Globally<InputT, OutputT> combineGlobally =\n           Combine.<InputT, OutputT>globally(fn).withoutDefaults().withFanout(fanout);\n       if (insertDefault) {\n@@ -1866,7 +1866,7 @@ public Integer apply(K unused) {\n     }\n \n     @Override\n-    public PCollection<KV<K, OutputT>> apply(PCollection<KV<K, InputT>> input) {\n+    public PCollection<KV<K, OutputT>> expand(PCollection<KV<K, InputT>> input) {\n       return input\n           .apply(GroupByKey.<K, InputT>create(fewKeys))\n           .apply(Combine.<K, InputT, OutputT>groupedValues(fn, fnDisplayData)\n@@ -1901,7 +1901,7 @@ private PerKeyWithHotKeyFanout(String name,\n     }\n \n     @Override\n-    public PCollection<KV<K, OutputT>> apply(PCollection<KV<K, InputT>> input) {\n+    public PCollection<KV<K, OutputT>> expand(PCollection<KV<K, InputT>> input) {\n       return applyHelper(input);\n     }\n \n@@ -2388,12 +2388,12 @@ private GroupedValues(\n     }\n \n     @Override\n-    public PCollection<KV<K, OutputT>> apply(\n+    public PCollection<KV<K, OutputT>> expand(\n         PCollection<? extends KV<K, ? extends Iterable<InputT>>> input) {\n \n       PCollection<KV<K, OutputT>> output = input.apply(ParDo.of(\n-          new OldDoFn<KV<K, ? extends Iterable<InputT>>, KV<K, OutputT>>() {\n-            @Override\n+          new DoFn<KV<K, ? extends Iterable<InputT>>, KV<K, OutputT>>() {\n+            @ProcessElement\n             public void processElement(final ProcessContext c) {\n               K key = c.element().getKey();\n ",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Combine.java",
                "sha": "3b07260f7131e196b24f5d4aedc687f23710f707",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/CombineWithContext.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/CombineWithContext.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/CombineWithContext.java",
                "patch": "@@ -48,7 +48,7 @@\n \n     /**\n      * Returns the value of the side input for the window corresponding to the\n-     * window of the main input element.\n+     * main input's window in which values are being combined.\n      */\n     public abstract <T> T sideInput(PCollectionView<T> view);\n   }",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/CombineWithContext.java",
                "sha": "cd0600a7a9516bcce2a17555719e3772235a02c2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Count.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Count.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Count.java",
                "patch": "@@ -103,7 +103,7 @@ private Count() {\n     public PerElement() { }\n \n     @Override\n-    public PCollection<KV<T, Long>> apply(PCollection<T> input) {\n+    public PCollection<KV<T, Long>> expand(PCollection<T> input) {\n       return\n           input\n           .apply(\"Init\", MapElements.via(new SimpleFunction<T, KV<T, Void>>() {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Count.java",
                "sha": "9101996da6c4bcdd1c5ccc1d1aae6b9efa79eb3a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Create.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Create.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Create.java",
                "patch": "@@ -240,7 +240,7 @@\n     }\n \n     @Override\n-    public PCollection<T> apply(PBegin input) {\n+    public PCollection<T> expand(PBegin input) {\n       try {\n         Coder<T> coder = getDefaultOutputCoder(input);\n         try {\n@@ -440,7 +440,7 @@ protected boolean advanceImpl() throws IOException {\n     }\n \n     @Override\n-    public PCollection<T> apply(PBegin input) {\n+    public PCollection<T> expand(PBegin input) {\n       try {\n         Iterable<T> rawElements =\n             Iterables.transform(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Create.java",
                "sha": "a48136fdfe8a9c38d087595c250c5732c7683a12",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Distinct.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Distinct.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Distinct.java",
                "patch": "@@ -82,7 +82,7 @@\n   }\n \n   @Override\n-  public PCollection<T> apply(PCollection<T> in) {\n+  public PCollection<T> expand(PCollection<T> in) {\n     return in\n         .apply(\"CreateIndex\", MapElements.via(new SimpleFunction<T, KV<T, Void>>() {\n           @Override\n@@ -121,7 +121,7 @@ private WithRepresentativeValues(\n     }\n \n     @Override\n-    public PCollection<T> apply(PCollection<T> in) {\n+    public PCollection<T> expand(PCollection<T> in) {\n       WithKeys<IdT, T> withKeys = WithKeys.of(fn);\n       if (representativeType != null) {\n         withKeys = withKeys.withKeyType(representativeType);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Distinct.java",
                "sha": "2d08cee932ebf40798132f092ea9379aa9c66db4",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java",
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.Timer;\n import org.apache.beam.sdk.util.TimerSpec;\n import org.apache.beam.sdk.util.state.State;\n@@ -294,6 +295,27 @@ protected final void setupDelegateAggregators() {\n     public abstract PaneInfo pane();\n   }\n \n+  /**\n+   * Information accessible when running a {@link DoFn.OnTimer} method.\n+   */\n+  public abstract class OnTimerContext extends Context {\n+\n+    /**\n+     * Returns the timestamp of the current timer.\n+     */\n+    public abstract Instant timestamp();\n+\n+    /**\n+     * Returns the window in which the timer is firing.\n+     */\n+    public abstract BoundedWindow window();\n+\n+    /**\n+     * Returns the time domain of the current timer.\n+     */\n+    public abstract TimeDomain timeDomain();\n+  }\n+\n   /**\n    * Returns the allowed timestamp skew duration, which is the maximum\n    * duration that timestamps can be shifted backward in",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java",
                "sha": "699403f5b1a1c97c2bf2701f3118f90aed41bce7",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnAdapters.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnAdapters.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnAdapters.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.beam.sdk.options.PipelineOptions;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n import org.apache.beam.sdk.transforms.DoFn.Context;\n+import org.apache.beam.sdk.transforms.DoFn.OnTimerContext;\n import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n import org.apache.beam.sdk.transforms.display.DisplayData;\n import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;\n@@ -344,6 +345,12 @@ public ProcessContext processContext(DoFn<InputT, OutputT> doFn) {\n           \"Can only get a ProcessContext in processElement\");\n     }\n \n+    @Override\n+    public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\n+          \"Timers are not supported for OldDoFn\");\n+    }\n+\n     @Override\n     public WindowingInternals<InputT, OutputT> windowingInternals() {\n       // The OldDoFn doesn't allow us to ask for these outside ProcessElements, so this\n@@ -459,6 +466,11 @@ public ProcessContext processContext(DoFn<InputT, OutputT> doFn) {\n       return this;\n     }\n \n+    @Override\n+    public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\"Timers are not supported for OldDoFn\");\n+    }\n+\n     @Override\n     public WindowingInternals<InputT, OutputT> windowingInternals() {\n       return context.windowingInternals();",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnAdapters.java",
                "sha": "e15b08b46d791365ff6742f1919480ecc42e1495",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnTester.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnTester.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 12,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnTester.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.beam.sdk.options.PipelineOptionsFactory;\n import org.apache.beam.sdk.testing.ValueInSingleWindow;\n import org.apache.beam.sdk.transforms.Combine.CombineFn;\n+import org.apache.beam.sdk.transforms.DoFn.OnTimerContext;\n import org.apache.beam.sdk.transforms.reflect.DoFnInvoker;\n import org.apache.beam.sdk.transforms.reflect.DoFnInvokers;\n import org.apache.beam.sdk.transforms.reflect.DoFnSignature;\n@@ -52,7 +53,6 @@\n import org.apache.beam.sdk.util.state.InMemoryStateInternals;\n import org.apache.beam.sdk.util.state.InMemoryTimerInternals;\n import org.apache.beam.sdk.util.state.StateInternals;\n-import org.apache.beam.sdk.util.state.TimerCallback;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TimestampedValue;\n import org.apache.beam.sdk.values.TupleTag;\n@@ -316,6 +316,12 @@ public BoundedWindow window() {\n               return processContext;\n             }\n \n+            @Override\n+            public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+              throw new UnsupportedOperationException(\n+                  \"DoFnTester doesn't support timers yet.\");\n+            }\n+\n             @Override\n             public DoFn.InputProvider<InputT> inputProvider() {\n               throw new UnsupportedOperationException(\n@@ -536,19 +542,14 @@ public T apply(ValueInSingleWindow<T> input) {\n     return extractAggregatorValue(agg.getName(), agg.getCombineFn());\n   }\n \n-  private static TimerCallback collectInto(final List<TimerInternals.TimerData> firedTimers) {\n-    return new TimerCallback() {\n-      @Override\n-      public void onTimer(TimerInternals.TimerData timer) throws Exception {\n-        firedTimers.add(timer);\n-      }\n-    };\n-  }\n-\n   public List<TimerInternals.TimerData> advanceInputWatermark(Instant newWatermark) {\n     try {\n+      timerInternals.advanceInputWatermark(newWatermark);\n       final List<TimerInternals.TimerData> firedTimers = new ArrayList<>();\n-      timerInternals.advanceInputWatermark(collectInto(firedTimers), newWatermark);\n+      TimerInternals.TimerData timer;\n+      while ((timer = timerInternals.removeNextEventTimer()) != null) {\n+        firedTimers.add(timer);\n+      }\n       return firedTimers;\n     } catch (Exception e) {\n       throw new RuntimeException(e);\n@@ -557,8 +558,12 @@ public void onTimer(TimerInternals.TimerData timer) throws Exception {\n \n   public List<TimerInternals.TimerData> advanceProcessingTime(Instant newProcessingTime) {\n     try {\n+      timerInternals.advanceProcessingTime(newProcessingTime);\n       final List<TimerInternals.TimerData> firedTimers = new ArrayList<>();\n-      timerInternals.advanceProcessingTime(collectInto(firedTimers), newProcessingTime);\n+      TimerInternals.TimerData timer;\n+      while ((timer = timerInternals.removeNextProcessingTimer()) != null) {\n+        firedTimers.add(timer);\n+      }\n       return firedTimers;\n     } catch (Exception e) {\n       throw new RuntimeException(e);",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFnTester.java",
                "sha": "93b3f5954898728ddba617062f84b33219ac431c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Filter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Filter.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Filter.java",
                "patch": "@@ -201,7 +201,7 @@ private Filter(SerializableFunction<T, Boolean> predicate,\n   }\n \n   @Override\n-  public PCollection<T> apply(PCollection<T> input) {\n+  public PCollection<T> expand(PCollection<T> input) {\n     return input.apply(ParDo.of(new DoFn<T, T>() {\n       @ProcessElement\n       public void processElement(ProcessContext c) {",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Filter.java",
                "sha": "a564999f8ec8fd0f5b91b23c96415cc2386f7987",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/FlatMapElements.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/FlatMapElements.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/FlatMapElements.java",
                "patch": "@@ -129,7 +129,7 @@ private FlatMapElements(\n   }\n \n   @Override\n-  public PCollection<OutputT> apply(PCollection<? extends InputT> input) {\n+  public PCollection<OutputT> expand(PCollection<? extends InputT> input) {\n     return input.apply(\n         \"FlatMap\",\n         ParDo.of(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/FlatMapElements.java",
                "sha": "c165f7f12929236c45dcbdf4689db1065b8e65e5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Flatten.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Flatten.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Flatten.java",
                "patch": "@@ -105,7 +105,7 @@\n     private FlattenPCollectionList() { }\n \n     @Override\n-    public PCollection<T> apply(PCollectionList<T> inputs) {\n+    public PCollection<T> expand(PCollectionList<T> inputs) {\n       WindowingStrategy<?, ?> windowingStrategy;\n       IsBounded isBounded = IsBounded.BOUNDED;\n       if (!inputs.getAll().isEmpty()) {\n@@ -163,7 +163,7 @@ private FlattenPCollectionList() { }\n       extends PTransform<PCollection<? extends Iterable<T>>, PCollection<T>> {\n \n     @Override\n-    public PCollection<T> apply(PCollection<? extends Iterable<T>> in) {\n+    public PCollection<T> expand(PCollection<? extends Iterable<T>> in) {\n       Coder<? extends Iterable<T>> inCoder = in.getCoder();\n       if (!(inCoder instanceof IterableLikeCoder)) {\n         throw new IllegalArgumentException(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Flatten.java",
                "sha": "3ef2e555677ef866908b593ed47839d4e50ba0a3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupByKey.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupByKey.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupByKey.java",
                "patch": "@@ -216,7 +216,7 @@ public void validate(PCollection<KV<K, V>> input) {\n   }\n \n   @Override\n-  public PCollection<KV<K, Iterable<V>>> apply(PCollection<KV<K, V>> input) {\n+  public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n     // This primitive operation groups by the combination of key and window,\n     // merging windows as needed, using the windows assigned to the\n     // key/value input elements and the window merge operation of the",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupByKey.java",
                "sha": "a339af74f514e7dd4a0e056f30c35fba2d520d7d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Keys.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Keys.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Keys.java",
                "patch": "@@ -56,7 +56,7 @@\n   private Keys() { }\n \n   @Override\n-  public PCollection<K> apply(PCollection<? extends KV<K, ?>> in) {\n+  public PCollection<K> expand(PCollection<? extends KV<K, ?>> in) {\n     return\n         in.apply(\"Keys\", MapElements.via(new SimpleFunction<KV<K, ?>, K>() {\n           @Override",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Keys.java",
                "sha": "c6f307de692a8184c61c7ea5ad382a5bf062f32e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/KvSwap.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/KvSwap.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/KvSwap.java",
                "patch": "@@ -60,7 +60,7 @@\n   private KvSwap() { }\n \n   @Override\n-  public PCollection<KV<V, K>> apply(PCollection<KV<K, V>> in) {\n+  public PCollection<KV<V, K>> expand(PCollection<KV<K, V>> in) {\n     return\n         in.apply(\"KvSwap\", MapElements.via(new SimpleFunction<KV<K, V>, KV<V, K>>() {\n           @Override",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/KvSwap.java",
                "sha": "dbe262b82d206be0d2e3a75536f7836548fe896a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Latest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Latest.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 2,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Latest.java",
                "patch": "@@ -159,7 +159,7 @@ public T extractOutput(TimestampedValue<T> accumulator) {\n \n   private static class Globally<T> extends PTransform<PCollection<T>, PCollection<T>> {\n     @Override\n-    public PCollection<T> apply(PCollection<T> input) {\n+    public PCollection<T> expand(PCollection<T> input) {\n       Coder<T> inputCoder = input.getCoder();\n \n       return input\n@@ -178,7 +178,7 @@ public void processElement(ProcessContext c) {\n   private static class PerKey<K, V>\n       extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, V>>> {\n     @Override\n-    public PCollection<KV<K, V>> apply(PCollection<KV<K, V>> input) {\n+    public PCollection<KV<K, V>> expand(PCollection<KV<K, V>> input) {\n       checkNotNull(input);\n       checkArgument(input.getCoder() instanceof KvCoder,\n           \"Input specifiedCoder must be an instance of KvCoder, but was %s\", input.getCoder());",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Latest.java",
                "sha": "9c2d7154e1552d2cd5fcf03e4d1248df72ed3b98",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/MapElements.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/MapElements.java?ref=4c445dd0b6de0f5045c02579cb432da4fbc5d486",
                "deletions": 1,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/MapElements.java",
                "patch": "@@ -111,7 +111,7 @@ private MapElements(SimpleFunction<InputT, OutputT> fn, Class<?> fnClass) {\n   }\n \n   @Override\n-  public PCollection<OutputT> apply(PCollection<? extends InputT> input) {\n+  public PCollection<OutputT> expand(PCollection<? extends InputT> input) {\n     return input.apply(\n         \"Map\",\n         ParDo.of(",
                "raw_url": "https://github.com/apache/beam/raw/4c445dd0b6de0f5045c02579cb432da4fbc5d486/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/MapElements.java",
                "sha": "421b2abb1a518b48b9798b1f86494d7900d1e829",
                "status": "modified"
            }
        ],
        "message": "This closes #1663: Merge master (b3de17b) into gearpump-runner\n\nAdjustments in gearpump-runner:\n\n  [BEAM-79] Upgrade to beam-0.5.0-incubating-SNAPSHOT\n  [BEAM-79] Update to latest Gearpump API\n\nFrom master:\n\n  Disable automatic archiving of Maven builds\n  [BEAM-59] initial interfaces and classes of Beam FileSystem.\n  Change counter name in TestDataflowRunner\n  More escaping in Jenkins timestamp spec\n  Add RunnableOnService test for Metrics\n  Fix seed job fetch spec\n  Show timestamps on log lines in Jenkins\n  [BEAM-1165] Fix unexpected file creation when checking dependencies\n  [BEAM-1178] Make naming of logger objects consistent\n  [BEAM-716] Fix javadoc on with* methods [BEAM-959] Improve check preconditions in JmsIO\n  [BEAM-716] Use AutoValue in JmsIO\n  Fix grammar error (repeated for)\n  Empty TestPipeline need not be run\n  [BEAM-85, BEAM-298] Make TestPipeline a JUnit Rule checking proper usage\n  Change counter name in TestDataflowRunner\n  BigQueryIO: fix streaming write, typo in API\n  [BEAM-853] Force streaming execution on batch pipelines for testing. Expose the adapted source.\n  Use empty SideInputReader, fixes NPE in SimpleDoFnRunnerTest\n  Test that SimpleDoFnRunner wraps exceptions in startBundle and finishBundle\n  Add timer support to DoFnRunner(s)\n  Make TimerSpec and StateSpec fields accessible\n  View.asMap: minor javadoc fixes\n  Revert \"Move InMemoryTimerInternals to runners-core\"\n  Revert \"Moves DoFnAdapters to runners-core\"\n  Revert \"Removes ArgumentProvider.windowingInternals\"\n  Revert \"Removes code for wrapping DoFn as an OldDoFn\"\n  checkstyle: missed newline in DistributionCell\n  Make {Metric,Counter,Distribution}Cell public\n  Add PTransformOverrideFactory to the Core SDK\n  Move ActiveWindowSet and implementations to runners-core\n  Update Dataflow worker to beam-master-20161216\n  [BEAM-1108] Remove outdated language about experimental autoscaling\n  [BEAM-450] Shade modules to separate paths\n  [BEAM-362] Port runners to runners-core AggregatoryFactory\n  Move InMemoryTimerInternals to runners-core\n  Delete deprecated TimerCallback\n  Remove deprecated methods of InMemoryTimerInternals\n  Don't incorrectly log error in MetricsEnvironment\n  Renames ParDo.getNewFn to getFn\n  Moves DoFnAdapters to runners-core\n  Removes unused code from NoOpOldDoFn\n  Removes ArgumentProvider.windowingInternals\n  Removes code for wrapping DoFn as an OldDoFn\n  Removes OldDoFn from ParDo\n  Pushes uses of OldDoFn deeper inside Flink runner\n  Remove ParDo.of(OldDoFn) from Apex runner\n  Converts all easy OldDoFns to DoFn\n  [BEAM-1022] Add testing coverage for BigQuery streaming writes\n  Fix mvn command args in Apex postcommit Jenkins job\n  [BEAM-932] Enable findbugs validation (and fix existing issues)\n  Fail to split in FileBasedSource if filePattern expands to empty.\n  [BEAM-1154] Get side input from proper window in ReduceFn\n  [BEAM-1153] GcsUtil: use non-batch API for single file size requests.\n  Fix NPE in StatefulParDoEvaluatorFactoryTest mocking\n  [BEAM-1033] Retry Bigquery Verifier when Query Fails\n  Implement GetDefaultOutputCoder in DirectGroupByKey\n  SimpleDoFnRunner observes window if SideInputReader is nonempty\n  Better comments and cleanup\n  Allow empty string value for ValueProvider types.\n  starter: fix typo in pom.xml\n  Revert \"Allow stateful DoFn in DataflowRunner\"\n  Re-exclude UsesStatefulParDo tests for Dataflow\n  Some minor changes and fixes for sorter module\n  [BEAM-1149] Explode windows when fn uses side inputs\n  Add Jenkins postcommit for RunnableOnService in Apex runner\n  Update version from 0.5.0-SNAPSHOT to 0.5.0-incubating-SNAPSHOT\n  Update Maven Archetype versions after cutting the release branch\n  Move PerKeyCombineFnRunner to runners-core\n  Update Dataflow worker to beam-master-20161212\n  [maven-release-plugin] prepare for next development iteration\n  [maven-release-plugin] prepare branch release-0.4.0-incubating\n  Fix version of Kryo in examples/java jenkins-precommit profile\n  Revert 91cc606 \"This closes #1586\": Kryo + UBRFBS\n  [BEAM-909] improve starter archetype\n  Fix JDom malformed comment in Apex runner.\n  [BEAM-927] Fix findbugs and re-enable Maven plugin in JmsIO\n  [BEAM-807] Replace OldDoFn with DoFn.\n  [BEAM-757] Use DoFnRunner in the implementation of DoFn via FlatMapFunction.\n  FileBasedSinkTest: fix tests in Windows OS by using IOChannelUtils.resolve().\n  FileBasedSink: ignore exceptions when removing temp output files for issues in Windows OS.\n  [BEAM-1142] Upgrade maven-invoker to address maven bug ARCHETYPE-488.\n  Add Tests for Kryo Serialization of URFBS\n  Add no-arg constructor for UnboundedReadFromBoundedSource\n  Revise WindowedWordCount for runner and execution mode portability\n  Factor out ShardedFile from FileChecksumMatcher\n  Add IntervalWindow coder to the standard registry\n  Stop expanding PValues in DirectRunner visitors\n  Migrate AppliedPTransform to use AutoValue\n  Enable and fix DirectRunnerTest case missing @Test\n  [BEAM-1130] SparkRunner ResumeFromCheckpointStreamingTest Failing.\n  [BEAM-1133] Add maxNumRecords per micro-batch for Spark runner options.\n  BigQueryIO.Write: support runtime schema and table\n  Fix handling of null ValueProviders in DisplayData\n  [BEAM-551] Fix handling of default for VP\n  [BEAM-1120] Move some DataflowRunner configurations from code to properties\n  [BEAM-551] Fix toString for FileBasedSource\n  [BEAM-921] spark-runner: register sources and coders to serialize with java serializer\n  [BEAM-551] Fix handling of TextIO.Sink\n  ...",
        "parent": "https://github.com/apache/beam/commit/b6e7bb659f33e346c00e66ca96e3c54dd7ef07da",
        "repo": "beam",
        "unit_tests": [
            "MapElementsTest.java"
        ]
    },
    "beam_51b56ba": {
        "bug_id": "beam_51b56ba",
        "commit": "https://github.com/apache/beam/commit/51b56ba4361591c7a38cbc68f958d70847b0547e",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/51b56ba4361591c7a38cbc68f958d70847b0547e/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java?ref=51b56ba4361591c7a38cbc68f958d70847b0547e",
                "deletions": 2,
                "filename": "runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java",
                "patch": "@@ -1540,7 +1540,8 @@ private void getConfigFromWindmill(String computation) {\n     for (Windmill.GetConfigResponse.ComputationConfigMapEntry computationConfig :\n         response.getComputationConfigMapList()) {\n       Map<String, String> transformUserNameToStateFamily =\n-          transformUserNameToStateFamilyByComputationId.get(computationConfig.getComputationId());\n+          transformUserNameToStateFamilyByComputationId.computeIfAbsent(\n+              computationConfig.getComputationId(), k -> new HashMap<>());\n       for (Windmill.ComputationConfig.TransformUserNameToStateFamilyEntry entry :\n           computationConfig.getComputationConfig().getTransformUserNameToStateFamilyList()) {\n         transformUserNameToStateFamily.put(entry.getTransformUserName(), entry.getStateFamily());\n@@ -1949,7 +1950,10 @@ public ComputationState(\n       this.computationId = computationId;\n       this.mapTask = mapTask;\n       this.executor = executor;\n-      this.transformUserNameToStateFamily = ImmutableMap.copyOf(transformUserNameToStateFamily);\n+      this.transformUserNameToStateFamily =\n+          transformUserNameToStateFamily != null\n+              ? ImmutableMap.copyOf(transformUserNameToStateFamily)\n+              : ImmutableMap.of();\n       Preconditions.checkNotNull(mapTask.getStageName());\n       Preconditions.checkNotNull(mapTask.getSystemName());\n     }",
                "raw_url": "https://github.com/apache/beam/raw/51b56ba4361591c7a38cbc68f958d70847b0547e/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java",
                "sha": "a6e8df6d2b0badd3a3cb632596e8a22121241271",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in ComputationState constructor introduced by PR/7846. The root cause was assuming an empty transformUserNameToStateFamily map was passed, when instead null was passed.",
        "parent": "https://github.com/apache/beam/commit/a06d9eb91e4025b4dad07ff830a0de57cb033a98",
        "repo": "beam",
        "unit_tests": [
            "StreamingDataflowWorkerTest.java"
        ]
    },
    "beam_61a8b10": {
        "bug_id": "beam_61a8b10",
        "commit": "https://github.com/apache/beam/commit/61a8b10dbddf93cb63faa0957d55695ab69ad81b",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/beam/blob/61a8b10dbddf93cb63faa0957d55695ab69ad81b/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java?ref=61a8b10dbddf93cb63faa0957d55695ab69ad81b",
                "deletions": 4,
                "filename": "runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "patch": "@@ -177,10 +177,28 @@ public void processElement(\n                         watermark)));\n           } else {\n             // End of input. Close the reader after finalizing old checkpoint.\n-            shard.getCheckpoint().finalizeCheckpoint();\n-            UnboundedReader<?> toClose = reader;\n-            reader = null; // Avoid double close below in case of an exception.\n-            toClose.close();\n+            // note: can be null for empty datasets so ensure to null check the checkpoint\n+            final CheckpointMarkT checkpoint = shard.getCheckpoint();\n+            IOException ioe = null;\n+            try {\n+              if (checkpoint != null) {\n+                checkpoint.finalizeCheckpoint();\n+              }\n+            } catch (final IOException finalizeCheckpointException) {\n+              ioe = finalizeCheckpointException;\n+            } finally {\n+              try {\n+                UnboundedReader<?> toClose = reader;\n+                reader = null; // Avoid double close below in case of an exception.\n+                toClose.close();\n+              } catch (final IOException closeEx) {\n+                if (ioe != null) {\n+                  ioe.addSuppressed(closeEx);\n+                } else {\n+                  throw closeEx;\n+                }\n+              }\n+            }\n           }\n         }\n       } catch (IOException e) {",
                "raw_url": "https://github.com/apache/beam/raw/61a8b10dbddf93cb63faa0957d55695ab69ad81b/runners/direct-java/src/main/java/org/apache/beam/runners/direct/UnboundedReadEvaluatorFactory.java",
                "sha": "a46d657abb47920c28df7f04bb2401a9488d6466",
                "status": "modified"
            }
        ],
        "message": "BEAM-3876 avoid NPE if checkpoint is null in an unbounded source",
        "parent": "https://github.com/apache/beam/commit/a6536f936b78e79058829740ba854ab3e82c6713",
        "repo": "beam",
        "unit_tests": [
            "UnboundedReadEvaluatorFactoryTest.java"
        ]
    },
    "beam_967fa79": {
        "bug_id": "beam_967fa79",
        "commit": "https://github.com/apache/beam/commit/967fa79aeab7a5eb91fa0ff9299e80a958acffe2",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/967fa79aeab7a5eb91fa0ff9299e80a958acffe2/sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java?ref=967fa79aeab7a5eb91fa0ff9299e80a958acffe2",
                "deletions": 1,
                "filename": "sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java",
                "patch": "@@ -332,7 +332,7 @@ public boolean requiresDeduping() {\n   private static class RabbitMQCheckpointMark\n       implements UnboundedSource.CheckpointMark, Serializable {\n     transient Channel channel;\n-    Instant oldestTimestamp;\n+    Instant oldestTimestamp = Instant.now();\n     final List<Long> sessionIds = new ArrayList<>();\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/967fa79aeab7a5eb91fa0ff9299e80a958acffe2/sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java",
                "sha": "61ef907b0707a5b164800a9ad429ed030fadf19d",
                "status": "modified"
            }
        ],
        "message": "[BEAM-6424] Avoid NPE when getWatermark() is called whereas the oldest timestamp is not yet set.",
        "parent": "https://github.com/apache/beam/commit/21f4b54ddb9e7fd81706069da9f1a4b5966a43d0",
        "repo": "beam",
        "unit_tests": [
            "RabbitMqIOTest.java"
        ]
    },
    "beam_9de343c": {
        "bug_id": "beam_9de343c",
        "commit": "https://github.com/apache/beam/commit/9de343c4185bdd7b741087fcb6088422d1d22adc",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/beam/blob/9de343c4185bdd7b741087fcb6088422d1d22adc/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java?ref=9de343c4185bdd7b741087fcb6088422d1d22adc",
                "deletions": 2,
                "filename": "runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java",
                "patch": "@@ -1540,7 +1540,8 @@ private void getConfigFromWindmill(String computation) {\n     for (Windmill.GetConfigResponse.ComputationConfigMapEntry computationConfig :\n         response.getComputationConfigMapList()) {\n       Map<String, String> transformUserNameToStateFamily =\n-          transformUserNameToStateFamilyByComputationId.get(computationConfig.getComputationId());\n+          transformUserNameToStateFamilyByComputationId.computeIfAbsent(\n+              computationConfig.getComputationId(), k -> new HashMap<>());\n       for (Windmill.ComputationConfig.TransformUserNameToStateFamilyEntry entry :\n           computationConfig.getComputationConfig().getTransformUserNameToStateFamilyList()) {\n         transformUserNameToStateFamily.put(entry.getTransformUserName(), entry.getStateFamily());\n@@ -1949,7 +1950,10 @@ public ComputationState(\n       this.computationId = computationId;\n       this.mapTask = mapTask;\n       this.executor = executor;\n-      this.transformUserNameToStateFamily = ImmutableMap.copyOf(transformUserNameToStateFamily);\n+      this.transformUserNameToStateFamily =\n+          transformUserNameToStateFamily != null\n+              ? ImmutableMap.copyOf(transformUserNameToStateFamily)\n+              : ImmutableMap.of();\n       Preconditions.checkNotNull(mapTask.getStageName());\n       Preconditions.checkNotNull(mapTask.getSystemName());\n     }",
                "raw_url": "https://github.com/apache/beam/raw/9de343c4185bdd7b741087fcb6088422d1d22adc/runners/google-cloud-dataflow-java/worker/src/main/java/org/apache/beam/runners/dataflow/worker/StreamingDataflowWorker.java",
                "sha": "a6e8df6d2b0badd3a3cb632596e8a22121241271",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #7869: Fix NPE in ComputationState constructor introduced by PR/7846",
        "parent": "https://github.com/apache/beam/commit/34e4bcaee0464ff426496d3f16f71465023e917b",
        "repo": "beam",
        "unit_tests": [
            "StreamingDataflowWorkerTest.java"
        ]
    },
    "beam_b861827": {
        "bug_id": "beam_b861827",
        "commit": "https://github.com/apache/beam/commit/b8618271d77235a881225954c9426e8edb4ec3db",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/b8618271d77235a881225954c9426e8edb4ec3db/sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java?ref=b8618271d77235a881225954c9426e8edb4ec3db",
                "deletions": 1,
                "filename": "sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java",
                "patch": "@@ -332,7 +332,7 @@ public boolean requiresDeduping() {\n   private static class RabbitMQCheckpointMark\n       implements UnboundedSource.CheckpointMark, Serializable {\n     transient Channel channel;\n-    Instant oldestTimestamp;\n+    Instant oldestTimestamp = Instant.now();\n     final List<Long> sessionIds = new ArrayList<>();\n \n     @Override",
                "raw_url": "https://github.com/apache/beam/raw/b8618271d77235a881225954c9426e8edb4ec3db/sdks/java/io/rabbitmq/src/main/java/org/apache/beam/sdk/io/rabbitmq/RabbitMqIO.java",
                "sha": "61ef907b0707a5b164800a9ad429ed030fadf19d",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #7602: [BEAM-6424] Avoid NPE when getWatermark() is called whereas the oldest timestamp is not yet set",
        "parent": "https://github.com/apache/beam/commit/5b46b02b49ca1c5c18682427a5a4a25596ca4287",
        "repo": "beam",
        "unit_tests": [
            "RabbitMqIOTest.java"
        ]
    },
    "beam_bbfe8a7": {
        "bug_id": "beam_bbfe8a7",
        "commit": "https://github.com/apache/beam/commit/bbfe8a75c1905fe8a1f04bfc4703799f01609943",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/beam/blob/bbfe8a75c1905fe8a1f04bfc4703799f01609943/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java?ref=bbfe8a75c1905fe8a1f04bfc4703799f01609943",
                "deletions": 2,
                "filename": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "patch": "@@ -29,6 +29,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.concurrent.ScheduledFuture;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.stream.Collectors;\n@@ -434,9 +435,9 @@ public void open() throws Exception {\n   @Override\n   public void dispose() throws Exception {\n     try {\n-      checkFinishBundleTimer.cancel(true);\n+      Optional.ofNullable(checkFinishBundleTimer).ifPresent(timer -> timer.cancel(true));\n       FlinkClassloading.deleteStaticCaches();\n-      doFnInvoker.invokeTeardown();\n+      Optional.ofNullable(doFnInvoker).ifPresent(DoFnInvoker::invokeTeardown);\n     } finally {\n       // This releases all task's resources. We need to call this last\n       // to ensure that state, timers, or output buffers can still be",
                "raw_url": "https://github.com/apache/beam/raw/bbfe8a75c1905fe8a1f04bfc4703799f01609943/runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/DoFnOperator.java",
                "sha": "aa1f18025faa78955e4a57352e6f1926024b4849",
                "status": "modified"
            }
        ],
        "message": "[BEAM-7091] fix NPE in DoFnOperator#dispose",
        "parent": "https://github.com/apache/beam/commit/21f1b0dab7ccb35f04bf0a0dc908f45c19a5d8c7",
        "repo": "beam",
        "unit_tests": [
            "DoFnOperatorTest.java"
        ]
    },
    "beam_c28957d": {
        "bug_id": "beam_c28957d",
        "commit": "https://github.com/apache/beam/commit/c28957d16fb0f63f82f578cf904df61bf7bb63e5",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java",
                "patch": "@@ -20,8 +20,11 @@\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.DoFn;\n import org.apache.beam.sdk.transforms.OldDoFn;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.KV;\n+import org.joda.time.Instant;\n \n /**\n  * An wrapper interface that represents the execution of a {@link DoFn}.\n@@ -38,6 +41,12 @@\n    */\n   void processElement(WindowedValue<InputT> elem);\n \n+  /**\n+   * Calls a {@link DoFn DoFn's} {@link DoFn.OnTimer @OnTimer} method for the given timer\n+   * in the given window.\n+   */\n+  void onTimer(String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain);\n+\n   /**\n    * Calls a {@link DoFn DoFn's} {@link DoFn.FinishBundle @FinishBundle} method and performs\n    * additional tasks, such as flushing in-memory states.",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/DoFnRunner.java",
                "sha": "7c73a349155695a7ac2452e92d343093170ae865",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.beam.sdk.transforms.Aggregator;\n import org.apache.beam.sdk.transforms.OldDoFn;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.WindowTracing;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -72,6 +73,12 @@ public void processElement(WindowedValue<KeyedWorkItem<K, InputT>> elem) {\n     doFnRunner.processElement(elem.withValue(keyedWorkItem));\n   }\n \n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+      TimeDomain timeDomain) {\n+    doFnRunner.onTimer(timerId, window, timestamp, timeDomain);\n+  }\n+\n   @Override\n   public void finishBundle() {\n     doFnRunner.finishBundle();",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/LateDataDroppingDoFnRunner.java",
                "sha": "290171ad22800375bb01ac7ecd180e0cbe58d272",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "patch": "@@ -25,8 +25,10 @@\n import java.util.Set;\n import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.values.PCollectionView;\n+import org.joda.time.Instant;\n \n /**\n  * A {@link DoFnRunner} that can refuse to process elements that are not ready, instead returning\n@@ -109,6 +111,12 @@ public void processElement(WindowedValue<InputT> elem) {\n     underlying.processElement(elem);\n   }\n \n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+      TimeDomain timeDomain) {\n+    underlying.onTimer(timerId, window, timestamp, timeDomain);\n+  }\n+\n   /**\n    * Call the underlying {@link DoFnRunner#finishBundle()}.\n    */",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunner.java",
                "sha": "2962832a5096cc729f098b1eda532ccec75f013b",
                "status": "modified"
            },
            {
                "additions": 235,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "changes": 236,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 1,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "patch": "@@ -50,8 +50,10 @@\n import org.apache.beam.sdk.util.ExecutionContext.StepContext;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.Timer;\n import org.apache.beam.sdk.util.TimerInternals;\n+import org.apache.beam.sdk.util.TimerSpec;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n import org.apache.beam.sdk.util.WindowingInternals;\n@@ -64,6 +66,7 @@\n import org.apache.beam.sdk.util.state.StateTags;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.apache.beam.sdk.values.TupleTag;\n+import org.joda.time.Duration;\n import org.joda.time.Instant;\n import org.joda.time.format.PeriodFormat;\n \n@@ -161,6 +164,35 @@ public void processElement(WindowedValue<InputT> compressedElem) {\n     }\n   }\n \n+  @Override\n+  public void onTimer(\n+      String timerId, BoundedWindow window, Instant timestamp, TimeDomain timeDomain) {\n+\n+    // The effective timestamp is when derived elements will have their timestamp set, if not\n+    // otherwise specified. If this is an event time timer, then they have the timestamp of the\n+    // timer itself. Otherwise, they are set to the input timestamp, which is by definition\n+    // non-late.\n+    Instant effectiveTimestamp;\n+    switch (timeDomain) {\n+      case EVENT_TIME:\n+        effectiveTimestamp = timestamp;\n+        break;\n+\n+      case PROCESSING_TIME:\n+      case SYNCHRONIZED_PROCESSING_TIME:\n+        effectiveTimestamp = context.stepContext.timerInternals().currentInputWatermarkTime();\n+        break;\n+\n+      default:\n+        throw new IllegalArgumentException(\n+            String.format(\"Unknown time domain: %s\", timeDomain));\n+    }\n+\n+    OnTimerArgumentProvider<InputT, OutputT> argumentProvider =\n+        new OnTimerArgumentProvider<>(fn, context, window, effectiveTimestamp, timeDomain);\n+    invoker.invokeOnTimer(timerId, argumentProvider);\n+  }\n+\n   private void invokeProcessElement(WindowedValue<InputT> elem) {\n     final DoFnProcessContext<InputT, OutputT> processContext = createProcessContext(elem);\n \n@@ -630,7 +662,13 @@ public State state(String stateId) {\n \n     @Override\n     public Timer timer(String timerId) {\n-      throw new UnsupportedOperationException(\"Timer parameters are not supported.\");\n+      try {\n+        TimerSpec spec =\n+            (TimerSpec) signature.timerDeclarations().get(timerId).field().get(fn);\n+        return new TimerInternalsTimer(getNamespace(), timerId, spec, stepContext.timerInternals());\n+      } catch (IllegalAccessException e) {\n+        throw new RuntimeException(e);\n+      }\n     }\n \n     @Override\n@@ -682,5 +720,201 @@ public void outputWindowedValue(\n         }\n       };\n     }\n+\n+  }\n+\n+  /**\n+   * A concrete implementation of {@link DoFnInvoker.ArgumentProvider} used for running a {@link\n+   * DoFn} on a timer.\n+   *\n+   * @param <InputT> the type of the {@link DoFn} (main) input elements\n+   * @param <OutputT> the type of the {@link DoFn} (main) output elements\n+   */\n+  private class OnTimerArgumentProvider<InputT, OutputT>\n+      extends DoFn<InputT, OutputT>.OnTimerContext\n+      implements DoFnInvoker.ArgumentProvider<InputT, OutputT> {\n+\n+    final DoFn<InputT, OutputT> fn;\n+    final DoFnContext<InputT, OutputT> context;\n+    private final BoundedWindow window;\n+    private final Instant timestamp;\n+    private final TimeDomain timeDomain;\n+\n+    /** Lazily initialized; should only be accessed via {@link #getNamespace()}. */\n+    private StateNamespace namespace;\n+\n+    /**\n+     * The state namespace for this context.\n+     *\n+     * <p>Any call to {@link #getNamespace()} when more than one window is present will crash; this\n+     * represents a bug in the runner or the {@link DoFnSignature}, since values must be in exactly\n+     * one window when state or timers are relevant.\n+     */\n+    private StateNamespace getNamespace() {\n+      if (namespace == null) {\n+        namespace = StateNamespaces.window(windowCoder, window);\n+      }\n+      return namespace;\n+    }\n+\n+    private OnTimerArgumentProvider(\n+        DoFn<InputT, OutputT> fn,\n+        DoFnContext<InputT, OutputT> context,\n+        BoundedWindow window,\n+        Instant timestamp,\n+        TimeDomain timeDomain) {\n+      fn.super();\n+      this.fn = fn;\n+      this.context = context;\n+      this.window = window;\n+      this.timestamp = timestamp;\n+      this.timeDomain = timeDomain;\n+    }\n+\n+    @Override\n+    public Instant timestamp() {\n+      return timestamp;\n+    }\n+\n+    @Override\n+    public BoundedWindow window() {\n+      return window;\n+    }\n+\n+    @Override\n+    public TimeDomain timeDomain() {\n+      return timeDomain;\n+    }\n+\n+    @Override\n+    public Context context(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\"Context parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public ProcessContext processContext(DoFn<InputT, OutputT> doFn) {\n+      throw new UnsupportedOperationException(\"ProcessContext parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public OnTimerContext onTimerContext(DoFn<InputT, OutputT> doFn) {\n+      return this;\n+    }\n+\n+    @Override\n+    public InputProvider<InputT> inputProvider() {\n+      throw new UnsupportedOperationException(\"InputProvider parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public OutputReceiver<OutputT> outputReceiver() {\n+      throw new UnsupportedOperationException(\"OutputReceiver parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public <RestrictionT> RestrictionTracker<RestrictionT> restrictionTracker() {\n+      throw new UnsupportedOperationException(\"RestrictionTracker parameters are not supported.\");\n+    }\n+\n+    @Override\n+    public State state(String stateId) {\n+      try {\n+        StateSpec<?, ?> spec =\n+            (StateSpec<?, ?>) signature.stateDeclarations().get(stateId).field().get(fn);\n+        return stepContext\n+            .stateInternals()\n+            .state(getNamespace(), StateTags.tagForSpec(stateId, (StateSpec) spec));\n+      } catch (IllegalAccessException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public Timer timer(String timerId) {\n+      try {\n+        TimerSpec spec =\n+            (TimerSpec) signature.timerDeclarations().get(timerId).field().get(fn);\n+        return new TimerInternalsTimer(getNamespace(), timerId, spec, stepContext.timerInternals());\n+      } catch (IllegalAccessException e) {\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public PipelineOptions getPipelineOptions() {\n+      return context.getPipelineOptions();\n+    }\n+\n+    @Override\n+    public void output(OutputT output) {\n+      context.outputWithTimestamp(output, timestamp);\n+    }\n+\n+    @Override\n+    public void outputWithTimestamp(OutputT output, Instant timestamp) {\n+      context.outputWithTimestamp(output, timestamp);\n+    }\n+\n+    @Override\n+    public <T> void sideOutput(TupleTag<T> tag, T output) {\n+      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    }\n+\n+    @Override\n+    public <T> void sideOutputWithTimestamp(TupleTag<T> tag, T output, Instant timestamp) {\n+      context.sideOutputWithTimestamp(tag, output, timestamp);\n+    }\n+\n+    @Override\n+    protected <AggInputT, AggOutputT> Aggregator<AggInputT, AggOutputT> createAggregator(\n+        String name,\n+        CombineFn<AggInputT, ?, AggOutputT> combiner) {\n+      throw new UnsupportedOperationException(\"Cannot createAggregator in @OnTimer method\");\n+    }\n+\n+    @Override\n+    public WindowingInternals<InputT, OutputT> windowingInternals() {\n+      throw new UnsupportedOperationException(\"WindowingInternals are unsupported.\");\n+    }\n+  }\n+\n+  private static class TimerInternalsTimer implements Timer {\n+    private final TimerInternals timerInternals;\n+    private final String timerId;\n+    private final TimerSpec spec;\n+    private final StateNamespace namespace;\n+\n+    public TimerInternalsTimer(\n+        StateNamespace namespace, String timerId, TimerSpec spec, TimerInternals timerInternals) {\n+      this.namespace = namespace;\n+      this.timerId = timerId;\n+      this.spec = spec;\n+      this.timerInternals = timerInternals;\n+    }\n+\n+    @Override\n+    public void setForNowPlus(Duration durationFromNow) {\n+      timerInternals.setTimer(\n+          namespace, timerId, getCurrentTime().plus(durationFromNow), spec.getTimeDomain());\n+    }\n+\n+    @Override\n+    public void cancel() {\n+      timerInternals.deleteTimer(namespace, timerId);\n+    }\n+\n+    private Instant getCurrentTime() {\n+      switch(spec.getTimeDomain()) {\n+        case EVENT_TIME:\n+          return timerInternals.currentInputWatermarkTime();\n+        case PROCESSING_TIME:\n+          return timerInternals.currentProcessingTime();\n+        case SYNCHRONIZED_PROCESSING_TIME:\n+          return timerInternals.currentSynchronizedProcessingTime();\n+        default:\n+          throw new IllegalStateException(\n+              String.format(\"Timer created for unknown time domain %s\", spec.getTimeDomain()));\n+      }\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleDoFnRunner.java",
                "sha": "a7d82bf52ed87780b02e99cc4bd0a799da6ff674",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 0,
                "filename": "runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.beam.sdk.util.ExecutionContext.StepContext;\n import org.apache.beam.sdk.util.SideInputReader;\n import org.apache.beam.sdk.util.SystemDoFnInternal;\n+import org.apache.beam.sdk.util.TimeDomain;\n import org.apache.beam.sdk.util.TimerInternals;\n import org.apache.beam.sdk.util.UserCodeException;\n import org.apache.beam.sdk.util.WindowedValue;\n@@ -107,6 +108,13 @@ public void processElement(WindowedValue<InputT> elem) {\n     }\n   }\n \n+  @Override\n+  public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+      TimeDomain timeDomain) {\n+    throw new UnsupportedOperationException(\n+        String.format(\"Timers are not supported by %s\", OldDoFn.class.getSimpleName()));\n+  }\n+\n   private void invokeProcessElement(WindowedValue<InputT> elem) {\n     final OldDoFn<InputT, OutputT>.ProcessContext processContext = createProcessContext(elem);\n     // This can contain user code. Wrap it in case it throws an exception.",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/main/java/org/apache/beam/runners/core/SimpleOldDoFnRunner.java",
                "sha": "342a4a8694583c3bf7eb0a6d058c2bc966e577ad",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 0,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.beam.runners.core;\n \n+import static org.hamcrest.Matchers.contains;\n import static org.hamcrest.Matchers.containsInAnyOrder;\n import static org.hamcrest.Matchers.emptyIterable;\n import static org.hamcrest.Matchers.equalTo;\n@@ -37,7 +38,10 @@\n import org.apache.beam.sdk.transforms.windowing.Window;\n import org.apache.beam.sdk.util.IdentitySideInputWindowFn;\n import org.apache.beam.sdk.util.ReadyCheckingSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.TimerInternals.TimerData;\n import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.state.StateNamespaces;\n import org.apache.beam.sdk.values.PCollection;\n import org.apache.beam.sdk.values.PCollectionView;\n import org.hamcrest.Matchers;\n@@ -215,22 +219,59 @@ public void processElementNoSideInputs() {\n     assertThat(underlying.inputElems, containsInAnyOrder(multiWindow));\n   }\n \n+  /** Tests that a call to onTimer gets delegated. */\n+  @Test\n+  public void testOnTimerCalled() {\n+    PushbackSideInputDoFnRunner<Integer, Integer> runner =\n+        createRunner(ImmutableList.<PCollectionView<?>>of());\n+\n+    String timerId = \"fooTimer\";\n+    IntervalWindow window = new IntervalWindow(new Instant(4), new Instant(16));\n+    Instant timestamp = new Instant(72);\n+\n+    // Mocking is not easily compatible with annotation analysis, so we manually record\n+    // the method call.\n+    runner.onTimer(timerId, window, new Instant(timestamp), TimeDomain.EVENT_TIME);\n+\n+    assertThat(\n+        underlying.firedTimers,\n+        contains(\n+            TimerData.of(\n+                timerId,\n+                StateNamespaces.window(IntervalWindow.getCoder(), window),\n+                timestamp,\n+                TimeDomain.EVENT_TIME)));\n+  }\n+\n   private static class TestDoFnRunner<InputT, OutputT> implements DoFnRunner<InputT, OutputT> {\n     List<WindowedValue<InputT>> inputElems;\n+    List<TimerData> firedTimers;\n     private boolean started = false;\n     private boolean finished = false;\n \n     @Override\n     public void startBundle() {\n       started = true;\n       inputElems = new ArrayList<>();\n+      firedTimers = new ArrayList<>();\n     }\n \n     @Override\n     public void processElement(WindowedValue<InputT> elem) {\n       inputElems.add(elem);\n     }\n \n+    @Override\n+    public void onTimer(String timerId, BoundedWindow window, Instant timestamp,\n+        TimeDomain timeDomain) {\n+      firedTimers.add(\n+          TimerData.of(\n+              timerId,\n+              StateNamespaces.window(IntervalWindow.getCoder(), (IntervalWindow) window),\n+              timestamp,\n+              timeDomain));\n+    }\n+\n     @Override\n     public void finishBundle() {\n       finished = true;",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/test/java/org/apache/beam/runners/core/PushbackSideInputDoFnRunnerTest.java",
                "sha": "a1cdbf6dce04c998773000cd8a17cb35bcae82a4",
                "status": "modified"
            },
            {
                "additions": 301,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "changes": 301,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 0,
                "filename": "runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "patch": "@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.core;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertThat;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindows;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.BaseExecutionContext.StepContext;\n+import org.apache.beam.sdk.util.NullSideInputReader;\n+import org.apache.beam.sdk.util.TimeDomain;\n+import org.apache.beam.sdk.util.Timer;\n+import org.apache.beam.sdk.util.TimerInternals;\n+import org.apache.beam.sdk.util.TimerInternals.TimerData;\n+import org.apache.beam.sdk.util.TimerSpec;\n+import org.apache.beam.sdk.util.TimerSpecs;\n+import org.apache.beam.sdk.util.UserCodeException;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.util.WindowingStrategy;\n+import org.apache.beam.sdk.util.state.StateNamespaces;\n+import org.apache.beam.sdk.values.TupleTag;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.mockito.Mock;\n+import org.mockito.MockitoAnnotations;\n+\n+/** Tests for {@link SimpleDoFnRunner}. */\n+@RunWith(JUnit4.class)\n+public class SimpleDoFnRunnerTest {\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  @Mock StepContext mockStepContext;\n+\n+  @Mock TimerInternals mockTimerInternals;\n+\n+  @Before\n+  public void setup() {\n+    MockitoAnnotations.initMocks(this);\n+    when(mockStepContext.timerInternals()).thenReturn(mockTimerInternals);\n+  }\n+\n+  @Test\n+  public void testProcessElementExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.processElement(WindowedValue.valueInGlobalWindow(\"anyValue\"));\n+  }\n+\n+  @Test\n+  public void testOnTimerExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.onTimer(\n+        ThrowingDoFn.TIMER_ID,\n+        GlobalWindow.INSTANCE,\n+        new Instant(0),\n+        TimeDomain.EVENT_TIME);\n+  }\n+\n+  /**\n+   * Tests that a users call to set a timer gets properly dispatched to the timer internals. From\n+   * there on, it is the duty of the runner & step context to set it in whatever way is right for\n+   * that runner.\n+   */\n+  @Test\n+  public void testTimerSet() {\n+    WindowFn<?, ?> windowFn = new GlobalWindows();\n+    DoFnWithTimers<GlobalWindow> fn = new DoFnWithTimers(windowFn.windowCoder());\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    // Setting the timer needs the current time, as it is set relative\n+    Instant currentTime = new Instant(42);\n+    when(mockTimerInternals.currentInputWatermarkTime()).thenReturn(currentTime);\n+\n+    runner.processElement(WindowedValue.valueInGlobalWindow(\"anyValue\"));\n+\n+    verify(mockTimerInternals)\n+        .setTimer(\n+            StateNamespaces.window(new GlobalWindows().windowCoder(), GlobalWindow.INSTANCE),\n+            DoFnWithTimers.TIMER_ID,\n+            currentTime.plus(DoFnWithTimers.TIMER_OFFSET),\n+            TimeDomain.EVENT_TIME);\n+  }\n+\n+  @Test\n+  public void testStartBundleExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.startBundle();\n+  }\n+\n+  @Test\n+  public void testFinishBundleExceptionsWrappedAsUserCodeException() {\n+    ThrowingDoFn fn = new ThrowingDoFn();\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(new GlobalWindows()));\n+\n+    thrown.expect(UserCodeException.class);\n+    thrown.expectCause(is(fn.exceptionToThrow));\n+\n+    runner.finishBundle();\n+  }\n+\n+\n+  /**\n+   * Tests that {@link SimpleDoFnRunner#onTimer} properly dispatches to the underlying\n+   * {@link DoFn}.\n+   */\n+  @Test\n+  public void testOnTimerCalled() {\n+    WindowFn<?, GlobalWindow> windowFn = new GlobalWindows();\n+    DoFnWithTimers<GlobalWindow> fn = new DoFnWithTimers(windowFn.windowCoder());\n+    DoFnRunner<String, String> runner =\n+        new SimpleDoFnRunner<>(\n+            null,\n+            fn,\n+            NullSideInputReader.empty(),\n+            null,\n+            null,\n+            Collections.<TupleTag<?>>emptyList(),\n+            mockStepContext,\n+            null,\n+            WindowingStrategy.of(windowFn));\n+\n+    Instant currentTime = new Instant(42);\n+    Duration offset = Duration.millis(37);\n+\n+    // Mocking is not easily compatible with annotation analysis, so we manually record\n+    // the method call.\n+    runner.onTimer(\n+        DoFnWithTimers.TIMER_ID,\n+        GlobalWindow.INSTANCE,\n+        currentTime.plus(offset),\n+        TimeDomain.EVENT_TIME);\n+\n+    assertThat(\n+        fn.onTimerInvocations,\n+        contains(\n+            TimerData.of(\n+                DoFnWithTimers.TIMER_ID,\n+                StateNamespaces.window(windowFn.windowCoder(), GlobalWindow.INSTANCE),\n+                currentTime.plus(offset),\n+                TimeDomain.EVENT_TIME)));\n+  }\n+\n+  static class ThrowingDoFn extends DoFn<String, String> {\n+    final Exception exceptionToThrow = new UnsupportedOperationException(\"Expected exception\");\n+\n+    static final String TIMER_ID = \"throwingTimerId\";\n+\n+    @TimerId(TIMER_ID)\n+    private static final TimerSpec timer = TimerSpecs.timer(TimeDomain.EVENT_TIME);\n+\n+    @StartBundle\n+    public void startBundle(Context c) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+\n+    @FinishBundle\n+    public void finishBundle(Context c) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+\n+    @ProcessElement\n+    public void processElement(ProcessContext c) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+\n+    @OnTimer(TIMER_ID)\n+    public void onTimer(OnTimerContext context) throws Exception {\n+      throw exceptionToThrow;\n+    }\n+  }\n+\n+  private static class DoFnWithTimers<W extends BoundedWindow> extends DoFn<String, String> {\n+    static final String TIMER_ID = \"testTimerId\";\n+\n+    static final Duration TIMER_OFFSET = Duration.millis(100);\n+\n+    private final Coder<W> windowCoder;\n+\n+    // Mutable\n+    List<TimerData> onTimerInvocations;\n+\n+    DoFnWithTimers(Coder<W> windowCoder) {\n+      this.windowCoder = windowCoder;\n+      this.onTimerInvocations = new ArrayList<>();\n+    }\n+\n+    @TimerId(TIMER_ID)\n+    private static final TimerSpec timer = TimerSpecs.timer(TimeDomain.EVENT_TIME);\n+\n+    @ProcessElement\n+    public void process(ProcessContext context, @TimerId(TIMER_ID) Timer timer) {\n+      timer.setForNowPlus(TIMER_OFFSET);\n+    }\n+\n+    @OnTimer(TIMER_ID)\n+    public void onTimer(OnTimerContext context) {\n+      onTimerInvocations.add(\n+          TimerData.of(\n+              DoFnWithTimers.TIMER_ID,\n+              StateNamespaces.window(windowCoder, (W) context.window()),\n+              context.timestamp(),\n+              context.timeDomain()));\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/runners/core-java/src/test/java/org/apache/beam/runners/core/SimpleDoFnRunnerTest.java",
                "sha": "ec5d375117e907d7a3ad2b2a854fef515aa36a01",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/beam/blob/c28957d16fb0f63f82f578cf904df61bf7bb63e5/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/reflect/DoFnSignatures.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/reflect/DoFnSignatures.java?ref=c28957d16fb0f63f82f578cf904df61bf7bb63e5",
                "deletions": 0,
                "filename": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/reflect/DoFnSignatures.java",
                "patch": "@@ -1040,6 +1040,8 @@ private static String getStateId(List<Annotation> annotations) {\n       ErrorReporter errors, Class<?> fnClazz) {\n     Map<String, DoFnSignature.TimerDeclaration> declarations = new HashMap<>();\n     for (Field field : declaredFieldsWithAnnotation(DoFn.TimerId.class, fnClazz, DoFn.class)) {\n+      // TimerSpec fields may generally be private, but will be accessed via the signature\n+      field.setAccessible(true);\n       String id = field.getAnnotation(DoFn.TimerId.class).value();\n       validateTimerField(errors, declarations, id, field);\n       declarations.put(id, DoFnSignature.TimerDeclaration.create(id, field));\n@@ -1205,6 +1207,8 @@ private static void validateTimerField(\n     Map<String, DoFnSignature.StateDeclaration> declarations = new HashMap<>();\n \n     for (Field field : declaredFieldsWithAnnotation(DoFn.StateId.class, fnClazz, DoFn.class)) {\n+      // StateSpec fields may generally be private, but will be accessed via the signature\n+      field.setAccessible(true);\n       String id = field.getAnnotation(DoFn.StateId.class).value();\n \n       if (declarations.containsKey(id)) {",
                "raw_url": "https://github.com/apache/beam/raw/c28957d16fb0f63f82f578cf904df61bf7bb63e5/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/reflect/DoFnSignatures.java",
                "sha": "d72cea40a41669bcee02b658a4b104992e5cf3f1",
                "status": "modified"
            }
        ],
        "message": "This closes #1612: [BEAM-27] Support timer setting and receiving in SimpleDoFnRunner\n\n  Use empty SideInputReader, fixes NPE in SimpleDoFnRunnerTest\n  Test that SimpleDoFnRunner wraps exceptions in startBundle and finishBundle\n  Add timer support to DoFnRunner(s)\n  Make TimerSpec and StateSpec fields accessible",
        "parent": "https://github.com/apache/beam/commit/5255a33812758bbb9d081962675bd0180802c82b",
        "repo": "beam",
        "unit_tests": [
            "DoFnSignaturesTest.java"
        ]
    },
    "beam_ceb9a99": {
        "bug_id": "beam_ceb9a99",
        "commit": "https://github.com/apache/beam/commit/ceb9a994461a9f4525750e6aa7a6f141f1533225",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/beam/blob/ceb9a994461a9f4525750e6aa7a6f141f1533225/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubJsonClient.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubJsonClient.java?ref=ceb9a994461a9f4525750e6aa7a6f141f1533225",
                "deletions": 1,
                "filename": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubJsonClient.java",
                "patch": "@@ -169,7 +169,7 @@ public int publish(TopicPath topic, List<OutgoingMessage> outgoingMessages) thro\n       @Nullable Map<String, String> attributes = pubsubMessage.getAttributes();\n \n       // Payload.\n-      byte[] elementBytes = pubsubMessage.decodeData();\n+      byte[] elementBytes = pubsubMessage.getData() == null ? null : pubsubMessage.decodeData();\n       if (elementBytes == null) {\n         elementBytes = new byte[0];\n       }",
                "raw_url": "https://github.com/apache/beam/raw/ceb9a994461a9f4525750e6aa7a6f141f1533225/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub/PubsubJsonClient.java",
                "sha": "136b1d2af9d05c18de7dbc132ecf0e8885e62310",
                "status": "modified"
            }
        ],
        "message": "fix PubsubJsonClientTest NPE",
        "parent": "https://github.com/apache/beam/commit/174b21837b968a4fc20f01970d1727ae36532bf1",
        "repo": "beam",
        "unit_tests": [
            "PubsubJsonClientTest.java"
        ]
    }
}