{
    "flink_04aae30": {
        "bug_id": "flink_04aae30",
        "commit": "https://github.com/apache/flink/commit/04aae3061aa0184e3ad610cda454c661f872a45d",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/flink/blob/04aae3061aa0184e3ad610cda454c661f872a45d/flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java?ref=04aae3061aa0184e3ad610cda454c661f872a45d",
                "deletions": 0,
                "filename": "flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java",
                "patch": "@@ -150,6 +150,19 @@ public static void destroyHDFS() {\n \t\thdfsCluster.shutdown();\n \t}\n \n+\t@Test\n+\tpublic void testClosingWithoutInput() throws Exception {\n+\t\tfinal File outDir = tempFolder.newFolder();\n+\n+\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0, 100);\n+\t\ttestHarness.setup();\n+\t\ttestHarness.open();\n+\n+\t\t// verify that we can close without ever having an input. An earlier version of the code\n+\t\t// was throwing an NPE because we never initialized some internal state\n+\t\ttestHarness.close();\n+\t}\n+\n \t@Test\n \tpublic void testInactivityPeriodWithLateNotify() throws Exception {\n \t\tfinal File outDir = tempFolder.newFolder();",
                "raw_url": "https://github.com/apache/flink/raw/04aae3061aa0184e3ad610cda454c661f872a45d/flink-connectors/flink-connector-filesystem/src/test/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSinkTest.java",
                "sha": "090c54a7566604f67d4cbdab3ffc7f1e80a1f5c7",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6294] Add close without input test for BucketingSink\n\nAnd earlier version of the code was throwing an NPE if the sink was\nclosed without ever seeing any input.",
        "parent": "https://github.com/apache/flink/commit/9d3f127e4f0ddd15ae7af393e0c5b0d3dd81d812",
        "patched_files": [
            "BucketingSink.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "BucketingSinkTest.java"
        ]
    },
    "flink_0b86903": {
        "bug_id": "flink_0b86903",
        "commit": "https://github.com/apache/flink/commit/0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "patch": "@@ -153,7 +153,7 @@ public void registerTask(Task task) throws IOException {\n \t\t\t\tbufferPool = networkBufferPool.createBufferPool(partition.getNumberOfSubpartitions(), false);\n \t\t\t\tpartition.registerBufferPool(bufferPool);\n \n-\t\t\t\tpartitionManager.registerIntermediateResultPartition(partition);\n+\t\t\t\tpartitionManager.registerResultPartition(partition);\n \t\t\t}\n \t\t\tcatch (Throwable t) {\n \t\t\t\tif (bufferPool != null) {",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "sha": "e5dc8a2cc5819957e73606233a4e9badf24abf02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 4,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "patch": "@@ -18,12 +18,10 @@\n \n package org.apache.flink.runtime.io.network.netty;\n \n-import com.google.common.base.Optional;\n import io.netty.channel.ChannelHandlerContext;\n import io.netty.channel.SimpleChannelInboundHandler;\n import org.apache.flink.runtime.io.network.TaskEventDispatcher;\n import org.apache.flink.runtime.io.network.buffer.BufferPool;\n-import org.apache.flink.runtime.io.network.buffer.BufferProvider;\n import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionProvider;\n import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;\n@@ -90,10 +88,10 @@ protected void channelRead0(ChannelHandlerContext ctx, NettyMessage msg) throws\n \t\t\t\tLOG.debug(\"Read channel on {}: {}.\",ctx.channel().localAddress(), request);\n \n \t\t\t\tResultSubpartitionView queueIterator =\n-\t\t\t\t\t\tpartitionProvider.getSubpartition(\n+\t\t\t\t\t\tpartitionProvider.createSubpartitionView(\n \t\t\t\t\t\t\t\trequest.partitionId,\n \t\t\t\t\t\t\t\trequest.queueIndex,\n-\t\t\t\t\t\t\t\tOptional.<BufferProvider>of(bufferPool));\n+\t\t\t\t\t\t\t\tbufferPool);\n \n \t\t\t\tif (queueIterator != null) {\n \t\t\t\t\toutboundQueue.enqueue(queueIterator, request.receiverId);",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "sha": "6f4becd7d04b565089534f0ed053bbe69aefb961",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.runtime.io.network.api.EndOfPartitionEvent;\n import org.apache.flink.runtime.io.network.api.serialization.EventSerializer;\n import org.apache.flink.runtime.io.network.buffer.Buffer;\n@@ -168,7 +167,7 @@ public int releaseMemory() {\n \t}\n \n \t@Override\n-\tpublic PipelinedSubpartitionView getReadView(Optional<BufferProvider> bufferProvider) {\n+\tpublic PipelinedSubpartitionView createReadView(BufferProvider bufferProvider) {\n \t\tsynchronized (buffers) {\n \t\t\tif (readView != null) {\n \t\t\t\tthrow new IllegalStateException(\"Subpartition is being or already has been \" +",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java",
                "sha": "4c8174a10fb32ccd94a1ca2470d20ca55fdf54cc",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionView.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionView.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionView.java",
                "patch": "@@ -50,7 +50,6 @@ public Buffer getNextBuffer() {\n \t@Override\n \tpublic boolean registerListener(NotificationListener listener) {\n \t\treturn !isReleased.get() && parent.registerListener(listener);\n-\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionView.java",
                "sha": "1e44324dd3d54c03013dd2fee509b6807a76dc26",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 4,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.runtime.executiongraph.IntermediateResultPartition;\n import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n import org.apache.flink.runtime.io.disk.iomanager.IOManager.IOMode;\n@@ -169,7 +168,7 @@ public ResultPartition(\n \t * to the life-cycle of task registrations in the {@link TaskManager}.\n \t */\n \tpublic void registerBufferPool(BufferPool bufferPool) {\n-\t\tcheckArgument(bufferPool.getNumberOfRequiredMemorySegments() == getNumberOfSubpartitions(),\n+\t\tcheckArgument(bufferPool.getNumberOfRequiredMemorySegments() >= getNumberOfSubpartitions(),\n \t\t\t\t\"Bug in result partition setup logic: Buffer pool has not enough guaranteed buffers for this result partition.\");\n \n \t\tcheckState(this.bufferPool == null, \"Bug in result partition setup logic: Already registered buffer pool.\");\n@@ -302,13 +301,13 @@ public void release() {\n \t/**\n \t * Returns the requested subpartition.\n \t */\n-\tpublic ResultSubpartitionView getSubpartition(int index, Optional<BufferProvider> bufferProvider) throws IOException {\n+\tpublic ResultSubpartitionView createSubpartitionView(int index, BufferProvider bufferProvider) throws IOException {\n \t\tint refCnt = pendingReferences.get();\n \n \t\tcheckState(refCnt != -1, \"Partition released.\");\n \t\tcheckState(refCnt > 0, \"Partition not pinned.\");\n \n-\t\treturn subpartitions[index].getReadView(bufferProvider);\n+\t\treturn subpartitions[index].createReadView(bufferProvider);\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java",
                "sha": "92e27d3803bbd0fff4b652a8355d2e7a205e470d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 5,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition;\n \n-import com.google.common.base.Optional;\n import com.google.common.collect.HashBasedTable;\n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.Table;\n@@ -46,7 +45,7 @@\n \n \tprivate boolean isShutdown;\n \n-\tpublic void registerIntermediateResultPartition(ResultPartition partition) throws IOException {\n+\tpublic void registerResultPartition(ResultPartition partition) throws IOException {\n \t\tsynchronized (registeredPartitions) {\n \t\t\tcheckState(!isShutdown, \"Result partition manager already shut down.\");\n \n@@ -64,10 +63,10 @@ public void registerIntermediateResultPartition(ResultPartition partition) throw\n \t}\n \n \t@Override\n-\tpublic ResultSubpartitionView getSubpartition(\n+\tpublic ResultSubpartitionView createSubpartitionView(\n \t\t\tResultPartitionID partitionId,\n \t\t\tint subpartitionIndex,\n-\t\t\tOptional<BufferProvider> bufferProvider) throws IOException {\n+\t\t\tBufferProvider bufferProvider) throws IOException {\n \n \t\tsynchronized (registeredPartitions) {\n \t\t\tfinal ResultPartition partition = registeredPartitions.get(partitionId.getProducerId(),\n@@ -79,7 +78,7 @@ public ResultSubpartitionView getSubpartition(\n \n \t\t\tLOG.debug(\"Requested partition {}.\", partition);\n \n-\t\t\treturn partition.getSubpartition(subpartitionIndex, bufferProvider);\n+\t\t\treturn partition.createSubpartitionView(subpartitionIndex, bufferProvider);\n \t\t}\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java",
                "sha": "a666208b383a30a18f7eebfff6e1123556c280d3",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionProvider.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionProvider.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionProvider.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.runtime.io.network.buffer.BufferProvider;\n \n import java.io.IOException;\n@@ -28,6 +27,9 @@\n \t/**\n \t * Returns the requested intermediate result partition input view.\n \t */\n-\tResultSubpartitionView getSubpartition(ResultPartitionID partitionId, int index, Optional<BufferProvider> bufferProvider) throws IOException;\n+\tResultSubpartitionView createSubpartitionView(\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint index,\n+\t\t\tBufferProvider bufferProvider) throws IOException;\n \n }",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionProvider.java",
                "sha": "23dd1d3973884fcdde8e5b08692a36caf85db8f0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultSubpartition.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultSubpartition.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultSubpartition.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.runtime.io.network.buffer.Buffer;\n import org.apache.flink.runtime.io.network.buffer.BufferProvider;\n \n@@ -74,7 +73,7 @@ protected void onConsumedSubpartition() {\n \n \tabstract public void release() throws IOException;\n \n-\tabstract public ResultSubpartitionView getReadView(Optional<BufferProvider> bufferProvider) throws IOException;\n+\tabstract public ResultSubpartitionView createReadView(BufferProvider bufferProvider) throws IOException;\n \n \tabstract int releaseMemory() throws IOException;\n ",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultSubpartition.java",
                "sha": "e2ce16e00a3a99484bf1e75c609bbb96b398dd09",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartition.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartition.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 5,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartition.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.runtime.io.disk.iomanager.BufferFileWriter;\n import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n import org.apache.flink.runtime.io.disk.iomanager.IOManager.IOMode;\n@@ -169,7 +168,7 @@ public int releaseMemory() throws IOException {\n \t}\n \n \t@Override\n-\tpublic ResultSubpartitionView getReadView(Optional<BufferProvider> bufferProvider) throws IOException {\n+\tpublic ResultSubpartitionView createReadView(BufferProvider bufferProvider) throws IOException {\n \t\tsynchronized (buffers) {\n \t\t\tif (!isFinished) {\n \t\t\t\tthrow new IllegalStateException(\"Subpartition has not been finished yet, \" +\n@@ -190,22 +189,22 @@ public ResultSubpartitionView getReadView(Optional<BufferProvider> bufferProvide\n \t\t\t\tif (ioMode.isSynchronous()) {\n \t\t\t\t\treadView = new SpilledSubpartitionViewSyncIO(\n \t\t\t\t\t\t\tthis,\n-\t\t\t\t\t\t\tbufferProvider.get().getMemorySegmentSize(),\n+\t\t\t\t\t\t\tbufferProvider.getMemorySegmentSize(),\n \t\t\t\t\t\t\tspillWriter.getChannelID(),\n \t\t\t\t\t\t\t0);\n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\treadView = new SpilledSubpartitionViewAsyncIO(\n \t\t\t\t\t\t\tthis,\n-\t\t\t\t\t\t\tbufferProvider.get(),\n+\t\t\t\t\t\t\tbufferProvider,\n \t\t\t\t\t\t\tioManager,\n \t\t\t\t\t\t\tspillWriter.getChannelID(),\n \t\t\t\t\t\t\t0);\n \t\t\t\t}\n \t\t\t}\n \t\t\telse {\n \t\t\t\treadView = new SpillableSubpartitionView(\n-\t\t\t\t\t\tthis, bufferProvider.get(), buffers.size(), ioMode);\n+\t\t\t\t\t\tthis, bufferProvider, buffers.size(), ioMode);\n \t\t\t}\n \n \t\t\treturn readView;",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartition.java",
                "sha": "7ec24ac062dd1c08e0b1ac4f46b7287ed5b5ba8d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartitionView.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartitionView.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartitionView.java",
                "patch": "@@ -107,15 +107,15 @@ public Buffer getNextBuffer() throws IOException, InterruptedException {\n \t\t\t\t\tparent,\n \t\t\t\t\tbufferProvider.getMemorySegmentSize(),\n \t\t\t\t\tparent.spillWriter.getChannelID(),\n-\t\t\t\t\t0);\n+\t\t\t\t\tcurrentBytesRead);\n \t\t}\n \t\telse {\n \t\t\tspilledView = new SpilledSubpartitionViewAsyncIO(\n \t\t\t\t\tparent,\n \t\t\t\t\tbufferProvider,\n \t\t\t\t\tparent.ioManager,\n \t\t\t\t\tparent.spillWriter.getChannelID(),\n-\t\t\t\t\t0);\n+\t\t\t\t\tcurrentBytesRead);\n \t\t}\n \n \t\treturn spilledView.getNextBuffer();",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/SpillableSubpartitionView.java",
                "sha": "3d362d87d893d25cc8597d668310d2e40d0f8064",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/BufferOrEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/BufferOrEvent.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/BufferOrEvent.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.runtime.io.network.buffer.Buffer;\n \n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkNotNull;\n \n /**\n  * Either type for {@link Buffer} or {@link AbstractEvent} instances tagged with the channel index,\n@@ -36,14 +37,14 @@\n \tprivate int channelIndex;\n \n \tpublic BufferOrEvent(Buffer buffer, int channelIndex) {\n-\t\tthis.buffer = buffer;\n+\t\tthis.buffer = checkNotNull(buffer);\n \t\tthis.event = null;\n \t\tthis.channelIndex = channelIndex;\n \t}\n \n \tpublic BufferOrEvent(AbstractEvent event, int channelIndex) {\n \t\tthis.buffer = null;\n-\t\tthis.event = event;\n+\t\tthis.event = checkNotNull(event);\n \t\tthis.channelIndex = channelIndex;\n \t}\n \n@@ -71,4 +72,10 @@ public void setChannelIndex(int channelIndex) {\n \t\tcheckArgument(channelIndex >= 0);\n \t\tthis.channelIndex = channelIndex;\n \t}\n+\n+\t@Override\n+\tpublic String toString() {\n+\t\treturn String.format(\"BufferOrEvent [%s, channelIndex = %d]\",\n+\t\t\t\tisBuffer() ? buffer : event, channelIndex);\n+\t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/BufferOrEvent.java",
                "sha": "d2f3035f448528c0112a235e78ba5048d5f17bbe",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannel.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannel.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 20,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannel.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition.consumer;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.runtime.event.task.TaskEvent;\n import org.apache.flink.runtime.io.network.TaskEventDispatcher;\n import org.apache.flink.runtime.io.network.api.EndOfPartitionEvent;\n@@ -43,24 +42,27 @@\n \n \tprivate static final Logger LOG = LoggerFactory.getLogger(LocalInputChannel.class);\n \n+\t/** The local partition manager. */\n \tprivate final ResultPartitionManager partitionManager;\n \n+\t/** Task event dispatcher for backwards events. */\n \tprivate final TaskEventDispatcher taskEventDispatcher;\n \n-\tprivate ResultSubpartitionView queueIterator;\n+\t/** The consumed subpartition */\n+\tprivate volatile ResultSubpartitionView subpartitionView;\n \n \tprivate volatile boolean isReleased;\n \n \tprivate volatile Buffer lookAhead;\n \n \tLocalInputChannel(\n-\t\t\tSingleInputGate gate,\n+\t\t\tSingleInputGate inputGate,\n \t\t\tint channelIndex,\n \t\t\tResultPartitionID partitionId,\n \t\t\tResultPartitionManager partitionManager,\n \t\t\tTaskEventDispatcher taskEventDispatcher) {\n \n-\t\tsuper(gate, channelIndex, partitionId);\n+\t\tsuper(inputGate, channelIndex, partitionId);\n \n \t\tthis.partitionManager = checkNotNull(partitionManager);\n \t\tthis.taskEventDispatcher = checkNotNull(taskEventDispatcher);\n@@ -72,14 +74,15 @@\n \n \t@Override\n \tvoid requestSubpartition(int subpartitionIndex) throws IOException, InterruptedException {\n-\t\tif (queueIterator == null) {\n-\t\t\tLOG.debug(\"Requesting LOCAL queue {} of partition {}.\", subpartitionIndex, partitionId);\n+\t\tif (subpartitionView == null) {\n+\t\t\tLOG.debug(\"{}: Requesting LOCAL subpartition {} of partition {}.\",\n+\t\t\t\t\tthis, subpartitionIndex, partitionId);\n \n-\t\t\tqueueIterator = partitionManager\n-\t\t\t\t\t.getSubpartition(partitionId, subpartitionIndex, Optional.of(inputGate.getBufferProvider()));\n+\t\t\tsubpartitionView = partitionManager.createSubpartitionView(\n+\t\t\t\t\tpartitionId, subpartitionIndex, inputGate.getBufferProvider());\n \n-\t\t\tif (queueIterator == null) {\n-\t\t\t\tthrow new IOException(\"Error requesting sub partition.\");\n+\t\t\tif (subpartitionView == null) {\n+\t\t\t\tthrow new IOException(\"Error requesting subpartition.\");\n \t\t\t}\n \n \t\t\tgetNextLookAhead();\n@@ -88,11 +91,11 @@ void requestSubpartition(int subpartitionIndex) throws IOException, InterruptedE\n \n \t@Override\n \tBuffer getNextBuffer() throws IOException, InterruptedException {\n-\t\tcheckState(queueIterator != null, \"Queried for a buffer before requesting a queue.\");\n+\t\tcheckState(subpartitionView != null, \"Queried for a buffer before requesting the subpartition.\");\n \n \t\t// After subscribe notification\n \t\tif (lookAhead == null) {\n-\t\t\tlookAhead = queueIterator.getNextBuffer();\n+\t\t\tlookAhead = subpartitionView.getNextBuffer();\n \t\t}\n \n \t\tBuffer next = lookAhead;\n@@ -116,7 +119,7 @@ Buffer getNextBuffer() throws IOException, InterruptedException {\n \n \t@Override\n \tvoid sendTaskEvent(TaskEvent event) throws IOException {\n-\t\tcheckState(queueIterator != null, \"Tried to send task event to producer before requesting a queue.\");\n+\t\tcheckState(subpartitionView != null, \"Tried to send task event to producer before requesting the subpartition.\");\n \n \t\tif (!taskEventDispatcher.publish(partitionId, event)) {\n \t\t\tthrow new IOException(\"Error while publishing event \" + event + \" to producer. The producer could not be found.\");\n@@ -134,8 +137,8 @@ boolean isReleased() {\n \n \t@Override\n \tvoid notifySubpartitionConsumed() throws IOException {\n-\t\tif (queueIterator != null) {\n-\t\t\tqueueIterator.notifySubpartitionConsumed();\n+\t\tif (subpartitionView != null) {\n+\t\t\tsubpartitionView.notifySubpartitionConsumed();\n \t\t}\n \t}\n \n@@ -151,9 +154,9 @@ void releaseAllResources() throws IOException {\n \t\t\t\tlookAhead = null;\n \t\t\t}\n \n-\t\t\tif (queueIterator != null) {\n-\t\t\t\tqueueIterator.releaseAllResources();\n-\t\t\t\tqueueIterator = null;\n+\t\t\tif (subpartitionView != null) {\n+\t\t\t\tsubpartitionView.releaseAllResources();\n+\t\t\t\tsubpartitionView = null;\n \t\t\t}\n \n \t\t\tisReleased = true;\n@@ -186,15 +189,22 @@ public void onNotification() {\n \t// ------------------------------------------------------------------------\n \n \tprivate void getNextLookAhead() throws IOException, InterruptedException {\n+\n+\t\tfinal ResultSubpartitionView view = subpartitionView;\n+\n+\t\tif (view == null) {\n+\t\t\treturn;\n+\t\t}\n+\n \t\twhile (true) {\n-\t\t\tlookAhead = queueIterator.getNextBuffer();\n+\t\t\tlookAhead = view.getNextBuffer();\n \n \t\t\tif (lookAhead != null) {\n \t\t\t\tnotifyAvailableBuffer();\n \t\t\t\tbreak;\n \t\t\t}\n \n-\t\t\tif (queueIterator.registerListener(this) || queueIterator.isReleased()) {\n+\t\t\tif (view.registerListener(this) || view.isReleased()) {\n \t\t\t\treturn;\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannel.java",
                "sha": "7cb62f8cf11e5a80e35dab76766c558c66938f37",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionTest.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionTest.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 7,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionTest.java",
                "patch": "@@ -18,15 +18,14 @@\n \n package org.apache.flink.runtime.io.network.partition;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.core.memory.MemorySegment;\n import org.apache.flink.runtime.event.task.AbstractEvent;\n import org.apache.flink.runtime.io.network.buffer.Buffer;\n import org.apache.flink.runtime.io.network.buffer.BufferProvider;\n import org.apache.flink.runtime.io.network.partition.consumer.BufferOrEvent;\n-import org.apache.flink.runtime.io.network.util.TestPooledBufferProvider;\n import org.apache.flink.runtime.io.network.util.TestConsumerCallback;\n import org.apache.flink.runtime.io.network.util.TestNotificationListener;\n+import org.apache.flink.runtime.io.network.util.TestPooledBufferProvider;\n import org.apache.flink.runtime.io.network.util.TestProducerSource;\n import org.apache.flink.runtime.io.network.util.TestSubpartitionConsumer;\n import org.apache.flink.runtime.io.network.util.TestSubpartitionProducer;\n@@ -130,10 +129,10 @@ public void testIllegalReadViewRequest() throws Exception {\n \t\tfinal PipelinedSubpartition subpartition = createSubpartition();\n \n \t\t// Successful request\n-\t\tassertNotNull(subpartition.getReadView(Optional.<BufferProvider>absent()));\n+\t\tassertNotNull(subpartition.createReadView(null));\n \n \t\ttry {\n-\t\t\tsubpartition.getReadView(Optional.<BufferProvider>absent());\n+\t\t\tsubpartition.createReadView(null);\n \n \t\t\tfail(\"Did not throw expected exception after duplicate read view request.\");\n \t\t}\n@@ -147,7 +146,7 @@ public void testBasicPipelinedProduceConsumeLogic() throws Exception {\n \n \t\tTestNotificationListener listener = new TestNotificationListener();\n \n-\t\tResultSubpartitionView view = subpartition.getReadView(Optional.<BufferProvider>absent());\n+\t\tResultSubpartitionView view = subpartition.createReadView(null);\n \n \t\t// Empty => should return null\n \t\tassertNull(view.getNextBuffer());\n@@ -262,8 +261,7 @@ public void onEvent(AbstractEvent event) {\n \n \t\tfinal PipelinedSubpartition subpartition = createSubpartition();\n \n-\t\tfinal PipelinedSubpartitionView view = subpartition.getReadView(\n-\t\t\t\tOptional.<BufferProvider>absent());\n+\t\tfinal PipelinedSubpartitionView view = subpartition.createReadView(null);\n \n \t\tFuture<Boolean> producer = executorService.submit(\n \t\t\t\tnew TestSubpartitionProducer(subpartition, isSlowProducer, producerSource));",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionTest.java",
                "sha": "74549df3651668cb674afcfc29846ca5ec8ea871",
                "status": "modified"
            },
            {
                "additions": 283,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "changes": 283,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "patch": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.io.network.partition.consumer;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n+import org.apache.flink.runtime.io.network.TaskEventDispatcher;\n+import org.apache.flink.runtime.io.network.buffer.BufferPool;\n+import org.apache.flink.runtime.io.network.buffer.BufferProvider;\n+import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;\n+import org.apache.flink.runtime.io.network.partition.ResultPartition;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionConsumableNotifier;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionManager;\n+import org.apache.flink.runtime.io.network.partition.ResultPartitionType;\n+import org.apache.flink.runtime.io.network.util.TestBufferFactory;\n+import org.apache.flink.runtime.io.network.util.TestPartitionProducer;\n+import org.apache.flink.runtime.io.network.util.TestProducerSource;\n+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;\n+import org.apache.flink.runtime.jobgraph.JobID;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.flink.runtime.io.disk.iomanager.IOManager.IOMode.ASYNC;\n+import static org.mockito.Mockito.mock;\n+\n+public class LocalInputChannelTest {\n+\n+\t/**\n+\t * Tests the consumption of multiple subpartitions via local input channels.\n+\t *\n+\t * <p> Multiple producer tasks produce pipelined partitions, which are consumed by multiple\n+\t * tasks via local input channels.\n+\t */\n+\t@Test\n+\tpublic void testConcurrentConsumeMultiplePartitions() throws Exception {\n+\t\t// Config\n+\t\tfinal int parallelism = 32;\n+\t\tfinal int producerBufferPoolSize = parallelism + 1;\n+\t\tfinal int numberOfBuffersPerChannel = 1024;\n+\n+\t\tcheckArgument(parallelism >= 1);\n+\t\tcheckArgument(producerBufferPoolSize >= parallelism);\n+\t\tcheckArgument(numberOfBuffersPerChannel >= 1);\n+\n+\t\t// Setup\n+\t\t// One thread per produced partition and one per consumer\n+\t\tfinal ExecutorService executor = Executors.newFixedThreadPool(2 * parallelism);\n+\n+\t\tfinal NetworkBufferPool networkBuffers = new NetworkBufferPool(\n+\t\t\t\t(parallelism * producerBufferPoolSize) + (parallelism * parallelism),\n+\t\t\t\tTestBufferFactory.BUFFER_SIZE);\n+\n+\t\tfinal ResultPartitionConsumableNotifier partitionConsumableNotifier =\n+\t\t\t\tmock(ResultPartitionConsumableNotifier.class);\n+\n+\t\tfinal IOManager ioManager = mock(IOManager.class);\n+\n+\t\tfinal JobID jobId = new JobID();\n+\n+\t\tfinal ResultPartitionManager partitionManager = new ResultPartitionManager();\n+\n+\t\tfinal ResultPartitionID[] partitionIds = new ResultPartitionID[parallelism];\n+\t\tfinal TestPartitionProducer[] partitionProducers = new TestPartitionProducer[parallelism];\n+\n+\t\t// Create all partitions\n+\t\tfor (int i = 0; i < parallelism; i++) {\n+\t\t\tpartitionIds[i] = new ResultPartitionID();\n+\n+\t\t\tfinal ResultPartition partition = new ResultPartition(\n+\t\t\t\t\tjobId,\n+\t\t\t\t\tpartitionIds[i],\n+\t\t\t\t\tResultPartitionType.PIPELINED,\n+\t\t\t\t\tparallelism,\n+\t\t\t\t\tpartitionManager,\n+\t\t\t\t\tpartitionConsumableNotifier,\n+\t\t\t\t\tioManager,\n+\t\t\t\t\tASYNC);\n+\n+\t\t\t// Create a buffer pool for this partition\n+\t\t\tpartition.registerBufferPool(\n+\t\t\t\t\tnetworkBuffers.createBufferPool(producerBufferPoolSize, true));\n+\n+\t\t\t// Create the producer\n+\t\t\tpartitionProducers[i] = new TestPartitionProducer(\n+\t\t\t\t\tpartition,\n+\t\t\t\t\tfalse,\n+\t\t\t\t\tnew TestPartitionProducerBufferSource(\n+\t\t\t\t\t\t\tparallelism,\n+\t\t\t\t\t\t\tpartition.getBufferProvider(),\n+\t\t\t\t\t\t\tnumberOfBuffersPerChannel)\n+\t\t\t);\n+\n+\t\t\t// Register with the partition manager in order to allow the local input channels to\n+\t\t\t// request their respective partitions.\n+\t\t\tpartitionManager.registerResultPartition(partition);\n+\t\t}\n+\n+\t\t// Test\n+\t\ttry {\n+\t\t\t// Submit producer tasks\n+\t\t\tList<Future<?>> results = Lists.newArrayListWithCapacity(\n+\t\t\t\t\tparallelism + 1);\n+\n+\t\t\tfor (int i = 0; i < parallelism; i++) {\n+\t\t\t\tresults.add(executor.submit(partitionProducers[i]));\n+\t\t\t}\n+\n+\t\t\t// Submit consumer\n+\t\t\tfor (int i = 0; i < parallelism; i++) {\n+\t\t\t\tresults.add(executor.submit(\n+\t\t\t\t\t\tnew TestLocalInputChannelConsumer(\n+\t\t\t\t\t\t\t\ti,\n+\t\t\t\t\t\t\t\tparallelism,\n+\t\t\t\t\t\t\t\tnumberOfBuffersPerChannel,\n+\t\t\t\t\t\t\t\tnetworkBuffers.createBufferPool(parallelism, true),\n+\t\t\t\t\t\t\t\tpartitionManager,\n+\t\t\t\t\t\t\t\tnew TaskEventDispatcher(),\n+\t\t\t\t\t\t\t\tpartitionIds)));\n+\t\t\t}\n+\n+\t\t\t// Wait for all to finish\n+\t\t\tfor (Future<?> result : results) {\n+\t\t\t\tresult.get();\n+\t\t\t}\n+\t\t}\n+\t\tfinally {\n+\t\t\tnetworkBuffers.destroy();\n+\t\t\texecutor.shutdown();\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Returns the configured number of buffers for each channel in a random order.\n+\t */\n+\tprivate static class TestPartitionProducerBufferSource implements TestProducerSource {\n+\n+\t\tprivate final BufferProvider bufferProvider;\n+\n+\t\tprivate final List<Byte> channelIndexes;\n+\n+\t\tpublic TestPartitionProducerBufferSource(\n+\t\t\t\tint parallelism,\n+\t\t\t\tBufferProvider bufferProvider,\n+\t\t\t\tint numberOfBuffersToProduce) {\n+\n+\t\t\tthis.bufferProvider = bufferProvider;\n+\t\t\tthis.channelIndexes = Lists.newArrayListWithCapacity(\n+\t\t\t\t\tparallelism * numberOfBuffersToProduce);\n+\n+\t\t\t// Array of channel indexes to produce buffers for\n+\t\t\tfor (byte i = 0; i < parallelism; i++) {\n+\t\t\t\tfor (int j = 0; j < numberOfBuffersToProduce; j++) {\n+\t\t\t\t\tchannelIndexes.add(i);\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Random buffer to channel ordering\n+\t\t\tCollections.shuffle(channelIndexes);\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic BufferOrEvent getNextBufferOrEvent() throws Exception {\n+\t\t\tif (channelIndexes.size() > 0) {\n+\t\t\t\tfinal int channelIndex = channelIndexes.remove(0);\n+\n+\t\t\t\treturn new BufferOrEvent(bufferProvider.requestBufferBlocking(), channelIndex);\n+\t\t\t}\n+\n+\t\t\treturn null;\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Consumed the configured result partitions and verifies that each channel receives the\n+\t * expected number of buffers.\n+\t */\n+\tprivate static class TestLocalInputChannelConsumer implements Callable<Void> {\n+\n+\t\tprivate final SingleInputGate inputGate;\n+\n+\t\tprivate final int numberOfInputChannels;\n+\n+\t\tprivate final int numberOfExpectedBuffersPerChannel;\n+\n+\t\tpublic TestLocalInputChannelConsumer(\n+\t\t\t\tint subpartitionIndex,\n+\t\t\t\tint numberOfInputChannels,\n+\t\t\t\tint numberOfExpectedBuffersPerChannel,\n+\t\t\t\tBufferPool bufferPool,\n+\t\t\t\tResultPartitionManager partitionManager,\n+\t\t\t\tTaskEventDispatcher taskEventDispatcher,\n+\t\t\t\tResultPartitionID[] consumedPartitionIds) {\n+\n+\t\t\tcheckArgument(numberOfInputChannels >= 1);\n+\t\t\tcheckArgument(numberOfExpectedBuffersPerChannel >= 1);\n+\n+\t\t\tthis.inputGate = new SingleInputGate(\n+\t\t\t\t\tnew IntermediateDataSetID(), subpartitionIndex, numberOfInputChannels);\n+\n+\t\t\t// Set buffer pool\n+\t\t\tinputGate.setBufferPool(bufferPool);\n+\n+\t\t\t// Setup input channels\n+\t\t\tfor (int i = 0; i < numberOfInputChannels; i++) {\n+\t\t\t\tinputGate.setInputChannel(\n+\t\t\t\t\t\tnew IntermediateResultPartitionID(),\n+\t\t\t\t\t\tnew LocalInputChannel(\n+\t\t\t\t\t\t\t\tinputGate,\n+\t\t\t\t\t\t\t\ti,\n+\t\t\t\t\t\t\t\tconsumedPartitionIds[i],\n+\t\t\t\t\t\t\t\tpartitionManager,\n+\t\t\t\t\t\t\t\ttaskEventDispatcher));\n+\t\t\t}\n+\n+\t\t\tthis.numberOfInputChannels = numberOfInputChannels;\n+\t\t\tthis.numberOfExpectedBuffersPerChannel = numberOfExpectedBuffersPerChannel;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Void call() throws Exception {\n+\t\t\t// One counter per input channel. Expect the same number of buffers from each channel.\n+\t\t\tfinal int[] numberOfBuffersPerChannel = new int[numberOfInputChannels];\n+\n+\t\t\ttry {\n+\t\t\t\tBufferOrEvent boe;\n+\t\t\t\twhile ((boe = inputGate.getNextBufferOrEvent()) != null) {\n+\t\t\t\t\tif (boe.isBuffer()) {\n+\t\t\t\t\t\tboe.getBuffer().recycle();\n+\n+\t\t\t\t\t\t// Check that we don't receive too many buffers\n+\t\t\t\t\t\tif (++numberOfBuffersPerChannel[boe.getChannelIndex()]\n+\t\t\t\t\t\t\t\t> numberOfExpectedBuffersPerChannel) {\n+\n+\t\t\t\t\t\t\tthrow new IllegalStateException(\"Received more buffers than expected \" +\n+\t\t\t\t\t\t\t\t\t\"on channel \" + boe.getChannelIndex() + \".\");\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// Verify that we received the expected number of buffers on each channel\n+\t\t\t\tfor (int i = 0; i < numberOfBuffersPerChannel.length; i++) {\n+\t\t\t\t\tfinal int actualNumberOfReceivedBuffers = numberOfBuffersPerChannel[i];\n+\n+\t\t\t\t\tif (actualNumberOfReceivedBuffers != numberOfExpectedBuffersPerChannel) {\n+\t\t\t\t\t\tthrow new IllegalStateException(\"Received unexpected number of buffers \" +\n+\t\t\t\t\t\t\t\t\"on channel \" + i + \" (\" + actualNumberOfReceivedBuffers + \" instead \" +\n+\t\t\t\t\t\t\t\t\"of \" + numberOfExpectedBuffersPerChannel + \").\");\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tfinally {\n+\t\t\t\tinputGate.releaseAllResources();\n+\t\t\t}\n+\n+\t\t\treturn null;\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "sha": "cc90c44f679a557c995414973b667d61157076e1",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 5,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "patch": "@@ -18,7 +18,6 @@\n \n package org.apache.flink.runtime.io.network.partition.consumer;\n \n-import com.google.common.base.Optional;\n import org.apache.flink.core.memory.MemorySegment;\n import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;\n import org.apache.flink.runtime.deployment.ResultPartitionLocation;\n@@ -28,6 +27,7 @@\n import org.apache.flink.runtime.io.network.TaskEventDispatcher;\n import org.apache.flink.runtime.io.network.buffer.Buffer;\n import org.apache.flink.runtime.io.network.buffer.BufferPool;\n+import org.apache.flink.runtime.io.network.buffer.BufferProvider;\n import org.apache.flink.runtime.io.network.buffer.BufferRecycler;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionID;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionManager;\n@@ -90,7 +90,6 @@ public void testBasicGetNextLogic() throws Exception {\n \t}\n \n \t@Test\n-\t@SuppressWarnings(\"unchecked\")\n \tpublic void testBackwardsEventWithUninitializedChannel() throws Exception {\n \t\t// Setup environment\n \t\tfinal TaskEventDispatcher taskEventDispatcher = mock(TaskEventDispatcher.class);\n@@ -100,7 +99,7 @@ public void testBackwardsEventWithUninitializedChannel() throws Exception {\n \t\twhen(iterator.getNextBuffer()).thenReturn(new Buffer(new MemorySegment(new byte[1024]), mock(BufferRecycler.class)));\n \n \t\tfinal ResultPartitionManager partitionManager = mock(ResultPartitionManager.class);\n-\t\twhen(partitionManager.getSubpartition(any(ResultPartitionID.class), anyInt(), any(Optional.class))).thenReturn(iterator);\n+\t\twhen(partitionManager.createSubpartitionView(any(ResultPartitionID.class), anyInt(), any(BufferProvider.class))).thenReturn(iterator);\n \n \t\t// Setup reader with one local and one unknown input channel\n \t\tfinal IntermediateDataSetID resultId = new IntermediateDataSetID();\n@@ -129,7 +128,7 @@ public void testBackwardsEventWithUninitializedChannel() throws Exception {\n \t\tinputGate.requestPartitions();\n \n \t\t// Only the local channel can request\n-\t\tverify(partitionManager, times(1)).getSubpartition(any(ResultPartitionID.class), anyInt(), any(Optional.class));\n+\t\tverify(partitionManager, times(1)).createSubpartitionView(any(ResultPartitionID.class), anyInt(), any(BufferProvider.class));\n \n \t\t// Send event backwards and initialize unknown channel afterwards\n \t\tfinal TaskEvent event = new TestTaskEvent();\n@@ -141,7 +140,7 @@ public void testBackwardsEventWithUninitializedChannel() throws Exception {\n \t\t// After the update, the pending event should be send to local channel\n \t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(new ResultPartitionID(unknownPartitionId.getPartitionId(), unknownPartitionId.getProducerId()), ResultPartitionLocation.createLocal()));\n \n-\t\tverify(partitionManager, times(2)).getSubpartition(any(ResultPartitionID.class), anyInt(), any(Optional.class));\n+\t\tverify(partitionManager, times(2)).createSubpartitionView(any(ResultPartitionID.class), anyInt(), any(BufferProvider.class));\n \t\tverify(taskEventDispatcher, times(2)).publish(any(ResultPartitionID.class), any(TaskEvent.class));\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "sha": "66eeee05f2ac5baa1bbe1127fcaef15c72de61d3",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestBufferFactory.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestBufferFactory.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 8,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestBufferFactory.java",
                "patch": "@@ -30,11 +30,11 @@\n \n public class TestBufferFactory {\n \n-\tprivate static final int defaultSize = 32 * 1024;\n+\tpublic static final int BUFFER_SIZE = 32 * 1024;\n \n-\tprivate static final BufferRecycler discardingRecycler = new DiscardingRecycler();\n+\tprivate static final BufferRecycler RECYCLER = new DiscardingRecycler();\n \n-\tprivate static final Buffer mockBuffer = createBuffer();\n+\tprivate static final Buffer MOCK_BUFFER = createBuffer();\n \n \tprivate final int bufferSize;\n \n@@ -43,11 +43,11 @@\n \tprivate AtomicInteger numberOfCreatedBuffers = new AtomicInteger();\n \n \tpublic TestBufferFactory() {\n-\t\tthis(defaultSize, discardingRecycler);\n+\t\tthis(BUFFER_SIZE, RECYCLER);\n \t}\n \n \tpublic TestBufferFactory(int bufferSize) {\n-\t\tthis(bufferSize, discardingRecycler);\n+\t\tthis(bufferSize, RECYCLER);\n \t}\n \n \tpublic TestBufferFactory(int bufferSize, BufferRecycler bufferRecycler) {\n@@ -79,16 +79,16 @@ public int getBufferSize() {\n \t// ------------------------------------------------------------------------\n \n \tpublic static Buffer createBuffer() {\n-\t\treturn createBuffer(defaultSize);\n+\t\treturn createBuffer(BUFFER_SIZE);\n \t}\n \n \tpublic static Buffer createBuffer(int bufferSize) {\n \t\tcheckArgument(bufferSize > 0);\n \n-\t\treturn new Buffer(new MemorySegment(new byte[bufferSize]), discardingRecycler);\n+\t\treturn new Buffer(new MemorySegment(new byte[bufferSize]), RECYCLER);\n \t}\n \n \tpublic static Buffer getMockBuffer() {\n-\t\treturn mockBuffer;\n+\t\treturn MOCK_BUFFER;\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestBufferFactory.java",
                "sha": "cdba545977a31ae292190102fe79bdef49d4b853",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestPartitionProducer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestPartitionProducer.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestPartitionProducer.java",
                "patch": "@@ -68,6 +68,8 @@ public TestPartitionProducer(\n \t@Override\n \tpublic Boolean call() throws Exception {\n \n+\t\tboolean success = false;\n+\n \t\ttry {\n \t\t\tBufferOrEvent bufferOrEvent;\n \n@@ -98,10 +100,14 @@ else if (bufferOrEvent.isEvent()) {\n \n \t\t\tpartition.finish();\n \n+\t\t\tsuccess = true;\n+\n \t\t\treturn true;\n \t\t}\n \t\tfinally {\n-\t\t\tpartition.release();\n+\t\t\tif (!success) {\n+\t\t\t\tpartition.release();\n+\t\t\t}\n \t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestPartitionProducer.java",
                "sha": "d6dd9ee9be69dcab0121305516d4d5d1ee01d145",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestSubpartitionProducer.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestSubpartitionProducer.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestSubpartitionProducer.java",
                "patch": "@@ -68,6 +68,8 @@ public TestSubpartitionProducer(\n \t@Override\n \tpublic Boolean call() throws Exception {\n \n+\t\tboolean success = false;\n+\n \t\ttry {\n \t\t\tBufferOrEvent bufferOrEvent;\n \n@@ -96,10 +98,13 @@ else if (bufferOrEvent.isEvent()) {\n \n \t\t\tsubpartition.finish();\n \n+\t\t\tsuccess = true;\n \t\t\treturn true;\n \t\t}\n \t\tfinally {\n-\t\t\tsubpartition.release();\n+\t\t\tif (!success) {\n+\t\t\t\tsubpartition.release();\n+\t\t\t}\n \t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/util/TestSubpartitionProducer.java",
                "sha": "e5312ce3169048f620f983e7a8910c4b0b81977c",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/flink/blob/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleOrUpdateConsumersTest.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleOrUpdateConsumersTest.java?ref=0b86903dc88bae3c40b8b22a02ecb8faccd737a4",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleOrUpdateConsumersTest.java",
                "patch": "@@ -66,6 +66,23 @@ public static void tearDown() throws Exception {\n \t\tflink.stop();\n \t}\n \n+\t/**\n+\t * Tests notifications of multiple receivers when a task produces both a pipelined and blocking\n+\t * result.\n+\t *\n+\t * <pre>\n+\t *                             +----------+\n+\t *            +-- pipelined -> | Receiver |\n+\t * +--------+ |                +----------+\n+\t * | Sender |-|\n+\t * +--------+ |                +----------+\n+\t *            +-- blocking --> | Receiver |\n+\t *                             +----------+\n+\t * </pre>\n+\t *\n+\t * The pipelined receiver gets deployed after the first buffer is available and the blocking\n+\t * one after all subtasks are finished.\n+\t */\n \t@Test\n \tpublic void testMixedPipelinedAndBlockingResults() throws Exception {\n \t\tfinal AbstractJobVertex sender = new AbstractJobVertex(\"Sender\");\n@@ -92,13 +109,18 @@ public void testMixedPipelinedAndBlockingResults() throws Exception {\n \t\t\t\tDistributionPattern.ALL_TO_ALL,\n \t\t\t\tResultPartitionType.BLOCKING);\n \n-\t\tSlotSharingGroup slotSharingGroup = new SlotSharingGroup(sender.getID(), pipelinedReceiver.getID(), blockingReceiver.getID());\n+\t\tSlotSharingGroup slotSharingGroup = new SlotSharingGroup(\n+\t\t\t\tsender.getID(), pipelinedReceiver.getID(), blockingReceiver.getID());\n \n \t\tsender.setSlotSharingGroup(slotSharingGroup);\n \t\tpipelinedReceiver.setSlotSharingGroup(slotSharingGroup);\n \t\tblockingReceiver.setSlotSharingGroup(slotSharingGroup);\n \n-\t\tfinal JobGraph jobGraph = new JobGraph(\"\", sender, pipelinedReceiver, blockingReceiver);\n+\t\tfinal JobGraph jobGraph = new JobGraph(\n+\t\t\t\t\"Mixed pipelined and blocking result\",\n+\t\t\t\tsender,\n+\t\t\t\tpipelinedReceiver,\n+\t\t\t\tblockingReceiver);\n \n \t\tJobClient.submitJobAndWait(jobGraph, false, jobClient, TestingUtils.TESTING_DURATION());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/0b86903dc88bae3c40b8b22a02ecb8faccd737a4/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/ScheduleOrUpdateConsumersTest.java",
                "sha": "70bbc2562262c21568a43e48e2b19a39f7cde466",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/d72a3f7f99a7e242b191e993ced123d98a083d14/flink-tests/src/test/java/org/apache/flink/test/iterative/KMeansForTestITCase.java",
                "changes": 276,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests/src/test/java/org/apache/flink/test/iterative/KMeansForTestITCase.java?ref=d72a3f7f99a7e242b191e993ced123d98a083d14",
                "deletions": 276,
                "filename": "flink-tests/src/test/java/org/apache/flink/test/iterative/KMeansForTestITCase.java",
                "patch": "@@ -1,276 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.test.iterative;\n-\n-import org.apache.flink.api.common.functions.RichMapFunction;\n-import org.apache.flink.api.common.functions.RichReduceFunction;\n-import org.apache.flink.api.java.DataSet;\n-import org.apache.flink.api.java.ExecutionEnvironment;\n-import org.apache.flink.api.java.operators.IterativeDataSet;\n-import org.apache.flink.api.java.tuple.Tuple2;\n-import org.apache.flink.api.java.tuple.Tuple3;\n-import org.apache.flink.configuration.Configuration;\n-import org.apache.flink.test.localDistributed.PackagedProgramEndToEndITCase;\n-import org.apache.flink.test.testdata.KMeansData;\n-import org.apache.flink.test.util.JavaProgramTestBase;\n-import org.apache.flink.test.util.testjar.KMeansForTest;\n-\n-import java.io.Serializable;\n-import java.util.Collection;\n-\n-/**\n- * This K-Means is a copy of {@link KMeansForTest} from the {@link PackagedProgramEndToEndITCase},\n- * which detected a problem with the wiring of blocking intermediate results reproducibly with\n- * multiple runs, whereas other tests didn't.\n- *\n- * <p> The code is copied here, because the packaged program test removes the classes from the\n- * classpath.\n- *\n- * <p> It's safe to remove this test in the future.\n- */\n-public class KMeansForTestITCase extends JavaProgramTestBase {\n-\n-\tprotected String dataPath;\n-\tprotected String clusterPath;\n-\tprotected String resultPath;\n-\n-\tpublic KMeansForTestITCase(){\n-\t\tsetNumTaskManagers(2);\n-\t\tsetTaskManagerNumSlots(2);\n-\t\tsetNumberOfTestRepetitions(10);\n-\t}\n-\n-\t@Override\n-\tprotected void preSubmit() throws Exception {\n-\t\tdataPath = createTempFile(\"datapoints.txt\", KMeansData.DATAPOINTS);\n-\t\tclusterPath = createTempFile(\"initial_centers.txt\", KMeansData.INITIAL_CENTERS);\n-\t\tresultPath = getTempDirPath(\"result\");\n-\t}\n-\n-\t@Override\n-\tprotected void testProgram() throws Exception {\n-\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n-\n-\t\tint numIterations = 20;\n-\n-\t\t// get input data\n-\t\tDataSet<Point> points = env.readCsvFile(dataPath)\n-\t\t\t\t.fieldDelimiter(\"|\")\n-\t\t\t\t.includeFields(true, true)\n-\t\t\t\t.types(Double.class, Double.class)\n-\t\t\t\t.map(new TuplePointConverter());\n-\n-\t\tDataSet<Centroid> centroids = env.readCsvFile(clusterPath)\n-\t\t\t\t.fieldDelimiter(\"|\")\n-\t\t\t\t.includeFields(true, true, true)\n-\t\t\t\t.types(Integer.class, Double.class, Double.class)\n-\t\t\t\t.map(new TupleCentroidConverter());\n-\n-\t\t// set number of bulk iterations for KMeans algorithm\n-\t\tIterativeDataSet<Centroid> loop = centroids.iterate(numIterations);\n-\n-\t\tDataSet<Centroid> newCentroids = points\n-\t\t\t\t// compute closest centroid for each point\n-\t\t\t\t.map(new SelectNearestCenter()).withBroadcastSet(loop, \"centroids\")\n-\t\t\t\t\t\t// count and sum point coordinates for each centroid\n-\t\t\t\t.map(new CountAppender())\n-\t\t\t\t\t\t// !test if key expressions are working!\n-\t\t\t\t.groupBy(\"field0\").reduce(new CentroidAccumulator())\n-\t\t\t\t\t\t// compute new centroids from point counts and coordinate sums\n-\t\t\t\t.map(new CentroidAverager());\n-\n-\t\t// feed new centroids back into next iteration\n-\t\tDataSet<Centroid> finalCentroids = loop.closeWith(newCentroids);\n-\n-\t\tDataSet<Tuple2<Integer, Point>> clusteredPoints = points\n-\t\t\t\t// assign points to final clusters\n-\t\t\t\t.map(new SelectNearestCenter()).withBroadcastSet(finalCentroids, \"centroids\");\n-\n-\t\t// emit result\n-\t\tclusteredPoints.writeAsCsv(resultPath, \"\\n\", \" \");\n-\n-\t\tenv.execute(\"KMeansForTest\");\n-\t}\n-\n-\t// *************************************************************************\n-\t//     DATA TYPES\n-\t// *************************************************************************\n-\n-\t/**\n-\t * A simple two-dimensional point.\n-\t */\n-\tpublic static class Point implements Serializable {\n-\n-\t\tpublic double x, y;\n-\n-\t\tpublic Point() {}\n-\n-\t\tpublic Point(double x, double y) {\n-\t\t\tthis.x = x;\n-\t\t\tthis.y = y;\n-\t\t}\n-\n-\t\tpublic Point add(Point other) {\n-\t\t\tx += other.x;\n-\t\t\ty += other.y;\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic Point div(long val) {\n-\t\t\tx /= val;\n-\t\t\ty /= val;\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tpublic double euclideanDistance(Point other) {\n-\t\t\treturn Math.sqrt((x-other.x)*(x-other.x) + (y-other.y)*(y-other.y));\n-\t\t}\n-\n-\t\tpublic void clear() {\n-\t\t\tx = y = 0.0;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic String toString() {\n-\t\t\treturn x + \" \" + y;\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * A simple two-dimensional centroid, basically a point with an ID.\n-\t */\n-\tpublic static class Centroid extends Point {\n-\n-\t\tpublic int id;\n-\n-\t\tpublic Centroid() {}\n-\n-\t\tpublic Centroid(int id, double x, double y) {\n-\t\t\tsuper(x,y);\n-\t\t\tthis.id = id;\n-\t\t}\n-\n-\t\tpublic Centroid(int id, Point p) {\n-\t\t\tsuper(p.x, p.y);\n-\t\t\tthis.id = id;\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic String toString() {\n-\t\t\treturn id + \" \" + super.toString();\n-\t\t}\n-\t}\n-\n-\t// *************************************************************************\n-\t//     USER FUNCTIONS\n-\t// *************************************************************************\n-\n-\t/** Converts a Tuple2<Double,Double> into a Point. */\n-\tpublic static final class TuplePointConverter extends RichMapFunction<Tuple2<Double, Double>, Point> {\n-\n-\t\t@Override\n-\t\tpublic Point map(Tuple2<Double, Double> t) throws Exception {\n-\t\t\treturn new Point(t.f0, t.f1);\n-\t\t}\n-\t}\n-\n-\t/** Converts a Tuple3<Integer, Double,Double> into a Centroid. */\n-\tpublic static final class TupleCentroidConverter extends RichMapFunction<Tuple3<Integer, Double, Double>, Centroid> {\n-\n-\t\t@Override\n-\t\tpublic Centroid map(Tuple3<Integer, Double, Double> t) throws Exception {\n-\t\t\treturn new Centroid(t.f0, t.f1, t.f2);\n-\t\t}\n-\t}\n-\n-\t/** Determines the closest cluster center for a data point. */\n-\tpublic static final class SelectNearestCenter extends RichMapFunction<Point, Tuple2<Integer, Point>> {\n-\t\tprivate Collection<Centroid> centroids;\n-\n-\t\t/** Reads the centroid values from a broadcast variable into a collection. */\n-\t\t@Override\n-\t\tpublic void open(Configuration parameters) throws Exception {\n-\t\t\tthis.centroids = getRuntimeContext().getBroadcastVariable(\"centroids\");\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Tuple2<Integer, Point> map(Point p) throws Exception {\n-\n-\t\t\tdouble minDistance = Double.MAX_VALUE;\n-\t\t\tint closestCentroidId = -1;\n-\n-\t\t\t// check all cluster centers\n-\t\t\tfor (Centroid centroid : centroids) {\n-\t\t\t\t// compute distance\n-\t\t\t\tdouble distance = p.euclideanDistance(centroid);\n-\n-\t\t\t\t// update nearest cluster if necessary\n-\t\t\t\tif (distance < minDistance) {\n-\t\t\t\t\tminDistance = distance;\n-\t\t\t\t\tclosestCentroidId = centroid.id;\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\t// emit a new record with the center id and the data point.\n-\t\t\treturn new Tuple2<Integer, Point>(closestCentroidId, p);\n-\t\t}\n-\t}\n-\n-\t// Use this so that we can check whether POJOs and the POJO comparator also work\n-\tpublic static final class DummyTuple3IntPointLong {\n-\t\tpublic Integer field0;\n-\t\tpublic Point field1;\n-\t\tpublic Long field2;\n-\n-\t\tpublic DummyTuple3IntPointLong() {}\n-\n-\t\tDummyTuple3IntPointLong(Integer f0, Point f1, Long f2) {\n-\t\t\tthis.field0 = f0;\n-\t\t\tthis.field1 = f1;\n-\t\t\tthis.field2 = f2;\n-\t\t}\n-\t}\n-\n-\t/** Appends a count variable to the tuple. */\n-\tpublic static final class CountAppender extends RichMapFunction<Tuple2<Integer, Point>, DummyTuple3IntPointLong> {\n-\n-\t\t@Override\n-\t\tpublic DummyTuple3IntPointLong map(Tuple2<Integer, Point> t) {\n-\t\t\treturn new DummyTuple3IntPointLong(t.f0, t.f1, 1L);\n-\t\t}\n-\t}\n-\n-\t/** Sums and counts point coordinates. */\n-\tpublic static final class CentroidAccumulator extends RichReduceFunction<DummyTuple3IntPointLong> {\n-\n-\t\t@Override\n-\t\tpublic DummyTuple3IntPointLong reduce(DummyTuple3IntPointLong val1, DummyTuple3IntPointLong val2) {\n-\t\t\treturn new DummyTuple3IntPointLong(val1.field0, val1.field1.add(val2.field1), val1.field2 + val2.field2);\n-\t\t}\n-\t}\n-\n-\t/** Computes new centroid from coordinate sum and count of points. */\n-\tpublic static final class CentroidAverager extends RichMapFunction<DummyTuple3IntPointLong, Centroid> {\n-\n-\t\t@Override\n-\t\tpublic Centroid map(DummyTuple3IntPointLong value) {\n-\t\t\treturn new Centroid(value.field0, value.field1.div(value.field2));\n-\t\t}\n-\t}\n-}",
                "raw_url": "https://github.com/apache/flink/raw/d72a3f7f99a7e242b191e993ced123d98a083d14/flink-tests/src/test/java/org/apache/flink/test/iterative/KMeansForTestITCase.java",
                "sha": "732bd063bc4420f866a6020d6472d014d77fbc80",
                "status": "removed"
            }
        ],
        "message": "[FLINK-1755] Fix possible NullPointerException in LocalInputChannel\n\nThis commit squashes:\n\n - [runtime] Rename create read view methods to be consistent\n - [runtime] Fix spillable subpartition view read offset after spilling\n - [tests] Add comment to ScheduleOrUpdateConsumersTest",
        "parent": "https://github.com/apache/flink/commit/d72a3f7f99a7e242b191e993ced123d98a083d14",
        "patched_files": [
            "KMeansForTestITCase.java",
            "SpillableSubpartitionView.java",
            "PipelinedSubpartitionView.java",
            "BufferOrEvent.java",
            "ResultPartitionProvider.java",
            "PartitionRequestServerHandler.java",
            "ResultSubpartition.java",
            "SingleInputGate.java",
            "ResultPartition.java",
            "PipelinedSubpartition.java",
            "ResultPartitionManager.java",
            "NetworkEnvironment.java",
            "SpillableSubpartition.java",
            "LocalInputChannel.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "SingleInputGateTest.java",
            "TestBufferFactory.java",
            "TestPartitionProducer.java",
            "ScheduleOrUpdateConsumersTest.java",
            "PartitionRequestServerHandlerTest.java",
            "TestSubpartitionProducer.java",
            "ResultPartitionTest.java",
            "PipelinedSubpartitionTest.java",
            "LocalInputChannelTest.java",
            "ResultPartitionManagerTest.java"
        ]
    },
    "flink_101552b": {
        "bug_id": "flink_101552b",
        "commit": "https://github.com/apache/flink/commit/101552bf503cf0ca59493397ec4cd01bcc4c45a7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/101552bf503cf0ca59493397ec4cd01bcc4c45a7/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java?ref=101552bf503cf0ca59493397ec4cd01bcc4c45a7",
                "deletions": 1,
                "filename": "flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java",
                "patch": "@@ -123,7 +123,7 @@ public PojoSerializer(\n \t\t\tthis.fields[i].setAccessible(true);\n \t\t}\n \n-\t\tcl = Thread.currentThread().getContextClassLoader();\n+\t\tthis.cl = Thread.currentThread().getContextClassLoader();\n \n \t\t// We only want those classes that are not our own class and are actually sub-classes.\n \t\tLinkedHashSet<Class<?>> registeredSubclasses =\n@@ -156,6 +156,7 @@ public PojoSerializer(\n \t\tthis.registeredSerializers = checkNotNull(registeredSerializers);\n \t\tthis.subclassSerializerCache = checkNotNull(subclassSerializerCache);\n \t\tthis.executionConfig = checkNotNull(executionConfig);\n+\t\tthis.cl = Thread.currentThread().getContextClassLoader();\n \t}\n \t\n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/101552bf503cf0ca59493397ec4cd01bcc4c45a7/flink-core/src/main/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializer.java",
                "sha": "5c43d1e172eba312bfc6efdb385bc1add3d852fa",
                "status": "modified"
            }
        ],
        "message": "[FLINK-13159] Fix the NPE when PojoSerializer restored",
        "parent": "https://github.com/apache/flink/commit/886419f12f60df803c9d757e381f201920a8061a",
        "patched_files": [
            "PojoSerializer.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "PojoSerializerTest.java"
        ]
    },
    "flink_13150a4": {
        "bug_id": "flink_13150a4",
        "commit": "https://github.com/apache/flink/commit/13150a4ba26127b9ee2035fd3509b57bc3f7aa61",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java?ref=13150a4ba26127b9ee2035fd3509b57bc3f7aa61",
                "deletions": 1,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
                "patch": "@@ -91,7 +91,9 @@ protected void run() throws Exception {\n \n \t@Override\n \tprotected void cleanup() throws Exception {\n-\t\tinputProcessor.cleanup();\n+\t\tif (inputProcessor != null) {\n+\t\t\tinputProcessor.cleanup();\n+\t\t}\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
                "sha": "233e9f10db0c809213cafdedac435b7c84af65ef",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4631] Prevent NPE in TwoInputStreamTask\n\nCheck that the input processor has been created before cleaning it up.",
        "parent": "https://github.com/apache/flink/commit/4410c04a68c7b247bb3d7113e5f40f2a9c2165af",
        "patched_files": [
            "TwoInputStreamTask.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "TwoInputStreamTaskTest.java"
        ]
    },
    "flink_191b9df": {
        "bug_id": "flink_191b9df",
        "commit": "https://github.com/apache/flink/commit/191b9dff2f3faf281a77e211c6ef47243d6a9e8d",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java?ref=191b9dff2f3faf281a77e211c6ef47243d6a9e8d",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java",
                "patch": "@@ -100,13 +100,15 @@ protected SubtaskExecutionAttemptAccumulatorsInfo handleRequest(\n \n \t\t\t\tfor (int x = 0; x < subtask.getCurrentExecutionAttempt().getAttemptNumber(); x++) {\n \t\t\t\t\tAccessExecution attempt = subtask.getPriorExecutionAttempt(x);\n-\t\t\t\t\tResponseBody json = createAccumulatorInfo(attempt);\n-\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n-\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n-\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n-\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n-\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n-\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\tif (attempt != null){\n+\t\t\t\t\t\tResponseBody json = createAccumulatorInfo(attempt);\n+\t\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n+\t\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n+\t\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n+\t\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n+\t\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n+\t\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptAccumulatorsHandler.java",
                "sha": "97da25a1230e65b32d6d207cbd94f2f347160b53",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java?ref=191b9dff2f3faf281a77e211c6ef47243d6a9e8d",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java",
                "patch": "@@ -114,13 +114,15 @@ protected SubtaskExecutionAttemptDetailsInfo handleRequest(\n \n \t\t\t\tfor (int x = 0; x < subtask.getCurrentExecutionAttempt().getAttemptNumber(); x++) {\n \t\t\t\t\tAccessExecution attempt = subtask.getPriorExecutionAttempt(x);\n-\t\t\t\t\tResponseBody json = createDetailsInfo(attempt, graph.getJobID(), task.getJobVertexId(), null);\n-\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n-\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n-\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n-\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n-\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n-\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\tif (attempt != null) {\n+\t\t\t\t\t\tResponseBody json = createDetailsInfo(attempt, graph.getJobID(), task.getJobVertexId(), null);\n+\t\t\t\t\t\tString path = getMessageHeaders().getTargetRestEndpointURL()\n+\t\t\t\t\t\t\t.replace(':' + JobIDPathParameter.KEY, graph.getJobID().toString())\n+\t\t\t\t\t\t\t.replace(':' + JobVertexIdPathParameter.KEY, task.getJobVertexId().toString())\n+\t\t\t\t\t\t\t.replace(':' + SubtaskIndexPathParameter.KEY, String.valueOf(subtask.getParallelSubtaskIndex()))\n+\t\t\t\t\t\t\t.replace(':' + SubtaskAttemptPathParameter.KEY, String.valueOf(attempt.getAttemptNumber()));\n+\t\t\t\t\t\tarchive.add(new ArchivedJson(path, json));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/191b9dff2f3faf281a77e211c6ef47243d6a9e8d/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/job/SubtaskExecutionAttemptDetailsHandler.java",
                "sha": "75fd100062e22e175fc4a748aa4271f044991737",
                "status": "modified"
            }
        ],
        "message": "[FLINK-12247][rest] Fix NPE when writing the archive json file to FileSystem\n\nThis closes #8250.",
        "parent": "https://github.com/apache/flink/commit/8a174833bee081f4f4a24caa5ddc5fe45996de13",
        "patched_files": [
            "SubtaskExecutionAttemptDetailsHandler.java",
            "SubtaskExecutionAttemptAccumulatorsHandler.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "SubtaskExecutionAttemptDetailsHandlerTest.java",
            "SubtaskExecutionAttemptAccumulatorsHandlerTest.java"
        ]
    },
    "flink_233ae24": {
        "bug_id": "flink_233ae24",
        "commit": "https://github.com/apache/flink/commit/233ae24cf5e255e0e1a66446aea04f051f0c09de",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/233ae24cf5e255e0e1a66446aea04f051f0c09de/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/execution/Environment.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/execution/Environment.java?ref=233ae24cf5e255e0e1a66446aea04f051f0c09de",
                "deletions": 2,
                "filename": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/execution/Environment.java",
                "patch": "@@ -472,9 +472,9 @@ public Thread getExecutingThread() {\n \n \t\t\tif (this.executingThread == null) {\n \t\t\t\tif (this.taskName == null) {\n-\t\t\t\t\tthis.executingThread = new Thread(this, this.taskName);\n-\t\t\t\t} else {\n \t\t\t\t\tthis.executingThread = new Thread(this);\n+\t\t\t\t} else {\n+\t\t\t\t\tthis.executingThread = new Thread(this, this.taskName);\n \t\t\t\t}\n \t\t\t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/233ae24cf5e255e0e1a66446aea04f051f0c09de/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/execution/Environment.java",
                "sha": "7bd58654b65cae8d85df18f7af7bebc31427f6ae",
                "status": "modified"
            }
        ],
        "message": "Fixed possible NullPointerException in Environment",
        "parent": "https://github.com/apache/flink/commit/011fec333805daa3100603f3639ac0af64bc9443",
        "patched_files": [
            "Environment.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "TestEnvironment.java",
            "EnvironmentTest.java"
        ]
    },
    "flink_26bac51": {
        "bug_id": "flink_26bac51",
        "commit": "https://github.com/apache/flink/commit/26bac51cae1d298078902a02e196fffc16ea5704",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/26bac51cae1d298078902a02e196fffc16ea5704/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java?ref=26bac51cae1d298078902a02e196fffc16ea5704",
                "deletions": 1,
                "filename": "flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java",
                "patch": "@@ -373,7 +373,10 @@ private GetRecordsResult getRecords(String shardItr, int maxNumberOfRecords) thr\n \t\t\t\tgetRecordsResult = kinesis.getRecords(shardItr, maxNumberOfRecords);\n \n \t\t\t\t// Update millis behind latest so it gets reported by the millisBehindLatest gauge\n-\t\t\t\tshardMetricsReporter.setMillisBehindLatest(getRecordsResult.getMillisBehindLatest());\n+\t\t\t\tLong millisBehindLatest = getRecordsResult.getMillisBehindLatest();\n+\t\t\t\tif (millisBehindLatest != null) {\n+\t\t\t\t\tshardMetricsReporter.setMillisBehindLatest(millisBehindLatest);\n+\t\t\t\t}\n \t\t\t} catch (ExpiredIteratorException eiEx) {\n \t\t\t\tLOG.warn(\"Encountered an unexpected expired iterator {} for shard {};\" +\n \t\t\t\t\t\" refreshing the iterator ...\", shardItr, subscribedShard);",
                "raw_url": "https://github.com/apache/flink/raw/26bac51cae1d298078902a02e196fffc16ea5704/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/ShardConsumer.java",
                "sha": "36a4e92c179135fcb9f459fcff746be4069c184a",
                "status": "modified"
            }
        ],
        "message": "[FLINK-10358] fix NPE when running flink-kinesis connector against dynamodb streams\n\nThis closes #6708.",
        "parent": "https://github.com/apache/flink/commit/e58cc14db007123c6325c7e51291650da69a4ca2",
        "patched_files": [
            "ShardConsumer.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "ShardConsumerTest.java"
        ]
    },
    "flink_26bc3c8": {
        "bug_id": "flink_26bc3c8",
        "commit": "https://github.com/apache/flink/commit/26bc3c8c65c757285c58b2cfcb0ba81111395ea4",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/26bc3c8c65c757285c58b2cfcb0ba81111395ea4/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java?ref=26bc3c8c65c757285c58b2cfcb0ba81111395ea4",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java",
                "patch": "@@ -109,15 +109,20 @@ public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJso\n \t\t\tByteArrayOutputStream output = new ByteArrayOutputStream()) {\n \t\t\tIOUtils.copyBytes(input, output);\n \n-\t\t\tJsonNode archive = mapper.readTree(output.toByteArray());\n+\t\t\ttry {\n+\t\t\t\tJsonNode archive = mapper.readTree(output.toByteArray());\n \n-\t\t\tCollection<ArchivedJson> archives = new ArrayList<>();\n-\t\t\tfor (JsonNode archivePart : archive.get(ARCHIVE)) {\n-\t\t\t\tString path = archivePart.get(PATH).asText();\n-\t\t\t\tString json = archivePart.get(JSON).asText();\n-\t\t\t\tarchives.add(new ArchivedJson(path, json));\n+\t\t\t\tCollection<ArchivedJson> archives = new ArrayList<>();\n+\t\t\t\tfor (JsonNode archivePart : archive.get(ARCHIVE)) {\n+\t\t\t\t\tString path = archivePart.get(PATH).asText();\n+\t\t\t\t\tString json = archivePart.get(JSON).asText();\n+\t\t\t\t\tarchives.add(new ArchivedJson(path, json));\n+\t\t\t\t}\n+\t\t\t\treturn archives;\n+\t\t\t} catch (NullPointerException npe) {\n+\t\t\t\t// occurs if the archive is empty or any of the expected fields are not present\n+\t\t\t\tthrow new IOException(\"Job archive (\" + file.getPath() + \") did not conform to expected format.\");\n \t\t\t}\n-\t\t\treturn archives;\n \t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/26bc3c8c65c757285c58b2cfcb0ba81111395ea4/flink-runtime/src/main/java/org/apache/flink/runtime/history/FsJobArchivist.java",
                "sha": "ab1e34d74078a89fda138bc8d4fcc9c656e58700",
                "status": "modified"
            }
        ],
        "message": "[FLINK-14337][hs] Prevent NPE on corrupt archives",
        "parent": "https://github.com/apache/flink/commit/f22f1eba8f7695857a2015ed178365191849dac4",
        "patched_files": [
            "FsJobArchivist.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "FsJobArchivistTest.java"
        ]
    },
    "flink_29e849b": {
        "bug_id": "flink_29e849b",
        "commit": "https://github.com/apache/flink/commit/29e849b1bf9180a5aa5f2d500efb283a39839caa",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/29e849b1bf9180a5aa5f2d500efb283a39839caa/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala?ref=29e849b1bf9180a5aa5f2d500efb283a39839caa",
                "deletions": 3,
                "filename": "flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala",
                "patch": "@@ -1211,15 +1211,15 @@ abstract class CodeGenerator(\n     }\n \n     val wrappedCode = if (nullCheck && !isReference(fieldType)) {\n+      // assumes that fieldType is a boxed primitive.\n       s\"\"\"\n-        |$tmpTypeTerm $tmpTerm = $unboxedFieldCode;\n-        |boolean $nullTerm = $tmpTerm == null;\n+        |boolean $nullTerm = $fieldTerm == null;\n         |$resultTypeTerm $resultTerm;\n         |if ($nullTerm) {\n         |  $resultTerm = $defaultValue;\n         |}\n         |else {\n-        |  $resultTerm = $tmpTerm;\n+        |  $resultTerm = $fieldTerm;\n         |}\n         |\"\"\".stripMargin\n     } else if (nullCheck) {",
                "raw_url": "https://github.com/apache/flink/raw/29e849b1bf9180a5aa5f2d500efb283a39839caa/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/codegen/CodeGenerator.scala",
                "sha": "bf6ee217b9683edd1bdf0bf5080555fddcc6335c",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/flink/blob/29e849b1bf9180a5aa5f2d500efb283a39839caa/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/expressions/TemporalTypesTest.scala",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/expressions/TemporalTypesTest.scala?ref=29e849b1bf9180a5aa5f2d500efb283a39839caa",
                "deletions": 2,
                "filename": "flink-libraries/flink-table/src/test/scala/org/apache/flink/table/expressions/TemporalTypesTest.scala",
                "patch": "@@ -538,10 +538,31 @@ class TemporalTypesTest extends ExpressionTestBase {\n       \"1990-09-12 10:20:45.123\")\n   }\n \n+  @Test\n+  def testSelectNullValues(): Unit ={\n+    testAllApis(\n+      'f11,\n+      \"f11\",\n+      \"f11\",\n+      \"null\"\n+    )\n+    testAllApis(\n+      'f12,\n+      \"f12\",\n+      \"f12\",\n+      \"null\"\n+    )\n+    testAllApis(\n+      'f13,\n+      \"f13\",\n+      \"f13\",\n+      \"null\"\n+    )\n+  }\n   // ----------------------------------------------------------------------------------------------\n \n   def testData: Row = {\n-    val testData = new Row(11)\n+    val testData = new Row(14)\n     testData.setField(0, Date.valueOf(\"1990-10-14\"))\n     testData.setField(1, Time.valueOf(\"10:20:45\"))\n     testData.setField(2, Timestamp.valueOf(\"1990-10-14 10:20:45.123\"))\n@@ -553,6 +574,10 @@ class TemporalTypesTest extends ExpressionTestBase {\n     testData.setField(8, 1467012213000L)\n     testData.setField(9, 24)\n     testData.setField(10, 12000L)\n+    // null selection test.\n+    testData.setField(11, null)\n+    testData.setField(12, null)\n+    testData.setField(13, null)\n     testData\n   }\n \n@@ -568,6 +593,9 @@ class TemporalTypesTest extends ExpressionTestBase {\n       Types.INT,\n       Types.LONG,\n       Types.INTERVAL_MONTHS,\n-      Types.INTERVAL_MILLIS).asInstanceOf[TypeInformation[Any]]\n+      Types.INTERVAL_MILLIS,\n+      Types.SQL_DATE,\n+      Types.SQL_TIME,\n+      Types.SQL_TIMESTAMP).asInstanceOf[TypeInformation[Any]]\n   }\n }",
                "raw_url": "https://github.com/apache/flink/raw/29e849b1bf9180a5aa5f2d500efb283a39839caa/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/expressions/TemporalTypesTest.scala",
                "sha": "8fae11a8f4de5d1ef52364f3df5bd18f4a047b76",
                "status": "modified"
            }
        ],
        "message": "[FLINK-7309] [table] Fix NullPointerException when selecting null fields.\n\nThis closes #4479.",
        "parent": "https://github.com/apache/flink/commit/68fdaa57e35b8ee30a262aad4d26926b18054c57",
        "patched_files": [
            "CodeGenerator.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "TemporalTypesTest.java"
        ]
    },
    "flink_2b0baea": {
        "bug_id": "flink_2b0baea",
        "commit": "https://github.com/apache/flink/commit/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java",
                "patch": "@@ -130,6 +130,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> MapOperator<T, R> map(MapFunction<T, R> mapper) {\n+\t\tif (mapper == null) {\n+\t\t\tthrow new NullPointerException(\"Map function must not be null.\");\n+\t\t}\n \t\treturn new MapOperator<T, R>(this, mapper);\n \t}\n \t\n@@ -146,6 +149,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> FlatMapOperator<T, R> flatMap(FlatMapFunction<T, R> flatMapper) {\n+\t\tif (flatMapper == null) {\n+\t\t\tthrow new NullPointerException(\"FlatMap function must not be null.\");\n+\t\t}\n \t\treturn new FlatMapOperator<T, R>(this, flatMapper);\n \t}\n \t\n@@ -163,6 +169,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic FilterOperator<T> filter(FilterFunction<T> filter) {\n+\t\tif (filter == null) {\n+\t\t\tthrow new NullPointerException(\"Filter function must not be null.\");\n+\t\t}\n \t\treturn new FilterOperator<T>(this, filter);\n \t}\n \t\n@@ -229,6 +238,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic ReduceOperator<T> reduce(ReduceFunction<T> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceOperator<T>(this, reducer);\n \t}\n \t\n@@ -246,6 +258,9 @@ public ExecutionEnvironment getExecutionEnvironment() {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n \t",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/DataSet.java",
                "sha": "758cbf2133192b330a4719ef71ad3d718a6ebc23",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java",
                "patch": "@@ -439,6 +439,9 @@ private CoGroupOperatorWithoutFunction(Keys<I2> keys2) {\n \t\t\t\t * @see DataSet\n \t\t\t\t */\n \t\t\t\tpublic <R> CoGroupOperator<I1, I2, R> with(CoGroupFunction<I1, I2, R> function) {\n+\t\t\t\t\tif (function == null) {\n+\t\t\t\t\t\tthrow new NullPointerException(\"CoGroup function must not be null.\");\n+\t\t\t\t\t}\n \t\t\t\t\tTypeInformation<R> returnType = TypeExtractor.getCoGroupReturnTypes(function, input1.getType(), input2.getType());\n \t\t\t\t\treturn new CoGroupOperator<I1, I2, R>(input1, input2, keys1, keys2, function, returnType);\n \t\t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CoGroupOperator.java",
                "sha": "ca4b1db6cb9f978ab5ab60580490a4a7087e28c3",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java",
                "patch": "@@ -113,6 +113,9 @@ public DefaultCross(DataSet<I1> input1, DataSet<I2> input2) {\n \t\t * @see DataSet\n \t\t */\n \t\tpublic <R> CrossOperator<I1, I2, R> with(CrossFunction<I1, I2, R> function) {\n+\t\t\tif (function == null) {\n+\t\t\t\tthrow new NullPointerException(\"Cross function must not be null.\");\n+\t\t\t}\n \t\t\tTypeInformation<R> returnType = TypeExtractor.getCrossReturnTypes(function, input1.getType(), input2.getType());\n \t\t\treturn new CrossOperator<I1, I2, R>(input1, input2, function, returnType);\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/CrossOperator.java",
                "sha": "3566224eac5ee4b6a5da772318d054b63277a594",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 4,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java",
                "patch": "@@ -34,10 +34,6 @@\n \tpublic FilterOperator(DataSet<T> input, FilterFunction<T> function) {\n \t\tsuper(input, input.getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Filter function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FilterOperator.java",
                "sha": "adbe77f3782e234af68cfbf53c1f9476eda43c84",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 4,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java",
                "patch": "@@ -37,10 +37,6 @@\n \tpublic FlatMapOperator(DataSet<IN> input, FlatMapFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getFlatMapReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"FlatMap function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/FlatMapOperator.java",
                "sha": "32f2343fa82fff567cf03d3a4f66224dbcc3c0b8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java",
                "patch": "@@ -421,6 +421,9 @@ protected DefaultJoin(DataSet<I1> input1, DataSet<I2> input2,\n \t\t * @see DataSet\n \t\t */\n \t\tpublic <R> EquiJoin<I1, I2, R> with(JoinFunction<I1, I2, R> function) {\n+\t\t\tif (function == null) {\n+\t\t\t\tthrow new NullPointerException(\"Join function must not be null.\");\n+\t\t\t}\n \t\t\tTypeInformation<R> returnType = TypeExtractor.getJoinReturnTypes(function, getInput1Type(), getInput2Type());\n \t\t\treturn new EquiJoin<I1, I2, R>(getInput1(), getInput2(), getKeys1(), getKeys2(), function, returnType, getJoinHint());\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/JoinOperator.java",
                "sha": "992cc0a74f5434945e25fed90d03006e42ffa9ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 4,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java",
                "patch": "@@ -39,10 +39,6 @@\n \tpublic MapOperator(DataSet<IN> input, MapFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getMapReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Map function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\textractSemanticAnnotationsFromUdf(function.getClass());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/MapOperator.java",
                "sha": "00bbb27f89f3c85fb801d56ec32a8f518d69e1a1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 8,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java",
                "patch": "@@ -58,10 +58,6 @@\n \tpublic ReduceGroupOperator(DataSet<IN> input, GroupReduceFunction<IN, OUT> function) {\n \t\tsuper(input, TypeExtractor.getGroupReduceReturnTypes(function, input.getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = null;\n \t\tcheckCombinability();\n@@ -76,10 +72,6 @@ public ReduceGroupOperator(DataSet<IN> input, GroupReduceFunction<IN, OUT> funct\n \tpublic ReduceGroupOperator(Grouping<IN> input, GroupReduceFunction<IN, OUT> function) {\n \t\tsuper(input != null ? input.getDataSet() : null, TypeExtractor.getGroupReduceReturnTypes(function, input.getDataSet().getType()));\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = input;\n \t\tcheckCombinability();",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceGroupOperator.java",
                "sha": "e001d910b6bb292d9f885b2f4e4af19e5fab5422",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 8,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java",
                "patch": "@@ -53,10 +53,6 @@\n \tpublic ReduceOperator(DataSet<IN> input, ReduceFunction<IN> function) {\n \t\tsuper(input, input.getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = null;\n \t\t\n@@ -67,10 +63,6 @@ public ReduceOperator(DataSet<IN> input, ReduceFunction<IN> function) {\n \tpublic ReduceOperator(Grouping<IN> input, ReduceFunction<IN> function) {\n \t\tsuper(input.getDataSet(), input.getDataSet().getType());\n \t\t\n-\t\tif (function == null) {\n-\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n-\t\t}\n-\t\t\n \t\tthis.function = function;\n \t\tthis.grouper = input;\n \t\t",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/ReduceOperator.java",
                "sha": "6056e8b545ee7d21a2482c990c3c4b4c524f5786",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java",
                "patch": "@@ -72,6 +72,9 @@ public SortedGrouping(DataSet<T> set, Keys<T> keys, int field, Order order) {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n \t",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/SortedGrouping.java",
                "sha": "dc26a2b697345e49b2f134f416831033cd7bf38a",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java?ref=2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc",
                "deletions": 0,
                "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java",
                "patch": "@@ -64,6 +64,9 @@ public UnsortedGrouping(DataSet<T> set, Keys<T> keys) {\n \t * @see DataSet\n \t */\n \tpublic ReduceOperator<T> reduce(ReduceFunction<T> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"Reduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceOperator<T>(this, reducer);\n \t}\n \t\n@@ -81,6 +84,9 @@ public UnsortedGrouping(DataSet<T> set, Keys<T> keys) {\n \t * @see DataSet\n \t */\n \tpublic <R> ReduceGroupOperator<T, R> reduceGroup(GroupReduceFunction<T, R> reducer) {\n+\t\tif (reducer == null) {\n+\t\t\tthrow new NullPointerException(\"GroupReduce function must not be null.\");\n+\t\t}\n \t\treturn new ReduceGroupOperator<T, R>(this, reducer);\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/2b0baea9b8a6dd99052c2dfa98cae719a39d6bbc/stratosphere-java/src/main/java/eu/stratosphere/api/java/operators/UnsortedGrouping.java",
                "sha": "95e40bc44e3abbad5f6a307be680381cb8b84f94",
                "status": "modified"
            }
        ],
        "message": "Fixes bugs where the TypeExtractor throws an NPE instead of the operators",
        "parent": "https://github.com/apache/flink/commit/b746f452e7187dad08340b9cfdc2fa18a516a6c7",
        "patched_files": [
            "MapOperator.java",
            "ReduceOperator.java",
            "CrossOperator.java",
            "CoGroupOperator.java",
            "JoinOperator.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "ReduceOperatorTest.java",
            "CoGroupOperatorTest.java",
            "CrossOperatorTest.java",
            "MapOperatorTest.java",
            "JoinOperatorTest.java"
        ]
    },
    "flink_2b8db40": {
        "bug_id": "flink_2b8db40",
        "commit": "https://github.com/apache/flink/commit/2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/flink/blob/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/main/java/org/apache/flink/client/RemoteExecutor.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/main/java/org/apache/flink/client/RemoteExecutor.java?ref=2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
                "deletions": 3,
                "filename": "flink-clients/src/main/java/org/apache/flink/client/RemoteExecutor.java",
                "patch": "@@ -124,18 +124,26 @@ public String getOptimizerPlanAsJSON(Plan plan) throws Exception {\n \t// --------------------------------------------------------------------------------------------\n \t//   Utilities\n \t// --------------------------------------------------------------------------------------------\n-\tpublic static InetSocketAddress getInetFromHostport(String hostport) {\n+\n+\t/**\n+\t * Utility method that converts a string of the form \"host:port\" into an {@link InetSocketAddress}.\n+\t * The returned InetSocketAddress may be unresolved!\n+\t * \n+\t * @param hostport The \"host:port\" string.\n+\t * @return The converted InetSocketAddress.\n+\t */\n+\tprivate static InetSocketAddress getInetFromHostport(String hostport) {\n \t\t// from http://stackoverflow.com/questions/2345063/java-common-way-to-validate-and-convert-hostport-to-inetsocketaddress\n \t\tURI uri;\n \t\ttry {\n \t\t\turi = new URI(\"my://\" + hostport);\n \t\t} catch (URISyntaxException e) {\n-\t\t\tthrow new RuntimeException(\"Could not identify hostname and port\", e);\n+\t\t\tthrow new RuntimeException(\"Could not identify hostname and port in '\" + hostport + \"'.\", e);\n \t\t}\n \t\tString host = uri.getHost();\n \t\tint port = uri.getPort();\n \t\tif (host == null || port == -1) {\n-\t\t\tthrow new RuntimeException(\"Could not identify hostname and port\");\n+\t\t\tthrow new RuntimeException(\"Could not identify hostname and port in '\" + hostport + \"'.\");\n \t\t}\n \t\treturn new InetSocketAddress(host, port);\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/main/java/org/apache/flink/client/RemoteExecutor.java",
                "sha": "373d70c8009456791313fae311f28c51f781b682",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/flink/blob/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/main/java/org/apache/flink/client/program/Client.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/main/java/org/apache/flink/client/program/Client.java?ref=2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
                "deletions": 39,
                "filename": "flink-clients/src/main/java/org/apache/flink/client/program/Client.java",
                "patch": "@@ -22,11 +22,14 @@\n import java.io.File;\n import java.io.IOException;\n import java.io.PrintStream;\n+import java.net.InetAddress;\n import java.net.InetSocketAddress;\n+import java.net.UnknownHostException;\n import java.util.List;\n \n import org.apache.flink.api.common.JobID;\n import org.apache.flink.api.common.JobSubmissionResult;\n+import org.apache.flink.configuration.IllegalConfigurationException;\n import org.apache.flink.runtime.akka.AkkaUtils;\n import org.apache.flink.api.common.JobExecutionResult;\n import org.apache.flink.api.common.Plan;\n@@ -43,7 +46,6 @@\n import org.apache.flink.optimizer.plantranslate.JobGraphGenerator;\n import org.apache.flink.configuration.ConfigConstants;\n import org.apache.flink.configuration.Configuration;\n-import org.apache.flink.configuration.GlobalConfiguration;\n import org.apache.flink.core.fs.Path;\n import org.apache.flink.runtime.client.JobClient;\n import org.apache.flink.runtime.client.JobExecutionException;\n@@ -65,12 +67,20 @@\n public class Client {\n \t\n \tprivate static final Logger LOG = LoggerFactory.getLogger(Client.class);\n+\n+\t/** The configuration to use for the client (optimizer, timeouts, ...) */\n+\tprivate final Configuration configuration;\n+\n+\t/** The address of the JobManager to send the program to */\n+\tprivate final InetSocketAddress jobManagerAddress;\n+\n+\t/** The optimizer used in the optimization of batch programs */\n+\tprivate final Optimizer compiler;\n+\n+\t/** The class loader to use for classes from the user program (e.g., functions and data types) */\n+\tprivate final ClassLoader userCodeClassLoader;\n \t\n-\t\n-\tprivate final Configuration configuration;\t// the configuration describing the job manager address\n-\t\n-\tprivate final Optimizer compiler;\t\t// the compiler to compile the jobs\n-\t\n+\t/** Flag indicating whether to sysout print execution updates */\n \tprivate boolean printStatusDuringExecution = true;\n \n \t/**\n@@ -79,12 +89,9 @@\n \t */\n \tprivate int maxSlots = -1;\n \n-\t/**\n-\t * ID of the last job submitted with this client.\n-\t */\n+\t/** ID of the last job submitted with this client. */\n \tprivate JobID lastJobId = null;\n-\n-\tprivate ClassLoader userCodeClassLoader;\n+\t\n \t\n \t// ------------------------------------------------------------------------\n \t//                            Construction\n@@ -96,56 +103,86 @@\n \t * \n \t * @param jobManagerAddress Address and port of the job-manager.\n \t */\n-\tpublic Client(InetSocketAddress jobManagerAddress, Configuration config, ClassLoader userCodeClassLoader, int maxSlots) {\n+\tpublic Client(InetSocketAddress jobManagerAddress, Configuration config, \n+\t\t\t\t\t\t\tClassLoader userCodeClassLoader, int maxSlots) throws UnknownHostException\n+\t{\n+\t\tPreconditions.checkNotNull(jobManagerAddress, \"JobManager address is null\");\n \t\tPreconditions.checkNotNull(config, \"Configuration is null\");\n+\t\tPreconditions.checkNotNull(userCodeClassLoader, \"User code ClassLoader is null\");\n+\t\t\n \t\tthis.configuration = config;\n \t\t\n-\t\t// using the host string instead of the host name saves a reverse name lookup\n-\t\tconfiguration.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, jobManagerAddress.getAddress().getHostAddress());\n-\t\tconfiguration.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, jobManagerAddress.getPort());\n+\t\tif (jobManagerAddress.isUnresolved()) {\n+\t\t\t// address is unresolved, resolve it\n+\t\t\tString host = jobManagerAddress.getHostString();\n+\t\t\ttry {\n+\t\t\t\tInetAddress address = InetAddress.getByName(host);\n+\t\t\t\tthis.jobManagerAddress = new InetSocketAddress(address, jobManagerAddress.getPort());\n+\t\t\t}\n+\t\t\tcatch (UnknownHostException e) {\n+\t\t\t\tthrow new UnknownHostException(\"Cannot resolve JobManager host name '\" + host + \"'.\");\n+\t\t\t}\n+\t\t}\n+\t\telse {\n+\t\t\t// address is already resolved, use it as is\n+\t\t\tthis.jobManagerAddress = jobManagerAddress;\n+\t\t}\n \t\t\n \t\tthis.compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), configuration);\n \t\tthis.userCodeClassLoader = userCodeClassLoader;\n \t\tthis.maxSlots = maxSlots;\n \t}\n \n \t/**\n-\t * Creates a instance that submits the programs to the job-manager defined in the\n-\t * configuration.\n+\t * Creates a instance that submits the programs to the JobManager defined in the\n+\t * configuration. This method will try to resolve the JobManager hostname and throw an exception\n+\t * if that is not possible.\n \t * \n \t * @param config The config used to obtain the job-manager's address.\n+\t * @param userCodeClassLoader The class loader to use for loading user code classes.   \n \t */\n-\tpublic Client(Configuration config, ClassLoader userCodeClassLoader) {\n+\tpublic Client(Configuration config, ClassLoader userCodeClassLoader) throws UnknownHostException {\n \t\tPreconditions.checkNotNull(config, \"Configuration is null\");\n+\t\tPreconditions.checkNotNull(userCodeClassLoader, \"User code ClassLoader is null\");\n+\t\t\n \t\tthis.configuration = config;\n+\t\tthis.userCodeClassLoader = userCodeClassLoader;\n \t\t\n \t\t// instantiate the address to the job manager\n \t\tfinal String address = config.getString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, null);\n \t\tif (address == null) {\n-\t\t\tthrow new CompilerException(\"Cannot find address to job manager's RPC service in the global configuration.\");\n+\t\t\tthrow new IllegalConfigurationException(\n+\t\t\t\t\t\"Cannot find address to job manager's RPC service in the global configuration.\");\n \t\t}\n \t\t\n-\t\tfinal int port = GlobalConfiguration.getInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, ConfigConstants.DEFAULT_JOB_MANAGER_IPC_PORT);\n+\t\tfinal int port = config.getInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY,\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_IPC_PORT);\n \t\tif (port < 0) {\n-\t\t\tthrow new CompilerException(\"Cannot find port to job manager's RPC service in the global configuration.\");\n+\t\t\tthrow new IllegalConfigurationException(\"Cannot find port to job manager's RPC service in the global configuration.\");\n+\t\t}\n+\t\t\n+\t\ttry {\n+\t\t\tInetAddress inetAddress = InetAddress.getByName(address);\n+\t\t\tthis.jobManagerAddress = new InetSocketAddress(inetAddress, port);\n+\t\t}\n+\t\tcatch (UnknownHostException e) {\n+\t\t\tthrow new UnknownHostException(\"Cannot resolve the JobManager hostname '\" + address\n+\t\t\t\t\t+ \"' specified in the configuration\");\n \t\t}\n \n \t\tthis.compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), configuration);\n-\t\tthis.userCodeClassLoader = userCodeClassLoader;\n \t}\n-\t\n+\n+\t/**\n+\t * Configures whether the client should print progress updates during the execution to {@code System.out}.\n+\t * All updates are logged via the SLF4J loggers regardless of this setting.\n+\t * \n+\t * @param print True to print updates to standard out during execution, false to not print them.\n+\t */\n \tpublic void setPrintStatusDuringExecution(boolean print) {\n \t\tthis.printStatusDuringExecution = print;\n \t}\n \n-\tpublic String getJobManagerAddress() {\n-\t\treturn this.configuration.getString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, null);\n-\t}\n-\t\n-\tpublic int getJobManagerPort() {\n-\t\treturn this.configuration.getInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, -1);\n-\t}\n-\n \t/**\n \t * @return -1 if unknown. The maximum number of available processing slots at the Flink cluster\n \t * connected to this client.\n@@ -316,14 +353,7 @@ public JobSubmissionResult run(OptimizedPlan compiledPlan, List<File> libraries,\n \n \tpublic JobSubmissionResult run(JobGraph jobGraph, boolean wait) throws ProgramInvocationException {\n \t\tthis.lastJobId = jobGraph.getJobID();\n-\n-\t\tInetSocketAddress jobManagerAddress;\n-\t\ttry {\n-\t\t\tjobManagerAddress = JobClient.getJobManagerAddress(configuration);\n-\t\t}\n-\t\tcatch (IOException e) {\n-\t\t\tthrow new ProgramInvocationException(e.getMessage(), e);\n-\t\t}\n+\t\t\n \t\tLOG.info(\"JobManager actor system address is \" + jobManagerAddress);\n \t\t\n \t\tLOG.info(\"Starting client actor system\");",
                "raw_url": "https://github.com/apache/flink/raw/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/main/java/org/apache/flink/client/program/Client.java",
                "sha": "c61e814eb87473b69d2bfe85ec4c22a1fc0c5a21",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/test/java/org/apache/flink/client/CliFrontendPackageProgramTest.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/test/java/org/apache/flink/client/CliFrontendPackageProgramTest.java?ref=2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
                "deletions": 11,
                "filename": "flink-clients/src/test/java/org/apache/flink/client/CliFrontendPackageProgramTest.java",
                "patch": "@@ -294,25 +294,23 @@ public void testPlanWithExternalClass() throws CompilerException, ProgramInvocat\n \t\t\tassertArrayEquals(progArgs, prog.getArguments());\n \n \t\t\tConfiguration c = new Configuration();\n-\t\t\tc.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"devil\");\n+\t\t\tc.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, \"localhost\");\n \t\t\tClient cli = new Client(c, getClass().getClassLoader());\n \t\t\t\n+\t\t\t// we expect this to fail with a \"ClassNotFoundException\"\n \t\t\tcli.getOptimizedPlanAsJson(prog, 666);\n+\t\t\tfail(\"Should have failed with a ClassNotFoundException\");\n \t\t}\n-\t\tcatch (ProgramInvocationException pie) {\n-\t\t\tassertTrue(\"Classloader was not called\", callme[0]);\n-\t\t\t// class not found exception is expected as some point\n-\t\t\tif( ! ( pie.getCause() instanceof ClassNotFoundException ) ) {\n-\t\t\t\tSystem.err.println(pie.getMessage());\n-\t\t\t\tpie.printStackTrace();\n-\t\t\t\tfail(\"Program caused an exception: \" + pie.getMessage());\n+\t\tcatch (ProgramInvocationException e) {\n+\t\t\tif (!(e.getCause() instanceof ClassNotFoundException)) {\n+\t\t\t\te.printStackTrace();\n+\t\t\t\tfail(\"Program didn't throw ClassNotFoundException\");\n \t\t\t}\n+\t\t\tassertTrue(\"Classloader was not called\", callme[0]);\n \t\t}\n \t\tcatch (Exception e) {\n-\t\t\tSystem.err.println(e.getMessage());\n \t\t\te.printStackTrace();\n-\t\t\tassertTrue(\"Classloader was not called\", callme[0]);\n-\t\t\tfail(\"Program caused an exception: \" + e.getMessage());\n+\t\t\tfail(\"Program failed with the wrong exception: \" + e.getClass().getName());\n \t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/test/java/org/apache/flink/client/CliFrontendPackageProgramTest.java",
                "sha": "c9ce12bcf9c83e33d0c0d8872545ec0edd787714",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/flink/blob/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/test/java/org/apache/flink/client/RemoteExecutorHostnameResolutionTest.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/test/java/org/apache/flink/client/RemoteExecutorHostnameResolutionTest.java?ref=2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
                "deletions": 0,
                "filename": "flink-clients/src/test/java/org/apache/flink/client/RemoteExecutorHostnameResolutionTest.java",
                "patch": "@@ -0,0 +1,78 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.client;\n+\n+import org.apache.flink.api.common.Plan;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.flink.api.java.io.DiscardingOutputFormat;\n+import org.junit.Test;\n+\n+import java.net.InetSocketAddress;\n+import java.net.UnknownHostException;\n+import java.util.Collections;\n+\n+import static org.junit.Assert.fail;\n+\n+public class RemoteExecutorHostnameResolutionTest {\n+\n+\tprivate static final String nonExistingHostname = \"foo.bar.com.invalid\";\n+\tprivate static final int port = 14451;\n+\t\n+\t\n+\t@Test\n+\tpublic void testUnresolvableHostname1() {\n+\t\ttry {\n+\t\t\tRemoteExecutor exec = new RemoteExecutor(nonExistingHostname, port);\n+\t\t\texec.executePlan(getProgram());\n+\t\t\tfail(\"This should fail with an UnknownHostException\");\n+\t\t}\n+\t\tcatch (UnknownHostException e) {\n+\t\t\t// that is what we want!\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tSystem.err.println(\"Wrong exception!\");\n+\t\t\te.printStackTrace();\n+\t\t\tfail(e.getMessage());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testUnresolvableHostname2() {\n+\t\ttry {\n+\t\t\tInetSocketAddress add = new InetSocketAddress(nonExistingHostname, port);\n+\t\t\tRemoteExecutor exec = new RemoteExecutor(add, Collections.<String>emptyList());\n+\t\t\texec.executePlan(getProgram());\n+\t\t\tfail(\"This should fail with an UnknownHostException\");\n+\t\t}\n+\t\tcatch (UnknownHostException e) {\n+\t\t\t// that is what we want!\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tSystem.err.println(\"Wrong exception!\");\n+\t\t\te.printStackTrace();\n+\t\t\tfail(e.getMessage());\n+\t\t}\n+\t}\n+\t\n+\tprivate static Plan getProgram() {\n+\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n+\t\tenv.fromElements(1, 2, 3).output(new DiscardingOutputFormat<Integer>());\n+\t\treturn env.createProgramPlan();\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/test/java/org/apache/flink/client/RemoteExecutorHostnameResolutionTest.java",
                "sha": "a1bd0e2dd839ed2e326273640ee55921e1e62586",
                "status": "added"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/flink/blob/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/test/java/org/apache/flink/client/program/ClientHostnameResolutionTest.java",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/test/java/org/apache/flink/client/program/ClientHostnameResolutionTest.java?ref=2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
                "deletions": 0,
                "filename": "flink-clients/src/test/java/org/apache/flink/client/program/ClientHostnameResolutionTest.java",
                "patch": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.client.program;\n+\n+import org.apache.flink.configuration.ConfigConstants;\n+import org.apache.flink.configuration.Configuration;\n+import org.junit.Test;\n+\n+import java.net.InetSocketAddress;\n+import java.net.UnknownHostException;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Tests that verify that the client correctly handles non-resolvable host names and does not\n+ * fail with another exception\n+ */\n+public class ClientHostnameResolutionTest {\n+\t\n+\tprivate static final String nonExistingHostname = \"foo.bar.com.invalid\";\n+\t\n+\t@Test\n+\tpublic void testUnresolvableHostname1() {\n+\t\ttry {\n+\t\t\tInetSocketAddress addr = new InetSocketAddress(nonExistingHostname, 17234);\n+\t\t\tnew Client(addr, new Configuration(), getClass().getClassLoader(), 1);\n+\t\t\tfail(\"This should fail with an UnknownHostException\");\n+\t\t}\n+\t\tcatch (UnknownHostException e) {\n+\t\t\t// that is what we want!\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tSystem.err.println(\"Wrong exception!\");\n+\t\t\te.printStackTrace();\n+\t\t\tfail(e.getMessage());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testUnresolvableHostname2() {\n+\t\ttry {\n+\t\t\tConfiguration config = new Configuration();\n+\t\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, nonExistingHostname);\n+\t\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, 17234);\n+\t\t\t\n+\t\t\tnew Client(config, getClass().getClassLoader());\n+\t\t\tfail(\"This should fail with an UnknownHostException\");\n+\t\t}\n+\t\tcatch (UnknownHostException e) {\n+\t\t\t// that is what we want!\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tSystem.err.println(\"Wrong exception!\");\n+\t\t\te.printStackTrace();\n+\t\t\tfail(e.getMessage());\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-clients/src/test/java/org/apache/flink/client/program/ClientHostnameResolutionTest.java",
                "sha": "2cdb1a0a9325439cf3332826a88754220a9ea807",
                "status": "added"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/flink/blob/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java?ref=2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
                "deletions": 13,
                "filename": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.List;\n \n@@ -37,9 +38,9 @@\n public class RemoteStreamEnvironment extends StreamExecutionEnvironment {\n \tprivate static final Logger LOG = LoggerFactory.getLogger(RemoteStreamEnvironment.class);\n \n-\tprivate String host;\n-\tprivate int port;\n-\tprivate List<File> jarFiles;\n+\tprivate final String host;\n+\tprivate final int port;\n+\tprivate final List<File> jarFiles;\n \n \t/**\n \t * Creates a new RemoteStreamEnvironment that points to the master\n@@ -82,14 +83,14 @@ public RemoteStreamEnvironment(String host, int port, String... jarFiles) {\n \t}\n \n \t@Override\n-\tpublic JobExecutionResult execute() {\n+\tpublic JobExecutionResult execute() throws ProgramInvocationException {\n \n \t\tJobGraph jobGraph = streamGraph.getJobGraph();\n \t\treturn executeRemotely(jobGraph);\n \t}\n \n \t@Override\n-\tpublic JobExecutionResult execute(String jobName) {\n+\tpublic JobExecutionResult execute(String jobName) throws ProgramInvocationException {\n \n \t\tJobGraph jobGraph = streamGraph.getJobGraph(jobName);\n \t\treturn executeRemotely(jobGraph);\n@@ -102,7 +103,7 @@ public JobExecutionResult execute(String jobName) {\n \t *            jobGraph to execute\n \t * @return The result of the job execution, containing elapsed time and accumulators.\n \t */\n-\tprivate JobExecutionResult executeRemotely(JobGraph jobGraph) {\n+\tprivate JobExecutionResult executeRemotely(JobGraph jobGraph) throws ProgramInvocationException {\n \t\tif (LOG.isInfoEnabled()) {\n \t\t\tLOG.info(\"Running remotely at {}:{}\", host, port);\n \t\t}\n@@ -112,20 +113,29 @@ private JobExecutionResult executeRemotely(JobGraph jobGraph) {\n \t\t}\n \n \t\tConfiguration configuration = jobGraph.getJobConfiguration();\n-\t\tClient client = new Client(new InetSocketAddress(host, port), configuration,\n-\t\t\t\tJobWithJars.buildUserCodeClassLoader(jarFiles, JobWithJars.class.getClassLoader()), -1);\n-\t\tclient.setPrintStatusDuringExecution(getConfig().isSysoutLoggingEnabled());\n-\t\t\n+\t\tClassLoader usercodeClassLoader = JobWithJars.buildUserCodeClassLoader(jarFiles, getClass().getClassLoader());\n+\n \t\ttry {\n+\t\t\tClient client = new Client(new InetSocketAddress(host, port), configuration, usercodeClassLoader, -1);\n+\t\t\tclient.setPrintStatusDuringExecution(getConfig().isSysoutLoggingEnabled());\n+\t\t\t\n \t\t\tJobSubmissionResult result = client.run(jobGraph, true);\n-\t\t\tif(result instanceof JobExecutionResult) {\n+\t\t\tif (result instanceof JobExecutionResult) {\n \t\t\t\treturn (JobExecutionResult) result;\n \t\t\t} else {\n \t\t\t\tLOG.warn(\"The Client didn't return a JobExecutionResult\");\n \t\t\t\treturn new JobExecutionResult(result.getJobID(), -1, null);\n \t\t\t}\n-\t\t} catch (ProgramInvocationException e) {\n-\t\t\tthrow new RuntimeException(\"Cannot execute job due to ProgramInvocationException\", e);\n+\t\t}\n+\t\tcatch (ProgramInvocationException e) {\n+\t\t\tthrow e;\n+\t\t}\n+\t\tcatch (UnknownHostException e) {\n+\t\t\tthrow new ProgramInvocationException(e.getMessage(), e);\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tString term = e.getMessage() == null ? \".\" : (\": \" + e.getMessage());\n+\t\t\tthrow new ProgramInvocationException(\"The program execution failed\" + term, e);\n \t\t}\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java",
                "sha": "50127cf8214c8621212580c10e5fa18e3dc12c21",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/ExecutionEnvironmentITCase.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/ExecutionEnvironmentITCase.java?ref=2b8db40ac40d70027ce331f3a04c6ca7aa562a84",
                "deletions": 0,
                "filename": "flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/ExecutionEnvironmentITCase.java",
                "patch": "@@ -43,6 +43,7 @@\n /**\n  * Test ExecutionEnvironment from user perspective\n  */\n+@SuppressWarnings(\"serial\")\n @RunWith(Parameterized.class)\n public class ExecutionEnvironmentITCase extends MultipleProgramsTestBase {\n \tprivate static final int PARALLELISM = 5;\n@@ -66,8 +67,10 @@ public ExecutionEnvironmentITCase(TestExecutionMode mode) {\n \tpublic void testLocalEnvironmentWithConfig() throws Exception {\n \t\tConfiguration conf = new Configuration();\n \t\tconf.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, PARALLELISM);\n+\t\t\n \t\tfinal ExecutionEnvironment env = ExecutionEnvironment.createLocalEnvironment(conf);\n \t\tenv.setParallelism(ExecutionConfig.PARALLELISM_AUTO_MAX);\n+\t\tenv.getConfig().disableSysoutLogging();\n \n \t\tDataSet<Integer> result = env.createInput(new ParallelismDependentInputFormat())\n \t\t\t\t.rebalance()",
                "raw_url": "https://github.com/apache/flink/raw/2b8db40ac40d70027ce331f3a04c6ca7aa562a84/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/ExecutionEnvironmentITCase.java",
                "sha": "109af1e35b68291d41d6fdf94e5ed759421d4b19",
                "status": "modified"
            }
        ],
        "message": "[FLINK-1918] [client] Fix misleading NullPointerException in case of unresolvable host names",
        "parent": "https://github.com/apache/flink/commit/b70431239a5e18555866addb41ee6edf2b79ff60",
        "patched_files": [
            "RemoteExecutor.java",
            "RemoteStreamEnvironment.java",
            "ExecutionEnvironmentITCase.java",
            "Client.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "RemoteExecutorHostnameResolutionTest.java",
            "CliFrontendPackageProgramTest.java",
            "ClientTest.java",
            "ClientHostnameResolutionTest.java"
        ]
    },
    "flink_32ad8e5": {
        "bug_id": "flink_32ad8e5",
        "commit": "https://github.com/apache/flink/commit/32ad8e5ede160e641201ed4804b820e89a711806",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudInstance.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudInstance.java?ref=32ad8e5ede160e641201ed4804b820e89a711806",
                "deletions": 13,
                "filename": "nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudInstance.java",
                "patch": "@@ -40,20 +40,18 @@\n \n \t/** The instance ID. */\n \tprivate final String instanceID;\n-\t\n+\n \t/** The AWS Access Key to access this machine */\n \tprivate String awsAccessKey;\n-\t\n+\n \t/** The AWS Secret Key to access this machine */\n \tprivate String awsSecretKey;\n-\t\n+\n \t/** The time the instance was allocated. */\n \tprivate final long launchTime;\n \n \t/** The last received heart beat. */\n \tprivate long lastReceivedHeartBeat = System.currentTimeMillis();\n-\t\n-\t\n \n \t/**\n \t * Creates a new cloud instance.\n@@ -71,23 +69,24 @@\n \t * @param hardwareDescription\n \t *        the hardware description reported by the instance itself\n \t * @param awsAccessKey\n-\t * \t\t  The AWS Access Key to access this machine\n+\t *        The AWS Access Key to access this machine\n \t * @param awsSecretKey\n-\t * \t\t  The AWS Secret Key to access this machine\n+\t *        The AWS Secret Key to access this machine\n \t */\n-\tpublic EC2CloudInstance(String instanceID, InstanceType type, \n+\tpublic EC2CloudInstance(String instanceID, InstanceType type,\n \t\t\tInstanceConnectionInfo instanceConnectionInfo, long launchTime, NetworkNode parentNode,\n-\t\t\tNetworkTopology networkTopology, HardwareDescription hardwareDescription, String awsAccessKey, String awsSecretKey) {\n+\t\t\tNetworkTopology networkTopology, HardwareDescription hardwareDescription, String awsAccessKey,\n+\t\t\tString awsSecretKey) {\n \t\tsuper(type, instanceConnectionInfo, parentNode, networkTopology, hardwareDescription);\n \n \t\tthis.allocatedResource = new AllocatedResource(this, type, new AllocationID());\n \n \t\tthis.instanceID = instanceID;\n \n \t\tthis.launchTime = launchTime;\n-\t\t\n+\n \t\tthis.awsAccessKey = awsAccessKey;\n-\t\t\n+\n \t\tthis.awsSecretKey = awsSecretKey;\n \t}\n \n@@ -131,9 +130,11 @@ public AllocatedResource asAllocatedResource() {\n \n \t/**\n \t * Returns this Instance as a FloatingInstance object\n+\t * \n \t * @return\n \t */\n-\tpublic FloatingInstance asFloatingInstance(){\n-\t\treturn new FloatingInstance(this.instanceID, this.getInstanceConnectionInfo(), this.launchTime, this.getType(), this.awsAccessKey, this.awsSecretKey);\n+\tpublic FloatingInstance asFloatingInstance() {\n+\t\treturn new FloatingInstance(this.instanceID, this.getInstanceConnectionInfo(), this.launchTime, this.getType(),\n+\t\t\tthis.getHardwareDescription(), this.awsAccessKey, this.awsSecretKey);\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudInstance.java",
                "sha": "08c15a24bcd33b1a68342212b17cf54d9c78ed0c",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/flink/blob/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudManager.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudManager.java?ref=32ad8e5ede160e641201ed4804b820e89a711806",
                "deletions": 39,
                "filename": "nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudManager.java",
                "patch": "@@ -58,6 +58,7 @@\n import eu.stratosphere.nephele.instance.InstanceTypeDescriptionFactory;\n import eu.stratosphere.nephele.instance.InstanceTypeFactory;\n import eu.stratosphere.nephele.jobgraph.JobID;\n+import eu.stratosphere.nephele.topology.NetworkNode;\n import eu.stratosphere.nephele.topology.NetworkTopology;\n import eu.stratosphere.nephele.util.SerializableHashMap;\n import eu.stratosphere.nephele.util.StringUtils;\n@@ -103,8 +104,10 @@\n \t */\n \tprivate static final int DEFAULTCLEANUPINTERVAL = 2 * 60 * 1000; // 2 min\n \n-\t/** TMs that send HeartBeats but do not belong to any job are kept in this set. */\n-\tprivate final HashSet<InstanceConnectionInfo> orphanedTMs = new HashSet<InstanceConnectionInfo>();\n+\t/**\n+\t * Instances that send heart beats but do not belong to any job are kept in this map.\n+\t **/\n+\tprivate final Map<InstanceConnectionInfo, HardwareDescription> orphanedInstances = new HashMap<InstanceConnectionInfo, HardwareDescription>();\n \n \t/** The array of all available instance types in the cloud. */\n \tprivate final InstanceType[] availableInstanceTypes;\n@@ -136,9 +139,9 @@\n \tprivate long cleanUpInterval = DEFAULTCLEANUPINTERVAL;\n \n \t/**\n-\t * The network topology inside the cloud (currently only a fake).\n+\t * The network topology for each job.\n \t */\n-\tprivate final NetworkTopology networkTopology;\n+\tprivate final Map<JobID, NetworkTopology> networkTopologies = new HashMap<JobID, NetworkTopology>();\n \n \t/**\n \t * The preferred availability for instances on EC2.\n@@ -161,8 +164,6 @@ public EC2CloudManager() {\n \t\t\tthis.cleanUpInterval = DEFAULTCLEANUPINTERVAL;\n \t\t}\n \n-\t\tthis.networkTopology = NetworkTopology.createEmptyTopology();\n-\n \t\tthis.availabilityZone = GlobalConfiguration.getString(\"instancemanager.ec2.availabilityzone\", null);\n \t\tif (this.availabilityZone == null) {\n \t\t\tLOG.info(\"No preferred availability zone configured\");\n@@ -294,7 +295,9 @@ public synchronized void releaseAllocatedResource(final JobID jobID, final Confi\n \t\t\tthis.floatingInstances.put(instance.getInstanceConnectionInfo(),\n \t\t\t\t((EC2CloudInstance) instance).asFloatingInstance());\n \n-\t\t\tLOG.info(\"Convert \" + ((EC2CloudInstance) instance).getInstanceID()\n+\t\t\t// TODO: Clean up job to instance mapping and network topology\n+\n+\t\t\tLOG.info(\"Converting \" + ((EC2CloudInstance) instance).getInstanceID()\n \t\t\t\t+ \" from allocated instance to floating instance\");\n \n \t\t} else {\n@@ -311,8 +314,10 @@ public synchronized void reportHeartBeat(final InstanceConnectionInfo instanceCo\n \t\t\tfinal HardwareDescription hardwareDescription) {\n \n \t\t// Check if this TM is orphaned\n-\t\tif (this.orphanedTMs.contains(instanceConnectionInfo)) {\n-\t\t\tLOG.debug(\"Received HeartBeat from orphaned TM \" + instanceConnectionInfo);\n+\t\tif (this.orphanedInstances.containsKey(instanceConnectionInfo)) {\n+\t\t\tif (LOG.isDebugEnabled()) {\n+\t\t\t\tLOG.debug(\"Received heart beat from orphaned instance \" + instanceConnectionInfo);\n+\t\t\t}\n \t\t\treturn;\n \t\t}\n \n@@ -333,7 +338,7 @@ public synchronized void reportHeartBeat(final InstanceConnectionInfo instanceCo\n \n \t\t// Check if heart beat belongs to a reserved instance\n \t\ttry {\n-\t\t\tinstance = isReservedInstance(instanceConnectionInfo);\n+\t\t\tinstance = isReservedInstance(instanceConnectionInfo, hardwareDescription);\n \t\t} catch (InstanceException e) {\n \t\t\tLOG.error(e);\n \t\t}\n@@ -362,8 +367,9 @@ public synchronized void reportHeartBeat(final InstanceConnectionInfo instanceCo\n \t\t}\n \n \t\t// This TM seems to be unknown to the JobManager.. blacklist\n-\t\tLOG.info(\"Received HeartBeat from unknown TM. Put into orphaned TM set. Address is: \" + instanceConnectionInfo);\n-\t\tthis.orphanedTMs.add(instanceConnectionInfo);\n+\t\tLOG.info(\"Received heart beat from unknown instance \" + instanceConnectionInfo\n+\t\t\t+ \", converting it into orphaned instance\");\n+\t\tthis.orphanedInstances.put(instanceConnectionInfo, hardwareDescription);\n \n \t}\n \n@@ -399,11 +405,14 @@ private EC2CloudInstance isAssignedInstance(final InstanceConnectionInfo instanc\n \t * \n \t * @param instanceConnectionInfo\n \t *        the {@link InstanceConnectionInfo} object identifying the instance\n+\t * @param hardwareDescription\n+\t *        the actual hardware description of the instance\n \t * @return a cloud instance\n \t * @throws InstanceException\n \t *         something wrong happens to the global configuration\n \t */\n-\tprivate EC2CloudInstance isReservedInstance(final InstanceConnectionInfo instanceConnectionInfo)\n+\tprivate EC2CloudInstance isReservedInstance(final InstanceConnectionInfo instanceConnectionInfo,\n+\t\t\tfinal HardwareDescription hardwareDescription)\n \t\t\tthrows InstanceException {\n \n \t\tif (instanceConnectionInfo == null) {\n@@ -424,18 +433,18 @@ private EC2CloudInstance isReservedInstance(final InstanceConnectionInfo instanc\n \t\t\t\tjobsWithReservedInstances.add(id);\n \t\t\t}\n \n-\t\t\t// Now we call the webservice for each job..\n+\t\t\t// Now we call the web service for each job..\n \n-\t\t\tfor (JobID id : jobsWithReservedInstances) {\n+\t\t\tfor (final JobID jobID : jobsWithReservedInstances) {\n \n \t\t\t\tJobToInstancesMapping mapping = null;\n \n \t\t\t\tsynchronized (this.jobToInstancesAssignmentMap) {\n-\t\t\t\t\tmapping = this.jobToInstancesAssignmentMap.get(id);\n+\t\t\t\t\tmapping = this.jobToInstancesAssignmentMap.get(jobID);\n \t\t\t\t}\n \n \t\t\t\tif (mapping == null) {\n-\t\t\t\t\tLOG.error(\"Unknown mapping for job ID \" + id);\n+\t\t\t\t\tLOG.error(\"Unknown mapping for job ID \" + jobID);\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \n@@ -458,8 +467,18 @@ private EC2CloudInstance isReservedInstance(final InstanceConnectionInfo instanc\n \t\t\t\t\t\t}\n \n \t\t\t\t\t\tif (instanceConnectionInfo.getAddress().equals(candidateAddress)) {\n+\n+\t\t\t\t\t\t\tNetworkTopology networkTopology;\n+\t\t\t\t\t\t\tsynchronized (this.networkTopologies) {\n+\t\t\t\t\t\t\t\tnetworkTopology = this.networkTopologies.get(jobID);\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\tif (networkTopology == null) {\n+\t\t\t\t\t\t\t\tthrow new InstanceException(\"Cannot find network topology for job \" + jobID);\n+\t\t\t\t\t\t\t}\n+\n \t\t\t\t\t\t\treturn convertIntoCloudInstance(t, instanceConnectionInfo, mapping.getAwsAccessId(),\n-\t\t\t\t\t\t\t\tmapping.getAwsSecretKey());\n+\t\t\t\t\t\t\t\tmapping.getAwsSecretKey(), networkTopology.getRootNode(), hardwareDescription);\n \t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n@@ -480,8 +499,9 @@ private EC2CloudInstance isReservedInstance(final InstanceConnectionInfo instanc\n \t *        the information required to connect to the instance's task manager later on\n \t * @return a cloud instance\n \t */\n-\tprivate EC2CloudInstance convertIntoCloudInstance(final com.amazonaws.services.ec2.model.Instance instance,\n-\t\t\tfinal InstanceConnectionInfo instanceConnectionInfo, final String awsAccessKey, final String awsSecretKey) {\n+\tprivate EC2CloudInstance convertIntoCloudInstance(final Instance instance,\n+\t\t\tfinal InstanceConnectionInfo instanceConnectionInfo, final String awsAccessKey, final String awsSecretKey,\n+\t\t\tfinal NetworkNode parentNode, final HardwareDescription hardwareDescription) {\n \n \t\tInstanceType type = null;\n \n@@ -501,7 +521,7 @@ private EC2CloudInstance convertIntoCloudInstance(final com.amazonaws.services.e\n \n \t\tfinal EC2CloudInstance cloudInstance = new EC2CloudInstance(instance.getInstanceId(), type,\n \t\t\tinstanceConnectionInfo,\n-\t\t\tinstance.getLaunchTime().getTime(), this.networkTopology.getRootNode(), this.networkTopology, null,\n+\t\t\tinstance.getLaunchTime().getTime(), parentNode, parentNode.getNetworkTopology(), hardwareDescription,\n \t\t\tawsAccessKey, awsSecretKey);\n \n \t\t// TODO: Define hardware descriptions for cloud instance types\n@@ -537,7 +557,7 @@ public void requestInstance(final JobID jobID, Configuration conf,\n \t\t\tthrow new InstanceException(\"Unable to allocate cloud instance: Cannot find AMI image ID\");\n \t\t}\n \n-\t\t// First we check, if there are any orphaned TMs that are accessible with the provided configuration\n+\t\t// First we check, if there are any orphaned instances that are accessible with the provided configuration\n \t\tcheckAndConvertOrphanedInstances(conf);\n \n \t\t// Check if there already exist a mapping for this job\n@@ -553,6 +573,16 @@ public void requestInstance(final JobID jobID, Configuration conf,\n \t\t\t}\n \t\t}\n \n+\t\t// Check if there already exists a network topology for this job\n+\t\tNetworkTopology networkTopology = null;\n+\t\tsynchronized (this.networkTopologies) {\n+\t\t\tnetworkTopology = this.networkTopologies.get(jobID);\n+\t\t\tif (networkTopology == null) {\n+\t\t\t\tnetworkTopology = NetworkTopology.createEmptyTopology();\n+\t\t\t\tthis.networkTopologies.put(jobID, networkTopology);\n+\t\t\t}\n+\t\t}\n+\n \t\t// Our bill with all instances that we will provide...\n \t\tfinal LinkedList<FloatingInstance> floatingInstances = new LinkedList<FloatingInstance>();\n \t\tfinal LinkedList<String> requestedInstances = new LinkedList<String>();\n@@ -611,7 +641,7 @@ public void requestInstance(final JobID jobID, Configuration conf,\n \n \t\t// Convert and allocate Floating Instances...\n \t\tfor (final FloatingInstance fi : floatingInstances) {\n-\t\t\tfinal EC2CloudInstance ci = fi.asCloudInstance();\n+\t\t\tfinal EC2CloudInstance ci = fi.asCloudInstance(networkTopology.getRootNode());\n \t\t\tjobToInstanceMapping.assignInstanceToJob(ci);\n \t\t\tfinal EC2CloudInstanceNotifier notifier = new EC2CloudInstanceNotifier(this.instanceListener, jobID,\n \t\t\t\tci.asAllocatedResource());\n@@ -721,15 +751,15 @@ public void requestInstance(final JobID jobID, Configuration conf,\n \t */\n \tprivate void checkAndConvertOrphanedInstances(final Configuration conf) throws InstanceException {\n \n-\t\tif (this.orphanedTMs.size() == 0) {\n+\t\tif (this.orphanedInstances.size() == 0) {\n \t\t\treturn;\n \t\t}\n \n \t\tfinal String awsAccessId = conf.getString(AWS_ACCESS_ID_KEY, null);\n \t\tfinal String awsSecretKey = conf.getString(AWS_SECRET_KEY_KEY, null);\n \n \t\tif (LOG.isDebugEnabled()) {\n-\t\t\tLOG.debug(\"Checking orphaned instances: \" + this.orphanedTMs.size() + \" orphaned instances listed.\");\n+\t\t\tLOG.debug(\"Checking orphaned instances, \" + this.orphanedInstances.size() + \" orphaned instances listed.\");\n \t\t}\n \t\tfinal AmazonEC2Client ec2client = EC2ClientFactory.getEC2Client(awsAccessId, awsSecretKey);\n \n@@ -766,16 +796,21 @@ private void checkAndConvertOrphanedInstances(final Configuration conf) throws I\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \n-\t\t\t\tfinal Iterator<InstanceConnectionInfo> it = this.orphanedTMs.iterator();\n+\t\t\t\tfinal Iterator<Map.Entry<InstanceConnectionInfo, HardwareDescription>> it = this.orphanedInstances\n+\t\t\t\t\t.entrySet().iterator();\n \n \t\t\t\twhile (it.hasNext()) {\n-\t\t\t\t\tfinal InstanceConnectionInfo oi = it.next();\n+\t\t\t\t\tfinal Map.Entry<InstanceConnectionInfo, HardwareDescription> entry = it.next();\n+\n+\t\t\t\t\tfinal InstanceConnectionInfo oi = entry.getKey();\n+\t\t\t\t\tfinal HardwareDescription hd = entry.getValue();\n+\n \t\t\t\t\tif (oi.getAddress().equals(inetAddress) && type != null) {\n-\t\t\t\t\t\tLOG.info(\"Orphaned Instance \" + oi + \" converted into floating instance.\");\n+\t\t\t\t\t\tLOG.info(\"Orphaned instance \" + oi + \" converted into floating instance.\");\n \n \t\t\t\t\t\t// We have found the corresponding orphaned TM.. convert it back to a floating instance.\n-\t\t\t\t\t\tFloatingInstance floatinginstance = new FloatingInstance(t.getInstanceId(), oi, t\n-\t\t\t\t\t\t\t.getLaunchTime().getTime(), type, awsAccessId, awsSecretKey);\n+\t\t\t\t\t\tfinal FloatingInstance floatinginstance = new FloatingInstance(t.getInstanceId(), oi, t\n+\t\t\t\t\t\t\t.getLaunchTime().getTime(), type, hd, awsAccessId, awsSecretKey);\n \n \t\t\t\t\t\tthis.floatingInstances.put(oi, floatinginstance);\n \t\t\t\t\t\tit.remove();\n@@ -845,14 +880,13 @@ public void run() {\n \t\t\twhile (it.hasNext()) {\n \t\t\t\tfinal Map.Entry<InstanceConnectionInfo, FloatingInstance> entry = it.next();\n \n-\t\t\t\t// Call lifecycle method for each floating instance. If true, remove from floatinginstances list.\n-\t\t\t\tif (entry.getValue().checkIfLifeCycleEnded()) {\n+\t\t\t\t// Call life cycle method for each floating instance. If true, remove from floating instances list.\n+\t\t\t\tif (entry.getValue().hasLifeCycleEnded()) {\n \t\t\t\t\tit.remove();\n-\t\t\t\t\tLOG.info(\"Floating Instance \" + entry.getValue().getInstanceID()\n-\t\t\t\t\t\t+ \" ended its lifecycle. Terminated\");\n+\t\t\t\t\tLOG.info(\"Lifecycle of floating instance \" + entry.getValue().getInstanceID()\n+\t\t\t\t\t\t+ \" has ended, terminating...\");\n \t\t\t\t}\n \t\t\t}\n-\n \t\t}\n \t}\n \n@@ -898,17 +932,19 @@ public void shutdown() {\n \t}\n \n \t@Override\n-\tpublic NetworkTopology getNetworkTopology(JobID jobID) {\n+\tpublic NetworkTopology getNetworkTopology(final JobID jobID) {\n+\n+\t\tsynchronized (this.networkTopologies) {\n \n-\t\t// TODO: Make topology job specific\n-\t\treturn this.networkTopology;\n+\t\t\treturn this.networkTopologies.get(jobID);\n+\t\t}\n \t}\n \n \t/**\n \t * {@inheritDoc}\n \t */\n \t@Override\n-\tpublic void setInstanceListener(InstanceListener instanceListener) {\n+\tpublic void setInstanceListener(final InstanceListener instanceListener) {\n \n \t\tthis.instanceListener = instanceListener;\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/EC2CloudManager.java",
                "sha": "2ed441a1dd54862fd262137ac9d4c13213b1011d",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/flink/blob/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/FloatingInstance.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/FloatingInstance.java?ref=32ad8e5ede160e641201ed4804b820e89a711806",
                "deletions": 13,
                "filename": "nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/FloatingInstance.java",
                "patch": "@@ -20,20 +20,26 @@\n import com.amazonaws.services.ec2.AmazonEC2Client;\n import com.amazonaws.services.ec2.model.TerminateInstancesRequest;\n \n+import eu.stratosphere.nephele.instance.HardwareDescription;\n import eu.stratosphere.nephele.instance.InstanceConnectionInfo;\n import eu.stratosphere.nephele.instance.InstanceType;\n+import eu.stratosphere.nephele.topology.NetworkNode;\n \n /**\n  * A FloatingInstance is an instance in the cloud allocated for a user. It is idle and carries out no task.\n  * However, the owner of a floating instance can employ it for executing new jobs until it is terminated.\n  */\n class FloatingInstance {\n \n-\t/** The user pays fee for his instances every time unit. */\n-\tprivate static final long TIMEUNIT = 60 * 60 * 1000; // 1 hour in ms.\n+\t/**\n+\t * The minimum period of time for which a user has to lease an instance on Amazon EC2.\n+\t */\n+\tprivate static final long LEASE_PERIOD = 60 * 60 * 1000; // 1 hour in ms.\n \n-\t/** Timelimit to full next hour when instance is kicked. */\n-\tprivate static final long TIMETHRESHOLD = 2 * 60 * 1000; // 2 mins in ms.\n+\t/**\n+\t * Time limit to full next hour when instance is terminate.\n+\t **/\n+\tprivate static final long TIME_THRESHOLD = 2 * 60 * 1000; // 2 mins in ms.\n \n \t/** The instance ID. */\n \tprivate final String instanceID;\n@@ -50,9 +56,14 @@\n \t/** The AWS Secret Key to access this machine */\n \tprivate String awsSecretKey;\n \n-\t/** The instance Type */\n+\t/** The instance type */\n \tprivate InstanceType type;\n \n+\t/**\n+\t * The instance's hardware description.\n+\t */\n+\tprivate final HardwareDescription hardwareDescription;\n+\n \t/** The last received heart beat. */\n \tprivate long lastHeartBeat;\n \n@@ -73,14 +84,15 @@\n \t *        The AWS Secret Key to access this machine\n \t */\n \tpublic FloatingInstance(String instanceID, InstanceConnectionInfo instanceConnectionInfo, long launchTime,\n-\t\t\tInstanceType type, String awsAccessKey, String awsSecretKey) {\n+\t\t\tInstanceType type, HardwareDescription hardwareDescription, String awsAccessKey, String awsSecretKey) {\n \t\tthis.instanceID = instanceID;\n \t\tthis.instanceConnectionInfo = instanceConnectionInfo;\n \t\tthis.launchTime = launchTime;\n \t\tthis.lastHeartBeat = System.currentTimeMillis();\n \t\tthis.awsAccessKey = awsAccessKey;\n \t\tthis.awsSecretKey = awsSecretKey;\n \t\tthis.type = type;\n+\t\tthis.hardwareDescription = hardwareDescription;\n \t}\n \n \t/**\n@@ -155,21 +167,22 @@ public long getLaunchTime() {\n \t * \n \t * @return\n \t */\n-\tpublic EC2CloudInstance asCloudInstance() {\n-\t\treturn new EC2CloudInstance(this.instanceID, this.type, this.getInstanceConnectionInfo(), this.launchTime, null,\n-\t\t\tnull, null, this.awsAccessKey, this.awsSecretKey);\n+\tpublic EC2CloudInstance asCloudInstance(final NetworkNode parentNode) {\n+\n+\t\treturn new EC2CloudInstance(this.instanceID, this.type, this.getInstanceConnectionInfo(), this.launchTime,\n+\t\t\tparentNode, parentNode.getNetworkTopology(), this.hardwareDescription, this.awsAccessKey, this.awsSecretKey);\n \t}\n \n \t/**\n-\t * This method checks, if this floating instance has reached the end of its lifecycle and - if so - terminates\n+\t * This method checks if this floating instance has reached the end of its life cycle and, if so, terminates\n \t * itself.\n \t */\n-\tpublic boolean checkIfLifeCycleEnded() {\n+\tpublic boolean hasLifeCycleEnded() {\n \n \t\tfinal long currentTime = System.currentTimeMillis();\n-\t\tfinal long msremaining = TIMEUNIT - ((currentTime - this.launchTime) % TIMEUNIT);\n+\t\tfinal long msremaining = LEASE_PERIOD - ((currentTime - this.launchTime) % LEASE_PERIOD);\n \n-\t\tif (msremaining < TIMETHRESHOLD) {\n+\t\tif (msremaining < TIME_THRESHOLD) {\n \t\t\t// Destroy this instance.\n \t\t\tfinal AmazonEC2Client client = EC2ClientFactory.getEC2Client(this.awsAccessKey, this.awsSecretKey);\n \t\t\tfinal TerminateInstancesRequest tr = new TerminateInstancesRequest();\n@@ -183,4 +196,13 @@ public boolean checkIfLifeCycleEnded() {\n \t\t}\n \t}\n \n+\t/**\n+\t * Returns the hardware description of the floating instance.\n+\t * \n+\t * @return the hardware description of the floating instance\n+\t */\n+\tpublic HardwareDescription getHardwareDescription() {\n+\n+\t\treturn this.hardwareDescription;\n+\t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/main/java/eu/stratosphere/nephele/instance/ec2/FloatingInstance.java",
                "sha": "a36af055199df607493e095d2003f834225af393",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/test/java/eu/stratosphere/nephele/instance/ec2/FloatingInstanceTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele/nephele-ec2cloudmanager/src/test/java/eu/stratosphere/nephele/instance/ec2/FloatingInstanceTest.java?ref=32ad8e5ede160e641201ed4804b820e89a711806",
                "deletions": 1,
                "filename": "nephele/nephele-ec2cloudmanager/src/test/java/eu/stratosphere/nephele/instance/ec2/FloatingInstanceTest.java",
                "patch": "@@ -30,7 +30,7 @@\n \tpublic void testHeartBeat() {\n \n \t\tFloatingInstance fi = new FloatingInstance(\"i-1234ABCD\", new InstanceConnectionInfo(new InetSocketAddress(\n-\t\t\t\"localhost\", 6122).getAddress(), 6122, 6121), System.currentTimeMillis(), null, null, null);\n+\t\t\t\"localhost\", 6122).getAddress(), 6122, 6121), System.currentTimeMillis(), null, null, null, null);\n \n \t\tlong lastHeartBeat = fi.getLastReceivedHeartBeat();\n \t\ttry {",
                "raw_url": "https://github.com/apache/flink/raw/32ad8e5ede160e641201ed4804b820e89a711806/nephele/nephele-ec2cloudmanager/src/test/java/eu/stratosphere/nephele/instance/ec2/FloatingInstanceTest.java",
                "sha": "f1aae3080712bc43e3719985c37ddd7270063954",
                "status": "modified"
            }
        ],
        "message": "Fixed NullPointerException due to missing topology information",
        "parent": "https://github.com/apache/flink/commit/c1ec22c9d31cee59480a6ce11d1e1d80528a5770",
        "patched_files": [
            "FloatingInstance.java",
            "EC2CloudManager.java",
            "EC2CloudInstance.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "FloatingInstanceTest.java"
        ]
    },
    "flink_3a89ab2": {
        "bug_id": "flink_3a89ab2",
        "commit": "https://github.com/apache/flink/commit/3a89ab2ed130a2f75245efae20ec4b925a2b7c19",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/3a89ab2ed130a2f75245efae20ec4b925a2b7c19/flink-connectors/flink-hbase/src/test/resources/log4j-test.properties",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-hbase/src/test/resources/log4j-test.properties?ref=3a89ab2ed130a2f75245efae20ec4b925a2b7c19",
                "deletions": 2,
                "filename": "flink-connectors/flink-hbase/src/test/resources/log4j-test.properties",
                "patch": "@@ -15,9 +15,12 @@\n # specific language governing permissions and limitations\n # under the License.\n \n-log4j.rootLogger=DEBUG, stdout\n+# NOTE: The current Calcite version has a bug that causes execution to fail if the\n+#       log level is set to DEBUG!\n+\n+log4j.rootLogger=OFF, stdout\n log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n log4j.appender.stdout.Target=System.out\n-log4j.appender.stdout.threshold=INFO\n+log4j.appender.stdout.threshold=OFF\n log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n log4j.appender.stdout.layout.ConversionPattern=%d{ABSOLUTE} %-5p %30c{1}:%4L - %m%n",
                "raw_url": "https://github.com/apache/flink/raw/3a89ab2ed130a2f75245efae20ec4b925a2b7c19/flink-connectors/flink-hbase/src/test/resources/log4j-test.properties",
                "sha": "d4d059b5cbf1fba251d7c53e0ebe76a0b6feda7d",
                "status": "modified"
            }
        ],
        "message": "[hotfix] [hbase] Set root log level to OFF for flink-hbase tests.\n\nLog level is changed due to a buggy Calcite check that causes a NPE.\nThe check is only performed if log level DEBUG is enabled.\n\nThis closes #4771",
        "parent": "https://github.com/apache/flink/commit/818cccd3d447ccc4235840fd8bd9a35486ed4f49",
        "patched_files": [],
        "repo": "flink",
        "unit_tests": [
            "log4j-test.java"
        ]
    },
    "flink_3ce8596": {
        "bug_id": "flink_3ce8596",
        "commit": "https://github.com/apache/flink/commit/3ce8596b43f88b2b6d51dab687ab224a43b825fb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/3ce8596b43f88b2b6d51dab687ab224a43b825fb/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java?ref=3ce8596b43f88b2b6d51dab687ab224a43b825fb",
                "deletions": 1,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java",
                "patch": "@@ -69,7 +69,9 @@ protected void run() throws Exception {\n \n \t@Override\n \tprotected void cleanup() throws Exception {\n-\t\tinputProcessor.cleanup();\n+\t\tif (inputProcessor != null) {\n+\t\t\tinputProcessor.cleanup();\n+\t\t}\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/3ce8596b43f88b2b6d51dab687ab224a43b825fb/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OneInputStreamTask.java",
                "sha": "0f8f4a4cf43914ca7a894f6793bbea1abe81737b",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4631] Prevent NPE in OneInputStreamTask\n\nThis closes #2709.",
        "parent": "https://github.com/apache/flink/commit/211f5db9d764efe5318867993f1b33f4eee48117",
        "patched_files": [
            "OneInputStreamTask.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "OneInputStreamTaskTest.java"
        ]
    },
    "flink_4064b5b": {
        "bug_id": "flink_4064b5b",
        "commit": "https://github.com/apache/flink/commit/4064b5b67d6d220e1d5518bca96688f51cbbb891",
        "file": [
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java",
                "patch": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.heartbeat;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+\n+/**\n+ * {@link HeartbeatManager} implementation which does nothing.\n+ *\n+ * @param <I> ignored\n+ * @param <O> ignored\n+ */\n+public class NoOpHeartbeatManager<I, O> implements HeartbeatManager<I, O> {\n+\tprivate static final NoOpHeartbeatManager<Object, Object> INSTANCE = new NoOpHeartbeatManager<>();\n+\n+\tprivate NoOpHeartbeatManager() {}\n+\n+\t@Override\n+\tpublic void monitorTarget(ResourceID resourceID, HeartbeatTarget<O> heartbeatTarget) {}\n+\n+\t@Override\n+\tpublic void unmonitorTarget(ResourceID resourceID) {}\n+\n+\t@Override\n+\tpublic void stop() {}\n+\n+\t@Override\n+\tpublic long getLastHeartbeatFrom(ResourceID resourceId) {\n+\t\treturn 0;\n+\t}\n+\n+\t@Override\n+\tpublic void receiveHeartbeat(ResourceID heartbeatOrigin, I heartbeatPayload) {}\n+\n+\t@Override\n+\tpublic void requestHeartbeat(ResourceID requestOrigin, I heartbeatPayload) {}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic static <A, B> NoOpHeartbeatManager<A, B> getInstance() {\n+\t\treturn (NoOpHeartbeatManager<A, B>) INSTANCE;\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/NoOpHeartbeatManager.java",
                "sha": "965a50b3f3539236a7e706dd51137b6b66ffcc57",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 9,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.flink.runtime.heartbeat.HeartbeatManager;\n import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n import org.apache.flink.runtime.heartbeat.HeartbeatTarget;\n+import org.apache.flink.runtime.heartbeat.NoOpHeartbeatManager;\n import org.apache.flink.runtime.highavailability.HighAvailabilityServices;\n import org.apache.flink.runtime.io.network.partition.PartitionTracker;\n import org.apache.flink.runtime.io.network.partition.PartitionTrackerFactory;\n@@ -269,6 +270,8 @@ public JobMaster(\n \t\tthis.establishedResourceManagerConnection = null;\n \n \t\tthis.accumulators = new HashMap<>();\n+\t\tthis.taskManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n+\t\tthis.resourceManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n \t}\n \n \tprivate SchedulerNG createScheduler(final JobManagerJobMetricGroup jobManagerJobMetricGroup) throws Exception {\n@@ -785,15 +788,8 @@ private Acknowledge suspendExecution(final Exception cause) {\n \t}\n \n \tprivate void stopHeartbeatServices() {\n-\t\tif (taskManagerHeartbeatManager != null) {\n-\t\t\ttaskManagerHeartbeatManager.stop();\n-\t\t\ttaskManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (resourceManagerHeartbeatManager != null) {\n-\t\t\tresourceManagerHeartbeatManager.stop();\n-\t\t\tresourceManagerHeartbeatManager = null;\n-\t\t}\n+\t\ttaskManagerHeartbeatManager.stop();\n+\t\tresourceManagerHeartbeatManager.stop();\n \t}\n \n \tprivate void startHeartbeatServices() {",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "sha": "665c4aa479d08a98efeed8e1c6b3fee49a99cb38",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.flink.runtime.heartbeat.HeartbeatManager;\n import org.apache.flink.runtime.heartbeat.HeartbeatServices;\n import org.apache.flink.runtime.heartbeat.HeartbeatTarget;\n+import org.apache.flink.runtime.heartbeat.NoOpHeartbeatManager;\n import org.apache.flink.runtime.highavailability.HighAvailabilityServices;\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.InstanceID;\n@@ -178,6 +179,9 @@ public ResourceManager(\n \t\tthis.jmResourceIdRegistrations = new HashMap<>(4);\n \t\tthis.taskExecutors = new HashMap<>(8);\n \t\tthis.taskExecutorGatewayFutures = new HashMap<>(8);\n+\n+\t\tthis.jobManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n+\t\tthis.taskManagerHeartbeatManager = NoOpHeartbeatManager.getInstance();\n \t}\n \n \n@@ -972,15 +976,8 @@ private void startHeartbeatServices() {\n \t}\n \n \tprivate void stopHeartbeatServices() {\n-\t\tif (taskManagerHeartbeatManager != null) {\n \t\t\ttaskManagerHeartbeatManager.stop();\n-\t\t\ttaskManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (jobManagerHeartbeatManager != null) {\n \t\t\tjobManagerHeartbeatManager.stop();\n-\t\t\tjobManagerHeartbeatManager = null;\n-\t\t}\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/resourcemanager/ResourceManager.java",
                "sha": "8698e842ac548596330b545a4d74601cfd473046",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/flink/blob/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java?ref=4064b5b67d6d220e1d5518bca96688f51cbbb891",
                "deletions": 36,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java",
                "patch": "@@ -157,8 +157,6 @@\n \t/** The task manager configuration. */\n \tprivate final TaskManagerConfiguration taskManagerConfiguration;\n \n-\tprivate final HeartbeatServices heartbeatServices;\n-\n \t/** The fatal error handler to use in case of a fatal error. */\n \tprivate final FatalErrorHandler fatalErrorHandler;\n \n@@ -207,10 +205,10 @@\n \tprivate FileCache fileCache;\n \n \t/** The heartbeat manager for job manager in the task manager. */\n-\tprivate HeartbeatManager<AllocatedSlotReport, AccumulatorReport> jobManagerHeartbeatManager;\n+\tprivate final HeartbeatManager<AllocatedSlotReport, AccumulatorReport> jobManagerHeartbeatManager;\n \n \t/** The heartbeat manager for resource manager in the task manager. */\n-\tprivate HeartbeatManager<Void, SlotReport> resourceManagerHeartbeatManager;\n+\tprivate final HeartbeatManager<Void, SlotReport> resourceManagerHeartbeatManager;\n \n \tprivate final PartitionTable<JobID> partitionTable;\n \n@@ -249,7 +247,6 @@ public TaskExecutor(\n \t\tcheckArgument(taskManagerConfiguration.getNumberSlots() > 0, \"The number of slots has to be larger than 0.\");\n \n \t\tthis.taskManagerConfiguration = checkNotNull(taskManagerConfiguration);\n-\t\tthis.heartbeatServices = checkNotNull(heartbeatServices);\n \t\tthis.taskExecutorServices = checkNotNull(taskExecutorServices);\n \t\tthis.haServices = checkNotNull(haServices);\n \t\tthis.fatalErrorHandler = checkNotNull(fatalErrorHandler);\n@@ -278,6 +275,26 @@ public TaskExecutor(\n \n \t\tthis.stackTraceSampleService = new StackTraceSampleService(rpcService.getScheduledExecutor());\n \t\tthis.taskCompletionTracker = new TaskCompletionTracker();\n+\n+\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n+\t\tthis.jobManagerHeartbeatManager = createJobManagerHeartbeatManager(heartbeatServices, resourceId);\n+\t\tthis.resourceManagerHeartbeatManager = createResourceManagerHeartbeatManager(heartbeatServices, resourceId);\n+\t}\n+\n+\tprivate HeartbeatManager<Void, SlotReport> createResourceManagerHeartbeatManager(HeartbeatServices heartbeatServices, ResourceID resourceId) {\n+\t\treturn heartbeatServices.createHeartbeatManager(\n+\t\t\tresourceId,\n+\t\t\tnew ResourceManagerHeartbeatListener(),\n+\t\t\tgetMainThreadExecutor(),\n+\t\t\tlog);\n+\t}\n+\n+\tprivate HeartbeatManager<AllocatedSlotReport, AccumulatorReport> createJobManagerHeartbeatManager(HeartbeatServices heartbeatServices, ResourceID resourceId) {\n+\t\treturn heartbeatServices.createHeartbeatManager(\n+\t\t\tresourceId,\n+\t\t\tnew JobManagerHeartbeatListener(),\n+\t\t\tgetMainThreadExecutor(),\n+\t\t\tlog);\n \t}\n \n \t@Override\n@@ -304,8 +321,6 @@ public void onStart() throws Exception {\n \n \tprivate void startTaskExecutorServices() throws Exception {\n \t\ttry {\n-\t\t\tstartHeartbeatServices();\n-\n \t\t\t// start by connecting to the ResourceManager\n \t\t\tresourceManagerLeaderRetriever.start(new ResourceManagerLeaderListener());\n \n@@ -412,38 +427,9 @@ private void stopTaskExecutorServices() throws Exception {\n \t\t// it will call close() recursively from the parent to children\n \t\ttaskManagerMetricGroup.close();\n \n-\t\tstopHeartbeatServices();\n-\n \t\tExceptionUtils.tryRethrowException(exception);\n \t}\n \n-\tprivate void startHeartbeatServices() {\n-\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n-\t\tjobManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n-\t\t\tresourceId,\n-\t\t\tnew JobManagerHeartbeatListener(),\n-\t\t\tgetMainThreadExecutor(),\n-\t\t\tlog);\n-\n-\t\tresourceManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n-\t\t\tresourceId,\n-\t\t\tnew ResourceManagerHeartbeatListener(),\n-\t\t\tgetMainThreadExecutor(),\n-\t\t\tlog);\n-\t}\n-\n-\tprivate void stopHeartbeatServices() {\n-\t\tif (jobManagerHeartbeatManager != null) {\n-\t\t\tjobManagerHeartbeatManager.stop();\n-\t\t\tjobManagerHeartbeatManager = null;\n-\t\t}\n-\n-\t\tif (resourceManagerHeartbeatManager != null) {\n-\t\t\tresourceManagerHeartbeatManager.stop();\n-\t\t\tresourceManagerHeartbeatManager = null;\n-\t\t}\n-\t}\n-\n \t// ======================================================================\n \t//  RPC methods\n \t// ======================================================================",
                "raw_url": "https://github.com/apache/flink/raw/4064b5b67d6d220e1d5518bca96688f51cbbb891/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java",
                "sha": "f0db4cd82b2d371a72c1a12cbf6f650cfbc876e3",
                "status": "modified"
            }
        ],
        "message": "[FLINK-14315] Make heartbeat manager fields non-nullable\n\nThis commit introduces the NoOpHeartbeatManager which can be used to initialize\nan unset heartbeat manager field. This allows to make the heartbeat manager fields\nnon-nullable which in turn avoid NPE.\n\nMoreover, this commit makes the heartbeat manager fields of the TaskExecutor\nfinal.\n\nThis closes #9837.",
        "parent": "https://github.com/apache/flink/commit/c5c59feec0ed8a0ac5213802d79956c815a2b812",
        "patched_files": [
            "TaskExecutor.java",
            "ResourceManager.java",
            "JobMaster.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "TaskExecutorTest.java",
            "JobMasterTest.java",
            "ResourceManagerTest.java"
        ]
    },
    "flink_47db9cb": {
        "bug_id": "flink_47db9cb",
        "commit": "https://github.com/apache/flink/commit/47db9cb1a867870a8da0b403e0ec217ac461ba04",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/flink/blob/47db9cb1a867870a8da0b403e0ec217ac461ba04/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java?ref=47db9cb1a867870a8da0b403e0ec217ac461ba04",
                "deletions": 2,
                "filename": "flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java",
                "patch": "@@ -184,8 +184,13 @@ public boolean delete(final Path f, final boolean recursive) throws IOException\n \t\tfinal File file = pathToFile(f);\n \t\tif (file.isFile()) {\n \t\t\treturn file.delete();\n-\t\t} else if ((!recursive) && file.isDirectory() && (file.listFiles().length != 0)) {\n-\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" is not empty\");\n+\t\t} else if ((!recursive) && file.isDirectory()) {\n+\t\t\tFile[] containedFiles = file.listFiles();\n+\t\t\tif (containedFiles == null) {\n+\t\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" does not exist or an I/O error occurred\");\n+\t\t\t} else if (containedFiles.length != 0) {\n+\t\t\t\tthrow new IOException(\"Directory \" + file.toString() + \" is not empty\");\n+\t\t\t}\n \t\t}\n \n \t\treturn delete(file);",
                "raw_url": "https://github.com/apache/flink/raw/47db9cb1a867870a8da0b403e0ec217ac461ba04/flink-core/src/main/java/org/apache/flink/core/fs/local/LocalFileSystem.java",
                "sha": "7ad68b35d7a5dedc5db6a8dcf212146c233fc481",
                "status": "modified"
            }
        ],
        "message": "[FLINK-5147] Prevent NPE in LocalFS#delete()\n\nThis closes #2859.",
        "parent": "https://github.com/apache/flink/commit/dc5dd5106738e393761a62a56d9e684c722c516f",
        "patched_files": [
            "LocalFileSystem.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "LocalFileSystemTest.java"
        ]
    },
    "flink_4b19e27": {
        "bug_id": "flink_4b19e27",
        "commit": "https://github.com/apache/flink/commit/4b19e272043907b70791bff8a85bd493e212947c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/4b19e272043907b70791bff8a85bd493e212947c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java?ref=4b19e272043907b70791bff8a85bd493e212947c",
                "deletions": 1,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java",
                "patch": "@@ -58,6 +58,8 @@ protected void run() throws Exception {\n \t\n \t@Override\n \tprotected void cancelTask() throws Exception {\n-\t\theadOperator.cancel();\n+\t\tif (headOperator != null) {\n+\t\t\theadOperator.cancel();\n+\t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/4b19e272043907b70791bff8a85bd493e212947c/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java",
                "sha": "18291408d0e35e73db8cdceb731870759b5f7c70",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6182] Fix possible NPE in SourceStreamTask\n\nThis closes #3606.",
        "parent": "https://github.com/apache/flink/commit/11fe3dc89f6b6b24fa21cc51d5e935e91634dbe5",
        "patched_files": [
            "SourceStreamTask.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "SourceStreamTaskTest.java"
        ]
    },
    "flink_5377c77": {
        "bug_id": "flink_5377c77",
        "commit": "https://github.com/apache/flink/commit/5377c772d3c9f54cd2040ca67743cad128999f57",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java?ref=5377c772d3c9f54cd2040ca67743cad128999f57",
                "deletions": 1,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java",
                "patch": "@@ -375,6 +375,8 @@ public void invoke(IN value, SinkFunction.Context context) throws Exception {\n \n \t@Override\n \tpublic void close() throws Exception {\n-\t\tbuckets.close();\n+\t\tif (buckets != null) {\n+\t\t\tbuckets.close();\n+\t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java",
                "sha": "6f57fee81f3030e831a5f8c6a1b14d26477724cb",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/flink/blob/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java?ref=5377c772d3c9f54cd2040ca67743cad128999f57",
                "deletions": 0,
                "filename": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java",
                "patch": "@@ -59,6 +59,16 @@ public void testClosingWithoutInput() throws Exception {\n \t\t}\n \t}\n \n+\t@Test\n+\tpublic void testClosingWithoutInitializingStateShouldNotFail() throws Exception {\n+\t\tfinal File outDir = TEMP_FOLDER.newFolder();\n+\n+\t\ttry (OneInputStreamOperatorTestHarness<Tuple2<String, Integer>, Object> testHarness =\n+\t\t\t\t\tTestUtils.createRescalingTestSink(outDir, 1, 0, 100L, 124L)) {\n+\t\t\ttestHarness.setup();\n+\t\t}\n+\t}\n+\n \t@Test\n \tpublic void testTruncateAfterRecoveryAndOverwrite() throws Exception {\n \t\tfinal File outDir = TEMP_FOLDER.newFolder();",
                "raw_url": "https://github.com/apache/flink/raw/5377c772d3c9f54cd2040ca67743cad128999f57/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/LocalStreamingFileSinkTest.java",
                "sha": "8bb35ff244a449c4d825aa9314a8e21c83ce2d1b",
                "status": "modified"
            }
        ],
        "message": "[FLINK-10663][streaming] Fix NPE when StreamingFileSink is closed without initialization. (#6915)",
        "parent": "https://github.com/apache/flink/commit/1906a1ee3f02ebe829837515f5eb065981421a37",
        "patched_files": [
            "StreamingFileSink.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "LocalStreamingFileSinkTest.java"
        ]
    },
    "flink_56017a9": {
        "bug_id": "flink_56017a9",
        "commit": "https://github.com/apache/flink/commit/56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/docs/monitoring/metrics.md",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/monitoring/metrics.md?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 3,
                "filename": "docs/monitoring/metrics.md",
                "patch": "@@ -424,7 +424,7 @@ of your Flink distribution.\n \n Parameters:\n \n-- `port` - (optional) the port the Prometheus exporter listens on, defaults to [9249](https://github.com/prometheus/prometheus/wiki/Default-port-allocations).\n+- `port` - (optional) the port the Prometheus exporter listens on, defaults to [9249](https://github.com/prometheus/prometheus/wiki/Default-port-allocations). In order to be able to run several instances of the reporter on one host (e.g. when one TaskManager is colocated with the JobManager) it is advisable to use a port range like `9250-9260`.\n \n Example configuration:\n \n@@ -440,11 +440,11 @@ Flink metric types are mapped to Prometheus metric types as follows:\n | Flink     | Prometheus | Note                                     |\n | --------- |------------|------------------------------------------|\n | Counter   | Gauge      |Prometheus counters cannot be decremented.|\n-| Gauge     | Gauge      |                                          |\n+| Gauge     | Gauge      |Only numbers and booleans are supported.  |\n | Histogram | Summary    |Quantiles .5, .75, .95, .98, .99 and .999 |\n | Meter     | Gauge      |The gauge exports the meter's rate.       |\n \n-All Flink metrics variables, such as `<host>`, `<job_name>`, `<tm_id>`, `<subtask_index>`, `<task_name>` and `<operator_name>`, are exported to Prometheus as labels. \n+All Flink metrics variables (see [List of all Variables](#list-of-all-variables)) are exported to Prometheus as labels. \n \n ### StatsD (org.apache.flink.metrics.statsd.StatsDReporter)\n ",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/docs/monitoring/metrics.md",
                "sha": "64d7318dab3d625343015d8550be1e93686cfea4",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/pom.xml",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/pom.xml?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 11,
                "filename": "flink-metrics/flink-metrics-prometheus/pom.xml",
                "patch": "@@ -40,6 +40,13 @@ under the License.\n \t\t\t<scope>provided</scope>\n \t\t</dependency>\n \n+\t\t<dependency>\n+\t\t\t<groupId>org.apache.flink</groupId>\n+\t\t\t<artifactId>flink-core</artifactId>\n+\t\t\t<version>${project.version}</version>\n+\t\t\t<scope>provided</scope>\n+\t\t</dependency>\n+\n \t\t<dependency>\n \t\t\t<groupId>org.apache.flink</groupId>\n \t\t\t<artifactId>flink-runtime_${scala.binary.version}</artifactId>\n@@ -62,16 +69,10 @@ under the License.\n \n \t\t<dependency>\n \t\t\t<groupId>io.prometheus</groupId>\n-\t\t\t<artifactId>simpleclient_servlet</artifactId>\n+\t\t\t<artifactId>simpleclient_httpserver</artifactId>\n \t\t\t<version>${prometheus.version}</version>\n \t\t</dependency>\n \n-\t\t<dependency>\n-\t\t\t<groupId>org.nanohttpd</groupId>\n-\t\t\t<artifactId>nanohttpd</artifactId>\n-\t\t\t<version>2.2.0</version>\n-\t\t</dependency>\n-\n \t\t<!-- test dependencies -->\n \n \t\t<dependency>\n@@ -114,10 +115,6 @@ under the License.\n \t\t\t\t\t\t\t\t\t<pattern>io.prometheus.client</pattern>\n \t\t\t\t\t\t\t\t\t<shadedPattern>org.apache.flink.shaded.io.prometheus.client</shadedPattern>\n \t\t\t\t\t\t\t\t</relocation>\n-\t\t\t\t\t\t\t\t<relocation>\n-\t\t\t\t\t\t\t\t\t<pattern>fi.iki.elonen</pattern>\n-\t\t\t\t\t\t\t\t\t<shadedPattern>org.apache.flink.shaded.fi.iki.elonen</shadedPattern>\n-\t\t\t\t\t\t\t\t</relocation>\n \t\t\t\t\t\t\t</relocations>\n \t\t\t\t\t\t</configuration>\n \t\t\t\t\t</execution>",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/pom.xml",
                "sha": "0e9b2611db6dec8eb47aae4441a3df09c43b5424",
                "status": "modified"
            },
            {
                "additions": 134,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java",
                "changes": 229,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 95,
                "filename": "flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java",
                "patch": "@@ -24,28 +24,28 @@\n import org.apache.flink.metrics.Counter;\n import org.apache.flink.metrics.Gauge;\n import org.apache.flink.metrics.Histogram;\n-import org.apache.flink.metrics.HistogramStatistics;\n import org.apache.flink.metrics.Meter;\n import org.apache.flink.metrics.Metric;\n import org.apache.flink.metrics.MetricConfig;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.flink.metrics.reporter.MetricReporter;\n import org.apache.flink.runtime.metrics.groups.AbstractMetricGroup;\n import org.apache.flink.runtime.metrics.groups.FrontMetricGroup;\n+import org.apache.flink.util.NetUtils;\n \n-import fi.iki.elonen.NanoHTTPD;\n import io.prometheus.client.Collector;\n import io.prometheus.client.CollectorRegistry;\n-import io.prometheus.client.exporter.common.TextFormat;\n+import io.prometheus.client.exporter.HTTPServer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import java.io.IOException;\n-import java.io.StringWriter;\n+import java.util.AbstractMap;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.HashMap;\n+import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -59,7 +59,7 @@\n \tprivate static final Logger LOG = LoggerFactory.getLogger(PrometheusReporter.class);\n \n \tstatic final String ARG_PORT = \"port\";\n-\tprivate static final int DEFAULT_PORT = 9249;\n+\tprivate static final String DEFAULT_PORT = \"9249\";\n \n \tprivate static final Pattern UNALLOWED_CHAR_PATTERN = Pattern.compile(\"[^a-zA-Z0-9:_]\");\n \tprivate static final CharacterFilter CHARACTER_FILTER = new CharacterFilter() {\n@@ -72,8 +72,8 @@ public String filterCharacters(String input) {\n \tprivate static final char SCOPE_SEPARATOR = '_';\n \tprivate static final String SCOPE_PREFIX = \"flink\" + SCOPE_SEPARATOR;\n \n-\tprivate PrometheusEndpoint prometheusEndpoint;\n-\tprivate final Map<String, Collector> collectorsByMetricName = new HashMap<>();\n+\tprivate HTTPServer httpServer;\n+\tprivate final Map<String, AbstractMap.SimpleImmutableEntry<Collector, Integer>> collectorsWithCountByMetricName = new HashMap<>();\n \n \t@VisibleForTesting\n \tstatic String replaceInvalidChars(final String input) {\n@@ -84,27 +84,34 @@ static String replaceInvalidChars(final String input) {\n \n \t@Override\n \tpublic void open(MetricConfig config) {\n-\t\tint port = config.getInteger(ARG_PORT, DEFAULT_PORT);\n-\t\tLOG.info(\"Using port {}.\", port);\n-\t\tprometheusEndpoint = new PrometheusEndpoint(port);\n-\t\ttry {\n-\t\t\tprometheusEndpoint.start(NanoHTTPD.SOCKET_READ_TIMEOUT, true);\n-\t\t} catch (IOException e) {\n-\t\t\tfinal String msg = \"Could not start PrometheusEndpoint on port \" + port;\n-\t\t\tLOG.warn(msg, e);\n-\t\t\tthrow new RuntimeException(msg, e);\n+\t\tString portsConfig = config.getString(ARG_PORT, DEFAULT_PORT);\n+\t\tIterator<Integer> ports = NetUtils.getPortRangeFromString(portsConfig);\n+\n+\t\twhile (ports.hasNext()) {\n+\t\t\tint port = ports.next();\n+\t\t\ttry {\n+\t\t\t\thttpServer = new HTTPServer(port);\n+\t\t\t\tLOG.info(\"Started PrometheusReporter HTTP server on port {}.\", port);\n+\t\t\t\tbreak;\n+\t\t\t} catch (IOException ioe) { //assume port conflict\n+\t\t\t\tLOG.debug(\"Could not start PrometheusReporter HTTP server on port {}.\", port, ioe);\n+\t\t\t}\n+\t\t}\n+\t\tif (httpServer == null) {\n+\t\t\tthrow new RuntimeException(\"Could not start PrometheusReporter HTTP server on any configured port. Ports: \" + portsConfig);\n \t\t}\n \t}\n \n \t@Override\n \tpublic void close() {\n-\t\tprometheusEndpoint.stop();\n+\t\tif (httpServer != null) {\n+\t\t\thttpServer.stop();\n+\t\t}\n \t\tCollectorRegistry.defaultRegistry.clear();\n \t}\n \n \t@Override\n \tpublic void notifyOfAddedMetric(final Metric metric, final String metricName, final MetricGroup group) {\n-\t\tfinal String scope = SCOPE_PREFIX + getLogicalScope(group);\n \n \t\tList<String> dimensionKeys = new LinkedList<>();\n \t\tList<String> dimensionValues = new LinkedList<>();\n@@ -114,146 +121,178 @@ public void notifyOfAddedMetric(final Metric metric, final String metricName, fi\n \t\t\tdimensionValues.add(CHARACTER_FILTER.filterCharacters(dimension.getValue()));\n \t\t}\n \n-\t\tfinal String validMetricName = scope + SCOPE_SEPARATOR + CHARACTER_FILTER.filterCharacters(metricName);\n-\t\tfinal String metricIdentifier = group.getMetricIdentifier(metricName);\n+\t\tfinal String scopedMetricName = getScopedName(metricName, group);\n+\t\tfinal String helpString = metricName + \" (scope: \" + getLogicalScope(group) + \")\";\n+\n \t\tfinal Collector collector;\n+\t\tInteger count = 0;\n+\n+\t\tsynchronized (this) {\n+\t\t\tif (collectorsWithCountByMetricName.containsKey(scopedMetricName)) {\n+\t\t\t\tfinal AbstractMap.SimpleImmutableEntry<Collector, Integer> collectorWithCount = collectorsWithCountByMetricName.get(scopedMetricName);\n+\t\t\t\tcollector = collectorWithCount.getKey();\n+\t\t\t\tcount = collectorWithCount.getValue();\n+\t\t\t} else {\n+\t\t\t\tcollector = createCollector(metric, dimensionKeys, dimensionValues, scopedMetricName, helpString);\n+\t\t\t\ttry {\n+\t\t\t\t\tcollector.register();\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tLOG.warn(\"There was a problem registering metric {}.\", metricName, e);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\taddMetric(metric, dimensionValues, collector);\n+\t\t\tcollectorsWithCountByMetricName.put(scopedMetricName, new AbstractMap.SimpleImmutableEntry<>(collector, count + 1));\n+\t\t}\n+\t}\n+\n+\tprivate static String getScopedName(String metricName, MetricGroup group) {\n+\t\treturn SCOPE_PREFIX + getLogicalScope(group) + SCOPE_SEPARATOR + CHARACTER_FILTER.filterCharacters(metricName);\n+\t}\n+\n+\tprivate static Collector createCollector(Metric metric, List<String> dimensionKeys, List<String> dimensionValues, String scopedMetricName, String helpString) {\n+\t\tCollector collector;\n+\t\tif (metric instanceof Gauge || metric instanceof Counter || metric instanceof Meter) {\n+\t\t\tcollector = io.prometheus.client.Gauge\n+\t\t\t\t.build()\n+\t\t\t\t.name(scopedMetricName)\n+\t\t\t\t.help(helpString)\n+\t\t\t\t.labelNames(toArray(dimensionKeys))\n+\t\t\t\t.create();\n+\t\t} else if (metric instanceof Histogram) {\n+\t\t\tcollector = new HistogramSummaryProxy((Histogram) metric, scopedMetricName, helpString, dimensionKeys, dimensionValues);\n+\t\t} else {\n+\t\t\tLOG.warn(\"Cannot create collector for unknown metric type: {}. This indicates that the metric type is not supported by this reporter.\",\n+\t\t\t\tmetric.getClass().getName());\n+\t\t\tcollector = null;\n+\t\t}\n+\t\treturn collector;\n+\t}\n+\n+\tprivate static void addMetric(Metric metric, List<String> dimensionValues, Collector collector) {\n \t\tif (metric instanceof Gauge) {\n-\t\t\tcollector = createGauge((Gauge) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Gauge) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Counter) {\n-\t\t\tcollector = createGauge((Counter) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Counter) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Meter) {\n-\t\t\tcollector = createGauge((Meter) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((io.prometheus.client.Gauge) collector).setChild(gaugeFrom((Meter) metric), toArray(dimensionValues));\n \t\t} else if (metric instanceof Histogram) {\n-\t\t\tcollector = createSummary((Histogram) metric, validMetricName, metricIdentifier, dimensionKeys, dimensionValues);\n+\t\t\t((HistogramSummaryProxy) collector).addChild((Histogram) metric, dimensionValues);\n \t\t} else {\n \t\t\tLOG.warn(\"Cannot add unknown metric type: {}. This indicates that the metric type is not supported by this reporter.\",\n \t\t\t\tmetric.getClass().getName());\n-\t\t\treturn;\n \t\t}\n-\t\tcollector.register();\n-\t\tcollectorsByMetricName.put(metricName, collector);\n \t}\n \n \t@Override\n \tpublic void notifyOfRemovedMetric(final Metric metric, final String metricName, final MetricGroup group) {\n-\t\tCollectorRegistry.defaultRegistry.unregister(collectorsByMetricName.get(metricName));\n-\t\tcollectorsByMetricName.remove(metricName);\n+\t\tfinal String scopedMetricName = getScopedName(metricName, group);\n+\t\tsynchronized (this) {\n+\t\t\tfinal AbstractMap.SimpleImmutableEntry<Collector, Integer> collectorWithCount = collectorsWithCountByMetricName.get(scopedMetricName);\n+\t\t\tfinal Integer count = collectorWithCount.getValue();\n+\t\t\tfinal Collector collector = collectorWithCount.getKey();\n+\t\t\tif (count == 1) {\n+\t\t\t\ttry {\n+\t\t\t\t\tCollectorRegistry.defaultRegistry.unregister(collector);\n+\t\t\t\t} catch (Exception e) {\n+\t\t\t\t\tLOG.warn(\"There was a problem unregistering metric {}.\", scopedMetricName, e);\n+\t\t\t\t}\n+\t\t\t\tcollectorsWithCountByMetricName.remove(scopedMetricName);\n+\t\t\t} else {\n+\t\t\t\tcollectorsWithCountByMetricName.put(scopedMetricName, new AbstractMap.SimpleImmutableEntry<>(collector, count - 1));\n+\t\t\t}\n+\t\t}\n \t}\n \n \t@SuppressWarnings(\"unchecked\")\n \tprivate static String getLogicalScope(MetricGroup group) {\n \t\treturn ((FrontMetricGroup<AbstractMetricGroup<?>>) group).getLogicalScope(CHARACTER_FILTER, SCOPE_SEPARATOR);\n \t}\n \n-\tprivate Collector createGauge(final Gauge gauge, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\t@VisibleForTesting\n+\tstatic io.prometheus.client.Gauge.Child gaugeFrom(Gauge gauge) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\tfinal Object value = gauge.getValue();\n+\t\t\t\tif (value == null) {\n+\t\t\t\t\tLOG.debug(\"Gauge {} is null-valued, defaulting to 0.\", gauge);\n+\t\t\t\t\treturn 0;\n+\t\t\t\t}\n \t\t\t\tif (value instanceof Double) {\n \t\t\t\t\treturn (double) value;\n \t\t\t\t}\n \t\t\t\tif (value instanceof Number) {\n \t\t\t\t\treturn ((Number) value).doubleValue();\n-\t\t\t\t} else if (value instanceof Boolean) {\n+\t\t\t\t}\n+\t\t\t\tif (value instanceof Boolean) {\n \t\t\t\t\treturn ((Boolean) value) ? 1 : 0;\n-\t\t\t\t} else {\n-\t\t\t\t\tLOG.debug(\"Invalid type for Gauge {}: {}, only number types and booleans are supported by this reporter.\",\n-\t\t\t\t\t\tgauge, value.getClass().getName());\n-\t\t\t\t\treturn 0;\n \t\t\t\t}\n+\t\t\t\tLOG.debug(\"Invalid type for Gauge {}: {}, only number types and booleans are supported by this reporter.\",\n+\t\t\t\t\tgauge, value.getClass().getName());\n+\t\t\t\treturn 0;\n \t\t\t}\n-\t\t});\n+\t\t};\n \t}\n \n-\tprivate static Collector createGauge(final Counter counter, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\tprivate static io.prometheus.client.Gauge.Child gaugeFrom(Counter counter) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\treturn (double) counter.getCount();\n \t\t\t}\n-\t\t});\n+\t\t};\n \t}\n \n-\tprivate Collector createGauge(final Meter meter, final String name, final String identifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\treturn newGauge(name, identifier, labelNames, labelValues, new io.prometheus.client.Gauge.Child() {\n+\tprivate static io.prometheus.client.Gauge.Child gaugeFrom(Meter meter) {\n+\t\treturn new io.prometheus.client.Gauge.Child() {\n \t\t\t@Override\n \t\t\tpublic double get() {\n \t\t\t\treturn meter.getRate();\n \t\t\t}\n-\t\t});\n-\t}\n-\n-\tprivate static Collector newGauge(String name, String identifier, List<String> labelNames, List<String> labelValues, io.prometheus.client.Gauge.Child child) {\n-\t\treturn io.prometheus.client.Gauge\n-\t\t\t.build()\n-\t\t\t.name(name)\n-\t\t\t.help(identifier)\n-\t\t\t.labelNames(toArray(labelNames))\n-\t\t\t.create()\n-\t\t\t.setChild(child, toArray(labelValues));\n-\t}\n-\n-\tprivate static HistogramSummaryProxy createSummary(final Histogram histogram, final String name, final String identifier, final List<String> dimensionKeys, final List<String> dimensionValues) {\n-\t\treturn new HistogramSummaryProxy(histogram, name, identifier, dimensionKeys, dimensionValues);\n-\t}\n-\n-\tstatic class PrometheusEndpoint extends NanoHTTPD {\n-\t\tstatic final String MIME_TYPE = \"plain/text\";\n-\n-\t\tPrometheusEndpoint(int port) {\n-\t\t\tsuper(port);\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic Response serve(IHTTPSession session) {\n-\t\t\tif (session.getUri().equals(\"/metrics\")) {\n-\t\t\t\tStringWriter writer = new StringWriter();\n-\t\t\t\ttry {\n-\t\t\t\t\tTextFormat.write004(writer, CollectorRegistry.defaultRegistry.metricFamilySamples());\n-\t\t\t\t} catch (IOException e) {\n-\t\t\t\t\treturn newFixedLengthResponse(Response.Status.INTERNAL_ERROR, MIME_TYPE, \"Unable to output metrics\");\n-\t\t\t\t}\n-\t\t\t\treturn newFixedLengthResponse(Response.Status.OK, TextFormat.CONTENT_TYPE_004, writer.toString());\n-\t\t\t} else {\n-\t\t\t\treturn newFixedLengthResponse(Response.Status.NOT_FOUND, MIME_TYPE, \"Not found\");\n-\t\t\t}\n-\t\t}\n+\t\t};\n \t}\n \n-\tprivate static class HistogramSummaryProxy extends Collector {\n-\t\tprivate static final List<Double> QUANTILES = Arrays.asList(.5, .75, .95, .98, .99, .999);\n+\t@VisibleForTesting\n+\tstatic class HistogramSummaryProxy extends Collector {\n+\t\tstatic final List<Double> QUANTILES = Arrays.asList(.5, .75, .95, .98, .99, .999);\n \n-\t\tprivate final Histogram histogram;\n \t\tprivate final String metricName;\n-\t\tprivate final String metricIdentifier;\n+\t\tprivate final String helpString;\n \t\tprivate final List<String> labelNamesWithQuantile;\n-\t\tprivate final List<String> labelValues;\n \n-\t\tHistogramSummaryProxy(final Histogram histogram, final String metricName, final String metricIdentifier, final List<String> labelNames, final List<String> labelValues) {\n-\t\t\tthis.histogram = histogram;\n+\t\tprivate final Map<List<String>, Histogram> histogramsByLabelValues = new HashMap<>();\n+\n+\t\tHistogramSummaryProxy(final Histogram histogram, final String metricName, final String helpString, final List<String> labelNames, final List<String> labelValues) {\n \t\t\tthis.metricName = metricName;\n-\t\t\tthis.metricIdentifier = metricIdentifier;\n+\t\t\tthis.helpString = helpString;\n \t\t\tthis.labelNamesWithQuantile = addToList(labelNames, \"quantile\");\n-\t\t\tthis.labelValues = labelValues;\n+\t\t\thistogramsByLabelValues.put(labelValues, histogram);\n \t\t}\n \n \t\t@Override\n \t\tpublic List<MetricFamilySamples> collect() {\n \t\t\t// We cannot use SummaryMetricFamily because it is impossible to get a sum of all values (at least for Dropwizard histograms,\n \t\t\t// whose snapshot's values array only holds a sample of recent values).\n \n-\t\t\tfinal HistogramStatistics statistics = histogram.getStatistics();\n-\n \t\t\tList<MetricFamilySamples.Sample> samples = new LinkedList<>();\n+\t\t\tfor (Map.Entry<List<String>, Histogram> labelValuesToHistogram : histogramsByLabelValues.entrySet()) {\n+\t\t\t\taddSamples(labelValuesToHistogram.getKey(), labelValuesToHistogram.getValue(), samples);\n+\t\t\t}\n+\t\t\treturn Collections.singletonList(new MetricFamilySamples(metricName, Type.SUMMARY, helpString, samples));\n+\t\t}\n+\n+\t\tvoid addChild(final Histogram histogram, final List<String> labelValues) {\n+\t\t\thistogramsByLabelValues.put(labelValues, histogram);\n+\t\t}\n+\n+\t\tprivate void addSamples(final List<String> labelValues, final Histogram histogram, final List<MetricFamilySamples.Sample> samples) {\n \t\t\tsamples.add(new MetricFamilySamples.Sample(metricName + \"_count\",\n \t\t\t\tlabelNamesWithQuantile.subList(0, labelNamesWithQuantile.size() - 1), labelValues, histogram.getCount()));\n \t\t\tfor (final Double quantile : QUANTILES) {\n \t\t\t\tsamples.add(new MetricFamilySamples.Sample(metricName, labelNamesWithQuantile,\n \t\t\t\t\taddToList(labelValues, quantile.toString()),\n-\t\t\t\t\tstatistics.getQuantile(quantile)));\n+\t\t\t\t\thistogram.getStatistics().getQuantile(quantile)));\n \t\t\t}\n-\t\t\treturn Collections.singletonList(new MetricFamilySamples(metricName, Type.SUMMARY, metricIdentifier, samples));\n \t\t}\n \t}\n \n@@ -263,7 +302,7 @@ public Response serve(IHTTPSession session) {\n \t\treturn result;\n \t}\n \n-\tprivate static String[] toArray(List<String> labelNames) {\n-\t\treturn labelNames.toArray(new String[labelNames.size()]);\n+\tprivate static String[] toArray(List<String> list) {\n+\t\treturn list.toArray(new String[list.size()]);\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/main/java/org/apache/flink/metrics/prometheus/PrometheusReporter.java",
                "sha": "1e44ab966cf8478eb36dc2eb303532730986fbc1",
                "status": "modified"
            },
            {
                "additions": 188,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java",
                "changes": 188,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 0,
                "filename": "flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java",
                "patch": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.metrics.prometheus;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.metrics.Counter;\n+import org.apache.flink.metrics.Gauge;\n+import org.apache.flink.metrics.Histogram;\n+import org.apache.flink.metrics.Meter;\n+import org.apache.flink.metrics.SimpleCounter;\n+import org.apache.flink.metrics.util.TestMeter;\n+import org.apache.flink.runtime.jobgraph.JobVertexID;\n+import org.apache.flink.runtime.metrics.MetricRegistry;\n+import org.apache.flink.runtime.metrics.MetricRegistryConfiguration;\n+import org.apache.flink.runtime.metrics.groups.TaskManagerJobMetricGroup;\n+import org.apache.flink.runtime.metrics.groups.TaskManagerMetricGroup;\n+import org.apache.flink.runtime.metrics.groups.TaskMetricGroup;\n+import org.apache.flink.runtime.metrics.util.TestingHistogram;\n+import org.apache.flink.util.AbstractID;\n+\n+import com.mashape.unirest.http.exceptions.UnirestException;\n+import io.prometheus.client.CollectorRegistry;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+\n+import static org.apache.flink.metrics.prometheus.PrometheusReporterTest.createConfigWithOneReporter;\n+import static org.apache.flink.metrics.prometheus.PrometheusReporterTest.pollMetrics;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.nullValue;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Test for {@link PrometheusReporter} that registers several instances of the same metric for different subtasks.\n+ */\n+public class PrometheusReporterTaskScopeTest {\n+\tprivate static final String[] LABEL_NAMES = {\"job_id\", \"task_id\", \"task_attempt_id\", \"host\", \"task_name\", \"task_attempt_num\", \"job_name\", \"tm_id\", \"subtask_index\"};\n+\n+\tprivate static final String TASK_MANAGER_HOST = \"taskManagerHostName\";\n+\tprivate static final String TASK_MANAGER_ID = \"taskManagerId\";\n+\tprivate static final String JOB_NAME = \"jobName\";\n+\tprivate static final String TASK_NAME = \"taskName\";\n+\tprivate static final int ATTEMPT_NUMBER = 0;\n+\tprivate static final int SUBTASK_INDEX_1 = 0;\n+\tprivate static final int SUBTASK_INDEX_2 = 1;\n+\n+\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9429\")));\n+\n+\tprivate final JobID jobId = new JobID();\n+\tprivate final JobVertexID taskId1 = new JobVertexID();\n+\tprivate final AbstractID taskAttemptId1 = new AbstractID();\n+\tprivate final String[] labelValues1 = {jobId.toString(), taskId1.toString(), taskAttemptId1.toString(), TASK_MANAGER_HOST, TASK_NAME, \"\" + ATTEMPT_NUMBER, JOB_NAME, TASK_MANAGER_ID, \"\" + SUBTASK_INDEX_1};\n+\tprivate final JobVertexID taskId2 = new JobVertexID();\n+\tprivate final AbstractID taskAttemptId2 = new AbstractID();\n+\tprivate final String[] labelValues2 = {jobId.toString(), taskId2.toString(), taskAttemptId2.toString(), TASK_MANAGER_HOST, TASK_NAME, \"\" + ATTEMPT_NUMBER, JOB_NAME, TASK_MANAGER_ID, \"\" + SUBTASK_INDEX_2};\n+\n+\tprivate final TaskManagerMetricGroup tmMetricGroup = new TaskManagerMetricGroup(registry, TASK_MANAGER_HOST, TASK_MANAGER_ID);\n+\tprivate final TaskManagerJobMetricGroup tmJobMetricGroup = new TaskManagerJobMetricGroup(registry, tmMetricGroup, jobId, JOB_NAME);\n+\tprivate final TaskMetricGroup taskMetricGroup1 = new TaskMetricGroup(registry, tmJobMetricGroup, taskId1, taskAttemptId1, TASK_NAME, SUBTASK_INDEX_1, ATTEMPT_NUMBER);\n+\tprivate final TaskMetricGroup taskMetricGroup2 = new TaskMetricGroup(registry, tmJobMetricGroup, taskId2, taskAttemptId2, TASK_NAME, SUBTASK_INDEX_2, ATTEMPT_NUMBER);\n+\n+\t@Test\n+\tpublic void countersCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tCounter counter1 = new SimpleCounter();\n+\t\tcounter1.inc(1);\n+\t\tCounter counter2 = new SimpleCounter();\n+\t\tcounter2.inc(2);\n+\n+\t\ttaskMetricGroup1.counter(\"my_counter\", counter1);\n+\t\ttaskMetricGroup2.counter(\"my_counter\", counter2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(2.));\n+\t}\n+\n+\t@Test\n+\tpublic void gaugesCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tGauge<Integer> gauge1 = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn 3;\n+\t\t\t}\n+\t\t};\n+\t\tGauge<Integer> gauge2 = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn 4;\n+\t\t\t}\n+\t\t};\n+\n+\t\ttaskMetricGroup1.gauge(\"my_gauge\", gauge1);\n+\t\ttaskMetricGroup2.gauge(\"my_gauge\", gauge2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_gauge\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(3.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_gauge\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(4.));\n+\t}\n+\n+\t@Test\n+\tpublic void metersCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tMeter meter = new TestMeter();\n+\n+\t\ttaskMetricGroup1.meter(\"my_meter\", meter);\n+\t\ttaskMetricGroup2.meter(\"my_meter\", meter);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_meter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(5.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_meter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(5.));\n+\t}\n+\n+\t@Test\n+\tpublic void histogramsCanBeAddedSeveralTimesIfTheyDifferInLabels() throws UnirestException {\n+\t\tHistogram histogram = new TestingHistogram();\n+\n+\t\ttaskMetricGroup1.histogram(\"my_histogram\", histogram);\n+\t\ttaskMetricGroup2.histogram(\"my_histogram\", histogram);\n+\n+\t\tfinal String exportedMetrics = pollMetrics().getBody();\n+\t\tassertThat(exportedMetrics, containsString(\"subtask_index=\\\"0\\\",quantile=\\\"0.5\\\",} 0.5\")); // histogram\n+\t\tassertThat(exportedMetrics, containsString(\"subtask_index=\\\"1\\\",quantile=\\\"0.5\\\",} 0.5\")); // histogram\n+\n+\t\tfinal String[] labelNamesWithQuantile = addToArray(LABEL_NAMES, \"quantile\");\n+\t\tfor (Double quantile : PrometheusReporter.HistogramSummaryProxy.QUANTILES) {\n+\t\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_histogram\", labelNamesWithQuantile, addToArray(labelValues1, \"\" + quantile)),\n+\t\t\t\tequalTo(quantile));\n+\t\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_histogram\", labelNamesWithQuantile, addToArray(labelValues2, \"\" + quantile)),\n+\t\t\t\tequalTo(quantile));\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void removingSingleInstanceOfMetricDoesNotBreakOtherInstances() throws UnirestException {\n+\t\tCounter counter1 = new SimpleCounter();\n+\t\tcounter1.inc(1);\n+\t\tCounter counter2 = new SimpleCounter();\n+\t\tcounter2.inc(2);\n+\n+\t\ttaskMetricGroup1.counter(\"my_counter\", counter1);\n+\t\ttaskMetricGroup2.counter(\"my_counter\", counter2);\n+\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues2),\n+\t\t\tequalTo(2.));\n+\n+\t\ttaskMetricGroup2.close();\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tequalTo(1.));\n+\n+\t\ttaskMetricGroup1.close();\n+\t\tassertThat(CollectorRegistry.defaultRegistry.getSampleValue(\"flink_taskmanager_job_task_my_counter\", LABEL_NAMES, labelValues1),\n+\t\t\tnullValue());\n+\t}\n+\n+\tprivate String[] addToArray(String[] array, String element) {\n+\t\tfinal String[] labelNames = Arrays.copyOf(array, LABEL_NAMES.length + 1);\n+\t\tlabelNames[LABEL_NAMES.length] = element;\n+\t\treturn labelNames;\n+\t}\n+\n+\t@After\n+\tpublic void shutdownRegistry() {\n+\t\tregistry.shutdown();\n+\t}\n+\n+}",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTaskScopeTest.java",
                "sha": "c7d4040734ec7451e19a17649c2372d18bff3bfb",
                "status": "added"
            },
            {
                "additions": 143,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java",
                "changes": 185,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 42,
                "filename": "flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java",
                "patch": "@@ -47,13 +47,13 @@\n import java.util.Arrays;\n \n import static org.apache.flink.metrics.prometheus.PrometheusReporter.ARG_PORT;\n-import static org.apache.flink.runtime.metrics.scope.ScopeFormat.SCOPE_SEPARATOR;\n import static org.hamcrest.Matchers.containsString;\n import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n import static org.junit.Assert.assertThat;\n \n /**\n- * Test for {@link PrometheusReporter}.\n+ * Basic test for {@link PrometheusReporter}.\n  */\n public class PrometheusReporterTest extends TestLogger {\n \tprivate static final int NON_DEFAULT_PORT = 9429;\n@@ -70,22 +70,21 @@\n \t@Rule\n \tpublic ExpectedException thrown = ExpectedException.none();\n \n-\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter()));\n+\tprivate final MetricRegistry registry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"\" + NON_DEFAULT_PORT)));\n+\tprivate final FrontMetricGroup<TaskManagerMetricGroup> metricGroup = new FrontMetricGroup<>(0, new TaskManagerMetricGroup(registry, HOST_NAME, TASK_MANAGER));\n \tprivate final MetricReporter reporter = registry.getReporters().get(0);\n \n+\t/**\n+\t * {@link io.prometheus.client.Counter} may not decrease, so report {@link Counter} as {@link io.prometheus.client.Gauge}.\n+\t *\n+\t * @throws UnirestException Might be thrown on HTTP problems.\n+\t */\n \t@Test\n \tpublic void counterIsReportedAsPrometheusGauge() throws UnirestException {\n-\t\t//Prometheus counters may not decrease\n \t\tCounter testCounter = new SimpleCounter();\n \t\ttestCounter.inc(7);\n \n-\t\tString counterName = \"testCounter\";\n-\t\tString gaugeName = SCOPE_PREFIX + counterName;\n-\n-\t\tassertThat(addMetricAndPollResponse(testCounter, counterName),\n-\t\t\tequalTo(HELP_PREFIX + gaugeName + \" \" + getFullMetricName(counterName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + gaugeName + \" gauge\" + \"\\n\" +\n-\t\t\t\tgaugeName + DEFAULT_LABELS + \" 7.0\" + \"\\n\"));\n+\t\tassertThatGaugeIsExported(testCounter, \"testCounter\", \"7.0\");\n \t}\n \n \t@Test\n@@ -97,13 +96,34 @@ public Integer getValue() {\n \t\t\t}\n \t\t};\n \n-\t\tString gaugeName = \"testGauge\";\n-\t\tString prometheusGaugeName = SCOPE_PREFIX + gaugeName;\n+\t\tassertThatGaugeIsExported(testGauge, \"testGauge\", \"1.0\");\n+\t}\n+\n+\t@Test\n+\tpublic void nullGaugeDoesNotBreakReporter() throws UnirestException {\n+\t\tGauge<Integer> testGauge = new Gauge<Integer>() {\n+\t\t\t@Override\n+\t\t\tpublic Integer getValue() {\n+\t\t\t\treturn null;\n+\t\t\t}\n+\t\t};\n+\n+\t\tassertThatGaugeIsExported(testGauge, \"testGauge\", \"0.0\");\n+\t}\n+\n+\t@Test\n+\tpublic void meterRateIsReportedAsPrometheusGauge() throws UnirestException {\n+\t\tMeter testMeter = new TestMeter();\n+\n+\t\tassertThatGaugeIsExported(testMeter, \"testMeter\", \"5.0\");\n+\t}\n \n-\t\tassertThat(addMetricAndPollResponse(testGauge, gaugeName),\n-\t\t\tequalTo(HELP_PREFIX + prometheusGaugeName + \" \" + getFullMetricName(gaugeName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + prometheusGaugeName + \" gauge\" + \"\\n\" +\n-\t\t\t\tprometheusGaugeName + DEFAULT_LABELS + \" 1.0\" + \"\\n\"));\n+\tprivate void assertThatGaugeIsExported(Metric metric, String name, String expectedValue) throws UnirestException {\n+\t\tfinal String prometheusName = SCOPE_PREFIX + name;\n+\t\tassertThat(addMetricAndPollResponse(metric, name),\n+\t\t\tcontainsString(HELP_PREFIX + prometheusName + \" \" + name + \" (scope: taskmanager)\\n\" +\n+\t\t\t\tTYPE_PREFIX + prometheusName + \" gauge\" + \"\\n\" +\n+\t\t\t\tprometheusName + DEFAULT_LABELS + \" \" + expectedValue + \"\\n\"));\n \t}\n \n \t@Test\n@@ -114,7 +134,7 @@ public void histogramIsReportedAsPrometheusSummary() throws UnirestException {\n \t\tString summaryName = SCOPE_PREFIX + histogramName;\n \n \t\tString response = addMetricAndPollResponse(testHistogram, histogramName);\n-\t\tassertThat(response, containsString(HELP_PREFIX + summaryName + \" \" + getFullMetricName(histogramName) + \"\\n\" +\n+\t\tassertThat(response, containsString(HELP_PREFIX + summaryName + \" \" + histogramName + \" (scope: taskmanager)\\n\" +\n \t\t\tTYPE_PREFIX + summaryName + \" summary\" + \"\\n\" +\n \t\t\tsummaryName + \"_count\" + DEFAULT_LABELS + \" 1.0\" + \"\\n\"));\n \t\tfor (String quantile : Arrays.asList(\"0.5\", \"0.75\", \"0.95\", \"0.98\", \"0.99\", \"0.999\")) {\n@@ -123,19 +143,6 @@ public void histogramIsReportedAsPrometheusSummary() throws UnirestException {\n \t\t}\n \t}\n \n-\t@Test\n-\tpublic void meterRateIsReportedAsPrometheusGauge() throws UnirestException {\n-\t\tMeter testMeter = new TestMeter();\n-\n-\t\tString meterName = \"testMeter\";\n-\t\tString counterName = SCOPE_PREFIX + meterName;\n-\n-\t\tassertThat(addMetricAndPollResponse(testMeter, meterName),\n-\t\t\tequalTo(HELP_PREFIX + counterName + \" \" + getFullMetricName(meterName) + \"\\n\" +\n-\t\t\t\tTYPE_PREFIX + counterName + \" gauge\" + \"\\n\" +\n-\t\t\t\tcounterName + DEFAULT_LABELS + \" 5.0\" + \"\\n\"));\n-\t}\n-\n \t@Test\n \tpublic void endpointIsUnavailableAfterReporterIsClosed() throws UnirestException {\n \t\treporter.close();\n@@ -160,25 +167,119 @@ public void invalidCharactersAreReplacedWithUnderscore() {\n \t\tassertThat(PrometheusReporter.replaceInvalidChars(\"a,=;:?'b,=;:?'c\"), equalTo(\"a___:__b___:__c\"));\n \t}\n \n+\t@Test\n+\tpublic void doubleGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Double>() {\n+\t\t\t@Override\n+\t\t\tpublic Double getValue() {\n+\t\t\t\treturn 3.14;\n+\t\t\t}\n+\t\t}).get(), equalTo(3.14));\n+\t}\n+\n+\t@Test\n+\tpublic void shortGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Short>() {\n+\t\t\t@Override\n+\t\t\tpublic Short getValue() {\n+\t\t\t\treturn 13;\n+\t\t\t}\n+\t\t}).get(), equalTo(13.));\n+\t}\n+\n+\t@Test\n+\tpublic void booleanGaugeIsConvertedCorrectly() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<Boolean>() {\n+\t\t\t@Override\n+\t\t\tpublic Boolean getValue() {\n+\t\t\t\treturn true;\n+\t\t\t}\n+\t\t}).get(), equalTo(1.));\n+\t}\n+\n+\t/**\n+\t * Prometheus only supports numbers, so report non-numeric gauges as 0.\n+\t */\n+\t@Test\n+\tpublic void stringGaugeCannotBeConverted() {\n+\t\tassertThat(PrometheusReporter.gaugeFrom(new Gauge<String>() {\n+\t\t\t@Override\n+\t\t\tpublic String getValue() {\n+\t\t\t\treturn \"I am not a number\";\n+\t\t\t}\n+\t\t}).get(), equalTo(0.));\n+\t}\n+\n+\t@Test\n+\tpublic void registeringSameMetricTwiceDoesNotThrowException() {\n+\t\tCounter counter = new SimpleCounter();\n+\t\tcounter.inc();\n+\t\tString counterName = \"testCounter\";\n+\n+\t\treporter.notifyOfAddedMetric(counter, counterName, metricGroup);\n+\t\treporter.notifyOfAddedMetric(counter, counterName, metricGroup);\n+\t}\n+\n+\t@Test\n+\tpublic void addingUnknownMetricTypeDoesNotThrowException(){\n+\t\tclass SomeMetricType implements Metric{}\n+\n+\t\treporter.notifyOfAddedMetric(new SomeMetricType(), \"name\", metricGroup);\n+\t}\n+\n+\t@Test\n+\tpublic void cannotStartTwoReportersOnSamePort() {\n+\t\tfinal MetricRegistry fixedPort1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"12345\")));\n+\t\tfinal MetricRegistry fixedPort2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"12345\")));\n+\n+\t\tassertThat(fixedPort1.getReporters(), hasSize(1));\n+\t\tassertThat(fixedPort2.getReporters(), hasSize(0));\n+\n+\t\tfixedPort1.shutdown();\n+\t\tfixedPort2.shutdown();\n+\t}\n+\n+\t@Test\n+\tpublic void canStartTwoReportersWhenUsingPortRange() {\n+\t\tfinal MetricRegistry portRange1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9249-9252\")));\n+\t\tfinal MetricRegistry portRange2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"9249-9252\")));\n+\n+\t\tassertThat(portRange1.getReporters(), hasSize(1));\n+\t\tassertThat(portRange2.getReporters(), hasSize(1));\n+\n+\t\tportRange1.shutdown();\n+\t\tportRange2.shutdown();\n+\t}\n+\n+\t@Test\n+\tpublic void cannotStartThreeReportersWhenPortRangeIsTooSmall() {\n+\t\tfinal MetricRegistry smallPortRange1 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test1\", \"9253-9254\")));\n+\t\tfinal MetricRegistry smallPortRange2 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test2\", \"9253-9254\")));\n+\t\tfinal MetricRegistry smallPortRange3 = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(createConfigWithOneReporter(\"test3\", \"9253-9254\")));\n+\n+\t\tassertThat(smallPortRange1.getReporters(), hasSize(1));\n+\t\tassertThat(smallPortRange2.getReporters(), hasSize(1));\n+\t\tassertThat(smallPortRange3.getReporters(), hasSize(0));\n+\n+\t\tsmallPortRange1.shutdown();\n+\t\tsmallPortRange2.shutdown();\n+\t\tsmallPortRange3.shutdown();\n+\t}\n+\n \tprivate String addMetricAndPollResponse(Metric metric, String metricName) throws UnirestException {\n-\t\treporter.notifyOfAddedMetric(metric, metricName, new FrontMetricGroup<>(0, new TaskManagerMetricGroup(registry, HOST_NAME, TASK_MANAGER)));\n+\t\treporter.notifyOfAddedMetric(metric, metricName, metricGroup);\n \t\treturn pollMetrics().getBody();\n \t}\n \n-\tprivate static HttpResponse<String> pollMetrics() throws UnirestException {\n+\tstatic HttpResponse<String> pollMetrics() throws UnirestException {\n \t\treturn Unirest.get(\"http://localhost:\" + NON_DEFAULT_PORT + \"/metrics\").asString();\n \t}\n \n-\tprivate static String getFullMetricName(String metricName) {\n-\t\treturn HOST_NAME + SCOPE_SEPARATOR + \"taskmanager\" + SCOPE_SEPARATOR + TASK_MANAGER + SCOPE_SEPARATOR + metricName;\n-\t}\n-\n-\tprivate static Configuration createConfigWithOneReporter() {\n+\tstatic Configuration createConfigWithOneReporter(String reporterName, String portString) {\n \t\tConfiguration cfg = new Configuration();\n-\t\tcfg.setString(MetricOptions.REPORTERS_LIST, \"test1\");\n-\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"test1.\" +\n-\t\t\tConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, PrometheusReporter.class.getName());\n-\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + \"test1.\" + ARG_PORT, \"\" + NON_DEFAULT_PORT);\n+\t\tcfg.setString(MetricOptions.REPORTERS_LIST, reporterName);\n+\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + reporterName + \".\" + ConfigConstants.METRICS_REPORTER_CLASS_SUFFIX, PrometheusReporter.class.getName());\n+\t\tcfg.setString(ConfigConstants.METRICS_REPORTER_PREFIX + reporterName + \".\" + ARG_PORT, portString);\n \t\treturn cfg;\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/flink-metrics/flink-metrics-prometheus/src/test/java/org/apache/flink/metrics/prometheus/PrometheusReporterTest.java",
                "sha": "956339b818930d9334d877f1cbc80c7e12e508a1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pom.xml?ref=56017a98fa61fdfae1c8dadd90a378ffdb3fea72",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -116,7 +116,7 @@ under the License.\n \t\t<curator.version>2.12.0</curator.version>\n \t\t<jackson.version>2.7.4</jackson.version>\n \t\t<metrics.version>3.1.5</metrics.version>\n-\t\t<prometheus.version>0.0.21</prometheus.version>\n+\t\t<prometheus.version>0.0.26</prometheus.version>\n \t\t<junit.version>4.12</junit.version>\n \t\t<mockito.version>1.10.19</mockito.version>\n \t\t<powermock.version>1.6.5</powermock.version>",
                "raw_url": "https://github.com/apache/flink/raw/56017a98fa61fdfae1c8dadd90a378ffdb3fea72/pom.xml",
                "sha": "9384a48fe8472c0708f01a7cd5061d6e12d3b03e",
                "status": "modified"
            }
        ],
        "message": "[FLINK-7502][metrics] Improve PrometheusReporter\n\n* Do not throw exception when same metric is added twice\n* Add possibility to configure port range\n* Bump prometheus.version 0.0.21 -> 0.0.26\n* Use simpleclient_httpserver instead of nanohttpd\n* guard gauge report against null\n* guard close() vs NPE\n\nThis closes #4586.",
        "parent": "https://github.com/apache/flink/commit/c0199f5d181a8d249201004f6ec6f897f9b799c4",
        "patched_files": [
            "metrics.java",
            "PrometheusReporter.java",
            "pom.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "PrometheusReporterTaskScopeTest.java",
            "PrometheusReporterTest.java"
        ]
    },
    "flink_5dfb8a0": {
        "bug_id": "flink_5dfb8a0",
        "commit": "https://github.com/apache/flink/commit/5dfb8a013cbf769bee641c04ebc666a8dc2f3e14",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/5dfb8a013cbf769bee641c04ebc666a8dc2f3e14/flink-runtime/src/main/java/org/apache/flink/runtime/net/ConnectionUtils.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/net/ConnectionUtils.java?ref=5dfb8a013cbf769bee641c04ebc666a8dc2f3e14",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/net/ConnectionUtils.java",
                "patch": "@@ -240,7 +240,11 @@ private static InetAddress findAddressUsingStrategy(AddressDetectionState strate\n \t\t\t}\n \t\t}\n \n-\t\tfinal byte[] targetAddressBytes = targetAddress.getAddress().getAddress();\n+\t\tfinal InetAddress address = targetAddress.getAddress();\n+\t\tif (address == null) {\n+\t\t\treturn null;\n+\t\t}\n+\t\tfinal byte[] targetAddressBytes = address.getAddress();\n \n \t\t// for each network interface\n \t\tEnumeration<NetworkInterface> e = NetworkInterface.getNetworkInterfaces();",
                "raw_url": "https://github.com/apache/flink/raw/5dfb8a013cbf769bee641c04ebc666a8dc2f3e14/flink-runtime/src/main/java/org/apache/flink/runtime/net/ConnectionUtils.java",
                "sha": "77324fa3ba6fffe5c9dfbc1b90ebfde72f484dd9",
                "status": "modified"
            }
        ],
        "message": "[FLINK-3892] ConnectionUtils may die with NullPointerException\n\nThis fixes a NullPointerException that would occur if the host name\ncould not be resolved.\n\nThis closes #2008",
        "parent": "https://github.com/apache/flink/commit/92e1c82cc545b80c3f82e01a97708aa8d70b3806",
        "patched_files": [
            "ConnectionUtils.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "ConnectionUtilsTest.java"
        ]
    },
    "flink_5dfc897": {
        "bug_id": "flink_5dfc897",
        "commit": "https://github.com/apache/flink/commit/5dfc897beb99d2d8f6d7becba972ff5756b3fb19",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/flink/blob/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java?ref=5dfc897beb99d2d8f6d7becba972ff5756b3fb19",
                "deletions": 15,
                "filename": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java",
                "patch": "@@ -17,20 +17,20 @@\n \n package org.apache.flink.streaming.api.functions.source;\n \n-import java.io.IOException;\n-import java.net.URI;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n import org.apache.flink.api.java.tuple.Tuple3;\n import org.apache.flink.core.fs.FileStatus;\n import org.apache.flink.core.fs.FileSystem;\n import org.apache.flink.core.fs.Path;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n public class FileMonitoringFunction implements SourceFunction<Tuple3<String, Long, Long>> {\n \tprivate static final long serialVersionUID = 1L;\n \n@@ -95,16 +95,21 @@ public void run(SourceContext<Tuple3<String, Long, Long>> ctx) throws Exception\n \n \t\tFileStatus[] statuses = fileSystem.listStatus(new Path(path));\n \n-\t\tfor (FileStatus status : statuses) {\n-\t\t\tPath filePath = status.getPath();\n-\t\t\tString fileName = filePath.getName();\n-\t\t\tlong modificationTime = status.getModificationTime();\n-\n-\t\t\tif (!isFiltered(fileName, modificationTime)) {\n-\t\t\t\tfiles.add(filePath.toString());\n-\t\t\t\tmodificationTimes.put(fileName, modificationTime);\n+\t\tif (statuses == null) {\n+\t\t\tLOG.warn(\"Path does not exist: {}\", path);\n+\t\t} else {\n+\t\t\tfor (FileStatus status : statuses) {\n+\t\t\t\tPath filePath = status.getPath();\n+\t\t\t\tString fileName = filePath.getName();\n+\t\t\t\tlong modificationTime = status.getModificationTime();\n+\n+\t\t\t\tif (!isFiltered(fileName, modificationTime)) {\n+\t\t\t\t\tfiles.add(filePath.toString());\n+\t\t\t\t\tmodificationTimes.put(fileName, modificationTime);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n+\n \t\treturn files;\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunction.java",
                "sha": "a2179238c396e621c441403f829ffe5ada392ec8",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/flink/blob/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java?ref=5dfc897beb99d2d8f6d7becba972ff5756b3fb19",
                "deletions": 0,
                "filename": "flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java",
                "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *\t http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.streaming.api.functions.source;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.streaming.api.operators.Output;\n+import org.apache.flink.streaming.api.operators.StreamSource;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.junit.Test;\n+\n+/**\n+ * Tests for the {@link org.apache.flink.streaming.api.functions.source.FileMonitoringFunction}.\n+ */\n+public class FileMonitoringFunctionTest {\n+\n+\t@Test\n+\tpublic void testForEmptyLocation() throws Exception {\n+\t\tfinal FileMonitoringFunction fileMonitoringFunction\n+\t\t\t= new FileMonitoringFunction(\"?non-existing-path\", 1L, FileMonitoringFunction.WatchType.ONLY_NEW_FILES);\n+\n+        new Thread() {\n+            @Override\n+            public void run() {\n+                try {\n+                    Thread.sleep(1000L);\n+                } catch (InterruptedException e) {\n+                    e.printStackTrace();\n+                }\n+                fileMonitoringFunction.cancel();\n+            }\n+        }.start();\n+\n+\t\tfileMonitoringFunction.run(\n+            new StreamSource.NonWatermarkContext<Tuple3<String, Long, Long>>(\n+                new Object(),\n+                new Output<StreamRecord<Tuple3<String, Long, Long>>>() {\n+                    @Override\n+                    public void emitWatermark(Watermark mark) { }\n+                    @Override\n+                    public void collect(StreamRecord<Tuple3<String, Long, Long>> record) { }\n+                    @Override\n+                    public void close() { }\n+                })\n+        );\n+\t}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/flink/raw/5dfc897beb99d2d8f6d7becba972ff5756b3fb19/flink-staging/flink-streaming/flink-streaming-core/src/test/java/org/apache/flink/streaming/api/functions/source/FileMonitoringFunctionTest.java",
                "sha": "2d9921ae943e149880bacc889ab87dfa456619e0",
                "status": "added"
            }
        ],
        "message": "[FLINK-2817] [streaming] FileMonitoring function logs on empty location\n\nInstead of throwing NPE when location is empty\n\nCloses #1251",
        "parent": "https://github.com/apache/flink/commit/2475c82d425b4a6a564f0d38f352a0f3b8753d72",
        "patched_files": [
            "FileMonitoringFunction.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "FileMonitoringFunctionTest.java"
        ]
    },
    "flink_5e32eb5": {
        "bug_id": "flink_5e32eb5",
        "commit": "https://github.com/apache/flink/commit/5e32eb549d3bc2195548620005fcf54437e75f48",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/5e32eb549d3bc2195548620005fcf54437e75f48/flink-clients/src/main/java/org/apache/flink/client/CliFrontend.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/main/java/org/apache/flink/client/CliFrontend.java?ref=5e32eb549d3bc2195548620005fcf54437e75f48",
                "deletions": 0,
                "filename": "flink-clients/src/main/java/org/apache/flink/client/CliFrontend.java",
                "patch": "@@ -842,6 +842,12 @@ protected int executeProgram(PackagedProgram program, ClusterClient client, int\n \t\t\tprogram.deleteExtractedLibraries();\n \t\t}\n \n+\t\tif (null == result) {\n+\t\t\tlogAndSysout(\"No JobSubmissionResult returned, please make sure you called \" +\n+\t\t\t\t\"ExecutionEnvironment.execute()\");\n+\t\t\treturn 1;\n+\t\t}\n+\n \t\tif (result.isJobExecutionResult()) {\n \t\t\tlogAndSysout(\"Program execution finished\");\n \t\t\tJobExecutionResult execResult = result.getJobExecutionResult();",
                "raw_url": "https://github.com/apache/flink/raw/5e32eb549d3bc2195548620005fcf54437e75f48/flink-clients/src/main/java/org/apache/flink/client/CliFrontend.java",
                "sha": "8c84c5ae7f629ed5d8dfc023257fe2709a8775ea",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/5e32eb549d3bc2195548620005fcf54437e75f48/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java?ref=5e32eb549d3bc2195548620005fcf54437e75f48",
                "deletions": 1,
                "filename": "flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java",
                "patch": "@@ -172,7 +172,8 @@ public JobGraph compileJobGraph(OptimizedPlan program) {\n \t\n \tpublic JobGraph compileJobGraph(OptimizedPlan program, JobID jobId) {\n \t\tif (program == null) {\n-\t\t\tthrow new NullPointerException();\n+\t\t\tthrow new NullPointerException(\"Program is null, did you called \" +\n+\t\t\t\t\"ExecutionEnvironment.execute()\");\n \t\t}\n \t\t\n \t\tif (jobId == null) {",
                "raw_url": "https://github.com/apache/flink/raw/5e32eb549d3bc2195548620005fcf54437e75f48/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java",
                "sha": "6f7b04a35515aaaaae1d3049dc1eb279f92a438e",
                "status": "modified"
            }
        ],
        "message": "[FLINK-5739] [client] Fix NullPointerException in CliFrontend\n\nThis closes #3292",
        "parent": "https://github.com/apache/flink/commit/3104619250fa0e0e87b4bb3e05b1cce9d39e6983",
        "patched_files": [
            "JobGraphGenerator.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "JobGraphGeneratorTest.java"
        ]
    },
    "flink_5f11df6": {
        "bug_id": "flink_5f11df6",
        "commit": "https://github.com/apache/flink/commit/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java?ref=5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65",
                "deletions": 0,
                "filename": "flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java",
                "patch": "@@ -35,6 +35,7 @@ private CliStrings() {\n \n \tpublic static final String CLI_NAME = \"Flink SQL CLI Client\";\n \tpublic static final String DEFAULT_MARGIN = \" \";\n+\tpublic static final String NULL_COLUMN = \"(NULL)\";\n \n \t// --------------------------------------------------------------------------------------------\n ",
                "raw_url": "https://github.com/apache/flink/raw/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java",
                "sha": "1e8f696bdf26cb4826f9cbe6c96f8c15d279a209",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java?ref=5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65",
                "deletions": 1,
                "filename": "flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java",
                "patch": "@@ -93,7 +93,12 @@ public static void normalizeColumn(AttributedStringBuilder sb, String col, int m\n \tpublic static String[] rowToString(Row row) {\n \t\tfinal String[] fields = new String[row.getArity()];\n \t\tfor (int i = 0; i < row.getArity(); i++) {\n-\t\t\tfields[i] = row.getField(i).toString();\n+\t\t\tfinal Object field = row.getField(i);\n+\t\t\tif (field == null) {\n+\t\t\t\tfields[i] = CliStrings.NULL_COLUMN;\n+\t\t\t} else {\n+\t\t\t\tfields[i] = field.toString();\n+\t\t\t}\n \t\t}\n \t\treturn fields;\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/5f11df6eedf64f81ffdbf4afc2ad5d84b1b2ae65/flink-libraries/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliUtils.java",
                "sha": "77894e8b7e192dc37b305e43f2530c91bae7c034",
                "status": "modified"
            }
        ],
        "message": "[hotfix] [sql-client] Fix NPE when column is null",
        "parent": "https://github.com/apache/flink/commit/e8e74a648a134fe054081a49a36b8c45f30c21bc",
        "patched_files": [
            "CliUtils.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "CliUtilsTest.java"
        ]
    },
    "flink_5f86f2d": {
        "bug_id": "flink_5f86f2d",
        "commit": "https://github.com/apache/flink/commit/5f86f2d1ae608a4d2259bfcf637d97b490dbd328",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/5f86f2d1ae608a4d2259bfcf637d97b490dbd328/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarOperatorGens.scala",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarOperatorGens.scala?ref=5f86f2d1ae608a4d2259bfcf637d97b490dbd328",
                "deletions": 3,
                "filename": "flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarOperatorGens.scala",
                "patch": "@@ -1279,21 +1279,26 @@ object ScalarOperatorGens {\n \n       val Seq(resultTerm, nullTerm) = newNames(\"result\", \"isNull\")\n       val resultTypeTerm = primitiveTypeTermForType(resultType)\n+      val defaultValue = primitiveDefaultValue(resultType)\n \n       val operatorCode = if (ctx.nullCheck) {\n         s\"\"\"\n            |${condition.code}\n-           |$resultTypeTerm $resultTerm;\n+           |$resultTypeTerm $resultTerm = $defaultValue;\n            |boolean $nullTerm;\n            |if (${condition.resultTerm}) {\n            |  ${trueAction.code}\n-           |  $resultTerm = ${trueAction.resultTerm};\n            |  $nullTerm = ${trueAction.nullTerm};\n+           |  if (!$nullTerm) {\n+           |    $resultTerm = ${trueAction.resultTerm};\n+           |  }\n            |}\n            |else {\n            |  ${falseAction.code}\n-           |  $resultTerm = ${falseAction.resultTerm};\n            |  $nullTerm = ${falseAction.nullTerm};\n+           |  if (!$nullTerm) {\n+           |    $resultTerm = ${falseAction.resultTerm};\n+           |  }\n            |}\n            |\"\"\".stripMargin.trim\n       }",
                "raw_url": "https://github.com/apache/flink/raw/5f86f2d1ae608a4d2259bfcf637d97b490dbd328/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/calls/ScalarOperatorGens.scala",
                "sha": "200c0de57b0e9d57c40fb84031b5ee7cf2da2653",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/5f86f2d1ae608a4d2259bfcf637d97b490dbd328/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/expressions/ScalarOperatorsTest.scala",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/expressions/ScalarOperatorsTest.scala?ref=5f86f2d1ae608a4d2259bfcf637d97b490dbd328",
                "deletions": 0,
                "filename": "flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/expressions/ScalarOperatorsTest.scala",
                "patch": "@@ -122,5 +122,7 @@ class ScalarOperatorsTest extends ScalarOperatorsTestBase {\n     testSqlApi(\"CASE f7 WHEN 1 THEN 11 WHEN 2 THEN 4 ELSE NULL END\", \"null\")\n     testSqlApi(\"CASE 42 WHEN 1 THEN 'a' WHEN 2 THEN 'bcd' END\", \"null\")\n     testSqlApi(\"CASE 1 WHEN 1 THEN true WHEN 2 THEN false ELSE NULL END\", \"true\")\n+\n+    testSqlApi(\"CASE WHEN f2 = 1 THEN CAST('' as INT) ELSE 0 END\", \"null\")\n   }\n }",
                "raw_url": "https://github.com/apache/flink/raw/5f86f2d1ae608a4d2259bfcf637d97b490dbd328/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/expressions/ScalarOperatorsTest.scala",
                "sha": "04b20022f80ec240e467fdeffcee1a5ae64a2684",
                "status": "modified"
            }
        ],
        "message": "[FLINK-15266][table-planner-blink] Fix NPE for code generated CASE WHEN operator in blink planner (#10594)",
        "parent": "https://github.com/apache/flink/commit/b692d101e086ee2596d3c53a5ceec0a26c74ca3d",
        "patched_files": [
            "ScalarOperatorGens.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "ScalarOperatorsTest.java"
        ]
    },
    "flink_6a54544": {
        "bug_id": "flink_6a54544",
        "commit": "https://github.com/apache/flink/commit/6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 0,
                "filename": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "patch": "@@ -82,4 +82,12 @@ public Ordering createNewOrderingUpToIndex(int exclusiveIndex) {\n \t\t}\n \t\treturn newOrdering;\n \t}\n+\t\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic Ordering clone() {\n+\t\tOrdering newOrdering = new Ordering();\n+\t\tnewOrdering.indexes = (ArrayList<Integer>) this.indexes.clone();\n+\t\tnewOrdering.orders = (ArrayList<Order>) this.orders.clone();\n+\t\treturn this;\n+\t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "sha": "a357491a6c09f53b61a16b7d11040c4d85c6399e",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 7,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java",
                "patch": "@@ -236,15 +236,13 @@ public boolean isMetBy(GlobalProperties other) {\n \t\t\tif (other.partitionedFields == null) {\n \t\t\t\treturn false;\n \t\t\t}\n-\t\t\tif (this.partitionedFields.size() > otherPartitionedFields.size()) {\n+\t\t\tif (this.partitionedFields.size() < otherPartitionedFields.size()) {\n \t\t\t\treturn false;\n \t\t\t}\n \t\t\t\n-\t\t\tfor (Integer fieldIndex : this.partitionedFields) {\n-\t\t\t\tif (otherPartitionedFields.contains(fieldIndex) == false) {\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t}\t\n+\t\t\tif (this.partitionedFields.containsAll(otherPartitionedFields) == false) {\n+\t\t\t\treturn false;\n+\t\t\t}\n \t\t}\n \t\t\n \t\treturn (this.ordering == null || this.ordering.isMetBy(other.getOrdering()));\n@@ -322,7 +320,14 @@ public String toString() {\n \t * @see java.lang.Object#clone()\n \t */\n \tpublic GlobalProperties clone() throws CloneNotSupportedException {\n-\t\treturn (GlobalProperties) super.clone();\n+\t\tGlobalProperties newProps = (GlobalProperties) super.clone();\n+\t\tif (this.ordering != null) {\n+\t\t\tnewProps.ordering = this.ordering.clone();\t\n+\t\t}\n+\t\tif (this.partitionedFields != null) {\n+\t\t\tnewProps.partitionedFields = (FieldSet) this.partitionedFields.clone();\t\n+\t\t}\n+\t\treturn newProps;\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/GlobalProperties.java",
                "sha": "d18269622a12284567778195fd65a098c26de900",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 5,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java",
                "patch": "@@ -223,11 +223,12 @@ public boolean isMetBy(LocalProperties other) {\n \t\t\t\t\t\treturn false;\n \t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t\tgroupingFulfilled = true;\n+\t\t\t}\n+\t\t\t\n+\t\t\tif (groupingFulfilled == false) {\n+\t\t\t\treturn false;\n \t\t\t}\n-\t\t}\n-\n-\t\tif (groupingFulfilled == false) {\n-\t\t\treturn false;\n \t\t}\n \t\t// check the order\n \t\treturn (this.ordering == null || this.ordering.isMetBy(other.getOrdering()));\n@@ -296,7 +297,14 @@ public String toString() {\n \t */\n \t@Override\n \tpublic LocalProperties clone() throws CloneNotSupportedException {\n-\t\treturn (LocalProperties) super.clone();\n+\t\tLocalProperties newProps = (LocalProperties) super.clone();\n+\t\tif (this.ordering != null) {\n+\t\t\tnewProps.ordering = this.ordering.clone();\t\n+\t\t}\n+\t\tif (this.groupedFields != null) {\n+\t\t\tnewProps.groupedFields = (FieldSet) this.groupedFields.clone();\t\n+\t\t}\n+\t\treturn newProps;\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java",
                "sha": "8f8b5b2cd2bc6137d804e1c90537b1fe86e99a24",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 5,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java",
                "patch": "@@ -48,8 +48,8 @@\n \t * properties and a maximal cost of infinite.\n \t */\n \tpublic InterestingProperties() {\n-\t\t// instantiate the maximal costs to the possible maximum\n-\t\tthis.maximalCosts = new Costs(Long.MAX_VALUE, Long.MAX_VALUE);\n+\t\t// instantiate the maximal costs to 0\n+\t\tthis.maximalCosts = new Costs(0, 0);\n \n \t\tthis.globalProps = new GlobalProperties();\n \t\tthis.localProps = new LocalProperties();\n@@ -314,11 +314,18 @@ public static void mergeUnionOfInterestingProperties(List<InterestingProperties>\n \t\tList<InterestingProperties> preserved = new ArrayList<InterestingProperties>();\n \t\t\n \t\tfor (InterestingProperties p : props) {\n-\t\t\tboolean nonTrivial = p.getGlobalProperties().filterByNodesConstantSet(node, input);\n-\t\t\tnonTrivial |= p.getLocalProperties().filterByNodesConstantSet(node, input);\n+\t\t\tGlobalProperties preservedGp = p.getGlobalProperties().createCopy();\n+\t\t\tLocalProperties preservedLp = p.getLocalProperties().createCopy();\n+\t\t\tboolean nonTrivial = preservedGp.filterByNodesConstantSet(node, input);\n+\t\t\tnonTrivial |= preservedLp.filterByNodesConstantSet(node, input);\n \n \t\t\tif (nonTrivial) {\n-\t\t\t\tpreserved.add(p);\n+\t\t\t\ttry {\n+\t\t\t\t\tpreserved.add(new InterestingProperties(p.getMaximalCosts().clone(), preservedGp, preservedLp));\n+\t\t\t\t} catch (CloneNotSupportedException cnse) {\n+\t\t\t\t\t// should never happen, but propagate just in case\n+\t\t\t\t\tthrow new RuntimeException(cnse);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/InterestingProperties.java",
                "sha": "062d470cd8365a2c3edb434b7cd976e86bbbe42d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 0,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java",
                "patch": "@@ -476,6 +476,10 @@ public boolean isFieldKept(int input, int fieldNumber) {\n \t\t\tthrow new IndexOutOfBoundsException();\n \t\t}\n \t\t\n+\t\tif (constantSetMode == null) {\n+\t\t\treturn false;\n+\t\t}\n+\t\t\n \t\tswitch (constantSetMode) {\n \t\tcase Constant:\n \t\t\treturn (constantSet != null && Arrays.binarySearch(constantSet, fieldNumber) >= 0);",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/SingleInputNode.java",
                "sha": "ecff08f1e0759b3e391a25689d02aa46442d4146",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java?ref=6a545443a4dfbcdf53779e4c9f488ee2889fd05a",
                "deletions": 0,
                "filename": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java",
                "patch": "@@ -858,6 +858,9 @@ public boolean isFieldKept(int input, int fieldNumber) {\n \t\t\tthrow new IndexOutOfBoundsException();\n \t\t}\n \t\t\n+\t\tif (constantSetMode == null) {\n+\t\t\treturn false;\n+\t\t}\n \t\t\n \t\tswitch (constantSetMode) {\n \t\tcase Constant:",
                "raw_url": "https://github.com/apache/flink/raw/6a545443a4dfbcdf53779e4c9f488ee2889fd05a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/TwoInputNode.java",
                "sha": "db8216a0d9bc92c12877d5a40b5d8ffbdfc7d75a",
                "status": "modified"
            }
        ],
        "message": "Fixed small bugs in Compiler (NPE, Maximal Costs of IP, Cloning of IPs)",
        "parent": "https://github.com/apache/flink/commit/dc1954428c82c85bb6a731edc7c68c248ce1fc98",
        "patched_files": [
            "Ordering.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "OrderingTest.java"
        ]
    },
    "flink_705938e": {
        "bug_id": "flink_705938e",
        "commit": "https://github.com/apache/flink/commit/705938e5965a98b17bd6ba3f1e06728a35e4f8a9",
        "file": [
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/flink/blob/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java?ref=705938e5965a98b17bd6ba3f1e06728a35e4f8a9",
                "deletions": 21,
                "filename": "flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java",
                "patch": "@@ -17,48 +17,58 @@\n  */\n package org.apache.flink.api.java.io.jdbc.split;\n \n+import static org.apache.flink.util.Preconditions.checkArgument;\n+\n import java.io.Serializable;\n \n /** \n  * \n- * This query generator assumes that the query to parameterize contains a BETWEEN constraint on a numeric column.\n- * The generated query set will be of size equal to the configured fetchSize (apart the last one range),\n- * ranging from the min value up to the max.\n+ * This query parameters generator is an helper class to parameterize from/to queries on a numeric column.\n+ * The generated array of from/to values will be equally sized to fetchSize (apart from the last one),\n+ * ranging from minVal up to maxVal.\n  * \n  * For example, if there's a table <CODE>BOOKS</CODE> with a numeric PK <CODE>id</CODE>, using a query like:\n  * <PRE>\n  *   SELECT * FROM BOOKS WHERE id BETWEEN ? AND ?\n  * </PRE>\n  *\n- * you can use this class to automatically generate the parameters of the BETWEEN clause,\n+ * you can take advantage of this class to automatically generate the parameters of the BETWEEN clause,\n  * based on the passed constructor parameters.\n  * \n  * */\n public class NumericBetweenParametersProvider implements ParameterValuesProvider {\n \n-\tprivate long fetchSize;\n-\tprivate final long min;\n-\tprivate final long max;\n+\tprivate final long fetchSize;\n+\tprivate final long minVal;\n+\tprivate final long maxVal;\n \t\n-\tpublic NumericBetweenParametersProvider(long fetchSize, long min, long max) {\n+\t/**\n+\t * NumericBetweenParametersProvider constructor.\n+\t * \n+\t * @param fetchSize the max distance between the produced from/to pairs\n+\t * @param minVal the lower bound of the produced \"from\" values\n+\t * @param maxVal the upper bound of the produced \"to\" values\n+\t */\n+\tpublic NumericBetweenParametersProvider(long fetchSize, long minVal, long maxVal) {\n+\t\tcheckArgument(fetchSize > 0, \"Fetch size must be greater than 0.\");\n+\t\tcheckArgument(minVal <= maxVal, \"Min value cannot be greater than max value.\");\n \t\tthis.fetchSize = fetchSize;\n-\t\tthis.min = min;\n-\t\tthis.max = max;\n+\t\tthis.minVal = minVal;\n+\t\tthis.maxVal = maxVal;\n \t}\n \n \t@Override\n-\tpublic Serializable[][] getParameterValues(){\n-\t\tdouble maxElemCount = (max - min) + 1;\n-\t\tint size = new Double(Math.ceil(maxElemCount / fetchSize)).intValue();\n-\t\tSerializable[][] parameters = new Serializable[size][2];\n-\t\tint count = 0;\n-\t\tfor (long i = min; i < max; i += fetchSize, count++) {\n-\t\t\tlong currentLimit = i + fetchSize - 1;\n-\t\t\tparameters[count] = new Long[]{i,currentLimit};\n-\t\t\tif (currentLimit + 1 + fetchSize > max) {\n-\t\t\t\tparameters[count + 1] = new Long[]{currentLimit + 1, max};\n-\t\t\t\tbreak;\n+\tpublic Serializable[][] getParameterValues() {\n+\t\tdouble maxElemCount = (maxVal - minVal) + 1;\n+\t\tint numBatches = new Double(Math.ceil(maxElemCount / fetchSize)).intValue();\n+\t\tSerializable[][] parameters = new Serializable[numBatches][2];\n+\t\tint batchIndex = 0;\n+\t\tfor (long start = minVal; start <= maxVal; start += fetchSize, batchIndex++) {\n+\t\t\tlong end = start + fetchSize - 1;\n+\t\t\tif (end > maxVal) {\n+\t\t\t\tend = maxVal;\n \t\t\t}\n+\t\t\tparameters[batchIndex] = new Long[]{start, end};\n \t\t}\n \t\treturn parameters;\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/split/NumericBetweenParametersProvider.java",
                "sha": "44201723a44b114452a7ee1dc5f6ad2bef50e12f",
                "status": "modified"
            },
            {
                "additions": 107,
                "blob_url": "https://github.com/apache/flink/blob/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java",
                "changes": 127,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java?ref=705938e5965a98b17bd6ba3f1e06728a35e4f8a9",
                "deletions": 20,
                "filename": "flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java",
                "patch": "@@ -18,19 +18,19 @@\n \n package org.apache.flink.api.java.io.jdbc;\n \n-import java.io.IOException;\n-import java.io.Serializable;\n-import java.sql.ResultSet;\n-\n import org.apache.flink.api.java.io.jdbc.split.GenericParameterValuesProvider;\n import org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider;\n import org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider;\n-import org.apache.flink.types.Row;\n import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.types.Row;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.sql.ResultSet;\n+\n public class JDBCInputFormatTest extends JDBCTestBase {\n \n \tprivate JDBCInputFormat jdbcInputFormat;\n@@ -116,11 +116,21 @@ public void testJDBCInputFormatWithoutParallelism() throws IOException, Instanti\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\t\n-\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\tif (next.getField(0) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(1) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(2) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(3) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t}\n+\t\t\tif (next.getField(4) != null) {\n+\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t}\n \n \t\t\tfor (int x = 0; x < 5; x++) {\n \t\t\t\tif(testData[recordCount][x]!=null) {\n@@ -162,11 +172,78 @@ public void testJDBCInputFormatWithParallelismAndNumericColumnSplitting() throws\n \t\t\t\tif (next == null) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n+\n+\t\t\t\tfor (int x = 0; x < 5; x++) {\n+\t\t\t\t\tif(testData[recordCount][x]!=null) {\n+\t\t\t\t\t\tAssert.assertEquals(testData[recordCount][x], next.getField(x));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\trecordCount++;\n+\t\t\t}\n+\t\t\tjdbcInputFormat.close();\n+\t\t}\n+\t\tjdbcInputFormat.closeInputFormat();\n+\t\tAssert.assertEquals(testData.length, recordCount);\n+\t}\n+\n+\t@Test\n+\tpublic void testJDBCInputFormatWithoutParallelismAndNumericColumnSplitting() throws IOException, InstantiationException, IllegalAccessException {\n+\t\tfinal Long min = new Long(JDBCTestBase.testData[0][0] + \"\");\n+\t\tfinal Long max = new Long(JDBCTestBase.testData[JDBCTestBase.testData.length - 1][0] + \"\");\n+\t\tfinal long fetchSize = max + 1;//generate a single split\n+\t\tParameterValuesProvider pramProvider = new NumericBetweenParametersProvider(fetchSize, min, max);\n+\t\tjdbcInputFormat = JDBCInputFormat.buildJDBCInputFormat()\n+\t\t\t\t.setDrivername(DRIVER_CLASS)\n+\t\t\t\t.setDBUrl(DB_URL)\n+\t\t\t\t.setQuery(JDBCTestBase.SELECT_ALL_BOOKS_SPLIT_BY_ID)\n+\t\t\t\t.setRowTypeInfo(rowTypeInfo)\n+\t\t\t\t.setParametersProvider(pramProvider)\n+\t\t\t\t.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)\n+\t\t\t\t.finish();\n+\n+\t\tjdbcInputFormat.openInputFormat();\n+\t\tInputSplit[] splits = jdbcInputFormat.createInputSplits(1);\n+\t\t//assert that a single split was generated\n+\t\tAssert.assertEquals(1, splits.length);\n+\t\tint recordCount = 0;\n+\t\tRow row =  new Row(5);\n+\t\tfor (int i = 0; i < splits.length; i++) {\n+\t\t\tjdbcInputFormat.open(splits[i]);\n+\t\t\twhile (!jdbcInputFormat.reachedEnd()) {\n+\t\t\t\tRow next = jdbcInputFormat.nextRecord(row);\n+\t\t\t\tif (next == null) {\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n \n \t\t\t\tfor (int x = 0; x < 5; x++) {\n \t\t\t\t\tif(testData[recordCount][x]!=null) {\n@@ -208,11 +285,21 @@ public void testJDBCInputFormatWithParallelismAndGenericSplitting() throws IOExc\n \t\t\t\tif (next == null) {\n \t\t\t\t\tbreak;\n \t\t\t\t}\n-\t\t\t\tif(next.getField(0)!=null) { Assert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());}\n-\t\t\t\tif(next.getField(1)!=null) { Assert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());}\n-\t\t\t\tif(next.getField(2)!=null) { Assert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());}\n-\t\t\t\tif(next.getField(3)!=null) { Assert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());}\n-\t\t\t\tif(next.getField(4)!=null) { Assert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());}\n+\t\t\t\tif (next.getField(0) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 0 should be int\", Integer.class, next.getField(0).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(1) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 1 should be String\", String.class, next.getField(1).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(2) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 2 should be String\", String.class, next.getField(2).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(3) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 3 should be float\", Double.class, next.getField(3).getClass());\n+\t\t\t\t}\n+\t\t\t\tif (next.getField(4) != null) {\n+\t\t\t\t\tAssert.assertEquals(\"Field 4 should be int\", Integer.class, next.getField(4).getClass());\n+\t\t\t\t}\n \n \t\t\t\trecordCount++;\n \t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/705938e5965a98b17bd6ba3f1e06728a35e4f8a9/flink-connectors/flink-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java",
                "sha": "bee3d251a986f81ffa2f0ba582f8ca08677daf65",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6271] [jdbc] Fix NPE when there's a single split\n\nThis closes #3686.",
        "parent": "https://github.com/apache/flink/commit/c96002cef5f1867573a473746241f86b59aeddd2",
        "patched_files": [
            "JDBCInputFormat.java",
            "NumericBetweenParametersProvider.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "JDBCInputFormatTest.java",
            "NumericBetweenParametersProviderTest.java"
        ]
    },
    "flink_74b09ce": {
        "bug_id": "flink_74b09ce",
        "commit": "https://github.com/apache/flink/commit/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
                "deletions": 12,
                "filename": "flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "patch": "@@ -31,7 +31,6 @@\n import org.apache.flink.api.java.typeutils.runtime.TupleSerializer;\n import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;\n import org.apache.flink.streaming.runtime.operators.GenericWriteAheadSink;\n-import org.apache.flink.types.IntValue;\n \n import java.util.UUID;\n import java.util.concurrent.atomic.AtomicInteger;\n@@ -97,7 +96,7 @@ public void close() throws Exception {\n \n \t@Override\n \tprotected boolean sendValues(Iterable<IN> values, long timestamp) throws Exception {\n-\t\tfinal IntValue updatesCount = new IntValue(0);\n+\t\tfinal AtomicInteger updatesCount = new AtomicInteger(0);\n \t\tfinal AtomicInteger updatesConfirmed = new AtomicInteger(0);\n \n \t\tfinal AtomicReference<Throwable> exception = new AtomicReference<>();\n@@ -106,8 +105,8 @@ protected boolean sendValues(Iterable<IN> values, long timestamp) throws Excepti\n \t\t\t@Override\n \t\t\tpublic void onSuccess(ResultSet resultSet) {\n \t\t\t\tupdatesConfirmed.incrementAndGet();\n-\t\t\t\tif (updatesCount.getValue() > 0) { // only set if all updates have been sent\n-\t\t\t\t\tif (updatesCount.getValue() == updatesConfirmed.get()) {\n+\t\t\t\tif (updatesCount.get() > 0) { // only set if all updates have been sent\n+\t\t\t\t\tif (updatesCount.get() == updatesConfirmed.get()) {\n \t\t\t\t\t\tsynchronized (updatesConfirmed) {\n \t\t\t\t\t\t\tupdatesConfirmed.notifyAll();\n \t\t\t\t\t\t}\n@@ -142,18 +141,19 @@ public void onFailure(Throwable throwable) {\n \t\t\t\tFutures.addCallback(result, callback);\n \t\t\t}\n \t\t}\n-\t\tupdatesCount.setValue(updatesSent);\n+\t\tupdatesCount.set(updatesSent);\n \n \t\tsynchronized (updatesConfirmed) {\n-\t\t\twhile (updatesSent != updatesConfirmed.get()) {\n-\t\t\t\tif (exception.get() != null) { // verify that no query failed until now\n-\t\t\t\t\tLOG.warn(\"Sending a value failed.\", exception.get());\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\twhile (exception.get() == null && updatesSent != updatesConfirmed.get()) {\n \t\t\t\tupdatesConfirmed.wait();\n \t\t\t}\n \t\t}\n-\t\tboolean success = updatesSent == updatesConfirmed.get();\n-\t\treturn success;\n+\n+\t\tif (exception.get() != null) {\n+\t\t\tLOG.warn(\"Sending a value failed.\", exception.get());\n+\t\t\treturn false;\n+\t\t} else {\n+\t\t\treturn true;\n+\t\t}\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "sha": "192843107ef3546b6a5896b1dd1a2564bd8529a8",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
                "deletions": 70,
                "filename": "flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java",
                "patch": "@@ -25,39 +25,33 @@\n import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.java.tuple.Tuple0;\n import org.apache.flink.api.java.typeutils.TupleTypeInfo;\n-import org.apache.flink.runtime.io.network.api.writer.ResultPartitionWriter;\n import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n-import org.apache.flink.util.IterableIterator;\n-import org.junit.Assert;\n import org.junit.Test;\n-import org.junit.runner.RunWith;\n import org.mockito.Matchers;\n import org.mockito.invocation.InvocationOnMock;\n import org.mockito.stubbing.Answer;\n-import org.powermock.core.classloader.annotations.PowerMockIgnore;\n-import org.powermock.core.classloader.annotations.PrepareForTest;\n-import org.powermock.modules.junit4.PowerMockRunner;\n \n-import java.util.Iterator;\n+import java.util.Collections;\n import java.util.concurrent.Executor;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import static org.junit.Assert.assertFalse;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyString;\n import static org.powermock.api.mockito.PowerMockito.doAnswer;\n import static org.powermock.api.mockito.PowerMockito.mock;\n import static org.powermock.api.mockito.PowerMockito.when;\n \n-@RunWith(PowerMockRunner.class)\n-@PrepareForTest({ResultPartitionWriter.class, CassandraTupleWriteAheadSink.class})\n-@PowerMockIgnore({\"javax.management.*\", \"com.sun.jndi.*\"})\n-public class CassandraConnectorUnitTest {\n-\t@Test\n+public class CassandraTupleWriteAheadSinkTest {\n+\n+\t@Test(timeout=20000)\n \tpublic void testAckLoopExitOnException() throws Exception {\n-\t\tfinal AtomicReference<Runnable> callback = new AtomicReference<>();\n+\t\tfinal AtomicReference<Runnable> runnableFuture = new AtomicReference<>();\n \n \t\tfinal ClusterBuilder clusterBuilder = new ClusterBuilder() {\n+\t\t\tprivate static final long serialVersionUID = 4624400760492936756L;\n+\n \t\t\t@Override\n \t\t\tprotected Cluster buildCluster(Cluster.Builder builder) {\n \t\t\t\ttry {\n@@ -73,7 +67,10 @@ protected Cluster buildCluster(Cluster.Builder builder) {\n \t\t\t\t\tdoAnswer(new Answer<Void>() {\n \t\t\t\t\t\t@Override\n \t\t\t\t\t\tpublic Void answer(InvocationOnMock invocationOnMock) throws Throwable {\n-\t\t\t\t\t\t\tcallback.set((((Runnable) invocationOnMock.getArguments()[0])));\n+\t\t\t\t\t\t\tsynchronized (runnableFuture) {\n+\t\t\t\t\t\t\t\trunnableFuture.set((((Runnable) invocationOnMock.getArguments()[0])));\n+\t\t\t\t\t\t\t\trunnableFuture.notifyAll();\n+\t\t\t\t\t\t\t}\n \t\t\t\t\t\t\treturn null;\n \t\t\t\t\t\t}\n \t\t\t\t\t}).when(future).addListener(any(Runnable.class), any(Executor.class));\n@@ -91,68 +88,40 @@ public Void answer(InvocationOnMock invocationOnMock) throws Throwable {\n \t\t\t}\n \t\t};\n \n-\t\tfinal IterableIterator<Tuple0> iter = new IterableIterator<Tuple0>() {\n-\t\t\tprivate boolean exhausted = false;\n-\n-\t\t\t@Override\n-\t\t\tpublic boolean hasNext() {\n-\t\t\t\treturn !exhausted;\n-\t\t\t}\n-\n-\t\t\t@Override\n-\t\t\tpublic Tuple0 next() {\n-\t\t\t\texhausted = true;\n-\t\t\t\treturn new Tuple0();\n-\t\t\t}\n-\n-\t\t\t@Override\n-\t\t\tpublic void remove() {\n-\t\t\t}\n-\n+\t\t// Our asynchronous executor thread\n+\t\tnew Thread(new Runnable() {\n \t\t\t@Override\n-\t\t\tpublic Iterator<Tuple0> iterator() {\n-\t\t\t\treturn this;\n-\t\t\t}\n-\t\t};\n-\n-\t\tfinal AtomicReference<Boolean> exceptionCaught = new AtomicReference<>();\n-\n-\t\tThread t = new Thread() {\n \t\t\tpublic void run() {\n-\t\t\t\ttry {\n-\t\t\t\t\tCheckpointCommitter cc = mock(CheckpointCommitter.class);\n-\t\t\t\t\tfinal CassandraTupleWriteAheadSink<Tuple0> sink = new CassandraTupleWriteAheadSink<>(\n-\t\t\t\t\t\t\"abc\",\n-\t\t\t\t\t\tTupleTypeInfo.of(Tuple0.class).createSerializer(new ExecutionConfig()),\n-\t\t\t\t\t\tclusterBuilder,\n-\t\t\t\t\t\tcc\n-\t\t\t\t\t);\n-\n-\t\t\t\t\tOneInputStreamOperatorTestHarness<Tuple0, Tuple0> harness = new OneInputStreamOperatorTestHarness(sink);\n-\t\t\t\t\tharness.getEnvironment().getTaskConfiguration().setBoolean(\"checkpointing\", true);\n-\n-\t\t\t\t\tharness.setup();\n-\t\t\t\t\tsink.open();\n-\t\t\t\t\tboolean result = sink.sendValues(iter, 0L);\n-\t\t\t\t\tsink.close();\n-\t\t\t\t\texceptionCaught.set(result == false);\n-\t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tthrow new RuntimeException(e);\n+\t\t\t\tsynchronized (runnableFuture) {\n+\t\t\t\t\twhile (runnableFuture.get() == null) {\n+\t\t\t\t\t\ttry {\n+\t\t\t\t\t\t\trunnableFuture.wait();\n+\t\t\t\t\t\t} catch (InterruptedException e) {\n+\t\t\t\t\t\t\t// ignore interrupts\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n \t\t\t\t}\n+\t\t\t\trunnableFuture.get().run();\n \t\t\t}\n-\t\t};\n-\t\tt.start();\n+\t\t}).start();\n+\n+\t\tCheckpointCommitter cc = mock(CheckpointCommitter.class);\n+\t\tfinal CassandraTupleWriteAheadSink<Tuple0> sink = new CassandraTupleWriteAheadSink<>(\n+\t\t\t\"abc\",\n+\t\t\tTupleTypeInfo.of(Tuple0.class).createSerializer(new ExecutionConfig()),\n+\t\t\tclusterBuilder,\n+\t\t\tcc\n+\t\t);\n \n-\t\tint count = 0;\n-\t\twhile (t.getState() != Thread.State.WAITING && count < 100) { // 10 second timeout 10 * 10 * 100ms\n-\t\t\tThread.sleep(100);\n-\t\t\tcount++;\n-\t\t}\n+\t\tOneInputStreamOperatorTestHarness<Tuple0, Tuple0> harness = new OneInputStreamOperatorTestHarness(sink);\n+\t\tharness.getEnvironment().getTaskConfiguration().setBoolean(\"checkpointing\", true);\n \n-\t\tcallback.get().run();\n+\t\tharness.setup();\n+\t\tsink.open();\n \n-\t\tt.join();\n+\t\t// we should leave the loop and return false since we've seen an exception\n+\t\tassertFalse(sink.sendValues(Collections.singleton(new Tuple0()), 0L));\n \n-\t\tAssert.assertTrue(exceptionCaught.get());\n+\t\tsink.close();\n \t}\n }",
                "previous_filename": "flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraConnectorUnitTest.java",
                "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSinkTest.java",
                "sha": "847d1a049e576a6f650fe84192e9719e55d1c966",
                "status": "renamed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java?ref=74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90",
                "deletions": 0,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java",
                "patch": "@@ -190,6 +190,9 @@ public void processWatermark(Watermark mark) throws Exception {\n \t * used since the last completed checkpoint.\n \t **/\n \tpublic static class ExactlyOnceState implements StateHandle<Serializable> {\n+\n+\t\tprivate static final long serialVersionUID = -3571063495273460743L;\n+\n \t\tprotected TreeMap<Long, Tuple2<Long, StateHandle<DataInputView>>> pendingHandles;\n \n \t\tpublic ExactlyOnceState() {",
                "raw_url": "https://github.com/apache/flink/raw/74b09ce0db4d24a0ac25de2ecac391fdf8bd5a90/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericWriteAheadSink.java",
                "sha": "5545717b24466fc5e1c6ae209c7323c9e551a8eb",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4123] [cassandra] Fix concurrency issue in CassandraTupleWriteAheadSink\n\nThe updatesCount variable in the CassandraTupleWriteAheadSink.sendValues did not have\nguaranteed visibility. Thus, it was possible that the callback thread would read an\noutdated value for updatesCount, resulting in a deadlock. Replacing IntValue updatesCount\nwith AtomicInteger updatesCount fixes this issue.\n\nFurthermore, the PR hardens the CassandraTupleWriteAheadSinkTest which could have failed\nwith a NPE if the callback runnable was not set in time.",
        "parent": "https://github.com/apache/flink/commit/5c2da21f25741502dd8ca64ce9d314a1ebea1441",
        "patched_files": [
            "CassandraTupleWriteAheadSink.java",
            "GenericWriteAheadSink.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "CassandraTupleWriteAheadSinkTest.java",
            "GenericWriteAheadSinkTest.java"
        ]
    },
    "flink_9e139a7": {
        "bug_id": "flink_9e139a7",
        "commit": "https://github.com/apache/flink/commit/9e139a72ba45f2dd820bd3b9ecdf8428588666fd",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/9e139a72ba45f2dd820bd3b9ecdf8428588666fd/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java?ref=9e139a72ba45f2dd820bd3b9ecdf8428588666fd",
                "deletions": 11,
                "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java",
                "patch": "@@ -591,17 +591,20 @@ public void collect(StreamRecord<T> record) {\n \t\t\t\toperator.setKeyContextElement1(copy);\n \t\t\t\toperator.processElement(copy);\n \t\t\t} catch (ClassCastException e) {\n-\t\t\t\t// Enrich error message\n-\t\t\t\tClassCastException replace = new ClassCastException(\n-\t\t\t\t\tString.format(\n-\t\t\t\t\t\t\"%s. Failed to push OutputTag with id '%s' to operator. \" +\n-\t\t\t\t\t\t\"This can occur when multiple OutputTags with different types \" +\n-\t\t\t\t\t\t\"but identical names are being used.\",\n-\t\t\t\t\t\te.getMessage(),\n-\t\t\t\t\t\toutputTag.getId()));\n-\n-\t\t\t\tthrow new ExceptionInChainedOperatorException(replace);\n-\n+\t\t\t\tif (outputTag != null) {\n+\t\t\t\t\t// Enrich error message\n+\t\t\t\t\tClassCastException replace = new ClassCastException(\n+\t\t\t\t\t\tString.format(\n+\t\t\t\t\t\t\t\"%s. Failed to push OutputTag with id '%s' to operator. \" +\n+\t\t\t\t\t\t\t\t\"This can occur when multiple OutputTags with different types \" +\n+\t\t\t\t\t\t\t\t\"but identical names are being used.\",\n+\t\t\t\t\t\t\te.getMessage(),\n+\t\t\t\t\t\t\toutputTag.getId()));\n+\n+\t\t\t\t\tthrow new ExceptionInChainedOperatorException(replace);\n+\t\t\t\t} else {\n+\t\t\t\t\tthrow new ExceptionInChainedOperatorException(e);\n+\t\t\t\t}\n \t\t\t} catch (Exception e) {\n \t\t\t\tthrow new ExceptionInChainedOperatorException(e);\n \t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/9e139a72ba45f2dd820bd3b9ecdf8428588666fd/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/OperatorChain.java",
                "sha": "f3c7293fe1b17e04a8057c56aa4c2af732ae68e5",
                "status": "modified"
            }
        ],
        "message": "[FLINK-8423] OperatorChain#pushToOperator catch block may fail with NPE\n\nThis closes #5447.",
        "parent": "https://github.com/apache/flink/commit/24c30878ed6f6ed1599a5ec23362055e0e88916f",
        "patched_files": [
            "OperatorChain.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "OperatorChainTest.java"
        ]
    },
    "flink_a0249d9": {
        "bug_id": "flink_a0249d9",
        "commit": "https://github.com/apache/flink/commit/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java?ref=a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3",
                "deletions": 2,
                "filename": "flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java",
                "patch": "@@ -149,7 +149,7 @@\n \tprivate final KinesisProxyInterface kinesis;\n \n \t/** Thread that executed runFetcher() */\n-\tprivate Thread mainThread;\n+\tprivate volatile Thread mainThread;\n \n \t/**\n \t * The current number of shards that are actively read by this fetcher.\n@@ -408,7 +408,10 @@ public void runFetcher() throws Exception {\n \t */\n \tpublic void shutdownFetcher() {\n \t\trunning = false;\n-\t\tmainThread.interrupt(); // the main thread may be sleeping for the discovery interval\n+\n+\t\tif (mainThread != null) {\n+\t\t\tmainThread.interrupt(); // the main thread may be sleeping for the discovery interval\n+\t\t}\n \n \t\tif (LOG.isInfoEnabled()) {\n \t\t\tLOG.info(\"Shutting down the shard consumer threads of subtask {} ...\", indexOfThisConsumerSubtask);",
                "raw_url": "https://github.com/apache/flink/raw/a0249d9935d54fbd6bb6c2cc130f51ce2ccafac3/flink-connectors/flink-connector-kinesis/src/main/java/org/apache/flink/streaming/connectors/kinesis/internals/KinesisDataFetcher.java",
                "sha": "8f7ca6c40f1e4871054374ae6b49797a665fdb15",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6311] [kinesis] NPE in FlinkKinesisConsumer if source was closed before run\n\nThis closes #3738.",
        "parent": "https://github.com/apache/flink/commit/42328bd9b7f216e4c3aae2086b822b4a3a564970",
        "patched_files": [
            "KinesisDataFetcher.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "KinesisDataFetcherTest.java"
        ]
    },
    "flink_a355df6": {
        "bug_id": "flink_a355df6",
        "commit": "https://github.com/apache/flink/commit/a355df6e33f402beac01c2908cb0c64cfeccadb2",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/a355df6e33f402beac01c2908cb0c64cfeccadb2/flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/library/CommunityDetection.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/library/CommunityDetection.java?ref=a355df6e33f402beac01c2908cb0c64cfeccadb2",
                "deletions": 1,
                "filename": "flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/library/CommunityDetection.java",
                "patch": "@@ -144,7 +144,7 @@ public void updateVertex(Vertex<K, Tuple2<Long, Double>> vertex,\n \n \t\t\tif (receivedLabelsWithScores.size() > 0) {\n \t\t\t\t// find the label with the highest score from the ones received\n-\t\t\t\tdouble maxScore = Double.MIN_VALUE;\n+\t\t\t\tdouble maxScore = -Double.MAX_VALUE;\n \t\t\t\tlong maxScoreLabel = vertex.getValue().f0;\n \t\t\t\tfor (long curLabel : receivedLabelsWithScores.keySet()) {\n ",
                "raw_url": "https://github.com/apache/flink/raw/a355df6e33f402beac01c2908cb0c64cfeccadb2/flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/library/CommunityDetection.java",
                "sha": "26291055b9a794b1cec50d641c0f5129f031c0cf",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/flink/blob/a355df6e33f402beac01c2908cb0c64cfeccadb2/flink-libraries/flink-gelly/src/test/java/org/apache/flink/graph/library/CommunityDetectionTest.java",
                "changes": 116,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-gelly/src/test/java/org/apache/flink/graph/library/CommunityDetectionTest.java?ref=a355df6e33f402beac01c2908cb0c64cfeccadb2",
                "deletions": 0,
                "filename": "flink-libraries/flink-gelly/src/test/java/org/apache/flink/graph/library/CommunityDetectionTest.java",
                "patch": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.flink.graph.library;\n+\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.graph.Edge;\n+import org.apache.flink.graph.Graph;\n+import org.apache.flink.graph.Vertex;\n+import org.apache.flink.graph.asm.AsmTestBase;\n+import org.apache.flink.graph.asm.dataset.ChecksumHashCode;\n+import org.apache.flink.graph.asm.dataset.ChecksumHashCode.Checksum;\n+import org.apache.flink.graph.generator.SingletonEdgeGraph;\n+import org.apache.flink.test.util.TestBaseUtils;\n+import org.apache.flink.types.IntValue;\n+import org.apache.flink.types.LongValue;\n+\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Tests for {@link CommunityDetection}.\n+ */\n+public class CommunityDetectionTest extends AsmTestBase {\n+\n+\t@Test\n+\tpublic void testWithSimpleGraph() throws Exception {\n+\t\tGraph<IntValue, Long, Double> result = undirectedSimpleGraph\n+\t\t\t.mapVertices(v -> (long) v.getId().getValue(),\n+\t\t\t\tnew TypeHint<Vertex<IntValue, Long>>(){}.getTypeInfo())\n+\t\t\t.mapEdges(e -> (double) e.getTarget().getValue() + e.getSource().getValue(),\n+\t\t\t\tnew TypeHint<Edge<IntValue, Double>>(){}.getTypeInfo())\n+\t\t\t.run(new CommunityDetection<>(10, 0.5));\n+\n+\t\tString expectedResult =\n+\t\t\t\"(0,3)\\n\" +\n+\t\t\t\"(1,5)\\n\" +\n+\t\t\t\"(2,5)\\n\" +\n+\t\t\t\"(3,3)\\n\" +\n+\t\t\t\"(4,5)\\n\" +\n+\t\t\t\"(5,5)\\n\";\n+\n+\t\tTestBaseUtils.compareResultAsText(result.getVertices().collect(), expectedResult);\n+\t}\n+\n+\t@Test\n+\tpublic void testWithSingletonEdgeGraph() throws Exception {\n+\t\tGraph<LongValue, Long, Double> result = new SingletonEdgeGraph(env, 1)\n+\t\t\t.generate()\n+\t\t\t.mapVertices(v -> v.getId().getValue(),\n+\t\t\t\tnew TypeHint<Vertex<LongValue, Long>>(){}.getTypeInfo())\n+\t\t\t.mapEdges(e -> 1.0,\n+\t\t\t\tnew TypeHint<Edge<LongValue, Double>>(){}.getTypeInfo())\n+\t\t\t.run(new CommunityDetection<>(10, 0.5));\n+\n+\t\tString expectedResult =\n+\t\t\t\"(0,0)\\n\" +\n+\t\t\t\"(1,1)\\n\";\n+\n+\t\tTestBaseUtils.compareResultAsText(result.getVertices().collect(), expectedResult);\n+\t}\n+\n+\t@Test\n+\tpublic void testWithEmptyGraphWithVertices() throws Exception {\n+\t\temptyGraphWithVertices\n+\t\t\t.mapVertices(v -> 0L,\n+\t\t\t\tnew TypeHint<Vertex<LongValue, Long>>(){}.getTypeInfo())\n+\t\t\t.mapEdges(e -> 0.0,\n+\t\t\t\tnew TypeHint<Edge<LongValue, Double>>(){}.getTypeInfo())\n+\t\t\t.run(new CommunityDetection<>(10, 0.5));\n+\t}\n+\n+\t@Test\n+\tpublic void testWithEmptyGraphWithoutVertices() throws Exception {\n+\t\temptyGraphWithoutVertices\n+\t\t\t.mapVertices(v -> 0L,\n+\t\t\t\tnew TypeHint<Vertex<LongValue, Long>>(){}.getTypeInfo())\n+\t\t\t.mapEdges(e -> 0.0,\n+\t\t\t\tnew TypeHint<Edge<LongValue, Double>>(){}.getTypeInfo())\n+\t\t\t.run(new CommunityDetection<>(10, 0.5));\n+\t}\n+\n+\t@Test\n+\tpublic void testWithRMatGraph() throws Exception {\n+\t\tGraph<LongValue, Long, Double> result = undirectedRMatGraph(8, 4)\n+\t\t\t.mapVertices(v -> v.getId().getValue(),\n+\t\t\t\tnew TypeHint<Vertex<LongValue, Long>>(){}.getTypeInfo())\n+\t\t\t.mapEdges(e -> (double) e.getTarget().getValue() - e.getSource().getValue(),\n+\t\t\t\tnew TypeHint<Edge<LongValue, Double>>(){}.getTypeInfo())\n+\t\t\t.run(new CommunityDetection<>(10, 0.5));\n+\n+\t\tChecksum checksum = new ChecksumHashCode<Vertex<LongValue, Long>>()\n+\t\t\t.run(result.getVertices())\n+\t\t\t.execute();\n+\n+\t\tassertEquals(184, checksum.getCount());\n+\t\tassertEquals(0x00000000000cdc96L, checksum.getChecksum());\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/a355df6e33f402beac01c2908cb0c64cfeccadb2/flink-libraries/flink-gelly/src/test/java/org/apache/flink/graph/library/CommunityDetectionTest.java",
                "sha": "cbabcfebe9e2e2c5814865baf722626bc5060705",
                "status": "added"
            }
        ],
        "message": "[FLINK-5506] [gelly] Fix CommunityDetection NullPointerException\n\nDouble.MIN_VALUE != min(double)\n\nThis closes #5126",
        "parent": "https://github.com/apache/flink/commit/1a98e327ea504f1422935c12a3342997145b9292",
        "patched_files": [
            "CommunityDetection.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "CommunityDetectionTest.java"
        ]
    },
    "flink_a442eb6": {
        "bug_id": "flink_a442eb6",
        "commit": "https://github.com/apache/flink/commit/a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/high_availability_zookeeper_configuration.html",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/high_availability_zookeeper_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 1,
                "filename": "docs/_includes/generated/high_availability_zookeeper_configuration.html",
                "patch": "@@ -60,7 +60,7 @@\n         <tr>\n             <td><h5>high-availability.zookeeper.path.mesos-workers</h5></td>\n             <td style=\"word-wrap: break-word;\">\"/mesos-workers\"</td>\n-            <td>ZooKeeper root path (ZNode) for Mesos workers.</td>\n+            <td>The ZooKeeper root path for persisting the Mesos worker information.</td>\n         </tr>\n         <tr>\n             <td><h5>high-availability.zookeeper.path.root</h5></td>",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/high_availability_zookeeper_configuration.html",
                "sha": "6577878674b3de994b64bd5f540ce14ba1e4d249",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_configuration.html",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/mesos_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 4,
                "filename": "docs/_includes/generated/mesos_configuration.html",
                "patch": "@@ -15,17 +15,17 @@\n         <tr>\n             <td><h5>mesos.initial-tasks</h5></td>\n             <td style=\"word-wrap: break-word;\">0</td>\n-            <td>The initial workers to bring up when the master starts</td>\n+            <td>The initial workers to bring up when the master starts. This option is ignored unless Flink is in <a href=\"#legacy\">legacy mode</a>.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.master</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>The Mesos master URL. The value should be in one of the following forms: \"host:port\", \"zk://host1:port1,host2:port2,.../path\", \"zk://username:password@host1:port1,host2:port2,.../path\" or \"file:///path/to/file\"</td>\n+            <td>The Mesos master URL. The value should be in one of the following forms: <ul><li>host:port</li><li>zk://host1:port1,host2:port2,.../path</li><li>zk://username:password@host1:port1,host2:port2,.../path</li><li>file:///path/to/file</li></ul></td>\n         </tr>\n         <tr>\n             <td><h5>mesos.maximum-failed-tasks</h5></td>\n             <td style=\"word-wrap: break-word;\">-1</td>\n-            <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature</td>\n+            <td>The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. This option is ignored unless Flink is in <a href=\"#legacy\">legacy mode</a>.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.artifactserver.port</h5></td>\n@@ -65,7 +65,7 @@\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.port-assignments</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>Comma-separated list of configuration keys which represent a configurable port.All port keys will dynamically get a port assigned through Mesos.</td>\n+            <td>Comma-separated list of configuration keys which represent a configurable port. All port keys will dynamically get a port assigned through Mesos.</td>\n         </tr>\n     </tbody>\n </table>",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_configuration.html",
                "sha": "54e92e5680c51e766cc07c1a976eb52aca573280",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_task_manager_configuration.html",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/_includes/generated/mesos_task_manager_configuration.html?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 4,
                "filename": "docs/_includes/generated/mesos_task_manager_configuration.html",
                "patch": "@@ -10,12 +10,12 @@\n         <tr>\n             <td><h5>mesos.constraints.hard.hostattribute</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td>Constraints for task placement on mesos.</td>\n+            <td>Constraints for task placement on Mesos based on agent attributes. Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. Example: az:eu-west-1a,series:t2</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.bootstrap-cmd</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td></td>\n+            <td>A command which is executed before the TaskManager is started.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.container.docker.force-pull-image</h5></td>\n@@ -50,12 +50,12 @@\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.gpus</h5></td>\n             <td style=\"word-wrap: break-word;\">0</td>\n-            <td></td>\n+            <td>GPUs to assign to the Mesos workers.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.hostname</h5></td>\n             <td style=\"word-wrap: break-word;\">(none)</td>\n-            <td></td>\n+            <td>Optional value to define the TaskManager\u2019s hostname. The pattern _TASK_ is replaced by the actual id of the Mesos task. This can be used to configure the TaskManager to use Mesos DNS (e.g. _TASK_.flink-service.mesos) for name lookups.</td>\n         </tr>\n         <tr>\n             <td><h5>mesos.resourcemanager.tasks.mem</h5></td>",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/_includes/generated/mesos_task_manager_configuration.html",
                "sha": "1e67f8429d74665ac0033d38fd26a330f2c4a892",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/ops/deployment/mesos.md",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/docs/ops/deployment/mesos.md?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 69,
                "filename": "docs/ops/deployment/mesos.md",
                "patch": "@@ -59,13 +59,11 @@ or configuration files. For instance, in non-containerized environments, the\n artifact server will provide the Flink binaries. What files will be served\n depends on the configuration overlay used.\n \n-### Flink's JobManager and Web Interface\n+### Flink's Dispatcher and Web Interface\n \n-The Mesos scheduler currently resides with the JobManager but will be started\n-independently of the JobManager in future versions (see\n-[FLIP-6](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077)). The\n-proposed changes will also add a Dispatcher component which will be the central\n-point for job submission and monitoring.\n+The Dispatcher and the web interface provide a central point for monitoring,\n+job submission, and other client interaction with the cluster\n+(see [FLIP-6](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65147077)).\n \n ### Startup script and configuration overlays\n \n@@ -139,7 +137,7 @@ More information about the deployment scripts can be found [here](http://mesos.a\n \n ### Installing Marathon\n \n-Optionally, you may also [install Marathon](https://mesosphere.github.io/marathon/docs/) which will be necessary to run Flink in high availability (HA) mode.\n+Optionally, you may also [install Marathon](https://mesosphere.github.io/marathon/docs/) which enables you to run Flink in [high availability (HA) mode](#high-availability).\n \n ### Pre-installing Flink vs Docker/Mesos containers\n \n@@ -171,8 +169,6 @@ which manage the Flink processes in a Mesos cluster:\n    It is automatically launched by the Mesos worker node to bring up a new TaskManager.\n \n In order to run the `mesos-appmaster.sh` script you have to define `mesos.master` in the `flink-conf.yaml` or pass it via `-Dmesos.master=...` to the Java process.\n-Additionally, you should define the number of task managers which are started by Mesos via `mesos.initial-tasks`.\n-This value can also be defined in the `flink-conf.yaml` or passed as a Java property.\n \n When executing `mesos-appmaster.sh`, it will create a job manager on the machine where you executed the script.\n In contrast to that, the task managers will be run as Mesos tasks in the Mesos cluster.\n@@ -188,19 +184,21 @@ For example:\n         -Djobmanager.heap.mb=1024 \\\n         -Djobmanager.rpc.port=6123 \\\n         -Drest.port=8081 \\\n-        -Dmesos.initial-tasks=10 \\\n         -Dmesos.resourcemanager.tasks.mem=4096 \\\n         -Dtaskmanager.heap.mb=3500 \\\n         -Dtaskmanager.numberOfTaskSlots=2 \\\n         -Dparallelism.default=10\n \n+<div class=\"alert alert-info\">\n+  <strong>Note:</strong> If Flink is in <a href=\"{{ site.baseurl }}/ops/config.html#legacy\">legacy mode</a>,\n+  you should additionally define the number of task managers that are started by Mesos via\n+  <a href=\"{{ site.baseurl }}/ops/config.html#mesos-initial-tasks\"><code>mesos.initial-tasks</code></a>.\n+</div>\n \n ### High Availability\n \n You will need to run a service like Marathon or Apache Aurora which takes care of restarting the Flink master process in case of node or process failures.\n-In addition, Zookeeper needs to be configured like described in the [High Availability section of the Flink docs]({{ site.baseurl }}/ops/jobmanager_high_availability.html)\n-\n-For the reconciliation of tasks to work correctly, please also set `high-availability.zookeeper.path.mesos-workers` to a valid Zookeeper path.\n+In addition, Zookeeper needs to be configured like described in the [High Availability section of the Flink docs]({{ site.baseurl }}/ops/jobmanager_high_availability.html).\n \n #### Marathon\n \n@@ -211,7 +209,7 @@ Here is an example configuration for Marathon:\n \n     {\n         \"id\": \"flink\",\n-        \"cmd\": \"$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.mb=1024 -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.initial-tasks=1 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.heap.mb=1024 -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1\",\n+        \"cmd\": \"$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.mb=1024 -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.heap.mb=1024 -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1\",\n         \"cpus\": 1.0,\n         \"mem\": 1024\n     }\n@@ -220,60 +218,7 @@ When running Flink with Marathon, the whole Flink cluster including the job mana\n \n ### Configuration parameters\n \n-`mesos.initial-tasks`: The initial workers to bring up when the master starts (**DEFAULT**: The number of workers specified at cluster startup).\n-\n-`mesos.constraints.hard.hostattribute`: Constraints for task placement on Mesos based on agent attributes (**DEFAULT**: None).\n-Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target\n-mesos agents.  Example: `az:eu-west-1a,series:t2`\n-\n-`mesos.maximum-failed-tasks`: The maximum number of failed workers before the cluster fails (**DEFAULT**: Number of initial workers).\n-May be set to -1 to disable this feature.\n-\n-`mesos.master`: The Mesos master URL. The value should be in one of the following forms:\n-\n-* `host:port`\n-* `zk://host1:port1,host2:port2,.../path`\n-* `zk://username:password@host1:port1,host2:port2,.../path`\n-* `file:///path/to/file`\n-\n-`mesos.failover-timeout`: The failover timeout in seconds for the Mesos scheduler, after which running tasks are automatically shut down (**DEFAULT:** 600).\n-\n-`mesos.resourcemanager.artifactserver.port`:The config parameter defining the Mesos artifact server port to use. Setting the port to 0 will let the OS choose an available port.\n-\n-`mesos.resourcemanager.framework.name`: Mesos framework name (**DEFAULT:** Flink)\n-\n-`mesos.resourcemanager.framework.role`: Mesos framework role definition (**DEFAULT:** *)\n-\n-`high-availability.zookeeper.path.mesos-workers`: The ZooKeeper root path for persisting the Mesos worker information.\n-\n-`mesos.resourcemanager.framework.principal`: Mesos framework principal (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.framework.secret`: Mesos framework secret (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.framework.user`: Mesos framework user (**DEFAULT:**\"\")\n-\n-`mesos.resourcemanager.artifactserver.ssl.enabled`: Enables SSL for the Flink artifact server (**DEFAULT**: true). Note that `security.ssl.enabled` also needs to be set to `true` encryption to enable encryption.\n-\n-`mesos.resourcemanager.tasks.mem`: Memory to assign to the Mesos workers in MB (**DEFAULT**: 1024)\n-\n-`mesos.resourcemanager.tasks.cpus`: CPUs to assign to the Mesos workers (**DEFAULT**: 0.0)\n-\n-`mesos.resourcemanager.tasks.gpus`: GPUs to assign to the Mesos workers (**DEFAULT**: 0.0)\n-\n-`mesos.resourcemanager.tasks.container.type`: Type of the containerization used: \"mesos\" or \"docker\" (DEFAULT: mesos);\n-\n-`mesos.resourcemanager.tasks.container.image.name`: Image name to use for the container (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.volumes`: A comma separated list of `[host_path:]`container_path`[:RO|RW]`. This allows for mounting additional volumes into your container. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.docker.parameters`: Custom parameters to be passed into docker run command when using the docker containerizer. Comma separated list of `key=value` pairs. `value` may contain '=' (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.uris`: A comma separated list of URIs of custom artifacts to be downloaded into the sandbox of Mesos workers. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.container.docker.force-pull-image`: Instruct the docker containerizer to forcefully pull the image rather than reuse a cached version. (**DEFAULT**: false)\n-\n-`mesos.resourcemanager.tasks.hostname`: Optional value to define the TaskManager's hostname. The pattern `_TASK_` is replaced by the actual id of the Mesos task. This can be used to configure the TaskManager to use Mesos DNS (e.g. `_TASK_.flink-service.mesos`) for name lookups. (**NO DEFAULT**)\n-\n-`mesos.resourcemanager.tasks.bootstrap-cmd`: A command which is executed before the TaskManager is started (**NO DEFAULT**).\n+For a list of Mesos specific configuration, refer to the [Mesos section]({{ site.baseurl }}/ops/config.html#mesos)\n+of the configuration documentation.\n \n {% top %}",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/docs/ops/deployment/mesos.md",
                "sha": "1ff8afad74ebb5edd81836abe3e4e1d53b78e21c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 1,
                "filename": "flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.annotation.docs.ConfigGroup;\n import org.apache.flink.annotation.docs.ConfigGroups;\n import org.apache.flink.annotation.docs.Documentation;\n+import org.apache.flink.configuration.description.Description;\n \n import static org.apache.flink.configuration.ConfigOptions.key;\n \n@@ -157,7 +158,9 @@\n \t\t\tkey(\"high-availability.zookeeper.path.mesos-workers\")\n \t\t\t.defaultValue(\"/mesos-workers\")\n \t\t\t.withDeprecatedKeys(\"recovery.zookeeper.path.mesos-workers\")\n-\t\t\t.withDescription(\"ZooKeeper root path (ZNode) for Mesos workers.\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The ZooKeeper root path for persisting the Mesos worker information.\")\n+\t\t\t\t.build());\n \n \t// ------------------------------------------------------------------------\n \t//  ZooKeeper Client Settings",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java",
                "sha": "787efffa3ede1004219cfd4d5192cb75c4e7b692",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 8,
                "filename": "flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java",
                "patch": "@@ -19,6 +19,9 @@\n package org.apache.flink.mesos.configuration;\n \n import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.description.Description;\n+import org.apache.flink.configuration.description.LinkElement;\n+import org.apache.flink.configuration.description.TextElement;\n \n import static org.apache.flink.configuration.ConfigOptions.key;\n \n@@ -33,7 +36,10 @@\n \tpublic static final ConfigOption<Integer> INITIAL_TASKS =\n \t\tkey(\"mesos.initial-tasks\")\n \t\t\t.defaultValue(0)\n-\t\t\t.withDescription(\"The initial workers to bring up when the master starts\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The initial workers to bring up when the master starts. \")\n+\t\t\t\t.text(\"This option is ignored unless Flink is in %s.\", LinkElement.link(\"#legacy\", \"legacy mode\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The maximum number of failed Mesos tasks before entirely stopping\n@@ -44,8 +50,10 @@\n \tpublic static final ConfigOption<Integer> MAX_FAILED_TASKS =\n \t\tkey(\"mesos.maximum-failed-tasks\")\n \t\t\t.defaultValue(-1)\n-\t\t\t.withDescription(\"The maximum number of failed workers before the cluster fails. May be set to -1 to disable\" +\n-\t\t\t\t\" this feature\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The maximum number of failed workers before the cluster fails. May be set to -1 to disable this feature. \")\n+\t\t\t\t.text(\"This option is ignored unless Flink is in %s.\", LinkElement.link(\"#legacy\", \"legacy mode\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The Mesos master URL.\n@@ -63,9 +71,14 @@\n \tpublic static final ConfigOption<String> MASTER_URL =\n \t\tkey(\"mesos.master\")\n \t\t\t.noDefaultValue()\n-\t\t\t.withDescription(\"The Mesos master URL. The value should be in one of the following forms:\" +\n-\t\t\t\t\" \\\"host:port\\\", \\\"zk://host1:port1,host2:port2,.../path\\\",\" +\n-\t\t\t\t\" \\\"zk://username:password@host1:port1,host2:port2,.../path\\\" or \\\"file:///path/to/file\\\"\");\n+\t\t\t.withDescription(Description.builder()\n+\t\t\t\t.text(\"The Mesos master URL. The value should be in one of the following forms: \")\n+\t\t\t\t.list(\n+\t\t\t\t\tTextElement.text(\"host:port\"),\n+\t\t\t\t\tTextElement.text(\"zk://host1:port1,host2:port2,.../path\"),\n+\t\t\t\t\tTextElement.text(\"zk://username:password@host1:port1,host2:port2,.../path\"),\n+\t\t\t\t\tTextElement.text(\"file:///path/to/file\"))\n+\t\t\t\t.build());\n \n \t/**\n \t * The failover timeout for the Mesos scheduler, after which running tasks are automatically shut down.\n@@ -125,7 +138,9 @@\n \t */\n \tpublic static final ConfigOption<String> PORT_ASSIGNMENTS = key(\"mesos.resourcemanager.tasks.port-assignments\")\n \t\t.defaultValue(\"\")\n-\t\t.withDescription(\"Comma-separated list of configuration keys which represent a configurable port.\" +\n-\t\t\t\"All port keys will dynamically get a port assigned through Mesos.\");\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Comma-separated list of configuration keys which represent a configurable port. \" +\n+\t\t\t\t\"All port keys will dynamically get a port assigned through Mesos.\")\n+\t\t\t.build());\n \n }",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/configuration/MesosOptions.java",
                "sha": "426a891e814237e410169e04c8210e2b4da9667b",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/flink/blob/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java?ref=a442eb6c0388558c6fb2e5e616cd1cd15038b95c",
                "deletions": 4,
                "filename": "flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.IllegalConfigurationException;\n import org.apache.flink.configuration.TaskManagerOptions;\n+import org.apache.flink.configuration.description.Description;\n import org.apache.flink.runtime.clusterframework.ContaineredTaskManagerParameters;\n import org.apache.flink.util.Preconditions;\n \n@@ -65,7 +66,8 @@\n \n \tpublic static final ConfigOption<Integer> MESOS_RM_TASKS_GPUS =\n \t\tkey(\"mesos.resourcemanager.tasks.gpus\")\n-\t\t.defaultValue(0);\n+\t\t.defaultValue(0)\n+\t\t.withDescription(Description.builder().text(\"GPUs to assign to the Mesos workers.\").build());\n \n \tpublic static final ConfigOption<String> MESOS_RM_CONTAINER_TYPE =\n \t\tkey(\"mesos.resourcemanager.tasks.container.type\")\n@@ -79,15 +81,23 @@\n \n \tpublic static final ConfigOption<String> MESOS_TM_HOSTNAME =\n \t\tkey(\"mesos.resourcemanager.tasks.hostname\")\n-\t\t.noDefaultValue();\n+\t\t.noDefaultValue()\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Optional value to define the TaskManager\u2019s hostname. \" +\n+\t\t\t\t\"The pattern _TASK_ is replaced by the actual id of the Mesos task. \" +\n+\t\t\t\t\"This can be used to configure the TaskManager to use Mesos DNS (e.g. _TASK_.flink-service.mesos) for name lookups.\")\n+\t\t\t.build());\n \n \tpublic static final ConfigOption<String> MESOS_TM_CMD =\n \t\tkey(\"mesos.resourcemanager.tasks.taskmanager-cmd\")\n \t\t.defaultValue(\"$FLINK_HOME/bin/mesos-taskmanager.sh\"); // internal\n \n \tpublic static final ConfigOption<String> MESOS_TM_BOOTSTRAP_CMD =\n \t\tkey(\"mesos.resourcemanager.tasks.bootstrap-cmd\")\n-\t\t.noDefaultValue();\n+\t\t.noDefaultValue()\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"A command which is executed before the TaskManager is started.\")\n+\t\t\t.build());\n \n \tpublic static final ConfigOption<String> MESOS_TM_URIS =\n \t\tkey(\"mesos.resourcemanager.tasks.uris\")\n@@ -116,7 +126,11 @@\n \tpublic static final ConfigOption<String> MESOS_CONSTRAINTS_HARD_HOSTATTR =\n \t\tkey(\"mesos.constraints.hard.hostattribute\")\n \t\t.noDefaultValue()\n-\t\t.withDescription(\"Constraints for task placement on mesos.\");\n+\t\t.withDescription(Description.builder()\n+\t\t\t.text(\"Constraints for task placement on Mesos based on agent attributes. \" +\n+\t\t\t\t\"Takes a comma-separated list of key:value pairs corresponding to the attributes exposed by the target mesos agents. \" +\n+\t\t\t\t\"Example: az:eu-west-1a,series:t2\")\n+\t\t\t.build());\n \n \t/**\n \t * Value for {@code MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE} setting. Tells to use the Mesos containerizer.",
                "raw_url": "https://github.com/apache/flink/raw/a442eb6c0388558c6fb2e5e616cd1cd15038b95c/flink-mesos/src/main/java/org/apache/flink/mesos/runtime/clusterframework/MesosTaskManagerParameters.java",
                "sha": "03156297188617e6670e72900264658fe6ec78e4",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9795][mesos, docs] Update Mesos documentation\n\n[FLINK-9795][mesos, docs] Remove unnecessary remark about task reconciliation.\n\nThe config key high-availability.zookeeper.path.mesos-workers already has a\ndefault value. Even without explicitly setting the key, the task reconciliation\nwill work. Moreover, if there would not be a default key, the code would throw an NPE. So\neither way, the remark is only confusing the reader.\n\n[FLINK-9795][mesos, docs] Remove configuration keys from Mesos Setup page.\n\n- Remove the Mesos specific configuration keys from the Mesos Setup page because\nthey duplicate what is already on the configuration page.\n- Add missing descriptions for some of the keys that are under the Mesos section of the configuration\npage.\n- Improve formatting of the descriptions.\n\n[FLINK-9795][mesos, docs] Document which config options are only used in legacy mode.\n\n[FLINK-9795][mesos, docs] Document that mesos.initial-tasks is only needed in legacy mode.\n\n[FLINK-9795][mesos, docs] Clarify necessity of Marathon in documentation.\n\n[FLINK-9795][mesos, docs] Rewrite \"Flink's JobManager and Web Interface\" section.\n\n[FLINK-9795][mesos, docs] Add missing period at the end of sentence.\n\nThis closes #6533.",
        "parent": "https://github.com/apache/flink/commit/04ba9a85920bcab6c7b6c001a58c7570e987aabb",
        "patched_files": [
            "MesosTaskManagerParameters.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "MesosTaskManagerParametersTest.java"
        ]
    },
    "flink_a532e51": {
        "bug_id": "flink_a532e51",
        "commit": "https://github.com/apache/flink/commit/a532e51d75aff1c46e87525416c9a7aed92c514c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/a532e51d75aff1c46e87525416c9a7aed92c514c/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/resettable/SpillingResettableIterator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/resettable/SpillingResettableIterator.java?ref=a532e51d75aff1c46e87525416c9a7aed92c514c",
                "deletions": 1,
                "filename": "pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/resettable/SpillingResettableIterator.java",
                "patch": "@@ -251,7 +251,7 @@ public T next() {\n \t}\n \n \tpublic void close() {\n-\t\tif (!fitsIntoMem) {\n+\t\tif (ioReader != null) {\n \t\t\ttry {\n \t\t\t\tioReader.close();\n \t\t\t\tioReader.deleteChannel();",
                "raw_url": "https://github.com/apache/flink/raw/a532e51d75aff1c46e87525416c9a7aed92c514c/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/resettable/SpillingResettableIterator.java",
                "sha": "b7b3d058b601cf59e217fb276c1d122e560eddac",
                "status": "modified"
            }
        ],
        "message": "- fixed NullPointerException in cancel behavior of SpillingResettableIterator",
        "parent": "https://github.com/apache/flink/commit/720d07454f37889f99beeb1588e127c7b08b3cc1",
        "patched_files": [
            "SpillingResettableIterator.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "SpillingResettableIteratorTest.java"
        ]
    },
    "flink_a95ec5a": {
        "bug_id": "flink_a95ec5a",
        "commit": "https://github.com/apache/flink/commit/a95ec5acf259884347ae539913bcffcad5bfc340",
        "file": [
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java",
                "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.jobmaster;\n+\n+import org.apache.flink.runtime.clusterframework.types.ResourceID;\n+import org.apache.flink.runtime.resourcemanager.ResourceManagerGateway;\n+import org.apache.flink.runtime.resourcemanager.ResourceManagerId;\n+\n+import javax.annotation.Nonnull;\n+\n+/**\n+ * Class which contains the connection details of an established\n+ * connection with the ResourceManager.\n+ */\n+class EstablishedResourceManagerConnection {\n+\n+\tprivate final ResourceManagerGateway resourceManagerGateway;\n+\n+\tprivate final ResourceManagerId resourceManagerId;\n+\n+\tprivate final ResourceID resourceManagerResourceID;\n+\n+\tEstablishedResourceManagerConnection(\n+\t\t\t@Nonnull ResourceManagerGateway resourceManagerGateway,\n+\t\t\t@Nonnull ResourceManagerId resourceManagerId,\n+\t\t\t@Nonnull ResourceID resourceManagerResourceID) {\n+\t\tthis.resourceManagerGateway = resourceManagerGateway;\n+\t\tthis.resourceManagerId = resourceManagerId;\n+\t\tthis.resourceManagerResourceID = resourceManagerResourceID;\n+\t}\n+\n+\tpublic ResourceManagerGateway getResourceManagerGateway() {\n+\t\treturn resourceManagerGateway;\n+\t}\n+\n+\tpublic ResourceManagerId getResourceManagerId() {\n+\t\treturn resourceManagerId;\n+\t}\n+\n+\tpublic ResourceID getResourceManagerResourceID() {\n+\t\treturn resourceManagerResourceID;\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/EstablishedResourceManagerConnection.java",
                "sha": "46c1b4bde7a3c0458bf49b046648fb352a5d156e",
                "status": "added"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340",
                "deletions": 35,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "patch": "@@ -191,9 +191,6 @@\n \n \tprivate LeaderRetrievalService resourceManagerLeaderRetriever;\n \n-\t@Nullable\n-\tprivate ResourceManagerConnection resourceManagerConnection;\n-\n \t// --------- TaskManagers --------\n \n \tprivate final Map<ResourceID, Tuple2<TaskManagerLocation, TaskExecutorGateway>> registeredTaskManagers;\n@@ -211,6 +208,12 @@\n \t@Nullable\n \tprivate String lastInternalSavepoint;\n \n+\t@Nullable\n+\tprivate ResourceManagerConnection resourceManagerConnection;\n+\n+\t@Nullable\n+\tprivate EstablishedResourceManagerConnection establishedResourceManagerConnection;\n+\n \t// ------------------------------------------------------------------------\n \n \tpublic JobMaster(\n@@ -290,6 +293,9 @@ public JobMaster(\n \t\tthis.jobManagerJobMetricGroup = jobMetricGroupFactory.create(jobGraph);\n \t\tthis.executionGraph = createAndRestoreExecutionGraph(jobManagerJobMetricGroup);\n \t\tthis.jobStatusListener = null;\n+\n+\t\tthis.resourceManagerConnection = null;\n+\t\tthis.establishedResourceManagerConnection = null;\n \t}\n \n \t//----------------------------------------------------------------------------------------------\n@@ -881,12 +887,16 @@ public void disconnectResourceManager(\n \t\t\tfinal ResourceManagerId resourceManagerId,\n \t\t\tfinal Exception cause) {\n \n-\t\tif (resourceManagerConnection != null\n-\t\t\t\t&& resourceManagerConnection.getTargetLeaderId().equals(resourceManagerId)) {\n+\t\tif (isConnectingToResourceManager(resourceManagerId)) {\n \t\t\tcloseResourceManagerConnection(cause);\n \t\t}\n \t}\n \n+\tprivate boolean isConnectingToResourceManager(ResourceManagerId resourceManagerId) {\n+\t\treturn resourceManagerConnection != null\n+\t\t\t\t&& resourceManagerConnection.getTargetLeaderId().equals(resourceManagerId);\n+\t}\n+\n \t@Override\n \tpublic void heartbeatFromTaskManager(final ResourceID resourceID, AccumulatorReport accumulatorReport) {\n \t\ttaskManagerHeartbeatManager.receiveHeartbeat(resourceID, accumulatorReport);\n@@ -1238,11 +1248,11 @@ private void notifyOfNewResourceManagerLeader(final String resourceManagerAddres\n \t\t\t\t\treturn;\n \t\t\t\t}\n \n-\t\t\t\tcloseResourceManagerConnection(new Exception(\n-\t\t\t\t\t\"ResourceManager leader changed to new address \" + resourceManagerAddress));\n-\n \t\t\t\tlog.info(\"ResourceManager leader changed from {} to {}. Registering at new leader.\",\n \t\t\t\t\tresourceManagerConnection.getTargetAddress(), resourceManagerAddress);\n+\n+\t\t\t\tcloseResourceManagerConnection(new Exception(\n+\t\t\t\t\t\"ResourceManager leader changed to new address \" + resourceManagerAddress));\n \t\t\t} else {\n \t\t\t\tlog.info(\"Current ResourceManager {} lost leader status. Waiting for new ResourceManager leader.\",\n \t\t\t\t\tresourceManagerConnection.getTargetAddress());\n@@ -1277,9 +1287,16 @@ private void establishResourceManagerConnection(final JobMasterRegistrationSucce\n \n \t\t\tfinal ResourceManagerGateway resourceManagerGateway = resourceManagerConnection.getTargetGateway();\n \n+\t\t\tfinal ResourceID resourceManagerResourceId = success.getResourceManagerResourceId();\n+\n+\t\t\testablishedResourceManagerConnection = new EstablishedResourceManagerConnection(\n+\t\t\t\tresourceManagerGateway,\n+\t\t\t\tsuccess.getResourceManagerId(),\n+\t\t\t\tresourceManagerResourceId);\n+\n \t\t\tslotPoolGateway.connectToResourceManager(resourceManagerGateway);\n \n-\t\t\tresourceManagerHeartbeatManager.monitorTarget(success.getResourceManagerResourceId(), new HeartbeatTarget<Void>() {\n+\t\t\tresourceManagerHeartbeatManager.monitorTarget(resourceManagerResourceId, new HeartbeatTarget<Void>() {\n \t\t\t\t@Override\n \t\t\t\tpublic void receiveHeartbeat(ResourceID resourceID, Void payload) {\n \t\t\t\t\tresourceManagerGateway.heartbeatFromJobManager(resourceID);\n@@ -1297,22 +1314,31 @@ public void requestHeartbeat(ResourceID resourceID, Void payload) {\n \t}\n \n \tprivate void closeResourceManagerConnection(Exception cause) {\n-\t\tif (resourceManagerConnection != null) {\n-\t\t\tif (log.isDebugEnabled()) {\n-\t\t\t\tlog.debug(\"Close ResourceManager connection {}.\", resourceManagerConnection.getResourceManagerResourceID(), cause);\n-\t\t\t} else {\n-\t\t\t\tlog.info(\"Close ResourceManager connection {}: {}.\", resourceManagerConnection.getResourceManagerResourceID(), cause.getMessage());\n-\t\t\t}\n-\n-\t\t\tresourceManagerHeartbeatManager.unmonitorTarget(resourceManagerConnection.getResourceManagerResourceID());\n-\n-\t\t\tResourceManagerGateway resourceManagerGateway = resourceManagerConnection.getTargetGateway();\n-\t\t\tresourceManagerGateway.disconnectJobManager(resourceManagerConnection.getJobID(), cause);\n+\t\tif (establishedResourceManagerConnection != null) {\n+\t\t\tdissolveResourceManagerConnection(establishedResourceManagerConnection, cause);\n+\t\t\testablishedResourceManagerConnection = null;\n+\t\t}\n \n+\t\tif (resourceManagerConnection != null) {\n+\t\t\t// stop a potentially ongoing registration process\n \t\t\tresourceManagerConnection.close();\n \t\t\tresourceManagerConnection = null;\n \t\t}\n+\t}\n+\n+\tprivate void dissolveResourceManagerConnection(EstablishedResourceManagerConnection establishedResourceManagerConnection, Exception cause) {\n+\t\tfinal ResourceID resourceManagerResourceID = establishedResourceManagerConnection.getResourceManagerResourceID();\n \n+\t\tif (log.isDebugEnabled()) {\n+\t\t\tlog.debug(\"Close ResourceManager connection {}.\", resourceManagerResourceID, cause);\n+\t\t} else {\n+\t\t\tlog.info(\"Close ResourceManager connection {}: {}.\", resourceManagerResourceID, cause.getMessage());\n+\t\t}\n+\n+\t\tresourceManagerHeartbeatManager.unmonitorTarget(resourceManagerResourceID);\n+\n+\t\tResourceManagerGateway resourceManagerGateway = establishedResourceManagerConnection.getResourceManagerGateway();\n+\t\tresourceManagerGateway.disconnectJobManager(jobGraph.getJobID(), cause);\n \t\tslotPoolGateway.disconnectResourceManager();\n \t}\n \n@@ -1473,8 +1499,6 @@ public void handleError(final Exception exception) {\n \n \t\tprivate final JobMasterId jobMasterId;\n \n-\t\tprivate ResourceID resourceManagerResourceID;\n-\n \t\tResourceManagerConnection(\n \t\t\t\tfinal Logger log,\n \t\t\t\tfinal JobID jobID,\n@@ -1498,7 +1522,7 @@ public void handleError(final Exception exception) {\n \t\t\t\t\tgetTargetAddress(), getTargetLeaderId()) {\n \t\t\t\t@Override\n \t\t\t\tprotected CompletableFuture<RegistrationResponse> invokeRegistration(\n-\t\t\t\t\t\tResourceManagerGateway gateway, ResourceManagerId fencingToken, long timeoutMillis) throws Exception {\n+\t\t\t\t\t\tResourceManagerGateway gateway, ResourceManagerId fencingToken, long timeoutMillis) {\n \t\t\t\t\tTime timeout = Time.milliseconds(timeoutMillis);\n \n \t\t\t\t\treturn gateway.registerJobManager(\n@@ -1513,24 +1537,13 @@ public void handleError(final Exception exception) {\n \n \t\t@Override\n \t\tprotected void onRegistrationSuccess(final JobMasterRegistrationSuccess success) {\n-\t\t\trunAsync(() -> {\n-\t\t\t\tresourceManagerResourceID = success.getResourceManagerResourceId();\n-\t\t\t\testablishResourceManagerConnection(success);\n-\t\t\t});\n+\t\t\trunAsync(() -> establishResourceManagerConnection(success));\n \t\t}\n \n \t\t@Override\n \t\tprotected void onRegistrationFailure(final Throwable failure) {\n \t\t\thandleJobMasterError(failure);\n \t\t}\n-\n-\t\tpublic ResourceID getResourceManagerResourceID() {\n-\t\t\treturn resourceManagerResourceID;\n-\t\t}\n-\n-\t\tpublic JobID getJobID() {\n-\t\t\treturn jobID;\n-\t\t}\n \t}\n \n \t//----------------------------------------------------------------------------------------------",
                "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
                "sha": "aff32808dd99070afac0e475efd0381734f40a6a",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/flink/blob/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java?ref=a95ec5acf259884347ae539913bcffcad5bfc340",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.flink.api.java.tuple.Tuple3;\n import org.apache.flink.configuration.BlobServerOptions;\n import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.testutils.OneShotLatch;\n import org.apache.flink.runtime.blob.BlobServer;\n import org.apache.flink.runtime.blob.VoidBlobStore;\n import org.apache.flink.runtime.checkpoint.CheckpointProperties;\n@@ -361,6 +362,53 @@ public void testCheckpointPrecedesSavepointRecovery() throws Exception {\n \t\t}\n \t}\n \n+\t/**\n+\t * Tests that we can close an unestablished ResourceManager connection.\n+\t */\n+\t@Test\n+\tpublic void testCloseUnestablishedResourceManagerConnection() throws Exception {\n+\t\tfinal JobMaster jobMaster = createJobMaster(\n+\t\t\tJobMasterConfiguration.fromConfiguration(configuration),\n+\t\t\tjobGraph,\n+\t\t\thaServices,\n+\t\t\tnew TestingJobManagerSharedServicesBuilder().build());\n+\n+\t\ttry {\n+\t\t\tjobMaster.start(JobMasterId.generate(), testingTimeout).get();\n+\t\t\tfinal ResourceManagerId resourceManagerId = ResourceManagerId.generate();\n+\t\t\tfinal String firstResourceManagerAddress = \"address1\";\n+\t\t\tfinal String secondResourceManagerAddress = \"address2\";\n+\n+\t\t\tfinal TestingResourceManagerGateway firstResourceManagerGateway = new TestingResourceManagerGateway();\n+\t\t\tfinal TestingResourceManagerGateway secondResourceManagerGateway = new TestingResourceManagerGateway();\n+\n+\t\t\trpcService.registerGateway(firstResourceManagerAddress, firstResourceManagerGateway);\n+\t\t\trpcService.registerGateway(secondResourceManagerAddress, secondResourceManagerGateway);\n+\n+\t\t\tfinal OneShotLatch firstJobManagerRegistration = new OneShotLatch();\n+\t\t\tfinal OneShotLatch secondJobManagerRegistration = new OneShotLatch();\n+\n+\t\t\tfirstResourceManagerGateway.setRegisterJobManagerConsumer(\n+\t\t\t\tjobMasterIdResourceIDStringJobIDTuple4 -> firstJobManagerRegistration.trigger());\n+\n+\t\t\tsecondResourceManagerGateway.setRegisterJobManagerConsumer(\n+\t\t\t\tjobMasterIdResourceIDStringJobIDTuple4 -> secondJobManagerRegistration.trigger());\n+\n+\t\t\trmLeaderRetrievalService.notifyListener(firstResourceManagerAddress, resourceManagerId.toUUID());\n+\n+\t\t\t// wait until we have seen the first registration attempt\n+\t\t\tfirstJobManagerRegistration.await();\n+\n+\t\t\t// this should stop the connection attempts towards the first RM\n+\t\t\trmLeaderRetrievalService.notifyListener(secondResourceManagerAddress, resourceManagerId.toUUID());\n+\n+\t\t\t// check that we start registering at the second RM\n+\t\t\tsecondJobManagerRegistration.await();\n+\t\t} finally {\n+\t\t\tRpcUtils.terminateRpcEndpoint(jobMaster, testingTimeout);\n+\t\t}\n+\t}\n+\n \tprivate File createSavepoint(long savepointId) throws IOException {\n \t\tfinal File savepointFile = temporaryFolder.newFile();\n \t\tfinal SavepointV2 savepoint = new SavepointV2(savepointId, Collections.emptyList(), Collections.emptyList());",
                "raw_url": "https://github.com/apache/flink/raw/a95ec5acf259884347ae539913bcffcad5bfc340/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
                "sha": "c0c916246637c173a961ee808831cec32640bbdb",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9358] Avoid NPE when closing an unestablished ResourceManager connection\n\nA NPE occurred when trying to disconnect an unestablished ResourceManager connection.\nIn order to fix this problem, we now check whether the connection has been established\nor not.\n\nThis closes #6011.",
        "parent": "https://github.com/apache/flink/commit/f4e03689dd5fef8eafeb0996a31ea021c5ea2203",
        "patched_files": [
            "EstablishedResourceManagerConnection.java",
            "JobMaster.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "JobMasterTest.java"
        ]
    },
    "flink_a973d84": {
        "bug_id": "flink_a973d84",
        "commit": "https://github.com/apache/flink/commit/a973d84b251ddf87bc47d9806d37d353febcab41",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/a973d84b251ddf87bc47d9806d37d353febcab41/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraSinkBase.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraSinkBase.java?ref=a973d84b251ddf87bc47d9806d37d353febcab41",
                "deletions": 2,
                "filename": "flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraSinkBase.java",
                "patch": "@@ -81,12 +81,16 @@ public void invoke(IN value) throws Exception {\n \t@Override\n \tpublic void close() {\n \t\ttry {\n-\t\t\tsession.close();\n+\t\t\tif (session != null) {\n+\t\t\t\tsession.close();\n+\t\t\t}\n \t\t} catch (Exception e) {\n \t\t\tLOG.error(\"Error while closing session.\", e);\n \t\t}\n \t\ttry {\n-\t\t\tcluster.close();\n+\t\t\tif (cluster != null) {\n+\t\t\t\tcluster.close();\n+\t\t\t}\n \t\t} catch (Exception e) {\n \t\t\tLOG.error(\"Error while closing cluster.\", e);\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/a973d84b251ddf87bc47d9806d37d353febcab41/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraSinkBase.java",
                "sha": "49b1efacbe5767c8737c9c4b056316db7e04b2c8",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/a973d84b251ddf87bc47d9806d37d353febcab41/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java?ref=a973d84b251ddf87bc47d9806d37d353febcab41",
                "deletions": 2,
                "filename": "flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "patch": "@@ -94,12 +94,16 @@ public void onFailure(Throwable throwable) {\n \tpublic void close() throws Exception {\n \t\tsuper.close();\n \t\ttry {\n-\t\t\tsession.close();\n+\t\t\tif (session != null) {\n+\t\t\t\tsession.close();\n+\t\t\t}\n \t\t} catch (Exception e) {\n \t\t\tLOG.error(\"Error while closing session.\", e);\n \t\t}\n \t\ttry {\n-\t\t\tcluster.close();\n+\t\t\tif (cluster != null) {\n+\t\t\t\tcluster.close();\n+\t\t\t}\n \t\t} catch (Exception e) {\n \t\t\tLOG.error(\"Error while closing cluster.\", e);\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/a973d84b251ddf87bc47d9806d37d353febcab41/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraTupleWriteAheadSink.java",
                "sha": "8bce9d6253f63ed25bff956de264be58a225fa2e",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4097] Fix NullPointerException on CassandraSinkBase and CassandraTupleWriteAheadSink's close()\n\nThis closes #2144",
        "parent": "https://github.com/apache/flink/commit/e7dfa28947f1582ab0b60ab9483b3fe06017f1ff",
        "patched_files": [
            "CassandraTupleWriteAheadSink.java",
            "CassandraSinkBase.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "CassandraTupleWriteAheadSinkTest.java",
            "CassandraSinkBaseTest.java"
        ]
    },
    "flink_ac7bf42": {
        "bug_id": "flink_ac7bf42",
        "commit": "https://github.com/apache/flink/commit/ac7bf427900e60abef3ff4344a5873349e6d108e",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-clients/src/main/java/org/apache/flink/client/program/Client.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/main/java/org/apache/flink/client/program/Client.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 22,
                "filename": "flink-clients/src/main/java/org/apache/flink/client/program/Client.java",
                "patch": "@@ -45,8 +45,10 @@\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.GlobalConfiguration;\n import org.apache.flink.core.fs.Path;\n+import org.apache.flink.runtime.client.JobCancellationException;\n import org.apache.flink.runtime.client.JobClient;\n import org.apache.flink.runtime.client.JobExecutionException;\n+import org.apache.flink.runtime.client.JobTimeoutException;\n import org.apache.flink.runtime.jobgraph.JobGraph;\n import org.apache.flink.runtime.messages.JobManagerMessages.SubmissionFailure;\n import org.apache.flink.runtime.messages.JobManagerMessages.SubmissionResponse;\n@@ -331,7 +333,11 @@ public JobExecutionResult run(JobGraph jobGraph, boolean wait) throws ProgramInv\n \n \t\ttry {\n \t\t\tJobClient.uploadJarFiles(jobGraph, hostname, client, timeout);\n+\t\t} catch (IOException e) {\n+\t\t\tthrow new ProgramInvocationException(\"Could not upload the programs JAR files to the JobManager.\", e);\n+\t\t}\n \n+\t\ttry{\n \t\t\tif (wait) {\n \t\t\t\treturn JobClient.submitJobAndWait(jobGraph, printStatusDuringExecution, client, timeout);\n \t\t\t}\n@@ -340,30 +346,20 @@ public JobExecutionResult run(JobGraph jobGraph, boolean wait) throws ProgramInv\n \t\t\t\tif (response instanceof SubmissionFailure) {\n \t\t\t\t\tSubmissionFailure failure = (SubmissionFailure) response;\n \t\t\t\t\tthrow new ProgramInvocationException(\n-\t\t\t\t\t\t\t\"Failed to submit the job to the flink JobManager\", failure.cause());\n+\t\t\t\t\t\t\t\"Failed to submit the job to the JobManager.\", failure.cause());\n \t\t\t\t}\n \t\t\t}\n-\t\t}\n-\t\tcatch (IOException e) {\n-\t\t\tthrow new ProgramInvocationException(\"Could not upload the programs JAR files to the JobManager.\", e);\n-\t\t}\n-\t\tcatch (JobExecutionException e) {\n-\t\t\tif (e.isJobCanceledByUser()) {\n-\t\t\t\tthrow new ProgramInvocationException(\"The program has been canceled.\");\n-\t\t\t}\n-\t\t\telse if (e.isConnectionTimedOut()) {\n-\t\t\t\tThrowable ae = null; //getAssociationError(monitoredErrors);\n-\t\t\t\tString message = ae == null ? \".\" : \": \" + ae.getMessage();\n-\t\t\t\tthrow new ProgramInvocationException(\"Lost connection to the JobManager\" + message);\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthrow new ProgramInvocationException(\"The program execution failed: \" + e.getMessage());\n-\t\t\t}\n-\t\t}\n-\t\tcatch (Exception e) {\n-\t\t\tThrowable ae = null; //getAssociationError(monitoredErrors);\n-\t\t\tString message = ae == null ? \".\" : \": \" + ae.getMessage();\n-\t\t\tthrow new ProgramInvocationException(\"Connection to JobManager failed\" + message);\n+\t\t} catch (JobExecutionException e) {\n+\t\t\tthrow new ProgramInvocationException(\"The program execution failed.\", e);\n+\t\t} catch (JobTimeoutException e) {\n+\t\t\tthrow new ProgramInvocationException(\"Lost connection to the JobManager.\", e);\n+\t\t} catch (JobCancellationException e) {\n+\t\t\tthrow new ProgramInvocationException(\"The program has been canceled.\", e);\n+\t\t} catch (ProgramInvocationException e) {\n+\t\t\t// forward exception resulting from submission failure\n+\t\t\tthrow e;\n+\t\t} catch (Exception e) {\n+\t\t\tthrow new ProgramInvocationException(\"Exception occurred during job execution.\", e);\n \t\t}\n \t\tfinally {\n \t\t\tactorSystem.shutdown();",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-clients/src/main/java/org/apache/flink/client/program/Client.java",
                "sha": "1fc7696a4b25a6fccab5449e1d17043181943517",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 10,
                "filename": "flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java",
                "patch": "@@ -94,8 +94,6 @@ public void testCancel() {\n \t@Test\n \tpublic void testList() {\n \t\ttry {\n-\t\t\tfinal ActorRef jm = actorSystem.actorOf(Props.create(CliJobManager.class, (Object)null));\n-\n \t\t\t// test unrecognized option\n \t\t\t{\n \t\t\t\tString[] parameters = {\"-v\", \"-k\"};\n@@ -104,16 +102,9 @@ public void testList() {\n \t\t\t\tassertTrue(retCode == 1);\n \t\t\t}\n \t\t\t\n-\t\t\t// test missing flags\n-\t\t\t{\n-\t\t\t\tString[] parameters = {};\n-\t\t\t\tCliFrontend testFrontend = new CliFrontendTestUtils.TestingCliFrontend();\n-\t\t\t\tint retCode = testFrontend.list(parameters);\n-\t\t\t\tassertTrue(retCode != 0);\n-\t\t\t}\n-\t\t\t\n \t\t\t// test list properly\n \t\t\t{\n+\t\t\t\tfinal ActorRef jm = actorSystem.actorOf(Props.create(CliJobManager.class, (Object)null));\n \t\t\t\tString[] parameters = {\"-r\", \"-s\"};\n \t\t\t\tInfoListTestCliFrontend testFrontend = new InfoListTestCliFrontend(jm);\n \t\t\t\tint retCode = testFrontend.list(parameters);",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java",
                "sha": "fafe92953148cb996b80dd2e7821de5d70f2b6fd",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-clients/src/test/resources/log4j-test.properties",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/test/resources/log4j-test.properties?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 1,
                "filename": "flink-clients/src/test/resources/log4j-test.properties",
                "patch": "@@ -16,4 +16,12 @@\n # limitations under the License.\n ################################################################################\n \n-log4j.rootLogger=OFF\n\\ No newline at end of file\n+# Set root logger level to OFF and its only appender to A1.\n+log4j.rootLogger=OFF, A1\n+\n+# A1 is set to be a ConsoleAppender.\n+log4j.appender.A1=org.apache.log4j.ConsoleAppender\n+\n+# A1 uses PatternLayout.\n+log4j.appender.A1.layout=org.apache.log4j.PatternLayout\n+log4j.appender.A1.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-clients/src/test/resources/log4j-test.properties",
                "sha": "04ec35570b7d540143437935e7036f670a6a0b10",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobCancellationException.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobCancellationException.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/client/JobCancellationException.java",
                "patch": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.client;\n+\n+/**\n+ * An exception which is thrown by the JobClient if a job is aborted as a result of a user\n+ * cancellation.\n+ */\n+public class JobCancellationException extends Exception {\n+\n+\tpublic JobCancellationException(final String msg, final Throwable cause){\n+\t\tsuper(msg, cause);\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobCancellationException.java",
                "sha": "1a9a1d0897c140fe6003c61a9925042d9ed8d05d",
                "status": "added"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobExecutionException.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobExecutionException.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 29,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/client/JobExecutionException.java",
                "patch": "@@ -19,43 +19,20 @@\n package org.apache.flink.runtime.client;\n \n /**\n- * This exception is thrown by the {@link JobClient} if a job has been aborted either as a result of a user\n- * request or an error which occurred during the execution.\n+ * This exception is thrown by the {@link JobClient} if a job has been aborted as a result of an\n+ * error which occurred during the execution.\n  */\n public class JobExecutionException extends Exception {\n \n-\tpublic static enum ExecutionErrorCause {\n-\t\tCANCELED,\n-\t\tTIMEOUT_TO_JOB_MANAGER,\n-\t\tERROR\n-\t}\n-\n-\t// ------------------------------------------------------------------------\n-\n \tprivate static final long serialVersionUID = 2818087325120827525L;\n \n-\tprivate final ExecutionErrorCause cause;\n-\n \t/**\n \t * Constructs a new job execution exception.\n \t * \n-\t * @param msg The message that shall be encapsulated by this exception.\n-\t * @param cause The cause for the execution exception.\n+\t * @param msg The cause for the execution exception.\n+\t * @param cause The cause of the exception\n \t */\n-\tpublic JobExecutionException(String msg, ExecutionErrorCause cause) {\n-\t\tsuper(msg);\n-\t\tthis.cause = cause;\n-\t}\n-\n-\tpublic boolean isJobCanceledByUser() {\n-\t\treturn cause == ExecutionErrorCause.CANCELED;\n-\t}\n-\n-\tpublic boolean isConnectionTimedOut() {\n-\t\treturn cause == ExecutionErrorCause.TIMEOUT_TO_JOB_MANAGER;\n-\t}\n-\n-\tpublic boolean isError() {\n-\t\treturn cause == ExecutionErrorCause.ERROR;\n+\tpublic JobExecutionException(String msg, Throwable cause) {\n+\t\tsuper(msg, cause);\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobExecutionException.java",
                "sha": "d5eb4922e17d9105500edccb901dadf839ef35d1",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobTimeoutException.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobTimeoutException.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/client/JobTimeoutException.java",
                "patch": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.client;\n+\n+/**\n+ * An exception which is thrown by the JobClient if the job manager is no longer reachable.\n+ */\n+public class JobTimeoutException extends Exception {\n+\n+\tpublic JobTimeoutException(final String msg, final Throwable cause) {\n+\t\tsuper(msg, cause);\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/client/JobTimeoutException.java",
                "sha": "2bd6ec59c255f02f51fe11fbab75ec85d38694fe",
                "status": "added"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/execution/CancelTaskException.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/execution/CancelTaskException.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/execution/CancelTaskException.java",
                "patch": "@@ -23,4 +23,12 @@\n  */\n public class CancelTaskException extends RuntimeException {\n \tprivate static final long serialVersionUID = 1L;\n+\n+\tpublic CancelTaskException(final String msg) {\n+\t\tsuper(msg);\n+\t}\n+\n+\tpublic CancelTaskException() {\n+\t\tsuper(\"\");\n+\t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/execution/CancelTaskException.java",
                "sha": "959ab07cd0ea52dce718b00b664291a134228d9d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java",
                "patch": "@@ -205,7 +205,7 @@ public void run() {\n \n \t\t\t// Make sure, we enter the catch block when the task has been canceled\n \t\t\tif (owner.isCanceledOrFailed()) {\n-\t\t\t\tthrow new CancelTaskException();\n+\t\t\t\tthrow new CancelTaskException(\"Task has been canceled or failed\");\n \t\t\t}\n \n \t\t\t// Finish the produced partitions",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/execution/RuntimeEnvironment.java",
                "sha": "f78ea928b7855d940437f90aa4c7b01722127acc",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 40,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "patch": "@@ -601,7 +601,7 @@ void sendPartitionInfos() {\n \n \t\t// check if the ExecutionVertex has already been archived and thus cleared the\n \t\t// partial partition infos queue\n-\t\tif(partialPartitionInfos != null) {\n+\t\tif(partialPartitionInfos != null && !partialPartitionInfos.isEmpty()) {\n \n \t\t\tPartialPartitionInfo partialPartitionInfo;\n \n@@ -730,64 +730,64 @@ else if (currentState == CANCELING || currentState == FAILED) {\n \n \tprivate void sendCancelRpcCall() {\n \t\tfinal SimpleSlot slot = this.assignedResource;\n-\t\tif (slot == null) {\n-\t\t\treturn;\n-\t\t}\n \n-\t\tFuture<Object> cancelResult = AkkaUtils.retry(slot.getInstance().getTaskManager(), new\n-\t\t\t\t\t\tTaskManagerMessages.CancelTask(attemptId), NUM_CANCEL_CALL_TRIES,\n-\t\t\t\tAkkaUtils.globalExecutionContext(), timeout);\n+\t\tif (slot != null) {\n \n-\t\tcancelResult.onComplete(new OnComplete<Object>(){\n+\t\t\tFuture<Object> cancelResult = AkkaUtils.retry(slot.getInstance().getTaskManager(), new\n+\t\t\t\t\t\t\tTaskManagerMessages.CancelTask(attemptId), NUM_CANCEL_CALL_TRIES,\n+\t\t\t\t\tAkkaUtils.globalExecutionContext(), timeout);\n \n-\t\t\t@Override\n-\t\t\tpublic void onComplete(Throwable failure, Object success) throws Throwable {\n-\t\t\t\tif (failure != null) {\n-\t\t\t\t\tfail(new Exception(\"Task could not be canceled.\", failure));\n-\t\t\t\t} else {\n-\t\t\t\t\tTaskOperationResult result = (TaskOperationResult)success;\n-\t\t\t\t\tif(!result.success()){\n-\t\t\t\t\t\tLOG.debug(\"Cancel task call did not find task. Probably akka message call\" +\n-\t\t\t\t\t\t\t\t\" race.\");\n+\t\t\tcancelResult.onComplete(new OnComplete<Object>() {\n+\n+\t\t\t\t@Override\n+\t\t\t\tpublic void onComplete(Throwable failure, Object success) throws Throwable {\n+\t\t\t\t\tif (failure != null) {\n+\t\t\t\t\t\tfail(new Exception(\"Task could not be canceled.\", failure));\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tTaskOperationResult result = (TaskOperationResult) success;\n+\t\t\t\t\t\tif (!result.success()) {\n+\t\t\t\t\t\t\tLOG.debug(\"Cancel task call did not find task. Probably akka message call\" +\n+\t\t\t\t\t\t\t\t\t\" race.\");\n+\t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t}\n-\t\t}, AkkaUtils.globalExecutionContext());\n+\t\t\t}, AkkaUtils.globalExecutionContext());\n+\t\t}\n \t}\n \n \tprivate void sendFailIntermediateResultPartitionsRPCCall() {\n \t\tfinal SimpleSlot slot = this.assignedResource;\n-\t\tif (slot == null) {\n-\t\t\treturn;\n-\t\t}\n \n-\t\tfinal Instance instance = slot.getInstance();\n+\t\tif (slot != null) {\n+\t\t\tfinal Instance instance = slot.getInstance();\n \n-\t\tif (instance.isAlive()) {\n-\t\t\ttry {\n-\t\t\t\t// TODO For some tests this could be a problem when querying too early if all resources were released\n-\t\t\t\tinstance.getTaskManager().tell(new TaskManagerMessages.FailIntermediateResultPartitions(attemptId), ActorRef.noSender());\n-\t\t\t}\n-\t\t\tcatch (Throwable t) {\n-\t\t\t\tfail(new Exception(\"Intermediate result partition could not be failed.\", t));\n+\t\t\tif (instance.isAlive()) {\n+\t\t\t\ttry {\n+\t\t\t\t\t// TODO For some tests this could be a problem when querying too early if all resources were released\n+\t\t\t\t\tinstance.getTaskManager().tell(new TaskManagerMessages.FailIntermediateResultPartitions(attemptId), ActorRef.noSender());\n+\t\t\t\t} catch (Throwable t) {\n+\t\t\t\t\tfail(new Exception(\"Intermediate result partition could not be failed.\", t));\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \t}\n \n \tprivate void sendUpdateTaskRpcCall(final SimpleSlot consumerSlot,\n \t\t\t\t\t\t\t\t\tfinal TaskManagerMessages.UpdateTask updateTaskMsg) {\n-\t\tfinal Instance instance = consumerSlot.getInstance();\n+\t\tif (consumerSlot != null) {\n+\t\t\tfinal Instance instance = consumerSlot.getInstance();\n \n-\t\tFuture<Object> futureUpdate = Patterns.ask(instance.getTaskManager(), updateTaskMsg,\n-\t\t\t\tnew Timeout(timeout));\n+\t\t\tFuture<Object> futureUpdate = Patterns.ask(instance.getTaskManager(), updateTaskMsg,\n+\t\t\t\t\tnew Timeout(timeout));\n \n-\t\tfutureUpdate.onFailure(new OnFailure() {\n-\t\t\t@Override\n-\t\t\tpublic void onFailure(Throwable failure) throws Throwable {\n-\t\t\t\tfail(new IllegalStateException(\"Update task on instance \" + instance +\n-\t\t\t\t\t\t\" failed due to:\", failure));\n-\t\t\t}\n-\t\t}, AkkaUtils.globalExecutionContext());\n+\t\t\tfutureUpdate.onFailure(new OnFailure() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic void onFailure(Throwable failure) throws Throwable {\n+\t\t\t\t\tfail(new IllegalStateException(\"Update task on instance \" + instance +\n+\t\t\t\t\t\t\t\" failed due to:\", failure));\n+\t\t\t\t}\n+\t\t\t}, AkkaUtils.globalExecutionContext());\n+\t\t}\n \t}\n \n \t// --------------------------------------------------------------------------------------------",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "sha": "56f9416a25543e18f247fafc935c6094c7a5c39a",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 6,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java",
                "patch": "@@ -398,10 +398,15 @@ public void fail(Throwable t) {\n \t\t\t}\n \t\t\telse if (transitionState(current, JobStatus.FAILING, t)) {\n \t\t\t\tthis.failureCause = t;\n-\t\t\t\t\n-\t\t\t\t// cancel all. what is failed will not cancel but stay failed\n-\t\t\t\tfor (ExecutionJobVertex ejv : verticesInCreationOrder) {\n-\t\t\t\t\tejv.cancel();\n+\n+\t\t\t\tif (!verticesInCreationOrder.isEmpty()) {\n+\t\t\t\t\t// cancel all. what is failed will not cancel but stay failed\n+\t\t\t\t\tfor (ExecutionJobVertex ejv : verticesInCreationOrder) {\n+\t\t\t\t\t\tejv.cancel();\n+\t\t\t\t\t}\n+\t\t\t\t} else {\n+\t\t\t\t\t// set the state of the job to failed\n+\t\t\t\t\ttransitionState(current, JobStatus.FAILED, t);\n \t\t\t\t}\n \t\t\t\t\n \t\t\t\treturn;\n@@ -568,15 +573,18 @@ public void registerExecutionListener(ActorRef listener){\n \t\tthis.executionListenerActors.add(listener);\n \t}\n \n+\tpublic boolean containsJobStatusListener(ActorRef listener) {\n+\t\treturn this.jobStatusListenerActors.contains(listener);\n+\t}\n+\n \t/**\n \t * NOTE: This method never throws an error, only logs errors caused by the notified listeners.\n \t */\n \tprivate void notifyJobStatusChange(JobStatus newState, Throwable error) {\n \t\tif(jobStatusListenerActors.size() > 0){\n-\t\t\tString message = error == null ? null : ExceptionUtils.stringifyException(error);\n \t\t\tfor(ActorRef listener: jobStatusListenerActors){\n \t\t\t\tlistener.tell(new ExecutionGraphMessages.JobStatusChanged(jobID, newState, System.currentTimeMillis(),\n-\t\t\t\t\t\t\t\tmessage), ActorRef.noSender());\n+\t\t\t\t\t\t\t\terror), ActorRef.noSender());\n \t\t\t}\n \t\t}\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java",
                "sha": "7198076b09815f030c1591c0fe8639abe8281b26",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 14,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala",
                "patch": "@@ -48,16 +48,28 @@ Actor with ActorLogMessages with ActorLogging {\n   override def receiveWithLogMessages: Receive = {\n     case SubmitJobDetached(jobGraph) =>\n       jobManager forward SubmitJob(jobGraph, registerForEvents = false, detached = true)\n+\n     case cancelJob: CancelJob =>\n       jobManager forward cancelJob\n+\n     case SubmitJobAndWait(jobGraph, listen) =>\n       val listener = context.actorOf(Props(classOf[JobClientListener], sender))\n       jobManager.tell(SubmitJob(jobGraph, registerForEvents = listen, detached = false), listener)\n+\n     case RequestBlobManagerPort =>\n       jobManager forward RequestBlobManagerPort\n+\n     case RequestJobManagerStatus =>\n       jobManager forward RequestJobManagerStatus\n   }\n+\n+  /**\n+   * Handle unmatched messages with an exception.\n+   */\n+  override def unhandled(message: Any): Unit = {\n+    // let the actor crash\n+    throw new RuntimeException(\"Received unknown message \" + message)\n+  }\n }\n \n /**\n@@ -70,30 +82,36 @@ Actor with ActorLogMessages with ActorLogging {\n class JobClientListener(jobSubmitter: ActorRef) extends Actor with ActorLogMessages with\n ActorLogging {\n   override def receiveWithLogMessages: Receive = {\n-    case SubmissionFailure(_, t) =>\n-      jobSubmitter ! Failure(t)\n-      self ! PoisonPill\n+    case SubmissionFailure(jobID, t) =>\n+      System.out.println(s\"Submission of job with ID $jobID was unsuccessful, \" +\n+        s\"because ${t.getMessage}.\")\n \n     case SubmissionSuccess(_) =>\n \n     case JobResultSuccess(_, duration, accumulatorResults) =>\n       jobSubmitter ! new JobExecutionResult(duration, accumulatorResults)\n       self ! PoisonPill\n \n-    case JobResultCanceled(_, msg) =>\n-      jobSubmitter ! Failure(\n-        new JobExecutionException(msg, JobExecutionException.ExecutionErrorCause.CANCELED))\n+    case JobResultCanceled(_, t) =>\n+      jobSubmitter ! Failure(new JobCancellationException(\"The job has been cancelled.\", t))\n       self ! PoisonPill\n \n-    case JobResultFailed(_, msg) =>\n-      jobSubmitter ! Failure(new JobExecutionException(msg,\n-        JobExecutionException.ExecutionErrorCause.ERROR))\n+    case JobResultFailed(_, t) =>\n+      jobSubmitter ! Failure(new JobExecutionException(\"The job execution failed.\", t))\n       self ! PoisonPill\n \n     case msg =>\n       // we have to use System.out.println here to avoid erroneous behavior for output redirection\n       System.out.println(msg.toString)\n   }\n+\n+  /**\n+   * Handle unmatched messages with an exception.\n+   */\n+  override def unhandled(message: Any): Unit = {\n+    // let the actor crash\n+    throw new RuntimeException(\"Received unknown message \" + message)\n+  }\n }\n \n /**\n@@ -193,7 +211,7 @@ object JobClient {\n    *                                                               execution fails.\n    * @return The job execution result\n    */\n-  @throws(classOf[JobExecutionException])\n+  @throws(classOf[Exception])\n   def submitJobAndWait(jobGraph: JobGraph, listenToStatusEvents: Boolean, jobClient: ActorRef)\n                       (implicit timeout: FiniteDuration): JobExecutionResult = {\n \n@@ -215,9 +233,7 @@ object JobClient {\n             Await.result(jmStatus, timeout)\n           } catch {\n             case t: Throwable =>\n-              throw new JobExecutionException(\n-                \"JobManager not reachable anymore. Terminate waiting for job answer.\",\n-                JobExecutionException.ExecutionErrorCause.TIMEOUT_TO_JOB_MANAGER)\n+              throw new JobTimeoutException(\"Lost connection to job manager.\", t)\n           }\n       }\n     }\n@@ -236,11 +252,19 @@ object JobClient {\n    * @param timeout Tiemout for futures\n    * @return The submission response\n    */\n+  @throws(classOf[Exception])\n   def submitJobDetached(jobGraph: JobGraph, jobClient: ActorRef)(implicit timeout: FiniteDuration):\n   SubmissionResponse = {\n     val response = (jobClient ? SubmitJobDetached(jobGraph))(timeout)\n \n-    Await.result(response.mapTo[SubmissionResponse], timeout)\n+    try {\n+      Await.result(response.mapTo[SubmissionResponse], timeout)\n+    } catch {\n+      case timeout: TimeoutException =>\n+        throw new JobTimeoutException(\"Timeout while waiting for the submission result.\", timeout);\n+      case t: Throwable =>\n+        throw new JobExecutionException(\"Exception while waiting for the submission result.\", t)\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/client/JobClient.scala",
                "sha": "4a64f87e4296fd175e45be504cdfeda93d835a37",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 40,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala",
                "patch": "@@ -50,7 +50,6 @@ import org.apache.flink.util.InstantiationUtil\n import org.slf4j.LoggerFactory\n \n import akka.actor._\n-import akka.pattern.ask\n \n import scala.concurrent._\n import scala.concurrent.duration._\n@@ -103,9 +102,6 @@ class JobManager(val configuration: Configuration,\n   // List of current jobs running\n   val currentJobs = scala.collection.mutable.HashMap[JobID, (ExecutionGraph, JobInfo)]()\n \n-  // Map of actors which want to be notified once a specific job terminates\n-  val finalJobStatusListener = scala.collection.mutable.HashMap[JobID, Set[ActorRef]]()\n-\n \n   override def preStart(): Unit = {\n     LOG.info(s\"Starting JobManager at ${self.path}.\")\n@@ -261,9 +257,9 @@ class JobManager(val configuration: Configuration,\n     case JobStatusChanged(jobID, newJobStatus, timeStamp, optionalMessage) =>\n       currentJobs.get(jobID) match {\n         case Some((executionGraph, jobInfo)) => executionGraph.getJobName\n-          log.info(\"Status of job {} ({}) changed to {}{}.\",\n+          log.info(\"Status of job {} ({}) changed to {} {}.\",\n             jobID, executionGraph.getJobName, newJobStatus,\n-            if(optionalMessage == null) \"\" else optionalMessage)\n+            if(optionalMessage == null) \"\" else optionalMessage.getMessage)\n \n           if(newJobStatus.isTerminalState) {\n             jobInfo.end = timeStamp\n@@ -279,14 +275,9 @@ class JobManager(val configuration: Configuration,\n                 case JobStatus.FAILED =>\n                   jobInfo.client ! JobResultFailed(jobID, optionalMessage)\n                 case x =>\n-                  jobInfo.client ! JobResultFailed(jobID, s\"$x is not a terminal state.\")\n-                  throw new IllegalStateException(s\"$x is not a terminal state.\")\n-              }\n-            }\n-\n-            finalJobStatusListener.get(jobID) foreach {\n-              _ foreach {\n-                _ ! CurrentJobStatus(jobID, newJobStatus)\n+                  val exception = new IllegalStateException(s\"$x is not a terminal state.\")\n+                  jobInfo.client ! JobResultFailed(jobID, exception)\n+                  throw exception\n               }\n             }\n \n@@ -296,16 +287,6 @@ class JobManager(val configuration: Configuration,\n           removeJob(jobID)\n       }\n \n-    case RequestFinalJobStatus(jobID) =>\n-      currentJobs.get(jobID) match {\n-        case Some(_) =>\n-          val listeners = finalJobStatusListener.getOrElse(jobID, Set())\n-          finalJobStatusListener += jobID -> (listeners + sender)\n-        case None =>\n-          // There is no job running with this job ID. Check the archive.\n-          archive forward RequestJobStatus(jobID)\n-      }\n-\n     case ScheduleOrUpdateConsumers(jobId, executionId, partitionIndex) =>\n       currentJobs.get(jobId) match {\n         case Some((executionGraph, _)) =>\n@@ -497,25 +478,12 @@ class JobManager(val configuration: Configuration,\n              * before. That way the proper cleanup of the job is triggered in the JobStatusChanged\n              * handler.\n              */\n-            val status = (self ? RequestFinalJobStatus(jobGraph.getJobID))(10 second)\n-\n-            /*\n-             * if we cannot register as final job status listener, then send manually a\n-             * JobStatusChanged message with JobStatus.FAILED.\n-             */\n-            val selfActorRef = self\n-            status.onFailure{\n-              case _: Throwable => selfActorRef ! JobStatusChanged(executionGraph.getJobID,\n-                JobStatus.FAILED, System.currentTimeMillis(), s\"Cleanup job ${jobGraph.getJobID}.\")\n+            if (!executionGraph.containsJobStatusListener(self)) {\n+              executionGraph.registerJobStatusListener(self)\n             }\n \n-            /*\n-             * Don't send the client the final job status because we will send him a\n-             * SubmissionFailure.\n-             */\n-            jobInfo.detached = true\n-\n             executionGraph.fail(t)\n+\n           case None =>\n             libraryCacheManager.unregisterJob(jobGraph.getJobID)\n             currentJobs.remove(jobGraph.getJobID)",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala",
                "sha": "1741cdbf65cccf02cb325e4379c9704b72d2bc52",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManagerProfiler.scala",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManagerProfiler.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManagerProfiler.scala",
                "patch": "@@ -41,4 +41,12 @@ class JobManagerProfiler extends Actor with ActorLogMessages with ActorLogging w\n           log.error(s\"Received unknown profiling data: ${x.getClass.getName}\" )\n       }\n   }\n+\n+  /**\n+   * Handle unmatched messages with an exception.\n+   */\n+  override def unhandled(message: Any): Unit = {\n+    // let the actor crash\n+    throw new RuntimeException(\"Received unknown message \" + message)\n+  }\n }",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManagerProfiler.scala",
                "sha": "eb8f913a342a5ffcab1b070b1ea1508ea5f82eb3",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala",
                "patch": "@@ -81,6 +81,14 @@ ActorLogging {\n       }\n   }\n \n+  /**\n+   * Handle unmatched messages with an exception.\n+   */\n+  override def unhandled(message: Any): Unit = {\n+    // let the actor crash\n+    throw new RuntimeException(\"Received unknown message \" + message)\n+  }\n+\n   /**\n    * Gets all graphs that have not been garbage collected.\n    * @return An iterable with all valid ExecutionGraphs",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/MemoryArchivist.scala",
                "sha": "0c3384cc63416cab8938f4e429ac5ea890c4eea7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 2,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala",
                "patch": "@@ -66,10 +66,10 @@ object ExecutionGraphMessages {\n    * @param jobID identifying the correspong job\n    * @param newJobStatus\n    * @param timestamp\n-   * @param optionalMessage\n+   * @param error\n    */\n   case class JobStatusChanged(jobID: JobID, newJobStatus: JobStatus, timestamp: Long,\n-                              optionalMessage: String){\n+                              error: Throwable){\n     override def toString: String = {\n       s\"${timestampToString(timestamp)}\\tJob execution switched to status $newJobStatus.\"\n     }",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/ExecutionGraphMessages.scala",
                "sha": "d413d1322ef6edd609756a00f32457742f625c60",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobmanagerMessages.scala",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobmanagerMessages.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 4,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobmanagerMessages.scala",
                "patch": "@@ -196,16 +196,16 @@ object JobManagerMessages {\n   /**\n    * Denotes a cancellation of the job.\n    * @param jobID\n-   * @param msg\n+   * @param t\n    */\n-  case class JobResultCanceled(jobID: JobID, msg: String) extends JobResult\n+  case class JobResultCanceled(jobID: JobID, t: Throwable) extends JobResult\n \n   /**\n    * Denotes a failed job execution.\n    * @param jobID\n-   * @param msg\n+   * @param t\n    */\n-  case class JobResultFailed(jobID: JobID, msg:String) extends JobResult\n+  case class JobResultFailed(jobID: JobID, t: Throwable) extends JobResult\n \n   sealed trait SubmissionResponse{\n     def jobID: JobID",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobmanagerMessages.scala",
                "sha": "f25083f3ba5311a4fcf500f182505692c107cccd",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala",
                "patch": "@@ -354,6 +354,20 @@ import scala.collection.JavaConverters._\n       cleanupTaskManager()\n \n       tryJobManagerRegistration()\n+\n+    case FailIntermediateResultPartitions(executionID) =>\n+      log.info(\"Fail intermediate result partitions associated with execution {}.\", executionID)\n+      networkEnvironment foreach {\n+        _.getPartitionManager.failIntermediateResultPartitions(executionID)\n+      }\n+  }\n+\n+  /**\n+   * Handle unmatched messages with an exception.\n+   */\n+  override def unhandled(message: Any): Unit = {\n+    // let the actor crash\n+    throw new RuntimeException(\"Received unknown message \" + message)\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala",
                "sha": "99d824b16713e3162b121b93fc0ad952028c7411",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerProfiler.scala",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerProfiler.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerProfiler.scala",
                "patch": "@@ -137,6 +137,14 @@ ActorLogMessages with ActorLogging {\n       }\n   }\n \n+  /**\n+   * Handle unmatched messages with an exception.\n+   */\n+  override def unhandled(message: Any): Unit = {\n+    // let the actor crash\n+    throw new RuntimeException(\"Received unknown message \" + message)\n+  }\n+\n   def startMonitoring(): Unit = {\n     val interval = new FiniteDuration(reportInterval, TimeUnit.MILLISECONDS)\n     val delay = new FiniteDuration((reportInterval * Math.random()).toLong, TimeUnit.MILLISECONDS)",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManagerProfiler.scala",
                "sha": "abd44b1026e0bee1fb8f6f763ad24b007c9bed70",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/test/resources/log4j-test.properties",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/resources/log4j-test.properties?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 1,
                "filename": "flink-runtime/src/test/resources/log4j-test.properties",
                "patch": "@@ -16,7 +16,7 @@\n # limitations under the License.\n ################################################################################\n \n-# Set root logger level to DEBUG and its only appender to A1.\n+# Set root logger level to OFF and its only appender to A1.\n log4j.rootLogger=OFF, A1\n \n # A1 is set to be a ConsoleAppender.",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/test/resources/log4j-test.properties",
                "sha": "04ec35570b7d540143437935e7036f670a6a0b10",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 1,
                "filename": "flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala",
                "patch": "@@ -72,7 +72,7 @@ WordSpecLike with Matchers with BeforeAndAfterAll {\n \n           expectMsg(SubmissionFailure(jobGraph.getJobID, new NoResourceAvailableException(1,1,0)))\n \n-          expectNoMsg()\n+          expectMsg(JobResultFailed(jobGraph.getJobID, new NoResourceAvailableException(1,1,0)))\n         }\n \n         jm ! NotifyWhenJobRemoved(jobGraph.getJobID)",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-runtime/src/test/scala/org/apache/flink/runtime/jobmanager/JobManagerITCase.scala",
                "sha": "89d5d438dff323545be29542ed7dab95cc34fe6c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-tests/src/test/java/org/apache/flink/test/cancelling/CancellingTestBase.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests/src/test/java/org/apache/flink/test/cancelling/CancellingTestBase.java?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 6,
                "filename": "flink-tests/src/test/java/org/apache/flink/test/cancelling/CancellingTestBase.java",
                "patch": "@@ -28,7 +28,7 @@\n import org.apache.flink.configuration.ConfigConstants;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.runtime.akka.AkkaUtils;\n-import org.apache.flink.runtime.client.JobExecutionException;\n+import org.apache.flink.runtime.client.JobCancellationException;\n import org.apache.flink.runtime.messages.JobClientMessages;\n import org.apache.flink.runtime.messages.JobManagerMessages;\n import org.apache.flink.test.util.ForkableFlinkMiniCluster;\n@@ -123,12 +123,10 @@ public void runAndCancelJob(Plan plan, int msecsTillCanceling, int maxTimeTillCa\n \n \t\t\ttry {\n \t\t\t\tAwait.result(result, AkkaUtils.getDefaultTimeout());\n-\t\t\t} catch (JobExecutionException exception) {\n-\t\t\t\tif (!exception.isJobCanceledByUser()) {\n-\t\t\t\t\tthrow new IllegalStateException(\"Job Failed.\");\n-\t\t\t\t}\n-\n+\t\t\t} catch (JobCancellationException exception) {\n \t\t\t\tjobSuccessfullyCancelled = true;\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tthrow new IllegalStateException(\"Job failed.\", e);\n \t\t\t}\n \n \t\t\tif (!jobSuccessfullyCancelled) {",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-tests/src/test/java/org/apache/flink/test/cancelling/CancellingTestBase.java",
                "sha": "3226651bb9b7920ebbaf855b8f15e8940814019f",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 0,
                "filename": "flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala",
                "patch": "@@ -137,4 +137,11 @@ class ApplicationClient extends Actor with ActorLogMessages with ActorLogging {\n       sender() ! messagesQueue.headOption\n   }\n \n+  /**\n+   * Handle unmatched messages with an exception.\n+   */\n+  override def unhandled(message: Any): Unit = {\n+    // let the actor crash\n+    throw new RuntimeException(\"Received unknown message \" + message)\n+  }\n }",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-yarn/src/main/scala/org/apache/flink/yarn/ApplicationClient.scala",
                "sha": "f7f6967fbc0e932e26b1f12a41417e9321cd80f6",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala?ref=ac7bf427900e60abef3ff4344a5873349e6d108e",
                "deletions": 3,
                "filename": "flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala",
                "patch": "@@ -18,7 +18,7 @@\n \n package org.apache.flink.yarn\n \n-import java.io.{IOException, File}\n+import java.io.File\n import java.nio.ByteBuffer\n import java.util.Collections\n \n@@ -114,7 +114,8 @@ trait YarnJobManager extends ActorLogMessages {\n       sender() ! new FlinkYarnClusterStatus(instanceManager.getNumberOfRegisteredTaskManagers,\n         instanceManager.getTotalNumberOfSlots)\n \n-    case StartYarnSession(conf, actorSystemPort, webServerPort) => startYarnSession(conf, actorSystemPort, webServerPort)\n+    case StartYarnSession(conf, actorSystemPort, webServerPort) =>\n+      startYarnSession(conf, actorSystemPort, webServerPort)\n \n     case PollContainerCompletion =>\n       rmClientOption match {\n@@ -168,7 +169,9 @@ trait YarnJobManager extends ActorLogMessages {\n       }\n   }\n \n-  private def startYarnSession(conf: Configuration, actorSystemPort: Int, webServerPort: Int): Unit = {\n+  private def startYarnSession(conf: Configuration,\n+                               actorSystemPort: Int,\n+                               webServerPort: Int): Unit = {\n     Try {\n       log.info(\"Start yarn session.\")\n       val memoryPerTaskManager = env.get(FlinkYarnClient.ENV_TM_MEMORY).toInt",
                "raw_url": "https://github.com/apache/flink/raw/ac7bf427900e60abef3ff4344a5873349e6d108e/flink-yarn/src/main/scala/org/apache/flink/yarn/YarnJobManager.scala",
                "sha": "2bc81fec8822771abcd804eca638cb0096751720",
                "status": "modified"
            }
        ],
        "message": "[FLINK-1556] [runtime] Corrects faulty JobClient behaviour in case of a submission failure\n\nFixes unhandled FailsIntermediateResultPartitions message. Changes JobResultFailed and JobResultCanceled to send the cause as a throwable instead of a string.\n\nFixes NullPointerException if an execution has already been prepared for archiving and sendPartitionInfos is called asynchronously\n\nThis closes #406.",
        "parent": "https://github.com/apache/flink/commit/649f158391aebcefcaabcdaa02d5c8b95be45777",
        "patched_files": [
            "JobCancellationException.java",
            "CancellingTestBase.java",
            "JobClient.java",
            "JobManagerProfiler.java",
            "Client.java",
            "ExecutionGraph.java",
            "ApplicationClient.java",
            "TaskManagerProfiler.java",
            "JobmanagerMessages.java",
            "RuntimeEnvironment.java",
            "TaskManager.java",
            "JobTimeoutException.java",
            "JobManager.java",
            "ExecutionGraphMessages.java",
            "YarnJobManager.java",
            "CancelTaskException.java",
            "MemoryArchivist.java",
            "Execution.java",
            "JobExecutionException.java",
            "JobManagerITCase.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "CliFrontendListCancelTest.java",
            "ExecutionTest.java",
            "ClientTest.java",
            "log4j-test.java"
        ]
    },
    "flink_ac9a911": {
        "bug_id": "flink_ac9a911",
        "commit": "https://github.com/apache/flink/commit/ac9a911723d0d81ee9c4d8231ee81bbbafff2575",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/ac9a911723d0d81ee9c4d8231ee81bbbafff2575/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java?ref=ac9a911723d0d81ee9c4d8231ee81bbbafff2575",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java",
                "patch": "@@ -54,7 +54,9 @@ public static void setUp() throws Exception {\n \n \t@AfterClass\n \tpublic static void tearDown() throws Exception {\n-\t\tflink.stop();\n+\t\tif (flink != null) {\n+\t\t\tflink.stop();\n+\t\t}\n \t}\n \n \t@Test",
                "raw_url": "https://github.com/apache/flink/raw/ac9a911723d0d81ee9c4d8231ee81bbbafff2575/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/SlotCountExceedingParallelismTest.java",
                "sha": "2783b6cc0abfa4dec74dfbee3e4e745bc3483e54",
                "status": "modified"
            }
        ],
        "message": "[FLINK-2599] [test-stability] Fix possible NPE in SlotCountExceedingParallelismTest",
        "parent": "https://github.com/apache/flink/commit/d5a90279fec810965da1a06bf7c90e7123c2719b",
        "patched_files": [],
        "repo": "flink",
        "unit_tests": [
            "SlotCountExceedingParallelismTest.java"
        ]
    },
    "flink_ad34540": {
        "bug_id": "flink_ad34540",
        "commit": "https://github.com/apache/flink/commit/ad3454069fa091cd453f6cefb0e56a8021a3c269",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java?ref=ad3454069fa091cd453f6cefb0e56a8021a3c269",
                "deletions": 3,
                "filename": "flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.flink.core.memory.DataInputView;\n import org.apache.flink.core.memory.DataOutputView;\n+import org.apache.flink.util.Preconditions;\n \n import java.io.IOException;\n import java.util.HashSet;\n@@ -56,7 +57,7 @@ public DelegatingConfiguration() {\n \t */\n \tpublic DelegatingConfiguration(Configuration backingConfig, String prefix)\n \t{\n-\t\tthis.backingConfig = backingConfig;\n+\t\tthis.backingConfig = Preconditions.checkNotNull(backingConfig);\n \t\tthis.prefix = prefix;\n \t}\n \n@@ -178,14 +179,19 @@ public String toString() {\n \n \t@Override\n \tpublic Set<String> keySet() {\n+\t\tif (this.prefix == null) {\n+\t\t\treturn this.backingConfig.keySet();\n+\t\t}\n+\n \t\tfinal HashSet<String> set = new HashSet<String>();\n-\t\tfinal int prefixLen = this.prefix == null ? 0 : this.prefix.length();\n+\t\tint prefixLen = this.prefix.length();\n \n \t\tfor (String key : this.backingConfig.keySet()) {\n-\t\t\tif (key.startsWith(this.prefix)) {\n+\t\t\tif (key.startsWith(prefix)) {\n \t\t\t\tset.add(key.substring(prefixLen));\n \t\t\t}\n \t\t}\n+\n \t\treturn set;\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/main/java/org/apache/flink/configuration/DelegatingConfiguration.java",
                "sha": "dba77f372326db7477c6c656e9aa2645d7fe0929",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/flink/blob/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java?ref=ad3454069fa091cd453f6cefb0e56a8021a3c269",
                "deletions": 0,
                "filename": "flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java",
                "patch": "@@ -26,8 +26,10 @@\n import java.lang.reflect.Modifier;\n import java.util.Arrays;\n import java.util.Comparator;\n+import java.util.Set;\n \n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertEquals;\n \n \n public class DelegatingConfigurationTest {\n@@ -88,4 +90,47 @@ private String typeParamToString(Class<?>[] classes) {\n \t\t\tassertTrue(\"Foo method '\" + configurationMethod.getName() + \"' has not been wrapped correctly in DelegatingConfiguration wrapper\", hasMethod);\n \t\t}\n \t}\n+\t\n+\t@Test\n+\tpublic void testDelegationConfigurationWithNullPrefix() {\n+\t\tConfiguration backingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(\"test-key\", \"value\");\n+\n+\t\tDelegatingConfiguration configuration = new DelegatingConfiguration(\n+\t\t\t\tbackingConf, null);\n+\t\tSet<String> keySet = configuration.keySet();\n+\n+\t\tassertEquals(keySet, backingConf.keySet());\n+\n+\t}\n+\n+\t@Test\n+\tpublic void testDelegationConfigurationWithPrefix() {\n+\t\tString prefix = \"pref-\";\n+\t\tString expectedKey = \"key\";\n+\n+\t\t/*\n+\t\t * Key matches the prefix\n+\t\t */\n+\t\tConfiguration backingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(prefix + expectedKey, \"value\");\n+\n+\t\tDelegatingConfiguration configuration = new DelegatingConfiguration(backingConf, prefix);\n+\t\tSet<String> keySet = configuration.keySet();\n+\t\t\n+\n+\t\tassertEquals(keySet.size(), 1);\n+\t\tassertEquals(keySet.iterator().next(), expectedKey);\n+\n+\t\t/*\n+\t\t * Key does not match the prefix\n+\t\t */\n+\t\tbackingConf = new Configuration();\n+\t\tbackingConf.setValueInternal(\"test-key\", \"value\");\n+\n+\t\tconfiguration = new DelegatingConfiguration(backingConf, prefix);\n+\t\tkeySet = configuration.keySet();\n+\n+\t\tassertTrue(keySet.isEmpty());\n+\t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/ad3454069fa091cd453f6cefb0e56a8021a3c269/flink-core/src/test/java/org/apache/flink/configuration/DelegatingConfigurationTest.java",
                "sha": "d8b782d4dbaf1c0d2d523deba0f1b73025bf0e27",
                "status": "modified"
            }
        ],
        "message": "[FLINK-4309] Fix potential NPE in DelegatingConfiguration\n\nThis closes #2371.",
        "parent": "https://github.com/apache/flink/commit/5ccd9071580e196d150905b2d05eef71e399a24c",
        "patched_files": [
            "DelegatingConfiguration.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "DelegatingConfigurationTest.java"
        ]
    },
    "flink_adb321d": {
        "bug_id": "flink_adb321d",
        "commit": "https://github.com/apache/flink/commit/adb321d61cc783b3a2a78f4e707104d75e1d63c0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/adb321d61cc783b3a2a78f4e707104d75e1d63c0/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java?ref=adb321d61cc783b3a2a78f4e707104d75e1d63c0",
                "deletions": 2,
                "filename": "flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java",
                "patch": "@@ -1163,8 +1163,9 @@ private void addLocalInfoFromChannelToConfig(Channel channel, TaskConfig config,\n \t\t\tfinal TempMode tm = channel.getTempMode();\n \n \t\t\tboolean needsMemory = false;\n-\t\t\t// Don't add a pipeline breaker if the data exchange is already blocking.\n-\t\t\tif (tm.breaksPipeline() && channel.getDataExchangeMode() != DataExchangeMode.BATCH) {\n+\t\t\t// Don't add a pipeline breaker if the data exchange is already blocking, EXCEPT the channel is within an iteration.\n+\t\t\tif (tm.breaksPipeline() &&\n+\t\t\t\t\t(channel.isOnDynamicPath() || channel.getDataExchangeMode() != DataExchangeMode.BATCH) ) {\n \t\t\t\tconfig.setInputAsynchronouslyMaterialized(inputNum, true);\n \t\t\t\tneedsMemory = true;\n \t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/adb321d61cc783b3a2a78f4e707104d75e1d63c0/flink-optimizer/src/main/java/org/apache/flink/optimizer/plantranslate/JobGraphGenerator.java",
                "sha": "26300196f33df109556c4c113dc2a6b017705018",
                "status": "modified"
            },
            {
                "additions": 81,
                "blob_url": "https://github.com/apache/flink/blob/adb321d61cc783b3a2a78f4e707104d75e1d63c0/flink-optimizer/src/test/java/org/apache/flink/optimizer/plantranslate/TempInIterationsTest.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-optimizer/src/test/java/org/apache/flink/optimizer/plantranslate/TempInIterationsTest.java?ref=adb321d61cc783b3a2a78f4e707104d75e1d63c0",
                "deletions": 0,
                "filename": "flink-optimizer/src/test/java/org/apache/flink/optimizer/plantranslate/TempInIterationsTest.java",
                "patch": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.optimizer.plantranslate;\n+\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.flink.api.java.io.DiscardingOutputFormat;\n+import org.apache.flink.api.java.operators.DeltaIteration;\n+import org.apache.flink.api.java.operators.translation.JavaPlan;\n+import org.apache.flink.api.java.tuple.Tuple2;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.optimizer.Optimizer;\n+import org.apache.flink.optimizer.plan.OptimizedPlan;\n+import org.apache.flink.optimizer.testfunctions.DummyFlatJoinFunction;\n+import org.apache.flink.runtime.jobgraph.AbstractJobVertex;\n+import org.apache.flink.runtime.jobgraph.JobGraph;\n+import org.apache.flink.runtime.operators.util.TaskConfig;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+public class TempInIterationsTest {\n+\n+\t/*\n+\t * Tests whether temps barriers are correctly set in within iterations\n+\t */\n+\t@Test\n+\tpublic void testTempInIterationTest() throws Exception {\n+\n+\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n+\n+\t\tDataSet<Tuple2<Long, Long>> input = env.readCsvFile(\"file:///does/not/exist\").types(Long.class, Long.class);\n+\n+\t\tDeltaIteration<Tuple2<Long, Long>, Tuple2<Long, Long>> iteration =\n+\t\t\t\tinput.iterateDelta(input, 1, 0);\n+\n+\t\tDataSet<Tuple2<Long, Long>> update = iteration.getWorkset()\n+\t\t\t\t.join(iteration.getSolutionSet()).where(0).equalTo(0)\n+\t\t\t\t\t.with(new DummyFlatJoinFunction<Tuple2<Long, Long>>());\n+\n+\t\titeration.closeWith(update, update)\n+\t\t\t\t.output(new DiscardingOutputFormat<Tuple2<Long, Long>>());\n+\n+\n+\t\tJavaPlan plan = env.createProgramPlan();\n+\t\tOptimizedPlan oPlan = (new Optimizer(new Configuration())).compile(plan);\n+\n+\t\tJobGraphGenerator jgg = new JobGraphGenerator();\n+\t\tJobGraph jg = jgg.compileJobGraph(oPlan);\n+\n+\t\tboolean solutionSetUpdateChecked = false;\n+\t\tfor(AbstractJobVertex v : jg.getVertices()) {\n+\t\t\tif(v.getName().equals(\"SolutionSet Delta\")) {\n+\n+\t\t\t\t// check if input of solution set delta is temped\n+\t\t\t\tTaskConfig tc = new TaskConfig(v.getConfiguration());\n+\t\t\t\tassertTrue(tc.isInputAsynchronouslyMaterialized(0));\n+\t\t\t\tsolutionSetUpdateChecked = true;\n+\t\t\t}\n+\t\t}\n+\t\tassertTrue(solutionSetUpdateChecked);\n+\n+\t}\n+\n+}",
                "raw_url": "https://github.com/apache/flink/raw/adb321d61cc783b3a2a78f4e707104d75e1d63c0/flink-optimizer/src/test/java/org/apache/flink/optimizer/plantranslate/TempInIterationsTest.java",
                "sha": "15cb03f8b179171826e06d6c8c9d343c7061a2a4",
                "status": "added"
            }
        ],
        "message": "[FLINK-1951] Fix NullPointerException in delta iteration due to missing temp\n\nThis closes #641",
        "parent": "https://github.com/apache/flink/commit/60ec683082124359162ac3c97b223b5cfe44cbbd",
        "patched_files": [
            "JobGraphGenerator.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "TempInIterationsTest.java",
            "JobGraphGeneratorTest.java"
        ]
    },
    "flink_b2c592a": {
        "bug_id": "flink_b2c592a",
        "commit": "https://github.com/apache/flink/commit/b2c592a87139c587777f9897613943639fac1d61",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/b2c592a87139c587777f9897613943639fac1d61/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java?ref=b2c592a87139c587777f9897613943639fac1d61",
                "deletions": 1,
                "filename": "flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java",
                "patch": "@@ -427,7 +427,7 @@ public void afterBulk(long executionId, BulkRequest request, BulkResponse respon\n \n \t\t@Override\n \t\tpublic void afterBulk(long executionId, BulkRequest request, Throwable failure) {\n-\t\t\tLOG.error(\"Failed Elasticsearch bulk request: {}\", failure.getMessage(), failure.getCause());\n+\t\t\tLOG.error(\"Failed Elasticsearch bulk request: {}\", failure.getMessage(), failure);\n \n \t\t\ttry {\n \t\t\t\tfor (ActionRequest action : request.requests()) {",
                "raw_url": "https://github.com/apache/flink/raw/b2c592a87139c587777f9897613943639fac1d61/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/ElasticsearchSinkBase.java",
                "sha": "96f4431493c9f6348f8675439391558c242360d7",
                "status": "modified"
            }
        ],
        "message": "[hotfix] [connectors] Fix shadowed NPE in elasticsearch sink connector\n\nThis closes #8849.",
        "parent": "https://github.com/apache/flink/commit/861bf73ada01e4d2d8c671e507974e5bfacd9218",
        "patched_files": [
            "ElasticsearchSinkBase.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "ElasticsearchSinkBaseTest.java"
        ]
    },
    "flink_b452c8b": {
        "bug_id": "flink_b452c8b",
        "commit": "https://github.com/apache/flink/commit/b452c8bbbaa9efb5de6cc66d2817b398ac9da041",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java?ref=b452c8bbbaa9efb5de6cc66d2817b398ac9da041",
                "deletions": 1,
                "filename": "flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java",
                "patch": "@@ -181,7 +181,7 @@ public void testCancelWithSavepoint() throws Exception {\n \t\t}\n \n \t\t{\n-\t\t\t// Cancel with savepoint (no target directory)and no job ID\n+\t\t\t// Cancel with savepoint (no target directory) and no job ID\n \t\t\tJobID jid = new JobID();\n \t\t\tUUID leaderSessionID = UUID.randomUUID();\n ",
                "raw_url": "https://github.com/apache/flink/raw/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-clients/src/test/java/org/apache/flink/client/CliFrontendListCancelTest.java",
                "sha": "4d3405faa09906cdc722d0fe89395eded967d2cd",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/flink/blob/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala?ref=b452c8bbbaa9efb5de6cc66d2817b398ac9da041",
                "deletions": 33,
                "filename": "flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala",
                "patch": "@@ -586,44 +586,54 @@ class JobManager(\n           defaultSavepointDir\n         }\n \n-        log.info(s\"Trying to cancel job $jobId with savepoint to $targetDirectory\")\n+        if (targetDirectory == null) {\n+          log.info(s\"Trying to cancel job $jobId with savepoint, but no \" +\n+            \"savepoint directory configured.\")\n+\n+          sender ! decorateMessage(CancellationFailure(jobId, new IllegalStateException(\n+            \"No savepoint directory configured. You can either specify a directory \" +\n+              \"while cancelling via -s :targetDirectory or configure a cluster-wide \" +\n+              \"default via key '\" + ConfigConstants.SAVEPOINT_DIRECTORY_KEY + \"'.\")))\n+        } else {\n+          log.info(s\"Trying to cancel job $jobId with savepoint to $targetDirectory\")\n \n-        currentJobs.get(jobId) match {\n-          case Some((executionGraph, _)) =>\n-            // We don't want any checkpoint between the savepoint and cancellation\n-            val coord = executionGraph.getCheckpointCoordinator\n-            coord.stopCheckpointScheduler()\n+          currentJobs.get(jobId) match {\n+            case Some((executionGraph, _)) =>\n+              // We don't want any checkpoint between the savepoint and cancellation\n+              val coord = executionGraph.getCheckpointCoordinator\n+              coord.stopCheckpointScheduler()\n \n-            // Trigger the savepoint\n-            val future = coord.triggerSavepoint(System.currentTimeMillis(), targetDirectory)\n+              // Trigger the savepoint\n+              val future = coord.triggerSavepoint(System.currentTimeMillis(), targetDirectory)\n \n-            val senderRef = sender()\n-            future.handleAsync[Void](\n-              new BiFunction[CompletedCheckpoint, Throwable, Void] {\n-                override def apply(success: CompletedCheckpoint, cause: Throwable): Void = {\n-                  if (success != null) {\n-                    val path = success.getExternalPath()\n-                    log.info(s\"Savepoint stored in $path. Now cancelling $jobId.\")\n-                    executionGraph.cancel()\n-                    senderRef ! decorateMessage(CancellationSuccess(jobId, path))\n-                  } else {\n-                    val msg = CancellationFailure(\n-                      jobId,\n-                      new Exception(\"Failed to trigger savepoint.\", cause))\n-                    senderRef ! decorateMessage(msg)\n+              val senderRef = sender()\n+              future.handleAsync[Void](\n+                new BiFunction[CompletedCheckpoint, Throwable, Void] {\n+                  override def apply(success: CompletedCheckpoint, cause: Throwable): Void = {\n+                    if (success != null) {\n+                      val path = success.getExternalPath()\n+                      log.info(s\"Savepoint stored in $path. Now cancelling $jobId.\")\n+                      executionGraph.cancel()\n+                      senderRef ! decorateMessage(CancellationSuccess(jobId, path))\n+                    } else {\n+                      val msg = CancellationFailure(\n+                        jobId,\n+                        new Exception(\"Failed to trigger savepoint.\", cause))\n+                      senderRef ! decorateMessage(msg)\n+                    }\n+                    null\n                   }\n-                  null\n-                }\n-              },\n-              context.dispatcher)\n+                },\n+                context.dispatcher)\n \n-          case None =>\n-            log.info(s\"No job found with ID $jobId.\")\n-            sender ! decorateMessage(\n-              CancellationFailure(\n-                jobId,\n-                new IllegalArgumentException(s\"No job found with ID $jobId.\"))\n-            )\n+            case None =>\n+              log.info(s\"No job found with ID $jobId.\")\n+              sender ! decorateMessage(\n+                CancellationFailure(\n+                  jobId,\n+                  new IllegalArgumentException(s\"No job found with ID $jobId.\"))\n+              )\n+          }\n         }\n       } catch {\n         case t: Throwable =>",
                "raw_url": "https://github.com/apache/flink/raw/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala",
                "sha": "81e9fc4f6fdb0a0bc6074dab5d9aecb03a9d280c",
                "status": "modified"
            },
            {
                "additions": 104,
                "blob_url": "https://github.com/apache/flink/blob/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java",
                "changes": 107,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java?ref=b452c8bbbaa9efb5de6cc66d2817b398ac9da041",
                "deletions": 3,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java",
                "patch": "@@ -894,9 +894,110 @@ public void testCancelWithSavepoint() throws Exception {\n \t}\n \n \t/**\n-\t * Tests that we can trigger a\n-\t *\n-\t * @throws Exception\n+\t * Tests that a meaningful exception is returned if no savepoint directory is\n+\t * configured.\n+\t */\n+\t@Test\n+\tpublic void testCancelWithSavepointNoDirectoriesConfigured() throws Exception {\n+\t\tFiniteDuration timeout = new FiniteDuration(30, TimeUnit.SECONDS);\n+\t\tConfiguration config = new Configuration();\n+\n+\t\tActorSystem actorSystem = null;\n+\t\tActorGateway jobManager = null;\n+\t\tActorGateway archiver = null;\n+\t\tActorGateway taskManager = null;\n+\t\ttry {\n+\t\t\tactorSystem = AkkaUtils.createLocalActorSystem(new Configuration());\n+\n+\t\t\tTuple2<ActorRef, ActorRef> master = JobManager.startJobManagerActors(\n+\t\t\t\tconfig,\n+\t\t\t\tactorSystem,\n+\t\t\t\tactorSystem.dispatcher(),\n+\t\t\t\tactorSystem.dispatcher(),\n+\t\t\t\tOption.apply(\"jm\"),\n+\t\t\t\tOption.apply(\"arch\"),\n+\t\t\t\tTestingJobManager.class,\n+\t\t\t\tTestingMemoryArchivist.class);\n+\n+\t\t\tjobManager = new AkkaActorGateway(master._1(), null);\n+\t\t\tarchiver = new AkkaActorGateway(master._2(), null);\n+\n+\t\t\tActorRef taskManagerRef = TaskManager.startTaskManagerComponentsAndActor(\n+\t\t\t\tconfig,\n+\t\t\t\tResourceID.generate(),\n+\t\t\t\tactorSystem,\n+\t\t\t\t\"localhost\",\n+\t\t\t\tOption.apply(\"tm\"),\n+\t\t\t\tOption.<LeaderRetrievalService>apply(new StandaloneLeaderRetrievalService(jobManager.path())),\n+\t\t\t\ttrue,\n+\t\t\t\tTestingTaskManager.class);\n+\n+\t\t\ttaskManager = new AkkaActorGateway(taskManagerRef, null);\n+\n+\t\t\t// Wait until connected\n+\t\t\tObject msg = new TestingTaskManagerMessages.NotifyWhenRegisteredAtJobManager(jobManager.actor());\n+\t\t\tAwait.ready(taskManager.ask(msg, timeout), timeout);\n+\n+\t\t\t// Create job graph\n+\t\t\tJobVertex sourceVertex = new JobVertex(\"Source\");\n+\t\t\tsourceVertex.setInvokableClass(BlockingStatefulInvokable.class);\n+\t\t\tsourceVertex.setParallelism(1);\n+\n+\t\t\tJobGraph jobGraph = new JobGraph(\"TestingJob\", sourceVertex);\n+\n+\t\t\tJobSnapshottingSettings snapshottingSettings = new JobSnapshottingSettings(\n+\t\t\t\tCollections.singletonList(sourceVertex.getID()),\n+\t\t\t\tCollections.singletonList(sourceVertex.getID()),\n+\t\t\t\tCollections.singletonList(sourceVertex.getID()),\n+\t\t\t\t3600000,\n+\t\t\t\t3600000,\n+\t\t\t\t0,\n+\t\t\t\tInteger.MAX_VALUE,\n+\t\t\t\tExternalizedCheckpointSettings.none(),\n+\t\t\t\ttrue);\n+\n+\t\t\tjobGraph.setSnapshotSettings(snapshottingSettings);\n+\n+\t\t\t// Submit job graph\n+\t\t\tmsg = new JobManagerMessages.SubmitJob(jobGraph, ListeningBehaviour.DETACHED);\n+\t\t\tAwait.result(jobManager.ask(msg, timeout), timeout);\n+\n+\t\t\t// Wait for all tasks to be running\n+\t\t\tmsg = new TestingJobManagerMessages.WaitForAllVerticesToBeRunning(jobGraph.getJobID());\n+\t\t\tAwait.result(jobManager.ask(msg, timeout), timeout);\n+\n+\t\t\t// Cancel with savepoint\n+\t\t\tmsg = new JobManagerMessages.CancelJobWithSavepoint(jobGraph.getJobID(), null);\n+\t\t\tCancellationResponse cancelResp = (CancellationResponse) Await.result(jobManager.ask(msg, timeout), timeout);\n+\n+\t\t\tif (cancelResp instanceof CancellationFailure) {\n+\t\t\t\tCancellationFailure failure = (CancellationFailure) cancelResp;\n+\t\t\t\tassertTrue(failure.cause() instanceof IllegalStateException);\n+\t\t\t\tassertTrue(failure.cause().getMessage().contains(\"savepoint directory\"));\n+\t\t\t} else {\n+\t\t\t\tfail(\"Unexpected cancellation response from JobManager: \" + cancelResp);\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tif (actorSystem != null) {\n+\t\t\t\tactorSystem.shutdown();\n+\t\t\t}\n+\n+\t\t\tif (archiver != null) {\n+\t\t\t\tarchiver.actor().tell(PoisonPill.getInstance(), ActorRef.noSender());\n+\t\t\t}\n+\n+\t\t\tif (jobManager != null) {\n+\t\t\t\tjobManager.actor().tell(PoisonPill.getInstance(), ActorRef.noSender());\n+\t\t\t}\n+\n+\t\t\tif (taskManager != null) {\n+\t\t\t\ttaskManager.actor().tell(PoisonPill.getInstance(), ActorRef.noSender());\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Tests that we can trigger a savepoint when periodic checkpoints are disabled.\n \t */\n \t@Test\n \tpublic void testSavepointWithDeactivatedPeriodicCheckpointing() throws Exception {",
                "raw_url": "https://github.com/apache/flink/raw/b452c8bbbaa9efb5de6cc66d2817b398ac9da041/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java",
                "sha": "b62727329851846dc8034dd2cad13e5311bfb57e",
                "status": "modified"
            }
        ],
        "message": "[FLINK-5699] [savepoints] Check target dir when cancelling with savepoint\n\nProblem: when cancelling a job with a savepoint and no savepoint directory\nis configured, triggering the savepoint fails with an NPE. This is then\nreturned to the user as the root cause.\n\nSolution: Instead of simply forwarding the argument (which is possibly\nnull), we check it for null and return a IllegalStateException with\na meaningful message.\n\nThis closes #3263.",
        "parent": "https://github.com/apache/flink/commit/11ebf484280314231d146dcfb0b973934448f00b",
        "patched_files": [
            "JobManager.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "JobManagerTest.java",
            "CliFrontendListCancelTest.java"
        ]
    },
    "flink_bcb0f32": {
        "bug_id": "flink_bcb0f32",
        "commit": "https://github.com/apache/flink/commit/bcb0f324f50adc74fb6122621794d6d7e37bc933",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/flink/blob/bcb0f324f50adc74fb6122621794d6d7e37bc933/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java?ref=bcb0f324f50adc74fb6122621794d6d7e37bc933",
                "deletions": 12,
                "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java",
                "patch": "@@ -496,18 +496,20 @@ static ContainerLaunchContext createTaskExecutorContext(\n \t\t\t// NOTE: must read the tokens from the local file, not from the UGI context, because if UGI is login\n \t\t\t// using Kerberos keytabs, there is no HDFS delegation token in the UGI context.\n \t\t\tString fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);\n-\t\t\tMethod readTokenStorageFileMethod = Credentials.class.getMethod(\n-\t\t\t\t\"readTokenStorageFile\", File.class, org.apache.hadoop.conf.Configuration.class);\n-\n-\t\t\tCredentials cred =\n-\t\t\t\t(Credentials) readTokenStorageFileMethod.invoke(\n-\t\t\t\t\tnull,\n-\t\t\t\t\tnew File(fileLocation),\n-\t\t\t\t\tHadoopUtils.getHadoopConfiguration(flinkConfig));\n-\n-\t\t\tcred.writeTokenStorageToStream(dob);\n-\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n-\t\t\tctx.setTokens(securityTokens);\n+\t\t\tif (fileLocation != null) {\n+\t\t\t\tMethod readTokenStorageFileMethod = Credentials.class.getMethod(\n+\t\t\t\t\t\"readTokenStorageFile\", File.class, org.apache.hadoop.conf.Configuration.class);\n+\n+\t\t\t\tCredentials cred =\n+\t\t\t\t\t(Credentials) readTokenStorageFileMethod.invoke(\n+\t\t\t\t\t\tnull,\n+\t\t\t\t\t\tnew File(fileLocation),\n+\t\t\t\t\t\tHadoopUtils.getHadoopConfiguration(flinkConfig));\n+\n+\t\t\t\tcred.writeTokenStorageToStream(dob);\n+\t\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n+\t\t\t\tctx.setTokens(securityTokens);\n+\t\t\t}\n \t\t}\n \t\tcatch (Throwable t) {\n \t\t\tlog.error(\"Getting current user info failed when trying to launch the container\", t);",
                "raw_url": "https://github.com/apache/flink/raw/bcb0f324f50adc74fb6122621794d6d7e37bc933/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java",
                "sha": "79a670315df29f4556dfe1259516ce07c0fe75bf",
                "status": "modified"
            }
        ],
        "message": "[FLINK-8830][YARN] YarnResourceManager throws NullPointerException",
        "parent": "https://github.com/apache/flink/commit/2dab4374bc5280a2b4536f7ad1e153d6361a8885",
        "patched_files": [
            "Utils.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "UtilsTest.java",
            "TestUtils.java"
        ]
    },
    "flink_bcead3b": {
        "bug_id": "flink_bcead3b",
        "commit": "https://github.com/apache/flink/commit/bcead3be32c624008730555d828fd8e9447fbeff",
        "file": [
            {
                "additions": 1402,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java",
                "changes": 1402,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java",
                "patch": "@@ -0,0 +1,1402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc;\n+\n+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n+import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n+import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.MapTypeInfo;\n+import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.types.Row;\n+\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.apache.orc.TypeDescription;\n+\n+import java.lang.reflect.Array;\n+import java.math.BigDecimal;\n+import java.nio.charset.StandardCharsets;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TimeZone;\n+import java.util.function.DoubleFunction;\n+import java.util.function.Function;\n+import java.util.function.LongFunction;\n+\n+/**\n+ * A class that provides utility methods for orc file reading.\n+ */\n+class OrcBatchReader {\n+\n+\tprivate static final long MILLIS_PER_DAY = 86400000; // = 24 * 60 * 60 * 1000\n+\tprivate static final TimeZone LOCAL_TZ = TimeZone.getDefault();\n+\n+\t/**\n+\t * Converts an ORC schema to a Flink TypeInformation.\n+\t *\n+\t * @param schema The ORC schema.\n+\t * @return The TypeInformation that corresponds to the ORC schema.\n+\t */\n+\tstatic TypeInformation schemaToTypeInfo(TypeDescription schema) {\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn BasicTypeInfo.BOOLEAN_TYPE_INFO;\n+\t\t\tcase BYTE:\n+\t\t\t\treturn BasicTypeInfo.BYTE_TYPE_INFO;\n+\t\t\tcase SHORT:\n+\t\t\t\treturn BasicTypeInfo.SHORT_TYPE_INFO;\n+\t\t\tcase INT:\n+\t\t\t\treturn BasicTypeInfo.INT_TYPE_INFO;\n+\t\t\tcase LONG:\n+\t\t\t\treturn BasicTypeInfo.LONG_TYPE_INFO;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn BasicTypeInfo.FLOAT_TYPE_INFO;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn BasicTypeInfo.DOUBLE_TYPE_INFO;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn BasicTypeInfo.BIG_DEC_TYPE_INFO;\n+\t\t\tcase STRING:\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\t\treturn BasicTypeInfo.STRING_TYPE_INFO;\n+\t\t\tcase DATE:\n+\t\t\t\treturn SqlTimeTypeInfo.DATE;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn SqlTimeTypeInfo.TIMESTAMP;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO;\n+\t\t\tcase STRUCT:\n+\t\t\t\tList<TypeDescription> fieldSchemas = schema.getChildren();\n+\t\t\t\tTypeInformation[] fieldTypes = new TypeInformation[fieldSchemas.size()];\n+\t\t\t\tfor (int i = 0; i < fieldSchemas.size(); i++) {\n+\t\t\t\t\tfieldTypes[i] = schemaToTypeInfo(fieldSchemas.get(i));\n+\t\t\t\t}\n+\t\t\t\tString[] fieldNames = schema.getFieldNames().toArray(new String[]{});\n+\t\t\t\treturn new RowTypeInfo(fieldTypes, fieldNames);\n+\t\t\tcase LIST:\n+\t\t\t\tTypeDescription elementSchema = schema.getChildren().get(0);\n+\t\t\t\tTypeInformation<?> elementType = schemaToTypeInfo(elementSchema);\n+\t\t\t\t// arrays of primitive types are handled as object arrays to support null values\n+\t\t\t\treturn ObjectArrayTypeInfo.getInfoFor(elementType);\n+\t\t\tcase MAP:\n+\t\t\t\tTypeDescription keySchema = schema.getChildren().get(0);\n+\t\t\t\tTypeDescription valSchema = schema.getChildren().get(1);\n+\t\t\t\tTypeInformation<?> keyType = schemaToTypeInfo(keySchema);\n+\t\t\t\tTypeInformation<?> valType = schemaToTypeInfo(valSchema);\n+\t\t\t\treturn new MapTypeInfo<>(keyType, valType);\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type is not supported yet.\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Fills an ORC batch into an array of Row.\n+\t *\n+\t * @param rows The batch of rows need to be filled.\n+\t * @param schema The schema of the ORC data.\n+\t * @param batch The ORC data.\n+\t * @param selectedFields The list of selected ORC fields.\n+\t * @return The number of rows that were filled.\n+\t */\n+\tstatic int fillRows(Row[] rows, TypeDescription schema, VectorizedRowBatch batch, int[] selectedFields) {\n+\n+\t\tint rowsToRead = Math.min((int) batch.count(), rows.length);\n+\n+\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n+\t\t// read each selected field\n+\t\tfor (int fieldIdx = 0; fieldIdx < selectedFields.length; fieldIdx++) {\n+\t\t\tint orcIdx = selectedFields[fieldIdx];\n+\t\t\treadField(rows, fieldIdx, fieldTypes.get(orcIdx), batch.cols[orcIdx], rowsToRead);\n+\t\t}\n+\t\treturn rowsToRead;\n+\t}\n+\n+\t/**\n+\t * Reads a vector of data into an array of objects.\n+\t *\n+\t * @param vals The array that needs to be filled.\n+\t * @param fieldIdx If the vals array is an array of Row, the index of the field that needs to be filled.\n+\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n+\t * @param schema The schema of the vector to read.\n+\t * @param vector The vector to read.\n+\t * @param childCount The number of vector entries to read.\n+\t */\n+\tprivate static void readField(Object[] vals, int fieldIdx, TypeDescription schema, ColumnVector vector, int childCount) {\n+\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readBoolean);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readBoolean);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase BYTE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readByte);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readByte);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase SHORT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readShort);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readShort);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase INT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readInt);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readInt);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase LONG:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readLong);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, childCount, OrcBatchReader::readLong);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase FLOAT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readFloat);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readFloat);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DOUBLE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readDouble);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, childCount, OrcBatchReader::readDouble);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DATE:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase BINARY:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase DECIMAL:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase STRUCT:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase LIST:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase MAP:\n+\t\t\t\tif (vector.noNulls) {\n+\t\t\t\t\treadNonNullMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, childCount);\n+\t\t\t\t} else {\n+\t\t\t\t\treadMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, childCount);\n+\t\t\t\t}\n+\t\t\t\tbreak;\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readNonNullLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\t\tint childCount, LongFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readNonNullDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\t\tint childCount, DoubleFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tString repeatingValue = readString(bytes.vector[0], bytes.start[0], bytes.length[0]);\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readString(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readString(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n+\t\t\t\t\tvals[i] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n+\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n+\t\t\t\t\tvals[i] = readDate(vector.vector[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n+\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse value to prevent object mutation\n+\t\t\t\t\tvals[i] = readTimestamp(vector.time[0], vector.nanos[0]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// do not reuse value to prevent object mutation\n+\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[0], vector.nanos[0]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[i], vector.nanos[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, readBigDecimal(vector.vector[0]), childCount);\n+\t\t} else {\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n+\n+\t\tint numFields = childrenTypes.size();\n+\t\t// create a batch of Rows to read the structs\n+\t\tRow[] structs = new Row[childCount];\n+\t\t// TODO: possible improvement: reuse existing Row objects\n+\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\tstructs[i] = new Row(numFields);\n+\t\t}\n+\n+\t\t// read struct fields\n+\t\t// we don't have to handle isRepeating because ORC assumes that it is propagated into the children.\n+\t\tfor (int i = 0; i < numFields; i++) {\n+\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], childCount);\n+\t\t}\n+\n+\t\tif (fieldIdx == -1) { // set struct as an object\n+\t\t\tSystem.arraycopy(structs, 0, vals, 0, childCount);\n+\t\t} else { // set struct as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, int childCount) {\n+\n+\t\tTypeDescription fieldType = schema.getChildren().get(0);\n+\t\t// get class of list elements\n+\t\tClass<?> classType = getClassForType(fieldType);\n+\n+\t\tif (list.isRepeating) {\n+\n+\t\t\tint offset = (int) list.offsets[0];\n+\t\t\tint length = (int) list.lengths[0];\n+\t\t\t// we only need to read until offset + length.\n+\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, entriesToRead);\n+\t\t\treadField(children, -1, fieldType, list.child, entriesToRead);\n+\n+\t\t\t// get function to copy list\n+\t\t\tFunction<Object, Object> copyList = getCopyFunction(schema);\n+\n+\t\t\t// create first list that will be copied\n+\t\t\tObject[] first;\n+\t\t\tif (offset == 0) {\n+\t\t\t\tfirst = children;\n+\t\t\t} else {\n+\t\t\t\tfirst = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\tSystem.arraycopy(children, offset, first, 0, length);\n+\t\t\t}\n+\n+\t\t\t// create copies of first list and set copies as result\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tObject[] copy = (Object[]) copyList.apply(first);\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = copy;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copy);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, list.childCount);\n+\t\t\treadField(children, -1, fieldType, list.child, list.childCount);\n+\n+\t\t\t// fill lists with children\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\tint length = (int) list.lengths[i];\n+\n+\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = temp;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readNonNullMapColumn(Object[] vals, int fieldIdx, MapColumnVector mapsVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> fieldType = schema.getChildren();\n+\t\tTypeDescription keyType = fieldType.get(0);\n+\t\tTypeDescription valueType = fieldType.get(1);\n+\n+\t\tColumnVector keys = mapsVector.keys;\n+\t\tColumnVector values = mapsVector.values;\n+\n+\t\tif (mapsVector.isRepeating) {\n+\t\t\t// first map is repeated\n+\n+\t\t\t// get map copy function\n+\t\t\tFunction<Object, Object> copyMap = getCopyFunction(schema);\n+\n+\t\t\t// set all key and value entries except those of the first map to null\n+\t\t\tint offset = (int) mapsVector.offsets[0];\n+\t\t\tint length = (int) mapsVector.lengths[0];\n+\t\t\t// we only need to read until offset + length.\n+\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\tObject[] keyRows = new Object[entriesToRead];\n+\t\t\tObject[] valueRows = new Object[entriesToRead];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, entriesToRead);\n+\t\t\treadField(valueRows, -1, valueType, values, entriesToRead);\n+\n+\t\t\t// create first map that will be copied\n+\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, length);\n+\n+\t\t\t// copy first map and set copy as result\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = copyMap.apply(map);\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copyMap.apply(map));\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} else {\n+\n+\t\t\tObject[] keyRows = new Object[mapsVector.childCount];\n+\t\t\tObject[] valueRows = new Object[mapsVector.childCount];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, keyRows.length);\n+\t\t\treadField(valueRows, -1, valueType, values, valueRows.length);\n+\n+\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n+\t\t\tint offset = 0;\n+\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tlong numMapEntries = lengthVectorMap[i];\n+\t\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, numMapEntries);\n+\t\t\t\toffset += numMapEntries;\n+\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = map;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, map);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t}\n+\n+\tprivate static <T> void readLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\tint childCount, LongFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call.\n+\t\t\t\treadNonNullLongColumn(vals, fieldIdx, vector, childCount, reader);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static <T> void readDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector,\n+\t\t\t\t\t\t\t\t\t\t\t\tint childCount, DoubleFunction<T> reader) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, vector, childCount, reader);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (bytes.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, bytes, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = bytes.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readString(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readString(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, int childCount) {\n+\n+\t\tif (bytes.isRepeating) { // fill complete column with first value\n+\t\t\tif (bytes.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, bytes, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = bytes.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t\t\tvals[i] = ts;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n+\t\t\t\t\t\trows[i].setField(fieldIdx, ts);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, int childCount) {\n+\n+\t\tif (vector.isRepeating) { // fill complete column with first value\n+\t\t\tif (vector.isNull[0]) {\n+\t\t\t\t// fill vals with null values\n+\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n+\t\t\t} else {\n+\t\t\t\t// read repeating non-null value by forwarding call\n+\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, vector, childCount);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tboolean[] isNullVector = vector.isNull;\n+\t\t\tif (fieldIdx == -1) { // set as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n+\n+\t\tint numFields = childrenTypes.size();\n+\n+\t\t// Early out if struct column is repeating and always null.\n+\t\t// This is the only repeating case we need to handle.\n+\t\t// ORC assumes that repeating values have been pushed to the children.\n+\t\tif (structVector.isRepeating && structVector.isNull[0]) {\n+\t\t\tif (fieldIdx < 0) {\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tvals[i] = null;\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn;\n+\t\t}\n+\n+\t\t// create a batch of Rows to read the structs\n+\t\tRow[] structs = new Row[childCount];\n+\t\t// TODO: possible improvement: reuse existing Row objects\n+\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\tstructs[i] = new Row(numFields);\n+\t\t}\n+\n+\t\t// read struct fields\n+\t\tfor (int i = 0; i < numFields; i++) {\n+\t\t\tColumnVector fieldVector = structVector.fields[i];\n+\t\t\tif (!fieldVector.isRepeating) {\n+\t\t\t\t// Reduce fieldVector reads by setting all entries null where struct is null.\n+\t\t\t\tif (fieldVector.noNulls) {\n+\t\t\t\t\t// fieldVector had no nulls. Just use struct null information.\n+\t\t\t\t\tSystem.arraycopy(structVector.isNull, 0, fieldVector.isNull, 0, structVector.isNull.length);\n+\t\t\t\t\tstructVector.fields[i].noNulls = false;\n+\t\t\t\t} else {\n+\t\t\t\t\t// fieldVector had nulls. Merge field nulls with struct nulls.\n+\t\t\t\t\tfor (int j = 0; j < structVector.isNull.length; j++) {\n+\t\t\t\t\t\tstructVector.fields[i].isNull[j] = structVector.isNull[j] || structVector.fields[i].isNull[j];\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], childCount);\n+\t\t}\n+\n+\t\tboolean[] isNullVector = structVector.isNull;\n+\n+\t\tif (fieldIdx == -1) { // set struct as an object\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\tvals[i] = null;\n+\t\t\t\t} else {\n+\t\t\t\t\tvals[i] = structs[i];\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else { // set struct as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t} else {\n+\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, int childCount) {\n+\n+\t\tTypeDescription fieldType = schema.getChildren().get(0);\n+\t\t// get class of list elements\n+\t\tClass<?> classType = getClassForType(fieldType);\n+\n+\t\tif (list.isRepeating) {\n+\t\t\t// list values are repeating. we only need to read the first list and copy it.\n+\n+\t\t\tif (list.isNull[0]) {\n+\t\t\t\t// Even better. The first list is null and so are all lists are null\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t} else {\n+\t\t\t\t// Get function to copy list\n+\t\t\t\tFunction<Object, Object> copyList = getCopyFunction(schema);\n+\n+\t\t\t\tint offset = (int) list.offsets[0];\n+\t\t\t\tint length = (int) list.lengths[0];\n+\t\t\t\t// we only need to read until offset + length.\n+\t\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t\t// read entries\n+\t\t\t\tObject[] children = (Object[]) Array.newInstance(classType, entriesToRead);\n+\t\t\t\treadField(children, -1, fieldType, list.child, entriesToRead);\n+\n+\t\t\t\t// create first list which will be copied\n+\t\t\t\tObject[] temp;\n+\t\t\t\tif (offset == 0) {\n+\t\t\t\t\ttemp = children;\n+\t\t\t\t} else {\n+\t\t\t\t\ttemp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\t}\n+\n+\t\t\t\t// copy repeated list and set copy as result\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tObject[] copy = (Object[]) copyList.apply(temp);\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = copy;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copy);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t} else {\n+\t\t\tif (!list.child.isRepeating) {\n+\t\t\t\tboolean[] childIsNull = new boolean[list.childCount];\n+\t\t\t\tArrays.fill(childIsNull, true);\n+\t\t\t\t// forward info of null lists into child vector\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info of entries of non-null lists\n+\t\t\t\t\tif (!list.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\t\t\tint length = (int) list.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(list.child.isNull, offset, childIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of children vector\n+\t\t\t\tlist.child.isNull = childIsNull;\n+\t\t\t\tlist.child.noNulls = false;\n+\t\t\t}\n+\n+\t\t\t// read children\n+\t\t\tObject[] children = (Object[]) Array.newInstance(classType, list.childCount);\n+\t\t\treadField(children, -1, fieldType, list.child, list.childCount);\n+\n+\t\t\tObject[] temp;\n+\t\t\t// fill lists with children\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\n+\t\t\t\tif (list.isNull[i]) {\n+\t\t\t\t\ttemp = null;\n+\t\t\t\t} else {\n+\t\t\t\t\tint offset = (int) list.offsets[i];\n+\t\t\t\t\tint length = (int) list.lengths[i];\n+\n+\t\t\t\t\ttemp = (Object[]) Array.newInstance(classType, length);\n+\t\t\t\t\tSystem.arraycopy(children, offset, temp, 0, length);\n+\t\t\t\t}\n+\n+\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\tvals[i] = temp;\n+\t\t\t\t} else {\n+\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static void readMapColumn(Object[] vals, int fieldIdx, MapColumnVector map, TypeDescription schema, int childCount) {\n+\n+\t\tList<TypeDescription> fieldType = schema.getChildren();\n+\t\tTypeDescription keyType = fieldType.get(0);\n+\t\tTypeDescription valueType = fieldType.get(1);\n+\n+\t\tColumnVector keys = map.keys;\n+\t\tColumnVector values = map.values;\n+\n+\t\tif (map.isRepeating) {\n+\t\t\t// map values are repeating. we only need to read the first map and copy it.\n+\n+\t\t\tif (map.isNull[0]) {\n+\t\t\t\t// Even better. The first map is null and so are all maps are null\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, null);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t} else {\n+\t\t\t\t// Get function to copy map\n+\t\t\t\tFunction<Object, Object> copyMap = getCopyFunction(schema);\n+\n+\t\t\t\tint offset = (int) map.offsets[0];\n+\t\t\t\tint length = (int) map.lengths[0];\n+\t\t\t\t// we only need to read until offset + length.\n+\t\t\t\tint entriesToRead = offset + length;\n+\n+\t\t\t\tObject[] keyRows = new Object[entriesToRead];\n+\t\t\t\tObject[] valueRows = new Object[entriesToRead];\n+\n+\t\t\t\t// read map keys and values\n+\t\t\t\treadField(keyRows, -1, keyType, keys, entriesToRead);\n+\t\t\t\treadField(valueRows, -1, valueType, values, entriesToRead);\n+\n+\t\t\t\t// create first map which will be copied\n+\t\t\t\tHashMap temp = readHashMap(keyRows, valueRows, offset, length);\n+\n+\t\t\t\t// copy repeated map and set copy as result\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (fieldIdx == -1) {\n+\t\t\t\t\t\tvals[i] = copyMap.apply(temp);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, copyMap.apply(temp));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// ensure only keys and values that are referenced by non-null maps are set to non-null\n+\n+\t\t\tif (!keys.isRepeating) {\n+\t\t\t\t// propagate is null info of map into keys vector\n+\t\t\t\tboolean[] keyIsNull = new boolean[map.childCount];\n+\t\t\t\tArrays.fill(keyIsNull, true);\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info for keys of non-null maps\n+\t\t\t\t\tif (!map.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) map.offsets[i];\n+\t\t\t\t\t\tint length = (int) map.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(keys.isNull, offset, keyIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of keys vector\n+\t\t\t\tkeys.isNull = keyIsNull;\n+\t\t\t\tkeys.noNulls = false;\n+\t\t\t}\n+\t\t\tif (!values.isRepeating) {\n+\t\t\t\t// propagate is null info of map into values vector\n+\t\t\t\tboolean[] valIsNull = new boolean[map.childCount];\n+\t\t\t\tArrays.fill(valIsNull, true);\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\t// preserve isNull info for vals of non-null maps\n+\t\t\t\t\tif (!map.isNull[i]) {\n+\t\t\t\t\t\tint offset = (int) map.offsets[i];\n+\t\t\t\t\t\tint length = (int) map.lengths[i];\n+\t\t\t\t\t\tSystem.arraycopy(values.isNull, offset, valIsNull, offset, length);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\t// override isNull of values vector\n+\t\t\t\tvalues.isNull = valIsNull;\n+\t\t\t\tvalues.noNulls = false;\n+\t\t\t}\n+\n+\t\t\tObject[] keyRows = new Object[map.childCount];\n+\t\t\tObject[] valueRows = new Object[map.childCount];\n+\n+\t\t\t// read map keys and values\n+\t\t\treadField(keyRows, -1, keyType, keys, keyRows.length);\n+\t\t\treadField(valueRows, -1, valueType, values, valueRows.length);\n+\n+\t\t\tboolean[] isNullVector = map.isNull;\n+\t\t\tlong[] lengths = map.lengths;\n+\t\t\tlong[] offsets = map.offsets;\n+\n+\t\t\tif (fieldIdx == -1) { // set map as an object\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\tvals[i] = null;\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tvals[i] = readHashMap(keyRows, valueRows, (int) offsets[i], lengths[i]);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else { // set map as a field of Row\n+\t\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\t\tif (isNullVector[i]) {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\trows[i].setField(fieldIdx, readHashMap(keyRows, valueRows, (int) offsets[i], lengths[i]));\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Sets a repeating value to all objects or row fields of the passed vals array.\n+\t *\n+\t * @param vals The array of objects or Rows.\n+\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n+\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n+\t * @param repeatingValue The value that is set.\n+\t * @param childCount The number of times the value is set.\n+\t */\n+\tprivate static void fillColumnWithRepeatingValue(Object[] vals, int fieldIdx, Object repeatingValue, int childCount) {\n+\n+\t\tif (fieldIdx == -1) {\n+\t\t\t// set value as an object\n+\t\t\tArrays.fill(vals, 0, childCount, repeatingValue);\n+\t\t} else {\n+\t\t\t// set value as a field of Row\n+\t\t\tRow[] rows = (Row[]) vals;\n+\t\t\tfor (int i = 0; i < childCount; i++) {\n+\t\t\t\trows[i].setField(fieldIdx, repeatingValue);\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static Class<?> getClassForType(TypeDescription schema) {\n+\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\t\treturn Boolean.class;\n+\t\t\tcase BYTE:\n+\t\t\t\treturn Byte.class;\n+\t\t\tcase SHORT:\n+\t\t\t\treturn Short.class;\n+\t\t\tcase INT:\n+\t\t\t\treturn Integer.class;\n+\t\t\tcase LONG:\n+\t\t\t\treturn Long.class;\n+\t\t\tcase FLOAT:\n+\t\t\t\treturn Float.class;\n+\t\t\tcase DOUBLE:\n+\t\t\t\treturn Double.class;\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\t\treturn String.class;\n+\t\t\tcase DATE:\n+\t\t\t\treturn Date.class;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn Timestamp.class;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn byte[].class;\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn BigDecimal.class;\n+\t\t\tcase STRUCT:\n+\t\t\t\treturn Row.class;\n+\t\t\tcase LIST:\n+\t\t\t\tClass<?> childClass = getClassForType(schema.getChildren().get(0));\n+\t\t\t\treturn Array.newInstance(childClass, 0).getClass();\n+\t\t\tcase MAP:\n+\t\t\t\treturn HashMap.class;\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static Boolean readBoolean(long l) {\n+\t\treturn l != 0;\n+\t}\n+\n+\tprivate static Byte readByte(long l) {\n+\t\treturn (byte) l;\n+\t}\n+\n+\tprivate static Short readShort(long l) {\n+\t\treturn (short) l;\n+\t}\n+\n+\tprivate static Integer readInt(long l) {\n+\t\treturn (int) l;\n+\t}\n+\n+\tprivate static Long readLong(long l) {\n+\t\treturn l;\n+\t}\n+\n+\tprivate static Float readFloat(double d) {\n+\t\treturn (float) d;\n+\t}\n+\n+\tprivate static Double readDouble(double d) {\n+\t\treturn d;\n+\t}\n+\n+\tprivate static Date readDate(long l) {\n+\t\t// day to milliseconds\n+\t\tfinal long t = l * MILLIS_PER_DAY;\n+\t\t// adjust by local timezone\n+\t\treturn new java.sql.Date(t - LOCAL_TZ.getOffset(t));\n+\t}\n+\n+\tprivate static String readString(byte[] bytes, int start, int length) {\n+\t\treturn new String(bytes, start, length, StandardCharsets.UTF_8);\n+\t}\n+\n+\tprivate static byte[] readBinary(byte[] src, int srcPos, int length) {\n+\t\tbyte[] result = new byte[length];\n+\t\tSystem.arraycopy(src, srcPos, result, 0, length);\n+\t\treturn result;\n+\t}\n+\n+\tprivate static BigDecimal readBigDecimal(HiveDecimalWritable hiveDecimalWritable) {\n+\t\tHiveDecimal hiveDecimal = hiveDecimalWritable.getHiveDecimal();\n+\t\treturn hiveDecimal.bigDecimalValue();\n+\t}\n+\n+\tprivate static Timestamp readTimestamp(long time, int nanos) {\n+\t\tTimestamp ts = new Timestamp(time);\n+\t\tts.setNanos(nanos);\n+\t\treturn ts;\n+\t}\n+\n+\tprivate static HashMap readHashMap(Object[] keyRows, Object[] valueRows, int offset, long length) {\n+\t\tHashMap<Object, Object> resultMap = new HashMap<>();\n+\t\tfor (int j = 0; j < length; j++) {\n+\t\t\tresultMap.put(keyRows[offset], valueRows[offset]);\n+\t\t\toffset++;\n+\t\t}\n+\t\treturn resultMap;\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static Function<Object, Object> getCopyFunction(TypeDescription schema) {\n+\t\t// check the type of the vector to decide how to read it.\n+\t\tswitch (schema.getCategory()) {\n+\t\t\tcase BOOLEAN:\n+\t\t\tcase BYTE:\n+\t\t\tcase SHORT:\n+\t\t\tcase INT:\n+\t\t\tcase LONG:\n+\t\t\tcase FLOAT:\n+\t\t\tcase DOUBLE:\n+\t\t\tcase CHAR:\n+\t\t\tcase VARCHAR:\n+\t\t\tcase STRING:\n+\t\t\tcase DECIMAL:\n+\t\t\t\treturn OrcBatchReader::returnImmutable;\n+\t\t\tcase DATE:\n+\t\t\t\treturn OrcBatchReader::copyDate;\n+\t\t\tcase TIMESTAMP:\n+\t\t\t\treturn OrcBatchReader::copyTimestamp;\n+\t\t\tcase BINARY:\n+\t\t\t\treturn OrcBatchReader::copyBinary;\n+\t\t\tcase STRUCT:\n+\t\t\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n+\t\t\t\tFunction<Object, Object>[] copyFields = new Function[fieldTypes.size()];\n+\t\t\t\tfor (int i = 0; i < fieldTypes.size(); i++) {\n+\t\t\t\t\tcopyFields[i] = getCopyFunction(fieldTypes.get(i));\n+\t\t\t\t}\n+\t\t\t\treturn new CopyStruct(copyFields);\n+\t\t\tcase LIST:\n+\t\t\t\tTypeDescription entryType = schema.getChildren().get(0);\n+\t\t\t\tFunction<Object, Object> copyEntry = getCopyFunction(entryType);\n+\t\t\t\tClass entryClass = getClassForType(entryType);\n+\t\t\t\treturn new CopyList(copyEntry, entryClass);\n+\t\t\tcase MAP:\n+\t\t\t\tTypeDescription keyType = schema.getChildren().get(0);\n+\t\t\t\tTypeDescription valueType = schema.getChildren().get(1);\n+\t\t\t\tFunction<Object, Object> copyKey = getCopyFunction(keyType);\n+\t\t\t\tFunction<Object, Object> copyValue = getCopyFunction(valueType);\n+\t\t\t\treturn new CopyMap(copyKey, copyValue);\n+\t\t\tcase UNION:\n+\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n+\t\t\tdefault:\n+\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n+\t\t}\n+\t}\n+\n+\tprivate static Object returnImmutable(Object o) {\n+\t\treturn o;\n+\t}\n+\n+\tprivate static Date copyDate(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tlong date = ((Date) o).getTime();\n+\t\t\treturn new Date(date);\n+\t\t}\n+\t}\n+\n+\tprivate static Timestamp copyTimestamp(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tlong millis = ((Timestamp) o).getTime();\n+\t\t\tint nanos = ((Timestamp) o).getNanos();\n+\t\t\tTimestamp copy = new Timestamp(millis);\n+\t\t\tcopy.setNanos(nanos);\n+\t\t\treturn copy;\n+\t\t}\n+\t}\n+\n+\tprivate static byte[] copyBinary(Object o) {\n+\t\tif (o == null) {\n+\t\t\treturn null;\n+\t\t} else {\n+\t\t\tint length = ((byte[]) o).length;\n+\t\t\treturn Arrays.copyOf((byte[]) o, length);\n+\t\t}\n+\t}\n+\n+\tprivate static class CopyStruct implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object>[] copyFields;\n+\n+\t\tCopyStruct(Function<Object, Object>[] copyFields) {\n+\t\t\tthis.copyFields = copyFields;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tRow r = (Row) o;\n+\t\t\t\tRow copy = new Row(copyFields.length);\n+\t\t\t\tfor (int i = 0; i < copyFields.length; i++) {\n+\t\t\t\t\tcopy.setField(i, copyFields[i].apply(r.getField(i)));\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tprivate static class CopyList implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object> copyEntry;\n+\t\tprivate final Class entryClass;\n+\n+\t\tCopyList(Function<Object, Object> copyEntry, Class entryClass) {\n+\t\t\tthis.copyEntry = copyEntry;\n+\t\t\tthis.entryClass = entryClass;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tObject[] l = (Object[]) o;\n+\t\t\t\tObject[] copy = (Object[]) Array.newInstance(entryClass, l.length);\n+\t\t\t\tfor (int i = 0; i < l.length; i++) {\n+\t\t\t\t\tcopy[i] = copyEntry.apply(l[i]);\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate static class CopyMap implements Function<Object, Object> {\n+\n+\t\tprivate final Function<Object, Object> copyKey;\n+\t\tprivate final Function<Object, Object> copyValue;\n+\n+\t\tCopyMap(Function<Object, Object> copyKey, Function<Object, Object> copyValue) {\n+\t\t\tthis.copyKey = copyKey;\n+\t\t\tthis.copyValue = copyValue;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic Object apply(Object o) {\n+\t\t\tif (o == null) {\n+\t\t\t\treturn null;\n+\t\t\t} else {\n+\t\t\t\tMap<Object, Object> m = (Map<Object, Object>) o;\n+\t\t\t\tHashMap<Object, Object> copy = new HashMap<>(m.size());\n+\n+\t\t\t\tfor (Map.Entry<Object, Object> e : m.entrySet()) {\n+\t\t\t\t\tObject keyCopy = copyKey.apply(e.getKey());\n+\t\t\t\t\tObject valueCopy = copyValue.apply(e.getValue());\n+\t\t\t\t\tcopy.put(keyCopy, valueCopy);\n+\t\t\t\t}\n+\t\t\t\treturn copy;\n+\t\t\t}\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java",
                "sha": "3ecdeb3c956dbeaacca29ebbfed1218d6a465a63",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 2,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java",
                "patch": "@@ -55,7 +55,7 @@\n import java.util.Arrays;\n import java.util.List;\n \n-import static org.apache.flink.orc.OrcUtils.fillRows;\n+import static org.apache.flink.orc.OrcBatchReader.fillRows;\n \n /**\n  * InputFormat to read ORC files.\n@@ -128,7 +128,7 @@ public OrcRowInputFormat(String path, TypeDescription orcSchema, Configuration o\n \n \t\t// configure OrcRowInputFormat\n \t\tthis.schema = orcSchema;\n-\t\tthis.rowType = (RowTypeInfo) OrcUtils.schemaToTypeInfo(schema);\n+\t\tthis.rowType = (RowTypeInfo) OrcBatchReader.schemaToTypeInfo(schema);\n \t\tthis.conf = orcConfig;\n \t\tthis.batchSize = batchSize;\n ",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcRowInputFormat.java",
                "sha": "61575ad727cb1338d029fcc87501156d79453925",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 1,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java",
                "patch": "@@ -127,7 +127,7 @@ private OrcTableSource(String path, TypeDescription orcSchema, Configuration orc\n \t\tthis.predicates = predicates;\n \n \t\t// determine the type information from the ORC schema\n-\t\tRowTypeInfo typeInfoFromSchema = (RowTypeInfo) OrcUtils.schemaToTypeInfo(this.orcSchema);\n+\t\tRowTypeInfo typeInfoFromSchema = (RowTypeInfo) OrcBatchReader.schemaToTypeInfo(this.orcSchema);\n \n \t\t// set return type info\n \t\tif (selectedFields == null) {",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcTableSource.java",
                "sha": "0eab4a043da3cd9c3d0429e90b62014621ef1f76",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java",
                "changes": 1508,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java?ref=3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb",
                "deletions": 1508,
                "filename": "flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java",
                "patch": "@@ -1,1508 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.flink.orc;\n-\n-import org.apache.flink.api.common.typeinfo.BasicTypeInfo;\n-import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;\n-import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;\n-import org.apache.flink.api.common.typeinfo.TypeInformation;\n-import org.apache.flink.api.java.typeutils.MapTypeInfo;\n-import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;\n-import org.apache.flink.api.java.typeutils.RowTypeInfo;\n-import org.apache.flink.types.Row;\n-\n-import org.apache.hadoop.hive.common.type.HiveDecimal;\n-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;\n-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n-import org.apache.orc.TypeDescription;\n-\n-import java.lang.reflect.Array;\n-import java.math.BigDecimal;\n-import java.nio.charset.StandardCharsets;\n-import java.sql.Date;\n-import java.sql.Timestamp;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.TimeZone;\n-import java.util.function.DoubleFunction;\n-import java.util.function.IntFunction;\n-import java.util.function.LongFunction;\n-\n-/**\n- * A class that provides utility methods for orc file reading.\n- */\n-class OrcUtils {\n-\n-\tprivate static final long MILLIS_PER_DAY = 86400000; // = 24 * 60 * 60 * 1000\n-\tprivate static final TimeZone LOCAL_TZ = TimeZone.getDefault();\n-\n-\t/**\n-\t * Converts an ORC schema to a Flink TypeInformation.\n-\t *\n-\t * @param schema The ORC schema.\n-\t * @return The TypeInformation that corresponds to the ORC schema.\n-\t */\n-\tstatic TypeInformation schemaToTypeInfo(TypeDescription schema) {\n-\t\tswitch (schema.getCategory()) {\n-\t\t\tcase BOOLEAN:\n-\t\t\t\treturn BasicTypeInfo.BOOLEAN_TYPE_INFO;\n-\t\t\tcase BYTE:\n-\t\t\t\treturn BasicTypeInfo.BYTE_TYPE_INFO;\n-\t\t\tcase SHORT:\n-\t\t\t\treturn BasicTypeInfo.SHORT_TYPE_INFO;\n-\t\t\tcase INT:\n-\t\t\t\treturn BasicTypeInfo.INT_TYPE_INFO;\n-\t\t\tcase LONG:\n-\t\t\t\treturn BasicTypeInfo.LONG_TYPE_INFO;\n-\t\t\tcase FLOAT:\n-\t\t\t\treturn BasicTypeInfo.FLOAT_TYPE_INFO;\n-\t\t\tcase DOUBLE:\n-\t\t\t\treturn BasicTypeInfo.DOUBLE_TYPE_INFO;\n-\t\t\tcase DECIMAL:\n-\t\t\t\treturn BasicTypeInfo.BIG_DEC_TYPE_INFO;\n-\t\t\tcase STRING:\n-\t\t\tcase CHAR:\n-\t\t\tcase VARCHAR:\n-\t\t\t\treturn BasicTypeInfo.STRING_TYPE_INFO;\n-\t\t\tcase DATE:\n-\t\t\t\treturn SqlTimeTypeInfo.DATE;\n-\t\t\tcase TIMESTAMP:\n-\t\t\t\treturn SqlTimeTypeInfo.TIMESTAMP;\n-\t\t\tcase BINARY:\n-\t\t\t\treturn PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO;\n-\t\t\tcase STRUCT:\n-\t\t\t\tList<TypeDescription> fieldSchemas = schema.getChildren();\n-\t\t\t\tTypeInformation[] fieldTypes = new TypeInformation[fieldSchemas.size()];\n-\t\t\t\tfor (int i = 0; i < fieldSchemas.size(); i++) {\n-\t\t\t\t\tfieldTypes[i] = schemaToTypeInfo(fieldSchemas.get(i));\n-\t\t\t\t}\n-\t\t\t\tString[] fieldNames = schema.getFieldNames().toArray(new String[]{});\n-\t\t\t\treturn new RowTypeInfo(fieldTypes, fieldNames);\n-\t\t\tcase LIST:\n-\t\t\t\tTypeDescription elementSchema = schema.getChildren().get(0);\n-\t\t\t\tTypeInformation<?> elementType = schemaToTypeInfo(elementSchema);\n-\t\t\t\t// arrays of primitive types are handled as object arrays to support null values\n-\t\t\t\treturn ObjectArrayTypeInfo.getInfoFor(elementType);\n-\t\t\tcase MAP:\n-\t\t\t\tTypeDescription keySchema = schema.getChildren().get(0);\n-\t\t\t\tTypeDescription valSchema = schema.getChildren().get(1);\n-\t\t\t\tTypeInformation<?> keyType = schemaToTypeInfo(keySchema);\n-\t\t\t\tTypeInformation<?> valType = schemaToTypeInfo(valSchema);\n-\t\t\t\treturn new MapTypeInfo<>(keyType, valType);\n-\t\t\tcase UNION:\n-\t\t\t\tthrow new UnsupportedOperationException(\"UNION type is not supported yet.\");\n-\t\t\tdefault:\n-\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Fills an ORC batch into an array of Row.\n-\t *\n-\t * @param rows The batch of rows need to be filled.\n-\t * @param schema The schema of the ORC data.\n-\t * @param batch The ORC data.\n-\t * @param selectedFields The list of selected ORC fields.\n-\t * @return The number of rows that were filled.\n-\t */\n-\tstatic int fillRows(Row[] rows, TypeDescription schema, VectorizedRowBatch batch, int[] selectedFields) {\n-\n-\t\tint rowsToRead = Math.min((int) batch.count(), rows.length);\n-\n-\t\tList<TypeDescription> fieldTypes = schema.getChildren();\n-\t\t// read each selected field\n-\t\tfor (int rowIdx = 0; rowIdx < selectedFields.length; rowIdx++) {\n-\t\t\tint orcIdx = selectedFields[rowIdx];\n-\t\t\treadField(rows, rowIdx, fieldTypes.get(orcIdx), batch.cols[orcIdx], null, rowsToRead);\n-\t\t}\n-\t\treturn rowsToRead;\n-\t}\n-\n-\t/**\n-\t * Reads a vector of data into an array of objects.\n-\t *\n-\t * @param vals The array that needs to be filled.\n-\t * @param fieldIdx If the vals array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param schema The schema of the vector to read.\n-\t * @param vector The vector to read.\n-\t * @param lengthVector If the vector is of type List or Map, the number of sub-elements to read for each field. Otherwise, it must be null.\n-\t * @param childCount The number of vector entries to read.\n-\t */\n-\tprivate static void readField(Object[] vals, int fieldIdx, TypeDescription schema, ColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check the type of the vector to decide how to read it.\n-\t\tswitch (schema.getCategory()) {\n-\t\t\tcase BOOLEAN:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readBoolean, OrcUtils::boolArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readBoolean, OrcUtils::boolArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase BYTE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readByte, OrcUtils::byteArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readByte, OrcUtils::byteArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase SHORT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readShort, OrcUtils::shortArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readShort, OrcUtils::shortArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase INT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readInt, OrcUtils::intArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readInt, OrcUtils::intArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase LONG:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readLong, OrcUtils::longArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumn(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount, OrcUtils::readLong, OrcUtils::longArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase FLOAT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readFloat, OrcUtils::floatArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readFloat, OrcUtils::floatArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DOUBLE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readDouble, OrcUtils::doubleArray);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDoubleColumn(vals, fieldIdx, (DoubleColumnVector) vector, lengthVector, childCount, OrcUtils::readDouble, OrcUtils::doubleArray);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase CHAR:\n-\t\t\tcase VARCHAR:\n-\t\t\tcase STRING:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadBytesColumnAsString(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DATE:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadLongColumnAsDate(vals, fieldIdx, (LongColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase TIMESTAMP:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadTimestampColumn(vals, fieldIdx, (TimestampColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase BINARY:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadBytesColumnAsBinary(vals, fieldIdx, (BytesColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase DECIMAL:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadDecimalColumn(vals, fieldIdx, (DecimalColumnVector) vector, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase STRUCT:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadStructColumn(vals, fieldIdx, (StructColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase LIST:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadListColumn(vals, fieldIdx, (ListColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase MAP:\n-\t\t\t\tif (vector.noNulls) {\n-\t\t\t\t\treadNonNullMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t} else {\n-\t\t\t\t\treadMapColumn(vals, fieldIdx, (MapColumnVector) vector, schema, lengthVector, childCount);\n-\t\t\t\t}\n-\t\t\t\tbreak;\n-\t\t\tcase UNION:\n-\t\t\t\tthrow new UnsupportedOperationException(\"UNION type not supported yet\");\n-\t\t\tdefault:\n-\t\t\t\tthrow new IllegalArgumentException(\"Unknown type \" + schema);\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readNonNullLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\t\tLongFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tT[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readNonNullDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\t\tDoubleFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tT[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tT repeatingValue = reader.apply(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\tString repeatingValue = new String(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, repeatingValue, childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = new String(bytes.vector[i], bytes.start[i], bytes.length[i], StandardCharsets.UTF_8);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, new String(bytes.vector[i], bytes.start[i], bytes.length[i], StandardCharsets.UTF_8));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tString[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\tString repeatingValue = new String(bytes.vector[0], bytes.start[0], bytes.length[0], StandardCharsets.UTF_8);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = new String(bytes.vector[offset], bytes.start[offset], bytes.length[offset], StandardCharsets.UTF_8);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n-\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// don't reuse repeating val to avoid object mutation\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tbyte[][] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[0], bytes.start[0], bytes.length[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n-\t\t\t\t\t\tvals[i] = readDate(vector.vector[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse repeated value due to mutability of Date\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tDate[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readDate(vector.vector[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readDate(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the timestamps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\tvals[i] = readTimestamp(vector.time[0], vector.nanos[0]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[0], vector.nanos[0]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readTimestamp(vector.time[i], vector.nanos[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tTimestamp[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\t// do not reuse value to prevent object mutation\n-\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[0], vector.nanos[0]);\n-\t\t\t\t\t}\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[offset], vector.nanos[offset]);\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the decimals need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, readBigDecimal(vector.vector[0]), childCount);\n-\t\t\t} else {\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tBigDecimal[] temp;\n-\t\t\tint offset = 0;\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\tBigDecimal repeatingValue = readBigDecimal(vector.vector[0]);\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tArrays.fill(temp, repeatingValue);\n-\t\t\t\t\toffset += temp.length;\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\ttemp[j] = readBigDecimal(vector.vector[offset++]);\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t}\n-\n-\tprivate static void readNonNullStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n-\n-\t\tint numFields = childrenTypes.size();\n-\t\t// create a batch of Rows to read the structs\n-\t\tRow[] structs = new Row[childCount];\n-\t\t// TODO: possible improvement: reuse existing Row objects\n-\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\tstructs[i] = new Row(numFields);\n-\t\t}\n-\n-\t\t// read struct fields\n-\t\tfor (int i = 0; i < numFields; i++) {\n-\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], null, childCount);\n-\t\t}\n-\n-\t\t// check if the structs need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (fieldIdx == -1) { // set struct as an object\n-\t\t\t\tSystem.arraycopy(structs, 0, vals, 0, childCount);\n-\t\t\t} else { // set struct as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // struct in a list\n-\t\t\tint offset = 0;\n-\t\t\tRow[] temp;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new Row[(int) lengthVector[i]];\n-\t\t\t\tSystem.arraycopy(structs, offset, temp, 0, temp.length);\n-\t\t\t\toffset = offset + temp.length;\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tTypeDescription fieldType = schema.getChildren().get(0);\n-\t\t// check if the list need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\treadField(vals, fieldIdx, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t} else { // list in a list\n-\t\t\tObject[] nestedLists = new Object[childCount];\n-\t\t\t// length vector for nested list\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\t// read nested list\n-\t\t\treadField(nestedLists, -1, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t\t// get type of nestedList\n-\t\t\tClass<?> classType = nestedLists[0].getClass();\n-\n-\t\t\t// fill outer list with nested list\n-\t\t\tint offset = 0;\n-\t\t\tint length;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\tlength = (int) lengthVector[i];\n-\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n-\t\t\t\tSystem.arraycopy(nestedLists, offset, temp, 0, length);\n-\t\t\t\toffset = offset + length;\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readNonNullMapColumn(Object[] vals, int fieldIdx, MapColumnVector mapsVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> fieldType = schema.getChildren();\n-\t\tTypeDescription keyType = fieldType.get(0);\n-\t\tTypeDescription valueType = fieldType.get(1);\n-\n-\t\tColumnVector keys = mapsVector.keys;\n-\t\tColumnVector values = mapsVector.values;\n-\t\tObject[] keyRows = new Object[mapsVector.childCount];\n-\t\tObject[] valueRows = new Object[mapsVector.childCount];\n-\n-\t\t// read map keys and values\n-\t\treadField(keyRows, -1, keyType, keys, null, keyRows.length);\n-\t\treadField(valueRows, -1, valueType, values, null, valueRows.length);\n-\n-\t\t// check if the maps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n-\t\t\tint offset = 0;\n-\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\tlong numMapEntries = lengthVectorMap[i];\n-\t\t\t\tHashMap map = readHashMap(keyRows, valueRows, offset, numMapEntries);\n-\t\t\t\toffset += numMapEntries;\n-\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = map;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, map);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // list of map\n-\n-\t\t\tlong[] lengthVectorMap = mapsVector.lengths;\n-\t\t\tint mapOffset = 0; // offset of map element\n-\t\t\tint offset = 0; // offset of map\n-\t\t\tHashMap[] temp;\n-\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new HashMap[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tlong numMapEntries = lengthVectorMap[offset];\n-\t\t\t\t\ttemp[j] = readHashMap(keyRows, valueRows, mapOffset, numMapEntries);\n-\t\t\t\t\tmapOffset += numMapEntries;\n-\t\t\t\t\toffset++;\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == 1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readLongColumn(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\tLongFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, array);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tT[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static <T> void readDoubleColumn(Object[] vals, int fieldIdx, DoubleColumnVector vector, long[] lengthVector, int childCount,\n-\t\t\t\t\t\t\t\t\t\t\t\tDoubleFunction<T> reader, IntFunction<T[]> array) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = reader.apply(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, reader.apply(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, array);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tT[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = array.apply((int) lengthVector[i]);\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = reader.apply(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readBytesColumnAsString(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = new String(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, new String(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (bytes.isRepeating) { // fill list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::stringArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tString[] temp;\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new String[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = new String(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readBytesColumnAsBinary(Object[] vals, int fieldIdx, BytesColumnVector bytes, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the binary need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (bytes.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readBinary(bytes.vector[i], bytes.start[i], bytes.length[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (bytes.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::binaryArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tbyte[][] temp;\n-\t\t\t\tboolean[] isNullVector = bytes.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new byte[(int) lengthVector[i]][];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readBinary(bytes.vector[offset], bytes.start[offset], bytes.length[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readLongColumnAsDate(Object[] vals, int fieldIdx, LongColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the values need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readDate(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readDate(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // in a list\n-\t\t\tif (vector.isRepeating) { // // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::dateArray);\n-\t\t\t} else {\n-\t\t\t\t// column contain null values\n-\t\t\t\tint offset = 0;\n-\t\t\t\tDate[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Date[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readDate(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readTimestampColumn(Object[] vals, int fieldIdx, TimestampColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the timestamps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t\t\tvals[i] = ts;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tTimestamp ts = readTimestamp(vector.time[i], vector.nanos[i]);\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, ts);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::timestampArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tTimestamp[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new Timestamp[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readTimestamp(vector.time[offset], vector.nanos[offset]);\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readDecimalColumn(Object[] vals, int fieldIdx, DecimalColumnVector vector, long[] lengthVector, int childCount) {\n-\n-\t\t// check if the decimals need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (vector.isRepeating) { // fill complete column with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillColumnWithRepeatingValue(vals, fieldIdx, null, childCount);\n-\t\t\t} else {\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tif (fieldIdx == -1) { // set as an object\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\tvals[i] = readBigDecimal(vector.vector[i]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t} else { // set as a field of Row\n-\t\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\trows[i].setField(fieldIdx, readBigDecimal(vector.vector[i]));\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif (vector.isRepeating) { // fill complete list with first value\n-\t\t\t\t// since the column contains null values and has just one distinct value, the repeated value is null\n-\t\t\t\tfillListWithRepeatingNull(vals, fieldIdx, lengthVector, childCount, OrcUtils::decimalArray);\n-\t\t\t} else {\n-\t\t\t\tint offset = 0;\n-\t\t\t\tBigDecimal[] temp;\n-\t\t\t\tboolean[] isNullVector = vector.isNull;\n-\t\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\t\ttemp = new BigDecimal[(int) lengthVector[i]];\n-\t\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\t\toffset++;\n-\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\ttemp[j] = readBigDecimal(vector.vector[offset++]);\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readStructColumn(Object[] vals, int fieldIdx, StructColumnVector structVector, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> childrenTypes = schema.getChildren();\n-\n-\t\tint numFields = childrenTypes.size();\n-\t\t// create a batch of Rows to read the structs\n-\t\tRow[] structs = new Row[childCount];\n-\t\t// TODO: possible improvement: reuse existing Row objects\n-\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\tstructs[i] = new Row(numFields);\n-\t\t}\n-\n-\t\t// read struct fields\n-\t\tfor (int i = 0; i < numFields; i++) {\n-\t\t\treadField(structs, i, childrenTypes.get(i), structVector.fields[i], null, childCount);\n-\t\t}\n-\n-\t\tboolean[] isNullVector = structVector.isNull;\n-\n-\t\t// check if the structs need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tif (fieldIdx == -1) { // set struct as an object\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tvals[i] = structs[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else { // set struct as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, structs[i]);\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // struct in a list\n-\t\t\tint offset = 0;\n-\t\t\tRow[] temp;\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new Row[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\ttemp[j] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\ttemp[j] = structs[offset++];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == -1) { // set list of structs as an object\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else { // set list of structs as field of row\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readListColumn(Object[] vals, int fieldIdx, ListColumnVector list, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tTypeDescription fieldType = schema.getChildren().get(0);\n-\t\t// check if the lists need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\treadField(vals, fieldIdx, fieldType, list.child, lengthVectorNested, list.childCount);\n-\t\t} else { // list in a list\n-\t\t\tObject[] nestedList = new Object[childCount];\n-\t\t\t// length vector for nested list\n-\t\t\tlong[] lengthVectorNested = list.lengths;\n-\t\t\t// read nested list\n-\t\t\treadField(nestedList, -1, fieldType, list.child, lengthVectorNested, list.childCount);\n-\n-\t\t\t// fill outer list with nested list\n-\t\t\tint offset = 0;\n-\t\t\tint length;\n-\t\t\t// get type of nestedList\n-\t\t\tClass<?> classType = nestedList[0].getClass();\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\tlength = (int) lengthVector[i];\n-\t\t\t\tObject[] temp = (Object[]) Array.newInstance(classType, length);\n-\t\t\t\tSystem.arraycopy(nestedList, offset, temp, 0, length);\n-\t\t\t\toffset = offset + length;\n-\t\t\t\tif (fieldIdx == -1) { // set list of list as an object\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else { // set list of list as field of row\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void readMapColumn(Object[] vals, int fieldIdx, MapColumnVector map, TypeDescription schema, long[] lengthVector, int childCount) {\n-\n-\t\tList<TypeDescription> fieldType = schema.getChildren();\n-\t\tTypeDescription keyType = fieldType.get(0);\n-\t\tTypeDescription valueType = fieldType.get(1);\n-\n-\t\tColumnVector keys = map.keys;\n-\t\tColumnVector values = map.values;\n-\t\tObject[] keyRows = new Object[map.childCount];\n-\t\tObject[] valueRows = new Object[map.childCount];\n-\n-\t\t// read map kes and values\n-\t\treadField(keyRows, -1, keyType, keys, null, keyRows.length);\n-\t\treadField(valueRows, -1, valueType, values, null, valueRows.length);\n-\n-\t\tboolean[] isNullVector = map.isNull;\n-\n-\t\t// check if the maps need to be read into lists or as single values\n-\t\tif (lengthVector == null) {\n-\t\t\tlong[] lengthVectorMap = map.lengths;\n-\t\t\tint offset = 0;\n-\t\t\tif (fieldIdx == -1) { // set map as an object\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\tvals[i] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tvals[i] = readHashMap(keyRows, valueRows, offset, lengthVectorMap[i]);\n-\t\t\t\t\t\toffset += lengthVectorMap[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t} else { // set map as a field of Row\n-\t\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\t\tif (isNullVector[i]) {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, null);\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\trows[i].setField(fieldIdx, readHashMap(keyRows, valueRows, offset, lengthVectorMap[i]));\n-\t\t\t\t\t\toffset += lengthVectorMap[i];\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else { // list of map\n-\t\t\tlong[] lengthVectorMap = map.lengths;\n-\t\t\tint mapOffset = 0; // offset of map element\n-\t\t\tint offset = 0; // offset of map\n-\t\t\tHashMap[] temp;\n-\n-\t\t\tfor (int i = 0; offset < childCount; i++) {\n-\t\t\t\ttemp = new HashMap[(int) lengthVector[i]];\n-\t\t\t\tfor (int j = 0; j < temp.length; j++) {\n-\t\t\t\t\tif (isNullVector[offset]) {\n-\t\t\t\t\t\ttemp[j] = null;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\ttemp[j] = readHashMap(keyRows, valueRows, mapOffset, lengthVectorMap[offset]);\n-\t\t\t\t\t\tmapOffset += lengthVectorMap[offset];\n-\t\t\t\t\t\toffset++;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif (fieldIdx == -1) {\n-\t\t\t\t\tvals[i] = temp;\n-\t\t\t\t} else {\n-\t\t\t\t\t((Row) vals[i]).setField(fieldIdx, temp);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Sets a repeating value to all objects or row fields of the passed vals array.\n-\t *\n-\t * @param vals The array of objects or Rows.\n-\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param repeatingValue The value that is set.\n-\t * @param childCount The number of times the value is set.\n-\t */\n-\tprivate static void fillColumnWithRepeatingValue(Object[] vals, int fieldIdx, Object repeatingValue, int childCount) {\n-\n-\t\tif (fieldIdx == -1) {\n-\t\t\t// set value as an object\n-\t\t\tArrays.fill(vals, 0, childCount, repeatingValue);\n-\t\t} else {\n-\t\t\t// set value as a field of Row\n-\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\trows[i].setField(fieldIdx, repeatingValue);\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t/**\n-\t * Sets arrays containing only null values to all objects or row fields of the passed vals array.\n-\t *\n-\t * @param vals The array of objects or Rows to which the empty arrays are set.\n-\t * @param fieldIdx If the objs array is an array of Row, the index of the field that needs to be filled.\n-\t *                 Otherwise a -1 must be passed and the data is directly filled into the array.\n-\t * @param lengthVector The vector containing the lengths of the individual empty arrays.\n-\t * @param childCount The number of objects or Rows to fill.\n-\t * @param array A method to create arrays of the appropriate type.\n-\t * @param <T> The type of the arrays to create.\n-\t */\n-\tprivate static <T> void fillListWithRepeatingNull(Object[] vals, int fieldIdx, long[] lengthVector, int childCount, IntFunction<T[]> array) {\n-\n-\t\tif (fieldIdx == -1) {\n-\t\t\t// set empty array as object\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\tvals[i] = array.apply((int) lengthVector[i]);\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// set empty array as field in Row\n-\t\t\tRow[] rows = (Row[]) vals;\n-\t\t\tfor (int i = 0; i < childCount; i++) {\n-\t\t\t\trows[i].setField(fieldIdx, array.apply((int) lengthVector[i]));\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static Boolean readBoolean(long l) {\n-\t\treturn l != 0;\n-\t}\n-\n-\tprivate static Byte readByte(long l) {\n-\t\treturn (byte) l;\n-\t}\n-\n-\tprivate static Short readShort(long l) {\n-\t\treturn (short) l;\n-\t}\n-\n-\tprivate static Integer readInt(long l) {\n-\t\treturn (int) l;\n-\t}\n-\n-\tprivate static Long readLong(long l) {\n-\t\treturn l;\n-\t}\n-\n-\tprivate static Float readFloat(double d) {\n-\t\treturn (float) d;\n-\t}\n-\n-\tprivate static Double readDouble(double d) {\n-\t\treturn d;\n-\t}\n-\n-\tprivate static Date readDate(long l) {\n-\t\t// day to milliseconds\n-\t\tfinal long t = l * MILLIS_PER_DAY;\n-\t\t// adjust by local timezone\n-\t\treturn new java.sql.Date(t - LOCAL_TZ.getOffset(t));\n-\t}\n-\n-\tprivate static byte[] readBinary(byte[] src, int srcPos, int length) {\n-\t\tbyte[] result = new byte[length];\n-\t\tSystem.arraycopy(src, srcPos, result, 0, length);\n-\t\treturn result;\n-\t}\n-\n-\tprivate static BigDecimal readBigDecimal(HiveDecimalWritable hiveDecimalWritable) {\n-\t\tHiveDecimal hiveDecimal = hiveDecimalWritable.getHiveDecimal();\n-\t\treturn hiveDecimal.bigDecimalValue();\n-\t}\n-\n-\tprivate static Timestamp readTimestamp(long time, int nanos) {\n-\t\tTimestamp ts = new Timestamp(time);\n-\t\tts.setNanos(nanos);\n-\t\treturn ts;\n-\t}\n-\n-\tprivate static HashMap readHashMap(Object[] keyRows, Object[] valueRows, int offset, long length) {\n-\t\tHashMap<Object, Object> resultMap = new HashMap<>();\n-\t\tfor (int j = 0; j < length; j++) {\n-\t\t\tresultMap.put(keyRows[offset], valueRows[offset]);\n-\t\t\toffset++;\n-\t\t}\n-\t\treturn resultMap;\n-\t}\n-\n-\tprivate static Boolean[] boolArray(int len) {\n-\t\treturn new Boolean[len];\n-\t}\n-\n-\tprivate static Byte[] byteArray(int len) {\n-\t\treturn new Byte[len];\n-\t}\n-\n-\tprivate static Short[] shortArray(int len) {\n-\t\treturn new Short[len];\n-\t}\n-\n-\tprivate static Integer[] intArray(int len) {\n-\t\treturn new Integer[len];\n-\t}\n-\n-\tprivate static Long[] longArray(int len) {\n-\t\treturn new Long[len];\n-\t}\n-\n-\tprivate static Float[] floatArray(int len) {\n-\t\treturn new Float[len];\n-\t}\n-\n-\tprivate static Double[] doubleArray(int len) {\n-\t\treturn new Double[len];\n-\t}\n-\n-\tprivate static Date[] dateArray(int len) {\n-\t\treturn new Date[len];\n-\t}\n-\n-\tprivate static byte[][] binaryArray(int len) {\n-\t\treturn new byte[len][];\n-\t}\n-\n-\tprivate static String[] stringArray(int len) {\n-\t\treturn new String[len];\n-\t}\n-\n-\tprivate static BigDecimal[] decimalArray(int len) {\n-\t\treturn new BigDecimal[len];\n-\t}\n-\n-\tprivate static Timestamp[] timestampArray(int len) {\n-\t\treturn new Timestamp[len];\n-\t}\n-\n-}",
                "raw_url": "https://github.com/apache/flink/raw/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcUtils.java",
                "sha": "cfb4e0e66a8183da7b44fcf4672b376b01e53766",
                "status": "removed"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 4,
                "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java",
                "patch": "@@ -31,10 +31,10 @@\n import org.junit.Test;\n \n /**\n- * Unit tests for {@link OrcUtils}.\n+ * Unit tests for {@link OrcBatchReader}.\n  *\n  */\n-public class OrcUtilsTest {\n+public class OrcBatchReaderTest {\n \n \t@Test\n \tpublic void testFlatSchemaToTypeInfo1() {\n@@ -54,7 +54,7 @@ public void testFlatSchemaToTypeInfo1() {\n \t\t\t\t\"timestamp1:timestamp,\" +\n \t\t\t\t\"decimal1:decimal(5,2)\" +\n \t\t\t\">\";\n-\t\tTypeInformation typeInfo = OrcUtils.schemaToTypeInfo(TypeDescription.fromString(schema));\n+\t\tTypeInformation typeInfo = OrcBatchReader.schemaToTypeInfo(TypeDescription.fromString(schema));\n \n \t\tAssert.assertNotNull(typeInfo);\n \t\tAssert.assertTrue(typeInfo instanceof RowTypeInfo);\n@@ -106,7 +106,7 @@ public void testNestedSchemaToTypeInfo1() {\n \t\t\t\t\t\">\" +\n \t\t\t\t\">\" +\n \t\t\t\">\";\n-\t\tTypeInformation typeInfo = OrcUtils.schemaToTypeInfo(TypeDescription.fromString(schema));\n+\t\tTypeInformation typeInfo = OrcBatchReader.schemaToTypeInfo(TypeDescription.fromString(schema));\n \n \t\tAssert.assertNotNull(typeInfo);\n \t\tAssert.assertTrue(typeInfo instanceof RowTypeInfo);",
                "previous_filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcUtilsTest.java",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcBatchReaderTest.java",
                "sha": "b90313ea2aaa1f7217989182618d94cf90e129cf",
                "status": "renamed"
            },
            {
                "additions": 211,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java",
                "changes": 218,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 7,
                "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.core.fs.FileInputSplit;\n import org.apache.flink.core.fs.Path;\n+import org.apache.flink.orc.util.OrcTestFileGenerator;\n import org.apache.flink.types.Row;\n import org.apache.flink.util.InstantiationUtil;\n \n@@ -50,6 +51,7 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.eq;\n import static org.mockito.Mockito.any;\n@@ -124,6 +126,32 @@ public void tearDown() throws IOException {\n \tprivate static final String TEST_FILE_NESTEDLIST = \"test-data-nestedlist.orc\";\n \tprivate static final String TEST_SCHEMA_NESTEDLIST = \"struct<mylist1:array<array<struct<mylong1:bigint>>>>\";\n \n+\t/** Generated by {@link OrcTestFileGenerator#writeCompositeTypesWithNullsFile(String)}. */\n+\tprivate static final String TEST_FILE_COMPOSITES_NULLS = \"test-data-composites-with-nulls.orc\";\n+\tprivate static final String TEST_SCHEMA_COMPOSITES_NULLS =\n+\t\t\"struct<\" +\n+\t\t\t\"int1:int,\" +\n+\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\"list1:array<array<array<struct<f1:string,f2:string>>>>,\" +\n+\t\t\t\"list2:array<map<string,int>>\" +\n+\t\t\">\";\n+\n+\t/** Generated by {@link OrcTestFileGenerator#writeCompositeTypesWithRepeatingFile(String)}. */\n+\tprivate static final String TEST_FILE_REPEATING = \"test-data-repeating.orc\";\n+\tprivate static final String TEST_SCHEMA_REPEATING =\n+\t\t\"struct<\" +\n+\t\t\t\"int1:int,\" +\n+\t\t\t\"int2:int,\" +\n+\t\t\t\"int3:int,\" +\n+\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\"record2:struct<f1:int,f2:string>,\" +\n+\t\t\t\"list1:array<int>,\" +\n+\t\t\t\"list2:array<int>,\" +\n+\t\t\t\"list3:array<int>,\" +\n+\t\t\t\"map1:map<int,string>,\" +\n+\t\t\t\"map2:map<int,string>\" +\n+\t\t\">\";\n+\n \t@Test(expected = FileNotFoundException.class)\n \tpublic void testInvalidPath() throws IOException{\n \t\trowOrcInputFormat =\n@@ -477,7 +505,7 @@ public void testPredicateWithInvalidColumn() throws Exception {\n \t}\n \n \t@Test\n-\tpublic void testReadNestedFile() throws IOException{\n+\tpublic void testReadNestedFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_NESTED), TEST_SCHEMA_NESTED, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -563,7 +591,7 @@ public void testReadNestedFile() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadTimeTypeFile() throws IOException{\n+\tpublic void testReadTimeTypeFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_TIMETYPES), TEST_SCHEMA_TIMETYPES, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -590,7 +618,7 @@ public void testReadTimeTypeFile() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadDecimalTypeFile() throws IOException{\n+\tpublic void testReadDecimalTypeFile() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_DECIMAL), TEST_SCHEMA_DECIMAL, new Configuration());\n \n \t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n@@ -653,7 +681,183 @@ public void testReadNestedListFile() throws Exception {\n \t}\n \n \t@Test\n-\tpublic void testReadWithProjection() throws IOException{\n+\tpublic void testReadCompositesNullsFile() throws Exception {\n+\t\trowOrcInputFormat = new OrcRowInputFormat(\n+\t\t\tgetPath(TEST_FILE_COMPOSITES_NULLS),\n+\t\t\tTEST_SCHEMA_COMPOSITES_NULLS,\n+\t\t\tnew Configuration());\n+\n+\t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n+\t\tassertEquals(1, splits.length);\n+\t\trowOrcInputFormat.openInputFormat();\n+\t\trowOrcInputFormat.open(splits[0]);\n+\n+\t\tassertFalse(rowOrcInputFormat.reachedEnd());\n+\n+\t\tRow row = null;\n+\t\tlong cnt = 0;\n+\n+\t\tint structNullCnt = 0;\n+\t\tint nestedListNullCnt = 0;\n+\t\tint mapListNullCnt = 0;\n+\n+\t\t// read all rows\n+\t\twhile (!rowOrcInputFormat.reachedEnd()) {\n+\n+\t\t\trow = rowOrcInputFormat.nextRecord(row);\n+\t\t\tassertEquals(4, row.getArity());\n+\n+\t\t\tassertTrue(row.getField(0) instanceof Integer);\n+\n+\t\t\tif (row.getField(1) == null) {\n+\t\t\t\tstructNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(1);\n+\t\t\t\tassertTrue(f instanceof Row);\n+\t\t\t\tassertEquals(2, ((Row) f).getArity());\n+\t\t\t}\n+\n+\t\t\tif (row.getField(2) == null) {\n+\t\t\t\tnestedListNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(2);\n+\t\t\t\tassertTrue(f instanceof Row[][][]);\n+\t\t\t\tassertEquals(4, ((Row[][][]) f).length);\n+\t\t\t}\n+\n+\t\t\tif (row.getField(3) == null) {\n+\t\t\t\tmapListNullCnt++;\n+\t\t\t} else {\n+\t\t\t\tObject f = row.getField(3);\n+\t\t\t\tassertTrue(f instanceof HashMap[]);\n+\t\t\t\tassertEquals(3, ((HashMap[]) f).length);\n+\t\t\t}\n+\t\t\tcnt++;\n+\t\t}\n+\t\t// number of rows in file\n+\t\tassertEquals(2500, cnt);\n+\t\t// check number of null fields\n+\t\tassertEquals(1250, structNullCnt);\n+\t\tassertEquals(835, nestedListNullCnt);\n+\t\tassertEquals(835, mapListNullCnt);\n+\t}\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\t@Test\n+\tpublic void testReadRepeatingValuesFile() throws IOException {\n+\t\trowOrcInputFormat = new OrcRowInputFormat(\n+\t\t\tgetPath(TEST_FILE_REPEATING),\n+\t\t\tTEST_SCHEMA_REPEATING,\n+\t\t\tnew Configuration());\n+\n+\t\tFileInputSplit[] splits = rowOrcInputFormat.createInputSplits(1);\n+\t\tassertEquals(1, splits.length);\n+\t\trowOrcInputFormat.openInputFormat();\n+\t\trowOrcInputFormat.open(splits[0]);\n+\n+\t\tassertFalse(rowOrcInputFormat.reachedEnd());\n+\n+\t\tRow row = null;\n+\t\tlong cnt = 0;\n+\n+\t\tRow firstRow1 = null;\n+\t\tInteger[] firstList1 = null;\n+\t\tHashMap firstMap1 = null;\n+\n+\t\t// read all rows\n+\t\twhile (!rowOrcInputFormat.reachedEnd()) {\n+\n+\t\t\tcnt++;\n+\t\t\trow = rowOrcInputFormat.nextRecord(row);\n+\t\t\tassertEquals(10, row.getArity());\n+\n+\t\t\t// check first int field (always 42)\n+\t\t\tassertNotNull(row.getField(0));\n+\t\t\tassertTrue(row.getField(0) instanceof Integer);\n+\t\t\tassertEquals(42, ((Integer) row.getField(0)).intValue());\n+\n+\t\t\t// check second int field (always null)\n+\t\t\tassertNull(row.getField(1));\n+\n+\t\t\t// check first int field (always 99)\n+\t\t\tassertNotNull(row.getField(2));\n+\t\t\tassertTrue(row.getField(2) instanceof Integer);\n+\t\t\tassertEquals(99, ((Integer) row.getField(2)).intValue());\n+\n+\t\t\t// check first row field (always (23, null))\n+\t\t\tassertNotNull(row.getField(3));\n+\t\t\tassertTrue(row.getField(3) instanceof Row);\n+\t\t\tRow nestedRow = (Row) row.getField(3);\n+\t\t\t// check first field of nested row\n+\t\t\tassertNotNull(nestedRow.getField(0));\n+\t\t\tassertTrue(nestedRow.getField(0) instanceof Integer);\n+\t\t\tassertEquals(23, ((Integer) nestedRow.getField(0)).intValue());\n+\t\t\t// check second field of nested row\n+\t\t\tassertNull(nestedRow.getField(1));\n+\t\t\t// validate reference\n+\t\t\tif (firstRow1 == null) {\n+\t\t\t\tfirstRow1 = nestedRow;\n+\t\t\t} else {\n+\t\t\t\t// repeated rows must be different instances\n+\t\t\t\tassertTrue(firstRow1 != nestedRow);\n+\t\t\t}\n+\n+\t\t\t// check second row field (always null)\n+\t\t\tassertNull(row.getField(4));\n+\n+\t\t\t// check first list field (always [1, 2, 3])\n+\t\t\tassertNotNull(row.getField(5));\n+\t\t\tassertTrue(row.getField(5) instanceof Integer[]);\n+\t\t\tInteger[] list1 = ((Integer[]) row.getField(5));\n+\t\t\tassertEquals(1, list1[0].intValue());\n+\t\t\tassertEquals(2, list1[1].intValue());\n+\t\t\tassertEquals(3, list1[2].intValue());\n+\t\t\t// validate reference\n+\t\t\tif (firstList1 == null) {\n+\t\t\t\tfirstList1 = list1;\n+\t\t\t} else {\n+\t\t\t\t// repeated list must be different instances\n+\t\t\t\tassertTrue(firstList1 != list1);\n+\t\t\t}\n+\n+\t\t\t// check second list field (always [7, 7, 7])\n+\t\t\tassertNotNull(row.getField(6));\n+\t\t\tassertTrue(row.getField(6) instanceof Integer[]);\n+\t\t\tInteger[] list2 = ((Integer[]) row.getField(6));\n+\t\t\tassertEquals(7, list2[0].intValue());\n+\t\t\tassertEquals(7, list2[1].intValue());\n+\t\t\tassertEquals(7, list2[2].intValue());\n+\n+\t\t\t// check third list field (always null)\n+\t\t\tassertNull(row.getField(7));\n+\n+\t\t\t// check first map field (always {2->\"Hello\", 4->\"Hello})\n+\t\t\tassertNotNull(row.getField(8));\n+\t\t\tassertTrue(row.getField(8) instanceof HashMap);\n+\t\t\tHashMap<Integer, String> map = (HashMap<Integer, String>) row.getField(8);\n+\t\t\tassertEquals(2, map.size());\n+\t\t\tassertEquals(\"Hello\", map.get(2));\n+\t\t\tassertEquals(\"Hello\", map.get(4));\n+\t\t\t// validate reference\n+\t\t\tif (firstMap1 == null) {\n+\t\t\t\tfirstMap1 = map;\n+\t\t\t} else {\n+\t\t\t\t// repeated list must be different instances\n+\t\t\t\tassertTrue(firstMap1 != map);\n+\t\t\t}\n+\n+\t\t\t// check second map field (always null)\n+\t\t\tassertNull(row.getField(9));\n+\t\t}\n+\n+\t\trowOrcInputFormat.close();\n+\t\trowOrcInputFormat.closeInputFormat();\n+\n+\t\tassertEquals(256, cnt);\n+\t}\n+\n+\t@Test\n+\tpublic void testReadWithProjection() throws IOException {\n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_NESTED), TEST_SCHEMA_NESTED, new Configuration());\n \n \t\trowOrcInputFormat.selectFields(7, 0, 10, 8);\n@@ -691,7 +895,7 @@ public void testReadWithProjection() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileInSplits() throws IOException{\n+\tpublic void testReadFileInSplits() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_FLAT), TEST_SCHEMA_FLAT, new Configuration());\n \t\trowOrcInputFormat.selectFields(0, 1);\n@@ -717,7 +921,7 @@ public void testReadFileInSplits() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileWithFilter() throws IOException{\n+\tpublic void testReadFileWithFilter() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(getPath(TEST_FILE_FLAT), TEST_SCHEMA_FLAT, new Configuration());\n \t\trowOrcInputFormat.selectFields(0, 1);\n@@ -751,7 +955,7 @@ public void testReadFileWithFilter() throws IOException{\n \t}\n \n \t@Test\n-\tpublic void testReadFileWithEvolvedSchema() throws IOException{\n+\tpublic void testReadFileWithEvolvedSchema() throws IOException {\n \n \t\trowOrcInputFormat = new OrcRowInputFormat(\n \t\t\tgetPath(TEST_FILE_FLAT),",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/OrcRowInputFormatTest.java",
                "sha": "2eb3231eedabcfb5bf2e949b3390768803a52f7b",
                "status": "modified"
            },
            {
                "additions": 373,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java",
                "changes": 373,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java",
                "patch": "@@ -0,0 +1,373 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.orc.util;\n+\n+import org.apache.flink.orc.OrcRowInputFormatTest;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.OrcFile;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.Writer;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * A generator for ORC test files.\n+ */\n+public class OrcTestFileGenerator {\n+\n+\tpublic static void main(String[] args) throws IOException {\n+\t\twriteCompositeTypesWithNullsFile(args[0]);\n+//\t\twriteCompositeTypesWithRepeatingFile(args[0]);\n+\t}\n+\n+\t/**\n+\t * Writes an ORC file with nested composite types and null values on different levels.\n+\t * Generates {@link OrcRowInputFormatTest#TEST_FILE_COMPOSITES_NULLS}.\n+\t */\n+\tprivate static void writeCompositeTypesWithNullsFile(String path) throws IOException {\n+\n+\t\tPath filePath = new Path(path);\n+\t\tConfiguration conf = new Configuration();\n+\n+\t\tTypeDescription schema =\n+\t\t\tTypeDescription.fromString(\n+\t\t\t\t\"struct<\" +\n+\t\t\t\t\t\"int1:int,\" +\n+\t\t\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"list1:array<array<array<struct<f1:string,f2:string>>>>,\" +\n+\t\t\t\t\t\"list2:array<map<string,int>>\" +\n+\t\t\t\t\">\");\n+\n+\t\tWriter writer =\n+\t\t\tOrcFile.createWriter(filePath,\n+\t\t\t\tOrcFile.writerOptions(conf).setSchema(schema));\n+\n+\t\tVectorizedRowBatch batch = schema.createRowBatch();\n+\t\tLongColumnVector int1 = (LongColumnVector) batch.cols[0];\n+\n+\t\tStructColumnVector record1 = (StructColumnVector) batch.cols[1];\n+\t\tLongColumnVector record1F1 = (LongColumnVector) record1.fields[0];\n+\t\tBytesColumnVector record1F2 = (BytesColumnVector) record1.fields[1];\n+\n+\t\tListColumnVector list1 = (ListColumnVector) batch.cols[2];\n+\t\tListColumnVector nestedList = (ListColumnVector) list1.child;\n+\t\tListColumnVector nestedList2 = (ListColumnVector) nestedList.child;\n+\t\tStructColumnVector listEntries = (StructColumnVector) nestedList2.child;\n+\t\tBytesColumnVector entryField1 = (BytesColumnVector) listEntries.fields[0];\n+\t\tBytesColumnVector entryField2 = (BytesColumnVector) listEntries.fields[1];\n+\n+\t\tListColumnVector list2 = (ListColumnVector) batch.cols[3];\n+\t\tMapColumnVector map1 = (MapColumnVector) list2.child;\n+\t\tBytesColumnVector keys = (BytesColumnVector) map1.keys;\n+\t\tLongColumnVector vals = (LongColumnVector) map1.values;\n+\n+\t\tfinal int list1Size = 4;\n+\t\tfinal int nestedListSize = 3;\n+\t\tfinal int nestedList2Size = 2;\n+\t\tfinal int list2Size = 3;\n+\t\tfinal int mapSize = 3;\n+\n+\t\tfinal int batchSize = batch.getMaxSize();\n+\n+\t\t// Ensure the vectors have sufficient capacity\n+\t\tnestedList.ensureSize(batchSize * list1Size, false);\n+\t\tnestedList2.ensureSize(batchSize * list1Size * nestedListSize, false);\n+\t\tlistEntries.ensureSize(batchSize * list1Size * nestedListSize * nestedList2Size, false);\n+\t\tmap1.ensureSize(batchSize * list2Size, false);\n+\t\tkeys.ensureSize(batchSize * list2Size * mapSize, false);\n+\t\tvals.ensureSize(batchSize * list2Size * mapSize, false);\n+\n+\t\t// add 2500 rows to file\n+\t\tfor (int r = 0; r < 2500; ++r) {\n+\t\t\tint row = batch.size++;\n+\n+\t\t\t// mark nullable fields\n+\t\t\tlist1.noNulls = false;\n+\t\t\tnestedList.noNulls = false;\n+\t\t\tlistEntries.noNulls = false;\n+\t\t\tentryField1.noNulls = false;\n+\t\t\trecord1.noNulls = false;\n+\t\t\trecord1F2.noNulls = false;\n+\t\t\tlist2.noNulls = false;\n+\t\t\tmap1.noNulls = false;\n+\t\t\tkeys.noNulls = false;\n+\t\t\tvals.noNulls = false;\n+\n+\t\t\t// first field: int1\n+\t\t\tint1.vector[row] = r;\n+\n+\t\t\t// second field: struct\n+\t\t\tif (row % 2 != 0) {\n+\t\t\t\t// in every second row, the struct is null\n+\t\t\t\trecord1F1.vector[row] = row;\n+\t\t\t\tif (row % 5 != 0) {\n+\t\t\t\t\t// in every fifth row, the second field of the struct is null\n+\t\t\t\t\trecord1F2.setVal(row, (\"f2-\" + row).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t} else {\n+\t\t\t\t\trecord1F2.isNull[row] = true;\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\trecord1.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\t// third field: deeply nested list\n+\t\t\tif (row % 3 != 0) {\n+\t\t\t\t// in every third row, the nested list is null\n+\t\t\t\tlist1.offsets[row] = list1.childCount;\n+\t\t\t\tlist1.lengths[row] = list1Size;\n+\t\t\t\tlist1.childCount += list1Size;\n+\n+\t\t\t\tfor (int i = 0; i < list1Size; i++) {\n+\n+\t\t\t\t\tint listOffset = (int) list1.offsets[row] + i;\n+\t\t\t\t\tif (i != 2) {\n+\t\t\t\t\t\t// second nested list is always null\n+\t\t\t\t\t\tnestedList.offsets[listOffset] = nestedList.childCount;\n+\t\t\t\t\t\tnestedList.lengths[listOffset] = nestedListSize;\n+\t\t\t\t\t\tnestedList.childCount += nestedListSize;\n+\n+\t\t\t\t\t\tfor (int j = 0; j < nestedListSize; j++) {\n+\t\t\t\t\t\t\tint nestedOffset = (int) nestedList.offsets[listOffset] + j;\n+\t\t\t\t\t\t\tnestedList2.offsets[nestedOffset] = nestedList2.childCount;\n+\t\t\t\t\t\t\tnestedList2.lengths[nestedOffset] = nestedList2Size;\n+\t\t\t\t\t\t\tnestedList2.childCount += nestedList2Size;\n+\n+\t\t\t\t\t\t\tfor (int k = 0; k < nestedList2Size; k++) {\n+\t\t\t\t\t\t\t\tint nestedOffset2 = (int) nestedList2.offsets[nestedOffset] + k;\n+\t\t\t\t\t\t\t\t// list entries\n+\t\t\t\t\t\t\t\tif (k != 1) {\n+\t\t\t\t\t\t\t\t\t// second struct is always null\n+\t\t\t\t\t\t\t\t\tif (k != 0) {\n+\t\t\t\t\t\t\t\t\t\t// first struct field in first struct is always null\n+\t\t\t\t\t\t\t\t\t\tentryField1.setVal(nestedOffset2, (\"f1-\" + k).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t\t\tentryField1.isNull[nestedOffset2] = true;\n+\t\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t\t\tentryField2.setVal(nestedOffset2, (\"f2-\" + k).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\t\tlistEntries.isNull[nestedOffset2] = true;\n+\t\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tnestedList.isNull[listOffset] = true;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tlist1.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\t// forth field: map in list\n+\t\t\tif (row % 3 != 0) {\n+\t\t\t\t// in every third row, the map list is null\n+\t\t\t\tlist2.offsets[row] = list2.childCount;\n+\t\t\t\tlist2.lengths[row] = list2Size;\n+\t\t\t\tlist2.childCount += list2Size;\n+\n+\t\t\t\tfor (int i = 0; i < list2Size; i++) {\n+\t\t\t\t\tint mapOffset = (int) list2.offsets[row] + i;\n+\n+\t\t\t\t\tif (i != 2) {\n+\t\t\t\t\t\t// second map list entry is always null\n+\t\t\t\t\t\tmap1.offsets[mapOffset] = map1.childCount;\n+\t\t\t\t\t\tmap1.lengths[mapOffset] = mapSize;\n+\t\t\t\t\t\tmap1.childCount += mapSize;\n+\n+\t\t\t\t\t\tfor (int j = 0; j < mapSize; j++) {\n+\t\t\t\t\t\t\tint mapEntryOffset = (int) map1.offsets[mapOffset] + j;\n+\n+\t\t\t\t\t\t\tif (j != 1) {\n+\t\t\t\t\t\t\t\t// key in second map entry is always null\n+\t\t\t\t\t\t\t\tkeys.setVal(mapEntryOffset, (\"key-\" + row + \"-\" + j).getBytes(StandardCharsets.UTF_8));\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tkeys.isNull[mapEntryOffset] = true;\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t\tif (j != 2) {\n+\t\t\t\t\t\t\t\t// value in third map entry is always null\n+\t\t\t\t\t\t\t\tvals.vector[mapEntryOffset] = row + i + j;\n+\t\t\t\t\t\t\t} else {\n+\t\t\t\t\t\t\t\tvals.isNull[mapEntryOffset] = true;\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\tmap1.isNull[mapOffset] = true;\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tlist2.isNull[row] = true;\n+\t\t\t}\n+\n+\t\t\tif (row == batchSize - 1) {\n+\t\t\t\twriter.addRowBatch(batch);\n+\t\t\t\tbatch.reset();\n+\t\t\t}\n+\t\t}\n+\t\tif (batch.size != 0) {\n+\t\t\twriter.addRowBatch(batch);\n+\t\t\tbatch.reset();\n+\t\t}\n+\t\twriter.close();\n+\t}\n+\n+\t/**\n+\t * Writes an ORC file with nested composite types and repeated values.\n+\t * Generates {@link OrcRowInputFormatTest#TEST_FILE_REPEATING}.\n+\t */\n+\tprivate static void writeCompositeTypesWithRepeatingFile(String path) throws IOException {\n+\n+\t\tPath filePath = new Path(path);\n+\t\tConfiguration conf = new Configuration();\n+\n+\t\tTypeDescription schema =\n+\t\t\tTypeDescription.fromString(\n+\t\t\t\t\"struct<\" +\n+\t\t\t\t\t\"int1:int,\" +\n+\t\t\t\t\t\"int2:int,\" +\n+\t\t\t\t\t\"int3:int,\" +\n+\t\t\t\t\t\"record1:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"record2:struct<f1:int,f2:string>,\" +\n+\t\t\t\t\t\"list1:array<int>,\" +\n+\t\t\t\t\t\"list2:array<int>,\" +\n+\t\t\t\t\t\"list3:array<int>,\" +\n+\t\t\t\t\t\"map1:map<int,string>,\" +\n+\t\t\t\t\t\"map2:map<int,string>\" +\n+\t\t\t\t\">\");\n+\n+\t\tWriter writer =\n+\t\t\tOrcFile.createWriter(filePath,\n+\t\t\t\tOrcFile.writerOptions(conf).setSchema(schema));\n+\n+\t\tVectorizedRowBatch batch = schema.createRowBatch();\n+\n+\t\tLongColumnVector int1 = (LongColumnVector) batch.cols[0];\n+\t\tLongColumnVector int2 = (LongColumnVector) batch.cols[1];\n+\t\tLongColumnVector int3 = (LongColumnVector) batch.cols[2];\n+\n+\t\tStructColumnVector record1 = (StructColumnVector) batch.cols[3];\n+\t\tLongColumnVector record1F1 = (LongColumnVector) record1.fields[0];\n+\t\tBytesColumnVector record1F2 = (BytesColumnVector) record1.fields[1];\n+\t\tStructColumnVector record2 = (StructColumnVector) batch.cols[4];\n+\n+\t\tListColumnVector list1 = (ListColumnVector) batch.cols[5];\n+\t\tLongColumnVector list1int = (LongColumnVector) list1.child;\n+\t\tListColumnVector list2 = (ListColumnVector) batch.cols[6];\n+\t\tLongColumnVector list2int = (LongColumnVector) list2.child;\n+\t\tListColumnVector list3 = (ListColumnVector) batch.cols[7];\n+\n+\t\tMapColumnVector map1 = (MapColumnVector) batch.cols[8];\n+\t\tLongColumnVector map1keys = (LongColumnVector) map1.keys;\n+\t\tBytesColumnVector map1vals = (BytesColumnVector) map1.values;\n+\t\tMapColumnVector map2 = (MapColumnVector) batch.cols[9];\n+\n+\t\tfinal int listSize = 3;\n+\t\tfinal int mapSize = 2;\n+\n+\t\tfinal int batchSize = batch.getMaxSize();\n+\n+\t\t// Ensure the vectors have sufficient capacity\n+\t\tlist1int.ensureSize(batchSize * listSize, false);\n+\t\tlist2int.ensureSize(batchSize * listSize, false);\n+\t\tmap1keys.ensureSize(batchSize * mapSize, false);\n+\t\tmap1vals.ensureSize(batchSize * mapSize, false);\n+\n+\t\t// int1: all values are 42\n+\t\tint1.noNulls = true;\n+\t\tint1.setRepeating(true);\n+\t\tint1.vector[0] = 42;\n+\n+\t\t// int2: all values are null\n+\t\tint2.noNulls = false;\n+\t\tint2.setRepeating(true);\n+\t\tint2.isNull[0] = true;\n+\n+\t\t// int3: all values are 99\n+\t\tint3.noNulls = false;\n+\t\tint3.setRepeating(true);\n+\t\tint3.isNull[0] = false;\n+\t\tint3.vector[0] = 99;\n+\n+\t\t// record1: all records are [23, \"Hello\"]\n+\t\trecord1.noNulls = true;\n+\t\trecord1.setRepeating(true);\n+\t\tfor (int i = 0; i < batchSize; i++) {\n+\t\t\trecord1F1.vector[i] = i + 23;\n+\t\t}\n+\t\trecord1F2.noNulls = false;\n+\t\trecord1F2.isNull[0] = true;\n+\n+\t\t// record2: all records are null\n+\t\trecord2.noNulls = false;\n+\t\trecord2.setRepeating(true);\n+\t\trecord2.isNull[0] = true;\n+\n+\t\t// list1: all lists are [1, 2, 3]\n+\t\tlist1.noNulls = true;\n+\t\tlist1.setRepeating(true);\n+\t\tlist1.lengths[0] = listSize;\n+\t\tlist1.offsets[0] = 1;\n+\t\tfor (int i = 0; i < batchSize * listSize; i++) {\n+\t\t\tlist1int.vector[i] = i;\n+\t\t}\n+\n+\t\t// list2: all lists are [7, 7, 7]\n+\t\tlist2.noNulls = true;\n+\t\tlist2.setRepeating(true);\n+\t\tlist2.lengths[0] = listSize;\n+\t\tlist2.offsets[0] = 0;\n+\t\tlist2int.setRepeating(true);\n+\t\tlist2int.vector[0] = 7;\n+\n+\t\t// list3: all lists are null\n+\t\tlist3.noNulls = false;\n+\t\tlist3.setRepeating(true);\n+\t\tlist3.isNull[0] = true;\n+\n+\t\t// map1: all maps are [2 -> \"HELLO\", 4 -> \"HELLO\"]\n+\t\tmap1.noNulls = true;\n+\t\tmap1.setRepeating(true);\n+\t\tmap1.lengths[0] = mapSize;\n+\t\tmap1.offsets[0] = 1;\n+\t\tfor (int i = 0; i < batchSize * mapSize; i++) {\n+\t\t\tmap1keys.vector[i] = i * 2;\n+\t\t}\n+\t\tmap1vals.setRepeating(true);\n+\t\tmap1vals.setVal(0, \"Hello\".getBytes(StandardCharsets.UTF_8));\n+\n+\t\t// map2: all maps are null\n+\t\tmap2.noNulls = false;\n+\t\tmap2.setRepeating(true);\n+\t\tmap2.isNull[0] = true;\n+\n+\t\tbatch.size = 256;\n+\n+\t\twriter.addRowBatch(batch);\n+\t\tbatch.reset();\n+\t\twriter.close();\n+\t}\n+\n+}",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/java/org/apache/flink/orc/util/OrcTestFileGenerator.java",
                "sha": "9d3be63b294c0c22e35f44abe450e22abc6062d3",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-composites-with-nulls.orc",
                "sha": "eed1c554cc019a6181e1e65e0f1a049e22c00dd3",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/flink/blob/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc?ref=bcead3be32c624008730555d828fd8e9447fbeff",
                "deletions": 0,
                "filename": "flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc",
                "raw_url": "https://github.com/apache/flink/raw/bcead3be32c624008730555d828fd8e9447fbeff/flink-connectors/flink-orc/src/test/resources/test-data-repeating.orc",
                "sha": "ff2c917c6d47587813dee98a33192430550ef979",
                "status": "added"
            }
        ],
        "message": "[FLINK-8230] [orc] Fix NPEs when reading nested columns.\n\n- fixes NPEs for null-valued structs, lists, and maps\n- fixes NPEs for repeating structs, lists, and maps\n- adds test for deeply nested data with nulls\n- adds test for columns with repeating values\n\nThis closes #5373.",
        "parent": "https://github.com/apache/flink/commit/3cfc5ae9fb50ec45b72b343cc7e8f45901c57beb",
        "patched_files": [
            "OrcTestFileGenerator.java",
            "OrcUtils.java",
            "OrcTableSource.java",
            "OrcBatchReader.java",
            "OrcRowInputFormat.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "test-data-repeating.java",
            "OrcRowInputFormatTest.java",
            "test-data-composites-with-nulls.java",
            "OrcTableSourceTest.java",
            "OrcBatchReaderTest.java"
        ]
    },
    "flink_c5dd1f1": {
        "bug_id": "flink_c5dd1f1",
        "commit": "https://github.com/apache/flink/commit/c5dd1f11f71471ba42e3a075651649e2ca258551",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/c5dd1f11f71471ba42e3a075651649e2ca258551/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java?ref=c5dd1f11f71471ba42e3a075651649e2ca258551",
                "deletions": 2,
                "filename": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "patch": "@@ -180,8 +180,10 @@ public String toString()\n \t\t\t\tbuf.append(\",\");\n \t\t\t}\n \t\t\tbuf.append(this.indexes.get(i));\n-\t\t\tbuf.append(\":\");\n-\t\t\tbuf.append(this.types.get(i).getName());\n+\t\t\tif (this.types.get(i) != null) {\n+\t\t\t\tbuf.append(\":\");\n+\t\t\t\tbuf.append(this.types.get(i).getName());\n+\t\t\t}\n \t\t\tbuf.append(\":\");\n \t\t\tbuf.append(this.orders.get(i).name());\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/c5dd1f11f71471ba42e3a075651649e2ca258551/pact/pact-common/src/main/java/eu/stratosphere/pact/common/contract/Ordering.java",
                "sha": "ca0e4f8e38e1825fea35a9a382e5642ab5ce8615",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in Ordering toString method: Keytypes may be null",
        "parent": "https://github.com/apache/flink/commit/692318593be6e5f7aa5fc62bf624ef34cb5a357a",
        "patched_files": [
            "Ordering.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "OrderingTest.java"
        ]
    },
    "flink_cc41285": {
        "bug_id": "flink_cc41285",
        "commit": "https://github.com/apache/flink/commit/cc412859001a9437e5176596dc284f05bd740a40",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java",
                "patch": "@@ -99,9 +99,11 @@ public boolean isAssigned() {\n \t *\n \t * @return True if the location has been assigned and the shared slot is alive,\n \t *         false otherwise.\n+\t * @deprecated Should only be called by legacy code (if using {@link Scheduler})\n \t */\n+\t@Deprecated\n \tpublic boolean isAssignedAndAlive() {\n-\t\treturn lockedLocation != null && sharedSlot.isAlive();\n+\t\treturn lockedLocation != null && sharedSlot != null && sharedSlot.isAlive();\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/scheduler/CoLocationConstraint.java",
                "sha": "23c9c214ccc31ba6a8437c1948a16d37004edd8e",
                "status": "modified"
            },
            {
                "additions": 143,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java",
                "changes": 143,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java",
                "patch": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.executiongraph;\n+\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.runtime.concurrent.ScheduledExecutor;\n+import org.apache.flink.runtime.executiongraph.restart.RestartCallback;\n+import org.apache.flink.runtime.executiongraph.restart.RestartStrategy;\n+import org.apache.flink.runtime.jobgraph.JobStatus;\n+import org.apache.flink.runtime.jobgraph.JobVertex;\n+import org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint;\n+import org.apache.flink.runtime.jobmanager.scheduler.SchedulerTestBase;\n+import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n+import org.apache.flink.util.FlinkException;\n+\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.runtime.jobgraph.JobStatus.FINISHED;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+\n+/**\n+ * Additional {@link ExecutionGraph} restart tests {@link ExecutionGraphRestartTest} which\n+ * require the usage of a {@link SlotProvider}.\n+ */\n+@RunWith(Parameterized.class)\n+public class ExecutionGraphCoLocationRestartTest extends SchedulerTestBase {\n+\n+\tprivate static final int NUM_TASKS = 31;\n+\n+\tpublic ExecutionGraphCoLocationRestartTest(SchedulerType schedulerType) {\n+\t\tsuper(schedulerType);\n+\t}\n+\n+\t@Test\n+\tpublic void testConstraintsAfterRestart() throws Exception {\n+\t\tfinal long timeout = 5000L;\n+\n+\t\t//setting up\n+\t\ttestingSlotProvider.addTaskManager(NUM_TASKS);\n+\n+\t\tJobVertex groupVertex = ExecutionGraphTestUtils.createNoOpVertex(NUM_TASKS);\n+\t\tJobVertex groupVertex2 = ExecutionGraphTestUtils.createNoOpVertex(NUM_TASKS);\n+\n+\t\tSlotSharingGroup sharingGroup = new SlotSharingGroup();\n+\t\tgroupVertex.setSlotSharingGroup(sharingGroup);\n+\t\tgroupVertex2.setSlotSharingGroup(sharingGroup);\n+\t\tgroupVertex.setStrictlyCoLocatedWith(groupVertex2);\n+\n+\t\t//initiate and schedule job\n+\t\tfinal ExecutionGraph eg = ExecutionGraphTestUtils.createSimpleTestGraph(\n+\t\t\tnew JobID(),\n+\t\t\ttestingSlotProvider,\n+\t\t\tnew OneTimeDirectRestartStrategy(),\n+\t\t\tgroupVertex,\n+\t\t\tgroupVertex2);\n+\n+\t\tif (schedulerType == SchedulerType.SLOT_POOL) {\n+\t\t\t// enable the queued scheduling for the slot pool\n+\t\t\teg.setQueuedSchedulingAllowed(true);\n+\t\t}\n+\n+\t\tassertEquals(JobStatus.CREATED, eg.getState());\n+\n+\t\teg.scheduleForExecution();\n+\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\ttimeout);\n+\n+\t\tassertEquals(JobStatus.RUNNING, eg.getState());\n+\n+\t\t//sanity checks\n+\t\tvalidateConstraints(eg);\n+\n+\t\tExecutionGraphTestUtils.failExecutionGraph(eg, new FlinkException(\"Test exception\"));\n+\n+\t\t// wait until we have restarted\n+\t\tExecutionGraphTestUtils.waitUntilJobStatus(eg, JobStatus.RUNNING, timeout);\n+\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\ttimeout);\n+\n+\t\t//checking execution vertex properties\n+\t\tvalidateConstraints(eg);\n+\n+\t\tExecutionGraphTestUtils.finishAllVertices(eg);\n+\n+\t\tassertThat(eg.getState(), is(FINISHED));\n+\t}\n+\n+\tprivate void validateConstraints(ExecutionGraph eg) {\n+\n+\t\tExecutionJobVertex[] tasks = eg.getAllVertices().values().toArray(new ExecutionJobVertex[2]);\n+\n+\t\tfor(int i = 0; i < NUM_TASKS; i++){\n+\t\t\tCoLocationConstraint constr1 = tasks[0].getTaskVertices()[i].getLocationConstraint();\n+\t\t\tCoLocationConstraint constr2 = tasks[1].getTaskVertices()[i].getLocationConstraint();\n+\t\t\tassertThat(constr1.isAssigned(), is(true));\n+\t\t\tassertThat(constr1.getLocation(), equalTo(constr2.getLocation()));\n+\t\t}\n+\n+\t}\n+\n+\tprivate static final class OneTimeDirectRestartStrategy implements RestartStrategy {\n+\t\tprivate boolean hasRestarted = false;\n+\n+\t\t@Override\n+\t\tpublic boolean canRestart() {\n+\t\t\treturn !hasRestarted;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void restart(RestartCallback restarter, ScheduledExecutor executor) {\n+\t\t\thasRestarted = true;\n+\t\t\trestarter.triggerFullRecovery();\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphCoLocationRestartTest.java",
                "sha": "aba6bfad5ec22a10e07b43d98ca532939c280e9d",
                "status": "added"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 101,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java",
                "patch": "@@ -41,19 +41,18 @@\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n-import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionType;\n import org.apache.flink.runtime.jobgraph.DistributionPattern;\n import org.apache.flink.runtime.jobgraph.JobGraph;\n import org.apache.flink.runtime.jobgraph.JobStatus;\n import org.apache.flink.runtime.jobgraph.JobVertex;\n import org.apache.flink.runtime.jobgraph.ScheduleMode;\n-import org.apache.flink.runtime.jobmanager.scheduler.CoLocationConstraint;\n import org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException;\n import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;\n import org.apache.flink.runtime.jobmanager.scheduler.SlotSharingGroup;\n import org.apache.flink.runtime.jobmanager.slots.ActorTaskManagerGateway;\n import org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.taskmanager.TaskManagerLocation;\n import org.apache.flink.runtime.testingUtils.TestingUtils;\n import org.apache.flink.runtime.testtasks.NoOpInvokable;\n@@ -73,6 +72,7 @@\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Consumer;\n \n@@ -125,62 +125,6 @@ public void testNoManualRestart() throws Exception {\n \t\tassertEquals(JobStatus.FAILED, eg.getState());\n \t}\n \n-\t@Test\n-\tpublic void testConstraintsAfterRestart() throws Exception {\n-\t\t\n-\t\t//setting up\n-\t\tInstance instance = ExecutionGraphTestUtils.getInstance(\n-\t\t\tnew ActorTaskManagerGateway(\n-\t\t\t\tnew SimpleActorGateway(TestingUtils.directExecutionContext())),\n-\t\t\tNUM_TASKS);\n-\t\t\n-\t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n-\t\tscheduler.newInstanceAvailable(instance);\n-\n-\t\tJobVertex groupVertex = newJobVertex(\"Task1\", NUM_TASKS, NoOpInvokable.class);\n-\t\tJobVertex groupVertex2 = newJobVertex(\"Task2\", NUM_TASKS, NoOpInvokable.class);\n-\n-\t\tSlotSharingGroup sharingGroup = new SlotSharingGroup();\n-\t\tgroupVertex.setSlotSharingGroup(sharingGroup);\n-\t\tgroupVertex2.setSlotSharingGroup(sharingGroup);\n-\t\tgroupVertex.setStrictlyCoLocatedWith(groupVertex2);\n-\t\t\n-\t\t//initiate and schedule job\n-\t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", groupVertex, groupVertex2);\n-\t\tExecutionGraph eg = newExecutionGraph(new FixedDelayRestartStrategy(1, 0L), scheduler);\n-\t\teg.attachJobGraph(jobGraph.getVerticesSortedTopologicallyFromSources());\n-\n-\t\tassertEquals(JobStatus.CREATED, eg.getState());\n-\t\t\n-\t\teg.scheduleForExecution();\n-\t\tassertEquals(JobStatus.RUNNING, eg.getState());\n-\t\t\n-\t\t//sanity checks\n-\t\tvalidateConstraints(eg);\n-\n-\t\t//restart automatically\n-\t\trestartAfterFailure(eg, new FiniteDuration(2, TimeUnit.MINUTES), false);\n-\t\t\n-\t\t//checking execution vertex properties\n-\t\tvalidateConstraints(eg);\n-\n-\t\thaltExecution(eg);\n-\t}\n-\n-\tprivate void validateConstraints(ExecutionGraph eg) {\n-\t\t\n-\t\tExecutionJobVertex[] tasks = eg.getAllVertices().values().toArray(new ExecutionJobVertex[2]);\n-\t\t\n-\t\tfor(int i=0; i<NUM_TASKS; i++){\n-\t\t\tCoLocationConstraint constr1 = tasks[0].getTaskVertices()[i].getLocationConstraint();\n-\t\t\tCoLocationConstraint constr2 = tasks[1].getTaskVertices()[i].getLocationConstraint();\n-\t\t\tassertNotNull(constr1.getSharedSlot());\n-\t\t\tassertTrue(constr1.isAssigned());\n-\t\t\tassertEquals(constr1, constr2);\n-\t\t}\n-\t\t\n-\t}\n-\n \t@Test\n \tpublic void testRestartAutomatically() throws Exception {\n \t\tRestartStrategy restartStrategy = new FixedDelayRestartStrategy(1, 1000);\n@@ -383,8 +327,8 @@ public void testFailingExecutionAfterRestart() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex sender = newJobVertex(\"Task1\", 1, NoOpInvokable.class);\n-\t\tJobVertex receiver = newJobVertex(\"Task2\", 1, NoOpInvokable.class);\n+\t\tJobVertex sender = ExecutionGraphTestUtils.createJobVertex(\"Task1\", 1, NoOpInvokable.class);\n+\t\tJobVertex receiver = ExecutionGraphTestUtils.createJobVertex(\"Task2\", 1, NoOpInvokable.class);\n \t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", sender, receiver);\n \t\tExecutionGraph eg = newExecutionGraph(new FixedDelayRestartStrategy(1, 1000), scheduler);\n \t\teg.attachJobGraph(jobGraph.getVerticesSortedTopologicallyFromSources());\n@@ -447,7 +391,7 @@ public void testFailExecutionAfterCancel() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex vertex = newJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n+\t\tJobVertex vertex = ExecutionGraphTestUtils.createJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n \n \t\tExecutionConfig executionConfig = new ExecutionConfig();\n \t\texecutionConfig.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n@@ -493,7 +437,7 @@ public void testFailExecutionGraphAfterCancel() throws Exception {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex vertex = newJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n+\t\tJobVertex vertex = ExecutionGraphTestUtils.createJobVertex(\"Test Vertex\", 1, NoOpInvokable.class);\n \n \t\tExecutionConfig executionConfig = new ExecutionConfig();\n \t\texecutionConfig.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n@@ -918,7 +862,7 @@ public void run() {\n \t\tScheduler scheduler = new Scheduler(TestingUtils.defaultExecutionContext());\n \t\tscheduler.newInstanceAvailable(instance);\n \n-\t\tJobVertex sender = newJobVertex(\"Task\", NUM_TASKS, NoOpInvokable.class);\n+\t\tJobVertex sender = ExecutionGraphTestUtils.createJobVertex(\"Task\", NUM_TASKS, NoOpInvokable.class);\n \n \t\tJobGraph jobGraph = new JobGraph(\"Pointwise job\", sender);\n \n@@ -935,13 +879,6 @@ public void run() {\n \t\treturn new Tuple2<>(eg, instance);\n \t}\n \n-\tprivate static JobVertex newJobVertex(String task1, int numTasks, Class<NoOpInvokable> invokable) {\n-\t\tJobVertex groupVertex = new JobVertex(task1);\n-\t\tgroupVertex.setInvokableClass(invokable);\n-\t\tgroupVertex.setParallelism(numTasks);\n-\t\treturn groupVertex;\n-\t}\n-\n \tprivate static ExecutionGraph newExecutionGraph(RestartStrategy restartStrategy, Scheduler scheduler) throws IOException {\n \t\treturn new ExecutionGraph(\n \t\t\tTestingUtils.defaultExecutor(),\n@@ -955,8 +892,11 @@ private static ExecutionGraph newExecutionGraph(RestartStrategy restartStrategy,\n \t\t\tscheduler);\n \t}\n \n-\tprivate static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeout, boolean haltAfterRestart) throws InterruptedException {\n-\t\tmakeAFailureAndWait(eg, timeout);\n+\tprivate static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeout, boolean haltAfterRestart) throws InterruptedException, TimeoutException {\n+\t\tExecutionGraphTestUtils.failExecutionGraph(eg, new Exception(\"Test Exception\"));\n+\n+\t\t// Wait for async restart\n+\t\twaitForAsyncRestart(eg, timeout);\n \n \t\tassertEquals(JobStatus.RUNNING, eg.getState());\n \n@@ -973,32 +913,11 @@ private static void restartAfterFailure(ExecutionGraph eg, FiniteDuration timeou\n \t\t}\n \t}\n \n-\tprivate static void waitForAllResourcesToBeAssignedAfterAsyncRestart(ExecutionGraph eg, Deadline deadline) throws InterruptedException {\n-\t\tboolean success = false;\n-\n-\t\twhile (deadline.hasTimeLeft() && !success) {\n-\t\t\tsuccess = true;\n-\n-\t\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\t\tif (vertex.getCurrentExecutionAttempt().getAssignedResource() == null) {\n-\t\t\t\t\tsuccess = false;\n-\t\t\t\t\tThread.sleep(100);\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tprivate static void makeAFailureAndWait(ExecutionGraph eg, FiniteDuration timeout) throws InterruptedException {\n-\t\teg.getAllExecutionVertices().iterator().next().fail(new Exception(\"Test Exception\"));\n-\t\tassertEquals(JobStatus.FAILING, eg.getState());\n-\n-\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\tvertex.getCurrentExecutionAttempt().cancelingComplete();\n-\t\t}\n-\n-\t\t// Wait for async restart\n-\t\twaitForAsyncRestart(eg, timeout);\n+\tprivate static void waitForAllResourcesToBeAssignedAfterAsyncRestart(ExecutionGraph eg, Deadline deadline) throws TimeoutException {\n+\t\tExecutionGraphTestUtils.waitForAllExecutionsPredicate(\n+\t\t\teg,\n+\t\t\tExecutionGraphTestUtils.hasResourceAssigned,\n+\t\t\tdeadline.timeLeft().toMillis());\n \t}\n \n \tprivate static void waitForAsyncRestart(ExecutionGraph eg, FiniteDuration timeout) throws InterruptedException {\n@@ -1009,9 +928,7 @@ private static void waitForAsyncRestart(ExecutionGraph eg, FiniteDuration timeou\n \t}\n \n \tprivate static void haltExecution(ExecutionGraph eg) {\n-\t\tfor (ExecutionVertex vertex : eg.getAllExecutionVertices()) {\n-\t\t\tvertex.getCurrentExecutionAttempt().markFinished();\n-\t\t}\n+\t\tfinishAllVertices(eg);\n \n \t\tassertEquals(JobStatus.FINISHED, eg.getState());\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphRestartTest.java",
                "sha": "9b98de7814377a76c3dfbab96da76b3ee3d4b2c7",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 3,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.flink.api.common.ExecutionConfig;\n import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.time.Deadline;\n import org.apache.flink.api.common.time.Time;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.metrics.groups.UnregisteredMetricsGroup;\n@@ -41,16 +42,16 @@\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n import org.apache.flink.runtime.instance.SimpleSlot;\n-import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n+import org.apache.flink.runtime.instance.SimpleSlotContext;\n import org.apache.flink.runtime.jobgraph.JobGraph;\n import org.apache.flink.runtime.jobgraph.JobStatus;\n import org.apache.flink.runtime.jobgraph.JobVertex;\n import org.apache.flink.runtime.jobgraph.JobVertexID;\n import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;\n import org.apache.flink.runtime.jobmanager.scheduler.Scheduler;\n-import org.apache.flink.runtime.instance.SimpleSlotContext;\n-import org.apache.flink.runtime.jobmaster.SlotOwner;\n import org.apache.flink.runtime.jobmanager.slots.TaskManagerGateway;\n+import org.apache.flink.runtime.jobmaster.SlotOwner;\n+import org.apache.flink.runtime.jobmaster.slotpool.SlotProvider;\n import org.apache.flink.runtime.messages.Acknowledge;\n import org.apache.flink.runtime.messages.TaskMessages.CancelTask;\n import org.apache.flink.runtime.messages.TaskMessages.FailIntermediateResultPartitions;\n@@ -65,11 +66,14 @@\n import org.slf4j.LoggerFactory;\n \n import javax.annotation.Nullable;\n+\n import java.lang.reflect.Field;\n import java.net.InetAddress;\n+import java.time.Duration;\n import java.util.List;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n \n import scala.concurrent.ExecutionContext;\n import scala.concurrent.ExecutionContext$;\n@@ -144,6 +148,53 @@ public static void waitUntilExecutionState(Execution execution, ExecutionState s\n \t\t}\n \t}\n \n+\t/**\n+\t * Waits until all executions fulfill the given predicate.\n+\t *\n+\t * @param executionGraph for which to check the executions\n+\t * @param executionPredicate predicate which is to be fulfilled\n+\t * @param maxWaitMillis timeout for the wait operation\n+\t * @throws TimeoutException if the executions did not reach the target state in time\n+\t */\n+\tpublic static void waitForAllExecutionsPredicate(\n+\t\t\tExecutionGraph executionGraph,\n+\t\t\tPredicate<Execution> executionPredicate,\n+\t\t\tlong maxWaitMillis) throws TimeoutException {\n+\t\tfinal Iterable<ExecutionVertex> allExecutionVertices = executionGraph.getAllExecutionVertices();\n+\n+\t\tfinal Deadline deadline = Deadline.fromNow(Duration.ofMillis(maxWaitMillis));\n+\t\tboolean predicateResult;\n+\n+\t\tdo {\n+\t\t\tpredicateResult = true;\n+\t\t\tfor (ExecutionVertex executionVertex : allExecutionVertices) {\n+\t\t\t\tfinal Execution currentExecution = executionVertex.getCurrentExecutionAttempt();\n+\n+\t\t\t\tif (currentExecution == null || !executionPredicate.test(currentExecution)) {\n+\t\t\t\t\tpredicateResult = false;\n+\t\t\t\t\tbreak;\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif (!predicateResult) {\n+\t\t\t\ttry {\n+\t\t\t\t\tThread.sleep(2L);\n+\t\t\t\t} catch (InterruptedException ignored) {\n+\t\t\t\t\tThread.currentThread().interrupt();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} while (!predicateResult && deadline.hasTimeLeft());\n+\n+\t\tif (!predicateResult) {\n+\t\t\tthrow new TimeoutException(\"Not all executions fulfilled the predicate in time.\");\n+\t\t}\n+\t}\n+\n+\t/**\n+\t * Predicate which is true if the given {@link Execution} has a resource assigned.\n+\t */\n+\tpublic static final Predicate<Execution> hasResourceAssigned = (Execution execution) -> execution.getAssignedResource() != null;\n+\n \tpublic static void waitUntilFailoverRegionState(FailoverRegion region, JobStatus status, long maxWaitMillis)\n \t\t\tthrows TimeoutException {\n \n@@ -165,6 +216,15 @@ public static void waitUntilFailoverRegionState(FailoverRegion region, JobStatus\n \t\t}\n \t}\n \n+\tpublic static void failExecutionGraph(ExecutionGraph executionGraph, Exception cause) {\n+\t\texecutionGraph.getAllExecutionVertices().iterator().next().fail(cause);\n+\t\tassertEquals(JobStatus.FAILING, executionGraph.getState());\n+\n+\t\tfor (ExecutionVertex vertex : executionGraph.getAllExecutionVertices()) {\n+\t\t\tvertex.getCurrentExecutionAttempt().cancelingComplete();\n+\t\t}\n+\t}\n+\n \t/**\n \t * Takes all vertices in the given ExecutionGraph and switches their current\n \t * execution to RUNNING.\n@@ -382,6 +442,13 @@ public static Instance getInstance(final TaskManagerGateway gateway, final int n\n \t\treturn new Instance(gateway, connection, new InstanceID(), hardwareDescription, numberOfSlots);\n \t}\n \n+\tpublic static JobVertex createJobVertex(String task1, int numTasks, Class<NoOpInvokable> invokable) {\n+\t\tJobVertex groupVertex = new JobVertex(task1);\n+\t\tgroupVertex.setInvokableClass(invokable);\n+\t\tgroupVertex.setParallelism(numTasks);\n+\t\treturn groupVertex;\n+\t}\n+\n \t@SuppressWarnings(\"serial\")\n \tpublic static class SimpleActorGateway extends BaseTestingActorGateway {\n ",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/ExecutionGraphTestUtils.java",
                "sha": "9cfe90ec070763028624cfc4c5a9b184db00304c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java",
                "patch": "@@ -72,11 +72,11 @@\n \n \tprotected TestingSlotProvider testingSlotProvider;\n \n-\tprivate SchedulerType schedulerType;\n+\tprotected SchedulerType schedulerType;\n \n \tprivate RpcService rpcService;\n \n-\tenum SchedulerType {\n+\tpublic enum SchedulerType {\n \t\tSCHEDULER,\n \t\tSLOT_POOL\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestBase.java",
                "sha": "3d5441293bd5ae25470df4724a762b46be51c1cd",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java?ref=cc412859001a9437e5176596dc284f05bd740a40",
                "deletions": 3,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java",
                "patch": "@@ -23,12 +23,11 @@\n import org.apache.flink.runtime.executiongraph.Execution;\n import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;\n import org.apache.flink.runtime.executiongraph.ExecutionVertex;\n-import org.apache.flink.runtime.instance.DummyActorGateway;\n+import org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway;\n import org.apache.flink.runtime.instance.HardwareDescription;\n import org.apache.flink.runtime.instance.Instance;\n import org.apache.flink.runtime.instance.InstanceID;\n import org.apache.flink.runtime.jobgraph.JobVertexID;\n-import org.apache.flink.runtime.jobmanager.slots.ActorTaskManagerGateway;\n import org.apache.flink.runtime.taskmanager.TaskManagerLocation;\n \n import java.net.InetAddress;\n@@ -75,7 +74,7 @@ public static Instance getRandomInstance(int numSlots) {\n \t\tHardwareDescription resources = new HardwareDescription(4, 4*GB, 3*GB, 2*GB);\n \t\t\n \t\treturn new Instance(\n-\t\t\tnew ActorTaskManagerGateway(DummyActorGateway.INSTANCE),\n+\t\t\tnew SimpleAckingTaskManagerGateway(),\n \t\t\tci,\n \t\t\tnew InstanceID(),\n \t\t\tresources,",
                "raw_url": "https://github.com/apache/flink/raw/cc412859001a9437e5176596dc284f05bd740a40/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/scheduler/SchedulerTestUtils.java",
                "sha": "3c074d1b6e0f4cef175d06c0baa8b9a457ffef3b",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9458] Ignore SharedSlot in CoLocationConstraint when not using legacy mode\n\nThe SharedSlot in CoLocationConstraint is only set when using the legacy mode. Thus,\nCoLocationConstraint#isAssignedAlive should only check the SharedSlot if it was\npreviously set. This fixes the NPE when restarting a job with a co-location constraint\nwhen using the new mode.\n\nThis closes #6119.",
        "parent": "https://github.com/apache/flink/commit/fdabce0339a5ba89787f8719e463cd01f35b4601",
        "patched_files": [
            "SchedulerTestUtils.java",
            "SchedulerTestBase.java",
            "CoLocationConstraint.java",
            "ExecutionGraphTestUtils.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "ExecutionGraphCoLocationRestartTest.java",
            "CoLocationConstraintTest.java",
            "ExecutionGraphRestartTest.java"
        ]
    },
    "flink_cc5c855": {
        "bug_id": "flink_cc5c855",
        "commit": "https://github.com/apache/flink/commit/cc5c8554740f8a5e554d98de236ca9e17bf2c67a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala?ref=cc5c8554740f8a5e554d98de236ca9e17bf2c67a",
                "deletions": 6,
                "filename": "flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala",
                "patch": "@@ -115,12 +115,8 @@ class CRowSerializer(val rowSerializer: TypeSerializer[Row]) extends TypeSeriali\n \n object CRowSerializer {\n \n-  class CRowSerializerConfigSnapshot(\n-      private val rowSerializer: TypeSerializer[Row])\n-    extends CompositeTypeSerializerConfigSnapshot(rowSerializer) {\n-\n-    /** This empty nullary constructor is required for deserializing the configuration. */\n-    def this() = this(null)\n+  class CRowSerializerConfigSnapshot(rowSerializers: TypeSerializer[Row]*)\n+    extends CompositeTypeSerializerConfigSnapshot(rowSerializers: _*) {\n \n     override def getVersion: Int = CRowSerializerConfigSnapshot.VERSION\n   }",
                "raw_url": "https://github.com/apache/flink/raw/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/types/CRowSerializer.scala",
                "sha": "9418095d5f7ca581ce63d5c448b9ad194eee778b",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/flink/blob/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala?ref=cc5c8554740f8a5e554d98de236ca9e17bf2c67a",
                "deletions": 0,
                "filename": "flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala",
                "patch": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.table.runtime.types\n+\n+import org.apache.flink.util.TestLogger\n+import org.junit.Test\n+\n+class CRowSerializerTest extends TestLogger {\n+\n+  /**\n+    * This empty constructor is required for deserializing the configuration.\n+    */\n+  @Test\n+  def testDefaultConstructor(): Unit = {\n+    new CRowSerializer.CRowSerializerConfigSnapshot()\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/flink/raw/cc5c8554740f8a5e554d98de236ca9e17bf2c67a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/types/CRowSerializerTest.scala",
                "sha": "7483b04d9cac10fd8df849d07312644a73ad8457",
                "status": "added"
            }
        ],
        "message": "[FLINK-9694][table] Fix NPE in CRowSerializerConfigSnapshot constructor\n\nThis closes #6392.",
        "parent": "https://github.com/apache/flink/commit/378cbb7c2e580ba73f215234e7dff542c3e2bc97",
        "patched_files": [
            "CRowSerializer.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "CRowSerializerTest.java"
        ]
    },
    "flink_ccd574a": {
        "bug_id": "flink_ccd574a",
        "commit": "https://github.com/apache/flink/commit/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "patch": "@@ -328,7 +328,7 @@ public void deployToSlot(final SimpleSlot slot) throws JobException {\n \t\t\t// register this execution at the execution graph, to receive call backs\n \t\t\tvertex.getExecutionGraph().registerExecution(this);\n \n-\t\t\tInstance instance = slot.getInstance();\n+\t\t\tfinal Instance instance = slot.getInstance();\n \t\t\tFuture<Object> deployAction = Patterns.ask(instance.getTaskManager(),\n \t\t\t\t\tnew SubmitTask(deployment), new Timeout(timeout));\n \n@@ -338,7 +338,9 @@ public void deployToSlot(final SimpleSlot slot) throws JobException {\n \t\t\t\tpublic void onComplete(Throwable failure, Object success) throws Throwable {\n \t\t\t\t\tif (failure != null) {\n \t\t\t\t\t\tif (failure instanceof TimeoutException) {\n-\t\t\t\t\t\t\tmarkFailed(new Exception(\"Cannot deploy task - TaskManager not responding.\", failure));\n+\t\t\t\t\t\t\tmarkFailed(new Exception(\n+\t\t\t\t\t\t\t\t\t\"Cannot deploy task - TaskManager \" + instance + \" not responding.\",\n+\t\t\t\t\t\t\t\t\tfailure));\n \t\t\t\t\t\t}\n \t\t\t\t\t\telse {\n \t\t\t\t\t\t\tmarkFailed(failure);",
                "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java",
                "sha": "baed9474dcefaff1d5bfcf97dde0a280334ce4d6",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
                "deletions": 7,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java",
                "patch": "@@ -68,10 +68,6 @@ public InputSplit getNextInputSplit() {\n \n \t\t\tfinal Object result = Await.result(response, timeout.duration());\n \n-\t\t\tif (result == null) {\n-\t\t\t\treturn null;\n-\t\t\t}\n-\n \t\t\tif(!(result instanceof JobManagerMessages.NextInputSplit)){\n \t\t\t\tthrow new RuntimeException(\"RequestNextInputSplit requires a response of type \" +\n \t\t\t\t\t\t\"NextInputSplit. Instead response is of type \" + result.getClass() + \".\");\n@@ -80,9 +76,14 @@ public InputSplit getNextInputSplit() {\n \t\t\t\t\t\t(JobManagerMessages.NextInputSplit) result;\n \n \t\t\t\tbyte[] serializedData = nextInputSplit.splitData();\n-\t\t\t\tObject deserialized = InstantiationUtil.deserializeObject(serializedData,\n-\t\t\t\t\t\tusercodeClassLoader);\n-\t\t\t\treturn (InputSplit) deserialized;\n+\n+\t\t\t\tif(serializedData == null) {\n+\t\t\t\t\treturn null;\n+\t\t\t\t} else {\n+\t\t\t\t\tObject deserialized = InstantiationUtil.deserializeObject(serializedData,\n+\t\t\t\t\t\t\tusercodeClassLoader);\n+\t\t\t\t\treturn (InputSplit) deserialized;\n+\t\t\t\t}\n \t\t\t}\n \t\t} catch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Requesting the next InputSplit failed.\", e);",
                "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProvider.java",
                "sha": "5a698509c9ed068135a2c26cc2881620882cccc6",
                "status": "modified"
            },
            {
                "additions": 93,
                "blob_url": "https://github.com/apache/flink/blob/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java?ref=ccd574a46e6fce44a9c1d0bf0ec72424c8252c98",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java",
                "patch": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.taskmanager;\n+\n+import akka.actor.ActorRef;\n+import akka.actor.ActorSystem;\n+import akka.actor.Props;\n+import akka.actor.Status;\n+import akka.actor.UntypedActor;\n+import akka.testkit.JavaTestKit;\n+import akka.util.Timeout;\n+import org.apache.flink.api.common.JobID;\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;\n+import org.apache.flink.runtime.jobgraph.JobVertexID;\n+import org.apache.flink.runtime.messages.JobManagerMessages;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+public class TaskInputSplitProviderTest {\n+\n+\tprivate static ActorSystem system;\n+\n+\t@BeforeClass\n+\tpublic static void setup() throws Exception {\n+\t\tsystem = ActorSystem.create(\"TestActorSystem\", TestingUtils.testConfig());\n+\t}\n+\n+\t@AfterClass\n+\tpublic static void teardown() throws Exception {\n+\t\tJavaTestKit.shutdownActorSystem(system);\n+\t\tsystem = null;\n+\t}\n+\n+\t@Test\n+\tpublic void testRequestNextInputSplitWithInvalidExecutionID() {\n+\n+\t\tfinal JobID jobID = new JobID();\n+\t\tfinal JobVertexID vertexID = new JobVertexID();\n+\t\tfinal ExecutionAttemptID executionID = new ExecutionAttemptID();\n+\t\tfinal Timeout timeout = new Timeout(10, TimeUnit.SECONDS);\n+\n+\t\tfinal ActorRef jobManagerRef = system.actorOf(Props.create(NullInputSplitJobManager.class));\n+\n+\t\tfinal TaskInputSplitProvider provider = new TaskInputSplitProvider(\n+\t\t\t\tjobManagerRef,\n+\t\t\t\tjobID,\n+\t\t\t\tvertexID,\n+\t\t\t\texecutionID,\n+\t\t\t\tgetClass().getClassLoader(),\n+\t\t\t\ttimeout\n+\t\t);\n+\n+\t\t// The jobManager will return a\n+\t\tInputSplit nextInputSplit = provider.getNextInputSplit();\n+\n+\t\tassertTrue(nextInputSplit == null);\n+\t}\n+\n+\tpublic static class NullInputSplitJobManager extends UntypedActor {\n+\n+\t\t@Override\n+\t\tpublic void onReceive(Object message) throws Exception {\n+\t\t\tif(message instanceof JobManagerMessages.RequestNextInputSplit) {\n+\t\t\t\tsender().tell(new JobManagerMessages.NextInputSplit(null), getSelf());\n+\t\t\t} else {\n+\t\t\t\tsender().tell(new Status.Failure(new Exception(\"Invalid message type\")), getSelf());\n+\t\t\t}\n+\t\t}\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/ccd574a46e6fce44a9c1d0bf0ec72424c8252c98/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskInputSplitProviderTest.java",
                "sha": "f0689789a2d223e6ac814bbf774b38da27786595",
                "status": "added"
            }
        ],
        "message": "[FLINK-1922] [runtime] Fixes NPE when TM receives a null input split\n\nThis closes #631",
        "parent": "https://github.com/apache/flink/commit/9a18e579021304f6ee0687cd1c9579740b11b98d",
        "patched_files": [
            "TaskInputSplitProvider.java",
            "Execution.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "TaskInputSplitProviderTest.java",
            "ExecutionTest.java"
        ]
    },
    "flink_cf9f4c7": {
        "bug_id": "flink_cf9f4c7",
        "commit": "https://github.com/apache/flink/commit/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java?ref=cf9f4c77c8e785567f036cc2fbb97c0cd16979a7",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java",
                "patch": "@@ -78,7 +78,9 @@ public static void discardStateFuture(RunnableFuture<? extends StateObject> stat\n \t\t\tif (!stateFuture.cancel(true)) {\n \t\t\t\tStateObject stateObject = FutureUtil.runIfNotDoneAndGet(stateFuture);\n \n-\t\t\t\tstateObject.discardState();\n+\t\t\t\tif (null != stateObject) {\n+\t\t\t\t\tstateObject.discardState();\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/main/java/org/apache/flink/runtime/state/StateUtil.java",
                "sha": "c6f5c8698f3d59c8b80a1e88fcac1b2a076da25d",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/flink/blob/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java?ref=cf9f4c77c8e785567f036cc2fbb97c0cd16979a7",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java",
                "patch": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.flink.runtime.state;\n+\n+import org.apache.flink.util.TestLogger;\n+import org.junit.Test;\n+\n+import java.util.concurrent.RunnableFuture;\n+\n+public class StateUtilTest extends TestLogger {\n+\n+\t/**\n+\t * Tests that {@link StateUtil#discardStateFuture} can handle state futures with null value.\n+\t */\n+\t@Test\n+\tpublic void testDiscardRunnableFutureWithNullValue() throws Exception {\n+\t\tRunnableFuture<StateHandle<?>> stateFuture = new DoneFuture<>(null);\n+\t\tStateUtil.discardStateFuture(stateFuture);\n+\t}\n+}",
                "raw_url": "https://github.com/apache/flink/raw/cf9f4c77c8e785567f036cc2fbb97c0cd16979a7/flink-runtime/src/test/java/org/apache/flink/runtime/state/StateUtilTest.java",
                "sha": "e59d027e87befe7fe8d87a7d66f958b9a8ebd9b4",
                "status": "added"
            }
        ],
        "message": "[FLINK-5643] Fix NPE in StateUtil\n\nIntroduces a null check to deal with state futures which have a null value.\n\nThis closes #3212.",
        "parent": "https://github.com/apache/flink/commit/cf6b3fb22c67a540dbfa96497d66041ba95ad358",
        "patched_files": [
            "StateUtil.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "StateUtilTest.java"
        ]
    },
    "flink_d433ba9": {
        "bug_id": "flink_d433ba9",
        "commit": "https://github.com/apache/flink/commit/d433ba9f032e5361ae894562b7a8be13cd3efe13",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/flink/blob/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java?ref=d433ba9f032e5361ae894562b7a8be13cd3efe13",
                "deletions": 1,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java",
                "patch": "@@ -66,7 +66,7 @@\n \t */\n \tprivate final ConcurrentMap<InputChannelID, InputChannelID> cancelled = Maps.newConcurrentMap();\n \n-\tprivate ChannelHandlerContext ctx;\n+\tprivate volatile ChannelHandlerContext ctx;\n \n \t// ------------------------------------------------------------------------\n \t// Input channel/receiver registration\n@@ -85,6 +85,10 @@ void removeInputChannel(RemoteInputChannel listener) {\n \t}\n \n \tvoid cancelRequestFor(InputChannelID inputChannelId) {\n+\t\tif (inputChannelId == null || ctx == null) {\n+\t\t\treturn;\n+\t\t}\n+\n \t\tif (cancelled.putIfAbsent(inputChannelId, inputChannelId) == null) {\n \t\t\tctx.writeAndFlush(new NettyMessage.CancelPartitionRequest(inputChannelId));\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java",
                "sha": "51b436bb0f8ebcc2b25c3a6a811f8f7a8be49cb3",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/flink/blob/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java?ref=d433ba9f032e5361ae894562b7a8be13cd3efe13",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "patch": "@@ -142,6 +142,22 @@ public void testReceivePartitionNotFoundException() throws Exception {\n \t\tverify(inputChannel, times(1)).onFailedPartitionRequest();\n \t}\n \n+\t@Test\n+\tpublic void testCancelBeforeActive() throws Exception {\n+\n+\t\tfinal RemoteInputChannel inputChannel = mock(RemoteInputChannel.class);\n+\t\twhen(inputChannel.getInputChannelId()).thenReturn(new InputChannelID());\n+\n+\t\tfinal PartitionRequestClientHandler client = new PartitionRequestClientHandler();\n+\t\tclient.addInputChannel(inputChannel);\n+\n+\t\t// Don't throw NPE\n+\t\tclient.cancelRequestFor(null);\n+\n+\t\t// Don't throw NPE, because channel is not active yet\n+\t\tclient.cancelRequestFor(inputChannel.getInputChannelId());\n+\t}\n+\n \t// ---------------------------------------------------------------------------------------------\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/d433ba9f032e5361ae894562b7a8be13cd3efe13/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "sha": "2c08cc5a2e4f821e41d570fa2441eba16f4566da",
                "status": "modified"
            }
        ],
        "message": "[FLINK-2177] [runtime] Fix possible NPE when closing Netty channel, before it is active",
        "parent": "https://github.com/apache/flink/commit/58b9a3772f5027f58335fb299b122e8ecb9db218",
        "patched_files": [
            "PartitionRequestClientHandler.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "PartitionRequestClientHandlerTest.java"
        ]
    },
    "flink_d4aed45": {
        "bug_id": "flink_d4aed45",
        "commit": "https://github.com/apache/flink/commit/d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/flink/blob/d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java?ref=d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5",
                "deletions": 8,
                "filename": "pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java",
                "patch": "@@ -388,8 +388,7 @@ public void assertEquals(final TestRecords expectedValues, FuzzyValueMatcher fuz\n \n \t\t\t// initialize with null\n \t\t\tList<Key> currentKeys = new ArrayList<Key>(Arrays.asList(new Key[sortInfo.sortKeys.size()])), nextKeys =\n-\t\t\t\tnew ArrayList<Key>(\n-\t\t\t\t\tcurrentKeys);\n+\t\t\t\tnew ArrayList<Key>(currentKeys);\n \t\t\tint itemIndex = 0;\n \t\t\tList<PactRecord> expectedValuesWithCurrentKey = new ArrayList<PactRecord>();\n \t\t\tList<PactRecord> actualValuesWithCurrentKey = new ArrayList<PactRecord>();\n@@ -497,24 +496,30 @@ private void matchValues(final Iterator<PactRecord> actualIterator, List<Key> cu\n \n \t\tif (actualValuesWithCurrentKey.isEmpty())\n \t\t\tthrow new ArrayComparisonFailure(\"Unexpected value for key \" + currentKeys, new AssertionFailedError(\n-\t\t\t\tAssert.format(\" \", expectedValuesWithCurrentKey, actualRecord)), itemIndex\n+\t\t\t\tAssert.format(\" \", toString(expectedValuesWithCurrentKey.iterator(), schema), toString(actualRecord,\n+\t\t\t\t\tschema))), itemIndex\n \t\t\t\t+ expectedValuesWithCurrentKey.size() - 1);\n \n \t\tfuzzyMatcher.removeMatchingValues(similarityMap, schema, expectedValuesWithCurrentKey,\n \t\t\tactualValuesWithCurrentKey);\n \n \t\tif (!expectedValuesWithCurrentKey.isEmpty() || !actualValuesWithCurrentKey.isEmpty())\n \t\t\tthrow new ArrayComparisonFailure(\"Unexpected values for key \" + currentKeys + \": \",\n-\t\t\t\tnew AssertionFailedError(Assert.format(\" \", expectedValuesWithCurrentKey, actualValuesWithCurrentKey)),\n+\t\t\t\tnew AssertionFailedError(Assert.format(\" \", toString(expectedValuesWithCurrentKey.iterator(), schema),\n+\t\t\t\t\ttoString(actualValuesWithCurrentKey.iterator(), schema))),\n \t\t\t\titemIndex - expectedValuesWithCurrentKey.size());\n \n \t\tif (actualRecord != null)\n \t\t\tactualValuesWithCurrentKey.add(actualRecord);\n \t}\n \n \tprivate static Object toString(Iterator<PactRecord> iterator, Class<? extends Value>[] schema) {\n+\t\treturn toString(iterator, schema, 20);\n+\t}\n+\n+\tprivate static Object toString(Iterator<PactRecord> iterator, Class<? extends Value>[] schema, int maxNum) {\n \t\tStringBuilder builder = new StringBuilder();\n-\t\tfor (int index = 0; index < 20 && iterator.hasNext(); index++) {\n+\t\tfor (int index = 0; index < maxNum && iterator.hasNext(); index++) {\n \t\t\tbuilder.append(toString(iterator.next(), schema));\n \t\t\tif (iterator.hasNext())\n \t\t\t\tbuilder.append(\", \");\n@@ -660,9 +665,15 @@ public void setSchema(Class<? extends Value>[] schema) {\n \t\t\t\t@Override\n \t\t\t\tpublic int compare(PactRecord o1, PactRecord o2) {\n \t\t\t\t\tfor (int index = 0; index < info.keyClasses.size(); index++) {\n-\t\t\t\t\t\tint comparison = info.comparators.get(index).compare(\n-\t\t\t\t\t\t\to1.getField(info.sortKeys.get(index), info.keyClasses.get(index)),\n-\t\t\t\t\t\t\to2.getField(info.sortKeys.get(index), info.keyClasses.get(index)));\n+\t\t\t\t\t\tKey f1 = o1.getField(info.sortKeys.get(index), info.keyClasses.get(index));\n+\t\t\t\t\t\tKey f2 = o2.getField(info.sortKeys.get(index), info.keyClasses.get(index));\n+\t\t\t\t\t\tif (f1 == f2)\n+\t\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t\tif (f1 == null)\n+\t\t\t\t\t\t\treturn -1;\n+\t\t\t\t\t\tif (f2 == null)\n+\t\t\t\t\t\t\treturn 1;\n+\t\t\t\t\t\tint comparison = info.comparators.get(index).compare(f1, f2);\n \t\t\t\t\t\tif (comparison != 0)\n \t\t\t\t\t\t\treturn comparison;\n \t\t\t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/d4aed45bfd813c7fdd7885a095fdf8c685b4fbd5/pact/pact-clients/src/main/java/eu/stratosphere/pact/testing/TestRecords.java",
                "sha": "b464b5ba447c1a03e0b722a4e1371ec8d164227a",
                "status": "modified"
            }
        ],
        "message": "Fixed NPE for TestRecords sorting",
        "parent": "https://github.com/apache/flink/commit/0c37ffe118c97a2e80029e1ee9e91f4aa2064fdd",
        "patched_files": [],
        "repo": "flink",
        "unit_tests": [
            "TestRecords.java"
        ]
    },
    "flink_d6435e8": {
        "bug_id": "flink_d6435e8",
        "commit": "https://github.com/apache/flink/commit/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 1,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java",
                "patch": "@@ -127,8 +127,9 @@ public Quantifier getQuantifier() {\n \t * @return The pattern with the new condition is set.\n \t */\n \tpublic Pattern<T, F> where(IterativeCondition<F> condition) {\n-\t\tClosureCleaner.clean(condition, true);\n+\t\tPreconditions.checkNotNull(condition, \"The condition cannot be null.\");\n \n+\t\tClosureCleaner.clean(condition, true);\n \t\tif (this.condition == null) {\n \t\t\tthis.condition = condition;\n \t\t} else {\n@@ -148,6 +149,8 @@ public Quantifier getQuantifier() {\n \t * @return The pattern with the new condition is set.\n \t */\n \tpublic Pattern<T, F> or(IterativeCondition<F> condition) {\n+\t\tPreconditions.checkNotNull(condition, \"The condition cannot be null.\");\n+\n \t\tClosureCleaner.clean(condition, true);\n \n \t\tif (this.condition == null) {\n@@ -167,6 +170,8 @@ public Quantifier getQuantifier() {\n \t * @return The same pattern with the new subtype constraint\n \t */\n \tpublic <S extends F> Pattern<T, S> subtype(final Class<S> subtypeClass) {\n+\t\tPreconditions.checkNotNull(subtypeClass, \"The class cannot be null.\");\n+\n \t\tif (condition == null) {\n \t\t\tthis.condition = new SubtypeCondition<F>(subtypeClass);\n \t\t} else {",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/Pattern.java",
                "sha": "b100bc5f80135981aa94027bb068a0f75a2ceb51",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 2,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which combines two conditions with a logical\n  * {@code AND} and returns {@code true} if both are {@code true}.\n@@ -32,8 +34,8 @@\n \tprivate final IterativeCondition<T> right;\n \n \tpublic AndCondition(final IterativeCondition<T> left, final IterativeCondition<T> right) {\n-\t\tthis.left = left;\n-\t\tthis.right = right;\n+\t\tthis.left = Preconditions.checkNotNull(left, \"The condition cannot be null.\");\n+\t\tthis.right = Preconditions.checkNotNull(right, \"The condition cannot be null.\");\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/AndCondition.java",
                "sha": "ac34c41301b9e24c2c5818f94b4a12dfad5f930f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 1,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java",
                "patch": "@@ -35,6 +35,6 @@ public NotCondition(final IterativeCondition<T> original) {\n \n \t@Override\n \tpublic boolean filter(T value, Context<T> ctx) throws Exception {\n-\t\treturn !original.filter(value, ctx);\n+\t\treturn original != null && !original.filter(value, ctx);\n \t}\n }",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/NotCondition.java",
                "sha": "9318c2f67726fb2cb6d30c1bad6824c302e2fc39",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 2,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which combines two conditions with a logical\n  * {@code OR} and returns {@code true} if at least one is {@code true}.\n@@ -32,8 +34,8 @@\n \tprivate final IterativeCondition<T> right;\n \n \tpublic OrCondition(final IterativeCondition<T> left, final IterativeCondition<T> right) {\n-\t\tthis.left = left;\n-\t\tthis.right = right;\n+\t\tthis.left = Preconditions.checkNotNull(left, \"The condition cannot be null.\");\n+\t\tthis.right = Preconditions.checkNotNull(right, \"The condition cannot be null.\");\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/OrCondition.java",
                "sha": "d3690ab4da065734b933c95dd954dc559798501d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 1,
                "filename": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.flink.cep.pattern.conditions;\n \n+import org.apache.flink.util.Preconditions;\n+\n /**\n  * A {@link IterativeCondition condition} which filters elements of the given type.\n  * An element is filtered out iff it is not assignable to the given subtype of {@code T}.\n@@ -31,7 +33,7 @@\n \tprivate final Class<? extends T> subtype;\n \n \tpublic SubtypeCondition(final Class<? extends T> subtype) {\n-\t\tthis.subtype = subtype;\n+\t\tthis.subtype = Preconditions.checkNotNull(subtype, \"The subtype cannot be null.\");\n \t}\n \n \t@Override",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/pattern/conditions/SubtypeCondition.java",
                "sha": "cff8693c588aacb7f1ed0d702b59e50fb768d4a3",
                "status": "modified"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/flink/blob/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java?ref=d6435e87cd4c58dfa26c2acf10474d7eb7c46f57",
                "deletions": 0,
                "filename": "flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
                "patch": "@@ -50,6 +50,72 @@\n @SuppressWarnings(\"unchecked\")\n public class NFAITCase extends TestLogger {\n \n+\t@Test\n+\tpublic void testNoConditionNFA() {\n+\t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();\n+\n+\t\tEvent a = new Event(40, \"a\", 1.0);\n+\t\tEvent b = new Event(41, \"b\", 2.0);\n+\t\tEvent c = new Event(42, \"c\", 3.0);\n+\t\tEvent d = new Event(43, \"d\", 4.0);\n+\t\tEvent e = new Event(44, \"e\", 5.0);\n+\n+\t\tinputEvents.add(new StreamRecord<>(a, 1));\n+\t\tinputEvents.add(new StreamRecord<>(b, 2));\n+\t\tinputEvents.add(new StreamRecord<>(c, 3));\n+\t\tinputEvents.add(new StreamRecord<>(d, 4));\n+\t\tinputEvents.add(new StreamRecord<>(e, 5));\n+\n+\t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").followedBy(\"end\");\n+\n+\t\tNFA<Event> nfa = NFACompiler.compile(pattern, Event.createTypeSerializer(), false);\n+\n+\t\tList<List<Event>> resultingPatterns = feedNFA(inputEvents, nfa);\n+\n+\t\tcompareMaps(resultingPatterns, Lists.<List<Event>>newArrayList(\n+\t\t\t\tLists.newArrayList(a, b),\n+\t\t\t\tLists.newArrayList(b, c),\n+\t\t\t\tLists.newArrayList(c, d),\n+\t\t\t\tLists.newArrayList(d, e)\n+\t\t));\n+\t}\n+\n+\t@Test\n+\tpublic void testAnyWithNoConditionNFA() {\n+\t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();\n+\n+\t\tEvent a = new Event(40, \"a\", 1.0);\n+\t\tEvent b = new Event(41, \"b\", 2.0);\n+\t\tEvent c = new Event(42, \"c\", 3.0);\n+\t\tEvent d = new Event(43, \"d\", 4.0);\n+\t\tEvent e = new Event(44, \"e\", 5.0);\n+\n+\t\tinputEvents.add(new StreamRecord<>(a, 1));\n+\t\tinputEvents.add(new StreamRecord<>(b, 2));\n+\t\tinputEvents.add(new StreamRecord<>(c, 3));\n+\t\tinputEvents.add(new StreamRecord<>(d, 4));\n+\t\tinputEvents.add(new StreamRecord<>(e, 5));\n+\n+\t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").followedByAny(\"end\");\n+\n+\t\tNFA<Event> nfa = NFACompiler.compile(pattern, Event.createTypeSerializer(), false);\n+\n+\t\tList<List<Event>> resultingPatterns = feedNFA(inputEvents, nfa);\n+\n+\t\tcompareMaps(resultingPatterns, Lists.<List<Event>>newArrayList(\n+\t\t\t\tLists.newArrayList(a, b),\n+\t\t\t\tLists.newArrayList(a, c),\n+\t\t\t\tLists.newArrayList(a, d),\n+\t\t\t\tLists.newArrayList(a, e),\n+\t\t\t\tLists.newArrayList(b, c),\n+\t\t\t\tLists.newArrayList(b, d),\n+\t\t\t\tLists.newArrayList(b, e),\n+\t\t\t\tLists.newArrayList(c, d),\n+\t\t\t\tLists.newArrayList(c, e),\n+\t\t\t\tLists.newArrayList(d, e)\n+\t\t));\n+\t}\n+\n \t@Test\n \tpublic void testSimplePatternNFA() {\n \t\tList<StreamRecord<Event>> inputEvents = new ArrayList<>();",
                "raw_url": "https://github.com/apache/flink/raw/d6435e87cd4c58dfa26c2acf10474d7eb7c46f57/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
                "sha": "fe31564af30af948fa8149b5a62215f800ab3bb2",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6445] [cep] Fix NPE in no-condition patterns.",
        "parent": "https://github.com/apache/flink/commit/a2ec3ee664b540c1213991d7fcf56d8873e60d40",
        "patched_files": [
            "Pattern.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "PatternTest.java"
        ]
    },
    "flink_d7911c5": {
        "bug_id": "flink_d7911c5",
        "commit": "https://github.com/apache/flink/commit/d7911c5a8a6896261c55b61ea4633e706270baa1",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/d7911c5a8a6896261c55b61ea4633e706270baa1/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java?ref=d7911c5a8a6896261c55b61ea4633e706270baa1",
                "deletions": 2,
                "filename": "flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java",
                "patch": "@@ -414,8 +414,10 @@ private void initFileSystem() throws IOException {\n \n \t@Override\n \tpublic void close() throws Exception {\n-\t\tfor (Map.Entry<String, BucketState<T>> entry : state.bucketStates.entrySet()) {\n-\t\t\tcloseCurrentPartFile(entry.getValue());\n+\t\tif (state != null) {\n+\t\t\tfor (Map.Entry<String, BucketState<T>> entry : state.bucketStates.entrySet()) {\n+\t\t\t\tcloseCurrentPartFile(entry.getValue());\n+\t\t\t}\n \t\t}\n \t}\n ",
                "raw_url": "https://github.com/apache/flink/raw/d7911c5a8a6896261c55b61ea4633e706270baa1/flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java",
                "sha": "db0a5d859bed4592627860748d687762a35964a7",
                "status": "modified"
            }
        ],
        "message": "[FLINK-6294] Fix potential NPE in BucketingSink.close()",
        "parent": "https://github.com/apache/flink/commit/d7c2c417213502130b1aeab1868313df178555cc",
        "patched_files": [
            "BucketingSink.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "BucketingSinkTest.java"
        ]
    },
    "flink_e0d1fd5": {
        "bug_id": "flink_e0d1fd5",
        "commit": "https://github.com/apache/flink/commit/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/flink/blob/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java?ref=e0d1fd504d6313b1b02a06ef7935cf7fd7069a39",
                "deletions": 2,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java",
                "patch": "@@ -147,8 +147,11 @@ else if (msgId == TaskEventRequest.ID) {\n \t\t\telse if (msgId == ErrorResponse.ID) {\n \t\t\t\tdecodedMsg = new ErrorResponse();\n \t\t\t}\n+\t\t\telse if (msgId == CancelPartitionRequest.ID) {\n+\t\t\t\tdecodedMsg = new CancelPartitionRequest();\n+\t\t\t}\n \t\t\telse {\n-\t\t\t\tthrow new IllegalStateException(\"Received unknown message from producer: \" + decodedMsg.getClass());\n+\t\t\t\tthrow new IllegalStateException(\"Received unknown message from producer: \" + msg);\n \t\t\t}\n \n \t\t\tif (decodedMsg != null) {\n@@ -486,6 +489,9 @@ public void readFrom(ByteBuf buffer) {\n \n \t\tInputChannelID receiverId;\n \n+\t\tpublic CancelPartitionRequest() {\n+\t\t}\n+\n \t\tpublic CancelPartitionRequest(InputChannelID receiverId) {\n \t\t\tthis.receiverId = receiverId;\n \t\t}\n@@ -495,7 +501,7 @@ ByteBuf write(ByteBufAllocator allocator) throws Exception {\n \t\t\tByteBuf result = null;\n \n \t\t\ttry {\n-\t\t\t\tresult = allocateBuffer(allocator, ID);\n+\t\t\t\tresult = allocateBuffer(allocator, ID, 16);\n \t\t\t\treceiverId.writeTo(result);\n \t\t\t}\n \t\t\tcatch (Throwable t) {",
                "raw_url": "https://github.com/apache/flink/raw/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java",
                "sha": "d0840b07ad843a51e439baac0d449dff2d8512ce",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/flink/blob/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java?ref=e0d1fd504d6313b1b02a06ef7935cf7fd7069a39",
                "deletions": 0,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java",
                "patch": "@@ -140,6 +140,13 @@ public void testEncodeDecode() {\n \t\t\tassertEquals(expected.partitionId, actual.partitionId);\n \t\t\tassertEquals(expected.receiverId, actual.receiverId);\n \t\t}\n+\n+\t\t{\n+\t\t\tNettyMessage.CancelPartitionRequest expected = new NettyMessage.CancelPartitionRequest(new InputChannelID());\n+\t\t\tNettyMessage.CancelPartitionRequest actual = encodeAndDecode(expected);\n+\n+\t\t\tassertEquals(expected.receiverId, actual.receiverId);\n+\t\t}\n \t}\n \n \t@SuppressWarnings(\"unchecked\")",
                "raw_url": "https://github.com/apache/flink/raw/e0d1fd504d6313b1b02a06ef7935cf7fd7069a39/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/NettyMessageSerializationTest.java",
                "sha": "b464692032647a0a3bf6747626a0352c5124eecd",
                "status": "modified"
            }
        ],
        "message": "[runtime] [tests] Fix possible NPE and add Netty serialization test",
        "parent": "https://github.com/apache/flink/commit/ae446388b91ecc0f08887da19400395b96b32f6c",
        "patched_files": [
            "NettyMessage.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "NettyMessageSerializationTest.java"
        ]
    },
    "flink_eb525b7": {
        "bug_id": "flink_eb525b7",
        "commit": "https://github.com/apache/flink/commit/eb525b7f889600fa4f4dbbdbee161848e5d570dd",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/flink/blob/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala?ref=eb525b7f889600fa4f4dbbdbee161848e5d570dd",
                "deletions": 2,
                "filename": "flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala",
                "patch": "@@ -132,6 +132,14 @@ class ProcTimeBoundedRangeOver(\n \n     val currentTime = timestamp - 1\n     var i = 0\n+    // get the list of elements of current proctime\n+    val currentElements = rowMapState.get(currentTime)\n+\n+    // Expired clean-up timers pass the needToCleanupState() check.\n+    // Perform a null check to verify that we have data to process.\n+    if (null == currentElements) {\n+      return\n+    }\n \n     // initialize the accumulators\n     var accumulators = accumulatorState.value()\n@@ -172,8 +180,7 @@ class ProcTimeBoundedRangeOver(\n       i += 1\n     }\n \n-    // get the list of elements of current proctime\n-    val currentElements = rowMapState.get(currentTime)\n+\n     // add current elements to aggregator. Multiple elements might\n     // have arrived in the same proctime\n     // the same accumulator value will be computed for all elements",
                "raw_url": "https://github.com/apache/flink/raw/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/runtime/aggregate/ProcTimeBoundedRangeOver.scala",
                "sha": "591b942571f196400866b2d8f8cdb261bf0d8017",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/flink/blob/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala?ref=eb525b7f889600fa4f4dbbdbee161848e5d570dd",
                "deletions": 0,
                "filename": "flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala",
                "patch": "@@ -208,6 +208,21 @@ class OverWindowHarnessTest extends HarnessTestBase{\n \n     testHarness.setProcessingTime(11006)\n \n+    // test for clean-up timer NPE\n+    testHarness.setProcessingTime(20000)\n+\n+    // timer registered for 23000\n+    testHarness.processElement(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 10L: JLong), change = true)))\n+\n+    // update clean-up timer to 25500. Previous timer should not clean up\n+    testHarness.setProcessingTime(22500)\n+    testHarness.processElement(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 20L: JLong), change = true)))\n+\n+    // 23000 clean-up timer should fire but not fail with an NPE\n+    testHarness.setProcessingTime(23001)\n+\n     val result = testHarness.getOutput\n \n     val expectedOutput = new ConcurrentLinkedQueue[Object]()\n@@ -241,6 +256,10 @@ class OverWindowHarnessTest extends HarnessTestBase{\n       CRow(Row.of(0L: JLong, \"aaa\", 10L: JLong, 7L: JLong, 10L: JLong), change = true)))\n     expectedOutput.add(new StreamRecord(\n       CRow(Row.of(0L: JLong, \"bbb\", 40L: JLong, 40L: JLong, 40L: JLong), change = true)))\n+    expectedOutput.add(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 10L: JLong, 10L: JLong, 10L: JLong), change = true)))\n+    expectedOutput.add(new StreamRecord(\n+      CRow(Row.of(0L: JLong, \"ccc\", 20L: JLong, 10L: JLong, 20L: JLong), change = true)))\n \n     verify(expectedOutput, result, new RowResultSortComparator())\n ",
                "raw_url": "https://github.com/apache/flink/raw/eb525b7f889600fa4f4dbbdbee161848e5d570dd/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/harness/OverWindowHarnessTest.scala",
                "sha": "218cae2dc193e71acc1c6ecc5eed4eeded05e548",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9524] [table] Check for expired clean-up timers to prevent NPE in ProcTimeBoundedRangeOver.\n\nThis closes #6180.",
        "parent": "https://github.com/apache/flink/commit/a161606a6e9c7191a8afd8a003c8a46be2350f76",
        "patched_files": [
            "ProcTimeBoundedRangeOver.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "OverWindowHarnessTest.java"
        ]
    },
    "flink_ecde6c3": {
        "bug_id": "flink_ecde6c3",
        "commit": "https://github.com/apache/flink/commit/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 0,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "patch": "@@ -157,6 +157,10 @@ public int getPartitionRequestMaxBackoff() {\n \t\treturn partitionRequestMaxBackoff;\n \t}\n \n+\tpublic boolean isCreditBased() {\n+\t\treturn enableCreditBased;\n+\t}\n+\n \tpublic KvStateRegistry getKvStateRegistry() {\n \t\treturn kvStateRegistry;\n \t}",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java",
                "sha": "f25475638723d8d364fe5e04c3b6b05bd2eed56d",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 5,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java",
                "patch": "@@ -157,9 +157,11 @@\n \t */\n \tprivate BufferPool bufferPool;\n \n-\t/** Global network buffer pool to request and recycle exclusive buffers. */\n+\t/** Global network buffer pool to request and recycle exclusive buffers (only for credit-based). */\n \tprivate NetworkBufferPool networkBufferPool;\n \n+\tprivate final boolean isCreditBased;\n+\n \tprivate boolean hasReceivedAllEndOfPartitionEvents;\n \n \t/** Flag indicating whether partitions have been requested. */\n@@ -189,7 +191,8 @@ public SingleInputGate(\n \t\tint consumedSubpartitionIndex,\n \t\tint numberOfInputChannels,\n \t\tTaskActions taskActions,\n-\t\tTaskIOMetricGroup metrics) {\n+\t\tTaskIOMetricGroup metrics,\n+\t\tboolean isCreditBased) {\n \n \t\tthis.owningTaskName = checkNotNull(owningTaskName);\n \t\tthis.jobId = checkNotNull(jobId);\n@@ -208,6 +211,7 @@ public SingleInputGate(\n \t\tthis.enqueuedInputChannelsWithData = new BitSet(numberOfInputChannels);\n \n \t\tthis.taskActions = checkNotNull(taskActions);\n+\t\tthis.isCreditBased = isCreditBased;\n \t}\n \n \t// ------------------------------------------------------------------------\n@@ -288,6 +292,7 @@ public void setBufferPool(BufferPool bufferPool) {\n \t * @param networkBuffersPerChannel The number of exclusive buffers for each channel\n \t */\n \tpublic void assignExclusiveSegments(NetworkBufferPool networkBufferPool, int networkBuffersPerChannel) throws IOException {\n+\t\tcheckState(this.isCreditBased, \"Bug in input gate setup logic: exclusive buffers only exist with credit-based flow control.\");\n \t\tcheckState(this.networkBufferPool == null, \"Bug in input gate setup logic: global buffer pool has\" +\n \t\t\t\"already been set for this input gate.\");\n \n@@ -347,8 +352,13 @@ public void updateInputChannel(InputChannelDeploymentDescriptor icdd) throws IOE\n \t\t\t\t}\n \t\t\t\telse if (partitionLocation.isRemote()) {\n \t\t\t\t\tnewChannel = unknownChannel.toRemoteInputChannel(partitionLocation.getConnectionId());\n-\t\t\t\t\t((RemoteInputChannel)newChannel).assignExclusiveSegments(\n-\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n+\n+\t\t\t\t\tif (this.isCreditBased) {\n+\t\t\t\t\t\tcheckState(this.networkBufferPool != null, \"Bug in input gate setup logic: \" +\n+\t\t\t\t\t\t\t\"global buffer pool has not been set for this input gate.\");\n+\t\t\t\t\t\t((RemoteInputChannel) newChannel).assignExclusiveSegments(\n+\t\t\t\t\t\t\tnetworkBufferPool.requestMemorySegments(networkBuffersPerChannel));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {\n \t\t\t\t\tthrow new IllegalStateException(\"Tried to update unknown channel with unknown channel.\");\n@@ -661,7 +671,7 @@ public static SingleInputGate create(\n \n \t\tfinal SingleInputGate inputGate = new SingleInputGate(\n \t\t\towningTaskName, jobId, consumedResultId, consumedPartitionType, consumedSubpartitionIndex,\n-\t\t\ticdd.length, taskActions, metrics);\n+\t\t\ticdd.length, taskActions, metrics, networkEnvironment.isCreditBased());\n \n \t\t// Create the input channels. There is one input channel for each consumed partition.\n \t\tfinal InputChannel[] inputChannels = new InputChannel[icdd.length];",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java",
                "sha": "06e80ff531aaff98c8de7fd0ecc221e800578bf0",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java",
                "patch": "@@ -329,7 +329,7 @@ private static ResultPartition createResultPartition(\n \t *\n \t * @return input gate with some fake settings\n \t */\n-\tprivate static SingleInputGate createSingleInputGate(\n+\tprivate SingleInputGate createSingleInputGate(\n \t\t\tfinal ResultPartitionType partitionType, final int channels) {\n \t\treturn spy(new SingleInputGate(\n \t\t\t\"Test Task Name\",\n@@ -339,7 +339,8 @@ private static SingleInputGate createSingleInputGate(\n \t\t\t0,\n \t\t\tchannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()));\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\tenableCreditBasedFlowControl));\n \t}\n \n \tprivate static void createRemoteInputChannel(",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java",
                "sha": "f790b5f02b960c85f954b7778be1b4d99f1366d4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "patch": "@@ -221,7 +221,8 @@ static SingleInputGate createSingleInputGate() {\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t}\n \n \t/**",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java",
                "sha": "842aed8186d2ee53f34901ed0c5ef110d2a1626c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 3,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java",
                "patch": "@@ -66,7 +66,8 @@ public void testConsumptionWithLocalChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -102,7 +103,8 @@ public void testConsumptionWithRemoteChannels() throws Exception {\n \t\t\t\t0,\n \t\t\t\tnumChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tRemoteInputChannel channel = new RemoteInputChannel(\n@@ -151,7 +153,8 @@ public void testConsumptionWithMixedChannels() throws Exception {\n \t\t\t\t0,\n \t\t\t\tnumChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0, local = 0; i < numChannels; i++) {\n \t\t\tif (localOrRemote.get(i)) {",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateConcurrentTest.java",
                "sha": "73f3cfbc49a392925f56a09cab1dd3ccda650c18",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 6,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java",
                "patch": "@@ -93,7 +93,8 @@ public void testFairConsumptionLocalChannelsPreFilled() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -146,7 +147,8 @@ public void testFairConsumptionLocalChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\t\tfor (int i = 0; i < numChannels; i++) {\n \t\t\t\tLocalInputChannel channel = new LocalInputChannel(gate, i, new ResultPartitionID(),\n@@ -196,7 +198,8 @@ public void testFairConsumptionRemoteChannelsPreFilled() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfinal ConnectionManager connManager = createDummyConnectionManager();\n \n@@ -251,7 +254,8 @@ public void testFairConsumptionRemoteChannels() throws Exception {\n \t\t\t\tnew IntermediateDataSetID(),\n \t\t\t\t0, numChannels,\n \t\t\t\tmock(TaskActions.class),\n-\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\ttrue);\n \n \t\tfinal ConnectionManager connManager = createDummyConnectionManager();\n \n@@ -349,11 +353,12 @@ public FairnessVerifyingInputGate(\n \t\t\t\tint consumedSubpartitionIndex,\n \t\t\t\tint numberOfInputChannels,\n \t\t\t\tTaskActions taskActions,\n-\t\t\t\tTaskIOMetricGroup metrics) {\n+\t\t\t\tTaskIOMetricGroup metrics,\n+\t\t\t\tboolean isCreditBased) {\n \n \t\t\tsuper(owningTaskName, jobId, consumedResultId, ResultPartitionType.PIPELINED,\n \t\t\t\tconsumedSubpartitionIndex,\n-\t\t\t\t\tnumberOfInputChannels, taskActions, metrics);\n+\t\t\t\t\tnumberOfInputChannels, taskActions, metrics, isCreditBased);\n \n \t\t\ttry {\n \t\t\t\tField f = SingleInputGate.class.getDeclaredField(\"inputChannelsWithData\");",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/InputGateFairnessTest.java",
                "sha": "82a27cc92c0255b97102f4d95910534b73252041",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "patch": "@@ -293,7 +293,8 @@ public void testConcurrentReleaseAndRetriggerPartitionRequest() throws Exception\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue\n \t\t);\n \n \t\tResultPartitionManager partitionManager = mock(ResultPartitionManager.class);\n@@ -490,7 +491,8 @@ public TestLocalInputChannelConsumer(\n \t\t\t\t\tsubpartitionIndex,\n \t\t\t\t\tnumberOfInputChannels,\n \t\t\t\t\tmock(TaskActions.class),\n-\t\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\t\t\ttrue);\n \n \t\t\t// Set buffer pool\n \t\t\tinputGate.setBufferPool(bufferPool);",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java",
                "sha": "1ecb67ff82c62f50a753a7c519081d7e37e647fb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java",
                "patch": "@@ -889,7 +889,8 @@ private SingleInputGate createSingleInputGate() {\n \t\t\t0,\n \t\t\t1,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t}\n \n \tprivate RemoteInputChannel createRemoteInputChannel(SingleInputGate inputGate)",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java",
                "sha": "802cb936c09493db28adb8820bce809fc37a77cf",
                "status": "modified"
            },
            {
                "additions": 248,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "changes": 318,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 70,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "patch": "@@ -19,13 +19,13 @@\n package org.apache.flink.runtime.io.network.partition.consumer;\n \n import org.apache.flink.api.common.JobID;\n-import org.apache.flink.core.memory.MemorySegment;\n import org.apache.flink.core.memory.MemorySegmentFactory;\n import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;\n import org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor;\n import org.apache.flink.runtime.deployment.ResultPartitionLocation;\n import org.apache.flink.runtime.event.TaskEvent;\n import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;\n+import org.apache.flink.runtime.io.disk.iomanager.IOManager;\n import org.apache.flink.runtime.io.network.ConnectionID;\n import org.apache.flink.runtime.io.network.ConnectionManager;\n import org.apache.flink.runtime.io.network.LocalConnectionManager;\n@@ -45,31 +45,51 @@\n import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;\n import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;\n import org.apache.flink.runtime.metrics.groups.UnregisteredMetricGroups;\n+import org.apache.flink.runtime.query.KvStateRegistry;\n import org.apache.flink.runtime.taskmanager.TaskActions;\n \n import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.concurrent.atomic.AtomicReference;\n \n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyInt;\n-import static org.mockito.Matchers.anyListOf;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+/**\n+ * Tests for {@link SingleInputGate}.\n+ */\n+@RunWith(Parameterized.class)\n public class SingleInputGateTest {\n \n+\t@Parameterized.Parameter\n+\tpublic boolean enableCreditBasedFlowControl;\n+\n+\t@Parameterized.Parameters(name = \"Credit-based = {0}\")\n+\tpublic static List<Boolean> parameters() {\n+\t\treturn Arrays.asList(Boolean.TRUE, Boolean.FALSE);\n+\t}\n+\n \t/**\n \t * Tests basic correctness of buffer-or-event interleaving and correct <code>null</code> return\n \t * value after receiving all end-of-partition events.\n@@ -324,12 +344,7 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t\tint initialBackoff = 137;\n \t\tint maxBackoff = 1001;\n \n-\t\tNetworkEnvironment netEnv = mock(NetworkEnvironment.class);\n-\t\twhen(netEnv.getResultPartitionManager()).thenReturn(new ResultPartitionManager());\n-\t\twhen(netEnv.getTaskEventDispatcher()).thenReturn(new TaskEventDispatcher());\n-\t\twhen(netEnv.getPartitionRequestInitialBackoff()).thenReturn(initialBackoff);\n-\t\twhen(netEnv.getPartitionRequestMaxBackoff()).thenReturn(maxBackoff);\n-\t\twhen(netEnv.getConnectionManager()).thenReturn(new LocalConnectionManager());\n+\t\tfinal NetworkEnvironment netEnv = createNetworkEnvironment(2, 8, initialBackoff, maxBackoff);\n \n \t\tSingleInputGate gate = SingleInputGate.create(\n \t\t\t\"TestTask\",\n@@ -340,37 +355,43 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t\t\tmock(TaskActions.class),\n \t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n \n-\t\tassertEquals(gateDesc.getConsumedPartitionType(), gate.getConsumedPartitionType());\n+\t\ttry {\n+\t\t\tassertEquals(gateDesc.getConsumedPartitionType(), gate.getConsumedPartitionType());\n \n-\t\tMap<IntermediateResultPartitionID, InputChannel> channelMap = gate.getInputChannels();\n+\t\t\tMap<IntermediateResultPartitionID, InputChannel> channelMap = gate.getInputChannels();\n \n-\t\tassertEquals(3, channelMap.size());\n-\t\tInputChannel localChannel = channelMap.get(partitionIds[0].getPartitionId());\n-\t\tassertEquals(LocalInputChannel.class, localChannel.getClass());\n+\t\t\tassertEquals(3, channelMap.size());\n+\t\t\tInputChannel localChannel = channelMap.get(partitionIds[0].getPartitionId());\n+\t\t\tassertEquals(LocalInputChannel.class, localChannel.getClass());\n \n-\t\tInputChannel remoteChannel = channelMap.get(partitionIds[1].getPartitionId());\n-\t\tassertEquals(RemoteInputChannel.class, remoteChannel.getClass());\n+\t\t\tInputChannel remoteChannel = channelMap.get(partitionIds[1].getPartitionId());\n+\t\t\tassertEquals(RemoteInputChannel.class, remoteChannel.getClass());\n \n-\t\tInputChannel unknownChannel = channelMap.get(partitionIds[2].getPartitionId());\n-\t\tassertEquals(UnknownInputChannel.class, unknownChannel.getClass());\n+\t\t\tInputChannel unknownChannel = channelMap.get(partitionIds[2].getPartitionId());\n+\t\t\tassertEquals(UnknownInputChannel.class, unknownChannel.getClass());\n \n-\t\tInputChannel[] channels = new InputChannel[]{localChannel, remoteChannel, unknownChannel};\n-\t\tfor (InputChannel ch : channels) {\n-\t\t\tassertEquals(0, ch.getCurrentBackoff());\n+\t\t\tInputChannel[] channels =\n+\t\t\t\tnew InputChannel[] {localChannel, remoteChannel, unknownChannel};\n+\t\t\tfor (InputChannel ch : channels) {\n+\t\t\t\tassertEquals(0, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff * 2, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff * 2, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(initialBackoff * 2 * 2, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(initialBackoff * 2 * 2, ch.getCurrentBackoff());\n \n-\t\t\tassertTrue(ch.increaseBackoff());\n-\t\t\tassertEquals(maxBackoff, ch.getCurrentBackoff());\n+\t\t\t\tassertTrue(ch.increaseBackoff());\n+\t\t\t\tassertEquals(maxBackoff, ch.getCurrentBackoff());\n \n-\t\t\tassertFalse(ch.increaseBackoff());\n+\t\t\t\tassertFalse(ch.increaseBackoff());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tgate.releaseAllResources();\n+\t\t\tnetEnv.shutdown();\n \t\t}\n \t}\n \n@@ -379,26 +400,39 @@ public void testRequestBackoffConfiguration() throws Exception {\n \t */\n \t@Test\n \tpublic void testRequestBuffersWithRemoteInputChannel() throws Exception {\n-\t\tfinal SingleInputGate inputGate = new SingleInputGate(\n-\t\t\t\"t1\",\n-\t\t\tnew JobID(),\n-\t\t\tnew IntermediateDataSetID(),\n-\t\t\tResultPartitionType.PIPELINED_BOUNDED,\n-\t\t\t0,\n-\t\t\t1,\n-\t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n-\n-\t\tRemoteInputChannel remote = mock(RemoteInputChannel.class);\n-\t\tinputGate.setInputChannel(new IntermediateResultPartitionID(), remote);\n-\n-\t\tfinal int buffersPerChannel = 2;\n-\t\tNetworkBufferPool network = mock(NetworkBufferPool.class);\n-\t\t// Trigger requests of segments from global pool and assign buffers to remote input channel\n-\t\tinputGate.assignExclusiveSegments(network, buffersPerChannel);\n-\n-\t\tverify(network, times(1)).requestMemorySegments(buffersPerChannel);\n-\t\tverify(remote, times(1)).assignExclusiveSegments(anyListOf(MemorySegment.class));\n+\t\tfinal SingleInputGate inputGate = createInputGate(1, ResultPartitionType.PIPELINED_BOUNDED);\n+\t\tint buffersPerChannel = 2;\n+\t\tint extraNetworkBuffersPerGate = 8;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel,\n+\t\t\textraNetworkBuffersPerGate, 0, 0);\n+\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n+\t\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\taddRemoteInputChannel(network, inputGate, connectionId, resultPartitionId, 0);\n+\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\n+\t\t\tNetworkBufferPool bufferPool = network.getNetworkBufferPool();\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool,\n+\t\t\t\t\ttimes(1)).requestMemorySegments(buffersPerChannel);\n+\t\t\t\tRemoteInputChannel remote = (RemoteInputChannel) inputGate.getInputChannels()\n+\t\t\t\t\t.get(resultPartitionId.getPartitionId());\n+\t\t\t\t// only the exclusive buffers should be assigned/available now\n+\t\t\t\tassertEquals(buffersPerChannel, remote.getNumberOfAvailableBuffers());\n+\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments() - buffersPerChannel,\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n \t}\n \n \t/**\n@@ -407,51 +441,195 @@ public void testRequestBuffersWithRemoteInputChannel() throws Exception {\n \t */\n \t@Test\n \tpublic void testRequestBuffersWithUnknownInputChannel() throws Exception {\n-\t\tfinal SingleInputGate inputGate = createInputGate(1);\n+\t\tfinal SingleInputGate inputGate = createInputGate(1, ResultPartitionType.PIPELINED_BOUNDED);\n+\t\tint buffersPerChannel = 2;\n+\t\tint extraNetworkBuffersPerGate = 8;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel, extraNetworkBuffersPerGate, 0, 0);\n \n-\t\tUnknownInputChannel unknown = mock(UnknownInputChannel.class);\n-\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n-\t\tinputGate.setInputChannel(resultPartitionId.getPartitionId(), unknown);\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID resultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, resultPartitionId, 0);\n \n-\t\tRemoteInputChannel remote = mock(RemoteInputChannel.class);\n-\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n-\t\twhen(unknown.toRemoteInputChannel(connectionId)).thenReturn(remote);\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\t\t\tNetworkBufferPool bufferPool = network.getNetworkBufferPool();\n \n-\t\tfinal int buffersPerChannel = 2;\n-\t\tNetworkBufferPool network = mock(NetworkBufferPool.class);\n-\t\tinputGate.assignExclusiveSegments(network, buffersPerChannel);\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool, times(0)).requestMemorySegments(buffersPerChannel);\n \n-\t\t// Trigger updates to remote input channel from unknown input channel\n-\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n-\t\t\tresultPartitionId,\n-\t\t\tResultPartitionLocation.createRemote(connectionId)));\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments(),\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\n+\t\t\t// Trigger updates to remote input channel from unknown input channel\n+\t\t\tfinal ConnectionID connectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tresultPartitionId,\n+\t\t\t\tResultPartitionLocation.createRemote(connectionId)));\n+\n+\t\t\tif (enableCreditBasedFlowControl) {\n+\t\t\t\tverify(bufferPool,\n+\t\t\t\t\ttimes(1)).requestMemorySegments(buffersPerChannel);\n+\t\t\t\tRemoteInputChannel remote = (RemoteInputChannel) inputGate.getInputChannels()\n+\t\t\t\t\t.get(resultPartitionId.getPartitionId());\n+\t\t\t\t// only the exclusive buffers should be assigned/available now\n+\t\t\t\tassertEquals(buffersPerChannel, remote.getNumberOfAvailableBuffers());\n+\n+\t\t\t\tassertEquals(bufferPool.getTotalNumberOfMemorySegments() - buffersPerChannel,\n+\t\t\t\t\tbufferPool.getNumberOfAvailableMemorySegments());\n+\t\t\t\t// note: exclusive buffers are not handed out into LocalBufferPool and are thus not counted\n+\t\t\t\tassertEquals(extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(buffersPerChannel + extraNetworkBuffersPerGate, bufferPool.countBuffers());\n+\t\t\t}\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n+\t}\n \n-\t\tverify(network, times(1)).requestMemorySegments(buffersPerChannel);\n-\t\tverify(remote, times(1)).assignExclusiveSegments(anyListOf(MemorySegment.class));\n+\t/**\n+\t * Tests that input gate can successfully convert unknown input channels into local and remote\n+\t * channels.\n+\t */\n+\t@Test\n+\tpublic void testUpdateUnknownInputChannel() throws Exception {\n+\t\tfinal SingleInputGate inputGate = createInputGate(2);\n+\t\tint buffersPerChannel = 2;\n+\t\tfinal NetworkEnvironment network = createNetworkEnvironment(buffersPerChannel, 8, 0, 0);\n+\n+\t\ttry {\n+\t\t\tfinal ResultPartitionID localResultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, localResultPartitionId, 0);\n+\n+\t\t\tfinal ResultPartitionID remoteResultPartitionId = new ResultPartitionID();\n+\t\t\taddUnknownInputChannel(network, inputGate, remoteResultPartitionId, 1);\n+\n+\t\t\tnetwork.setupInputGate(inputGate);\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\n+\t\t\t// Trigger updates to remote input channel from unknown input channel\n+\t\t\tfinal ConnectionID remoteConnectionId = new ConnectionID(new InetSocketAddress(\"localhost\", 5000), 0);\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tremoteResultPartitionId,\n+\t\t\t\tResultPartitionLocation.createRemote(remoteConnectionId)));\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((RemoteInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((UnknownInputChannel.class))));\n+\n+\t\t\t// Trigger updates to local input channel from unknown input channel\n+\t\t\tinputGate.updateInputChannel(new InputChannelDeploymentDescriptor(\n+\t\t\t\tlocalResultPartitionId,\n+\t\t\t\tResultPartitionLocation.createLocal()));\n+\n+\t\t\tassertThat(inputGate.getInputChannels().get(remoteResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((RemoteInputChannel.class))));\n+\t\t\tassertThat(inputGate.getInputChannels().get(localResultPartitionId.getPartitionId()),\n+\t\t\t\tis(instanceOf((LocalInputChannel.class))));\n+\t\t} finally {\n+\t\t\tinputGate.releaseAllResources();\n+\t\t\tnetwork.shutdown();\n+\t\t}\n \t}\n \n \t// ---------------------------------------------------------------------------------------------\n \n-\tprivate static SingleInputGate createInputGate() {\n+\tprivate NetworkEnvironment createNetworkEnvironment(\n+\t\t\tint buffersPerChannel,\n+\t\t\tint extraNetworkBuffersPerGate,\n+\t\t\tint initialBackoff,\n+\t\t\tint maxBackoff) {\n+\t\treturn new NetworkEnvironment(\n+\t\t\tspy(new NetworkBufferPool(100, 32)),\n+\t\t\tnew LocalConnectionManager(),\n+\t\t\tnew ResultPartitionManager(),\n+\t\t\tnew TaskEventDispatcher(),\n+\t\t\tnew KvStateRegistry(),\n+\t\t\tnull,\n+\t\t\tnull,\n+\t\t\tIOManager.IOMode.SYNC,\n+\t\t\tinitialBackoff,\n+\t\t\tmaxBackoff,\n+\t\t\tbuffersPerChannel,\n+\t\t\textraNetworkBuffersPerGate,\n+\t\t\tenableCreditBasedFlowControl);\n+\t}\n+\n+\tprivate SingleInputGate createInputGate() {\n \t\treturn createInputGate(2);\n \t}\n \n-\tprivate static SingleInputGate createInputGate(int numberOfInputChannels) {\n+\tprivate SingleInputGate createInputGate(int numberOfInputChannels) {\n+\t\treturn createInputGate(numberOfInputChannels, ResultPartitionType.PIPELINED);\n+\t}\n+\n+\tprivate SingleInputGate createInputGate(\n+\t\t\tint numberOfInputChannels, ResultPartitionType partitionType) {\n \t\tSingleInputGate inputGate = new SingleInputGate(\n \t\t\t\"Test Task Name\",\n \t\t\tnew JobID(),\n \t\t\tnew IntermediateDataSetID(),\n-\t\t\tResultPartitionType.PIPELINED,\n+\t\t\tpartitionType,\n \t\t\t0,\n \t\t\tnumberOfInputChannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\tenableCreditBasedFlowControl);\n \n-\t\tassertEquals(ResultPartitionType.PIPELINED, inputGate.getConsumedPartitionType());\n+\t\tassertEquals(partitionType, inputGate.getConsumedPartitionType());\n \n \t\treturn inputGate;\n \t}\n \n+\tprivate void addUnknownInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\tUnknownInputChannel unknown =\n+\t\t\tcreateUnknownInputChannel(network, inputGate, partitionId, channelIndex);\n+\t\tinputGate.setInputChannel(partitionId.getPartitionId(), unknown);\n+\t}\n+\n+\tprivate UnknownInputChannel createUnknownInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\treturn new UnknownInputChannel(\n+\t\t\tinputGate,\n+\t\t\tchannelIndex,\n+\t\t\tpartitionId,\n+\t\t\tnetwork.getResultPartitionManager(),\n+\t\t\tnetwork.getTaskEventDispatcher(),\n+\t\t\tnetwork.getConnectionManager(),\n+\t\t\tnetwork.getPartitionRequestInitialBackoff(),\n+\t\t\tnetwork.getPartitionRequestMaxBackoff(),\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup()\n+\t\t);\n+\t}\n+\n+\tprivate void addRemoteInputChannel(\n+\t\t\tNetworkEnvironment network,\n+\t\t\tSingleInputGate inputGate,\n+\t\t\tConnectionID connectionId,\n+\t\t\tResultPartitionID partitionId,\n+\t\t\tint channelIndex) {\n+\t\tRemoteInputChannel remote =\n+\t\t\tcreateUnknownInputChannel(network, inputGate, partitionId, channelIndex)\n+\t\t\t\t.toRemoteInputChannel(connectionId);\n+\t\tinputGate.setInputChannel(partitionId.getPartitionId(), remote);\n+\t}\n+\n \tstatic void verifyBufferOrEvent(\n \t\t\tInputGate inputGate,\n \t\t\tboolean expectedIsBuffer,",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java",
                "sha": "c24466880369b5ba00ddaf0d8efc626145e55bdb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 1,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java",
                "patch": "@@ -60,7 +60,8 @@ public TestSingleInputGate(int numberOfInputChannels, boolean initialize) {\n \t\t\t0,\n \t\t\tnumberOfInputChannels,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \n \t\tthis.inputGate = spy(realGate);\n ",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java",
                "sha": "33dc1ca205dc84fe92134c8db734151ddf61274f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/flink/blob/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java?ref=ecde6c328b38d6f6efea4b0d62f4ec8fe0040240",
                "deletions": 2,
                "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java",
                "patch": "@@ -50,13 +50,15 @@ public void testBasicGetNextLogic() throws Exception {\n \t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t0, 3,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \t\tfinal SingleInputGate ig2 = new SingleInputGate(\n \t\t\ttestTaskName, new JobID(),\n \t\t\tnew IntermediateDataSetID(), ResultPartitionType.PIPELINED,\n \t\t\t0, 5,\n \t\t\tmock(TaskActions.class),\n-\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup());\n+\t\t\tUnregisteredMetricGroups.createUnregisteredTaskMetricGroup().getIOMetricGroup(),\n+\t\t\ttrue);\n \n \t\tfinal UnionInputGate union = new UnionInputGate(new SingleInputGate[]{ig1, ig2});\n ",
                "raw_url": "https://github.com/apache/flink/raw/ecde6c328b38d6f6efea4b0d62f4ec8fe0040240/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java",
                "sha": "081d97d5cbe409e05d350c0c17ce0d68ab90b339",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9256] [network] Fix NPE in SingleInputGate#updateInputChannel() for non-credit based flow control\n\nThis closes #5914",
        "parent": "https://github.com/apache/flink/commit/b957bf26fb588cab072e51240e9026456b862ce7",
        "patched_files": [
            "SingleInputGate.java",
            "NetworkEnvironment.java",
            "UnionInputGate.java",
            "RemoteInputChannel.java",
            "LocalInputChannel.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "PartitionRequestClientHandlerTest.java",
            "RemoteInputChannelTest.java",
            "SingleInputGateTest.java",
            "UnionInputGateTest.java",
            "InputGateConcurrentTest.java",
            "TestSingleInputGate.java",
            "LocalInputChannelTest.java",
            "NetworkEnvironmentTest.java",
            "InputGateFairnessTest.java"
        ]
    },
    "flink_eece0dd": {
        "bug_id": "flink_eece0dd",
        "commit": "https://github.com/apache/flink/commit/eece0dd05bc38b88fcb6cbcef15add7f98eab456",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/flink/blob/eece0dd05bc38b88fcb6cbcef15add7f98eab456/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java?ref=eece0dd05bc38b88fcb6cbcef15add7f98eab456",
                "deletions": 2,
                "filename": "flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java",
                "patch": "@@ -291,9 +291,9 @@ public void commitSpecificOffsetsToKafka(Map<KafkaTopicPartition, Long> offsets)\n \t\tfor (KafkaTopicPartitionState<TopicPartition> partition : partitions) {\n \t\t\t// committed offsets through the KafkaConsumer need to be 1 more than the last processed offset.\n \t\t\t// This does not affect Flink's checkpoints/saved state.\n-\t\t\tLong offsetToCommit = offsets.get(partition.getKafkaTopicPartition()) + 1;\n+\t\t\tLong offsetToCommit = offsets.get(partition.getKafkaTopicPartition());\n \t\t\tif (offsetToCommit != null) {\n-\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit));\n+\t\t\t\toffsetsToCommit.put(partition.getKafkaPartitionHandle(), new OffsetAndMetadata(offsetToCommit + 1));\n \t\t\t\tpartition.setCommittedOffset(offsetToCommit);\n \t\t\t}\n \t\t}",
                "raw_url": "https://github.com/apache/flink/raw/eece0dd05bc38b88fcb6cbcef15add7f98eab456/flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java",
                "sha": "ad7efa28014ffe6ac21c5d26f0b98b8fd2cb1ef5",
                "status": "modified"
            }
        ],
        "message": "[hotfix] [kafka] Fix NPE in Kafka09Fetcher",
        "parent": "https://github.com/apache/flink/commit/72e6b760fd951764c3ecc6fc191dc99a42d55e0b",
        "patched_files": [
            "Kafka09Fetcher.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "Kafka09FetcherTest.java"
        ]
    },
    "flink_f486a3f": {
        "bug_id": "flink_f486a3f",
        "commit": "https://github.com/apache/flink/commit/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/flink/blob/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java?ref=f486a3fd6ed80b67e8eeed9245ad37b6b0be740b",
                "deletions": 3,
                "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "patch": "@@ -20,9 +20,9 @@\n \n import org.apache.flink.runtime.io.network.NetworkSequenceViewReader;\n import org.apache.flink.runtime.io.network.TaskEventDispatcher;\n+import org.apache.flink.runtime.io.network.netty.NettyMessage.AddCredit;\n import org.apache.flink.runtime.io.network.netty.NettyMessage.CancelPartitionRequest;\n import org.apache.flink.runtime.io.network.netty.NettyMessage.CloseRequest;\n-import org.apache.flink.runtime.io.network.netty.NettyMessage.AddCredit;\n import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;\n import org.apache.flink.runtime.io.network.partition.ResultPartitionProvider;\n import org.apache.flink.runtime.io.network.partition.consumer.InputChannelID;\n@@ -99,12 +99,12 @@ protected void channelRead0(ChannelHandlerContext ctx, NettyMessage msg) throws\n \t\t\t\t\t\t\toutboundQueue);\n \t\t\t\t\t}\n \n-\t\t\t\t\toutboundQueue.notifyReaderCreated(reader);\n-\n \t\t\t\t\treader.requestSubpartitionView(\n \t\t\t\t\t\tpartitionProvider,\n \t\t\t\t\t\trequest.partitionId,\n \t\t\t\t\t\trequest.queueIndex);\n+\n+\t\t\t\t\toutboundQueue.notifyReaderCreated(reader);\n \t\t\t\t} catch (PartitionNotFoundException notFound) {\n \t\t\t\t\trespondWithError(ctx, notFound, request.receiverId);\n \t\t\t\t}",
                "raw_url": "https://github.com/apache/flink/raw/f486a3fd6ed80b67e8eeed9245ad37b6b0be740b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java",
                "sha": "e9ee10cbc4d953248955811899711ba3bfd12208",
                "status": "modified"
            }
        ],
        "message": "[FLINK-9057][network] fix an NPE when cleaning up before requesting a subpartition view\n\nIn PartitionRequestServerHandler, the view reader was created and immediately\nafterwards added to the PartitionRequestQueue which would attempt a cleanup of\nthe view reader's subpartition view. This view, however, was currently only\ncreated after adding the reader to the PartitionRequestQueue and may thus result\nin a NullPointerException if the cleanup happens very early in the\ninitialization phase, e.g. due to failures.\n\nThis closes #5747.",
        "parent": "https://github.com/apache/flink/commit/41ae13122bd1ca16f1c6779983dc0d17e3633e97",
        "patched_files": [
            "PartitionRequestServerHandler.java"
        ],
        "repo": "flink",
        "unit_tests": [
            "PartitionRequestServerHandlerTest.java"
        ]
    }
}