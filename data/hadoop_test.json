{
    "hadoop_0247cb6": {
        "bug_id": "hadoop_0247cb6",
        "commit": "https://github.com/apache/hadoop/commit/0247cb6318507afe06816e337a19f396afc53efa",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java?ref=0247cb6318507afe06816e337a19f396afc53efa",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "patch": "@@ -598,6 +598,11 @@ private ShortCircuitReplicaInfo requestFileDescriptors(DomainPeer peer,\n       sock.recvFileInputStreams(fis, buf, 0, buf.length);\n       ShortCircuitReplica replica = null;\n       try {\n+        if (fis[0] == null || fis[1] == null) {\n+          throw new IOException(\"the datanode \" + datanode + \" failed to \" +\n+              \"pass a file descriptor (might have reached open file limit).\");\n+        }\n+\n         ExtendedBlockId key =\n             new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());\n         if (buf[0] == USE_RECEIPT_VERIFICATION.getNumber()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "sha": "ce4318531a33c88440d79cfc54c8bd70fd43689f",
                "status": "modified"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hadoop/blob/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java?ref=0247cb6318507afe06816e337a19f396afc53efa",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java",
                "patch": "@@ -42,6 +42,10 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.ClientContext;\n+import org.apache.hadoop.hdfs.DFSClient;\n+import org.apache.hadoop.hdfs.DFSUtilClient;\n+import org.apache.hadoop.hdfs.PeerCache;\n import org.apache.hadoop.hdfs.client.impl.BlockReaderFactory;\n import org.apache.hadoop.hdfs.client.impl.BlockReaderTestUtil;\n import org.apache.hadoop.hdfs.DFSInputStream;\n@@ -50,10 +54,12 @@\n import org.apache.hadoop.hdfs.ExtendedBlockId;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;\n+import org.apache.hadoop.hdfs.client.impl.DfsClientConf;\n import org.apache.hadoop.hdfs.net.DomainPeer;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfoBuilder;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader;\n import org.apache.hadoop.hdfs.server.datanode.DataNodeFaultInjector;\n import org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry;\n@@ -66,9 +72,11 @@\n import org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm.Slot;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.ipc.RetriableException;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.unix.DomainSocket;\n import org.apache.hadoop.net.unix.TemporarySocketDirectory;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n+import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.util.DataChecksum;\n import org.apache.hadoop.util.Time;\n@@ -819,4 +827,85 @@ public void testFetchOrCreateRetries() throws Exception {\n         .fetch(Mockito.eq(extendedBlockId), Mockito.any());\n     }\n   }\n+\n+  @Test\n+  public void testRequestFileDescriptorsWhenULimit() throws Exception {\n+    TemporarySocketDirectory sockDir = new TemporarySocketDirectory();\n+    Configuration conf = createShortCircuitConf(\n+        \"testRequestFileDescriptorsWhenULimit\", sockDir);\n+\n+    final short replicas = 1;\n+    final int fileSize = 3;\n+    final String testFile = \"/testfile\";\n+\n+    try (MiniDFSCluster cluster =\n+        new MiniDFSCluster.Builder(conf).numDataNodes(replicas).build()) {\n+\n+      cluster.waitActive();\n+\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      DFSTestUtil.createFile(fs, new Path(testFile), fileSize, replicas, 0L);\n+\n+      LocatedBlock blk = new DFSClient(DFSUtilClient.getNNAddress(conf), conf)\n+          .getLocatedBlocks(testFile, 0, fileSize).get(0);\n+\n+      ClientContext clientContext = Mockito.mock(ClientContext.class);\n+      Mockito.when(clientContext.getPeerCache()).thenAnswer(\n+          (Answer<PeerCache>) peerCacheCall -> {\n+            PeerCache peerCache = new PeerCache(10, Long.MAX_VALUE);\n+            DomainPeer peer = Mockito.spy(getDomainPeerToDn(conf));\n+            peerCache.put(blk.getLocations()[0], peer);\n+\n+            Mockito.when(peer.getDomainSocket()).thenAnswer(\n+                (Answer<DomainSocket>) domainSocketCall -> {\n+                  DomainSocket domainSocket = Mockito.mock(DomainSocket.class);\n+                  Mockito.when(domainSocket\n+                      .recvFileInputStreams(\n+                          Mockito.any(FileInputStream[].class),\n+                          Mockito.any(byte[].class),\n+                          Mockito.anyInt(),\n+                          Mockito.anyInt())\n+                  ).thenAnswer(\n+                      // we are mocking the FileOutputStream array with nulls\n+                      (Answer<Void>) recvFileInputStreamsCall -> null\n+                  );\n+                  return domainSocket;\n+                }\n+            );\n+\n+            return peerCache;\n+          });\n+\n+      Mockito.when(clientContext.getShortCircuitCache()).thenAnswer(\n+          (Answer<ShortCircuitCache>) shortCircuitCacheCall -> {\n+            ShortCircuitCache cache = Mockito.mock(ShortCircuitCache.class);\n+            Mockito.when(cache.allocShmSlot(\n+                Mockito.any(DatanodeInfo.class),\n+                Mockito.any(DomainPeer.class),\n+                Mockito.any(MutableBoolean.class),\n+                Mockito.any(ExtendedBlockId.class),\n+                Mockito.anyString()))\n+                .thenAnswer((Answer<Slot>) call -> null);\n+\n+            return cache;\n+          }\n+      );\n+\n+      DatanodeInfo[] nodes = blk.getLocations();\n+\n+      try {\n+        Assert.assertNull(new BlockReaderFactory(new DfsClientConf(conf))\n+            .setInetSocketAddress(NetUtils.createSocketAddr(nodes[0]\n+                .getXferAddr()))\n+            .setClientCacheContext(clientContext)\n+            .setDatanodeInfo(blk.getLocations()[0])\n+            .setBlock(blk.getBlock())\n+            .setBlockToken(new Token())\n+            .createShortCircuitReplicaInfo());\n+      } catch (NullPointerException ex) {\n+        Assert.fail(\"Should not throw NPE when the native library is unable \" +\n+            \"to create new files!\");\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java",
                "sha": "ac29c3c33f7624ba717823ee7c01daf0e8bc950a",
                "status": "modified"
            }
        ],
        "message": "HDFS-13121. NPE when request file descriptors when SC read. Contributed by Zsolt Venczel.",
        "parent": "https://github.com/apache/hadoop/commit/061b168529a9cd5d6a3a482c890bacdb49186368",
        "patched_files": [
            "ShortCircuitCache.java",
            "BlockReaderFactory.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockReaderFactory.java",
            "TestShortCircuitCache.java"
        ]
    },
    "hadoop_03af442": {
        "bug_id": "hadoop_03af442",
        "commit": "https://github.com/apache/hadoop/commit/03af442e7608db2f8e6eb85a15aa0ba42781edab",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/03af442e7608db2f8e6eb85a15aa0ba42781edab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSLeafQueue.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSLeafQueue.java?ref=03af442e7608db2f8e6eb85a15aa0ba42781edab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSLeafQueue.java",
                "patch": "@@ -88,6 +88,7 @@ public void testUpdateDemand() {\n \n     FSAppAttempt app = mock(FSAppAttempt.class);\n     Mockito.when(app.getDemand()).thenReturn(maxResource);\n+    Mockito.when(app.getResourceUsage()).thenReturn(Resources.none());\n \n     schedulable.addApp(app, true);\n     schedulable.addApp(app, true);",
                "raw_url": "https://github.com/apache/hadoop/raw/03af442e7608db2f8e6eb85a15aa0ba42781edab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFSLeafQueue.java",
                "sha": "4a738ca07fb6d2b08e4760e270a463ac5af32acd",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/03af442e7608db2f8e6eb85a15aa0ba42781edab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=03af442e7608db2f8e6eb85a15aa0ba42781edab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -5252,8 +5252,10 @@ public void testUpdateDemand() throws IOException {\n \n     FSAppAttempt app1 = mock(FSAppAttempt.class);\n     Mockito.when(app1.getDemand()).thenReturn(maxResource);\n+    Mockito.when(app1.getResourceUsage()).thenReturn(Resources.none());\n     FSAppAttempt app2 = mock(FSAppAttempt.class);\n     Mockito.when(app2.getDemand()).thenReturn(maxResource);\n+    Mockito.when(app2.getResourceUsage()).thenReturn(Resources.none());\n \n     QueueManager queueManager = scheduler.getQueueManager();\n     FSParentQueue queue1 = queueManager.getParentQueue(\"queue1\", true);",
                "raw_url": "https://github.com/apache/hadoop/raw/03af442e7608db2f8e6eb85a15aa0ba42781edab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "cd0570a570151eb3733cd9f042b45234ad3d2c5f",
                "status": "modified"
            }
        ],
        "message": "YARN-7385. TestFairScheduler#testUpdateDemand and TestFSLeafQueue#testUpdateDemand are failing with NPE (yufeigu via rkanter)",
        "parent": "https://github.com/apache/hadoop/commit/1c5c2b5dde6f2cffc587ca8f79a18828e1b1faf9",
        "patched_files": [
            "FSLeafQueue.java",
            "FairScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFairScheduler.java",
            "TestFSLeafQueue.java"
        ]
    },
    "hadoop_06e4c84": {
        "bug_id": "hadoop_06e4c84",
        "commit": "https://github.com/apache/hadoop/commit/06e4c84b7d9a5c6d0c80bc86e92b97632e1e4114",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/06e4c84b7d9a5c6d0c80bc86e92b97632e1e4114/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=06e4c84b7d9a5c6d0c80bc86e92b97632e1e4114",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -347,6 +347,9 @@ Release 2.4.1 - UNRELEASED\n     HDFS-6198. DataNode rolling upgrade does not correctly identify current\n     block pool directory and replace with trash on Windows. (cnauroth)\n \n+    HDFS-6206. Fix NullPointerException in DFSUtil.substituteForWildcardAddress.\n+    (szetszwo) \n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/06e4c84b7d9a5c6d0c80bc86e92b97632e1e4114/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3d2675c1e915d6ef8280c62b184e1929d97b476c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/06e4c84b7d9a5c6d0c80bc86e92b97632e1e4114/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=06e4c84b7d9a5c6d0c80bc86e92b97632e1e4114",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -37,6 +37,7 @@\n import java.io.IOException;\n import java.io.PrintStream;\n import java.io.UnsupportedEncodingException;\n+import java.net.InetAddress;\n import java.net.InetSocketAddress;\n import java.net.URI;\n import java.net.URISyntaxException;\n@@ -1100,7 +1101,8 @@ static String substituteForWildcardAddress(String configuredAddress,\n     InetSocketAddress sockAddr = NetUtils.createSocketAddr(configuredAddress);\n     InetSocketAddress defaultSockAddr = NetUtils.createSocketAddr(defaultHost\n         + \":0\");\n-    if (sockAddr.getAddress().isAnyLocalAddress()) {\n+    final InetAddress addr = sockAddr.getAddress();\n+    if (addr != null && addr.isAnyLocalAddress()) {\n       if (UserGroupInformation.isSecurityEnabled() &&\n           defaultSockAddr.getAddress().isAnyLocalAddress()) {\n         throw new IOException(\"Cannot use a wildcard address with security. \" +",
                "raw_url": "https://github.com/apache/hadoop/raw/06e4c84b7d9a5c6d0c80bc86e92b97632e1e4114/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "347237bea6edddf3f9e8812f94fc7fb505a22d57",
                "status": "modified"
            }
        ],
        "message": "HDFS-6206. Fix NullPointerException in DFSUtil.substituteForWildcardAddress.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1586034 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/f13a0fd2adc32c60d195d9bc33c15def7400f128",
        "patched_files": [
            "DFSUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop_0753031": {
        "bug_id": "hadoop_0753031",
        "commit": "https://github.com/apache/hadoop/commit/07530314c2130ecd1525682c59bf51f15b82c024",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java?ref=07530314c2130ecd1525682c59bf51f15b82c024",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "patch": "@@ -26,9 +26,8 @@\n  */\n public class IpcException extends IOException {\n   private static final long serialVersionUID = 1L;\n-  \n-  final String errMsg;\n+\n   public IpcException(final String err) {\n-    errMsg = err; \n+    super(err);\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "sha": "61c42b80887f0f941885e52bb918b94b4f90d265",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=07530314c2130ecd1525682c59bf51f15b82c024",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2155,7 +2155,7 @@ private void doSaslReply(Message message) throws IOException {\n     private void doSaslReply(Exception ioe) throws IOException {\n       setupResponse(authFailedCall,\n           RpcStatusProto.FATAL, RpcErrorCodeProto.FATAL_UNAUTHORIZED,\n-          null, ioe.getClass().getName(), ioe.getLocalizedMessage());\n+          null, ioe.getClass().getName(), ioe.toString());\n       sendResponse(authFailedCall);\n     }\n \n@@ -2550,7 +2550,8 @@ private void processOneRpc(ByteBuffer bb)\n         final RpcCall call = new RpcCall(this, callId, retry);\n         setupResponse(call,\n             rse.getRpcStatusProto(), rse.getRpcErrorCodeProto(), null,\n-            t.getClass().getName(), t.getMessage());\n+            t.getClass().getName(),\n+            t.getMessage() != null ? t.getMessage() : t.toString());\n         sendResponse(call);\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "14fc2de530ce6cf2fe4550380c2bfec774ba14e4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-9844. NPE when trying to create an error message response of SASL RPC\n\nThis closes #55\n\nChange-Id: I10a20380565fa89762f4aa564b2f1c83b9aeecdc\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/98653ecccb80a7d793b2d27c81aebd3347f64b3c",
        "patched_files": [
            "Server.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop_07650bc": {
        "bug_id": "hadoop_07650bc",
        "commit": "https://github.com/apache/hadoop/commit/07650bc37a3c78ecc6566d813778d0954d0b06b0",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/07650bc37a3c78ecc6566d813778d0954d0b06b0/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=07650bc37a3c78ecc6566d813778d0954d0b06b0",
                "deletions": 5,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -1827,13 +1827,24 @@ private boolean delBlockFromDisk(File blockFile, File metaFile, Block b) {\n     Map<String, BlockListAsLongs.Builder> builders =\n         new HashMap<String, BlockListAsLongs.Builder>();\n \n-    List<FsVolumeImpl> curVolumes = volumes.getVolumes();\n-    for (FsVolumeSpi v : curVolumes) {\n-      builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n-    }\n-\n+    List<FsVolumeImpl> curVolumes = null;\n     try (AutoCloseableLock lock = datasetLock.acquire()) {\n+      curVolumes = volumes.getVolumes();\n+      for (FsVolumeSpi v : curVolumes) {\n+        builders.put(v.getStorageID(), BlockListAsLongs.builder(maxDataLength));\n+      }\n+\n+      Set<String> missingVolumesReported = new HashSet<>();\n       for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n+        String volStorageID = b.getVolume().getStorageID();\n+        if (!builders.containsKey(volStorageID)) {\n+          if (!missingVolumesReported.contains(volStorageID)) {\n+            LOG.warn(\"Storage volume: \" + volStorageID + \" missing for the\"\n+                + \" replica block: \" + b + \". Probably being removed!\");\n+            missingVolumesReported.add(volStorageID);\n+          }\n+          continue;\n+        }\n         switch(b.getState()) {\n           case FINALIZED:\n           case RBW:",
                "raw_url": "https://github.com/apache/hadoop/raw/07650bc37a3c78ecc6566d813778d0954d0b06b0/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "e0d2baf8b688bb50e8bf07baa413909f1f01c98b",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hadoop/blob/07650bc37a3c78ecc6566d813778d0954d0b06b0/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java?ref=07650bc37a3c78ecc6566d813778d0954d0b06b0",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "patch": "@@ -590,27 +590,43 @@ public void testRemoveVolumeBeingWritten() throws Exception {\n     final ExtendedBlock eb = new ExtendedBlock(BLOCK_POOL_IDS[0], 0);\n     final CountDownLatch startFinalizeLatch = new CountDownLatch(1);\n     final CountDownLatch brReceivedLatch = new CountDownLatch(1);\n+    final CountDownLatch volRemovedLatch = new CountDownLatch(1);\n     class BlockReportThread extends Thread {\n       public void run() {\n+        // Lets wait for the volume remove process to start\n+        try {\n+          volRemovedLatch.await();\n+        } catch (Exception e) {\n+          LOG.info(\"Unexpected exception when waiting for vol removal:\", e);\n+        }\n         LOG.info(\"Getting block report\");\n         dataset.getBlockReports(eb.getBlockPoolId());\n         LOG.info(\"Successfully received block report\");\n         brReceivedLatch.countDown();\n       }\n     }\n \n-    final BlockReportThread brt = new BlockReportThread();\n     class ResponderThread extends Thread {\n       public void run() {\n         try (ReplicaHandler replica = dataset\n             .createRbw(StorageType.DEFAULT, eb, false)) {\n-          LOG.info(\"createRbw finished\");\n+          LOG.info(\"CreateRbw finished\");\n           startFinalizeLatch.countDown();\n \n-          // Slow down while we're holding the reference to the volume\n-          Thread.sleep(1000);\n+          // Slow down while we're holding the reference to the volume.\n+          // As we finalize a block, the volume is removed in parallel.\n+          // Ignore any interrupts coming out of volume shutdown.\n+          try {\n+            Thread.sleep(1000);\n+          } catch (InterruptedException ie) {\n+            LOG.info(\"Ignoring \", ie);\n+          }\n+\n+          // Lets wait for the other thread finish getting block report\n+          brReceivedLatch.await();\n+\n           dataset.finalizeBlock(eb);\n-          LOG.info(\"finalizeBlock finished\");\n+          LOG.info(\"FinalizeBlock finished\");\n         } catch (Exception e) {\n           LOG.warn(\"Exception caught. This should not affect the test\", e);\n         }\n@@ -621,13 +637,28 @@ public void run() {\n     res.start();\n     startFinalizeLatch.await();\n \n+    // Verify if block report can be received\n+    // when volume is being removed\n+    final BlockReportThread brt = new BlockReportThread();\n+    brt.start();\n+\n     Set<File> volumesToRemove = new HashSet<>();\n     volumesToRemove.add(\n         StorageLocation.parse(dataset.getVolume(eb).getBasePath()).getFile());\n-    LOG.info(\"Removing volume \" + volumesToRemove);\n-    // Verify block report can be received during this\n-    brt.start();\n-    dataset.removeVolumes(volumesToRemove, true);\n+    /**\n+     * TODO: {@link FsDatasetImpl#removeVolumes(Set, boolean)} is throwing\n+     * IllegalMonitorStateException when there is a parallel reader/writer\n+     * to the volume. Remove below try/catch block after fixing HDFS-10830.\n+     */\n+    try {\n+      LOG.info(\"Removing volume \" + volumesToRemove);\n+      dataset.removeVolumes(volumesToRemove, true);\n+    } catch (Exception e) {\n+      LOG.info(\"Unexpected issue while removing volume: \", e);\n+    } finally {\n+      volRemovedLatch.countDown();\n+    }\n+\n     LOG.info(\"Volumes removed\");\n     brReceivedLatch.await();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/07650bc37a3c78ecc6566d813778d0954d0b06b0/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "sha": "b9468032811d6d360c86546cb3e3e9f0d82cc2a9",
                "status": "modified"
            }
        ],
        "message": "HDFS-9781. FsDatasetImpl#getBlockReports can occasionally throw NullPointerException. Contributed by Manoj Govindassamy.",
        "parent": "https://github.com/apache/hadoop/commit/f6ea9be5473ab66798b0536317d2f32c5348eb57",
        "patched_files": [
            "FsDatasetImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_07e4fb1": {
        "bug_id": "hadoop_07e4fb1",
        "commit": "https://github.com/apache/hadoop/commit/07e4fb1455abc33584fc666ef745abe256ebd7d1",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=07e4fb1455abc33584fc666ef745abe256ebd7d1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -244,12 +244,14 @@ Trunk (Unreleased)\n     HDFS-5636. Enforce a max TTL per cache pool. (awang via cmccabe)\n \n   OPTIMIZATIONS\n+\n     HDFS-5349. DNA_CACHE and DNA_UNCACHE should be by blockId only. (cmccabe)\n \n     HDFS-5665. Remove the unnecessary writeLock while initializing CacheManager\n     in FsNameSystem Ctor. (Uma Maheswara Rao G via Andrew Wang)\n \n   BUG FIXES\n+\n     HADOOP-9635 Fix potential Stack Overflow in DomainSocket.c (V. Karthik Kumar\n                 via cmccabe)\n \n@@ -456,6 +458,10 @@ Trunk (Unreleased)\n     HDFS-5701. Fix the CacheAdmin -addPool -maxTtl option name.\n     (Stephen Chu via wang)\n \n+    HDFS-5708. The CacheManager throws a NPE in the DataNode logs when\n+    processing cache reports that refer to a block not known to the\n+    BlockManager. (cmccabe via wang)\n+\n   BREAKDOWN OF HDFS-2832 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4985. Add storage type to the protocol and expose it in block report",
                "raw_url": "https://github.com/apache/hadoop/raw/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "b1bcdf81d36625433c59e105d302386686221198",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop/blob/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java?ref=07e4fb1455abc33584fc666ef745abe256ebd7d1",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
                "patch": "@@ -460,14 +460,21 @@ private void rescanFile(CacheDirective directive, INodeFile file) {\n             directive.getReplication()) * blockInfo.getNumBytes();\n         cachedTotal += cachedByBlock;\n \n-        if (mark != ocblock.getMark()) {\n-          // Mark hasn't been set in this scan, so update replication and mark.\n+        if ((mark != ocblock.getMark()) ||\n+            (ocblock.getReplication() < directive.getReplication())) {\n+          //\n+          // Overwrite the block's replication and mark in two cases:\n+          //\n+          // 1. If the mark on the CachedBlock is different from the mark for\n+          // this scan, that means the block hasn't been updated during this\n+          // scan, and we should overwrite whatever is there, since it is no\n+          // longer valid.\n+          //\n+          // 2. If the replication in the CachedBlock is less than what the\n+          // directive asks for, we want to increase the block's replication\n+          // field to what the directive asks for.\n+          //\n           ocblock.setReplicationAndMark(directive.getReplication(), mark);\n-        } else {\n-          // Mark already set in this scan.  Set replication to highest value in\n-          // any CacheDirective that covers this file.\n-          ocblock.setReplicationAndMark((short)Math.max(\n-              directive.getReplication(), ocblock.getReplication()), mark);\n         }\n       }\n     }\n@@ -483,6 +490,36 @@ private void rescanFile(CacheDirective directive, INodeFile file) {\n     }\n   }\n \n+  private String findReasonForNotCaching(CachedBlock cblock, \n+          BlockInfo blockInfo) {\n+    if (blockInfo == null) {\n+      // Somehow, a cache report with the block arrived, but the block\n+      // reports from the DataNode haven't (yet?) described such a block.\n+      // Alternately, the NameNode might have invalidated the block, but the\n+      // DataNode hasn't caught up.  In any case, we want to tell the DN\n+      // to uncache this.\n+      return \"not tracked by the BlockManager\";\n+    } else if (!blockInfo.isComplete()) {\n+      // When a cached block changes state from complete to some other state\n+      // on the DataNode (perhaps because of append), it will begin the\n+      // uncaching process.  However, the uncaching process is not\n+      // instantaneous, especially if clients have pinned the block.  So\n+      // there may be a period of time when incomplete blocks remain cached\n+      // on the DataNodes.\n+      return \"not complete\";\n+    }  else if (cblock.getReplication() == 0) {\n+      // Since 0 is not a valid value for a cache directive's replication\n+      // field, seeing a replication of 0 on a CacheBlock means that it\n+      // has never been reached by any sweep.\n+      return \"not needed by any directives\";\n+    } else if (cblock.getMark() != mark) { \n+      // Although the block was needed in the past, we didn't reach it during\n+      // the current sweep.  Therefore, it doesn't need to be cached any more.\n+      return \"no longer needed by any directives\";\n+    }\n+    return null;\n+  }\n+\n   /**\n    * Scan through the cached block map.\n    * Any blocks which are under-replicated should be assigned new Datanodes.\n@@ -508,11 +545,17 @@ private void rescanCachedBlockMap() {\n           iter.remove();\n         }\n       }\n-      // If the block's mark doesn't match with the mark of this scan, that\n-      // means that this block couldn't be reached during this scan.  That means\n-      // it doesn't need to be cached any more.\n-      int neededCached = (cblock.getMark() != mark) ?\n-          0 : cblock.getReplication();\n+      BlockInfo blockInfo = blockManager.\n+            getStoredBlock(new Block(cblock.getBlockId()));\n+      String reason = findReasonForNotCaching(cblock, blockInfo);\n+      int neededCached = 0;\n+      if (reason != null) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"not caching \" + cblock + \" because it is \" + reason);\n+        }\n+      } else {\n+        neededCached = cblock.getReplication();\n+      }\n       int numCached = cached.size();\n       if (numCached >= neededCached) {\n         // If we have enough replicas, drop all pending cached.\n@@ -612,8 +655,10 @@ private void addNewPendingCached(int neededCached,\n     BlockInfo blockInfo = blockManager.\n           getStoredBlock(new Block(cachedBlock.getBlockId()));\n     if (blockInfo == null) {\n-      LOG.debug(\"Not caching block \" + cachedBlock + \" because it \" +\n-          \"was deleted from all DataNodes.\");\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Not caching block \" + cachedBlock + \" because there \" +\n+            \"is no record of it on the NameNode.\");\n+      }\n       return;\n     }\n     if (!blockInfo.isComplete()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java",
                "sha": "e86f345a4995b903a613b1af5ab91e8935f9bcba",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java?ref=07e4fb1455abc33584fc666ef745abe256ebd7d1",
                "deletions": 25,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
                "patch": "@@ -62,7 +62,6 @@\n import org.apache.hadoop.hdfs.protocol.CachePoolInfo;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n-import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n import org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;\n@@ -940,39 +939,28 @@ private void processCacheReportImpl(final DatanodeDescriptor datanode,\n       final List<Long> blockIds) {\n     CachedBlocksList cached = datanode.getCached();\n     cached.clear();\n+    CachedBlocksList cachedList = datanode.getCached();\n+    CachedBlocksList pendingCachedList = datanode.getPendingCached();\n     for (Iterator<Long> iter = blockIds.iterator(); iter.hasNext(); ) {\n-      Block block = new Block(iter.next());\n-      BlockInfo blockInfo = blockManager.getStoredBlock(block);\n-      if (!blockInfo.isComplete()) {\n-        LOG.warn(\"Ignoring block id \" + block.getBlockId() + \", because \" +\n-            \"it is in not complete yet.  It is in state \" + \n-            blockInfo.getBlockUCState());\n-        continue;\n-      }\n-      Collection<DatanodeDescriptor> corruptReplicas =\n-          blockManager.getCorruptReplicas(blockInfo);\n-      if ((corruptReplicas != null) && corruptReplicas.contains(datanode)) {\n-        // The NameNode will eventually remove or update the corrupt block.\n-        // Until then, we pretend that it isn't cached.\n-        LOG.warn(\"Ignoring cached replica on \" + datanode + \" of \" + block +\n-            \" because it is corrupt.\");\n-        continue;\n-      }\n+      long blockId = iter.next();\n       CachedBlock cachedBlock =\n-          new CachedBlock(block.getBlockId(), (short)0, false);\n+          new CachedBlock(blockId, (short)0, false);\n       CachedBlock prevCachedBlock = cachedBlocks.get(cachedBlock);\n-      // Use the existing CachedBlock if it's present; otherwise,\n-      // insert a new one.\n+      // Add the block ID from the cache report to the cachedBlocks map\n+      // if it's not already there.\n       if (prevCachedBlock != null) {\n         cachedBlock = prevCachedBlock;\n       } else {\n         cachedBlocks.put(cachedBlock);\n       }\n-      if (!cachedBlock.isPresent(datanode.getCached())) {\n-        datanode.getCached().add(cachedBlock);\n+      // Add the block to the datanode's implicit cached block list\n+      // if it's not already there.  Similarly, remove it from the pending\n+      // cached block list if it exists there.\n+      if (!cachedBlock.isPresent(cachedList)) {\n+        cachedList.add(cachedBlock);\n       }\n-      if (cachedBlock.isPresent(datanode.getPendingCached())) {\n-        datanode.getPendingCached().remove(cachedBlock);\n+      if (cachedBlock.isPresent(pendingCachedList)) {\n+        pendingCachedList.remove(cachedBlock);\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
                "sha": "f24b386df16942d0f23b2dbe5e22d29caea28c42",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java?ref=07e4fb1455abc33584fc666ef745abe256ebd7d1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "patch": "@@ -69,6 +69,7 @@\n import org.apache.hadoop.hdfs.protocol.CachePoolStats;\n import org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor.CachedBlocksList.Type;\n+import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n import org.apache.hadoop.io.nativeio.NativeIO;\n import org.apache.hadoop.io.nativeio.NativeIO.POSIX.CacheManipulator;\n@@ -796,7 +797,15 @@ public Boolean get() {\n       }\n     }, 500, 60000);\n \n+    // Send a cache report referring to a bogus block.  It is important that\n+    // the NameNode be robust against this.\n     NamenodeProtocols nnRpc = namenode.getRpcServer();\n+    DataNode dn0 = cluster.getDataNodes().get(0);\n+    String bpid = cluster.getNamesystem().getBlockPoolId();\n+    LinkedList<Long> bogusBlockIds = new LinkedList<Long> ();\n+    bogusBlockIds.add(999999L);\n+    nnRpc.cacheReport(dn0.getDNRegistrationForBP(bpid), bpid, bogusBlockIds);\n+\n     Path rootDir = helper.getDefaultWorkingDirectory(dfs);\n     // Create the pool\n     final String pool = \"friendlyPool\";",
                "raw_url": "https://github.com/apache/hadoop/raw/07e4fb1455abc33584fc666ef745abe256ebd7d1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "sha": "6ab808ea167465887f592f8f916321d5d1c3480d",
                "status": "modified"
            }
        ],
        "message": "HDFS-5708. The CacheManager throws a NPE in the DataNode logs when processing cache reports that refer to a block not known to the BlockManager. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1554594 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/97e881b955b91b07ac7b6fbc0718f0ecf009dc84",
        "patched_files": [
            "CacheManager.java",
            "CacheReplicationMonitor.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCacheDirectives.java"
        ]
    },
    "hadoop_08d5060": {
        "bug_id": "hadoop_08d5060",
        "commit": "https://github.com/apache/hadoop/commit/08d5060605af81a3d6048044176dc656c0dad56c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java?ref=08d5060605af81a3d6048044176dc656c0dad56c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java",
                "patch": "@@ -115,6 +115,11 @@ private void addTag(T type, String tag) {\n \n     private void removeTagFromInnerMap(Map<String, Long> innerMap, String tag) {\n       Long count = innerMap.get(tag);\n+      if (count == null) {\n+        LOG.warn(\"Trying to remove tags, however the tag \" + tag\n+            + \" no longer exists on this node/rack.\");\n+        return;\n+      }\n       if (count > 1) {\n         innerMap.put(tag, count - 1);\n       } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java",
                "sha": "6f160b6363a767cee0f52cee58ed868ea983cede",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop/blob/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java?ref=08d5060605af81a3d6048044176dc656c0dad56c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import com.google.common.collect.ImmutableSet;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n@@ -38,6 +39,7 @@\n import org.mockito.Mockito;\n \n import java.util.List;\n+import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n \n@@ -60,6 +62,41 @@ public void setup() {\n     rmContext = rm.getRMContext();\n   }\n \n+  @Test\n+  public void testMultipleAddRemoveContainer() {\n+    AllocationTagsManager atm = new AllocationTagsManager(rmContext);\n+\n+    NodeId nodeId = NodeId.fromString(\"host1:123\");\n+    ContainerId cid1 = TestUtils.getMockContainerId(1, 1);\n+    ContainerId cid2 = TestUtils.getMockContainerId(1, 2);\n+    ContainerId cid3 = TestUtils.getMockContainerId(1, 3);\n+    Set<String> tags1 = ImmutableSet.of(\"mapper\", \"reducer\");\n+    Set<String> tags2 = ImmutableSet.of(\"mapper\");\n+    Set<String> tags3 = ImmutableSet.of(\"zk\");\n+\n+    // node - mapper : 2\n+    //      - reduce : 1\n+    atm.addContainer(nodeId, cid1, tags1);\n+    atm.addContainer(nodeId, cid2, tags2);\n+    atm.addContainer(nodeId, cid3, tags3);\n+    Assert.assertEquals(2L,\n+        (long) atm.getAllocationTagsWithCount(nodeId).get(\"mapper\"));\n+    Assert.assertEquals(1L,\n+        (long) atm.getAllocationTagsWithCount(nodeId).get(\"reducer\"));\n+\n+    // remove container1\n+    atm.removeContainer(nodeId, cid1, tags1);\n+    Assert.assertEquals(1L,\n+        (long) atm.getAllocationTagsWithCount(nodeId).get(\"mapper\"));\n+    Assert.assertNull(atm.getAllocationTagsWithCount(nodeId).get(\"reducer\"));\n+\n+    // remove the same container again, the reducer no longer exists,\n+    // make sure there is no NPE here\n+    atm.removeContainer(nodeId, cid1, tags1);\n+    Assert.assertNull(atm.getAllocationTagsWithCount(nodeId).get(\"mapper\"));\n+    Assert.assertNull(atm.getAllocationTagsWithCount(nodeId).get(\"reducer\"));\n+  }\n+\n   @Test\n   public void testAllocationTagsManagerSimpleCases()\n       throws InvalidAllocationTagsQueryException {",
                "raw_url": "https://github.com/apache/hadoop/raw/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java",
                "sha": "9095ac1291c73385899b5ba4cd79d1dc9ccf3bed",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java?ref=08d5060605af81a3d6048044176dc656c0dad56c",
                "deletions": 25,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java",
                "patch": "@@ -163,6 +163,11 @@ private ContainerId newContainerId(ApplicationId appId) {\n         ApplicationAttemptId.newInstance(appId, 0), 0);\n   }\n \n+  private ContainerId newContainerId(ApplicationId appId, int containerId) {\n+    return ContainerId.newContainerId(\n+        ApplicationAttemptId.newInstance(appId, 0), containerId);\n+  }\n+\n   private SchedulerNode newSchedulerNode(String hostname, String rackName,\n       NodeId nodeId) {\n     SchedulerNode node = mock(SchedulerNode.class);\n@@ -271,12 +276,10 @@ public void testMultiTagsPlacementConstraints()\n     SchedulerNode schedulerNode3 =newSchedulerNode(n3_r2.getHostName(),\n         n3_r2.getRackName(), n3_r2.getNodeID());\n \n-    ContainerId ca = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId ca = newContainerId(appId1, 0);\n     tm.addContainer(n0_r1.getNodeID(), ca, ImmutableSet.of(\"A\"));\n \n-    ContainerId cb = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId cb = newContainerId(appId1, 1);\n     tm.addContainer(n1_r1.getNodeID(), cb, ImmutableSet.of(\"B\"));\n \n     // n0 and n1 has A/B so they cannot satisfy the PC\n@@ -297,11 +300,9 @@ public void testMultiTagsPlacementConstraints()\n      * n2: A(1), B(1)\n      * n3:\n      */\n-    ContainerId ca1 = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId ca1 = newContainerId(appId1, 2);\n     tm.addContainer(n2_r2.getNodeID(), ca1, ImmutableSet.of(\"A\"));\n-    ContainerId cb1 = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId cb1 = newContainerId(appId1, 3);\n     tm.addContainer(n2_r2.getNodeID(), cb1, ImmutableSet.of(\"B\"));\n \n     // Only n2 has both A and B so only it can satisfy the PC\n@@ -468,9 +469,9 @@ public void testORConstraintAssignment()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(appId1, 1), ImmutableSet.of(\"hbase-m\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-rs\"));\n+        newContainerId(appId1, 2), ImmutableSet.of(\"hbase-rs\"));\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n0r1.getNodeID())\n         .get(\"hbase-m\").longValue());\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n2r2.getNodeID())\n@@ -504,7 +505,7 @@ public void testORConstraintAssignment()\n      *  n3: hbase-rs(1)\n      */\n     tm.addContainer(n3r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-rs\"));\n+        newContainerId(appId1, 2), ImmutableSet.of(\"hbase-rs\"));\n     // n3 is qualified now because it is allocated with hbase-rs tag\n     Assert.assertTrue(PlacementConstraintsUtil.canSatisfyConstraints(appId1,\n         createSchedulingRequest(sourceTag1), schedulerNode3, pcm, tm));\n@@ -518,7 +519,7 @@ public void testORConstraintAssignment()\n      */\n     // Place\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"spark\"));\n+        newContainerId(appId1, 3), ImmutableSet.of(\"spark\"));\n     // According to constraint, \"zk\" is allowed to be placed on a node\n     // has \"hbase-m\" tag OR a node has both \"hbase-rs\" and \"spark\" tags.\n     Assert.assertTrue(PlacementConstraintsUtil.canSatisfyConstraints(appId1,\n@@ -552,9 +553,9 @@ public void testANDConstraintAssignment()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(appId1, 0), ImmutableSet.of(\"hbase-m\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(appId1, 1), ImmutableSet.of(\"hbase-m\"));\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n0r1.getNodeID())\n         .get(\"hbase-m\").longValue());\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n2r2.getNodeID())\n@@ -589,7 +590,7 @@ public void testANDConstraintAssignment()\n      */\n     for (int i=0; i<4; i++) {\n       tm.addContainer(n1r1.getNodeID(),\n-          newContainerId(appId1), ImmutableSet.of(\"spark\"));\n+          newContainerId(appId1, i+2), ImmutableSet.of(\"spark\"));\n     }\n     Assert.assertEquals(4L, tm.getAllocationTagsWithCount(n1r1.getNodeID())\n         .get(\"spark\").longValue());\n@@ -633,19 +634,19 @@ public void testGlobalAppConstraints()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"A\"));\n+        newContainerId(application1, 0), ImmutableSet.of(\"A\"));\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application2), ImmutableSet.of(\"A\"));\n+        newContainerId(application2, 1), ImmutableSet.of(\"A\"));\n     tm.addContainer(n1r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"A\"));\n+        newContainerId(application3, 2), ImmutableSet.of(\"A\"));\n     tm.addContainer(n1r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"A\"));\n+        newContainerId(application3, 3), ImmutableSet.of(\"A\"));\n     tm.addContainer(n1r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"A\"));\n+        newContainerId(application3, 4), ImmutableSet.of(\"A\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"A\"));\n+        newContainerId(application1, 5), ImmutableSet.of(\"A\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"A\"));\n+        newContainerId(application1, 6), ImmutableSet.of(\"A\"));\n \n     SchedulerNode schedulerNode0 = newSchedulerNode(n0r1.getHostName(),\n         n0r1.getRackName(), n0r1.getNodeID());\n@@ -888,9 +889,9 @@ public void testInterAppConstraintsByAppID()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(application1, 0), ImmutableSet.of(\"hbase-m\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(application1, 1), ImmutableSet.of(\"hbase-m\"));\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n0r1.getNodeID())\n         .get(\"hbase-m\").longValue());\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n2r2.getNodeID())\n@@ -958,7 +959,7 @@ public void testInterAppConstraintsByAppID()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(application3, 0), ImmutableSet.of(\"hbase-m\"));\n \n     // Anti-affinity to self/hbase-m\n     Assert.assertFalse(PlacementConstraintsUtil",
                "raw_url": "https://github.com/apache/hadoop/raw/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java",
                "sha": "5dbdc8a1b9468964a8d3474d467a4d20bffeca5d",
                "status": "modified"
            }
        ],
        "message": "YARN-8521. NPE in AllocationTagsManager when a container is removed more than once. Contributed by Weiwei Yang.",
        "parent": "https://github.com/apache/hadoop/commit/f5dbbfe2e97a8c11e3df0f95ae4a493f11fdbc28",
        "patched_files": [
            "AllocationTagsManager.java",
            "PlacementConstraintsUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAllocationTagsManager.java",
            "TestPlacementConstraintsUtil.java"
        ]
    },
    "hadoop_0a2252b": {
        "bug_id": "hadoop_0a2252b",
        "commit": "https://github.com/apache/hadoop/commit/0a2252bdda57afa5b953658a4f7ee80d7a4d69d4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0a2252bdda57afa5b953658a4f7ee80d7a4d69d4/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=0a2252bdda57afa5b953658a4f7ee80d7a4d69d4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -313,6 +313,9 @@ Branch-2 ( Unreleased changes )\n     HADOOP-8563. don't package hadoop-pipes examples/bin\n     (Colin Patrick McCabe via tgraves)\n \n+    HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no \n+    package (primitive types and arrays). (tucu)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HADOOP-8220. ZKFailoverController doesn't handle failure to become active",
                "raw_url": "https://github.com/apache/hadoop/raw/0a2252bdda57afa5b953658a4f7ee80d7a4d69d4/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "ece64bd2966b9bbcf1ae12f575865f439dc4aead",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0a2252bdda57afa5b953658a4f7ee80d7a4d69d4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java?ref=0a2252bdda57afa5b953658a4f7ee80d7a4d69d4",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java",
                "patch": "@@ -58,8 +58,8 @@ public synchronized boolean accept(Class<?> c) {\n     if (packages == null) {\n       getPackages();\n     }\n-    return AvroReflectSerializable.class.isAssignableFrom(c) || \n-      packages.contains(c.getPackage().getName());\n+    return AvroReflectSerializable.class.isAssignableFrom(c) ||\n+      (c.getPackage() != null && packages.contains(c.getPackage().getName()));\n   }\n \n   private void getPackages() {",
                "raw_url": "https://github.com/apache/hadoop/raw/0a2252bdda57afa5b953658a4f7ee80d7a4d69d4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java",
                "sha": "cfbc60d10452b20976c4d0a91728342d851c5c0c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/0a2252bdda57afa5b953658a4f7ee80d7a4d69d4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java?ref=0a2252bdda57afa5b953658a4f7ee80d7a4d69d4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java",
                "patch": "@@ -21,6 +21,7 @@\n import junit.framework.TestCase;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.serializer.SerializationFactory;\n import org.apache.hadoop.io.serializer.SerializationTestUtil;\n \n public class TestAvroSerialization extends TestCase {\n@@ -43,6 +44,12 @@ public void testReflectPkg() throws Exception {\n     assertEquals(before, after);\n   }\n \n+  public void testAcceptHandlingPrimitivesAndArrays() throws Exception {\n+    SerializationFactory factory = new SerializationFactory(conf);\n+    assertNull(factory.getSerializer(byte[].class));\n+    assertNull(factory.getSerializer(byte.class));\n+  }\n+\n   public void testReflectInnerClass() throws Exception {\n     InnerRecord before = new InnerRecord();\n     before.x = 10;",
                "raw_url": "https://github.com/apache/hadoop/raw/0a2252bdda57afa5b953658a4f7ee80d7a4d69d4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java",
                "sha": "181419c137ee185c20dc5106ee5421c68eb50264",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1358454 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/141127660c2723e67409f5ad96ecca50217d5a37",
        "patched_files": [
            "CHANGES.java",
            "AvroSerialization.java",
            "AvroReflectSerialization.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAvroSerialization.java"
        ]
    },
    "hadoop_0aad2d5": {
        "bug_id": "hadoop_0aad2d5",
        "commit": "https://github.com/apache/hadoop/commit/0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -183,6 +183,8 @@ Release 2.5.0 - UNRELEASED\n     YARN-2103. Inconsistency between viaProto flag and initial value of \n     SerializedExceptionProto.Builder (Binglin Chang via junping_du)\n \n+    YARN-1550. NPE in FairSchedulerAppsBlock#render. (Anubhav Dhoot via kasha)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6/hadoop-yarn-project/CHANGES.txt",
                "sha": "84563027cbec909539a2a57376d83158af716c40",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java?ref=0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "patch": "@@ -25,6 +25,8 @@\n \n import java.util.Collection;\n import java.util.HashSet;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.lang.StringEscapeUtils;\n@@ -35,6 +37,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerInfo;\n@@ -60,7 +63,14 @@\n     super(ctx);\n     FairScheduler scheduler = (FairScheduler) rm.getResourceScheduler();\n     fsinfo = new FairSchedulerInfo(scheduler);\n-    apps = rmContext.getRMApps();\n+    apps = new ConcurrentHashMap<ApplicationId, RMApp>();\n+    for (Map.Entry<ApplicationId, RMApp> entry : rmContext.getRMApps().entrySet()) {\n+      if (!(RMAppState.NEW.equals(entry.getValue().getState())\n+          || RMAppState.NEW_SAVING.equals(entry.getValue().getState())\n+          || RMAppState.SUBMITTED.equals(entry.getValue().getState()))) {\n+        apps.put(entry.getKey(), entry.getValue());\n+      }\n+    }\n     this.conf = conf;\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "sha": "b1aff9078ca9dea312d3ced677ebfcd3afe37508",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop/blob/0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "changes": 116,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java?ref=0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "patch": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager.webapp;\n+\n+import com.google.common.collect.Maps;\n+import com.google.inject.Binder;\n+import com.google.inject.Injector;\n+import com.google.inject.Module;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n+import org.apache.hadoop.yarn.webapp.test.WebAppTests;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentMap;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class TestRMWebAppFairScheduler {\n+\n+  @Test\n+  public void testFairSchedulerWebAppPage() {\n+    List<RMAppState> appStates = Arrays.asList(RMAppState.NEW,\n+        RMAppState.NEW_SAVING, RMAppState.SUBMITTED);\n+    final RMContext rmContext = mockRMContext(appStates);\n+    Injector injector = WebAppTests.createMockInjector(RMContext.class,\n+        rmContext,\n+        new Module() {\n+          @Override\n+          public void configure(Binder binder) {\n+            try {\n+              ResourceManager mockRmWithFairScheduler =\n+                  mockRm(rmContext);\n+              binder.bind(ResourceManager.class).toInstance\n+                  (mockRmWithFairScheduler);\n+\n+            } catch (IOException e) {\n+              throw new IllegalStateException(e);\n+            }\n+          }\n+        });\n+    FairSchedulerPage fsViewInstance = injector.getInstance(FairSchedulerPage\n+        .class);\n+    fsViewInstance.render();\n+    WebAppTests.flushOutput(injector);\n+  }\n+\n+  private static RMContext mockRMContext(List<RMAppState> states) {\n+    final ConcurrentMap<ApplicationId, RMApp> applicationsMaps = Maps\n+        .newConcurrentMap();\n+    int i = 0;\n+    for (RMAppState state : states) {\n+      MockRMApp app = new MockRMApp(i, i, state);\n+      applicationsMaps.put(app.getApplicationId(), app);\n+      i++;\n+    }\n+\n+    return new RMContextImpl(null, null, null, null,\n+        null, null, null, null, null, null) {\n+      @Override\n+      public ConcurrentMap<ApplicationId, RMApp> getRMApps() {\n+        return applicationsMaps;\n+      }\n+    };\n+  }\n+\n+  private static ResourceManager mockRm(RMContext rmContext) throws\n+      IOException {\n+    ResourceManager rm = mock(ResourceManager.class);\n+    ResourceScheduler rs = mockFairScheduler();\n+    when(rm.getResourceScheduler()).thenReturn(rs);\n+    when(rm.getRMContext()).thenReturn(rmContext);\n+    return rm;\n+  }\n+\n+  private static FairScheduler mockFairScheduler() throws IOException {\n+    FairScheduler fs = new FairScheduler();\n+    FairSchedulerConfiguration conf = new FairSchedulerConfiguration();\n+    fs.setRMContext(new RMContextImpl(null, null, null, null, null,\n+        null, new RMContainerTokenSecretManager(conf),\n+        new NMTokenSecretManagerInRM(conf),\n+        new ClientToAMTokenSecretManagerInRM(), null));\n+    fs.init(conf);\n+    return fs;\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/0aad2d56dfb5c12b38e26e34ea5ceeec7c5cd6b6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "sha": "1de64896c55c3d684d568814af455366605c6032",
                "status": "added"
            }
        ],
        "message": "YARN-1550. NPE in FairSchedulerAppsBlock#render. (Anubhav Dhoot via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1599345 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/790cbbf3e22dc634b722562d68ae3f11c7d8327f",
        "patched_files": [
            "FairSchedulerAppsBlock.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMWebAppFairScheduler.java"
        ]
    },
    "hadoop_0ab3f9d": {
        "bug_id": "hadoop_0ab3f9d",
        "commit": "https://github.com/apache/hadoop/commit/0ab3f9d56465bf31668159c562305a3b8222004c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -1240,6 +1240,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-11628. SPNEGO auth does not work with CNAMEs in JDK8.\n     (Daryn Sharp via stevel).\n \n+    HADOOP-10941. Proxy user verification NPEs if remote host is unresolvable.\n+    (Benoy Antony via stevel).\n+\n   OPTIMIZATIONS\n \n     HADOOP-12051. ProtobufRpcEngine.invoke() should use Exception.toString()",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2c9e86a5f0ea63a8114c1b83c547b9835511906e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "patch": "@@ -108,6 +108,10 @@ public Configuration getConf() {\n   public void authorize(UserGroupInformation user, \n       String remoteAddress) throws AuthorizationException {\n     \n+    if (user == null) {\n+      throw new IllegalArgumentException(\"user is null.\");\n+    }\n+\n     UserGroupInformation realUser = user.getRealUser();\n     if (realUser == null) {\n       return;",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "sha": "26cd7ab2614a61100223fc6b3cb45879695d3ec6",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.hadoop.util;\n \n import java.net.InetAddress;\n-\n import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.Collection;\n@@ -141,6 +140,10 @@ public boolean includes(String ipAddress) {\n       return true;\n     }\n     \n+    if (ipAddress == null) {\n+      throw new IllegalArgumentException(\"ipAddress is null.\");\n+    }\n+\n     //check in the set of ipAddresses\n     if ((ipAddresses != null) && ipAddresses.contains(ipAddress)) {\n       return true;",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java",
                "sha": "2e6c079d0f2470699ab6cbe282a98e47e8c27697",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "patch": "@@ -334,6 +334,45 @@ public void testIPRange() {\n     assertNotAuthorized(proxyUserUgi, \"10.221.0.0\");\n   }\n \n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNullUser() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserGroupConfKey(REAL_USER_NAME),\n+        \"*\");\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserIpConfKey(REAL_USER_NAME),\n+        PROXY_IP_RANGE);\n+    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);\n+    // user is null\n+    ProxyUsers.authorize(null, \"10.222.0.0\");\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNullIpAddress() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserGroupConfKey(REAL_USER_NAME),\n+        \"*\");\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserIpConfKey(REAL_USER_NAME),\n+        PROXY_IP_RANGE);\n+    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);\n+\n+    // First try proxying a group that's allowed\n+    UserGroupInformation realUserUgi = UserGroupInformation\n+        .createRemoteUser(REAL_USER_NAME);\n+    UserGroupInformation proxyUserUgi = UserGroupInformation.createProxyUserForTesting(\n+        PROXY_USER_NAME, realUserUgi, GROUP_NAMES);\n+\n+    // remote address is null\n+    ProxyUsers.authorize(proxyUserUgi, null);\n+  }\n+\n   @Test\n   public void testWithDuplicateProxyGroups() throws Exception {\n     Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "sha": "577f11b92961a2b70b059b84badd83dd1f1dfa54",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java",
                "patch": "@@ -176,7 +176,15 @@ public void testCIDRs() {\n \n     //test for exclusion with an unknown IP\n     assertFalse(ml.includes(\"10.119.103.111\"));\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNullIpAddress() {\n+    //create MachineList with a list of of ip ranges specified in CIDR format\n+    MachineList ml = new MachineList(CIDR_LIST);\n \n+    //test for exclusion with a null IP\n+    assertFalse(ml.includes(null));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java",
                "sha": "d721c29530f1769d8f0a0c75982f6a6f07873a01",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10941. Proxy user verification NPEs if remote host is unresolvable (Benoy Antony via stevel).",
        "parent": "https://github.com/apache/hadoop/commit/e286512a7143427f2975ec92cdc4fad0a093a456",
        "patched_files": [
            "CHANGES.java",
            "MachineList.java",
            "DefaultImpersonationProvider.java",
            "ProxyUsers.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestProxyUsers.java",
            "TestMachineList.java"
        ]
    },
    "hadoop_0ad6cdd": {
        "bug_id": "hadoop_0ad6cdd",
        "commit": "https://github.com/apache/hadoop/commit/0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -69,3 +69,5 @@ HDFS-5535 subtasks:\n     HDFS-5987. Fix findbugs warnings in Rolling Upgrade branch. (seztszwo via\n     Arpit Agarwal)\n \n+    HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for\n+    TestOfflineEditsViewer.  (szetszwo)",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "4c567902c863adaa5719b632106cb992a0247b66",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.InputStreamReader;\n@@ -72,10 +73,6 @@ public static void verifySavedMD5(File dataFile, MD5Hash expectedMD5)\n    *   where group(1) is the md5 string and group(2) is the data file path.\n    */\n   private static Matcher readStoredMd5(File md5File) throws IOException {\n-    if (!md5File.exists()) {\n-      return null;\n-    }\n-    \n     BufferedReader reader =\n         new BufferedReader(new InputStreamReader(new FileInputStream(\n             md5File), Charsets.UTF_8));\n@@ -105,6 +102,10 @@ private static Matcher readStoredMd5(File md5File) throws IOException {\n    */\n   public static MD5Hash readStoredMd5ForFile(File dataFile) throws IOException {\n     final File md5File = getDigestFileForFile(dataFile);\n+    if (!md5File.exists()) {\n+      return null;\n+    }\n+\n     final Matcher matcher = readStoredMd5(md5File);\n     String storedHash = matcher.group(1);\n     File referencedFile = new File(matcher.group(2));\n@@ -165,6 +166,10 @@ private static void saveMD5File(File dataFile, String digestString)\n   public static void renameMD5File(File oldDataFile, File newDataFile)\n       throws IOException {\n     final File fromFile = getDigestFileForFile(oldDataFile);\n+    if (!fromFile.exists()) {\n+      throw new FileNotFoundException(fromFile + \" does not exist.\");\n+    }\n+\n     final String digestString = readStoredMd5(fromFile).group(1);\n     saveMD5File(newDataFile, digestString);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "sha": "d87ffbf3154236c316020cdb0520469379d87d12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "patch": "@@ -130,7 +130,10 @@ private CheckpointSignature runOperations() throws IOException {\n     DFSTestUtil.runOperations(cluster, dfs, cluster.getConfiguration(0),\n         dfs.getDefaultBlockSize(), 0);\n \n+    // OP_ROLLING_UPGRADE_START\n     cluster.getNamesystem().getEditLog().logStartRollingUpgrade(Time.now());\n+    // OP_ROLLING_UPGRADE_FINALIZE\n+    cluster.getNamesystem().getEditLog().logFinalizeRollingUpgrade(Time.now());\n \n     // Force a roll so we get an OP_END_LOG_SEGMENT txn\n     return cluster.getNameNodeRpc().rollEditLog();",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "sha": "c6364b174d0b0763c271e09fff004d0b2abdcab0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "patch": "@@ -95,6 +95,7 @@ public void testGenerated() throws IOException {\n     // edits generated by nnHelper (MiniDFSCluster), should have all op codes\n     // binary, XML, reparsed binary\n     String edits = nnHelper.generateEdits();\n+    LOG.info(\"Generated edits=\" + edits);\n     String editsParsedXml = folder.newFile(\"editsParsed.xml\").getAbsolutePath();\n     String editsReparsed = folder.newFile(\"editsParsed\").getAbsolutePath();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "sha": "19d98ab6988a92753296f65452c519f0fb878653",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "sha": "6c17f4af4a9ec863a56a6b27dec867a38eb6210e",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "changes": 578,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 476,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "patch": "@@ -1,10 +1,6 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <EDITS>\n-<<<<<<< .working\n-  <EDITS_VERSION>-50</EDITS_VERSION>\n-=======\n-  <EDITS_VERSION>-51</EDITS_VERSION>\n->>>>>>> .merge-right.r1559304\n+  <EDITS_VERSION>-55</EDITS_VERSION>\n   <RECORD>\n     <OPCODE>OP_START_LOG_SEGMENT</OPCODE>\n     <DATA>\n@@ -17,13 +13,8 @@\n       <TXID>2</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>1</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314720</EXPIRY_DATE>\n-        <KEY>d2a03d66ebfac521</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460949</EXPIRY_DATE>\n-        <KEY>dc8d30edc97df67d</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283650</EXPIRY_DATE>\n+        <KEY>76e6d2854a753680</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -33,13 +24,8 @@\n       <TXID>3</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>2</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314722</EXPIRY_DATE>\n-        <KEY>ef94532092f55aef</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460952</EXPIRY_DATE>\n-        <KEY>096bc20b6debed03</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283653</EXPIRY_DATE>\n+        <KEY>939fb7b875c956cd</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -51,36 +37,18 @@\n       <INODEID>16386</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115316</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828264873</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084379</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>6</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>7</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -91,22 +59,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115327</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828265699</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084397</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -119,15 +78,9 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115331</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>8</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265705</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>11</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084400</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>9</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -136,15 +89,9 @@\n       <TXID>7</TXID>\n       <LENGTH>0</LENGTH>\n       <PATH>/file_moved</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115336</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265712</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>12</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084413</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>10</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -154,17 +101,9 @@\n       <LENGTH>0</LENGTH>\n       <INODEID>16387</INODEID>\n       <PATH>/directory_mkdir</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115342</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265722</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084419</TIMESTAMP>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>493</MODE>\n       </PERMISSION_STATUS>\n@@ -197,13 +136,8 @@\n       <TXID>12</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot1</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>14</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>15</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -213,13 +147,8 @@\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTOLDNAME>snapshot1</SNAPSHOTOLDNAME>\n       <SNAPSHOTNEWNAME>snapshot2</SNAPSHOTNEWNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>15</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>18</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>16</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -228,13 +157,8 @@\n       <TXID>14</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot2</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>16</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>19</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>17</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -245,36 +169,18 @@\n       <INODEID>16388</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115362</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265757</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084440</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>20</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>18</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -285,22 +191,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115363</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265759</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084441</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -356,19 +253,10 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115378</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265782</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084455</TIMESTAMP>\n       <OPTIONS>NONE</OPTIONS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>24</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>27</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>25</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -379,36 +267,18 @@\n       <INODEID>16389</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115382</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828265787</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084459</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>26</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>29</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>27</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -513,13 +383,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115461</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828266540</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084525</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -539,11 +404,7 @@\n         <GENSTAMP>1003</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -557,36 +418,18 @@\n       <INODEID>16390</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115463</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266544</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084527</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>39</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>41</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>40</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -691,13 +534,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115477</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266569</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084542</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -717,11 +555,7 @@\n         <GENSTAMP>1006</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -735,36 +569,18 @@\n       <INODEID>16391</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115479</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266572</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084544</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>51</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>53</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>52</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -869,13 +685,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115495</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266599</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084559</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -895,11 +706,7 @@\n         <GENSTAMP>1009</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -911,22 +718,13 @@\n       <TXID>56</TXID>\n       <LENGTH>0</LENGTH>\n       <TRG>/file_concat_target</TRG>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115498</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828266603</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084561</TIMESTAMP>\n       <SOURCES>\n         <SOURCE1>/file_concat_0</SOURCE1>\n         <SOURCE2>/file_concat_1</SOURCE2>\n       </SOURCES>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>62</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>64</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>63</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -937,190 +735,37 @@\n       <INODEID>16392</INODEID>\n       <PATH>/file_symlink</PATH>\n       <VALUE>/file_concat_target</VALUE>\n-<<<<<<< .working\n-      <MTIME>1388730115502</MTIME>\n-      <ATIME>1388730115502</ATIME>\n-=======\n-      <MTIME>1389828266633</MTIME>\n-      <ATIME>1389828266633</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084564</MTIME>\n+      <ATIME>1392957084564</ATIME>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>511</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>63</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>66</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>64</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_ADD</OPCODE>\n     <DATA>\n       <TXID>58</TXID>\n-<<<<<<< .working\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515505</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_RENEW_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>59</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515564</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_CANCEL_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>60</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>61</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>szetszwo</OWNERNAME>\n-      <GROUPNAME>staff</GROUPNAME>\n-      <MODE>493</MODE>\n-      <LIMIT>9223372036854775807</LIMIT>\n-      <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>62</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>carlton</OWNERNAME>\n-      <GROUPNAME>party</GROUPNAME>\n-      <MODE>448</MODE>\n-      <LIMIT>1989</LIMIT>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>68</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>63</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar</PATH>\n-      <REPLICATION>1</REPLICATION>\n-      <POOL>poolparty</POOL>\n-      <EXPIRATION>2305844397943809533</EXPIRATION>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>69</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>64</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar2</PATH>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>70</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>65</TXID>\n-      <ID>1</ID>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>71</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>66</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>72</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD</OPCODE>\n-    <DATA>\n-      <TXID>67</TXID>\n-=======\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>16393</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115596</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828266637</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084567</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>73</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>65</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1175,38 +820,22 @@\n   <RECORD>\n     <OPCODE>OP_REASSIGN_LEASE</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>73</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_381408282_1</LEASEHOLDER>\n-=======\n       <TXID>64</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_16108824_1</LEASEHOLDER>\n->>>>>>> .merge-right.r1559304\n+      <LEASEHOLDER>DFSClient_NONMAPREDUCE_-1178237747_1</LEASEHOLDER>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <NEWHOLDER>HDFS_NameNode</NEWHOLDER>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_CLOSE</OPCODE>\n-    <DATA>\n-      <TXID>74</TXID>\n-=======\n     <OPCODE>OP_CLOSE</OPCODE>\n     <DATA>\n       <TXID>65</TXID>\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>0</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730118281</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828269751</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957087263</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -1216,36 +845,24 @@\n         <GENSTAMP>1011</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_UPGRADE_MARKER</OPCODE>\n-    <DATA>\n-      <TXID>75</TXID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-=======\n     <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>66</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <OWNERNAME>jing</OWNERNAME>\n+      <OWNERNAME>szetszwo</OWNERNAME>\n       <GROUPNAME>staff</GROUPNAME>\n       <MODE>493</MODE>\n       <LIMIT>9223372036854775807</LIMIT>\n       <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>74</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>72</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1254,8 +871,8 @@\n       <TXID>67</TXID>\n       <POOLNAME>pool1</POOLNAME>\n       <LIMIT>99</LIMIT>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>75</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>73</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1266,9 +883,9 @@\n       <PATH>/path</PATH>\n       <REPLICATION>1</REPLICATION>\n       <POOL>pool1</POOL>\n-      <EXPIRATION>2305844399041964876</EXPIRATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>76</RPC_CALLID>\n+      <EXPIRATION>2305844402170781554</EXPIRATION>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>74</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1277,44 +894,53 @@\n       <TXID>69</TXID>\n       <ID>1</ID>\n       <REPLICATION>2</REPLICATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>77</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>75</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n     <DATA>\n       <TXID>70</TXID>\n       <ID>1</ID>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>78</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>76</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>71</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>79</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>77</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n->>>>>>> .merge-right.r1559304\n-    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <OPCODE>OP_SET_ACL</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>76</TXID>\n-=======\n       <TXID>72</TXID>\n->>>>>>> .merge-right.r1559304\n+      <SRC>/file_concat_target</SRC>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-    <OPCODE>OP_SET_ACL</OPCODE>\n+    <OPCODE>OP_ROLLING_UPGRADE_START</OPCODE>\n     <DATA>\n       <TXID>73</TXID>\n-      <SRC>/file_set_acl</SRC>\n+      <STARTTIME>1392957087621</STARTTIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_ROLLING_UPGRADE_FINALIZE</OPCODE>\n+    <DATA>\n+      <TXID>74</TXID>\n+      <FINALIZETIME>1392957087621</FINALIZETIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <DATA>\n+      <TXID>75</TXID>\n     </DATA>\n   </RECORD>\n </EDITS>",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "sha": "b3115591d0327d924a4eb098147c8da374327d5b",
                "status": "modified"
            }
        ],
        "message": "HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for TestOfflineEditsViewer.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1570690 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/37afb4d6834f04c511a977d6b4205930293aa4a0",
        "patched_files": [
            "MD5FileUtils.java",
            "OfflineEditsViewer.java",
            "CHANGES_HDFS-5535.java",
            "OfflineEditsViewerHelper.java",
            "editsStored.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOfflineEditsViewer.java",
            "TestMD5FileUtils.java"
        ]
    },
    "hadoop_0c2887b": {
        "bug_id": "hadoop_0c2887b",
        "commit": "https://github.com/apache/hadoop/commit/0c2887b6172bda7fbff27705ec536715c8e9e2b8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0c2887b6172bda7fbff27705ec536715c8e9e2b8/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=0c2887b6172bda7fbff27705ec536715c8e9e2b8",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -818,6 +818,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4053. Counters group names deprecation is wrong, iterating over\n     group names deprecated names don't show up  (Robert Evans via tgraves)\n \n+    MAPREDUCE-3506. Calling getPriority on JobInfo after parsing a history log\n+    with JobHistoryParser throws a NullPointerException (Jason Lowe via bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/0c2887b6172bda7fbff27705ec536715c8e9e2b8/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "a070488c9e4eb770e8a0f5d054c795dcbcefca8e",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/0c2887b6172bda7fbff27705ec536715c8e9e2b8/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java?ref=0c2887b6172bda7fbff27705ec536715c8e9e2b8",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java",
                "patch": "@@ -441,6 +441,7 @@ public JobInfo() {\n       username = jobname = jobConfPath = jobQueueName = \"\";\n       tasksMap = new HashMap<TaskID, TaskInfo>();\n       jobACLs = new HashMap<JobACL, AccessControlList>();\n+      priority = JobPriority.NORMAL;\n     }\n     \n     /** Print all the job information */\n@@ -454,12 +455,20 @@ public void printAll() {\n       System.out.println(\"PRIORITY: \" + priority);\n       System.out.println(\"TOTAL_MAPS: \" + totalMaps);\n       System.out.println(\"TOTAL_REDUCES: \" + totalReduces);\n-      System.out.println(\"MAP_COUNTERS:\" + mapCounters.toString());\n-      System.out.println(\"REDUCE_COUNTERS:\" + reduceCounters.toString());\n-      System.out.println(\"TOTAL_COUNTERS: \" + totalCounters.toString());\n+      if (mapCounters != null) {\n+        System.out.println(\"MAP_COUNTERS:\" + mapCounters.toString());\n+      }\n+      if (reduceCounters != null) {\n+        System.out.println(\"REDUCE_COUNTERS:\" + reduceCounters.toString());\n+      }\n+      if (totalCounters != null) {\n+        System.out.println(\"TOTAL_COUNTERS: \" + totalCounters.toString());\n+      }\n       System.out.println(\"UBERIZED: \" + uberized);\n-      for (AMInfo amInfo : amInfos) {\n-        amInfo.printAll();\n+      if (amInfos != null) {\n+        for (AMInfo amInfo : amInfos) {\n+          amInfo.printAll();\n+        }\n       }\n       for (TaskInfo ti: tasksMap.values()) {\n         ti.printAll();",
                "raw_url": "https://github.com/apache/hadoop/raw/0c2887b6172bda7fbff27705ec536715c8e9e2b8/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryParser.java",
                "sha": "48c004b23b74db5767d699642e72eafa1016cb03",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/0c2887b6172bda7fbff27705ec536715c8e9e2b8/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java?ref=0c2887b6172bda7fbff27705ec536715c8e9e2b8",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "patch": "@@ -83,6 +83,13 @@\n     }\n   }\n \n+  @Test\n+  public void testJobInfo() throws Exception {\n+    JobInfo info = new JobInfo();\n+    Assert.assertEquals(\"NORMAL\", info.getPriority());\n+    info.printAll();\n+  }\n+\n   @Test\n   public void testHistoryParsing() throws Exception {\n     LOG.info(\"STARTING testHistoryParsing()\");",
                "raw_url": "https://github.com/apache/hadoop/raw/0c2887b6172bda7fbff27705ec536715c8e9e2b8/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "sha": "b596a2123a7e19b99395d72781dcb7c789ca2f2c",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3506. Calling getPriority on JobInfo after parsing a history log with JobHistoryParser throws a NullPointerException (Jason Lowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1375602 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/f2dd818201402d0ca8a7049ba7abf77188443a64",
        "patched_files": [
            "JobHistoryParser.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJobHistoryParsing.java"
        ]
    },
    "hadoop_0c58890": {
        "bug_id": "hadoop_0c58890",
        "commit": "https://github.com/apache/hadoop/commit/0c588904f8b68cad219d2bd8e33081d5cae656e5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=0c588904f8b68cad219d2bd8e33081d5cae656e5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -258,6 +258,9 @@ Release 2.7.0 - UNRELEASED\n     MAPREDUCE-6172. TestDbClasses timeouts are too aggressive (Varun Saxena\n     via jlowe)\n \n+    MAPREDUCE-6160. Potential NullPointerException in MRClientProtocol\n+    interface implementation. (Rohith via jlowe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "5417c3ee09b81ff0f1a242fad30badc68f2df45f",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java?ref=0c588904f8b68cad219d2bd8e33081d5cae656e5",
                "deletions": 10,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java",
                "patch": "@@ -184,11 +184,14 @@ public InetSocketAddress getConnectAddress() {\n       return getBindAddress();\n     }\n     \n-    private Job verifyAndGetJob(JobId jobID,\n-        JobACL accessType) throws IOException {\n+    private Job verifyAndGetJob(JobId jobID, JobACL accessType,\n+        boolean exceptionThrow) throws IOException {\n       Job job = appContext.getJob(jobID);\n+      if (job == null && exceptionThrow) {\n+        throw new IOException(\"Unknown Job \" + jobID);\n+      }\n       UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n-      if (!job.checkAccess(ugi, accessType)) {\n+      if (job != null && !job.checkAccess(ugi, accessType)) {\n         throw new AccessControlException(\"User \" + ugi.getShortUserName()\n             + \" cannot perform operation \" + accessType.name() + \" on \"\n             + jobID);\n@@ -198,8 +201,8 @@ private Job verifyAndGetJob(JobId jobID,\n  \n     private Task verifyAndGetTask(TaskId taskID, \n         JobACL accessType) throws IOException {\n-      Task task = verifyAndGetJob(taskID.getJobId(), \n-          accessType).getTask(taskID);\n+      Task task =\n+          verifyAndGetJob(taskID.getJobId(), accessType, true).getTask(taskID);\n       if (task == null) {\n         throw new IOException(\"Unknown Task \" + taskID);\n       }\n@@ -220,7 +223,7 @@ private TaskAttempt verifyAndGetAttempt(TaskAttemptId attemptID,\n     public GetCountersResponse getCounters(GetCountersRequest request) \n       throws IOException {\n       JobId jobId = request.getJobId();\n-      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB);\n+      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB, true);\n       GetCountersResponse response =\n         recordFactory.newRecordInstance(GetCountersResponse.class);\n       response.setCounters(TypeConverter.toYarn(job.getAllCounters()));\n@@ -231,7 +234,8 @@ public GetCountersResponse getCounters(GetCountersRequest request)\n     public GetJobReportResponse getJobReport(GetJobReportRequest request) \n       throws IOException {\n       JobId jobId = request.getJobId();\n-      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB);\n+      // false is for retain compatibility\n+      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB, false);\n       GetJobReportResponse response = \n         recordFactory.newRecordInstance(GetJobReportResponse.class);\n       if (job != null) {\n@@ -272,7 +276,7 @@ public GetTaskAttemptCompletionEventsResponse getTaskAttemptCompletionEvents(\n       JobId jobId = request.getJobId();\n       int fromEventId = request.getFromEventId();\n       int maxEvents = request.getMaxEvents();\n-      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB);\n+      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB, true);\n       \n       GetTaskAttemptCompletionEventsResponse response = \n         recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsResponse.class);\n@@ -290,7 +294,7 @@ public KillJobResponse killJob(KillJobRequest request)\n       String message = \"Kill job \" + jobId + \" received from \" + callerUGI\n           + \" at \" + Server.getRemoteAddress();\n       LOG.info(message);\n-      verifyAndGetJob(jobId, JobACL.MODIFY_JOB);\n+      verifyAndGetJob(jobId, JobACL.MODIFY_JOB, false);\n       appContext.getEventHandler().handle(\n           new JobDiagnosticsUpdateEvent(jobId, message));\n       appContext.getEventHandler().handle(\n@@ -382,7 +386,7 @@ public GetTaskReportsResponse getTaskReports(\n       GetTaskReportsResponse response = \n         recordFactory.newRecordInstance(GetTaskReportsResponse.class);\n       \n-      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB);\n+      Job job = verifyAndGetJob(jobId, JobACL.VIEW_JOB, true);\n       Collection<Task> tasks = job.getTasks(taskType).values();\n       LOG.info(\"Getting task report for \" + taskType + \"   \" + jobId\n           + \". Report-size will be \" + tasks.size());",
                "raw_url": "https://github.com/apache/hadoop/raw/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java",
                "sha": "b52afd85628e89a3c239e3a6c6760e67daa43450",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRClientService.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRClientService.java?ref=0c588904f8b68cad219d2bd8e33081d5cae656e5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRClientService.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import static org.junit.Assert.fail;\n \n+import java.io.IOException;\n import java.security.PrivilegedExceptionAction;\n import java.util.Iterator;\n import java.util.List;\n@@ -28,8 +29,10 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.JobACL;\n+import org.apache.hadoop.mapreduce.JobID;\n import org.apache.hadoop.mapreduce.MRConfig;\n import org.apache.hadoop.mapreduce.MRJobConfig;\n+import org.apache.hadoop.mapreduce.TypeConverter;\n import org.apache.hadoop.mapreduce.v2.api.MRClientProtocol;\n import org.apache.hadoop.mapreduce.v2.api.protocolrecords.FailTaskAttemptRequest;\n import org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetCountersRequest;\n@@ -179,6 +182,19 @@ public void test() throws Exception {\n             TaskAttemptEventType.TA_DONE));\n \n     app.waitForState(job, JobState.SUCCEEDED);\n+\n+    // For invalid jobid, throw IOException\n+    gtreportsRequest =\n+        recordFactory.newRecordInstance(GetTaskReportsRequest.class);\n+    gtreportsRequest.setJobId(TypeConverter.toYarn(JobID\n+        .forName(\"job_1415730144495_0001\")));\n+    gtreportsRequest.setTaskType(TaskType.REDUCE);\n+    try {\n+      proxy.getTaskReports(gtreportsRequest);\n+      fail(\"IOException not thrown for invalid job id\");\n+    } catch (IOException e) {\n+      // Expected\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRClientService.java",
                "sha": "77f9a092a1b2410139c2256fb067d649dbfb0765",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java?ref=0c588904f8b68cad219d2bd8e33081d5cae656e5",
                "deletions": 8,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java",
                "patch": "@@ -196,7 +196,8 @@ public InetSocketAddress getConnectAddress() {\n       return getBindAddress();\n     }\n     \n-    private Job verifyAndGetJob(final JobId jobID) throws IOException {\n+    private Job verifyAndGetJob(final JobId jobID, boolean exceptionThrow)\n+        throws IOException {\n       UserGroupInformation loginUgi = null;\n       Job job = null;\n       try {\n@@ -212,6 +213,11 @@ public Job run() throws Exception {\n       } catch (InterruptedException e) {\n         throw new IOException(e);\n       }\n+\n+      if (job == null && exceptionThrow) {\n+        throw new IOException(\"Unknown Job \" + jobID);\n+      }\n+\n       if (job != null) {\n         JobACL operation = JobACL.VIEW_JOB;\n         checkAccess(job, operation);\n@@ -223,7 +229,7 @@ public Job run() throws Exception {\n     public GetCountersResponse getCounters(GetCountersRequest request)\n         throws IOException {\n       JobId jobId = request.getJobId();\n-      Job job = verifyAndGetJob(jobId);\n+      Job job = verifyAndGetJob(jobId, true);\n       GetCountersResponse response = recordFactory.newRecordInstance(GetCountersResponse.class);\n       response.setCounters(TypeConverter.toYarn(job.getAllCounters()));\n       return response;\n@@ -233,7 +239,7 @@ public GetCountersResponse getCounters(GetCountersRequest request)\n     public GetJobReportResponse getJobReport(GetJobReportRequest request)\n         throws IOException {\n       JobId jobId = request.getJobId();\n-      Job job = verifyAndGetJob(jobId);\n+      Job job = verifyAndGetJob(jobId, false);\n       GetJobReportResponse response = recordFactory.newRecordInstance(GetJobReportResponse.class);\n       if (job != null) {\n         response.setJobReport(job.getReport());\n@@ -248,7 +254,7 @@ public GetJobReportResponse getJobReport(GetJobReportRequest request)\n     public GetTaskAttemptReportResponse getTaskAttemptReport(\n         GetTaskAttemptReportRequest request) throws IOException {\n       TaskAttemptId taskAttemptId = request.getTaskAttemptId();\n-      Job job = verifyAndGetJob(taskAttemptId.getTaskId().getJobId());\n+      Job job = verifyAndGetJob(taskAttemptId.getTaskId().getJobId(), true);\n       GetTaskAttemptReportResponse response = recordFactory.newRecordInstance(GetTaskAttemptReportResponse.class);\n       response.setTaskAttemptReport(job.getTask(taskAttemptId.getTaskId()).getAttempt(taskAttemptId).getReport());\n       return response;\n@@ -258,7 +264,7 @@ public GetTaskAttemptReportResponse getTaskAttemptReport(\n     public GetTaskReportResponse getTaskReport(GetTaskReportRequest request)\n         throws IOException {\n       TaskId taskId = request.getTaskId();\n-      Job job = verifyAndGetJob(taskId.getJobId());\n+      Job job = verifyAndGetJob(taskId.getJobId(), true);\n       GetTaskReportResponse response = recordFactory.newRecordInstance(GetTaskReportResponse.class);\n       response.setTaskReport(job.getTask(taskId).getReport());\n       return response;\n@@ -272,7 +278,7 @@ public GetTaskReportResponse getTaskReport(GetTaskReportRequest request)\n       int fromEventId = request.getFromEventId();\n       int maxEvents = request.getMaxEvents();\n \n-      Job job = verifyAndGetJob(jobId);\n+      Job job = verifyAndGetJob(jobId, true);\n       GetTaskAttemptCompletionEventsResponse response = recordFactory.newRecordInstance(GetTaskAttemptCompletionEventsResponse.class);\n       response.addAllCompletionEvents(Arrays.asList(job.getTaskAttemptCompletionEvents(fromEventId, maxEvents)));\n       return response;\n@@ -300,7 +306,7 @@ public GetDiagnosticsResponse getDiagnostics(GetDiagnosticsRequest request)\n         throws IOException {\n       TaskAttemptId taskAttemptId = request.getTaskAttemptId();\n \n-      Job job = verifyAndGetJob(taskAttemptId.getTaskId().getJobId());\n+      Job job = verifyAndGetJob(taskAttemptId.getTaskId().getJobId(), true);\n \n       GetDiagnosticsResponse response = recordFactory.newRecordInstance(GetDiagnosticsResponse.class);\n       response.addAllDiagnostics(job.getTask(taskAttemptId.getTaskId()).getAttempt(taskAttemptId).getDiagnostics());\n@@ -320,7 +326,7 @@ public GetTaskReportsResponse getTaskReports(GetTaskReportsRequest request)\n       TaskType taskType = request.getTaskType();\n \n       GetTaskReportsResponse response = recordFactory.newRecordInstance(GetTaskReportsResponse.class);\n-      Job job = verifyAndGetJob(jobId);\n+      Job job = verifyAndGetJob(jobId, true);\n       Collection<Task> tasks = job.getTasks(taskType).values();\n       for (Task task : tasks) {\n         response.addTaskReport(task.getReport());",
                "raw_url": "https://github.com/apache/hadoop/raw/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java",
                "sha": "3751ad9296bf67b4930ffa160341b83bee250754",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java?ref=0c588904f8b68cad219d2bd8e33081d5cae656e5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java",
                "patch": "@@ -19,11 +19,14 @@\n package org.apache.hadoop.mapreduce.v2.hs;\n \n \n+import java.io.IOException;\n import java.util.Map;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.mapreduce.JobID;\n import org.apache.hadoop.mapreduce.TaskCounter;\n+import org.apache.hadoop.mapreduce.TypeConverter;\n import org.apache.hadoop.mapreduce.v2.api.MRClientProtocol;\n import org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetDiagnosticsRequest;\n import org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetDiagnosticsResponse;\n@@ -33,11 +36,13 @@\n import org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskAttemptReportResponse;\n import org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportRequest;\n import org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportResponse;\n+import org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportsRequest;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskId;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskState;\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskType;\n import org.apache.hadoop.mapreduce.v2.app.MRApp;\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n import org.apache.hadoop.mapreduce.v2.app.job.Task;\n@@ -159,6 +164,20 @@ public void testReports() throws Exception {\n     // Task state should be SUCCEEDED\n     assertEquals(TaskState.SUCCEEDED, reportResponse.getTaskReport()\n             .getTaskState());\n+\n+    // For invalid jobid, throw IOException\n+    GetTaskReportsRequest gtreportsRequest =\n+        recordFactory.newRecordInstance(GetTaskReportsRequest.class);\n+    gtreportsRequest.setJobId(TypeConverter.toYarn(JobID\n+        .forName(\"job_1415730144495_0001\")));\n+    gtreportsRequest.setTaskType(TaskType.REDUCE);\n+    try {\n+      protocol.getTaskReports(gtreportsRequest);\n+      fail(\"IOException not thrown for invalid job id\");\n+    } catch (IOException e) {\n+      // Expected\n+    }\n+\n     // test getTaskAttemptCompletionEvents\n     GetTaskAttemptCompletionEventsRequest taskAttemptRequest = recordFactory\n             .newRecordInstance(GetTaskAttemptCompletionEventsRequest.class);",
                "raw_url": "https://github.com/apache/hadoop/raw/0c588904f8b68cad219d2bd8e33081d5cae656e5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryServer.java",
                "sha": "32b2cffec1b19dd39683f34f47312817c22f61f3",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6160. Potential NullPointerException in MRClientProtocol interface implementation. Contributed by Rohith",
        "parent": "https://github.com/apache/hadoop/commit/0f9528b99addbb0fd9a19d84db22a8c8e934b05f",
        "patched_files": [
            "MRClientService.java",
            "HistoryClientService.java",
            "JobHistoryServer.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJobHistoryServer.java",
            "TestMRClientService.java"
        ]
    },
    "hadoop_0cc98ae": {
        "bug_id": "hadoop_0cc98ae",
        "commit": "https://github.com/apache/hadoop/commit/0cc98ae0ec69419ded066f3f7decf59728b35e9d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0cc98ae0ec69419ded066f3f7decf59728b35e9d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=0cc98ae0ec69419ded066f3f7decf59728b35e9d",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -367,6 +367,8 @@ protected void serviceInit(Configuration conf) throws Exception {\n     \n     this.aclsManager = new ApplicationACLsManager(conf);\n \n+    this.dirsHandler = new LocalDirsHandlerService(metrics);\n+\n     boolean isDistSchedulingEnabled =\n         conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED,\n             YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);\n@@ -390,7 +392,6 @@ protected void serviceInit(Configuration conf) throws Exception {\n     // NodeManager level dispatcher\n     this.dispatcher = new AsyncDispatcher(\"NM Event dispatcher\");\n \n-    dirsHandler = new LocalDirsHandlerService(metrics);\n     nodeHealthChecker =\n         new NodeHealthCheckerService(\n             getNodeHealthScriptRunner(conf), dirsHandler);",
                "raw_url": "https://github.com/apache/hadoop/raw/0cc98ae0ec69419ded066f3f7decf59728b35e9d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "bddc7c34ee7afd05a10babf81e762e32008f8cca",
                "status": "modified"
            }
        ],
        "message": "YARN-7396. NPE when accessing container logs due to null dirsHandler. Contributed by Jonathan Hung",
        "parent": "https://github.com/apache/hadoop/commit/7a49ddfdde2e2a7b407f4a62a42d97bfe456075a",
        "patched_files": [
            "NodeManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_0d89859": {
        "bug_id": "hadoop_0d89859",
        "commit": "https://github.com/apache/hadoop/commit/0d89859b51157078cc504ac81dc8aa75ce6b1782",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=0d89859b51157078cc504ac81dc8aa75ce6b1782",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -266,6 +266,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-2920. Changed CapacityScheduler to kill containers on nodes where\n     node labels are changed. (Wangda Tan via jianhe)\n \n+    YARN-2340. Fixed NPE when queue is stopped during RM restart.\n+    (Rohith Sharmaks via jianhe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/CHANGES.txt",
                "sha": "cb2fc24ae23de919aeaf111bc642edce236117c5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=0d89859b51157078cc504ac81dc8aa75ce6b1782",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -706,11 +706,14 @@ private synchronized void addApplication(ApplicationId applicationId,\n     try {\n       queue.submitApplication(applicationId, user, queueName);\n     } catch (AccessControlException ace) {\n-      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n-          + queueName + \" from user \" + user, ace);\n-      this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n-      return;\n+      // Ignore the exception for recovered app as the app was previously accepted\n+      if (!isAppRecovering) {\n+        LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n+            + queueName + \" from user \" + user, ace);\n+        this.rmContext.getDispatcher().getEventHandler()\n+            .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n+        return;\n+      }\n     }\n     // update the metrics\n     queue.getMetrics().submitApp(user);",
                "raw_url": "https://github.com/apache/hadoop/raw/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "3648c5436d7a014d4fad12ff340aa8ee3d4bdac3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java?ref=0d89859b51157078cc504ac81dc8aa75ce6b1782",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "patch": "@@ -500,6 +500,8 @@ public void testCapacitySchedulerRecovery() throws Exception {\n     rm1.clearQueueMetrics(app1_2);\n     rm1.clearQueueMetrics(app2);\n \n+    csConf.set(\"yarn.scheduler.capacity.root.Default.QueueB.state\", \"STOPPED\");\n+\n     // Re-start RM\n     rm2 = new MockRM(csConf, memStore);\n     rm2.start();",
                "raw_url": "https://github.com/apache/hadoop/raw/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "sha": "842eaecad3202d1816163eda70495a0c5ec65bc7",
                "status": "modified"
            }
        ],
        "message": "YARN-2340. Fixed NPE when queue is stopped during RM restart. Contributed by Rohith Sharmaks",
        "parent": "https://github.com/apache/hadoop/commit/fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
        "patched_files": [
            "CapacityScheduler.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java",
            "TestWorkPreservingRMRestart.java"
        ]
    },
    "hadoop_0d9804d": {
        "bug_id": "hadoop_0d9804d",
        "commit": "https://github.com/apache/hadoop/commit/0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 27,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java",
                "patch": "@@ -430,13 +430,15 @@ public GetAllResourceTypeInfoResponse getResourceTypeInfo(\n     return pipeline.getRootInterceptor().getResourceTypeInfo(request);\n   }\n \n-  private RequestInterceptorChainWrapper getInterceptorChain()\n+  @VisibleForTesting\n+  protected RequestInterceptorChainWrapper getInterceptorChain()\n       throws IOException {\n     String user = UserGroupInformation.getCurrentUser().getUserName();\n-    if (!userPipelineMap.containsKey(user)) {\n-      initializePipeline(user);\n+    RequestInterceptorChainWrapper chain = userPipelineMap.get(user);\n+    if (chain != null && chain.getRootInterceptor() != null) {\n+      return chain;\n     }\n-    return userPipelineMap.get(user);\n+    return initializePipeline(user);\n   }\n \n   /**\n@@ -503,36 +505,33 @@ protected ClientRequestInterceptor createRequestInterceptorChain() {\n    *\n    * @param user\n    */\n-  private void initializePipeline(String user) {\n-    RequestInterceptorChainWrapper chainWrapper = null;\n+  private RequestInterceptorChainWrapper initializePipeline(String user) {\n     synchronized (this.userPipelineMap) {\n       if (this.userPipelineMap.containsKey(user)) {\n         LOG.info(\"Request to start an already existing user: {}\"\n             + \" was received, so ignoring.\", user);\n-        return;\n+        return userPipelineMap.get(user);\n       }\n \n-      chainWrapper = new RequestInterceptorChainWrapper();\n-      this.userPipelineMap.put(user, chainWrapper);\n-    }\n-\n-    // We register the pipeline instance in the map first and then initialize it\n-    // later because chain initialization can be expensive and we would like to\n-    // release the lock as soon as possible to prevent other applications from\n-    // blocking when one application's chain is initializing\n-    LOG.info(\"Initializing request processing pipeline for application \"\n-        + \"for the user: {}\", user);\n-\n-    try {\n-      ClientRequestInterceptor interceptorChain =\n-          this.createRequestInterceptorChain();\n-      interceptorChain.init(user);\n-      chainWrapper.init(interceptorChain);\n-    } catch (Exception e) {\n-      synchronized (this.userPipelineMap) {\n-        this.userPipelineMap.remove(user);\n+      RequestInterceptorChainWrapper chainWrapper =\n+          new RequestInterceptorChainWrapper();\n+      try {\n+        // We should init the pipeline instance after it is created and then\n+        // add to the map, to ensure thread safe.\n+        LOG.info(\"Initializing request processing pipeline for application \"\n+            + \"for the user: {}\", user);\n+\n+        ClientRequestInterceptor interceptorChain =\n+            this.createRequestInterceptorChain();\n+        interceptorChain.init(user);\n+        chainWrapper.init(interceptorChain);\n+      } catch (Exception e) {\n+        LOG.error(\"Init ClientRequestInterceptor error for user: \" + user, e);\n+        throw e;\n       }\n-      throw e;\n+\n+      this.userPipelineMap.put(user, chainWrapper);\n+      return chainWrapper;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java",
                "sha": "bbb8047d98d3c0775fa6354248256aae6a1de3f9",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 26,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java",
                "patch": "@@ -165,13 +165,15 @@ protected void serviceStop() throws Exception {\n     return interceptorClassNames;\n   }\n \n-  private RequestInterceptorChainWrapper getInterceptorChain()\n+  @VisibleForTesting\n+  protected RequestInterceptorChainWrapper getInterceptorChain()\n       throws IOException {\n     String user = UserGroupInformation.getCurrentUser().getUserName();\n-    if (!userPipelineMap.containsKey(user)) {\n-      initializePipeline(user);\n+    RequestInterceptorChainWrapper chain = userPipelineMap.get(user);\n+    if (chain != null && chain.getRootInterceptor() != null) {\n+      return chain;\n     }\n-    return userPipelineMap.get(user);\n+    return initializePipeline(user);\n   }\n \n   /**\n@@ -239,35 +241,32 @@ protected RMAdminRequestInterceptor createRequestInterceptorChain() {\n    *\n    * @param user\n    */\n-  private void initializePipeline(String user) {\n-    RequestInterceptorChainWrapper chainWrapper = null;\n+  private RequestInterceptorChainWrapper initializePipeline(String user) {\n     synchronized (this.userPipelineMap) {\n       if (this.userPipelineMap.containsKey(user)) {\n         LOG.info(\"Request to start an already existing user: {}\"\n             + \" was received, so ignoring.\", user);\n-        return;\n+        return userPipelineMap.get(user);\n       }\n \n-      chainWrapper = new RequestInterceptorChainWrapper();\n-      this.userPipelineMap.put(user, chainWrapper);\n-    }\n-\n-    // We register the pipeline instance in the map first and then initialize it\n-    // later because chain initialization can be expensive and we would like to\n-    // release the lock as soon as possible to prevent other applications from\n-    // blocking when one application's chain is initializing\n-    LOG.info(\"Initializing request processing pipeline for the user: {}\", user);\n-\n-    try {\n-      RMAdminRequestInterceptor interceptorChain =\n-          this.createRequestInterceptorChain();\n-      interceptorChain.init(user);\n-      chainWrapper.init(interceptorChain);\n-    } catch (Exception e) {\n-      synchronized (this.userPipelineMap) {\n-        this.userPipelineMap.remove(user);\n+      RequestInterceptorChainWrapper chainWrapper =\n+          new RequestInterceptorChainWrapper();\n+      try {\n+        // We should init the pipeline instance after it is created and then\n+        // add to the map, to ensure thread safe.\n+        LOG.info(\"Initializing request processing pipeline for user: {}\", user);\n+\n+        RMAdminRequestInterceptor interceptorChain =\n+            this.createRequestInterceptorChain();\n+        interceptorChain.init(user);\n+        chainWrapper.init(interceptorChain);\n+      } catch (Exception e) {\n+        LOG.error(\"Init RMAdminRequestInterceptor error for user: \" + user, e);\n+        throw e;\n       }\n-      throw e;\n+\n+      this.userPipelineMap.put(user, chainWrapper);\n+      return chainWrapper;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java",
                "sha": "ef30613f50c4bf250742e2d8787e292008385dfe",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 25,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java",
                "patch": "@@ -173,10 +173,11 @@ protected RequestInterceptorChainWrapper getInterceptorChain(\n     } catch (IOException e) {\n       LOG.error(\"Cannot get user: {}\", e.getMessage());\n     }\n-    if (!userPipelineMap.containsKey(user)) {\n-      initializePipeline(user);\n+    RequestInterceptorChainWrapper chain = userPipelineMap.get(user);\n+    if (chain != null && chain.getRootInterceptor() != null) {\n+      return chain;\n     }\n-    return userPipelineMap.get(user);\n+    return initializePipeline(user);\n   }\n \n   /**\n@@ -242,35 +243,32 @@ protected RESTRequestInterceptor createRequestInterceptorChain() {\n    *\n    * @param user\n    */\n-  private void initializePipeline(String user) {\n-    RequestInterceptorChainWrapper chainWrapper = null;\n+  private RequestInterceptorChainWrapper initializePipeline(String user) {\n     synchronized (this.userPipelineMap) {\n       if (this.userPipelineMap.containsKey(user)) {\n         LOG.info(\"Request to start an already existing user: {}\"\n             + \" was received, so ignoring.\", user);\n-        return;\n+        return userPipelineMap.get(user);\n       }\n \n-      chainWrapper = new RequestInterceptorChainWrapper();\n-      this.userPipelineMap.put(user, chainWrapper);\n-    }\n-\n-    // We register the pipeline instance in the map first and then initialize it\n-    // later because chain initialization can be expensive and we would like to\n-    // release the lock as soon as possible to prevent other applications from\n-    // blocking when one application's chain is initializing\n-    LOG.info(\"Initializing request processing pipeline for the user: {}\", user);\n-\n-    try {\n-      RESTRequestInterceptor interceptorChain =\n-          this.createRequestInterceptorChain();\n-      interceptorChain.init(user);\n-      chainWrapper.init(interceptorChain);\n-    } catch (Exception e) {\n-      synchronized (this.userPipelineMap) {\n-        this.userPipelineMap.remove(user);\n+      RequestInterceptorChainWrapper chainWrapper =\n+          new RequestInterceptorChainWrapper();\n+      try {\n+        // We should init the pipeline instance after it is created and then\n+        // add to the map, to ensure thread safe.\n+        LOG.info(\"Initializing request processing pipeline for user: {}\", user);\n+\n+        RESTRequestInterceptor interceptorChain =\n+            this.createRequestInterceptorChain();\n+        interceptorChain.init(user);\n+        chainWrapper.init(interceptorChain);\n+      } catch (Exception e) {\n+        LOG.error(\"Init RESTRequestInterceptor error for user: \" + user, e);\n+        throw e;\n       }\n-      throw e;\n+\n+      this.userPipelineMap.put(user, chainWrapper);\n+      return chainWrapper;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java",
                "sha": "49de588ba851d7597e470353dac02b23d860707f",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java",
                "patch": "@@ -19,8 +19,10 @@\n package org.apache.hadoop.yarn.server.router.clientrm;\n \n import java.io.IOException;\n+import java.security.PrivilegedExceptionAction;\n import java.util.Map;\n \n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.api.protocolrecords.GetClusterMetricsResponse;\n import org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodeLabelsResponse;\n import org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodesResponse;\n@@ -207,4 +209,62 @@ public void testUsersChainMapWithLRUCache()\n     Assert.assertNull(\"test2 should have been evicted\", chain);\n   }\n \n+  /**\n+   * This test validates if the ClientRequestInterceptor chain for the user\n+   * can build and init correctly when a multi-client process begins to\n+   * request RouterClientRMService for the same user simultaneously.\n+   */\n+  @Test\n+  public void testClientPipelineConcurrent() throws InterruptedException {\n+    final String user = \"test1\";\n+\n+    /*\n+     * ClientTestThread is a thread to simulate a client request to get a\n+     * ClientRequestInterceptor for the user.\n+     */\n+    class ClientTestThread extends Thread {\n+      private ClientRequestInterceptor interceptor;\n+      @Override public void run() {\n+        try {\n+          interceptor = pipeline();\n+        } catch (IOException | InterruptedException e) {\n+          e.printStackTrace();\n+        }\n+      }\n+      private ClientRequestInterceptor pipeline()\n+          throws IOException, InterruptedException {\n+        return UserGroupInformation.createRemoteUser(user).doAs(\n+            new PrivilegedExceptionAction<ClientRequestInterceptor>() {\n+              @Override\n+              public ClientRequestInterceptor run() throws Exception {\n+                RequestInterceptorChainWrapper wrapper =\n+                    getRouterClientRMService().getInterceptorChain();\n+                ClientRequestInterceptor interceptor =\n+                    wrapper.getRootInterceptor();\n+                Assert.assertNotNull(interceptor);\n+                LOG.info(\"init client interceptor success for user \" + user);\n+                return interceptor;\n+              }\n+            });\n+      }\n+    }\n+\n+    /*\n+     * We start the first thread. It should not finish initing a chainWrapper\n+     * before the other thread starts. In this way, the second thread can\n+     * init at the same time of the first one. In the end, we validate that\n+     * the 2 threads get the same chainWrapper without going into error.\n+     */\n+    ClientTestThread client1 = new ClientTestThread();\n+    ClientTestThread client2 = new ClientTestThread();\n+    client1.start();\n+    client2.start();\n+    client1.join();\n+    client2.join();\n+\n+    Assert.assertNotNull(client1.interceptor);\n+    Assert.assertNotNull(client2.interceptor);\n+    Assert.assertTrue(client1.interceptor == client2.interceptor);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java",
                "sha": "b03059decfab2555ac88c3aeaf119c0003b6dc3c",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java",
                "patch": "@@ -19,8 +19,10 @@\n package org.apache.hadoop.yarn.server.router.rmadmin;\n \n import java.io.IOException;\n+import java.security.PrivilegedExceptionAction;\n import java.util.Map;\n \n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.api.protocolrecords.AddToClusterNodeLabelsResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.CheckForDecommissioningNodesResponse;\n@@ -216,4 +218,62 @@ public void testUsersChainMapWithLRUCache()\n     Assert.assertNull(\"test2 should have been evicted\", chain);\n   }\n \n+  /**\n+   * This test validates if the RMAdminRequestInterceptor chain for the user\n+   * can build and init correctly when a multi-client process begins to\n+   * request RouterRMAdminService for the same user simultaneously.\n+   */\n+  @Test\n+  public void testRMAdminPipelineConcurrent() throws InterruptedException {\n+    final String user = \"test1\";\n+\n+    /*\n+     * ClientTestThread is a thread to simulate a client request to get a\n+     * RMAdminRequestInterceptor for the user.\n+     */\n+    class ClientTestThread extends Thread {\n+      private RMAdminRequestInterceptor interceptor;\n+      @Override public void run() {\n+        try {\n+          interceptor = pipeline();\n+        } catch (IOException | InterruptedException e) {\n+          e.printStackTrace();\n+        }\n+      }\n+      private RMAdminRequestInterceptor pipeline()\n+          throws IOException, InterruptedException {\n+        return UserGroupInformation.createRemoteUser(user).doAs(\n+            new PrivilegedExceptionAction<RMAdminRequestInterceptor>() {\n+              @Override\n+              public RMAdminRequestInterceptor run() throws Exception {\n+                RequestInterceptorChainWrapper wrapper =\n+                    getRouterRMAdminService().getInterceptorChain();\n+                RMAdminRequestInterceptor interceptor =\n+                    wrapper.getRootInterceptor();\n+                Assert.assertNotNull(interceptor);\n+                LOG.info(\"init rm admin interceptor success for user\" + user);\n+                return interceptor;\n+              }\n+            });\n+      }\n+    }\n+\n+    /*\n+     * We start the first thread. It should not finish initing a chainWrapper\n+     * before the other thread starts. In this way, the second thread can\n+     * init at the same time of the first one. In the end, we validate that\n+     * the 2 threads get the same chainWrapper without going into error.\n+     */\n+    ClientTestThread client1 = new ClientTestThread();\n+    ClientTestThread client2 = new ClientTestThread();\n+    client1.start();\n+    client2.start();\n+    client1.join();\n+    client2.join();\n+\n+    Assert.assertNotNull(client1.interceptor);\n+    Assert.assertNotNull(client2.interceptor);\n+    Assert.assertTrue(client1.interceptor == client2.interceptor);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java",
                "sha": "07ef73c3cdb857c32840115f55bf3ef660a4fd5b",
                "status": "modified"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java",
                "patch": "@@ -19,10 +19,12 @@\n package org.apache.hadoop.yarn.server.router.webapp;\n \n import java.io.IOException;\n+import java.security.PrivilegedExceptionAction;\n import java.util.Map;\n \n import javax.ws.rs.core.Response;\n \n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ActivitiesInfo;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppActivitiesInfo;\n@@ -49,12 +51,17 @@\n import org.apache.hadoop.yarn.server.webapp.dao.ContainersInfo;\n import org.junit.Assert;\n import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n /**\n  * Test class to validate the WebService interceptor model inside the Router.\n  */\n public class TestRouterWebServices extends BaseRouterWebServicesTest {\n \n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestRouterWebServices.class);\n+\n   private String user = \"test1\";\n \n   /**\n@@ -266,4 +273,62 @@ public void testUsersChainMapWithLRUCache()\n     Assert.assertNull(\"test2 should have been evicted\", chain);\n   }\n \n+  /**\n+   * This test validates if the RESTRequestInterceptor chain for the user\n+   * can build and init correctly when a multi-client process begins to\n+   * request RouterWebServices for the same user simultaneously.\n+   */\n+  @Test\n+  public void testWebPipelineConcurrent() throws InterruptedException {\n+    final String user = \"test1\";\n+\n+    /*\n+     * ClientTestThread is a thread to simulate a client request to get a\n+     * RESTRequestInterceptor for the user.\n+     */\n+    class ClientTestThread extends Thread {\n+      private RESTRequestInterceptor interceptor;\n+      @Override public void run() {\n+        try {\n+          interceptor = pipeline();\n+        } catch (IOException | InterruptedException e) {\n+          e.printStackTrace();\n+        }\n+      }\n+      private RESTRequestInterceptor pipeline()\n+          throws IOException, InterruptedException {\n+        return UserGroupInformation.createRemoteUser(user).doAs(\n+            new PrivilegedExceptionAction<RESTRequestInterceptor>() {\n+              @Override\n+              public RESTRequestInterceptor run() throws Exception {\n+                RequestInterceptorChainWrapper wrapper =\n+                    getInterceptorChain(user);\n+                RESTRequestInterceptor interceptor =\n+                    wrapper.getRootInterceptor();\n+                Assert.assertNotNull(interceptor);\n+                LOG.info(\"init web interceptor success for user\" + user);\n+                return interceptor;\n+              }\n+            });\n+      }\n+    }\n+\n+    /*\n+     * We start the first thread. It should not finish initing a chainWrapper\n+     * before the other thread starts. In this way, the second thread can\n+     * init at the same time of the first one. In the end, we validate that\n+     * the 2 threads get the same chainWrapper without going into error.\n+     */\n+    ClientTestThread client1 = new ClientTestThread();\n+    ClientTestThread client2 = new ClientTestThread();\n+    client1.start();\n+    client2.start();\n+    client1.join();\n+    client2.join();\n+\n+    Assert.assertNotNull(client1.interceptor);\n+    Assert.assertNotNull(client2.interceptor);\n+    Assert.assertTrue(client1.interceptor == client2.interceptor);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java",
                "sha": "14652435dac291f4cd09038f493ef4621819d036",
                "status": "modified"
            }
        ],
        "message": "YARN-8435. Fix NPE when the same client simultaneously contact for the first time Yarn Router. Contributed by Rang Jiaheng.",
        "parent": "https://github.com/apache/hadoop/commit/71df8c27c9a0e326232d3baf16414a63b5ea5a4b",
        "patched_files": [
            "RouterWebServices.java",
            "RouterClientRMService.java",
            "RouterRMAdminService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRouterClientRMService.java",
            "TestRouterWebServices.java",
            "TestRouterRMAdminService.java"
        ]
    },
    "hadoop_0ffca5d": {
        "bug_id": "hadoop_0ffca5d",
        "commit": "https://github.com/apache/hadoop/commit/0ffca5d347df0acb1979dff7a07ae88ea834adc7",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -275,10 +275,6 @@ protected void addSchedPriorityCommand(List<String> command) {\n     }\n   }\n \n-  protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n-    return PrivilegedOperationExecutor.getInstance(getConf());\n-  }\n-\n   @Override\n   public void init() throws IOException {\n     Configuration conf = super.getConf();\n@@ -289,7 +285,7 @@ public void init() throws IOException {\n       PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.CHECK_SETUP);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n           false);\n@@ -386,7 +382,7 @@ public void startLocalizer(LocalizerStartContext ctx)\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n           initializeContainerOp, null, null, false, true);\n@@ -534,9 +530,8 @@ public int launchContainer(ContainerStartContext ctx)\n         }\n         builder.append(\"Stack trace: \"\n             + StringUtils.stringifyException(e) + \"\\n\");\n-        String output = e.getOutput();\n-        if (output!= null && !e.getOutput().isEmpty()) {\n-          builder.append(\"Shell output: \" + output + \"\\n\");\n+        if (!e.getOutput().isEmpty()) {\n+          builder.append(\"Shell output: \" + e.getOutput() + \"\\n\");\n         }\n         String diagnostics = builder.toString();\n         logOutput(diagnostics);\n@@ -734,7 +729,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp,\n           false);\n@@ -764,7 +759,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n \n     try {\n       PrivilegedOperationExecutor privOpExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(super.getConf());\n \n       String results =\n           privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n@@ -823,7 +818,7 @@ public void mountCgroups(List<String> cgroupKVs, String hierarchy)\n \n       mountCGroupsOp.appendArgs(cgroupKVs);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(mountCGroupsOp,\n           false);",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "9a3b2d25cc6b384f27540953fa4e89ecb3fd7c09",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "patch": "@@ -24,7 +24,7 @@\n \n public class PrivilegedOperationException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private int exitCode = -1;\n+  private Integer exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -36,7 +36,7 @@ public PrivilegedOperationException(String message) {\n     super(message);\n   }\n \n-  public PrivilegedOperationException(String message, int exitCode,\n+  public PrivilegedOperationException(String message, Integer exitCode,\n       String output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n@@ -48,8 +48,8 @@ public PrivilegedOperationException(Throwable cause) {\n     super(cause);\n   }\n \n-  public PrivilegedOperationException(Throwable cause, int exitCode,\n-      String output, String errorOutput) {\n+  public PrivilegedOperationException(Throwable cause, Integer exitCode, String\n+      output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n@@ -59,7 +59,7 @@ public PrivilegedOperationException(String message, Throwable cause) {\n     super(message, cause);\n   }\n \n-  public int getExitCode() {\n+  public Integer getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "sha": "3622489a499f9bea2423e24bba7a8d1b46deacab",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "patch": "@@ -32,10 +32,10 @@\n @InterfaceStability.Unstable\n public class ContainerExecutionException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private static final int EXIT_CODE_UNSET = -1;\n+  private static final Integer EXIT_CODE_UNSET = -1;\n   private static final String OUTPUT_UNSET = \"<unknown>\";\n \n-  private int exitCode;\n+  private Integer exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -54,23 +54,23 @@ public ContainerExecutionException(Throwable throwable) {\n   }\n \n \n-  public ContainerExecutionException(String message, int exitCode, String\n+  public ContainerExecutionException(String message, Integer exitCode, String\n       output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public ContainerExecutionException(Throwable cause, int exitCode, String\n+  public ContainerExecutionException(Throwable cause, Integer exitCode, String\n       output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public int getExitCode() {\n+  public Integer getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "sha": "1fbece2205e27daeaa5798732f72177ad66c92cd",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 89,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -23,9 +23,7 @@\n import static org.junit.Assert.assertNotEquals;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n-import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n-import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n@@ -42,24 +40,20 @@\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.util.StringUtils;\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.ConfigurationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerDiagnosticsUpdateEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime;\n@@ -522,87 +516,4 @@ public void testDeleteAsUser() throws IOException {\n         appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n         readMockParams());\n   }\n-\n-  @Test\n-  public void testNoExitCodeFromPrivilegedOperation() throws Exception {\n-    Configuration conf = new Configuration();\n-    final PrivilegedOperationExecutor spyPrivilegedExecutor =\n-        spy(PrivilegedOperationExecutor.getInstance(conf));\n-    doThrow(new PrivilegedOperationException(\"interrupted\"))\n-        .when(spyPrivilegedExecutor).executePrivilegedOperation(\n-            any(List.class), any(PrivilegedOperation.class),\n-            any(File.class), any(Map.class), anyBoolean(), anyBoolean());\n-    LinuxContainerRuntime runtime = new DefaultLinuxContainerRuntime(\n-        spyPrivilegedExecutor);\n-    runtime.initialize(conf);\n-    mockExec = new LinuxContainerExecutor(runtime);\n-    mockExec.setConf(conf);\n-    LinuxContainerExecutor lce = new LinuxContainerExecutor(runtime) {\n-      @Override\n-      protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n-        return spyPrivilegedExecutor;\n-      }\n-    };\n-    lce.setConf(conf);\n-    InetSocketAddress address = InetSocketAddress.createUnresolved(\n-        \"localhost\", 8040);\n-    Path nmPrivateCTokensPath= new Path(\"file:///bin/nmPrivateCTokensPath\");\n-    LocalDirsHandlerService dirService = new LocalDirsHandlerService();\n-    dirService.init(conf);\n-\n-    String appSubmitter = \"nobody\";\n-    ApplicationId appId = ApplicationId.newInstance(1, 1);\n-    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\n-    ContainerId cid = ContainerId.newContainerId(attemptId, 1);\n-    HashMap<String, String> env = new HashMap<>();\n-    Container container = mock(Container.class);\n-    ContainerLaunchContext context = mock(ContainerLaunchContext.class);\n-    when(container.getContainerId()).thenReturn(cid);\n-    when(container.getLaunchContext()).thenReturn(context);\n-    when(context.getEnvironment()).thenReturn(env);\n-    Path workDir = new Path(\"/tmp\");\n-\n-    try {\n-      lce.startLocalizer(new LocalizerStartContext.Builder()\n-          .setNmPrivateContainerTokens(nmPrivateCTokensPath)\n-          .setNmAddr(address)\n-          .setUser(appSubmitter)\n-          .setAppId(appId.toString())\n-          .setLocId(\"12345\")\n-          .setDirsHandler(dirService)\n-          .build());\n-      Assert.fail(\"startLocalizer should have thrown an exception\");\n-    } catch (IOException e) {\n-      assertTrue(\"Unexpected exception \" + e,\n-          e.getMessage().contains(\"exitCode\"));\n-    }\n-\n-    lce.activateContainer(cid, new Path(workDir, \"pid.txt\"));\n-    lce.launchContainer(new ContainerStartContext.Builder()\n-        .setContainer(container)\n-        .setNmPrivateContainerScriptPath(new Path(\"file:///bin/echo\"))\n-        .setNmPrivateTokensPath(new Path(\"file:///dev/null\"))\n-        .setUser(appSubmitter)\n-        .setAppId(appId.toString())\n-        .setContainerWorkDir(workDir)\n-        .setLocalDirs(dirsHandler.getLocalDirs())\n-        .setLogDirs(dirsHandler.getLogDirs())\n-        .setFilecacheDirs(new ArrayList<>())\n-        .setUserLocalDirs(new ArrayList<>())\n-        .setContainerLocalDirs(new ArrayList<>())\n-        .setContainerLogDirs(new ArrayList<>())\n-        .build());\n-    lce.deleteAsUser(new DeletionAsUserContext.Builder()\n-        .setUser(appSubmitter)\n-        .setSubDir(new Path(\"/tmp/testdir\"))\n-        .build());\n-\n-    try {\n-      lce.mountCgroups(new ArrayList<String>(), \"hierarchy\");\n-      Assert.fail(\"mountCgroups should have thrown an exception\");\n-    } catch (IOException e) {\n-      assertTrue(\"Unexpected exception \" + e,\n-          e.getMessage().contains(\"exit code\"));\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "07134e8e36dd1ac5111088f2ffbf29d020b14abb",
                "status": "modified"
            }
        ],
        "message": "Revert \"YARN-6805. NPE in LinuxContainerExecutor due to null PrivilegedOperationException exit code. Contributed by Jason Lowe\"\n\nThis reverts commit f76f5c0919cdb0b032edb309d137093952e77268.",
        "parent": "https://github.com/apache/hadoop/commit/f76f5c0919cdb0b032edb309d137093952e77268",
        "patched_files": [
            "LinuxContainerExecutor.java",
            "PrivilegedOperationException.java",
            "ContainerExecutionException.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java",
            "TestLinuxContainerExecutorWithMocks.java"
        ]
    },
    "hadoop_10db613": {
        "bug_id": "hadoop_10db613",
        "commit": "https://github.com/apache/hadoop/commit/10db613389718d8df519493b958ecd97fca8686d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "patch": "@@ -51,3 +51,6 @@ fs-encryption (Unreleased)\n   BUG FIXES\n \n     HADOOP-10871. incorrect prototype in OpensslSecureRandom.c (cmccabe)\n+\n+    HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not \n+    loaded. (umamahesh)",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "sha": "498307e6bc7804db4ecdbc177b6561be579f7195",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "patch": "@@ -45,14 +45,21 @@\n   \n   /**\n    * Get crypto codec for specified algorithm/mode/padding.\n-   * @param conf the configuration\n-   * @param CipherSuite algorithm/mode/padding\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @param CipherSuite\n+   *          algorithm/mode/padding\n+   * @return CryptoCodec the codec object. Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf, \n       CipherSuite cipherSuite) {\n     List<Class<? extends CryptoCodec>> klasses = getCodecClasses(\n         conf, cipherSuite);\n+    if (klasses == null) {\n+      return null;\n+    }\n     CryptoCodec codec = null;\n     for (Class<? extends CryptoCodec> klass : klasses) {\n       try {\n@@ -80,10 +87,13 @@ public static CryptoCodec getInstance(Configuration conf,\n   }\n   \n   /**\n-   * Get crypto codec for algorithm/mode/padding in config value \n+   * Get crypto codec for algorithm/mode/padding in config value\n    * hadoop.security.crypto.cipher.suite\n-   * @param conf the configuration\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @return CryptoCodec the codec object Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf) {\n     String name = conf.get(HADOOP_SECURITY_CRYPTO_CIPHER_SUITE_KEY, \n@@ -97,6 +107,10 @@ public static CryptoCodec getInstance(Configuration conf) {\n     String configName = HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX + \n         cipherSuite.getConfigSuffix();\n     String codecString = conf.get(configName);\n+    if (codecString == null) {\n+      LOG.warn(\"No crypto codec classes with cipher suite configured.\");\n+      return null;\n+    }\n     for (String c : Splitter.on(',').trimResults().omitEmptyStrings().\n         split(codecString)) {\n       try {",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "sha": "9de7f95200f5f44e440619269fbc2ae1135f599d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.io.OutputStream;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.LocalFileSystem;\n@@ -50,6 +51,11 @@ public static void init() throws Exception {\n     conf = new Configuration(false);\n     conf.set(\"fs.file.impl\", LocalFileSystem.class.getName());\n     fileSys = FileSystem.getLocal(conf);\n+    conf.set(\n+        CommonConfigurationKeysPublic.HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX\n+            + CipherSuite.AES_CTR_NOPADDING.getConfigSuffix(),\n+        OpensslAesCtrCryptoCodec.class.getName() + \",\"\n+            + JceAesCtrCryptoCodec.class.getName());\n     codec = CryptoCodec.getInstance(conf);\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "sha": "765a364faa6d5cffff9bd1a811caf51bc51b0cc5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "patch": "@@ -598,7 +598,9 @@ public DFSClient(URI nameNodeUri, ClientProtocol rpcNamenode,\n         DFSUtil.getRandom().nextInt()  + \"_\" + Thread.currentThread().getId();\n     this.codec = CryptoCodec.getInstance(conf);\n     this.cipherSuites = Lists.newArrayListWithCapacity(1);\n-    cipherSuites.add(codec.getCipherSuite());\n+    if (codec != null) {\n+      cipherSuites.add(codec.getCipherSuite());\n+    }\n     provider = DFSUtil.createKeyProviderCryptoExtension(conf);\n     if (provider == null) {\n       LOG.info(\"No KeyProvider found.\");\n@@ -1333,9 +1335,12 @@ public HdfsDataInputStream createWrappedInputStream(DFSInputStream dfsis)\n     if (feInfo != null) {\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n+      CryptoCodec codec = CryptoCodec\n+          .getInstance(conf, feInfo.getCipherSuite());\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       final CryptoInputStream cryptoIn =\n-          new CryptoInputStream(dfsis, CryptoCodec.getInstance(conf, \n-              feInfo.getCipherSuite()), decrypted.getMaterial(),\n+          new CryptoInputStream(dfsis, codec, decrypted.getMaterial(),\n               feInfo.getIV());\n       return new HdfsDataInputStream(cryptoIn);\n     } else {\n@@ -1361,6 +1366,8 @@ public HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos,\n       FileSystem.Statistics statistics, long startPos) throws IOException {\n     final FileEncryptionInfo feInfo = dfsos.getFileEncryptionInfo();\n     if (feInfo != null) {\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n       final CryptoOutputStream cryptoOut =",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "sha": "d3532607c84b80f9588043f78c1202a625bc1db9",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -38,7 +38,6 @@\n import java.util.EnumSet;\n import java.util.List;\n import java.util.Random;\n-import java.util.concurrent.CancellationException;\n \n import org.apache.commons.lang.ArrayUtils;\n import org.apache.commons.logging.impl.Log4JLogger;",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "b71cc32fb80e93c51153e29045cbaa020c1bf461",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not loaded. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1615523 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/70c99278a9ffb8a22059c20357b435c7b576b3db",
        "patched_files": [
            "DFSClient.java",
            "CHANGES-fs-encryption.java",
            "CryptoCodec.java",
            "DistributedFileSystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCryptoStreamsForLocalFS.java",
            "TestDistributedFileSystem.java",
            "TestCryptoCodec.java"
        ]
    },
    "hadoop_11db469": {
        "bug_id": "hadoop_11db469",
        "commit": "https://github.com/apache/hadoop/commit/11db46956c6062a986caf9009d3e36049e5830d4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/11db46956c6062a986caf9009d3e36049e5830d4/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java?ref=11db46956c6062a986caf9009d3e36049e5830d4",
                "deletions": 7,
                "filename": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "patch": "@@ -65,14 +65,10 @@\n   public BaseHttpServer(Configuration conf, String name) throws IOException {\n     this.name = name;\n     this.conf = conf;\n+    policy = DFSUtil.getHttpPolicy(conf);\n     if (isEnabled()) {\n-      policy = DFSUtil.getHttpPolicy(conf);\n-      if (policy.isHttpEnabled()) {\n-        this.httpAddress = getHttpBindAddress();\n-      }\n-      if (policy.isHttpsEnabled()) {\n-        this.httpsAddress = getHttpsBindAddress();\n-      }\n+      this.httpAddress = getHttpBindAddress();\n+      this.httpsAddress = getHttpsBindAddress();\n       HttpServer2.Builder builder = null;\n       builder = DFSUtil.httpServerTemplateForNNAndJN(conf, this.httpAddress,\n           this.httpsAddress, name, getSpnegoPrincipal(), getKeytabFile());",
                "raw_url": "https://github.com/apache/hadoop/raw/11db46956c6062a986caf9009d3e36049e5830d4/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "sha": "9a1d4b3c77992f481fedfa572c7a02b32860afc6",
                "status": "modified"
            }
        ],
        "message": "HDDS-1235. BaseHttpServer NPE is HTTP policy is HTTPS_ONLY. Contributed by Xiaoyu Yao. \n\nCloses #572",
        "parent": "https://github.com/apache/hadoop/commit/8458ced11e5b6690a41593c247c1190bf3f31c0b",
        "patched_files": [
            "BaseHttpServer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBaseHttpServer.java"
        ]
    },
    "hadoop_137aa07": {
        "bug_id": "hadoop_137aa07",
        "commit": "https://github.com/apache/hadoop/commit/137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -203,6 +203,10 @@ Release 0.23.2 - UNRELEASED\n     MAPREDUCE-3816. capacity scheduler web ui bar graphs for used capacity wrong\n     (tgraves via bobby)\n \n+    MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a \n+    task attempt without an assigned container. (Robert Joseph Evans via\n+    sseth)\n+\n Release 0.23.1 - 2012-02-17\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d293b52b3f7f2466e11cf730a8d4aa9ef58ad197",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java?ref=137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "patch": "@@ -142,7 +142,7 @@ private static ApplicationId toApplicationId(\n   }\n \n   public static String toString(ContainerId cId) {\n-    return cId.toString();\n+    return cId == null ? null : cId.toString();\n   }\n \n   public static NodeId toNodeId(String nodeIdStr) {",
                "raw_url": "https://github.com/apache/hadoop/raw/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "sha": "21fe2d9874cf0223ea91b4fb1af96c13f306464c",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java?ref=137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.net.URISyntaxException;\n \n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.URL;\n import org.junit.Test;\n \n@@ -35,4 +36,17 @@ public void testConvertUrlWithNoPort() throws URISyntaxException {\n     assertEquals(expectedPath, actualPath);\n   }\n \n+  @Test\n+  public void testContainerId() throws URISyntaxException {\n+    ContainerId id = BuilderUtils.newContainerId(0, 0, 0, 0);\n+    String cid = ConverterUtils.toString(id);\n+    assertEquals(\"container_0_0000_00_000000\", cid);\n+    ContainerId gen = ConverterUtils.toContainerId(cid);\n+    assertEquals(gen, id);\n+  }\n+\n+  @Test\n+  public void testContainerIdNull() throws URISyntaxException {\n+    assertNull(ConverterUtils.toString((ContainerId)null));\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "sha": "7924124bf40e7a3d6332a3ee5485a6a595bacfcd",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a task attempt without an assigned container. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1294901 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ffdf980b2056b2a1b31ccb19746f23c31f7d08ef",
        "patched_files": [
            "ConverterUtils.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestConverterUtils.java"
        ]
    },
    "hadoop_1415ad3": {
        "bug_id": "hadoop_1415ad3",
        "commit": "https://github.com/apache/hadoop/commit/1415ad3800d117b4fff6ad0ef281acc7051a0bcf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/1415ad3800d117b4fff6ad0ef281acc7051a0bcf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java?ref=1415ad3800d117b4fff6ad0ef281acc7051a0bcf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -584,6 +584,7 @@ public boolean hasKerberosCredentials() {\n   @InterfaceAudience.Public\n   @InterfaceStability.Evolving\n   public static UserGroupInformation getCurrentUser() throws IOException {\n+    ensureInitialized();\n     AccessControlContext context = AccessController.getContext();\n     Subject subject = Subject.getSubject(context);\n     if (subject == null || subject.getPrincipals(User.class).isEmpty()) {\n@@ -670,6 +671,7 @@ public static UserGroupInformation getUGIFromSubject(Subject subject)\n   @InterfaceAudience.Public\n   @InterfaceStability.Evolving\n   public static UserGroupInformation getLoginUser() throws IOException {\n+    ensureInitialized();\n     UserGroupInformation loginUser = loginUserRef.get();\n     // a potential race condition exists only for the initial creation of\n     // the login user.  there's no need to penalize all subsequent calls",
                "raw_url": "https://github.com/apache/hadoop/raw/1415ad3800d117b4fff6ad0ef281acc7051a0bcf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "884380c43f236f365598139f5bf13a55ead30c42",
                "status": "modified"
            }
        ],
        "message": "HADOOP-16707. NPE in UGI.getCurrentUser in ITestAbfsIdentityTransformer setup.\n\nContributed by Steve Loughran.\r\n\r\nChange-Id: I38fdba2fa70e534d78b15e61de19368912588b0c",
        "parent": "https://github.com/apache/hadoop/commit/dfc61d8ea5971e9742af77fcae2cae28011ae0c9",
        "patched_files": [
            "UserGroupInformation.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_1425c65": {
        "bug_id": "hadoop_1425c65",
        "commit": "https://github.com/apache/hadoop/commit/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -174,6 +174,7 @@ Release 0.23.2 - UNRELEASED\n     (sharad, todd via todd)\n \n   BUG FIXES\n+    HADOOP-8054 NPE with FilterFileSystem (Daryn Sharp via bobby)\n \n     HADOOP-8042  When copying a file out of HDFS, modifying it, and uploading\n     it back into HDFS, the put fails due to a CRC mismatch",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "a199c6e1b88bf677994949ae87c9cf694836c645",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "patch": "@@ -80,6 +80,11 @@ public FileSystem getRawFileSystem() {\n    */\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n+    // this is less than ideal, but existing filesystems sometimes neglect\n+    // to initialize the embedded filesystem\n+    if (fs.getConf() == null) {\n+      fs.initialize(name, conf);\n+    }\n     String scheme = name.getScheme();\n     if (!scheme.equals(fs.getUri().getScheme())) {\n       swapScheme = scheme;",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "sha": "91ee2ae710566d54b4950413f17e01051ca0f618",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 7,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "patch": "@@ -48,13 +48,6 @@ public LocalFileSystem(FileSystem rawLocalFileSystem) {\n     super(rawLocalFileSystem);\n   }\n     \n-  @Override\n-  public void initialize(URI uri, Configuration conf) throws IOException {\n-    super.initialize(uri, conf);\n-    // ctor didn't initialize the filtered fs\n-    getRawFileSystem().initialize(uri, conf);\n-  }\n-  \n   /** Convert a path to a File. */\n   public File pathToFile(Path path) {\n     return ((RawLocalFileSystem)fs).pathToFile(path);",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "sha": "4aff81114b9befe0dc7daf6677c91e65c09356db",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "patch": "@@ -18,24 +18,39 @@\n \n package org.apache.hadoop.fs;\n \n+import static org.junit.Assert.*;\n+import static org.mockito.Matchers.*;\n+import static org.mockito.Mockito.*;\n+\n import java.io.IOException;\n import java.lang.reflect.Method;\n import java.lang.reflect.Modifier;\n+import java.net.URI;\n import java.util.EnumSet;\n import java.util.Iterator;\n \n-import junit.framework.TestCase;\n import org.apache.commons.logging.Log;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.Options.CreateOpts;\n import org.apache.hadoop.fs.Options.Rename;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.util.Progressable;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n \n-public class TestFilterFileSystem extends TestCase {\n+public class TestFilterFileSystem {\n \n   private static final Log LOG = FileSystem.LOG;\n+  private static final Configuration conf = new Configuration();\n \n+  @BeforeClass\n+  public static void setup() {\n+    conf.set(\"fs.flfs.impl\", FilterLocalFileSystem.class.getName());\n+    conf.setBoolean(\"fs.flfs.impl.disable.cache\", true);\n+    conf.setBoolean(\"fs.file.impl.disable.cache\", true);\n+  }\n+  \n   public static class DontCheck {\n     public BlockLocation[] getFileBlockLocations(Path p, \n         long start, long len) { return null; }\n@@ -153,6 +168,7 @@ public void primitiveMkdir(Path f, FsPermission absolutePermission,\n     \n   }\n   \n+  @Test\n   public void testFilterFileSystem() throws Exception {\n     for (Method m : FileSystem.class.getDeclaredMethods()) {\n       if (Modifier.isStatic(m.getModifiers()))\n@@ -176,4 +192,109 @@ public void testFilterFileSystem() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testFilterEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new FilterFileSystem(mockFs), true);\n+  }\n+\n+  @Test\n+  public void testFilterEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new FilterFileSystem(mockFs), false);\n+  }\n+\n+  @Test\n+  public void testLocalEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new LocalFileSystem(mockFs), true);\n+  }  \n+  \n+  @Test\n+  public void testLocalEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new LocalFileSystem(mockFs), false);\n+  }\n+  \n+  private FileSystem createMockFs(boolean useConf) {\n+    FileSystem mockFs = mock(FileSystem.class);\n+    when(mockFs.getUri()).thenReturn(URI.create(\"mock:/\"));\n+    when(mockFs.getConf()).thenReturn(useConf ? conf : null);\n+    return mockFs;\n+  }\n+\n+  @Test\n+  public void testGetLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = FileSystem.getLocal(conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testGetFilterLocalFsSetsConfs() throws Exception {\n+    FilterFileSystem flfs =\n+        (FilterFileSystem) FileSystem.get(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    lfs.initialize(lfs.getUri(), conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testInitFilterFsSetsEmbedConf() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    FilterFileSystem ffs = new FilterFileSystem(lfs);\n+    assertEquals(lfs, ffs.getRawFileSystem());\n+    checkFsConf(ffs, null, 3);\n+    ffs.initialize(URI.create(\"filter:/\"), conf);\n+    checkFsConf(ffs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitFilterLocalFsSetsEmbedConf() throws Exception {\n+    FilterFileSystem flfs = new FilterLocalFileSystem();\n+    assertEquals(LocalFileSystem.class, flfs.getRawFileSystem().getClass());\n+    checkFsConf(flfs, null, 3);\n+    flfs.initialize(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  private void checkInit(FilterFileSystem fs, boolean expectInit)\n+      throws Exception {\n+    URI uri = URI.create(\"filter:/\");\n+    fs.initialize(uri, conf);\n+    \n+    FileSystem embedFs = fs.getRawFileSystem();\n+    if (expectInit) {\n+      verify(embedFs, times(1)).initialize(eq(uri), eq(conf));\n+    } else {\n+      verify(embedFs, times(0)).initialize(any(URI.class), any(Configuration.class));\n+    }\n+  }\n+\n+  // check the given fs's conf, and all its filtered filesystems\n+  private void checkFsConf(FileSystem fs, Configuration conf, int expectDepth) {\n+    int depth = 0;\n+    while (true) {\n+      depth++; \n+      assertFalse(\"depth \"+depth+\">\"+expectDepth, depth > expectDepth);\n+      assertEquals(conf, fs.getConf());\n+      if (!(fs instanceof FilterFileSystem)) {\n+        break;\n+      }\n+      fs = ((FilterFileSystem) fs).getRawFileSystem();\n+    }\n+    assertEquals(expectDepth, depth);\n+  }\n+  \n+  private static class FilterLocalFileSystem extends FilterFileSystem {\n+    FilterLocalFileSystem() {\n+      super(new LocalFileSystem());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "sha": "364f46d2a40652beb7d20a27c3755376ba95d9b8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8054. NPE with FilterFileSystem (Daryn Sharp via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1245637 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/482f11a5785541ec8d6d812bee42e58c805cf9ce",
        "patched_files": [
            "CHANGES.java",
            "LocalFileSystem.java",
            "FilterFileSystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFilterFileSystem.java",
            "TestLocalFileSystem.java"
        ]
    },
    "hadoop_14384f5": {
        "bug_id": "hadoop_14384f5",
        "commit": "https://github.com/apache/hadoop/commit/14384f5b5142a98a10ce4bffadeb13e89bda9365",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=14384f5b5142a98a10ce4bffadeb13e89bda9365",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -482,6 +482,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-7939. Two fsimage_rollback_* files are created which are not deleted\n     after rollback. (J.Andreina via vinayakumarb)\n \n+    HDFS-8111. NPE thrown when invalid FSImage filename given for\n+    'hdfs oiv_legacy' cmd ( surendra singh lilhore via vinayakumarb )\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "abbfe6ae4e397ab25521f12df9b00a80a705d4ba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java?ref=14384f5b5142a98a10ce4bffadeb13e89bda9365",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "patch": "@@ -137,7 +137,11 @@ public void go() throws IOException  {\n       done = true;\n     } finally {\n       if (!done) {\n-        LOG.error(\"image loading failed at offset \" + tracker.getPos());\n+        if (tracker != null) {\n+          LOG.error(\"image loading failed at offset \" + tracker.getPos());\n+        } else {\n+          LOG.error(\"Failed to load image file.\");\n+        }\n       }\n       IOUtils.cleanup(LOG, in, tracker);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "sha": "7f81ba8e67fa6622b5e879eca44ff6bcf4d0417f",
                "status": "modified"
            }
        ],
        "message": "HDFS-8111. NPE thrown when invalid FSImage filename given for 'hdfs oiv_legacy' cmd ( Contributed by surendra singh lilhore )",
        "parent": "https://github.com/apache/hadoop/commit/f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
        "patched_files": [
            "OfflineImageViewer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOfflineImageViewer.java"
        ]
    },
    "hadoop_14e6f1e": {
        "bug_id": "hadoop_14e6f1e",
        "commit": "https://github.com/apache/hadoop/commit/14e6f1e796bfd77a9505063dfbb36579f124a2e9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -144,6 +144,9 @@ Release 0.23.1 - Unreleased\n     MAPREDUCE-3369. Migrate MR1 tests to run on MR2 using the new interfaces\n     introduced in MAPREDUCE-3169. (Ahmed Radwan via tomwhite)\n \n+    MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. \n+    (Jonathan Eagles via mahadev)\n+\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "f53ffec718a02c41aca8181ac32600b7c013dd30",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -432,7 +432,6 @@ public String getFailureInfo() throws IOException {\n \n   }\n \n-  Cluster cluster;\n   /**\n    * Ugi of the client. We store this ugi when the client is created and \n    * then make sure that the same ugi is used to run the various protocols.",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "fa3d799fe70d60bb3247edc940ff2f9d5f1d953e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "patch": "@@ -55,7 +55,7 @@\n @InterfaceStability.Stable\n public class CLI extends Configured implements Tool {\n   private static final Log LOG = LogFactory.getLog(CLI.class);\n-  private Cluster cluster;\n+  protected Cluster cluster;\n \n   public CLI() {\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "sha": "f7ac9c40a6a2074a6602bc8d12548d8c403682c0",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 8,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "patch": "@@ -19,21 +19,41 @@\n package org.apache.hadoop.mapred;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+import java.io.IOException;\n+import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobPriority;\n+import org.apache.hadoop.mapreduce.JobStatus;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.TaskReport;\n import org.junit.Test;\n \n public class JobClientUnitTest {\n   \n+  public class TestJobClient extends JobClient {\n+\n+    TestJobClient(JobConf jobConf) throws IOException {\n+      super(jobConf);\n+    }\n+\n+    void setCluster(Cluster cluster) {\n+      this.cluster = cluster;\n+    }\n+  }\n+\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testMapTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -47,9 +67,9 @@ public void testMapTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testReduceTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -63,9 +83,9 @@ public void testReduceTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testSetupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -79,9 +99,9 @@ public void testSetupTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testCleanupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -91,4 +111,49 @@ public void testCleanupTaskReportsWithNullJob() throws Exception {\n     \n     verify(mockCluster).getJob(id);\n   }\n+\n+  @Test\n+  public void testShowJob() throws Exception {\n+    TestJobClient client = new TestJobClient(new JobConf());\n+    JobID jobID = new JobID(\"test\", 0);\n+\n+    JobStatus mockJobStatus = mock(JobStatus.class);\n+    when(mockJobStatus.getJobID()).thenReturn(jobID);\n+    when(mockJobStatus.getState()).thenReturn(JobStatus.State.RUNNING);\n+    when(mockJobStatus.getStartTime()).thenReturn(0L);\n+    when(mockJobStatus.getUsername()).thenReturn(\"mockuser\");\n+    when(mockJobStatus.getQueue()).thenReturn(\"mockqueue\");\n+    when(mockJobStatus.getPriority()).thenReturn(JobPriority.NORMAL);\n+    when(mockJobStatus.getNumUsedSlots()).thenReturn(1);\n+    when(mockJobStatus.getNumReservedSlots()).thenReturn(1);\n+    when(mockJobStatus.getUsedMem()).thenReturn(1024);\n+    when(mockJobStatus.getReservedMem()).thenReturn(512);\n+    when(mockJobStatus.getNeededMem()).thenReturn(2048);\n+    when(mockJobStatus.getSchedulingInfo()).thenReturn(\"NA\");\n+\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getTaskReports(isA(TaskType.class))).thenReturn(new TaskReport[0]);\n+\n+    Cluster mockCluster = mock(Cluster.class);\n+    when(mockCluster.getJob(jobID)).thenReturn(mockJob);\n+\n+    client.setCluster(mockCluster);\n+    \n+    \n+    client.displayJobList(new JobStatus[] {mockJobStatus});\n+    verify(mockJobStatus, atLeastOnce()).getJobID();\n+    verify(mockJob, atLeastOnce()).getTaskReports(isA(TaskType.class));\n+    verify(mockCluster, atLeastOnce()).getJob(jobID);\n+    verify(mockJobStatus).getState();\n+    verify(mockJobStatus).getStartTime();\n+    verify(mockJobStatus).getUsername();\n+    verify(mockJobStatus).getQueue();\n+    verify(mockJobStatus).getPriority();\n+    verify(mockJobStatus).getNumUsedSlots();\n+    verify(mockJobStatus).getNumReservedSlots();\n+    verify(mockJobStatus).getUsedMem();\n+    verify(mockJobStatus).getReservedMem();\n+    verify(mockJobStatus).getNeededMem();\n+    verify(mockJobStatus).getSchedulingInfo();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "sha": "3f54e09a33d707a0e388239e7e2d8d9a0d274c07",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. (Jonathan Eagles via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213464 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/6fc7e2e002c1295a782fd2402d7f4e37194702b0",
        "patched_files": [
            "JobClient.java",
            "CHANGES.java",
            "CLI.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJobClient.java",
            "JobClientUnitTest.java",
            "TestCLI.java"
        ]
    },
    "hadoop_15241c6": {
        "bug_id": "hadoop_15241c6",
        "commit": "https://github.com/apache/hadoop/commit/15241c6349a5245761ed43bd0d38b25f783cc96b",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/15241c6349a5245761ed43bd0d38b25f783cc96b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java?ref=15241c6349a5245761ed43bd0d38b25f783cc96b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java",
                "patch": "@@ -167,6 +167,9 @@ public InMemoryAliasMapProtocolClientSideTranslatorPB(\n   public Optional<ProvidedStorageLocation> read(@Nonnull Block block)\n       throws IOException {\n \n+    if (block == null) {\n+      throw new IOException(\"Block cannot be null\");\n+    }\n     ReadRequestProto request =\n         ReadRequestProto\n             .newBuilder()\n@@ -191,6 +194,9 @@ public InMemoryAliasMapProtocolClientSideTranslatorPB(\n   public void write(@Nonnull Block block,\n       @Nonnull ProvidedStorageLocation providedStorageLocation)\n       throws IOException {\n+    if (block == null || providedStorageLocation == null) {\n+      throw new IOException(\"Provided block and location cannot be null\");\n+    }\n     WriteRequestProto request =\n         WriteRequestProto\n             .newBuilder()",
                "raw_url": "https://github.com/apache/hadoop/raw/15241c6349a5245761ed43bd0d38b25f783cc96b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/InMemoryAliasMapProtocolClientSideTranslatorPB.java",
                "sha": "d9e984b45c84494e6a429db077fa832bd75d8169",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/15241c6349a5245761ed43bd0d38b25f783cc96b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java?ref=15241c6349a5245761ed43bd0d38b25f783cc96b",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java",
                "patch": "@@ -150,11 +150,15 @@ public Configuration getConf() {\n   public void close() {\n     LOG.info(\"Stopping InMemoryLevelDBAliasMapServer\");\n     try {\n-      aliasMap.close();\n+      if (aliasMap != null) {\n+        aliasMap.close();\n+      }\n     } catch (IOException e) {\n       LOG.error(e.getMessage());\n     }\n-    aliasMapServer.stop();\n+    if (aliasMapServer != null) {\n+      aliasMapServer.stop();\n+    }\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/15241c6349a5245761ed43bd0d38b25f783cc96b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/aliasmap/InMemoryLevelDBAliasMapServer.java",
                "sha": "5c56736be43c8ee55cabce029aaebea2fd97255c",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/15241c6349a5245761ed43bd0d38b25f783cc96b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestInMemoryLevelDBAliasMapClient.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestInMemoryLevelDBAliasMapClient.java?ref=15241c6349a5245761ed43bd0d38b25f783cc96b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestInMemoryLevelDBAliasMapClient.java",
                "patch": "@@ -28,14 +28,19 @@\n import org.apache.hadoop.hdfs.server.aliasmap.InMemoryLevelDBAliasMapServer;\n import org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap;\n import org.apache.hadoop.hdfs.server.common.FileRegion;\n+import org.apache.hadoop.test.LambdaTestUtils;\n import org.junit.After;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n \n import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_BIND_HOST_KEY;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.junit.Assert.assertArrayEquals;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.File;\n import java.io.IOException;\n@@ -59,6 +64,9 @@\n   private Configuration conf;\n   private final static String BPID = \"BPID-0\";\n \n+  @Rule\n+  public final ExpectedException exception = ExpectedException.none();\n+\n   @Before\n   public void setUp() throws IOException {\n     conf = new Configuration();\n@@ -348,4 +356,35 @@ public void testServerBindHost() throws Exception {\n     conf.set(DFS_NAMENODE_SERVICE_RPC_BIND_HOST_KEY, \"0.0.0.0\");\n     writeRead();\n   }\n+\n+  @Test\n+  public void testNonExistentFile() throws Exception {\n+    // delete alias map location\n+    FileUtils.deleteDirectory(tempDir);\n+    // expect a RuntimeException when the aliasmap is started.\n+    exception.expect(RuntimeException.class);\n+    levelDBAliasMapServer.setConf(conf);\n+  }\n+\n+  @Test\n+  public void testNonExistentBlock() throws Exception {\n+    inMemoryLevelDBAliasMapClient.setConf(conf);\n+    levelDBAliasMapServer.setConf(conf);\n+    levelDBAliasMapServer.start();\n+    Block block1 = new Block(100, 43, 44);\n+    ProvidedStorageLocation providedStorageLocation1 = null;\n+    BlockAliasMap.Writer<FileRegion> writer1 =\n+        inMemoryLevelDBAliasMapClient.getWriter(null, BPID);\n+    try {\n+      writer1.store(new FileRegion(block1, providedStorageLocation1));\n+      fail(\"Should fail on writing a region with null ProvidedLocation\");\n+    } catch (IOException | IllegalArgumentException e) {\n+      assertTrue(e.getMessage().contains(\"not be null\"));\n+    }\n+\n+    BlockAliasMap.Reader<FileRegion> reader =\n+        inMemoryLevelDBAliasMapClient.getReader(null, BPID);\n+    LambdaTestUtils.assertOptionalUnset(\"Expected empty BlockAlias\",\n+        reader.resolve(block1));\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/15241c6349a5245761ed43bd0d38b25f783cc96b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/blockaliasmap/impl/TestInMemoryLevelDBAliasMapClient.java",
                "sha": "fccb6f2ab38b73ce25bd876658b9fb880731f388",
                "status": "modified"
            }
        ],
        "message": "HDFS-13795. Fix potential NPE in InMemoryLevelDBAliasMapServer.",
        "parent": "https://github.com/apache/hadoop/commit/0a71bf145293adbd3728525ab4c36c08d51377d3",
        "patched_files": [
            "InMemoryAliasMapProtocolClientSideTranslatorPB.java",
            "InMemoryLevelDBAliasMapServer.java",
            "InMemoryLevelDBAliasMapClient.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestInMemoryLevelDBAliasMapClient.java"
        ]
    },
    "hadoop_160b6fd": {
        "bug_id": "hadoop_160b6fd",
        "commit": "https://github.com/apache/hadoop/commit/160b6fd4966f5189f988eaf0f094867fb2155c04",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/160b6fd4966f5189f988eaf0f094867fb2155c04/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=160b6fd4966f5189f988eaf0f094867fb2155c04",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -455,6 +455,8 @@ Release 0.22.0 - Unreleased\n \n     HADOOP-7046. Fix Findbugs warning in Configuration. (Po Cheung via shv)\n \n+    HADOOP-7118. Fix NPE in Configuration.writeXml (todd)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop/raw/160b6fd4966f5189f988eaf0f094867fb2155c04/CHANGES.txt",
                "sha": "18504560f655dd8c1cf667f83c94190fcf02c6b3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/160b6fd4966f5189f988eaf0f094867fb2155c04/src/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/conf/Configuration.java?ref=160b6fd4966f5189f988eaf0f094867fb2155c04",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -1620,6 +1620,7 @@ private synchronized Document asXmlDocument() throws IOException {\n     Element conf = doc.createElement(\"configuration\");\n     doc.appendChild(conf);\n     conf.appendChild(doc.createTextNode(\"\\n\"));\n+    getProps(); // ensure properties is set\n     for (Enumeration e = properties.keys(); e.hasMoreElements();) {\n       String name = (String)e.nextElement();\n       Object object = properties.get(name);",
                "raw_url": "https://github.com/apache/hadoop/raw/160b6fd4966f5189f988eaf0f094867fb2155c04/src/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "c05acf36208020f5eca5fee3f4573c6e2292ace9",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/160b6fd4966f5189f988eaf0f094867fb2155c04/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/conf/TestConfiguration.java?ref=160b6fd4966f5189f988eaf0f094867fb2155c04",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.conf;\n \n import java.io.BufferedWriter;\n+import java.io.ByteArrayOutputStream;\n import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n@@ -255,6 +256,16 @@ public void testToString() throws IOException {\n     assertEquals(expectedOutput, conf.toString());\n   }\n   \n+  public void testWriteXml() throws IOException {\n+    Configuration conf = new Configuration();\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(); \n+    conf.writeXml(baos);\n+    String result = baos.toString();\n+    assertTrue(\"Result has proper header\", result.startsWith(\n+        \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" standalone=\\\"no\\\"?><configuration>\"));\n+    assertTrue(\"Result has proper footer\", result.endsWith(\"</configuration>\"));\n+  }\n+  \n   public void testIncludes() throws Exception {\n     tearDown();\n     System.out.println(\"XXX testIncludes\");",
                "raw_url": "https://github.com/apache/hadoop/raw/160b6fd4966f5189f988eaf0f094867fb2155c04/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "sha": "fc9deef34ff7c4cc1cf51427257e6731420180b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7118. Fix NPE in Configuration.writeXml. Contributed by Todd Lipcon\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1063613 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/448f8dbb9fd9bf2e0ef72dda7bb235915deca94f",
        "patched_files": [
            "CHANGES.java",
            "Configuration.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestConfiguration.java"
        ]
    },
    "hadoop_18a8c24": {
        "bug_id": "hadoop_18a8c24",
        "commit": "https://github.com/apache/hadoop/commit/18a8c2404e10f18e3a0024753d3f8f558fe604af",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/18a8c2404e10f18e3a0024753d3f8f558fe604af/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewer.java?ref=18a8c2404e10f18e3a0024753d3f8f558fe604af",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewer.java",
                "patch": "@@ -778,6 +778,7 @@ public void testDTRonAppSubmission()\n     RMContext mockContext = mock(RMContext.class);\n     when(mockContext.getSystemCredentialsForApps()).thenReturn(\n         new ConcurrentHashMap<ApplicationId, SystemCredentialsForAppsProto>());\n+    when(mockContext.getDispatcher()).thenReturn(dispatcher);\n     ClientRMService mockClientRMService = mock(ClientRMService.class);\n     when(mockContext.getClientRMService()).thenReturn(mockClientRMService);\n     InetSocketAddress sockAddr =\n@@ -837,6 +838,7 @@ public Long answer(InvocationOnMock invocation)\n     RMContext mockContext = mock(RMContext.class);\n     when(mockContext.getSystemCredentialsForApps()).thenReturn(\n         new ConcurrentHashMap<ApplicationId, SystemCredentialsForAppsProto>());\n+    when(mockContext.getDispatcher()).thenReturn(dispatcher);\n     ClientRMService mockClientRMService = mock(ClientRMService.class);         \n     when(mockContext.getClientRMService()).thenReturn(mockClientRMService);    \n     InetSocketAddress sockAddr =                                               \n@@ -1460,6 +1462,7 @@ public void testShutDown() {\n     RMContext mockContext = mock(RMContext.class);\n     when(mockContext.getSystemCredentialsForApps()).thenReturn(\n         new ConcurrentHashMap<ApplicationId, SystemCredentialsForAppsProto>());\n+    when(mockContext.getDispatcher()).thenReturn(dispatcher);\n     ClientRMService mockClientRMService = mock(ClientRMService.class);\n     when(mockContext.getClientRMService()).thenReturn(mockClientRMService);\n     InetSocketAddress sockAddr =\n@@ -1502,6 +1505,7 @@ public void testTokenSequenceNoAfterNewTokenAndRenewal() throws Exception {\n     RMContext mockContext = mock(RMContext.class);\n     when(mockContext.getSystemCredentialsForApps()).thenReturn(\n         new ConcurrentHashMap<ApplicationId, SystemCredentialsForAppsProto>());\n+    when(mockContext.getDispatcher()).thenReturn(dispatcher);\n     ClientRMService mockClientRMService = mock(ClientRMService.class);\n     when(mockContext.getClientRMService()).thenReturn(mockClientRMService);\n     InetSocketAddress sockAddr =",
                "raw_url": "https://github.com/apache/hadoop/raw/18a8c2404e10f18e3a0024753d3f8f558fe604af/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/security/TestDelegationTokenRenewer.java",
                "sha": "ee104b41336520083a8ee0d16cbee4ef32c78fbd",
                "status": "modified"
            }
        ],
        "message": "YARN-9857. TestDelegationTokenRenewer throws NPE but tests pass. Contributed by Ahmed Hussein",
        "parent": "https://github.com/apache/hadoop/commit/1a2a352ecd4cc7c8d71b6bebf52609c5764d2981",
        "patched_files": [
            "DelegationTokenRenewer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationTokenRenewer.java"
        ]
    },
    "hadoop_18d74fe": {
        "bug_id": "hadoop_18d74fe",
        "commit": "https://github.com/apache/hadoop/commit/18d74fe41c0982dc1540367805b0c3d0d4fc29d3",
        "file": [
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/18d74fe41c0982dc1540367805b0c3d0d4fc29d3/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java?ref=18d74fe41c0982dc1540367805b0c3d0d4fc29d3",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java",
                "patch": "@@ -31,6 +31,7 @@\n import java.util.Collections;\n import java.util.Date;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.LinkedHashMap;\n import java.util.LinkedHashSet;\n import java.util.LinkedList;\n@@ -183,6 +184,10 @@ public void close() {\n   @Override\n   public String getNamenodes() {\n     final Map<String, Map<String, Object>> info = new LinkedHashMap<>();\n+    if (membershipStore == null) {\n+      return \"{}\";\n+    }\n+\n     try {\n       // Get the values from the store\n       GetNamenodeRegistrationsRequest request =\n@@ -252,6 +257,9 @@ public String getNameservices() {\n   @Override\n   public String getMountTable() {\n     final List<Map<String, Object>> info = new LinkedList<>();\n+    if (mountTableStore == null) {\n+      return \"[]\";\n+    }\n \n     try {\n       // Get all the mount points in order\n@@ -302,6 +310,9 @@ public String getMountTable() {\n   @Override\n   public String getRouters() {\n     final Map<String, Map<String, Object>> info = new LinkedHashMap<>();\n+    if (routerStore == null) {\n+      return \"{}\";\n+    }\n     try {\n       // Get all the routers in order\n       GetRouterRegistrationsRequest request =\n@@ -391,6 +402,9 @@ public int getNumNameservices() {\n \n   @Override\n   public int getNumNamenodes() {\n+    if (membershipStore == null) {\n+      return 0;\n+    }\n     try {\n       GetNamenodeRegistrationsRequest request =\n           GetNamenodeRegistrationsRequest.newInstance();\n@@ -406,6 +420,9 @@ public int getNumNamenodes() {\n \n   @Override\n   public int getNumExpiredNamenodes() {\n+    if (membershipStore == null) {\n+      return 0;\n+    }\n     try {\n       GetNamenodeRegistrationsRequest request =\n           GetNamenodeRegistrationsRequest.newInstance();\n@@ -670,6 +687,9 @@ private String getSafeModeTip() {\n    */\n   private Collection<String> getNamespaceInfo(\n       Function<FederationNamespaceInfo, String> f) throws IOException {\n+    if (membershipStore == null) {\n+      return new HashSet<>();\n+    }\n     GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance();\n     GetNamespaceInfoResponse response =\n         membershipStore.getNamespaceInfo(request);\n@@ -719,8 +739,11 @@ private long getNameserviceAggregatedLong(ToLongFunction<MembershipStats> f) {\n    */\n   private List<MembershipState> getActiveNamenodeRegistrations()\n       throws IOException {\n-\n     List<MembershipState> resultList = new ArrayList<>();\n+    if (membershipStore == null) {\n+      return resultList;\n+    }\n+\n     GetNamespaceInfoRequest request = GetNamespaceInfoRequest.newInstance();\n     GetNamespaceInfoResponse response =\n         membershipStore.getNamespaceInfo(request);",
                "raw_url": "https://github.com/apache/hadoop/raw/18d74fe41c0982dc1540367805b0c3d0d4fc29d3/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/RBFMetrics.java",
                "sha": "86599c7fbe155cda2b7bc3d8eb77910e7e62a8f2",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/18d74fe41c0982dc1540367805b0c3d0d4fc29d3/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java?ref=18d74fe41c0982dc1540367805b0c3d0d4fc29d3",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java",
                "patch": "@@ -101,6 +101,7 @@\n import org.apache.hadoop.hdfs.server.federation.MockResolver;\n import org.apache.hadoop.hdfs.server.federation.RouterConfigBuilder;\n import org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics;\n+import org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics;\n import org.apache.hadoop.hdfs.server.federation.resolver.FileSubclusterResolver;\n import org.apache.hadoop.hdfs.server.namenode.FSDirectory;\n import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n@@ -1618,6 +1619,27 @@ public Boolean get() {\n     cluster.waitNamenodeRegistration();\n   }\n \n+  @Test\n+  public void testRBFMetricsMethodsRelayOnStateStore() {\n+    assertNull(router.getRouter().getStateStore());\n+\n+    RBFMetrics metrics = router.getRouter().getMetrics();\n+    assertEquals(\"{}\", metrics.getNamenodes());\n+    assertEquals(\"[]\", metrics.getMountTable());\n+    assertEquals(\"{}\", metrics.getRouters());\n+    assertEquals(0, metrics.getNumNamenodes());\n+    assertEquals(0, metrics.getNumExpiredNamenodes());\n+\n+    // These 2 methods relays on {@link RBFMetrics#getNamespaceInfo()}\n+    assertEquals(\"[]\", metrics.getClusterId());\n+    assertEquals(\"[]\", metrics.getBlockPoolId());\n+\n+    // These methods relays on\n+    // {@link RBFMetrics#getActiveNamenodeRegistration()}\n+    assertEquals(\"{}\", metrics.getNameservices());\n+    assertEquals(0, metrics.getNumLiveNodes());\n+  }\n+\n   @Test\n   public void testCacheAdmin() throws Exception {\n     DistributedFileSystem routerDFS = (DistributedFileSystem) routerFS;",
                "raw_url": "https://github.com/apache/hadoop/raw/18d74fe41c0982dc1540367805b0c3d0d4fc29d3/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpc.java",
                "sha": "3767d93d9c4b293972c9bfaad9a7df8345084bd6",
                "status": "modified"
            }
        ],
        "message": "HDFS-14711. RBF: RBFMetrics throws NullPointerException if stateStore disabled. Contributed by Chen Zhang.",
        "parent": "https://github.com/apache/hadoop/commit/fef65b4c2bcd1ea452f3a66eda91e58881b64427",
        "patched_files": [
            "RBFMetrics.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRouterRpc.java",
            "TestRBFMetrics.java"
        ]
    },
    "hadoop_1bea785": {
        "bug_id": "hadoop_1bea785",
        "commit": "https://github.com/apache/hadoop/commit/1bea785020a538115b3e08f41ff88167033d2775",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -839,8 +839,13 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n       new ArrayList<DatanodeStorageInfo>();\n     \n     NumberReplicas numReplicas = new NumberReplicas();\n+    BlockInfo blockInfo = getStoredBlock(block);\n+    if (blockInfo == null) {\n+      out.println(\"Block \"+ block + \" is Null\");\n+      return;\n+    }\n     // source node returned is not used\n-    chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n+    chooseSourceDatanodes(blockInfo, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         new ArrayList<Byte>(), LowRedundancyBlocks.LEVEL);\n     \n@@ -849,7 +854,7 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n     assert containingLiveReplicasNodes.size() >= numReplicas.liveReplicas();\n     int usableReplicas = numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n-    \n+\n     if (block instanceof BlockInfo) {\n       BlockCollection bc = getBlockCollection((BlockInfo)block);\n       String fileName = (bc == null) ? \"[orphaned]\" : bc.getName();\n@@ -1765,8 +1770,8 @@ public void setPostponeBlocksFromFuture(boolean postpone) {\n     this.shouldPostponeBlocksFromFuture  = postpone;\n   }\n \n-\n-  private void postponeBlock(Block blk) {\n+  @VisibleForTesting\n+  void postponeBlock(Block blk) {\n     postponedMisreplicatedBlocks.add(blk);\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "6d142f9e64ae60ac287304a712f94fafb898a7c6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1768,10 +1768,10 @@ public BlocksWithLocations getBlocks(DatanodeID datanode, long size, long\n   void metaSave(String filename) throws IOException {\n     String operationName = \"metaSave\";\n     checkSuperuserPrivilege(operationName);\n-    checkOperation(OperationCategory.UNCHECKED);\n+    checkOperation(OperationCategory.READ);\n     writeLock();\n     try {\n-      checkOperation(OperationCategory.UNCHECKED);\n+      checkOperation(OperationCategory.READ);\n       File file = new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out = new PrintWriter(new BufferedWriter(\n           new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "d0fdbac822803eaa2b336c5e992c482d44afe71d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
                "patch": "@@ -94,6 +94,7 @@\n import org.apache.hadoop.ipc.RemoteException;\n import org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB;\n import org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB;\n+import org.apache.hadoop.ipc.StandbyException;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.RefreshUserMappingsProtocol;\n import org.apache.hadoop.security.SecurityUtil;\n@@ -1537,11 +1538,20 @@ public int metaSave(String[] argv, int idx) throws IOException {\n           nsId, ClientProtocol.class);\n       List<IOException> exceptions = new ArrayList<>();\n       for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n-        try{\n+        try {\n           proxy.getProxy().metaSave(pathname);\n           System.out.println(\"Created metasave file \" + pathname\n               + \" in the log directory of namenode \" + proxy.getAddress());\n-        } catch (IOException ioe){\n+        } catch (RemoteException re) {\n+          Exception unwrapped =  re.unwrapRemoteException(\n+              StandbyException.class);\n+          if (unwrapped instanceof StandbyException) {\n+            System.out.println(\"Skip Standby NameNode, since it cannot perform\"\n+                + \" metasave operation\");\n+          } else {\n+            throw re;\n+          }\n+        } catch (IOException ioe) {\n           System.out.println(\"Created metasave file \" + pathname\n               + \" in the log directory of namenode \" + proxy.getAddress()\n               + \" failed\");",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
                "sha": "d1f83622498f38d5f62a5d2485f1750d87e49aec",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "patch": "@@ -1478,6 +1478,41 @@ private void verifyPlacementPolicy(final MiniDFSCluster cluster,\n     }\n   }\n \n+  /**\n+   * Unit test to check the race condition for adding a Block to\n+   * postponedMisreplicatedBlocks set which may not present in BlockManager\n+   * thus avoiding NullPointerException.\n+   **/\n+  @Test\n+  public void testMetaSavePostponedMisreplicatedBlocks() throws IOException {\n+    bm.postponeBlock(new Block());\n+\n+    File file = new File(\"test.log\");\n+    PrintWriter out = new PrintWriter(file);\n+\n+    bm.metaSave(out);\n+    out.flush();\n+\n+    FileInputStream fstream = new FileInputStream(file);\n+    DataInputStream in = new DataInputStream(fstream);\n+\n+    BufferedReader reader = new BufferedReader(new InputStreamReader(in));\n+    StringBuffer buffer = new StringBuffer();\n+    String line;\n+    try {\n+      while ((line = reader.readLine()) != null) {\n+        buffer.append(line);\n+      }\n+      String output = buffer.toString();\n+      assertTrue(\"Metasave output should not have null block \",\n+          output.contains(\"Block blk_0_0 is Null\"));\n+\n+    } finally {\n+      reader.close();\n+      file.delete();\n+    }\n+  }\n+\n   @Test\n   public void testMetaSaveMissingReplicas() throws Exception {\n     List<DatanodeStorageInfo> origStorages = getStorages(0, 1);",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "sha": "de0e1a69cd3e8d8b2b06c295563e39bc574ca3ee",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java",
                "patch": "@@ -414,16 +414,21 @@ public void testSetNegativeBalancerBandwidth() throws Exception {\n   @Test (timeout = 30000)\n   public void testMetaSave() throws Exception {\n     setUpHaCluster(false);\n+    cluster.getDfsCluster().transitionToActive(0);\n     int exitCode = admin.run(new String[] {\"-metasave\", \"dfs.meta\"});\n     assertEquals(err.toString().trim(), 0, exitCode);\n-    String message = \"Created metasave file dfs.meta in the log directory\"\n-        + \" of namenode.*\";\n-    assertOutputMatches(message + newLine + message + newLine);\n+    String messageFromActiveNN = \"Created metasave file dfs.meta \"\n+        + \"in the log directory of namenode.*\";\n+    String messageFromStandbyNN = \"Skip Standby NameNode, since it \"\n+        + \"cannot perform metasave operation\";\n+    assertOutputMatches(messageFromActiveNN + newLine +\n+        messageFromStandbyNN + newLine);\n   }\n \n   @Test (timeout = 30000)\n   public void testMetaSaveNN1UpNN2Down() throws Exception {\n     setUpHaCluster(false);\n+    cluster.getDfsCluster().transitionToActive(0);\n     cluster.getDfsCluster().shutdownNameNode(1);\n     int exitCode = admin.run(new String[] {\"-metasave\", \"dfs.meta\"});\n     assertNotEquals(err.toString().trim(), 0, exitCode);\n@@ -437,6 +442,7 @@ public void testMetaSaveNN1UpNN2Down() throws Exception {\n   @Test (timeout = 30000)\n   public void testMetaSaveNN1DownNN2Up() throws Exception {\n     setUpHaCluster(false);\n+    cluster.getDfsCluster().transitionToActive(1);\n     cluster.getDfsCluster().shutdownNameNode(0);\n     int exitCode = admin.run(new String[] {\"-metasave\", \"dfs.meta\"});\n     assertNotEquals(err.toString().trim(), 0, exitCode);",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java",
                "sha": "627ea0710df58272a12200e29d562f0201bfef56",
                "status": "modified"
            }
        ],
        "message": "HDFS-14081. hdfs dfsadmin -metasave metasave_test results NPE. Contributed by Shweta Yakkali.\n\nSigned-off-by: Wei-Chiu Chuang <weichiu@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/f5b4e0f971b138666a1f7015f387ae960f85d589",
        "patched_files": [
            "BlockManager.java",
            "FSNamesystem.java",
            "DFSAdmin.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSAdminWithHA.java",
            "TestBlockManager.java",
            "TestDFSAdmin.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_1c05393": {
        "bug_id": "hadoop_1c05393",
        "commit": "https://github.com/apache/hadoop/commit/1c05393b51748033279bff31dbc5c5cae7fc3a86",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=1c05393b51748033279bff31dbc5c5cae7fc3a86",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2415,6 +2415,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9467. Fix data race accessing writeLockHeldTimeStamp in FSNamesystem.\n     (Mingliang Liu via jing9)\n \n+    HDFS-9336. deleteSnapshot throws NPE when snapshotname is null.\n+    (Brahma Reddy Battula via aajisaka)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "bf77e73fe4bf597caef76013216626ab3a75dd47",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=1c05393b51748033279bff31dbc5c5cae7fc3a86",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -1646,6 +1646,9 @@ public String createSnapshot(String snapshotRoot, String snapshotName)\n   public void deleteSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n     checkNNStartup();\n+    if (snapshotName == null || snapshotName.isEmpty()) {\n+      throw new IOException(\"The snapshot name is null or empty.\");\n+    }\n     namesystem.checkOperation(OperationCategory.WRITE);\n     metrics.incrDeleteSnapshotOps();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);",
                "raw_url": "https://github.com/apache/hadoop/raw/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "490f3e403e9534f33f2dc70db779fbf44752f4f9",
                "status": "modified"
            },
            {
                "additions": 84,
                "blob_url": "https://github.com/apache/hadoop/blob/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java?ref=1c05393b51748033279bff31dbc5c5cae7fc3a86",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "patch": "@@ -0,0 +1,84 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.UnresolvedLinkException;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class TestNameNodeRpcServerMethods {\n+  private static NamenodeProtocols nnRpc;\n+  private static Configuration conf;\n+  private static MiniDFSCluster cluster;\n+\n+  /** Start a cluster */\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new HdfsConfiguration();\n+    cluster = new MiniDFSCluster.Builder(conf).build();\n+    cluster.waitActive();\n+    nnRpc = cluster.getNameNode().getRpcServer();\n+  }\n+\n+  /**\n+   * Cleanup after the test\n+   *\n+   * @throws IOException\n+   * @throws UnresolvedLinkException\n+   * @throws SafeModeException\n+   * @throws AccessControlException\n+   */\n+  @After\n+  public void cleanup() throws IOException {\n+    if (cluster != null)\n+      cluster.shutdown();\n+  }\n+\n+  @Test\n+  public void testDeleteSnapshotWhenSnapshotNameIsEmpty() throws Exception {\n+    String dir = \"/testNamenodeRetryCache/testDelete\";\n+    try {\n+      nnRpc.deleteSnapshot(dir, null);\n+      Assert.fail(\"testdeleteSnapshot is not thrown expected exception \");\n+    } catch (IOException e) {\n+      // expected\n+      GenericTestUtils.assertExceptionContains(\n+          \"The snapshot name is null or empty.\", e);\n+    }\n+    try {\n+      nnRpc.deleteSnapshot(dir, \"\");\n+      Assert.fail(\"testdeleteSnapshot is not thrown expected exception\");\n+    } catch (IOException e) {\n+      // expected\n+      GenericTestUtils.assertExceptionContains(\n+          \"The snapshot name is null or empty.\", e);\n+    }\n+\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "sha": "80d97224d6d3babc3111cc98e0330b1d42def52c",
                "status": "added"
            }
        ],
        "message": "HDFS-9336. deleteSnapshot throws NPE when snapshotname is null. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/9b8e50b424d060e16c1175b1811e7abc476e2468",
        "patched_files": [
            "CHANGES.java",
            "NameNodeRpcServer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeRpcServerMethods.java",
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop_1c93025": {
        "bug_id": "hadoop_1c93025",
        "commit": "https://github.com/apache/hadoop/commit/1c93025a1b370db46e345161dbc15e03f829823f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=1c93025a1b370db46e345161dbc15e03f829823f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -524,6 +524,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2624. Resource Localization fails on a cluster due to existing cache\n     directories (Anubhav Dhoot via jlowe)\n \n+    YARN-2527. Fixed the potential NPE in ApplicationACLsManager and added test\n+    cases for it. (Benoy Antony via zjshen)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/CHANGES.txt",
                "sha": "247167399d4b7679245d33f5ba8c1a86b1c8fd8a",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java?ref=1c93025a1b370db46e345161dbc15e03f829823f",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java",
                "patch": "@@ -41,6 +41,8 @@\n   private static final Log LOG = LogFactory\n       .getLog(ApplicationACLsManager.class);\n \n+  private static AccessControlList DEFAULT_YARN_APP_ACL \n+    = new AccessControlList(YarnConfiguration.DEFAULT_YARN_APP_ACL);\n   private final Configuration conf;\n   private final AdminACLsManager adminAclsManager;\n   private final ConcurrentMap<ApplicationId, Map<ApplicationAccessType, AccessControlList>> applicationACLS\n@@ -100,18 +102,26 @@ public boolean checkAccess(UserGroupInformation callerUGI,\n     if (!areACLsEnabled()) {\n       return true;\n     }\n-\n-    AccessControlList applicationACL = this.applicationACLS\n-        .get(applicationId).get(applicationAccessType);\n-    if (applicationACL == null) {\n+    AccessControlList applicationACL = DEFAULT_YARN_APP_ACL;\n+    Map<ApplicationAccessType, AccessControlList> acls = this.applicationACLS\n+        .get(applicationId);\n+    if (acls == null) {\n       if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"ACL not found for application \"\n+            + applicationId + \" owned by \"\n+            + applicationOwner + \". Using default [\"\n+            + YarnConfiguration.DEFAULT_YARN_APP_ACL + \"]\");\n+      }\n+    } else {\n+      AccessControlList applicationACLInMap = acls.get(applicationAccessType);\n+      if (applicationACLInMap != null) {\n+        applicationACL = applicationACLInMap;\n+      } else if (LOG.isDebugEnabled()) {\n         LOG.debug(\"ACL not found for access-type \" + applicationAccessType\n             + \" for application \" + applicationId + \" owned by \"\n             + applicationOwner + \". Using default [\"\n             + YarnConfiguration.DEFAULT_YARN_APP_ACL + \"]\");\n       }\n-      applicationACL =\n-          new AccessControlList(YarnConfiguration.DEFAULT_YARN_APP_ACL);\n     }\n \n     // Allow application-owner for any type of access on the application",
                "raw_url": "https://github.com/apache/hadoop/raw/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java",
                "sha": "e8e3cb56b66f4534335365c47bf86618c9594f6d",
                "status": "modified"
            },
            {
                "additions": 180,
                "blob_url": "https://github.com/apache/hadoop/blob/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java",
                "changes": 180,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java?ref=1c93025a1b370db46e345161dbc15e03f829823f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java",
                "patch": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.yarn.server.security;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.api.records.ApplicationAccessType;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.junit.Test;\n+\n+public class TestApplicationACLsManager {\n+\n+  private static final String ADMIN_USER = \"adminuser\";\n+  private static final String APP_OWNER = \"appuser\";\n+  private static final String TESTUSER1 = \"testuser1\";\n+  private static final String TESTUSER2 = \"testuser2\";\n+  private static final String TESTUSER3 = \"testuser3\";\n+\n+  @Test\n+  public void testCheckAccess() {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE,\n+        true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL,\n+        ADMIN_USER);\n+    ApplicationACLsManager aclManager = new ApplicationACLsManager(conf);\n+    Map<ApplicationAccessType, String> aclMap = \n+        new HashMap<ApplicationAccessType, String>();\n+    aclMap.put(ApplicationAccessType.VIEW_APP, TESTUSER1 + \",\" + TESTUSER3);\n+    aclMap.put(ApplicationAccessType.MODIFY_APP, TESTUSER1);\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    aclManager.addApplication(appId, aclMap);\n+\n+    //User in ACL, should be allowed access\n+    UserGroupInformation testUser1 = UserGroupInformation\n+        .createRemoteUser(TESTUSER1);\n+    assertTrue(aclManager.checkAccess(testUser1, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(testUser1, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //User NOT in ACL, should not be allowed access\n+    UserGroupInformation testUser2 = UserGroupInformation\n+        .createRemoteUser(TESTUSER2);\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //User has View access, but not modify access\n+    UserGroupInformation testUser3 = UserGroupInformation\n+        .createRemoteUser(TESTUSER3);\n+    assertTrue(aclManager.checkAccess(testUser3, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser3, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //Application Owner should have all access\n+    UserGroupInformation appOwner = UserGroupInformation\n+        .createRemoteUser(APP_OWNER);\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //Admin should have all access\n+    UserGroupInformation adminUser = UserGroupInformation\n+        .createRemoteUser(ADMIN_USER);\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+  }\n+\n+  @Test\n+  public void testCheckAccessWithNullACLS() {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE,\n+        true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL,\n+        ADMIN_USER);\n+    ApplicationACLsManager aclManager = new ApplicationACLsManager(conf);\n+    UserGroupInformation appOwner = UserGroupInformation\n+        .createRemoteUser(APP_OWNER);\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    //Application ACL is not added\n+\n+    //Application Owner should have all access even if Application ACL is not added\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+\n+    //Admin should have all access\n+    UserGroupInformation adminUser = UserGroupInformation\n+        .createRemoteUser(ADMIN_USER);\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    // A regular user should Not have access\n+    UserGroupInformation testUser1 = UserGroupInformation\n+        .createRemoteUser(TESTUSER1);\n+    assertFalse(aclManager.checkAccess(testUser1, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser1, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+  }\n+  \n+  @Test\n+  public void testCheckAccessWithPartialACLS() {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE,\n+        true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL,\n+        ADMIN_USER);\n+    ApplicationACLsManager aclManager = new ApplicationACLsManager(conf);\n+    UserGroupInformation appOwner = UserGroupInformation\n+        .createRemoteUser(APP_OWNER);\n+    // Add only the VIEW ACLS\n+    Map<ApplicationAccessType, String> aclMap = \n+        new HashMap<ApplicationAccessType, String>();\n+    aclMap.put(ApplicationAccessType.VIEW_APP, TESTUSER1 );\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    aclManager.addApplication(appId, aclMap);\n+\n+    //Application Owner should have all access even if Application ACL is not added\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+\n+    //Admin should have all access\n+    UserGroupInformation adminUser = UserGroupInformation\n+        .createRemoteUser(ADMIN_USER);\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    // testuser1 should  have view access only\n+    UserGroupInformation testUser1 = UserGroupInformation\n+        .createRemoteUser(TESTUSER1);\n+    assertTrue(aclManager.checkAccess(testUser1, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser1, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+    \n+    // A testuser2 should Not have access\n+    UserGroupInformation testUser2 = UserGroupInformation\n+        .createRemoteUser(TESTUSER2);\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java",
                "sha": "2db1da90416ab4fd4d6f4659cdda38fff90f0ba5",
                "status": "added"
            }
        ],
        "message": "YARN-2527. Fixed the potential NPE in ApplicationACLsManager and added test cases for it. Contributed by Benoy Antony.",
        "parent": "https://github.com/apache/hadoop/commit/f679ca38ce0365c97f1dba79e333a8de18733b8a",
        "patched_files": [
            "CHANGES.java",
            "ApplicationACLsManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationACLsManager.java"
        ]
    },
    "hadoop_1fbb662": {
        "bug_id": "hadoop_1fbb662",
        "commit": "https://github.com/apache/hadoop/commit/1fbb662c7092d08a540acff7e92715693412e486",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/1fbb662c7092d08a540acff7e92715693412e486/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=1fbb662c7092d08a540acff7e92715693412e486",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -4487,8 +4487,12 @@ private void scanAndCompactStorages() throws InterruptedException {\n         for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n           namesystem.writeLock();\n           try {\n-            DatanodeStorageInfo storage = datanodeManager.\n-                getDatanode(datanodesAndStorages.get(i)).\n+            final DatanodeDescriptor dn = datanodeManager.\n+                getDatanode(datanodesAndStorages.get(i));\n+            if (dn == null) {\n+              continue;\n+            }\n+            final DatanodeStorageInfo storage = dn.\n                 getStorageInfo(datanodesAndStorages.get(i + 1));\n             if (storage != null) {\n               boolean aborted =",
                "raw_url": "https://github.com/apache/hadoop/raw/1fbb662c7092d08a540acff7e92715693412e486/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "e83cbc6ef4a266939307a3b13b46c9b5765d07c9",
                "status": "modified"
            }
        ],
        "message": "HDFS-12363. Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages. Contributed by Xiao Chen",
        "parent": "https://github.com/apache/hadoop/commit/7ecc6dbed62c80397f71949bee41dcd03065755c",
        "patched_files": [
            "BlockManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_2044967": {
        "bug_id": "hadoop_2044967",
        "commit": "https://github.com/apache/hadoop/commit/2044967e7581f00c3f6378860426a69078faf694",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/2044967e7581f00c3f6378860426a69078faf694/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java?ref=2044967e7581f00c3f6378860426a69078faf694",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "patch": "@@ -72,6 +72,10 @@ public void setClient(YarnClient client) {\n   }\n \n   public void stop() {\n-    this.client.stop();\n+    // this.client may be null when it is called before\n+    // invoking `createAndStartYarnClient`\n+    if (this.client != null) {\n+      this.client.stop();\n+    }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/2044967e7581f00c3f6378860426a69078faf694/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "sha": "c1e02d5fd1ea0ff54c0cb35de9498a5f92276488",
                "status": "modified"
            }
        ],
        "message": "YARN-9246 NPE when executing a command yarn node -status or -states without additional arguments. Contributed by Masahiro Tanaka",
        "parent": "https://github.com/apache/hadoop/commit/194f0b49fb8ae3b876dde82f04f906f0ab0f5bdf",
        "patched_files": [
            "YarnCLI.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestYarnCLI.java"
        ]
    },
    "hadoop_209b169": {
        "bug_id": "hadoop_209b169",
        "commit": "https://github.com/apache/hadoop/commit/209b1699fcd150676d4cc47e8e817796086c1986",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=209b1699fcd150676d4cc47e8e817796086c1986",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -435,6 +435,9 @@ Release 2.6.0 - UNRELEASED\n     MAPREDUCE-5873. Shuffle bandwidth computation includes time spent waiting\n     for maps (Siqi Li via jlowe)\n \n+    MAPREDUCE-5542. Killing a job just as it finishes can generate an NPE in\n+    client (Rohith via jlowe)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "e152b480bc15917abc91ec151811a34eeb5e03a5",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hadoop/blob/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java?ref=209b1699fcd150676d4cc47e8e817796086c1986",
                "deletions": 21,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                "patch": "@@ -594,16 +594,50 @@ public JobStatus getJobStatus(JobID jobID) throws IOException,\n         .getTaskReports(jobID, taskType);\n   }\n \n+  private void killUnFinishedApplication(ApplicationId appId)\n+      throws IOException {\n+    ApplicationReport application = null;\n+    try {\n+      application = resMgrDelegate.getApplicationReport(appId);\n+    } catch (YarnException e) {\n+      throw new IOException(e);\n+    }\n+    if (application.getYarnApplicationState() == YarnApplicationState.FINISHED\n+        || application.getYarnApplicationState() == YarnApplicationState.FAILED\n+        || application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n+      return;\n+    }\n+    killApplication(appId);\n+  }\n+\n+  private void killApplication(ApplicationId appId) throws IOException {\n+    try {\n+      resMgrDelegate.killApplication(appId);\n+    } catch (YarnException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  private boolean isJobInTerminalState(JobStatus status) {\n+    return status.getState() == JobStatus.State.KILLED\n+        || status.getState() == JobStatus.State.FAILED\n+        || status.getState() == JobStatus.State.SUCCEEDED;\n+  }\n+\n   @Override\n   public void killJob(JobID arg0) throws IOException, InterruptedException {\n     /* check if the status is not running, if not send kill to RM */\n     JobStatus status = clientCache.getClient(arg0).getJobStatus(arg0);\n+    ApplicationId appId = TypeConverter.toYarn(arg0).getAppId();\n+\n+    // get status from RM and return\n+    if (status == null) {\n+      killUnFinishedApplication(appId);\n+      return;\n+    }\n+\n     if (status.getState() != JobStatus.State.RUNNING) {\n-      try {\n-        resMgrDelegate.killApplication(TypeConverter.toYarn(arg0).getAppId());\n-      } catch (YarnException e) {\n-        throw new IOException(e);\n-      }\n+      killApplication(appId);\n       return;\n     }\n \n@@ -612,26 +646,26 @@ public void killJob(JobID arg0) throws IOException, InterruptedException {\n       clientCache.getClient(arg0).killJob(arg0);\n       long currentTimeMillis = System.currentTimeMillis();\n       long timeKillIssued = currentTimeMillis;\n-      while ((currentTimeMillis < timeKillIssued + 10000L) && (status.getState()\n-          != JobStatus.State.KILLED)) {\n-          try {\n-            Thread.sleep(1000L);\n-          } catch(InterruptedException ie) {\n-            /** interrupted, just break */\n-            break;\n-          }\n-          currentTimeMillis = System.currentTimeMillis();\n-          status = clientCache.getClient(arg0).getJobStatus(arg0);\n+      while ((currentTimeMillis < timeKillIssued + 10000L)\n+          && !isJobInTerminalState(status)) {\n+        try {\n+          Thread.sleep(1000L);\n+        } catch (InterruptedException ie) {\n+          /** interrupted, just break */\n+          break;\n+        }\n+        currentTimeMillis = System.currentTimeMillis();\n+        status = clientCache.getClient(arg0).getJobStatus(arg0);\n+        if (status == null) {\n+          killUnFinishedApplication(appId);\n+          return;\n+        }\n       }\n     } catch(IOException io) {\n       LOG.debug(\"Error when checking for application status\", io);\n     }\n-    if (status.getState() != JobStatus.State.KILLED) {\n-      try {\n-        resMgrDelegate.killApplication(TypeConverter.toYarn(arg0).getAppId());\n-      } catch (YarnException e) {\n-        throw new IOException(e);\n-      }\n+    if (status != null && !isJobInTerminalState(status)) {\n+      killApplication(appId);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                "sha": "40ef9822783ba3e00505a4de90b01509e17c4541",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java?ref=209b1699fcd150676d4cc47e8e817796086c1986",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java",
                "patch": "@@ -86,6 +86,7 @@\n import org.apache.hadoop.yarn.api.records.ApplicationReport;\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.QueueInfo;\n import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;\n@@ -188,6 +189,16 @@ public ClientServiceDelegate answer(InvocationOnMock invocation)\n             State.RUNNING, JobPriority.HIGH, \"tmp\", \"tmp\", \"tmp\", \"tmp\"));\n     yarnRunner.killJob(jobId);\n     verify(clientDelegate).killJob(jobId);\n+\n+    when(clientDelegate.getJobStatus(any(JobID.class))).thenReturn(null);\n+    when(resourceMgrDelegate.getApplicationReport(any(ApplicationId.class)))\n+        .thenReturn(\n+            ApplicationReport.newInstance(appId, null, \"tmp\", \"tmp\", \"tmp\",\n+                \"tmp\", 0, null, YarnApplicationState.FINISHED, \"tmp\", \"tmp\",\n+                0l, 0l, FinalApplicationStatus.SUCCEEDED, null, null, 0f,\n+                \"tmp\", null));\n+    yarnRunner.killJob(jobId);\n+    verify(clientDelegate).killJob(jobId);\n   }\n \n   @Test(timeout=20000)",
                "raw_url": "https://github.com/apache/hadoop/raw/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java",
                "sha": "420a95f9bbfcf114379518d6ed8c77fcabc6a6e6",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5542. Killing a job just as it finishes can generate an NPE in client. Contributed by Rohith",
        "parent": "https://github.com/apache/hadoop/commit/a6aa6e42cacdbfcc1c2b7c19e7239204fe9ff654",
        "patched_files": [
            "CHANGES.java",
            "YARNRunner.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestYARNRunner.java"
        ]
    },
    "hadoop_20e9ce3": {
        "bug_id": "hadoop_20e9ce3",
        "commit": "https://github.com/apache/hadoop/commit/20e9ce3ab33599d2ac6859a8319e1ce0bf0a4363",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/20e9ce3ab33599d2ac6859a8319e1ce0bf0a4363/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java?ref=20e9ce3ab33599d2ac6859a8319e1ce0bf0a4363",
                "deletions": 12,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "patch": "@@ -55,18 +55,12 @@\n    */\n   public SerializationFactory(Configuration conf) {\n     super(conf);\n-    if (conf.get(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY).equals(\"\")) {\n-      LOG.warn(\"Serialization for various data types may not be available. Please configure \"\n-          + CommonConfigurationKeys.IO_SERIALIZATIONS_KEY\n-          + \" properly to have serialization support (it is currently not set).\");\n-    } else {\n-      for (String serializerName : conf.getTrimmedStrings(\n-          CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, new String[] {\n-              WritableSerialization.class.getName(),\n-              AvroSpecificSerialization.class.getName(),\n-              AvroReflectSerialization.class.getName() })) {\n-        add(conf, serializerName);\n-      }\n+    for (String serializerName : conf.getTrimmedStrings(\n+            CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,\n+            new String[]{WritableSerialization.class.getName(),\n+                    AvroSpecificSerialization.class.getName(),\n+                    AvroReflectSerialization.class.getName()})) {\n+      add(conf, serializerName);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/20e9ce3ab33599d2ac6859a8319e1ce0bf0a4363/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "sha": "ce0c3fe398eed8273e4d85eff45c24f3b585dad4",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/20e9ce3ab33599d2ac6859a8319e1ce0bf0a4363/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java?ref=20e9ce3ab33599d2ac6859a8319e1ce0bf0a4363",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "patch": "@@ -51,6 +51,19 @@ public void testSerializationKeyIsEmpty() {\n     SerializationFactory factory = new SerializationFactory(conf);\n   }\n \n+  /**\n+   * Test the case when {@code IO_SERIALIZATIONS_KEY}\n+   * is not set at all, because something unset this key.\n+   * This shouldn't result in any error, the defaults present\n+   * in construction should be used in this case.\n+   */\n+  @Test\n+  public void testSerializationKeyIsUnset() {\n+    Configuration conf = new Configuration();\n+    conf.unset(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY);\n+    SerializationFactory factory = new SerializationFactory(conf);\n+  }\n+\n   @Test\n   public void testSerializationKeyIsInvalid() {\n     Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop/raw/20e9ce3ab33599d2ac6859a8319e1ce0bf0a4363/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "sha": "d9a00090689fb76335d81fb442951fcfc69c0613",
                "status": "modified"
            }
        ],
        "message": "HADOOP-14459. SerializationFactory shouldn't throw a NullPointerException if the serializations list is not defined\n(Contributed by Nandor Kollar via Daniel Templeton)",
        "parent": "https://github.com/apache/hadoop/commit/acf5b880d8283f5a96455c89a6b1b548d0d8c0e7",
        "patched_files": [
            "SerializationFactory.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSerializationFactory.java"
        ]
    },
    "hadoop_229472c": {
        "bug_id": "hadoop_229472c",
        "commit": "https://github.com/apache/hadoop/commit/229472cea7920194c48f5294bf763a8bee2ade63",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=229472cea7920194c48f5294bf763a8bee2ade63",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -439,6 +439,9 @@ Release 2.3.0 - UNRELEASED\n \n     HADOOP-10100. MiniKDC shouldn't use apacheds-all artifact. (rkanter via tucu)\n \n+    HADOOP-10107. Server.getNumOpenConnections may throw NPE. (Kihwal Lee via\n+    jing9)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2b9aeb88a5a759ab34e1eccab28d261d53a65826",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=229472cea7920194c48f5294bf763a8bee2ade63",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2109,6 +2109,7 @@ protected Server(String bindAddress, int port,\n     // Start the listener here and let it bind to the port\n     listener = new Listener();\n     this.port = listener.getAddress().getPort();    \n+    connectionManager = new ConnectionManager();\n     this.rpcMetrics = RpcMetrics.create(this);\n     this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port);\n     this.tcpNoDelay = conf.getBoolean(\n@@ -2117,7 +2118,6 @@ protected Server(String bindAddress, int port,\n \n     // Create the responder here\n     responder = new Responder();\n-    connectionManager = new ConnectionManager();\n     \n     if (secretManager != null) {\n       SaslRpcServer.init(conf);",
                "raw_url": "https://github.com/apache/hadoop/raw/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "7fb395cdb056029d85a7940a7e7fbedd941a5a4c",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10107. Server.getNumOpenConnections may throw NPE. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543335 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/aa002344d0466a62672eae73cdb2bb2ae7c19a72",
        "patched_files": [
            "Server.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop_22d7d1f": {
        "bug_id": "hadoop_22d7d1f",
        "commit": "https://github.com/apache/hadoop/commit/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java?ref=22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "patch": "@@ -422,6 +422,10 @@ public GetSubClusterPoliciesConfigurationsResponse getPoliciesConfigurations(\n     try {\n       for (String child : zkManager.getChildren(policiesZNode)) {\n         SubClusterPolicyConfiguration policy = getPolicy(child);\n+        if (policy == null) {\n+          LOG.warn(\"Policy for queue: {} does not exist.\", child);\n+          continue;\n+        }\n         result.add(policy);\n       }\n     } catch (Exception e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "sha": "39f498c73a21e8bc4b8804952741b83590aeb91a",
                "status": "modified"
            }
        ],
        "message": "YARN-9601.Potential NPE in ZookeeperFederationStateStore#getPoliciesConfigurations (#908) Contributed by hunshenshi.",
        "parent": "https://github.com/apache/hadoop/commit/b0131bc265453051820e54908e70d39433c227ab",
        "patched_files": [
            "ZookeeperFederationStateStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestZookeeperFederationStateStore.java"
        ]
    },
    "hadoop_23248f6": {
        "bug_id": "hadoop_23248f6",
        "commit": "https://github.com/apache/hadoop/commit/23248f63aab74a19dba38d348f2b231c8360770a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -305,9 +305,7 @@ private boolean checkAccess(UserGroupInformation callerUGI, String owner,\n     return applicationsACLsManager\n         .checkAccess(callerUGI, operationPerformed, owner,\n             application.getApplicationId()) || queueACLsManager\n-        .checkAccess(callerUGI, QueueACL.ADMINISTER_QUEUE,\n-            application.getQueue(), application.getApplicationId(),\n-            application.getName());\n+        .checkAccess(callerUGI, QueueACL.ADMINISTER_QUEUE, application);\n   }\n \n   ApplicationId getNewApplicationId() {",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "c2c24b94cc755fa0cfdd5e34ebebcb4e7f68cf54",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java",
                "patch": "@@ -18,20 +18,27 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager.security;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.UserGroupInformation;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.QueueACL;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.security.AccessRequest;\n import org.apache.hadoop.yarn.security.YarnAuthorizationProvider;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n \n import com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler;\n \n public class QueueACLsManager {\n+\n+  private static final Log LOG = LogFactory.getLog(QueueACLsManager.class);\n+\n   private ResourceScheduler scheduler;\n   private boolean isACLsEnable;\n   private YarnAuthorizationProvider authorizer;\n@@ -49,17 +56,28 @@ public QueueACLsManager(ResourceScheduler scheduler, Configuration conf) {\n   }\n \n   public boolean checkAccess(UserGroupInformation callerUGI, QueueACL acl,\n-      String queueName, ApplicationId appId, String appName) {\n+      RMApp app) {\n     if (!isACLsEnable) {\n       return true;\n     }\n+\n     if (scheduler instanceof CapacityScheduler) {\n-      return authorizer.checkPermission(new AccessRequest(\n-          ((CapacityScheduler) scheduler).getQueue(queueName)\n-              .getPrivilegedEntity(), callerUGI,\n-          SchedulerUtils.toAccessType(acl), appId.toString(), appName));\n+      CSQueue queue = ((CapacityScheduler) scheduler).getQueue(app.getQueue());\n+      if (queue == null) {\n+        // Application exists but the associated queue does not exist.\n+        // This may be due to queue is removed after RM restarts. Here, we choose\n+        // to allow users to be able to view the apps for removed queue.\n+        LOG.error(\"Queue \" + app.getQueue() + \" does not exist for \" + app\n+            .getApplicationId());\n+        return true;\n+      }\n+\n+      return authorizer.checkPermission(\n+          new AccessRequest(queue.getPrivilegedEntity(), callerUGI,\n+              SchedulerUtils.toAccessType(acl),\n+              app.getApplicationId().toString(), app.getName()));\n     } else {\n-      return scheduler.checkAccess(callerUGI, acl, queueName);\n+      return scheduler.checkAccess(callerUGI, acl, app.getQueue());\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java",
                "sha": "15c7052ff004b841f9b87277714fbe9a127b0754",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "patch": "@@ -232,8 +232,7 @@ protected Boolean hasAccess(RMApp app, HttpServletRequest hsr) {\n               ApplicationAccessType.VIEW_APP, app.getUser(),\n               app.getApplicationId()) ||\n             this.rm.getQueueACLsManager().checkAccess(callerUGI,\n-                QueueACL.ADMINISTER_QUEUE, app.getQueue(),\n-                app.getApplicationId(), app.getName()))) {\n+                QueueACL.ADMINISTER_QUEUE, app))) {\n       return false;\n     }\n     return true;",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "sha": "dc8c6ab7cc18d7c7474833a1b7f45b2381333233",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java",
                "patch": "@@ -21,7 +21,6 @@\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n import static org.mockito.Matchers.any;\n-import static org.mockito.Matchers.anyString;\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n@@ -30,6 +29,7 @@\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.junit.Assert;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -112,8 +112,7 @@ protected QueueACLsManager createQueueACLsManager(\n           Configuration conf) {\n         QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n         when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-            any(QueueACL.class), anyString(), any(ApplicationId.class),\n-            anyString())).thenAnswer(new Answer() {\n+            any(QueueACL.class), any(RMApp.class))).thenAnswer(new Answer() {\n           public Object answer(InvocationOnMock invocation) {\n             return isQueueUser;\n           }",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java",
                "sha": "d6d697ece0375f91ea911bebf7c1fad374328bfe",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "patch": "@@ -473,8 +473,7 @@ public void handle(Event event) {\n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(\n         mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-            any(QueueACL.class), anyString(), any(ApplicationId.class),\n-            anyString())).thenReturn(true);\n+            any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     return new ClientRMService(rmContext, yarnScheduler, appManager,\n         mockAclsManager, mockQueueACLsManager, null);\n   }\n@@ -575,8 +574,7 @@ public void testGetQueueInfo() throws Exception {\n     ApplicationACLsManager mockAclsManager = mock(ApplicationACLsManager.class);\n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(true);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     when(mockAclsManager.checkAccess(any(UserGroupInformation.class),\n         any(ApplicationAccessType.class), anyString(),\n         any(ApplicationId.class))).thenReturn(true);\n@@ -602,8 +600,7 @@ public void testGetQueueInfo() throws Exception {\n     QueueACLsManager mockQueueACLsManager1 =\n         mock(QueueACLsManager.class);\n     when(mockQueueACLsManager1.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(false);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(false);\n     when(mockAclsManager1.checkAccess(any(UserGroupInformation.class),\n         any(ApplicationAccessType.class), anyString(),\n         any(ApplicationId.class))).thenReturn(false);\n@@ -642,8 +639,7 @@ public void handle(Event event) {}\n \n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(true);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     ClientRMService rmService =\n         new ClientRMService(rmContext, yarnScheduler, appManager,\n             mockAclsManager, mockQueueACLsManager, null);\n@@ -731,8 +727,7 @@ public void handle(Event event) {}\n     ApplicationACLsManager mockAclsManager = mock(ApplicationACLsManager.class);\n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(true);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     ClientRMService rmService =\n         new ClientRMService(rmContext, yarnScheduler, appManager,\n             mockAclsManager, mockQueueACLsManager, null);",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "sha": "8a6ddae29bcc3379fab95a3420f9348b4efe4482",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 18,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "patch": "@@ -18,23 +18,7 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n-import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.when;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.net.UnknownHostException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.concurrent.TimeUnit;\n-\n+import com.google.common.base.Supplier;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;\n@@ -99,7 +83,22 @@\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n-import com.google.common.base.Supplier;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.UnknownHostException;\n+import java.security.PrivilegedAction;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.junit.Assert.*;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n public class TestWorkPreservingRMRestart extends ParameterizedSchedulerTestBase {\n@@ -564,6 +563,43 @@ private void setupQueueConfigurationChildOfB(CapacitySchedulerConfiguration conf\n         .MAXIMUM_APPLICATION_MASTERS_RESOURCE_PERCENT, 0.5f);\n   }\n \n+  // 1. submit an app to default queue and let it finish\n+  // 2. restart rm with no default queue\n+  // 3. getApplicationReport call should succeed (with no NPE)\n+  @Test (timeout = 30000)\n+  public void testRMRestartWithRemovedQueue() throws Exception{\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE, true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL, \"\");\n+    MemoryRMStateStore memStore = new MemoryRMStateStore();\n+    memStore.init(conf);\n+    rm1 = new MockRM(conf, memStore);\n+    rm1.start();\n+    MockNM nm1 =\n+        new MockNM(\"127.0.0.1:1234\", 8192, rm1.getResourceTrackerService());\n+    nm1.registerNode();\n+    final RMApp app1 = rm1.submitApp(1024, \"app1\", USER_1, null);\n+    MockAM am1 = MockRM.launchAndRegisterAM(app1,rm1, nm1);\n+    MockRM.finishAMAndVerifyAppState(app1, rm1, nm1, am1);\n+\n+    CapacitySchedulerConfiguration csConf = new CapacitySchedulerConfiguration(conf);\n+    csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[]{QUEUE_DOESNT_EXIST});\n+    final String noQueue = CapacitySchedulerConfiguration.ROOT + \".\" + QUEUE_DOESNT_EXIST;\n+    csConf.setCapacity(noQueue, 100);\n+    rm2 = new MockRM(csConf,memStore);\n+\n+    rm2.start();\n+    UserGroupInformation user2 = UserGroupInformation.createRemoteUser(\"user2\");\n+\n+    ApplicationReport report =\n+        user2.doAs(new PrivilegedExceptionAction<ApplicationReport>() {\n+          @Override\n+          public ApplicationReport run() throws Exception {\n+            return rm2.getApplicationReport(app1.getApplicationId());\n+          }\n+    });\n+    Assert.assertNotNull(report);\n+  }\n+\n   // Test CS recovery with multi-level queues and multi-users:\n   // 1. setup 2 NMs each with 8GB memory;\n   // 2. setup 2 level queues: Default -> (QueueA, QueueB)",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "sha": "b6d6f691e60685bdff6c0963d83e26ea9f987ce6",
                "status": "modified"
            }
        ],
        "message": "getApplicationReport call may raise NPE for removed queues. (Jian He via wangda)",
        "parent": "https://github.com/apache/hadoop/commit/c9bb96fa81fc925e33ccc0b02c98cc2d929df120",
        "patched_files": [
            "QueueACLsManager.java",
            "ClientRMService.java",
            "RMWebServices.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationACLs.java",
            "TestClientRMService.java",
            "TestRMWebServices.java",
            "TestWorkPreservingRMRestart.java"
        ]
    },
    "hadoop_251f528": {
        "bug_id": "hadoop_251f528",
        "commit": "https://github.com/apache/hadoop/commit/251f528814c4a4647cac0af6effb9a73135db180",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=251f528814c4a4647cac0af6effb9a73135db180",
                "deletions": 14,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n@@ -737,21 +738,22 @@ public void transition(RMContainerImpl container, RMContainerEvent event) {\n \n     private static void updateAttemptMetrics(RMContainerImpl container) {\n       Resource resource = container.getContainer().getResource();\n-      RMAppAttempt rmAttempt = container.rmContext.getRMApps()\n-          .get(container.getApplicationAttemptId().getApplicationId())\n-          .getCurrentAppAttempt();\n-\n-      if (rmAttempt != null) {\n-        long usedMillis = container.finishTime - container.creationTime;\n-        rmAttempt.getRMAppAttemptMetrics()\n-            .updateAggregateAppResourceUsage(resource, usedMillis);\n-        // If this is a preempted container, update preemption metrics\n-        if (ContainerExitStatus.PREEMPTED == container.finishedStatus\n-            .getExitStatus()) {\n+      RMApp app = container.rmContext.getRMApps()\n+          .get(container.getApplicationAttemptId().getApplicationId());\n+      if (app != null) {\n+        RMAppAttempt rmAttempt = app.getCurrentAppAttempt();\n+        if (rmAttempt != null) {\n+          long usedMillis = container.finishTime - container.creationTime;\n           rmAttempt.getRMAppAttemptMetrics()\n-              .updatePreemptionInfo(resource, container);\n-          rmAttempt.getRMAppAttemptMetrics()\n-              .updateAggregatePreemptedAppResourceUsage(resource, usedMillis);\n+              .updateAggregateAppResourceUsage(resource, usedMillis);\n+          // If this is a preempted container, update preemption metrics\n+          if (ContainerExitStatus.PREEMPTED == container.finishedStatus\n+              .getExitStatus()) {\n+            rmAttempt.getRMAppAttemptMetrics()\n+                .updatePreemptionInfo(resource, container);\n+            rmAttempt.getRMAppAttemptMetrics()\n+                .updateAggregatePreemptedAppResourceUsage(resource, usedMillis);\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "b5c8e7cb8e975bd66b587c0c8c776a2b033a0f7c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java?ref=251f528814c4a4647cac0af6effb9a73135db180",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "patch": "@@ -1241,12 +1241,13 @@ public void incNumAllocatedContainers(NodeType containerType,\n       return;\n     }\n \n-    RMAppAttempt attempt =\n-        rmContext.getRMApps().get(attemptId.getApplicationId())\n-          .getCurrentAppAttempt();\n-    if (attempt != null) {\n-      attempt.getRMAppAttemptMetrics().incNumAllocatedContainers(containerType,\n-        requestType);\n+    RMApp app = rmContext.getRMApps().get(attemptId.getApplicationId());\n+    if (app != null) {\n+      RMAppAttempt attempt = app.getCurrentAppAttempt();\n+      if (attempt != null) {\n+        attempt.getRMAppAttemptMetrics()\n+            .incNumAllocatedContainers(containerType, requestType);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "sha": "005569cf2520751528b05cc317aed9585788e3ad",
                "status": "modified"
            }
        ],
        "message": "YARN-8222. Fix potential NPE when gets RMApp from RM context. Contributed by Tao Yang.",
        "parent": "https://github.com/apache/hadoop/commit/3265b55119d39ecbda6d75be04a9a1bf59c631f1",
        "patched_files": [
            "SchedulerApplicationAttempt.java",
            "RMContainerImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMContainerImpl.java",
            "TestSchedulerApplicationAttempt.java"
        ]
    },
    "hadoop_25df505": {
        "bug_id": "hadoop_25df505",
        "commit": "https://github.com/apache/hadoop/commit/25df5054216a6a76d09d9c49984f8075ebc6a197",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/25df5054216a6a76d09d9c49984f8075ebc6a197/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Resource.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Resource.java?ref=25df5054216a6a76d09d9c49984f8075ebc6a197",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Resource.java",
                "patch": "@@ -102,9 +102,12 @@ public static Resource newInstance(long memory, int vCores) {\n   @Stable\n   public static Resource newInstance(long memory, int vCores,\n       Map<String, Long> others) {\n-    ResourceInformation[] info = ResourceUtils.createResourceTypesArray(others);\n-\n-    return new LightWeightResource(memory, vCores, info);\n+    if (others != null) {\n+      return new LightWeightResource(memory, vCores,\n+          ResourceUtils.createResourceTypesArray(others));\n+    } else {\n+      return newInstance(memory, vCores);\n+    }\n   }\n \n   @InterfaceAudience.Private",
                "raw_url": "https://github.com/apache/hadoop/raw/25df5054216a6a76d09d9c49984f8075ebc6a197/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/Resource.java",
                "sha": "304a9639c5f413c03550e387159ea616611b8159",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/25df5054216a6a76d09d9c49984f8075ebc6a197/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceUtils.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceUtils.java?ref=25df5054216a6a76d09d9c49984f8075ebc6a197",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceUtils.java",
                "patch": "@@ -313,15 +313,13 @@ private static void updateResourceTypeIndex() {\n   }\n \n   public static ResourceInformation[] getResourceTypesArray() {\n-    initializeResourceTypesIfNeeded(null,\n-        YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE);\n+    initializeResourceTypesIfNeeded();\n     return resourceTypesArray;\n   }\n \n   public static int getNumberOfKnownResourceTypes() {\n     if (numKnownResourceTypes < 0) {\n-      initializeResourceTypesIfNeeded(null,\n-          YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE);\n+      initializeResourceTypesIfNeeded();\n     }\n     return numKnownResourceTypes;\n   }\n@@ -332,6 +330,11 @@ public static int getNumberOfKnownResourceTypes() {\n         YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE);\n   }\n \n+  private static void initializeResourceTypesIfNeeded() {\n+    initializeResourceTypesIfNeeded(null,\n+        YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE);\n+  }\n+\n   private static void initializeResourceTypesIfNeeded(Configuration conf,\n       String resourceFile) {\n     if (!initializedResources) {\n@@ -641,6 +644,8 @@ public static void reinitializeResources(\n    */\n   public static ResourceInformation[] createResourceTypesArray(Map<String,\n       Long> res) {\n+    initializeResourceTypesIfNeeded();\n+\n     ResourceInformation[] info = new ResourceInformation[resourceTypes.size()];\n \n     for (Entry<String, Integer> entry : RESOURCE_NAME_TO_INDEX.entrySet()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/25df5054216a6a76d09d9c49984f8075ebc6a197/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/util/resource/ResourceUtils.java",
                "sha": "76ae0614f67d019cc4270421533a28653df7794c",
                "status": "modified"
            }
        ],
        "message": "YARN-7589. TestPBImplRecords fails with NullPointerException. Contributed by Daniel Templeton",
        "parent": "https://github.com/apache/hadoop/commit/c83fe4491731c994a4867759d80db31d9c1cab60",
        "patched_files": [
            "ResourceUtils.java",
            "Resource.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResource.java",
            "TestResourceUtils.java"
        ]
    },
    "hadoop_2741a21": {
        "bug_id": "hadoop_2741a21",
        "commit": "https://github.com/apache/hadoop/commit/2741a2109b98d0febb463cb318018ecbd3995102",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2277,6 +2277,9 @@ Release 2.8.0 - UNRELEASED\n     initialization, because HftpFileSystem is missing.\n     (Mingliang Liu via cnauroth)\n \n+    HDFS-9249. NPE is thrown if an IOException is thrown in NameNode constructor.\n+    (Wei-Chiu Chuang via Yongjun Zhang)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "533fe34a3c95fd81a0f825fb08c4a48e48b2d157",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java",
                "patch": "@@ -217,7 +217,9 @@ void stop(boolean reportError) {\n \n     // Abort current log segment - otherwise the NN shutdown code\n     // will close it gracefully, which is incorrect.\n-    getFSImage().getEditLog().abortCurrentLogSegment();\n+    if (namesystem != null) {\n+      getFSImage().getEditLog().abortCurrentLogSegment();\n+    }\n \n     // Stop name-node threads\n     super.stop();",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java",
                "sha": "36053f71aba0efc933e7fe48ffb73f17f55e8175",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -889,15 +889,24 @@ protected NameNode(Configuration conf, NamenodeRole role)\n         haContext.writeUnlock();\n       }\n     } catch (IOException e) {\n-      this.stop();\n+      this.stopAtException(e);\n       throw e;\n     } catch (HadoopIllegalArgumentException e) {\n-      this.stop();\n+      this.stopAtException(e);\n       throw e;\n     }\n     this.started.set(true);\n   }\n \n+  private void stopAtException(Exception e){\n+    try {\n+      this.stop();\n+    } catch (Exception ex) {\n+      LOG.warn(\"Encountered exception when handling exception (\"\n+          + e.getMessage() + \"):\", ex);\n+    }\n+  }\n+\n   protected HAState createHAState(StartupOption startOpt) {\n     if (!haEnabled || startOpt == StartupOption.UPGRADE \n         || startOpt == StartupOption.UPGRADEONLY) {",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "7371d84f75e1420dfd9d5d11f1d2d45bff86ae3c",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "patch": "@@ -21,6 +21,7 @@\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.File;\n import java.io.IOException;\n@@ -30,7 +31,9 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.directory.api.ldap.aci.UserClass;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n@@ -46,6 +49,8 @@\n import org.apache.hadoop.hdfs.server.namenode.FileJournalManager.EditLogFile;\n import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.log4j.Level;\n import org.junit.Before;\n@@ -128,6 +133,72 @@ void waitCheckpointDone(MiniDFSCluster cluster, long txid) {\n         Collections.singletonList((int)thisCheckpointTxId));\n   }\n \n+\n+  /**\n+   *  Regression test for HDFS-9249.\n+   *  This test configures the primary name node with SIMPLE authentication,\n+   *  and configures the backup node with Kerberose authentication with\n+   *  invalid keytab settings.\n+   *\n+   *  This configuration causes the backup node to throw a NPE trying to abort\n+   *  the edit log.\n+   *  */\n+  @Test\n+    public void startBackupNodeWithIncorrectAuthentication() throws IOException {\n+    Configuration c = new HdfsConfiguration();\n+    StartupOption startupOpt = StartupOption.CHECKPOINT;\n+    String dirs = getBackupNodeDir(startupOpt, 1);\n+    c.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://127.0.0.1:1234\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY, \"localhost:0\");\n+    c.set(DFSConfigKeys.DFS_BLOCKREPORT_INITIAL_DELAY_KEY, \"0\");\n+    c.setInt(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,\n+        -1); // disable block scanner\n+    c.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, 1);\n+    c.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, dirs);\n+    c.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY,\n+        \"${\" + DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY + \"}\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_ADDRESS_KEY,\n+        \"127.0.0.1:0\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY,\n+        \"127.0.0.1:0\");\n+\n+    NameNode nn;\n+    try {\n+      Configuration nnconf = new HdfsConfiguration(c);\n+      DFSTestUtil.formatNameNode(nnconf);\n+      nn = NameNode.createNameNode(new String[] {}, nnconf);\n+    } catch (IOException e) {\n+      LOG.info(\"IOException is thrown creating name node\");\n+      throw e;\n+    }\n+\n+    c.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,\n+        \"kerberos\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY, \"\");\n+\n+    BackupNode bn = null;\n+    try {\n+      bn = (BackupNode)NameNode.createNameNode(\n+          new String[] {startupOpt.getName()}, c);\n+      assertTrue(\"Namesystem in BackupNode should be null\",\n+          bn.getNamesystem() == null);\n+      fail(\"Incorrect authentication setting should throw IOException\");\n+    } catch (IOException e) {\n+      LOG.info(\"IOException thrown as expected\", e);\n+    } finally {\n+      if (nn != null) {\n+        nn.stop();\n+      }\n+      if (bn != null) {\n+        bn.stop();\n+      }\n+      SecurityUtil.setAuthenticationMethod(\n+          UserGroupInformation.AuthenticationMethod.SIMPLE, c);\n+      // reset security authentication\n+      UserGroupInformation.setConfiguration(c);\n+    }\n+  }\n+\n   @Test\n   public void testCheckpointNode() throws Exception {\n     testCheckpoint(StartupOption.CHECKPOINT);",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "sha": "38d84e35dc54e2298822f75e214d93c2aacd5bc4",
                "status": "modified"
            }
        ],
        "message": "HDFS-9249. NPE is thrown if an IOException is thrown in NameNode constructor. (Wei-Chiu Chuang via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "patched_files": [
            "CHANGES.java",
            "BackupNode.java",
            "NameNode.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBackupNode.java"
        ]
    },
    "hadoop_27e0681": {
        "bug_id": "hadoop_27e0681",
        "commit": "https://github.com/apache/hadoop/commit/27e0681f28ee896ada163bbbc08fd44d113e7d15",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2974,6 +2974,9 @@ Release 2.7.3 - UNRELEASED\n     HDFS-9766. TestDataNodeMetrics#testDataNodeTimeSpend fails intermittently.\n     (Xiao Chen via aajisaka)\n \n+    HDFS-9851. NameNode throws NPE when setPermission is called on a path that\n+    does not exist. (Brahma Reddy Battula via aajisaka)\n+\n Release 2.7.2 - 2016-01-25\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "8be05bff22988fa3824f7e63735895c37d272911",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.hadoop.hdfs.protocolPB.PBHelperClient;\n import org.apache.hadoop.security.AccessControlException;\n \n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.EnumSet;\n import java.util.List;\n@@ -388,7 +389,7 @@ static XAttr unprotectedGetXAttrByPrefixedName(\n   private static void checkXAttrChangeAccess(\n       FSDirectory fsd, INodesInPath iip, XAttr xAttr,\n       FSPermissionChecker pc)\n-      throws AccessControlException {\n+      throws AccessControlException, FileNotFoundException {\n     if (fsd.isPermissionEnabled() && xAttr.getNameSpace() == XAttr.NameSpace\n         .USER) {\n       final INode inode = iip.getLastINode();",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
                "sha": "d27cec5773679ee6ba27bc2ede28f64e9a574a54",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1547,7 +1547,11 @@ FSPermissionChecker getPermissionChecker(String fsOwner, String superGroup,\n   }\n \n   void checkOwner(FSPermissionChecker pc, INodesInPath iip)\n-      throws AccessControlException {\n+      throws AccessControlException, FileNotFoundException {\n+    if (iip.getLastINode() == null) {\n+      throw new FileNotFoundException(\n+          \"Directory/File does not exist \" + iip.getPath());\n+    }\n     checkPermission(pc, iip, true, null, null, null, null);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "51b7747c1f3c16ca139252779890919c389c1652",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java",
                "patch": "@@ -3344,7 +3344,7 @@ public void testSnapshotReserved() throws IOException {\n       fs.createSnapshot(reserved, \"snap\");\n       fail(\"Can't create snapshot on /.reserved\");\n     } catch (FileNotFoundException e) {\n-      assertTrue(e.getMessage().contains(\"Directory does not exist\"));\n+      assertTrue(e.getMessage().contains(\"Directory/File does not exist\"));\n     }\n     cluster.shutdown();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java",
                "sha": "41cd5c0c675975a965b6762e8d9508fb761c98c2",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java",
                "patch": "@@ -51,6 +51,7 @@\n   final private static Path CHILD_DIR2 = new Path(ROOT_PATH, \"child2\");\n   final private static Path CHILD_FILE1 = new Path(ROOT_PATH, \"file1\");\n   final private static Path CHILD_FILE2 = new Path(ROOT_PATH, \"file2\");\n+  final private static Path CHILD_FILE3 = new Path(ROOT_PATH, \"file3\");\n \n   final private static int FILE_LEN = 100;\n   final private static Random RAN = new Random();\n@@ -270,6 +271,18 @@ public void testFilePermission() throws Exception {\n       final Path RENAME_PATH = new Path(\"/foo/bar\");\n       userfs.mkdirs(RENAME_PATH);\n       assertTrue(canRename(userfs, RENAME_PATH, CHILD_DIR1));\n+      // test permissions on files that do not exist\n+      assertFalse(userfs.exists(CHILD_FILE3));\n+      try {\n+        userfs.setOwner(CHILD_FILE3, \"foo\", \"bar\");\n+        fail(\"setOwner should fail for non-exist file\");\n+      } catch (java.io.FileNotFoundException ignored) {\n+      }\n+      try {\n+        userfs.setPermission(CHILD_FILE3, new FsPermission((short) 0777));\n+        fail(\"setPermission should fail for non-exist file\");\n+      } catch (java.io.FileNotFoundException ignored) {\n+      }\n     } finally {\n       cluster.shutdown();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java",
                "sha": "5a17bbc8efc211662ec60cf91ff04022da7b263e",
                "status": "modified"
            }
        ],
        "message": "HDFS-9851. NameNode throws NPE when setPermission is called on a path that does not exist. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/e2ddf824694eb4605f3bb04a9c26e4b98529f5bc",
        "patched_files": [
            "Permission.java",
            "FSDirectory.java",
            "FSDirXAttrOp.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestPermission.java",
            "TestDFSShell.java",
            "TestFSDirectory.java"
        ]
    },
    "hadoop_2837907": {
        "bug_id": "hadoop_2837907",
        "commit": "https://github.com/apache/hadoop/commit/28379070d461eebba0a49d6f722fb65753b48f5c",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/28379070d461eebba0a49d6f722fb65753b48f5c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=28379070d461eebba0a49d6f722fb65753b48f5c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -104,6 +104,10 @@ Trunk (Unreleased)\n     HADOOP-8814. Replace string equals \"\" by String#isEmpty().\n     (Brandon Li via suresh)\n \n+    HADOOP-8588. SerializationFactory shouldn't throw a\n+    NullPointerException if the serializations list is empty.\n+    (Sho Shimauchi via harsh)\n+\n   BUG FIXES\n \n     HADOOP-8177. MBeans shouldn't try to register when it fails to create MBeanName.",
                "raw_url": "https://github.com/apache/hadoop/raw/28379070d461eebba0a49d6f722fb65753b48f5c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "18e2f8fdd74d313c1e436b2991d32543171db787",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/28379070d461eebba0a49d6f722fb65753b48f5c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java?ref=28379070d461eebba0a49d6f722fb65753b48f5c",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "patch": "@@ -40,12 +40,12 @@\n @InterfaceAudience.LimitedPrivate({\"HDFS\", \"MapReduce\"})\n @InterfaceStability.Evolving\n public class SerializationFactory extends Configured {\n-  \n-  private static final Log LOG =\n+\n+  static final Log LOG =\n     LogFactory.getLog(SerializationFactory.class.getName());\n \n   private List<Serialization<?>> serializations = new ArrayList<Serialization<?>>();\n-  \n+\n   /**\n    * <p>\n    * Serializations are found by reading the <code>io.serializations</code>\n@@ -55,15 +55,21 @@\n    */\n   public SerializationFactory(Configuration conf) {\n     super(conf);\n-    for (String serializerName : conf.getStrings(\n-      CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,\n-      new String[]{WritableSerialization.class.getName(),\n-        AvroSpecificSerialization.class.getName(),\n-        AvroReflectSerialization.class.getName()})) {\n-      add(conf, serializerName);\n+    if (conf.get(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY).equals(\"\")) {\n+      LOG.warn(\"Serialization for various data types may not be available. Please configure \"\n+          + CommonConfigurationKeys.IO_SERIALIZATIONS_KEY\n+          + \" properly to have serialization support (it is currently not set).\");\n+    } else {\n+      for (String serializerName : conf.getStrings(\n+          CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, new String[] {\n+              WritableSerialization.class.getName(),\n+              AvroSpecificSerialization.class.getName(),\n+              AvroReflectSerialization.class.getName() })) {\n+        add(conf, serializerName);\n+      }\n     }\n   }\n-  \n+\n   @SuppressWarnings(\"unchecked\")\n   private void add(Configuration conf, String serializationName) {\n     try {\n@@ -101,5 +107,5 @@ private void add(Configuration conf, String serializationName) {\n     }\n     return null;\n   }\n-  \n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/28379070d461eebba0a49d6f722fb65753b48f5c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "sha": "d6c6588e7944c6413967fb9185030efa8ee954bf",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/28379070d461eebba0a49d6f722fb65753b48f5c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java?ref=28379070d461eebba0a49d6f722fb65753b48f5c",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "patch": "@@ -17,27 +17,62 @@\n  */\n package org.apache.hadoop.io.serializer;\n \n+import org.junit.BeforeClass;\n import org.junit.Test;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertNotNull;\n \n+import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.io.Writable;\n+import org.apache.log4j.Level;\n \n public class TestSerializationFactory {\n \n+  static {\n+    ((Log4JLogger) SerializationFactory.LOG).getLogger().setLevel(Level.ALL);\n+  }\n+\n+  static Configuration conf;\n+  static SerializationFactory factory;\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    conf = new Configuration();\n+    factory = new SerializationFactory(conf);\n+  }\n+\n+  @Test\n+  public void testSerializationKeyIsEmpty() {\n+    Configuration conf = new Configuration();\n+    conf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, \"\");\n+    SerializationFactory factory = new SerializationFactory(conf);\n+  }\n+\n   @Test\n-  public void testSerializerAvailability() {\n+  public void testSerializationKeyIsInvalid() {\n     Configuration conf = new Configuration();\n+    conf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY, \"INVALID_KEY_XXX\");\n     SerializationFactory factory = new SerializationFactory(conf);\n+  }\n+\n+  @Test\n+  public void testGetSerializer() {\n     // Test that a valid serializer class is returned when its present\n-    assertNotNull(\"A valid class must be returned for default Writable Serde\",\n+    assertNotNull(\"A valid class must be returned for default Writable SerDe\",\n         factory.getSerializer(Writable.class));\n-    assertNotNull(\"A valid class must be returned for default Writable serDe\",\n-        factory.getDeserializer(Writable.class));\n     // Test that a null is returned when none can be found.\n     assertNull(\"A null should be returned if there are no serializers found.\",\n         factory.getSerializer(TestSerializationFactory.class));\n+  }\n+\n+  @Test\n+  public void testGetDeserializer() {\n+    // Test that a valid serializer class is returned when its present\n+    assertNotNull(\"A valid class must be returned for default Writable SerDe\",\n+        factory.getDeserializer(Writable.class));\n+    // Test that a null is returned when none can be found.\n     assertNull(\"A null should be returned if there are no deserializers found\",\n         factory.getDeserializer(TestSerializationFactory.class));\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/28379070d461eebba0a49d6f722fb65753b48f5c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "sha": "c5805be5b47439ada980f5ecb1454161b6ae8b8f",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8588. SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty. Contributed by Sho Shimauchi. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1389002 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/28023b77595991fe3be590a929b7d162556f1d4a",
        "patched_files": [
            "SerializationFactory.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSerializationFactory.java"
        ]
    },
    "hadoop_28ad20a": {
        "bug_id": "hadoop_28ad20a",
        "commit": "https://github.com/apache/hadoop/commit/28ad20a711735243bc10b10e33866dc525f415eb",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/28ad20a711735243bc10b10e33866dc525f415eb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java?ref=28ad20a711735243bc10b10e33866dc525f415eb",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "patch": "@@ -1656,7 +1656,7 @@ public void testContainerRemovedBeforeAllocate() {\n     when(allocation.getContainers()).\n         thenReturn(Collections.singletonList(container));\n     when(scheduler.allocate(any(ApplicationAttemptId.class), any(List.class),\n-        any(List.class), any(List.class), any(List.class), any(List.class),\n+        any(), any(), any(), any(),\n         any(ContainerUpdates.class))).\n         thenReturn(allocation);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/28ad20a711735243bc10b10e33866dc525f415eb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "sha": "b5c15cd84038bb861cfed27dcdf22144f1b4c1b2",
                "status": "modified"
            }
        ],
        "message": "YARN-9262. TestRMAppAttemptTransitions is failing with an NPE. Contributed by lujie.",
        "parent": "https://github.com/apache/hadoop/commit/2c135130402255ce41e1ef958989e746f21ae1ab",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMAppAttemptTransitions.java"
        ]
    },
    "hadoop_28ceb34": {
        "bug_id": "hadoop_28ceb34",
        "commit": "https://github.com/apache/hadoop/commit/28ceb34a725cd06d28fb51361c49bb45464f5368",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java?ref=28ceb34a725cd06d28fb51361c49bb45464f5368",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "patch": "@@ -2158,6 +2158,10 @@ public SnapshotDiffReport getSnapshotDiffReport(String snapshotDir,\n       String fromSnapshot, String toSnapshot) throws IOException {\n     checkOpen();\n     try (TraceScope ignored = tracer.newScope(\"getSnapshotDiffReport\")) {\n+      Preconditions.checkArgument(fromSnapshot != null,\n+          \"null fromSnapshot\");\n+      Preconditions.checkArgument(toSnapshot != null,\n+          \"null toSnapshot\");\n       return namenode\n           .getSnapshotDiffReport(snapshotDir, fromSnapshot, toSnapshot);\n     } catch (RemoteException re) {",
                "raw_url": "https://github.com/apache/hadoop/raw/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "sha": "38072b21ddd380f6c95b1917af59d8c2839ad680",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java?ref=28ceb34a725cd06d28fb51361c49bb45464f5368",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java",
                "patch": "@@ -277,7 +277,7 @@ SnapshotDiffInfo computeDiff(final INodeDirectory snapshotRootDir,\n     Snapshot fromSnapshot = getSnapshotByName(snapshotRootDir, from);\n     Snapshot toSnapshot = getSnapshotByName(snapshotRootDir, to);\n     // if the start point is equal to the end point, return null\n-    if (from.equals(to)) {\n+    if (from != null && from.equals(to)) {\n       return null;\n     }\n     SnapshotDiffInfo diffs = new SnapshotDiffInfo(snapshotRootDir,",
                "raw_url": "https://github.com/apache/hadoop/raw/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/snapshot/DirectorySnapshottableFeature.java",
                "sha": "15aa22a0af24b4fbaffbacaa12362a6c9a65d6c9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java?ref=28ceb34a725cd06d28fb51361c49bb45464f5368",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
                "patch": "@@ -230,6 +230,12 @@ public void testDiffReport() throws Exception {\n     LOG.info(report.toString());\n     assertEquals(0, report.getDiffList().size());\n \n+    try {\n+      report = hdfs.getSnapshotDiffReport(subsubsub1, null, \"s2\");\n+      fail(\"Expect exception when providing null fromSnapshot \");\n+    } catch (IllegalArgumentException e) {\n+      GenericTestUtils.assertExceptionContains(\"null fromSnapshot\", e);\n+    }\n     report = hdfs.getSnapshotDiffReport(subsubsub1, \"s0\", \"s2\");\n     LOG.info(report.toString());\n     assertEquals(0, report.getDiffList().size());",
                "raw_url": "https://github.com/apache/hadoop/raw/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
                "sha": "18ec3c581d5023815ea1478608e4c7a0452eeca3",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java?ref=28ceb34a725cd06d28fb51361c49bb45464f5368",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "patch": "@@ -743,6 +743,12 @@ public void testWebHdfsSnapshotDiff() throws Exception {\n       Assert.assertTrue(diffReport.getDiffList().contains(entry3));\n       Assert.assertTrue(diffReport.getDiffList().contains(entry4));\n       Assert.assertEquals(diffReport.getDiffList().size(), 5);\n+\n+      // Test with fromSnapshot and toSnapshot as null.\n+      diffReport = webHdfs.getSnapshotDiffReport(foo, null, \"s2\");\n+      Assert.assertEquals(diffReport.getDiffList().size(), 0);\n+      diffReport = webHdfs.getSnapshotDiffReport(foo, \"s1\", null);\n+      Assert.assertEquals(diffReport.getDiffList().size(), 5);\n     } finally {\n       if (cluster != null) {\n         cluster.shutdown();",
                "raw_url": "https://github.com/apache/hadoop/raw/28ceb34a725cd06d28fb51361c49bb45464f5368/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "sha": "a766de699502797e0058470d24852f3ccbe63967",
                "status": "modified"
            }
        ],
        "message": "HDFS-13868. WebHDFS: GETSNAPSHOTDIFF API NPE when param \"snapshotname\" is given but \"oldsnapshotname\" is not. Contributed by Pranay Singh.",
        "parent": "https://github.com/apache/hadoop/commit/e435e12f1fd00a5f78621ec1933a60171fc224e6",
        "patched_files": [
            "DFSClient.java",
            "SnapshotDiffReport.java",
            "DirectorySnapshottableFeature.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestWebHDFS.java",
            "TestSnapshotDiffReport.java"
        ]
    },
    "hadoop_296c5de": {
        "bug_id": "hadoop_296c5de",
        "commit": "https://github.com/apache/hadoop/commit/296c5de0cfee88389cf9f90263280b2034e54cd5",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/296c5de0cfee88389cf9f90263280b2034e54cd5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=296c5de0cfee88389cf9f90263280b2034e54cd5",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -247,7 +247,9 @@ public RMContainerImpl(Container container,\n        YarnConfiguration\n                  .DEFAULT_APPLICATION_HISTORY_SAVE_NON_AM_CONTAINER_META_INFO);\n \n-    rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    if (container.getId() != null) {\n+      rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    }\n \n     // If saveNonAMContainerMetaInfo is true, store system metrics for all\n     // containers. If false, and if this container is marked as the AM, metrics\n@@ -892,6 +894,9 @@ public void setContainerId(ContainerId containerId) {\n     // container creation event to timeline service when id assigned.\n     container.setId(containerId);\n \n+    if (containerId != null) {\n+      rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    }\n     // If saveNonAMContainerMetaInfo is true, store system metrics for all\n     // containers. If false, and if this container is marked as the AM, metrics\n     // will still be published for this container, but that calculation happens",
                "raw_url": "https://github.com/apache/hadoop/raw/296c5de0cfee88389cf9f90263280b2034e54cd5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "f5d8b5b063344eabc7a6fc46b842dd5f223353d6",
                "status": "modified"
            }
        ],
        "message": "YARN-5873. RM crashes with NPE if generic application history is enabled. Contributed by Varun Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/04014c4c739bb4e3bc3fdf9299abc0f47521e8fd",
        "patched_files": [
            "RMContainerImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMContainerImpl.java"
        ]
    },
    "hadoop_2a07617": {
        "bug_id": "hadoop_2a07617",
        "commit": "https://github.com/apache/hadoop/commit/2a07617f852ceddcf6b38ddcefd912fd953823d9",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java?ref=2a07617f852ceddcf6b38ddcefd912fd953823d9",
                "deletions": 11,
                "filename": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "patch": "@@ -286,17 +286,6 @@\n   public static final double\n       HDDS_DATANODE_STORAGE_UTILIZATION_CRITICAL_THRESHOLD_DEFAULT = 0.75;\n \n-  public static final String\n-      HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY =\n-      \"hdds.write.lock.reporting.threshold.ms\";\n-  public static final long\n-      HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_DEFAULT = 5000L;\n-  public static final String\n-      HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_KEY =\n-      \"hdds.lock.suppress.warning.interval.ms\";\n-  public static final long\n-      HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_DEAFULT = 10000L;\n-\n   public static final String OZONE_CONTAINER_COPY_WORKDIR =\n       \"hdds.datanode.replication.work.dir\";\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "sha": "e8aa22c2ac03de14cd404926f268efafe70b49c1",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java?ref=2a07617f852ceddcf6b38ddcefd912fd953823d9",
                "deletions": 18,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "patch": "@@ -69,31 +69,39 @@ public VersionEndpointTask(EndpointStateMachine rpcEndPoint,\n       VersionResponse response = VersionResponse.getFromProtobuf(\n           versionResponse);\n       rpcEndPoint.setVersion(response);\n-      VolumeSet volumeSet = ozoneContainer.getVolumeSet();\n-      Map<String, HddsVolume> volumeMap = volumeSet.getVolumeMap();\n \n       String scmId = response.getValue(OzoneConsts.SCM_ID);\n       String clusterId = response.getValue(OzoneConsts.CLUSTER_ID);\n \n-      Preconditions.checkNotNull(scmId, \"Reply from SCM: scmId cannot be \" +\n-          \"null\");\n-      Preconditions.checkNotNull(clusterId, \"Reply from SCM: clusterId \" +\n-          \"cannot be null\");\n+      // Check volumes\n+      VolumeSet volumeSet = ozoneContainer.getVolumeSet();\n+      volumeSet.readLock();\n+      try {\n+        Map<String, HddsVolume> volumeMap = volumeSet.getVolumeMap();\n+\n+        Preconditions.checkNotNull(scmId, \"Reply from SCM: scmId cannot be \" +\n+            \"null\");\n+        Preconditions.checkNotNull(clusterId, \"Reply from SCM: clusterId \" +\n+            \"cannot be null\");\n \n-      // If version file does not exist create version file and also set scmId\n-      for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n-        HddsVolume hddsVolume = entry.getValue();\n-        boolean result = HddsVolumeUtil.checkVolume(hddsVolume, scmId,\n-            clusterId, LOG);\n-        if (!result) {\n-          volumeSet.failVolume(hddsVolume.getHddsRootDir().getPath());\n+        // If version file does not exist create version file and also set scmId\n+        for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n+          HddsVolume hddsVolume = entry.getValue();\n+          boolean result = HddsVolumeUtil.checkVolume(hddsVolume, scmId,\n+              clusterId, LOG);\n+          if (!result) {\n+            volumeSet.failVolume(hddsVolume.getHddsRootDir().getPath());\n+          }\n         }\n+        if (volumeSet.getVolumesList().size() == 0) {\n+          // All volumes are inconsistent state\n+          throw new DiskOutOfSpaceException(\"All configured Volumes are in \" +\n+              \"Inconsistent State\");\n+        }\n+      } finally {\n+        volumeSet.readUnlock();\n       }\n-      if (volumeSet.getVolumesList().size() == 0) {\n-        // All volumes are inconsistent state\n-        throw new DiskOutOfSpaceException(\"All configured Volumes are in \" +\n-            \"Inconsistent State\");\n-      }\n+\n       ozoneContainer.getDispatcher().setScmId(scmId);\n \n       EndpointStateMachine.EndPointStates nextState =",
                "raw_url": "https://github.com/apache/hadoop/raw/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "sha": "2d0467706ec921ee55cfea0711332ff0298e4b42",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java?ref=2a07617f852ceddcf6b38ddcefd912fd953823d9",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "patch": "@@ -164,7 +164,7 @@ private static String getProperty(Properties props, String propName, File\n   }\n \n   /**\n-   * Check Volume is consistent state or not.\n+   * Check Volume is in consistent state or not.\n    * @param hddsVolume\n    * @param scmId\n    * @param clusterId",
                "raw_url": "https://github.com/apache/hadoop/raw/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "sha": "cb356dadeb236360a214d729cd511e29a3047c6c",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop/blob/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "changes": 178,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java?ref=2a07617f852ceddcf6b38ddcefd912fd953823d9",
                "deletions": 81,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "patch": "@@ -33,15 +33,11 @@\n     .StorageContainerDatanodeProtocolProtos;\n import org.apache.hadoop.hdds.protocol.proto\n     .StorageContainerDatanodeProtocolProtos.NodeReportProto;\n-import org.apache.hadoop.ozone.OzoneConfigKeys;\n import org.apache.hadoop.ozone.common.InconsistentStorageStateException;\n import org.apache.hadoop.ozone.container.common.impl.StorageLocationReport;\n import org.apache.hadoop.ozone.container.common.utils.HddsVolumeUtil;\n import org.apache.hadoop.ozone.container.common.volume.HddsVolume.VolumeState;\n-import org.apache.hadoop.ozone.container.common.interfaces.VolumeChoosingPolicy;\n-import org.apache.hadoop.util.AutoCloseableLock;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n-import org.apache.hadoop.util.InstrumentedLock;\n import org.apache.hadoop.util.ShutdownHookManager;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -53,8 +49,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n /**\n  * VolumeSet to manage volumes in a DataNode.\n@@ -84,11 +79,12 @@\n   private EnumMap<StorageType, List<HddsVolume>> volumeStateMap;\n \n   /**\n-   * Lock to synchronize changes to the VolumeSet. Any update to\n-   * {@link VolumeSet#volumeMap}, {@link VolumeSet#failedVolumeMap}, or\n-   * {@link VolumeSet#volumeStateMap} should be done after acquiring this lock.\n+   * A Reentrant Read Write Lock to synchronize volume operations in VolumeSet.\n+   * Any update to {@link VolumeSet#volumeMap},\n+   * {@link VolumeSet#failedVolumeMap}, or {@link VolumeSet#volumeStateMap}\n+   * should be done after acquiring the write lock.\n    */\n-  private final AutoCloseableLock volumeSetLock;\n+  private final ReentrantReadWriteLock volumeSetRWLock;\n \n   private final String datanodeUuid;\n   private String clusterID;\n@@ -105,17 +101,7 @@ public VolumeSet(String dnUuid, String clusterID, Configuration conf)\n     this.datanodeUuid = dnUuid;\n     this.clusterID = clusterID;\n     this.conf = conf;\n-    this.volumeSetLock = new AutoCloseableLock(\n-        new InstrumentedLock(getClass().getName(), LOG,\n-            new ReentrantLock(true),\n-            conf.getTimeDuration(\n-                OzoneConfigKeys.HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY,\n-                OzoneConfigKeys.HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_DEFAULT,\n-                TimeUnit.MILLISECONDS),\n-            conf.getTimeDuration(\n-                OzoneConfigKeys.HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_KEY,\n-                OzoneConfigKeys.HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_DEAFULT,\n-                TimeUnit.MILLISECONDS)));\n+    this.volumeSetRWLock = new ReentrantReadWriteLock();\n \n     initializeVolumeSet();\n   }\n@@ -198,14 +184,35 @@ private void checkAndSetClusterID(String idFromVersionFile)\n     }\n   }\n \n-  public void acquireLock() {\n-    volumeSetLock.acquire();\n+  /**\n+   * Acquire Volume Set Read lock.\n+   */\n+  public void readLock() {\n+    volumeSetRWLock.readLock().lock();\n+  }\n+\n+  /**\n+   * Release Volume Set Read lock.\n+   */\n+  public void readUnlock() {\n+    volumeSetRWLock.readLock().unlock();\n   }\n \n-  public void releaseLock() {\n-    volumeSetLock.release();\n+  /**\n+   * Acquire Volume Set Write lock.\n+   */\n+  public void writeLock() {\n+    volumeSetRWLock.writeLock().lock();\n+  }\n+\n+  /**\n+   * Release Volume Set Write lock.\n+   */\n+  public void writeUnlock() {\n+    volumeSetRWLock.writeLock().unlock();\n   }\n \n+\n   private HddsVolume createVolume(String locationString,\n       StorageType storageType) throws IOException {\n     HddsVolume.Builder volumeBuilder = new HddsVolume.Builder(locationString)\n@@ -227,7 +234,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(volumeRoot);\n     boolean success;\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         LOG.warn(\"Volume : {} already exists in VolumeMap\", hddsRoot);\n         success = false;\n@@ -247,6 +255,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n     } catch (IOException ex) {\n       LOG.error(\"Failed to add volume \" + volumeRoot + \" to VolumeSet\", ex);\n       success = false;\n+    } finally {\n+      this.writeUnlock();\n     }\n     return success;\n   }\n@@ -255,7 +265,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n   public void failVolume(String dataDir) {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(dataDir);\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         HddsVolume hddsVolume = volumeMap.get(hddsRoot);\n         hddsVolume.failVolume();\n@@ -270,14 +281,17 @@ public void failVolume(String dataDir) {\n       } else {\n         LOG.warn(\"Volume : {} does not exist in VolumeSet\", hddsRoot);\n       }\n+    } finally {\n+      this.writeUnlock();\n     }\n   }\n \n   // Remove a volume from the VolumeSet completely.\n   public void removeVolume(String dataDir) throws IOException {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(dataDir);\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         HddsVolume hddsVolume = volumeMap.get(hddsRoot);\n         hddsVolume.shutdown();\n@@ -295,14 +309,11 @@ public void removeVolume(String dataDir) throws IOException {\n       } else {\n         LOG.warn(\"Volume : {} does not exist in VolumeSet\", hddsRoot);\n       }\n+    } finally {\n+      this.writeUnlock();\n     }\n   }\n \n-  public HddsVolume chooseVolume(long containerSize,\n-      VolumeChoosingPolicy choosingPolicy) throws IOException {\n-    return choosingPolicy.chooseVolume(getVolumesList(), containerSize);\n-  }\n-\n   /**\n    * This method, call shutdown on each volume to shutdown volume usage\n    * thread and write scmUsed on each volume.\n@@ -352,55 +363,60 @@ public void shutdown() {\n   public StorageContainerDatanodeProtocolProtos.NodeReportProto getNodeReport()\n       throws IOException {\n     boolean failed;\n-    StorageLocationReport[] reports = new StorageLocationReport[volumeMap\n-        .size() + failedVolumeMap.size()];\n-    int counter = 0;\n-    HddsVolume hddsVolume;\n-    for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n-      hddsVolume = entry.getValue();\n-      VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n-      long scmUsed = 0;\n-      long remaining = 0;\n-      failed = false;\n-      try {\n-        scmUsed = volumeInfo.getScmUsed();\n-        remaining = volumeInfo.getAvailable();\n-      } catch (IOException ex) {\n-        LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n-            \"storage location {}\", volumeInfo.getRootDir());\n-        // reset scmUsed and remaining if df/du failed.\n-        scmUsed = 0;\n-        remaining = 0;\n-        failed = true;\n-      }\n+    this.readLock();\n+    try {\n+      StorageLocationReport[] reports = new StorageLocationReport[volumeMap\n+          .size() + failedVolumeMap.size()];\n+      int counter = 0;\n+      HddsVolume hddsVolume;\n+      for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n+        hddsVolume = entry.getValue();\n+        VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n+        long scmUsed = 0;\n+        long remaining = 0;\n+        failed = false;\n+        try {\n+          scmUsed = volumeInfo.getScmUsed();\n+          remaining = volumeInfo.getAvailable();\n+        } catch (IOException ex) {\n+          LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n+              \"storage location {}\", volumeInfo.getRootDir());\n+          // reset scmUsed and remaining if df/du failed.\n+          scmUsed = 0;\n+          remaining = 0;\n+          failed = true;\n+        }\n \n-      StorageLocationReport.Builder builder =\n-          StorageLocationReport.newBuilder();\n-      builder.setStorageLocation(volumeInfo.getRootDir())\n-          .setId(hddsVolume.getStorageID())\n-          .setFailed(failed)\n-          .setCapacity(hddsVolume.getCapacity())\n-          .setRemaining(remaining)\n-          .setScmUsed(scmUsed)\n-          .setStorageType(hddsVolume.getStorageType());\n-      StorageLocationReport r = builder.build();\n-      reports[counter++] = r;\n-    }\n-    for (Map.Entry<String, HddsVolume> entry : failedVolumeMap.entrySet()) {\n-      hddsVolume = entry.getValue();\n-      StorageLocationReport.Builder builder = StorageLocationReport\n-          .newBuilder();\n-      builder.setStorageLocation(hddsVolume.getHddsRootDir()\n-          .getAbsolutePath()).setId(hddsVolume.getStorageID()).setFailed(true)\n-          .setCapacity(0).setRemaining(0).setScmUsed(0).setStorageType(\n-              hddsVolume.getStorageType());\n-      StorageLocationReport r = builder.build();\n-      reports[counter++] = r;\n-    }\n-    NodeReportProto.Builder nrb = NodeReportProto.newBuilder();\n-    for (int i = 0; i < reports.length; i++) {\n-      nrb.addStorageReport(reports[i].getProtoBufMessage());\n+        StorageLocationReport.Builder builder =\n+            StorageLocationReport.newBuilder();\n+        builder.setStorageLocation(volumeInfo.getRootDir())\n+            .setId(hddsVolume.getStorageID())\n+            .setFailed(failed)\n+            .setCapacity(hddsVolume.getCapacity())\n+            .setRemaining(remaining)\n+            .setScmUsed(scmUsed)\n+            .setStorageType(hddsVolume.getStorageType());\n+        StorageLocationReport r = builder.build();\n+        reports[counter++] = r;\n+      }\n+      for (Map.Entry<String, HddsVolume> entry : failedVolumeMap.entrySet()) {\n+        hddsVolume = entry.getValue();\n+        StorageLocationReport.Builder builder = StorageLocationReport\n+            .newBuilder();\n+        builder.setStorageLocation(hddsVolume.getHddsRootDir()\n+            .getAbsolutePath()).setId(hddsVolume.getStorageID()).setFailed(true)\n+            .setCapacity(0).setRemaining(0).setScmUsed(0).setStorageType(\n+            hddsVolume.getStorageType());\n+        StorageLocationReport r = builder.build();\n+        reports[counter++] = r;\n+      }\n+      NodeReportProto.Builder nrb = NodeReportProto.newBuilder();\n+      for (int i = 0; i < reports.length; i++) {\n+        nrb.addStorageReport(reports[i].getProtoBufMessage());\n+      }\n+      return nrb.build();\n+    } finally {\n+      this.readUnlock();\n     }\n-    return nrb.build();\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "sha": "5b6b823c9c69ef69fb14d0735bd42c0a84a534b2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java?ref=2a07617f852ceddcf6b38ddcefd912fd953823d9",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "patch": "@@ -108,8 +108,8 @@ public void create(VolumeSet volumeSet, VolumeChoosingPolicy\n     Preconditions.checkNotNull(scmId, \"scmId cannot be null\");\n \n     File containerMetaDataPath = null;\n-    //acquiring volumeset lock and container lock\n-    volumeSet.acquireLock();\n+    //acquiring volumeset read lock\n+    volumeSet.readLock();\n     long maxSize = containerData.getMaxSize();\n     try {\n       HddsVolume containerVolume = volumeChoosingPolicy.chooseVolume(volumeSet\n@@ -166,7 +166,7 @@ public void create(VolumeSet volumeSet, VolumeChoosingPolicy\n       throw new StorageContainerException(\"Container creation failed.\", ex,\n           CONTAINER_INTERNAL_ERROR);\n     } finally {\n-      volumeSet.releaseLock();\n+      volumeSet.readUnlock();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "sha": "e5b344de483770fc5e7b201fe12220438ebe7cbe",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java?ref=2a07617f852ceddcf6b38ddcefd912fd953823d9",
                "deletions": 2,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "patch": "@@ -271,14 +271,14 @@ ContainerCommandResponseProto handleCreateContainer(\n \n   public void populateContainerPathFields(KeyValueContainer container,\n       long maxSize) throws IOException {\n-    volumeSet.acquireLock();\n+    volumeSet.readLock();\n     try {\n       HddsVolume containerVolume = volumeChoosingPolicy.chooseVolume(volumeSet\n           .getVolumesList(), maxSize);\n       String hddsVolumeDir = containerVolume.getHddsRootDir().toString();\n       container.populatePathFields(scmID, containerVolume, hddsVolumeDir);\n     } finally {\n-      volumeSet.releaseLock();\n+      volumeSet.readUnlock();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2a07617f852ceddcf6b38ddcefd912fd953823d9/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "sha": "922db2ad888581d88778787c2ecd7576e9f7073a",
                "status": "modified"
            }
        ],
        "message": "HDDS-354. VolumeInfo.getScmUsed throws NPE. Contributed by Hanisha Koneru.",
        "parent": "https://github.com/apache/hadoop/commit/e6b77ad65f923858fb67f5c2165fefe52d6f8c17",
        "patched_files": [
            "KeyValueHandler.java",
            "KeyValueContainer.java",
            "VolumeSet.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestVolumeSet.java",
            "TestKeyValueHandler.java",
            "TestKeyValueContainer.java"
        ]
    },
    "hadoop_2b5ad48": {
        "bug_id": "hadoop_2b5ad48",
        "commit": "https://github.com/apache/hadoop/commit/2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java",
                "patch": "@@ -223,10 +223,17 @@ void commit() {\n    * Initialize lease recovery for this block.\n    * Find the first alive data-node starting from the previous primary and\n    * make it primary.\n+   * @param blockInfo Block to be recovered\n+   * @param recoveryId Recovery ID (new gen stamp)\n+   * @param startRecovery Issue recovery command to datanode if true.\n    */\n-  public void initializeBlockRecovery(BlockInfo blockInfo, long recoveryId) {\n+  public void initializeBlockRecovery(BlockInfo blockInfo, long recoveryId,\n+      boolean startRecovery) {\n     setBlockUCState(BlockUCState.UNDER_RECOVERY);\n     blockRecoveryId = recoveryId;\n+    if (!startRecovery) {\n+      return;\n+    }\n     if (replicas.length == 0) {\n       NameNode.blockStateChangeLog.warn(\"BLOCK*\" +\n           \" BlockUnderConstructionFeature.initializeBlockRecovery:\" +",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java",
                "sha": "61390d9fe835cf26af09a077a4d3acfb9b35aca3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "patch": "@@ -642,10 +642,11 @@ public DatanodeDescriptor getDatanode(DatanodeID nodeID)\n       String format, Object... args) throws UnregisteredNodeException {\n     storageIDs = storageIDs == null ? new String[0] : storageIDs;\n     if (datanodeID.length != storageIDs.length) {\n+      // Error for pre-2.0.0-alpha clients.\n       final String err = (storageIDs.length == 0?\n           \"Missing storageIDs: It is likely that the HDFS client,\"\n           + \" who made this call, is running in an older version of Hadoop\"\n-          + \" which does not support storageIDs.\"\n+          + \"(pre-2.0.0-alpha)  which does not support storageIDs.\"\n           : \"Length mismatched: storageIDs.length=\" + storageIDs.length + \" != \"\n           ) + \" datanodeID.length=\" + datanodeID.length;\n       throw new HadoopIllegalArgumentException(",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "sha": "c303594cda9b38e4c926a048843df00c40627596",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java",
                "patch": "@@ -270,7 +270,7 @@ static Block prepareFileForTruncate(FSNamesystem fsn, INodesInPath iip,\n     }\n     if (shouldRecoverNow) {\n       truncatedBlockUC.getUnderConstructionFeature().initializeBlockRecovery(\n-          truncatedBlockUC, newBlock.getGenerationStamp());\n+          truncatedBlockUC, newBlock.getGenerationStamp(), true);\n     }\n \n     return newBlock;",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java",
                "sha": "034812058eea93fc4358490fb30f1da58aadef40",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3250,7 +3250,7 @@ boolean internalReleaseLease(Lease lease, String src, INodesInPath iip,\n       } else if(truncateRecovery) {\n         recoveryBlock.setGenerationStamp(blockRecoveryId);\n       }\n-      uc.initializeBlockRecovery(lastBlock, blockRecoveryId);\n+      uc.initializeBlockRecovery(lastBlock, blockRecoveryId, true);\n       leaseManager.renewLease(lease);\n       // Cannot close file right now, since the last block requires recovery.\n       // This may potentially cause infinite loop in lease recovery",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "11b62d981c66c6a5a4d3bcd9dd6504f6e194fe72",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
                "patch": "@@ -475,9 +475,16 @@ synchronized boolean checkLeases() {\n           if (!p.startsWith(\"/\")) {\n             throw new IOException(\"Invalid path in the lease \" + p);\n           }\n-          boolean completed = fsnamesystem.internalReleaseLease(\n-              leaseToCheck, p, iip,\n-              HdfsServerConstants.NAMENODE_LEASE_HOLDER);\n+          boolean completed = false;\n+          try {\n+            completed = fsnamesystem.internalReleaseLease(\n+                leaseToCheck, p, iip,\n+                HdfsServerConstants.NAMENODE_LEASE_HOLDER);\n+          } catch (IOException e) {\n+            LOG.warn(\"Cannot release the path \" + p + \" in the lease \"\n+                + leaseToCheck + \". It will be retried.\", e);\n+            continue;\n+          }\n           if (LOG.isDebugEnabled()) {\n             if (completed) {\n               LOG.debug(\"Lease recovery for inode \" + id + \" is complete. \" +\n@@ -491,7 +498,7 @@ synchronized boolean checkLeases() {\n             needSync = true;\n           }\n         } catch (IOException e) {\n-          LOG.error(\"Cannot release the path \" + p + \" in the lease \"\n+          LOG.warn(\"Removing lease with an invalid path: \" + p + \",\"\n               + leaseToCheck, e);\n           removing.add(id);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
                "sha": "dcdb958c01b16c80fa3b45dc0f0643fb2742dc18",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java",
                "patch": "@@ -50,23 +50,23 @@ public void testInitializeBlockRecovery() throws Exception {\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -3 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -2 * 1000);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 1);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 1, true);\n     BlockInfo[] blockInfoRecovery = dd2.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n \n     // Recovery attempt #2.\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -3 * 1000);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 2);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 2, true);\n     blockInfoRecovery = dd1.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n \n     // Recovery attempt #3.\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -3 * 1000);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3, true);\n     blockInfoRecovery = dd3.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n \n@@ -75,7 +75,7 @@ public void testInitializeBlockRecovery() throws Exception {\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, 0);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3, true);\n     blockInfoRecovery = dd3.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java",
                "sha": "15502c92fb718a89339ab36d78297642056cca57",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
                "patch": "@@ -38,12 +38,16 @@\n import org.apache.hadoop.hdfs.protocol.LocatedBlocks;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.BlockUCState;\n import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n+import org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.LocatedBlockProto;\n+import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+\n public class TestBlockUnderConstruction {\n   static final String BASE_DIR = \"/test/TestBlockUnderConstruction\";\n   static final int BLOCK_SIZE = 8192; // same as TestFileCreation.blocksize\n@@ -183,4 +187,45 @@ public void testGetBlockLocations() throws IOException {\n     // close file\n     out.close();\n   }\n+\n+  /**\n+   * A storage ID can be invalid if the storage failed or the node\n+   * reregisters. When the node heart-beats, the storage report in it\n+   * causes storage volumes to be added back. An invalid storage ID\n+   * should not cause an NPE.\n+   */\n+  @Test\n+  public void testEmptyExpectedLocations() throws Exception {\n+    final NamenodeProtocols namenode = cluster.getNameNodeRpc();\n+    final FSNamesystem fsn = cluster.getNamesystem();\n+    final BlockManager bm = fsn.getBlockManager();\n+    final Path p = new Path(BASE_DIR, \"file2.dat\");\n+    final String src = p.toString();\n+    final FSDataOutputStream out = TestFileCreation.createFile(hdfs, p, 1);\n+    writeFile(p, out, 256);\n+    out.hflush();\n+\n+    // make sure the block is readable\n+    LocatedBlocks lbs = namenode.getBlockLocations(src, 0, 256);\n+    LocatedBlock lastLB = lbs.getLocatedBlocks().get(0);\n+    final Block b = lastLB.getBlock().getLocalBlock();\n+\n+    // fake a block recovery\n+    long blockRecoveryId = bm.nextGenerationStamp(false);\n+    BlockUnderConstructionFeature uc = bm.getStoredBlock(b).\n+        getUnderConstructionFeature();\n+    uc.initializeBlockRecovery(null, blockRecoveryId, false);\n+\n+    try {\n+      String[] storages = { \"invalid-storage-id1\" };\n+      fsn.commitBlockSynchronization(lastLB.getBlock(), blockRecoveryId, 256L,\n+          true, false, lastLB.getLocations(), storages);\n+    } catch (java.lang.IllegalStateException ise) {\n+       // Although a failure is expected as of now, future commit policy\n+       // changes may make it not fail. This is not critical to the test.\n+    }\n+\n+    // Invalid storage should not trigger an exception.\n+    lbs = namenode.getBlockLocations(src, 0, 256);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
                "sha": "49e79da674e5d0ec0cd359579aa36f5a6e0aa932",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "patch": "@@ -73,7 +73,7 @@ private FSNamesystem makeNameSystemSpy(Block block, INodeFile file)\n     blockInfo.setBlockCollectionId(file.getId());\n     blockInfo.setGenerationStamp(genStamp);\n     blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo,\n-        genStamp);\n+        genStamp, true);\n     doReturn(blockInfo).when(file).removeLastBlock(any(Block.class));\n     doReturn(true).when(file).isUnderConstruction();\n     doReturn(new BlockInfoContiguous[1]).when(file).getBlocks();",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "sha": "071e7171f6ed213293420d7c515c4774a653ca74",
                "status": "modified"
            }
        ],
        "message": "HDFS-11817. A faulty node can cause a lease leak and NPE on accessing data. Contributed by Kihwal Lee.",
        "parent": "https://github.com/apache/hadoop/commit/87590090c887829e874a7132be9cf8de061437d6",
        "patched_files": [
            "DatanodeManager.java",
            "LeaseManager.java",
            "FSDirTruncateOp.java",
            "FSNamesystem.java",
            "BlockUnderConstructionFeature.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java",
            "TestDatanodeManager.java",
            "TestBlockUnderConstructionFeature.java",
            "TestCommitBlockSynchronization.java",
            "TestBlockUnderConstruction.java",
            "TestLeaseManager.java"
        ]
    },
    "hadoop_2f26475": {
        "bug_id": "hadoop_2f26475",
        "commit": "https://github.com/apache/hadoop/commit/2f26475a39f94e756ef4d15ff8c863b3f692a29e",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=2f26475a39f94e756ef4d15ff8c863b3f692a29e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -105,3 +105,5 @@ HDFS-2766. Test for case where standby partially reads log and then performs che\n HDFS-2738. FSEditLog.selectinputStreams is reading through in-progress streams even when non-in-progress are requested. (atm)\n \n HDFS-2789. TestHAAdmin.testFailover is failing (eli)\n+\n+HDFS-2747. Entering safe mode after starting SBN can NPE. (Uma Maheswara Rao G via todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "fc245ed3e690cc629d69542cf53170b90417b462",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=2f26475a39f94e756ef4d15ff8c863b3f692a29e",
                "deletions": 15,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3774,21 +3774,28 @@ private long getCompleteBlocksTotal() {\n   void enterSafeMode(boolean resourcesLow) throws IOException {\n     writeLock();\n     try {\n-    // Ensure that any concurrent operations have been fully synced\n-    // before entering safe mode. This ensures that the FSImage\n-    // is entirely stable on disk as soon as we're in safe mode.\n-    getEditLog().logSyncAll();\n-    if (!isInSafeMode()) {\n-      safeMode = new SafeModeInfo(resourcesLow);\n-      return;\n-    }\n-    if (resourcesLow) {\n-      safeMode.setResourcesLow();\n-    }\n-    safeMode.setManual();\n-    getEditLog().logSyncAll();\n-    NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \" \n-                                + safeMode.getTurnOffTip());\n+      // Ensure that any concurrent operations have been fully synced\n+      // before entering safe mode. This ensures that the FSImage\n+      // is entirely stable on disk as soon as we're in safe mode.\n+      boolean isEditlogOpenForWrite = getEditLog().isOpenForWrite();\n+      // Before Editlog is in OpenForWrite mode, editLogStream will be null. So,\n+      // logSyncAll call can be called only when Edlitlog is in OpenForWrite mode\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      if (!isInSafeMode()) {\n+        safeMode = new SafeModeInfo(resourcesLow);\n+        return;\n+      }\n+      if (resourcesLow) {\n+        safeMode.setResourcesLow();\n+      }\n+      safeMode.setManual();\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \"\n+          + safeMode.getTurnOffTip());\n     } finally {\n       writeUnlock();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "f1664f7e62d02065e7fb1231a7238e45d257ac00",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/hadoop/blob/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java?ref=2f26475a39f94e756ef4d15ff8c863b3f692a29e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter;\n import org.junit.After;\n@@ -95,6 +96,68 @@ private void restartStandby() throws IOException {\n     nn1.getNamesystem().getEditLogTailer().interrupt();\n   }\n   \n+  /**\n+   * Test case for enter safemode in active namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInANNShouldNotThrowNPE() throws Exception {\n+    banner(\"Restarting active\");\n+    restartActive();\n+    FSNamesystem namesystem = nn0.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  /**\n+   * Test case for enter safemode in standby namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInSBNShouldNotThrowNPE() throws Exception {\n+    banner(\"Starting with NN0 active and NN1 standby, creating some blocks\");\n+    DFSTestUtil\n+        .createFile(fs, new Path(\"/test\"), 3 * BLOCK_SIZE, (short) 3, 1L);\n+    // Roll edit log so that, when the SBN restarts, it will load\n+    // the namespace during startup and enter safemode.\n+    nn0.getRpcServer().rollEditLog();\n+    banner(\"Creating some blocks that won't be in the edit log\");\n+    DFSTestUtil.createFile(fs, new Path(\"/test2\"), 5 * BLOCK_SIZE, (short) 3,\n+        1L);\n+    banner(\"Deleting the original blocks\");\n+    fs.delete(new Path(\"/test\"), true);\n+    banner(\"Restarting standby\");\n+    restartStandby();\n+    FSNamesystem namesystem = nn1.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  private void restartActive() throws IOException {\n+    cluster.shutdownNameNode(0);\n+    // Set the safemode extension to be lengthy, so that the tests\n+    // can check the safemode message after the safemode conditions\n+    // have been achieved, without being racy.\n+    cluster.getConfiguration(0).setInt(\n+        DFSConfigKeys.DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, 30000);\n+    cluster.restartNameNode(0);\n+    nn0 = cluster.getNameNode(0);\n+  }\n+  \n   /**\n    * Tests the case where, while a standby is down, more blocks are\n    * added to the namespace, but not rolled. So, when it starts up,",
                "raw_url": "https://github.com/apache/hadoop/raw/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "sha": "af7985e21d37e0577092f1822490922f9fb88eec",
                "status": "modified"
            }
        ],
        "message": "HDFS-2747. Entering safe mode after starting SBN can NPE. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232176 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/1c24ae0cd82f1cf583a691a6fcd285ed806cc08e",
        "patched_files": [
            "CHANGES.java",
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestHASafeMode.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_2f623fb": {
        "bug_id": "hadoop_2f623fb",
        "commit": "https://github.com/apache/hadoop/commit/2f623fb8cc3dc49221216c3b46b6f51144811904",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2510,6 +2510,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9570. Minor typos, grammar, and case sensitivity cleanup in\n     HdfsPermissionsGuide.md's (Travis Campbell via aw)\n \n+    HDFS-9515. NPE when MiniDFSCluster#shutdown is invoked on uninitialized\n+    reference. (Wei-Chiu Chuang via Arpit Agarwal)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "59b3f75541ba439ce6c42a75c3e4ad0adfbd7a58",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBootstrapStandbyWithBKJM.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBootstrapStandbyWithBKJM.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBootstrapStandbyWithBKJM.java",
                "patch": "@@ -63,6 +63,7 @@ public static void teardownBookkeeper() throws Exception {\n   public void teardown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestBootstrapStandbyWithBKJM.java",
                "sha": "ef7f7086dab131354c411557d140cf0b393c498a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestRefreshCallQueue.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestRefreshCallQueue.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestRefreshCallQueue.java",
                "patch": "@@ -86,6 +86,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if(cluster!=null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestRefreshCallQueue.java",
                "sha": "1be2752fe0a8f3b7f2b54eff33142b131a8757a7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestAclCLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestAclCLI.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestAclCLI.java",
                "patch": "@@ -49,9 +49,11 @@ public void tearDown() throws Exception {\n     super.tearDown();\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestAclCLI.java",
                "sha": "061d328a52c09a8a26eec27ba0327d13dc80fc7a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCacheAdminCLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCacheAdminCLI.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCacheAdminCLI.java",
                "patch": "@@ -76,9 +76,11 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (dfsCluster != null) {\n       dfsCluster.shutdown();\n+      dfsCluster = null;\n     }\n     Thread.sleep(2000);\n     super.tearDown();",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCacheAdminCLI.java",
                "sha": "28321cb613156769f0c13afc4f22437975efa56f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCryptoAdminCLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCryptoAdminCLI.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCryptoAdminCLI.java",
                "patch": "@@ -86,9 +86,11 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (dfsCluster != null) {\n       dfsCluster.shutdown();\n+      dfsCluster = null;\n     }\n     Thread.sleep(2000);\n     super.tearDown();",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestCryptoAdminCLI.java",
                "sha": "bd5349d82d6bd5e17b0ca1fb5805fff2e46fe348",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestDeleteCLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestDeleteCLI.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestDeleteCLI.java",
                "patch": "@@ -58,9 +58,11 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (dfsCluster != null) {\n       dfsCluster.shutdown();\n+      dfsCluster = null;\n     }\n     Thread.sleep(2000);\n     super.tearDown();",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestDeleteCLI.java",
                "sha": "4c27f79e16a83413454f07771348202c06e076a4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestErasureCodingCLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestErasureCodingCLI.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestErasureCodingCLI.java",
                "patch": "@@ -61,9 +61,11 @@ protected String getTestFile() {\n   public void tearDown() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (dfsCluster != null) {\n       dfsCluster.shutdown();\n+      dfsCluster = null;\n     }\n     Thread.sleep(2000);\n     super.tearDown();",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestErasureCodingCLI.java",
                "sha": "664c0448ecdf7bdc0390e2fd11285b9a8895f540",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestHDFSCLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestHDFSCLI.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestHDFSCLI.java",
                "patch": "@@ -77,9 +77,11 @@ protected String getTestFile() {\n   public void tearDown() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (dfsCluster != null) {\n       dfsCluster.shutdown();\n+      dfsCluster = null;\n     }\n     Thread.sleep(2000);\n     super.tearDown();",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestHDFSCLI.java",
                "sha": "e0e78941c7384657dc193412574f7913b4fb0d25",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestXAttrCLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestXAttrCLI.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestXAttrCLI.java",
                "patch": "@@ -67,9 +67,11 @@ protected String getTestFile() {\n   public void tearDown() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (dfsCluster != null) {\n       dfsCluster.shutdown();\n+      dfsCluster = null;\n     }\n     Thread.sleep(2000);\n     super.tearDown();",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/cli/TestXAttrCLI.java",
                "sha": "d83baf3a97162a8c224b69b967583577a87cc096",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsCreateMkdir.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsCreateMkdir.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsCreateMkdir.java",
                "patch": "@@ -58,7 +58,9 @@ public static void clusterSetupAtBegining()\n       \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();   \n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n   \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsCreateMkdir.java",
                "sha": "0d3e6ff8379fff0a1dea3e81b791529bd4568d2f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsPermission.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsPermission.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsPermission.java",
                "patch": "@@ -65,7 +65,9 @@ public static void clusterSetupAtBegining()\n       \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();   \n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n   \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsPermission.java",
                "sha": "10ae1ef4b1e5e323efaf8cf990fc7c541dd21be7",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsSetUMask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsSetUMask.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsSetUMask.java",
                "patch": "@@ -96,7 +96,9 @@ public static void clusterSetupAtBegining()\n \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();   \n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n   \n   {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestFcHdfsSetUMask.java",
                "sha": "09163472ca09a662b61013c686fcc9254ddaa433",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java",
                "patch": "@@ -96,7 +96,9 @@ public static void beforeClassSetup() throws Exception {\n \n   @AfterClass\n   public static void afterClassTeardown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Test(timeout=10000)",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java",
                "sha": "042431e2a6cbf47aa85110e1e475a661a6707195",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/HDFSContract.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/HDFSContract.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/HDFSContract.java",
                "patch": "@@ -62,6 +62,7 @@ public static void createCluster() throws IOException {\n   public static void destroyCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/contract/hdfs/HDFSContract.java",
                "sha": "74b9a35adfcdb3f4693aacf32599f8a32382b394",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/shell/TestHdfsTextCommand.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/shell/TestHdfsTextCommand.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/shell/TestHdfsTextCommand.java",
                "patch": "@@ -60,9 +60,11 @@ public void setUp() throws IOException{\n     public void tearDown() throws IOException{\n     if(fs != null){\n       fs.close();\n+      fs = null;\n     }\n     if(cluster != null){\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/shell/TestHdfsTextCommand.java",
                "sha": "23de65852968424ac5a53dc1aae3c9cb84fbc591",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java",
                "patch": "@@ -88,7 +88,9 @@ public static void clusterSetupAtBegining() throws IOException,\n       \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();   \n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemHdfs.java",
                "sha": "922131727841c5f000e2971afd173d6d4d938a42",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithAcls.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithAcls.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithAcls.java",
                "patch": "@@ -76,7 +76,9 @@ public static void clusterSetupAtBeginning() throws IOException {\n \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Before",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithAcls.java",
                "sha": "a3e56570863969772d467749ac9b5a234cfd1bff",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithXAttrs.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithXAttrs.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithXAttrs.java",
                "patch": "@@ -71,7 +71,9 @@ public static void clusterSetupAtBeginning() throws IOException {\n \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Before",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithXAttrs.java",
                "sha": "b487188c4e1fa56d34083be23e3a18099d02867b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsAtHdfsRoot.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsAtHdfsRoot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsAtHdfsRoot.java",
                "patch": "@@ -64,7 +64,9 @@ public static void clusterSetupAtBegining() throws IOException,\n       \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();   \n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsAtHdfsRoot.java",
                "sha": "886646518838bec813965db5267ab8f0655669dc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsHdfs.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsHdfs.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsHdfs.java",
                "patch": "@@ -64,7 +64,9 @@ public static void clusterSetupAtBegining() throws IOException,\n       \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();   \n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsHdfs.java",
                "sha": "fda667251b8e05ac5c547739ba9875dc422230e9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithAcls.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithAcls.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithAcls.java",
                "patch": "@@ -76,7 +76,9 @@ public static void clusterSetupAtBeginning() throws IOException {\n \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Before",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithAcls.java",
                "sha": "e7e664857c851acb9a2ee1ff30161659bf706727",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithXAttrs.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithXAttrs.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithXAttrs.java",
                "patch": "@@ -70,7 +70,9 @@ public static void clusterSetupAtBeginning() throws IOException {\n \n   @AfterClass\n   public static void ClusterShutdownAtEnd() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Before",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithXAttrs.java",
                "sha": "9a4223a86fe20c69b2d76881947981c45931328a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/FileAppendTest4.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/FileAppendTest4.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/FileAppendTest4.java",
                "patch": "@@ -68,7 +68,9 @@ public static void startUp () throws IOException {\n \n   @AfterClass\n   public static void tearDown() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/FileAppendTest4.java",
                "sha": "9e7b598b9bdae2b8614fa881ffdc359b3fb40670",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java",
                "patch": "@@ -56,8 +56,14 @@ public void setUp() throws Exception {\n \n   @After\n   public void tearDown() throws Exception {\n-    fs.close();\n-    cluster.shutdown();\n+    if (fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAbandonBlock.java",
                "sha": "301f6a7d730e66ea75b1354741fc9e8a8b0b20c1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlocksScheduledCounter.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlocksScheduledCounter.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlocksScheduledCounter.java",
                "patch": "@@ -43,9 +43,11 @@\n   public void tearDown() throws IOException {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if(cluster!=null){\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlocksScheduledCounter.java",
                "sha": "b9432197489576543e7181c28dd429454211bd8c",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientReportBadBlock.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientReportBadBlock.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientReportBadBlock.java",
                "patch": "@@ -82,8 +82,14 @@ public void startUpCluster() throws IOException {\n \n   @After\n   public void shutDownCluster() throws IOException {\n-    dfs.close();\n-    cluster.shutdown();\n+    if (dfs != null) {\n+      dfs.close();\n+      dfs = null;\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   /*",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestClientReportBadBlock.java",
                "sha": "ce2e79b73e47e6f254dc43a78aa6b5118a65ae68",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientExcludedNodes.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientExcludedNodes.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientExcludedNodes.java",
                "patch": "@@ -54,6 +54,7 @@ public void setUp() {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientExcludedNodes.java",
                "sha": "59cc154071668cf5547bb14da6f2316ea74b5ffa",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "patch": "@@ -88,6 +88,7 @@ public void setUpCluster() throws IOException {\n   public void tearDownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "sha": "75635193eb2b369c9880ac284870d8e092502b4e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientSocketSize.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientSocketSize.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientSocketSize.java",
                "patch": "@@ -82,6 +82,7 @@ public void tearDown() throws Exception {\n     if (cluster != null) {\n       LOG.info(\"Shutting down MiniDFSCluster.\");\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientSocketSize.java",
                "sha": "23765763d366cdf30a6c9530256e6271382b7436",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSFinalize.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSFinalize.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSFinalize.java",
                "patch": "@@ -188,7 +188,10 @@ public void testFinalize() throws Exception {\n   @After\n   public void tearDown() throws Exception {\n     LOG.info(\"Shutting down MiniDFSCluster\");\n-    if (cluster != null) cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   public static void main(String[] args) throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSFinalize.java",
                "sha": "d0df9fecf5917a00ad57debacb93f12be539df97",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSOutputStream.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSOutputStream.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSOutputStream.java",
                "patch": "@@ -150,6 +150,8 @@ public void testCongestionBackoff() throws IOException {\n \n   @AfterClass\n   public static void tearDown() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSOutputStream.java",
                "sha": "a404ac8174f9e3bc725691a4076d46f0b4d48501",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSPermission.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSPermission.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSPermission.java",
                "patch": "@@ -123,6 +123,7 @@ public void setUp() throws IOException {\n   public void tearDown() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSPermission.java",
                "sha": "8f2f0a04e876279910a9893ec524fbe8419e22ea",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java",
                "patch": "@@ -346,7 +346,10 @@ private void deleteMatchingFiles(File[] baseDirs, String regex) {\n   @After\n   public void tearDown() throws Exception {\n     LOG.info(\"Shutting down MiniDFSCluster\");\n-    if (cluster != null) cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   public static void main(String[] args) throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java",
                "sha": "25eb5b685db7a4afb50a4d05b4ae7edaf72ef87d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStartupVersions.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStartupVersions.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStartupVersions.java",
                "patch": "@@ -282,7 +282,10 @@ public void testVersions() throws Exception {\n   @After\n   public void tearDown() throws Exception {\n     LOG.info(\"Shutting down MiniDFSCluster\");\n-    if (cluster != null) cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   public static void main(String[] args) throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStartupVersions.java",
                "sha": "d2022234b88113543d0898e00f2cda72a394f5c0",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStorageStateRecovery.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStorageStateRecovery.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStorageStateRecovery.java",
                "patch": "@@ -452,6 +452,9 @@ public void setUp() throws Exception {\n   @After\n   public void tearDown() throws Exception {\n     LOG.info(\"Shutting down MiniDFSCluster\");\n-    if (cluster != null) cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStorageStateRecovery.java",
                "sha": "cd51631ba8c27c412e3fa748ea966c1d2dd33985",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "patch": "@@ -86,6 +86,7 @@ public void setup() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "sha": "3b8141f329ddca978e85ebb337d43606debc27d1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStream.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStream.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStream.java",
                "patch": "@@ -69,6 +69,7 @@ public void setup() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedOutputStream.java",
                "sha": "1846196877ac770e891c0ceb76e3c2ace9312d12",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferKeepalive.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferKeepalive.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferKeepalive.java",
                "patch": "@@ -69,7 +69,10 @@ public void setup() throws Exception {\n   \n   @After\n   public void teardown() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDataTransferKeepalive.java",
                "sha": "9e3ddcfbc9ac477b0acad27a33a5d0f156abe2e3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java",
                "patch": "@@ -126,6 +126,7 @@ public void teardown() throws IOException {\n     cleanupFile(localFileSys, dir);\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java",
                "sha": "edc81ae8dc975a192a8eee30547918c9d5cc4898",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "patch": "@@ -170,6 +170,7 @@ protected void setProvider() {\n   public void teardown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n     EncryptionFaultInjector.instance = new EncryptionFaultInjector();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "sha": "66fef2dca9866ae04e5350190e072e9a243a871e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZonesWithHA.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZonesWithHA.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZonesWithHA.java",
                "patch": "@@ -87,6 +87,7 @@ public void setupCluster() throws Exception {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZonesWithHA.java",
                "sha": "476d024c7d6e293970c0c8d312ef90f0e3eb8314",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicies.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicies.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicies.java",
                "patch": "@@ -61,6 +61,7 @@ public void setupCluster() throws IOException {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicies.java",
                "sha": "dcb528cd542fdd78d5be5c02f489a1605e179225",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicyWithSnapshot.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicyWithSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicyWithSnapshot.java",
                "patch": "@@ -57,6 +57,7 @@ public void setupCluster() throws IOException {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestErasureCodingPolicyWithSnapshot.java",
                "sha": "6b10d7e183719005599ff0bb961c44e80075ae8f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java",
                "patch": "@@ -84,8 +84,10 @@ public void setUp() throws IOException {\n \n   @After\n   public void tearDown() throws Exception {\n-    cluster.shutdown();\n-    cluster = null;\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   private void init(Configuration conf) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileConcurrentReader.java",
                "sha": "e0a0d3be2ed933ed15936b372f0cc25f38e8dccf",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatus.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatus.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatus.java",
                "patch": "@@ -77,8 +77,12 @@ public static void testSetUp() throws Exception {\n   \n   @AfterClass\n   public static void testTearDown() throws Exception {\n-    fs.close();\n-    cluster.shutdown();\n+    if (fs != null) {\n+      fs.close();\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n   \n   private void checkFile(FileSystem fileSys, Path name, int repl)",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatus.java",
                "sha": "dcb7af954bfcc60c194b3a5e3bbc04d150d16745",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatusWithECPolicy.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatusWithECPolicy.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatusWithECPolicy.java",
                "patch": "@@ -50,6 +50,7 @@ public void before() throws IOException {\n   public void after() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileStatusWithECPolicy.java",
                "sha": "adb66cabaec84763bfb285ef58cfe3dc1ad161c5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetFileChecksum.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetFileChecksum.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetFileChecksum.java",
                "patch": "@@ -52,6 +52,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetFileChecksum.java",
                "sha": "cf61e8451bfe07153f3010edac2043d639b43124",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSFileSystemContract.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSFileSystemContract.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSFileSystemContract.java",
                "patch": "@@ -45,8 +45,10 @@ protected void setUp() throws Exception {\n   @Override\n   protected void tearDown() throws Exception {\n     super.tearDown();\n-    cluster.shutdown();\n-    cluster = null;\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSFileSystemContract.java",
                "sha": "c1bf6f2f28f4da1c56e207f460c9ed75a0bd7601",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHdfsAdmin.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHdfsAdmin.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHdfsAdmin.java",
                "patch": "@@ -47,6 +47,7 @@ public void setUpCluster() throws IOException {\n   public void shutDownCluster() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHdfsAdmin.java",
                "sha": "0f5bdf5c21dc39b48811c429d4b415731c722e11",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java",
                "patch": "@@ -57,6 +57,7 @@\n   public void shutdown() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery.java",
                "sha": "d62194c16a264310fddc272de63699b905bb2aac",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java",
                "patch": "@@ -101,8 +101,10 @@ public static void startUp() throws IOException {\n    */\n   @AfterClass\n   public static void tearDown() throws IOException {\n-    IOUtils.closeStream(dfs);\n-    if (cluster != null) {cluster.shutdown();}\n+    if (cluster != null) {\n+      IOUtils.closeStream(dfs);\n+      cluster.shutdown();\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java",
                "sha": "13e864441d641332d7b546e74f69885c55378115",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInDFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInDFS.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInDFS.java",
                "patch": "@@ -46,8 +46,10 @@ public static void testSetUp() throws Exception {\n   \n   @AfterClass\n   public static void testShutdown() throws Exception {\n-    fs.close();\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      fs.close();\n+      cluster.shutdown();\n+    }\n   }\n   \n   protected static Path getTestDir() {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInDFS.java",
                "sha": "53b21d0b51455f4b34ae7e2eadbe2c2aaa3bfb48",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInFileContext.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInFileContext.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInFileContext.java",
                "patch": "@@ -83,7 +83,9 @@ private static void writeFile(FileContext fc, Path name, int fileSize)\n   \n   @AfterClass\n   public static void testShutdown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   /** Test when input path is a file */",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestListFilesInFileContext.java",
                "sha": "1e424177e954f3a64172231b08f7edaf7121fc4d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPipelines.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPipelines.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPipelines.java",
                "patch": "@@ -65,11 +65,14 @@ public void startUpCluster() throws IOException {\n \n   @After\n   public void shutDownCluster() throws IOException {\n-    if (fs != null)\n+    if (fs != null) {\n       fs.close();\n+      fs = null;\n+    }\n     if (cluster != null) {\n       cluster.shutdownDataNodes();\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPipelines.java",
                "sha": "5804d35fef725fd41b53306207cf9944eed6aefd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithDecoding.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithDecoding.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithDecoding.java",
                "patch": "@@ -77,6 +77,7 @@ public void setup() throws IOException {\n   public void tearDown() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithDecoding.java",
                "sha": "32b0216d739cb8c6e606a3934c2d041d8700d1b2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithMissingBlocks.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithMissingBlocks.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithMissingBlocks.java",
                "patch": "@@ -61,6 +61,7 @@ public void setup() throws IOException {\n   public void tearDown() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReadStripedFileWithMissingBlocks.java",
                "sha": "a362de8351ec1c05ae70f97ec612d7047b4fec35",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRecoverStripedFile.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRecoverStripedFile.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRecoverStripedFile.java",
                "patch": "@@ -97,6 +97,7 @@ public void setup() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRecoverStripedFile.java",
                "sha": "78349674f70396b8416411648dfabee795326e54",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java",
                "patch": "@@ -91,6 +91,7 @@ public void setup() throws Exception {\n   public void teardown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java",
                "sha": "84cfd80d287439ac6a83b42bc7a86e3f45ae32e5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeMode.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeMode.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeMode.java",
                "patch": "@@ -90,9 +90,11 @@ public void startUp() throws IOException {\n   public void tearDown() throws IOException {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeMode.java",
                "sha": "4940d451d18b416825b14cb01eabdc218d888512",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeModeWithStripedFile.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeModeWithStripedFile.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeModeWithStripedFile.java",
                "patch": "@@ -63,6 +63,7 @@ public void setup() throws IOException {\n   public void tearDown() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSafeModeWithStripedFile.java",
                "sha": "acfdddbecf211e901f6410da72f12291951ddb57",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteRead.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteRead.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteRead.java",
                "patch": "@@ -86,7 +86,10 @@ public void initJunitModeTest() throws Exception {\n \n   @After\n   public void shutdown() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   // Equivalence of @Before for cluster mode testing.",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteRead.java",
                "sha": "623dafff8b9534a6e6c29376161e275e8ddeb5d5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteReadStripedFile.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteReadStripedFile.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteReadStripedFile.java",
                "patch": "@@ -76,6 +76,7 @@ public void setup() throws IOException {\n   public void tearDown() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestWriteReadStripedFile.java",
                "sha": "4e5fbe44bd7b4a24a9d8ab50c0bba1ec0b3a0075",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/TestSaslDataTransfer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/TestSaslDataTransfer.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/TestSaslDataTransfer.java",
                "patch": "@@ -67,6 +67,7 @@ public void shutdown() {\n     IOUtils.cleanup(null, fs);\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/TestSaslDataTransfer.java",
                "sha": "2d4eb0d2b439774fe7564038c7a7914ebeb9c548",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java",
                "patch": "@@ -154,9 +154,11 @@ public void shutdown() throws IOException {\n     IOUtils.cleanup(null, fs);\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n     if (mjc != null) {\n       mjc.shutdown();\n+      mjc = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java",
                "sha": "197759edf58a9fd48d5dbd007f906f7c422b598a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java",
                "patch": "@@ -114,6 +114,7 @@ public void shutdown() throws IOException {\n     \n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java",
                "sha": "3ba573c919afa29c40194fe95404364a2a0e4244",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeMXBean.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeMXBean.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeMXBean.java",
                "patch": "@@ -61,6 +61,7 @@ public void setup() throws IOException {\n   public void cleanup() throws IOException {\n     if (jCluster != null) {\n       jCluster.shutdown();\n+      jCluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeMXBean.java",
                "sha": "45b245a64b248dc156d7ab278f40729a21e1b3c7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/TestDelegationToken.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/TestDelegationToken.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/TestDelegationToken.java",
                "patch": "@@ -84,6 +84,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if(cluster!=null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/TestDelegationToken.java",
                "sha": "f4aae8f9859b321d3921e48e7bf92b33ca52ee1d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java",
                "patch": "@@ -67,6 +67,7 @@ public void setup() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockStatsMXBean.java",
                "sha": "f5b56414bf215525c787cf107b1fed974001b191",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java",
                "patch": "@@ -70,6 +70,7 @@ public void setup() throws Exception {\n   public void teardown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestComputeInvalidateWork.java",
                "sha": "07455cf3a0677b41b7d566d26bb1386dc22c201f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingInvalidateBlock.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingInvalidateBlock.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingInvalidateBlock.java",
                "patch": "@@ -74,6 +74,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestPendingInvalidateBlock.java",
                "sha": "a588a73150e310b4efa3adae26e936dc1ac5f430",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSequentialBlockGroupId.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSequentialBlockGroupId.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSequentialBlockGroupId.java",
                "patch": "@@ -93,6 +93,7 @@ public void setup() throws Exception {\n   public void teardown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestSequentialBlockGroupId.java",
                "sha": "648745c3480339a2fc734cfa6e3eec5ced981fd9",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java",
                "patch": "@@ -115,10 +115,12 @@ public void startUpCluster() throws IOException {\n   public void shutDownCluster() throws IOException {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (cluster != null) {\n       cluster.shutdownDataNodes();\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java",
                "sha": "4279f96a001411385424390555c0e7ff109305e8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeExit.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeExit.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeExit.java",
                "patch": "@@ -59,8 +59,10 @@ public void setUp() throws IOException {\n \n   @After\n   public void tearDown() throws Exception {\n-    if (cluster != null)\n+    if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   private void stopBPServiceThreads(int numStopThreads, DataNode dn)",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeExit.java",
                "sha": "918ad83bbd4376e039d01e13f8c97e76a464f98c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java",
                "patch": "@@ -116,12 +116,15 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if(data_fail != null) {\n       FileUtil.setWritable(data_fail, true);\n+      data_fail = null;\n     }\n     if(failedDir != null) {\n       FileUtil.setWritable(failedDir, true);\n+      failedDir = null;\n     }\n     if(cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailure.java",
                "sha": "90e000bb3bc43ba17912b809c7ae25020b136725",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java",
                "patch": "@@ -89,6 +89,7 @@ public void tearDown() throws Exception {\n     IOUtils.cleanup(LOG, fs);\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java",
                "sha": "d25a8a2edf4c2c5876a463e97654068931f75536",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java",
                "patch": "@@ -78,6 +78,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java",
                "sha": "1eb8bcaf77d6de625706e58f971ae411eef4229c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "patch": "@@ -71,7 +71,10 @@ public void setUp() throws Exception {\n \n   @After\n   public void tearDown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "sha": "000277a8416c6744a0425d4d21cd1ee9b3a66e9d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java",
                "patch": "@@ -149,9 +149,11 @@ public void tearDown() throws Exception {\n     DFSTestUtil.verifyExpectedCacheUsage(0, 0, fsd);\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n     // Restore the original CacheManipulator\n     NativeIO.POSIX.setCacheManipulator(prevCacheManipulator);",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java",
                "sha": "77b0c44789784835d18b7122eeec8c911c2e68ae",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java",
                "patch": "@@ -101,10 +101,13 @@ public void startUpCluster() throws IOException {\n \n   @After\n   public void shutDownCluster() throws IOException {\n-    client.close();\n-    fs.close();\n-    cluster.shutdownDataNodes();\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      client.close();\n+      fs.close();\n+      cluster.shutdownDataNodes();\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestIncrementalBrVariations.java",
                "sha": "f2570c205dc4df15295574452e5b35cc50fd9612",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclConfigFlag.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclConfigFlag.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclConfigFlag.java",
                "patch": "@@ -58,6 +58,7 @@ public void shutdown() throws Exception {\n     IOUtils.cleanup(null, fs);\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclConfigFlag.java",
                "sha": "36539e59c99e18f2f069079888bdd3c851c887ab",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlock.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlock.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlock.java",
                "patch": "@@ -60,6 +60,7 @@ public void setup() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlock.java",
                "sha": "9e9890f10316a49e6cebd28af7f0f26942be00be",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java",
                "patch": "@@ -66,6 +66,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java",
                "sha": "a8cd5b9a825ac9c8b54b9da53e043696ce98d584",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "patch": "@@ -75,6 +75,7 @@ public void setup() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "sha": "2be9726379e9ab3d0afe8e07f3c0af3610fd084b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddStripedBlocks.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddStripedBlocks.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddStripedBlocks.java",
                "patch": "@@ -82,6 +82,7 @@ public void setup() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddStripedBlocks.java",
                "sha": "7d740121e338ee8fc5964b809ff2126f0c2b92c2",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogs.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogs.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogs.java",
                "patch": "@@ -137,8 +137,14 @@ public void setupCluster() throws Exception {\n   @After\n   public void teardownCluster() throws Exception {\n     util.cleanup(fs, \"/srcdat\");\n-    fs.close();\n-    cluster.shutdown();\n+    if (fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   /** test that allowed operation puts proper entry in audit log */",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogs.java",
                "sha": "37b30637ada7346e3c9d0f1b26eb9c6b61edf083",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "patch": "@@ -494,7 +494,9 @@ void testCheckpoint(StartupOption op) throws Exception {\n       assertTrue(e.getLocalizedMessage(), false);\n     } finally {\n       fileSys.close();\n-      cluster.shutdown();\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "sha": "5ad9d9b41ba447556f6b701529611f22f7f244d0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockPlacementPolicyRackFaultTolerant.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockPlacementPolicyRackFaultTolerant.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockPlacementPolicyRackFaultTolerant.java",
                "patch": "@@ -83,6 +83,7 @@ public void setup() throws IOException {\n   public void teardown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockPlacementPolicyRackFaultTolerant.java",
                "sha": "9f844d7fc7e3e02b59b67d67a0b143fc06019424",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "patch": "@@ -158,6 +158,7 @@ public void teardown() throws Exception {\n     waitForCachedBlocks(namenode, 0, 0, \"teardown\");\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n     // Restore the original CacheManipulator\n     NativeIO.POSIX.setCacheManipulator(prevCacheManipulator);",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java",
                "sha": "45d819e50a9a901ec8f75f03059a1119dd4e658e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockWithInvalidGenStamp.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockWithInvalidGenStamp.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockWithInvalidGenStamp.java",
                "patch": "@@ -58,6 +58,7 @@ public void setUp() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockWithInvalidGenStamp.java",
                "sha": "77a32680f32072783c5bc93529f7a6ad13220bbd",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java",
                "patch": "@@ -62,7 +62,10 @@\n \n   @After\n   public void cleanup() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDeadDatanode.java",
                "sha": "7fd0c30046249762cd9eccc43448cde2bcb62c73",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDefaultBlockPlacementPolicy.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDefaultBlockPlacementPolicy.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDefaultBlockPlacementPolicy.java",
                "patch": "@@ -71,6 +71,7 @@ public void setup() throws IOException {\n   public void teardown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDefaultBlockPlacementPolicy.java",
                "sha": "a54040b2e8d0d0aa9abf2b18898bed0af00314b6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDiskspaceQuotaUpdate.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDiskspaceQuotaUpdate.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDiskspaceQuotaUpdate.java",
                "patch": "@@ -71,6 +71,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestDiskspaceQuotaUpdate.java",
                "sha": "d459db09b2e911dc02ca33eeab9ff0c2b4240c98",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogAutoroll.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogAutoroll.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogAutoroll.java",
                "patch": "@@ -97,9 +97,11 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogAutoroll.java",
                "sha": "f22ee2fcc24b0ae913a4155fe773acbc3404331b",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogJournalFailures.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogJournalFailures.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogJournalFailures.java",
                "patch": "@@ -70,11 +70,14 @@ public void setUpMiniCluster(Configuration conf, boolean manageNameDfsDirs)\n   \n   @After\n   public void shutDownMiniCluster() throws IOException {\n-    if (fs != null)\n+    if (fs != null) {\n       fs.close();\n+      fs = null;\n+    }\n     if (cluster != null) {\n       try {\n         cluster.shutdown();\n+        cluster = null;\n       } catch (ExitException ee) {\n         // Ignore ExitExceptions as the tests may result in the\n         // NameNode doing an immediate shutdown.",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogJournalFailures.java",
                "sha": "51dfc3e993de903714f331bcf18453ac23af7fad",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSDirectory.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSDirectory.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSDirectory.java",
                "patch": "@@ -108,6 +108,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSDirectory.java",
                "sha": "2b43c0f953603614dccc687d04b488df2594240f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java",
                "patch": "@@ -55,7 +55,9 @@ public static void setUp() throws IOException {\n \n   @AfterClass\n   public static void tearDown() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   private void testAcl(boolean persistNamespace) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java",
                "sha": "48d3dea81c3e725b9f62e1e53ea1b3d90f396830",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "patch": "@@ -89,6 +89,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "sha": "1904bbc122eac4ccf852e7e8cf0db7bde79c739d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithXAttr.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithXAttr.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithXAttr.java",
                "patch": "@@ -62,7 +62,9 @@ public static void setUp() throws IOException {\n \n   @AfterClass\n   public static void tearDown() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   private void testXAttr(boolean persistNamespace) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithXAttr.java",
                "sha": "4cef3214e9bab4a629c135649f81c9028d422742",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileTruncate.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileTruncate.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileTruncate.java",
                "patch": "@@ -110,8 +110,14 @@ public void setUp() throws IOException {\n \n   @After\n   public void tearDown() throws IOException {\n-    if(fs != null)      fs.close();\n-    if(cluster != null) cluster.shutdown();\n+    if(fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+    if(cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileTruncate.java",
                "sha": "56846af87693f4fc0b233a3fea4faa7dbecb851b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java",
                "patch": "@@ -84,10 +84,12 @@ public void startUpCluster() throws IOException {\n   public void shutDownCluster() throws IOException {\n     if(dfs != null) {\n       dfs.close();\n+      dfs = null;\n     }\n     if(cluster != null) {\n       cluster.shutdownDataNodes();\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestHDFSConcat.java",
                "sha": "b5e0efe0649ecd49b31efa73d7291f949ff95218",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "patch": "@@ -186,6 +186,7 @@ public void cleanUp() throws IOException {\n     CALLED.clear();\n     if (miniDFS != null) {\n       miniDFS.shutdown();\n+      miniDFS = null;\n     }\n     Assert.assertTrue(CALLED.contains(\"stop\"));\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "sha": "ffdc5356ac2cd3fe472b11fb56ca297a672c5d2a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMalformedURLs.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMalformedURLs.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMalformedURLs.java",
                "patch": "@@ -54,6 +54,7 @@ public void testTryStartingCluster() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMalformedURLs.java",
                "sha": "0c71445b0eb5d06011e19f57f7b9ce0ee9611a04",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetadataVersionOutput.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetadataVersionOutput.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetadataVersionOutput.java",
                "patch": "@@ -45,6 +45,7 @@\n   public void tearDown() throws Exception {\n     if (dfsCluster != null) {\n       dfsCluster.shutdown();\n+      dfsCluster = null;\n     }\n     Thread.sleep(2000);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetadataVersionOutput.java",
                "sha": "dc8f70a5b5bd59e8d08c3c40f93a99fc143b5acd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetadataConsistency.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetadataConsistency.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetadataConsistency.java",
                "patch": "@@ -63,6 +63,7 @@ public void InitTest() throws IOException {\n   public void cleanup() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMetadataConsistency.java",
                "sha": "d4fb0d17cc53a2b8817863818381b74b89080b7c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRetryCacheMetrics.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRetryCacheMetrics.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRetryCacheMetrics.java",
                "patch": "@@ -80,6 +80,7 @@ public void setup() throws Exception {\n   public void cleanup() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRetryCacheMetrics.java",
                "sha": "fb3a84083cd7923388c4ef4924b584ad434ee610",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "patch": "@@ -55,8 +55,10 @@ public void setup() throws Exception {\n    */\n   @After\n   public void cleanup() throws IOException {\n-    if (cluster != null)\n+    if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "sha": "a32e2188e88224fe3eec102b3a89538e6398df03",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java",
                "patch": "@@ -105,7 +105,10 @@ public void setup() throws Exception {\n    * @throws AccessControlException */\n   @After\n   public void cleanup() throws IOException {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   /** Set the current Server RPC call */",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java",
                "sha": "767562cf9ceed5b36df6bb5c4c71a2c13dcdd33e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaByStorageType.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaByStorageType.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaByStorageType.java",
                "patch": "@@ -74,6 +74,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaByStorageType.java",
                "sha": "ee02fd0f9f853af0334fd7bd6c0412f56e023db9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaWithStripedBlocks.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaWithStripedBlocks.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaWithStripedBlocks.java",
                "patch": "@@ -76,6 +76,7 @@ public void setUp() throws IOException {\n   public void tearDown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestQuotaWithStripedBlocks.java",
                "sha": "9aa0e07c85bc2bbed42fc95be72c937f5310e9bc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestXAttrConfigFlag.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestXAttrConfigFlag.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestXAttrConfigFlag.java",
                "patch": "@@ -51,6 +51,7 @@ public void shutdown() throws Exception {\n     IOUtils.cleanup(null, fs);\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestXAttrConfigFlag.java",
                "sha": "5064a0339d278ab46cd97612ea9f8f3f57ee961f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java",
                "patch": "@@ -90,6 +90,7 @@ public void setupCluster() throws IOException {\n   public void shutdownCluster() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java",
                "sha": "8c39548fc83e9d9224b73f6d40a326e8634a6c96",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandbyWithQJM.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandbyWithQJM.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandbyWithQJM.java",
                "patch": "@@ -81,9 +81,11 @@ public void setup() throws Exception {\n   public void cleanup() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n     if (jCluster != null) {\n       jCluster.shutdown();\n+      jCluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandbyWithQJM.java",
                "sha": "10194baf2dd856f211927bb53e567c8c3eebb88d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDNFencing.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDNFencing.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDNFencing.java",
                "patch": "@@ -118,6 +118,7 @@ public void shutdownCluster() throws Exception {\n       banner(\"Shutting down cluster. NN2 metadata:\");\n       doMetasave(nn2);\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDNFencing.java",
                "sha": "96822d625b1709c0449bb8b11fbc0d13ff2100e9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java",
                "patch": "@@ -107,6 +107,7 @@ public void setupCluster() throws Exception {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java",
                "sha": "16b27d87851c924889af605d50157f280b858156",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailoverWithBlockTokensEnabled.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailoverWithBlockTokensEnabled.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailoverWithBlockTokensEnabled.java",
                "patch": "@@ -77,6 +77,7 @@ public void startCluster() throws IOException {\n   public void shutDownCluster() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailoverWithBlockTokensEnabled.java",
                "sha": "43ab69d82d101483018bbc5c4df0a1fae6cc7334",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailureToReadEdits.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailureToReadEdits.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailureToReadEdits.java",
                "patch": "@@ -130,15 +130,18 @@ public void setUpCluster() throws Exception {\n   public void tearDownCluster() throws Exception {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     \n     if (clusterType == TestType.SHARED_DIR_HA) {\n       if (cluster != null) {\n         cluster.shutdown();\n+        cluster = null;\n       }\n     } else {\n       if (miniQjmHaCluster != null) {\n         miniQjmHaCluster.shutdown();\n+        miniQjmHaCluster = null;\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestFailureToReadEdits.java",
                "sha": "f82b6164fca6e16cee937425bab5fb347d421587",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestGetGroupsWithHA.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestGetGroupsWithHA.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestGetGroupsWithHA.java",
                "patch": "@@ -46,6 +46,7 @@ public void setUpNameNode() throws IOException {\n   public void tearDownNameNode() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestGetGroupsWithHA.java",
                "sha": "c3c22759d967a2adb209e4e8c88ef5155c2dcc3d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "patch": "@@ -112,6 +112,7 @@ public void setupCluster() throws Exception {\n   public void shutdownCluster() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "sha": "4b1d27d5b9048ab1d23489359ebeb9596a9ed954",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java",
                "patch": "@@ -78,6 +78,7 @@ public void setupCluster() throws IOException {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestInitializeSharedEdits.java",
                "sha": "856ed8fbc8670b675dea3652a6a725ab27435131",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestNNHealthCheck.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestNNHealthCheck.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestNNHealthCheck.java",
                "patch": "@@ -53,6 +53,7 @@ public void setup() {\n   public void shutdown() {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestNNHealthCheck.java",
                "sha": "22d4ef41afc04935725c6283675edd80f664e2e6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestQuotasWithHA.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestQuotasWithHA.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestQuotasWithHA.java",
                "patch": "@@ -80,6 +80,7 @@ public void setupCluster() throws Exception {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestQuotasWithHA.java",
                "sha": "e768306cfa587163b5951ead0bc23fa1f48985ac",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java",
                "patch": "@@ -148,6 +148,7 @@ public void setup() throws Exception {\n   public void cleanup() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java",
                "sha": "b4c383afa2f3d105ba7d6c835442a41a412acb57",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyCheckpoints.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyCheckpoints.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyCheckpoints.java",
                "patch": "@@ -139,6 +139,7 @@ protected Configuration setupCommonConfig() {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestStandbyCheckpoints.java",
                "sha": "37d346aafca3b11638c497ec7512f501776f0abf",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestXAttrsWithHA.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestXAttrsWithHA.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestXAttrsWithHA.java",
                "patch": "@@ -82,6 +82,7 @@ public void setupCluster() throws Exception {\n   public void shutdownCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestXAttrsWithHA.java",
                "sha": "bb44d05e2c644651cd6ba605e4ccb77816ca5174",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNNMetricFilesInGetListingOps.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNNMetricFilesInGetListingOps.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNNMetricFilesInGetListingOps.java",
                "patch": "@@ -62,7 +62,10 @@ public void setUp() throws Exception {\n \n   @After\n   public void tearDown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   /** create a file with a length of <code>fileLen</code> */",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNNMetricFilesInGetListingOps.java",
                "sha": "110615d9098871cfe43884dbd40fd24d181e11f6",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java",
                "patch": "@@ -124,7 +124,10 @@ public void tearDown() throws Exception {\n       MetricsRecordBuilder rb = getMetrics(source);\n       assertQuantileGauges(\"GetGroups1s\", rb);\n     }\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   /** create a file with a length of <code>fileLen</code> */",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java",
                "sha": "a9eeaac0f5865873cd5f5850c7f5b53ed2cf4370",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestFileContextSnapshot.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestFileContextSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestFileContextSnapshot.java",
                "patch": "@@ -67,6 +67,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestFileContextSnapshot.java",
                "sha": "a6cd1dd9b5ad8f4e794a2ea36877ee8faa641e89",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestGetContentSummaryWithSnapshot.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestGetContentSummaryWithSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestGetContentSummaryWithSnapshot.java",
                "patch": "@@ -70,6 +70,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestGetContentSummaryWithSnapshot.java",
                "sha": "dc6f584a2b8b8d2f45b986821f1edb3407c610a9",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestINodeFileUnderConstructionWithSnapshot.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestINodeFileUnderConstructionWithSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestINodeFileUnderConstructionWithSnapshot.java",
                "patch": "@@ -91,6 +91,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestINodeFileUnderConstructionWithSnapshot.java",
                "sha": "65b2f8c3f93232d708561396abe84ee0c7a61a2f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestNestedSnapshots.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestNestedSnapshots.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestNestedSnapshots.java",
                "patch": "@@ -78,6 +78,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestNestedSnapshots.java",
                "sha": "5b7ed41f42e951bae585683618577dc81f0d4649",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java",
                "patch": "@@ -52,9 +52,11 @@ public void setup() throws IOException {\n   public void teardown() throws IOException {\n     if (fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n \n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java",
                "sha": "694d15e63f33e6958622c7a7752abeeb599b8ab7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "patch": "@@ -122,6 +122,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "sha": "827feb6a2a8f860da34a2a407e69bde751a12797",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSetQuotaWithSnapshot.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSetQuotaWithSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSetQuotaWithSnapshot.java",
                "patch": "@@ -76,6 +76,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSetQuotaWithSnapshot.java",
                "sha": "c5ac26ed95a324d57a92969bbffabf36c2374574",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java",
                "patch": "@@ -131,6 +131,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java",
                "sha": "6313ce1aa4dc47b23279b0d4b3ee08d3cc318ab4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java",
                "patch": "@@ -84,6 +84,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java",
                "sha": "76f1e8f8beb12d28057bd9804272f0df8f13abd4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java",
                "patch": "@@ -106,6 +106,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n     ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java",
                "sha": "bf462da2f0f3dcd71c4e0fe2d7705f320df55d63",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
                "patch": "@@ -71,6 +71,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java",
                "sha": "453afacddac6ab5c4668963ee3e3d0463f03dfad",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotListing.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotListing.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotListing.java",
                "patch": "@@ -61,6 +61,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotListing.java",
                "sha": "03a2ff4e64e5fa0df1bb8bd69d01f33195c734a2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java",
                "patch": "@@ -69,6 +69,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java",
                "sha": "f64caff75bc3c1115c205a2b90a1da3d3d883bee",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotRename.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotRename.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotRename.java",
                "patch": "@@ -82,6 +82,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotRename.java",
                "sha": "8c8fca71e8238f49772e9cfbee0dd329a05968f2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotReplication.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotReplication.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotReplication.java",
                "patch": "@@ -75,6 +75,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotReplication.java",
                "sha": "d073228a70058e69793b22653a615fbee949d0a3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshottableDirListing.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshottableDirListing.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshottableDirListing.java",
                "patch": "@@ -67,6 +67,7 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if (cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshottableDirListing.java",
                "sha": "5611eb925a65b5542d77b4b5c745524a1a347a33",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java",
                "patch": "@@ -82,7 +82,10 @@ public void setup() throws IOException {\n \n   @After\n   public void shutdown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java",
                "sha": "a21a31d9e0feb8456e188f57625daa6162a2c5ce",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSZKFailoverController.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSZKFailoverController.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSZKFailoverController.java",
                "patch": "@@ -111,16 +111,22 @@ public void setup() throws Exception {\n   \n   @After\n   public void shutdown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n     \n     if (thr1 != null) {\n       thr1.interrupt();\n+      thr1 = null;\n     }\n     if (thr2 != null) {\n       thr2.interrupt();\n+      thr2 = null;\n     }\n     if (ctx != null) {\n       ctx.stop();\n+      ctx = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSZKFailoverController.java",
                "sha": "94cccedc02f33dc80189d1e8538f950fa38c08fd",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetGroups.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetGroups.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetGroups.java",
                "patch": "@@ -42,7 +42,10 @@ public void setUpNameNode() throws IOException {\n   \n   @After\n   public void tearDownNameNode() {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetGroups.java",
                "sha": "3d9631dfe122289ed975b9bdefd4c01932b4ae12",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestStoragePolicyCommands.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestStoragePolicyCommands.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestStoragePolicyCommands.java",
                "patch": "@@ -54,9 +54,11 @@ public void clusterSetUp() throws IOException {\n   public void clusterShutdown() throws IOException{\n     if(fs != null) {\n       fs.close();\n+      fs = null;\n     }\n     if(cluster != null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestStoragePolicyCommands.java",
                "sha": "ec0bb665de44024ed5ebc47d852a3bc344f81ba5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestHttpsFileSystem.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestHttpsFileSystem.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestHttpsFileSystem.java",
                "patch": "@@ -79,7 +79,9 @@ public static void setUp() throws Exception {\n \n   @AfterClass\n   public static void tearDown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n     FileUtil.fullyDelete(new File(BASEDIR));\n     KeyStoreTestUtil.cleanupSSLConfig(keystoresDir, sslConfDir);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestHttpsFileSystem.java",
                "sha": "0f0ac3bf1036ffe942ecf66381e2b5db605b2629",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsWithAuthenticationFilter.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsWithAuthenticationFilter.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsWithAuthenticationFilter.java",
                "patch": "@@ -84,8 +84,12 @@ public static void setUp() throws IOException {\n \n   @AfterClass\n   public static void tearDown() throws IOException {\n-    fs.close();\n-    cluster.shutdown();\n+    if (fs != null) {\n+      fs.close();\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsWithAuthenticationFilter.java",
                "sha": "f21fde46224ac50295f53e6565edb2e6f6a92bfd",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestRefreshUserMappings.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestRefreshUserMappings.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestRefreshUserMappings.java",
                "patch": "@@ -99,10 +99,12 @@ public void setUp() throws Exception {\n   public void tearDown() throws Exception {\n     if(cluster!=null) {\n       cluster.shutdown();\n+      cluster = null;\n     }\n     if(tempResource!=null) {\n       File f = new File(tempResource);\n       f.delete();\n+      tempResource = null;\n     }\n   }\n     ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestRefreshUserMappings.java",
                "sha": "c76033c4a5614be3b98e675273c0e0cc37a93091",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tools/TestJMXGet.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tools/TestJMXGet.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 7,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tools/TestJMXGet.java",
                "patch": "@@ -73,13 +73,16 @@ public void setUp() throws Exception {\n    */\n   @After\n   public void tearDown() throws Exception {\n-    if(cluster.isClusterUp())\n-      cluster.shutdown();\n-\n-    File data_dir = new File(cluster.getDataDirectory());\n-    if(data_dir.exists() && !FileUtil.fullyDelete(data_dir)) {\n-      throw new IOException(\"Could not delete hdfs directory in tearDown '\"\n-          + data_dir + \"'\");\n+    if (cluster != null) {\n+      if (cluster.isClusterUp()) {\n+        cluster.shutdown();\n+      }\n+      File data_dir = new File(cluster.getDataDirectory());\n+      if (data_dir.exists() && !FileUtil.fullyDelete(data_dir)) {\n+        throw new IOException(\n+            \"Could not delete hdfs directory in tearDown '\" + data_dir + \"'\");\n+      }\n+      cluster = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tools/TestJMXGet.java",
                "sha": "a9e41ec17775b4a2a1dece9b7e18af454affdd62",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tracing/TestTracing.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tracing/TestTracing.java?ref=2f623fb8cc3dc49221216c3b46b6f51144811904",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tracing/TestTracing.java",
                "patch": "@@ -222,7 +222,10 @@ public void startCluster() throws IOException {\n \n   @After\n   public void shutDown() throws IOException {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n     FsTracer.clear();\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f623fb8cc3dc49221216c3b46b6f51144811904/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/tracing/TestTracing.java",
                "sha": "bdad46a4e6f779c97654c6f8543cd0e6a3f92ebf",
                "status": "modified"
            }
        ],
        "message": "HDFS-9515. NPE when MiniDFSCluster#shutdown is invoked on uninitialized reference. (Contributed by Wei-Chiu Chuang)",
        "parent": "https://github.com/apache/hadoop/commit/db37f02dc704ad9eec8c56e3e466a5f37d138d74",
        "patched_files": [
            "BlockReportTestBase.java",
            "JournalNodeMXBean.java",
            "GetGroups.java",
            "JMXGet.java",
            "FsDatasetCache.java",
            "FileStatus.java",
            "CHANGES.java",
            "DFSZKFailoverController.java",
            "SafeMode.java",
            "HdfsAdmin.java",
            "HDFSContract.java",
            "BlockPlacementPolicyRackFaultTolerant.java",
            "Snapshot.java",
            "DFSStripedOutputStream.java",
            "DelegationToken.java",
            "BackupNode.java",
            "QuorumJournalManager.java",
            "NameNodeMetrics.java",
            "FileAppendTest4.java",
            "BootstrapStandby.java",
            "INodeAttributeProvider.java",
            "SnapshotDiffReport.java",
            "HDFSConcat.java",
            "DFSOutputStream.java",
            "DFSStripedInputStream.java",
            "FSDirectory.java",
            "BlockStatsMXBean.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSnapshotListing.java",
            "TestNNMetricFilesInGetListingOps.java",
            "TestSnapshottableDirListing.java",
            "TestDataNodeVolumeFailureReporting.java",
            "TestGetGroupsWithHA.java",
            "TestPendingInvalidateBlock.java",
            "TestViewFileSystemWithXAttrs.java",
            "TestViewFsHdfs.java",
            "TestWriteReadStripedFile.java",
            "TestMalformedURLs.java",
            "TestBlocksScheduledCounter.java",
            "TestHASafeMode.java",
            "TestDeleteCLI.java",
            "TestDataTransferKeepalive.java",
            "TestFcHdfsSetUMask.java",
            "TestOpenFilesWithSnapshot.java",
            "TestReadStripedFileWithDecoding.java",
            "TestDFSFinalize.java",
            "TestDFSClientFailover.java",
            "TestPipelines.java",
            "TestAddOverReplicatedStripedBlocks.java",
            "TestHDFSFileSystemContract.java",
            "TestCryptoAdminCLI.java",
            "TestTracing.java",
            "TestFsDatasetCache.java",
            "TestCommitBlockWithInvalidGenStamp.java",
            "TestNamenodeRetryCache.java",
            "TestDFSPermission.java",
            "TestFSImageWithAcl.java",
            "TestRecoverStripedFile.java",
            "TestFileStatusWithECPolicy.java",
            "TestDataNodeVolumeFailure.java",
            "TestDFSRollback.java",
            "TestComputeInvalidateWork.java",
            "TestXAttrCLI.java",
            "TestGetFileChecksum.java",
            "TestDelegationTokensWithHA.java",
            "TestWebHdfsWithAuthenticationFilter.java",
            "TestSequentialBlockGroupId.java",
            "TestSnapshotDiffReport.java",
            "TestNestedSnapshots.java",
            "TestDFSClientExcludedNodes.java",
            "TestNameNodeMetadataConsistency.java",
            "TestCacheAdminCLI.java",
            "TestViewFsWithXAttrs.java",
            "TestFileStatus.java",
            "TestRefreshCallQueue.java",
            "TestViewFsWithAcls.java",
            "TestFcHdfsPermission.java",
            "TestDataNodeExit.java",
            "TestAclCLI.java",
            "TestJournalNodeMXBean.java",
            "TestEditLogJournalFailures.java",
            "TestEncryptionZones.java",
            "TestReadStripedFileWithMissingBlocks.java",
            "TestNNHealthCheck.java",
            "TestSnapshotBlocksMap.java",
            "TestNameNodeMetrics.java",
            "TestDFSHAAdminMiniCluster.java",
            "TestDeadDatanode.java",
            "TestQuotaWithStripedBlocks.java",
            "TestXAttrsWithHA.java",
            "TestViewFileSystemHdfs.java",
            "TestAbandonBlock.java",
            "TestBlockPlacementPolicyRackFaultTolerant.java",
            "TestBootstrapStandbyWithBKJM.java",
            "TestBlockStatsMXBean.java",
            "TestQuorumJournalManager.java",
            "TestNameNodeRpcServerMethods.java",
            "TestErasureCodingPolicies.java",
            "TestListFilesInDFS.java",
            "TestReservedRawPaths.java",
            "TestFileContextSnapshot.java",
            "TestQuotasWithHA.java",
            "TestGetContentSummaryWithSnapshot.java",
            "TestFSDirectory.java",
            "TestINodeFileUnderConstructionWithSnapshot.java",
            "TestDFSZKFailoverController.java",
            "TestAddStripedBlocks.java",
            "TestWriteRead.java",
            "TestSetQuotaWithSnapshot.java",
            "TestLeaseRecovery2.java",
            "TestLeaseRecovery.java",
            "TestDataNodeVolumeFailureToleration.java",
            "TestMetadataVersionOutput.java",
            "TestDefaultBlockPlacementPolicy.java",
            "TestFailureToReadEdits.java",
            "TestSecureNNWithQJM.java",
            "TestHdfsAdmin.java",
            "TestSaslDataTransfer.java",
            "TestStandbyCheckpoints.java",
            "TestDelegationToken.java",
            "TestDFSStripedInputStream.java",
            "TestINodeAttributeProvider.java",
            "TestHDFSCLI.java",
            "TestDFSOutputStream.java",
            "TestDecommission.java",
            "TestSnapshotMetrics.java",
            "TestSnapshotDeletion.java",
            "TestViewFileSystemWithAcls.java",
            "TestJMXGet.java",
            "TestFileTruncate.java",
            "TestXAttrConfigFlag.java",
            "TestCacheDirectives.java",
            "TestRenameWithSnapshots.java",
            "TestDFSStripedOutputStream.java",
            "TestErasureCodingCLI.java",
            "TestHdfsTextCommand.java",
            "TestListFilesInFileContext.java",
            "TestNameNodeRetryCacheMetrics.java",
            "TestGetGroups.java",
            "TestHttpsFileSystem.java",
            "TestSafeMode.java",
            "TestDFSClientSocketSize.java",
            "TestStoragePolicyCommands.java",
            "TestSafeModeWithStripedFile.java",
            "TestQuotaByStorageType.java",
            "TestEditLogAutoroll.java",
            "TestFileConcurrentReader.java",
            "TestBootstrapStandbyWithQJM.java",
            "TestInitializeSharedEdits.java",
            "TestDiskspaceQuotaUpdate.java",
            "TestRefreshUserMappings.java",
            "TestIncrementalBrVariations.java",
            "TestClientReportBadBlock.java",
            "TestDNFencing.java",
            "TestRetryCacheWithHA.java",
            "TestDiskError.java",
            "TestDFSStartupVersions.java",
            "TestAclConfigFlag.java",
            "TestFailoverWithBlockTokensEnabled.java",
            "TestDFSStorageStateRecovery.java",
            "TestSnapshot.java",
            "TestBootstrapStandby.java",
            "TestFSImageWithXAttr.java",
            "TestBackupNode.java",
            "TestFSImageWithSnapshot.java",
            "TestSymlinkHdfs.java",
            "TestAddBlockRetry.java",
            "TestAddBlock.java",
            "TestHDFSConcat.java",
            "TestErasureCodingPolicyWithSnapshot.java",
            "TestFcHdfsCreateMkdir.java",
            "TestViewFsAtHdfsRoot.java",
            "TestEncryptionZonesWithHA.java",
            "TestAuditLogs.java",
            "TestSnapshotRename.java",
            "TestSnapshotReplication.java"
        ]
    },
    "hadoop_303c8dc": {
        "bug_id": "hadoop_303c8dc",
        "commit": "https://github.com/apache/hadoop/commit/303c8dc9b6c853c0939ea9ba14388897cc258071",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/303c8dc9b6c853c0939ea9ba14388897cc258071/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=303c8dc9b6c853c0939ea9ba14388897cc258071",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3870,9 +3870,13 @@ private void clearCorruptLazyPersistFiles()\n         while (it.hasNext()) {\n           Block b = it.next();\n           BlockInfo blockInfo = blockManager.getStoredBlock(b);\n-          BlockCollection bc = getBlockCollection(blockInfo);\n-          if (bc.getStoragePolicyID() == lpPolicy.getId()) {\n-            filesToDelete.add(bc);\n+          if (blockInfo == null) {\n+            LOG.info(\"Cannot find block info for block \" + b);\n+          } else {\n+            BlockCollection bc = getBlockCollection(blockInfo);\n+            if (bc.getStoragePolicyID() == lpPolicy.getId()) {\n+              filesToDelete.add(bc);\n+            }\n           }\n         }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/303c8dc9b6c853c0939ea9ba14388897cc258071/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "997fd920c9d76c87302ae4da00cd53f4d9ac4a2b",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in LazyPersistFileScrubber. Contributed by Inigo Goiri.",
        "parent": "https://github.com/apache/hadoop/commit/d81372dfad32488e7c46ffcfccdf0aa26bee04a5",
        "patched_files": [
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_32d4c14": {
        "bug_id": "hadoop_32d4c14",
        "commit": "https://github.com/apache/hadoop/commit/32d4c148dfd203789386a0587480bd974fbf4c1a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -488,6 +488,8 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n+    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "30d61c92e5b840b8c6a48e79d0ab2a3a58d1075e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -538,7 +538,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if(UserGroupInformation.isSecurityEnabled()) {\n+    if (UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -556,7 +556,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.getNamesystem().verifyToken(id, token.getPassword());\n+        nn.verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "39aa8db16d1c8139107767c3529deef7a1900cea",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 11,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5460,20 +5460,10 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-  \n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n+\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "88716a5ecdb890136bec739c7e5fdfc5501c42c3",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -78,6 +79,7 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n+import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1283,7 +1285,18 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t.getMessage());\n   }\n-  \n+\n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+\n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "2d5a90a8adb10e085161687ad7a661a74e29ea38",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "f00bb9c40a12d05de8d69885452b4d02a1604419",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n+import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -69,6 +70,7 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n+    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -79,6 +81,8 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n+    when(context.getAttribute(\n+        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "8dad3b33e6fd65d7eefcf83507b93dd5257c86a8",
                "status": "modified"
            }
        ],
        "message": "HDFS-3654. TestJspHelper#testGetUgi fails with NPE. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1361463 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/f5dd4583bb8d3705313534f7677df13dc45d36ab",
        "patched_files": [
            "FSNamesystem.java",
            "NameNode.java",
            "JspHelper.java",
            "CHANGES.java",
            "NameNodeHttpServer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJspHelper.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_35d4f32": {
        "bug_id": "hadoop_35d4f32",
        "commit": "https://github.com/apache/hadoop/commit/35d4f32b32a3ba05332811caf3d245d9c3dcf1a5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/35d4f32b32a3ba05332811caf3d245d9c3dcf1a5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java?ref=35d4f32b32a3ba05332811caf3d245d9c3dcf1a5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java",
                "patch": "@@ -188,7 +188,7 @@ public String getErasureCodingPolicyName(INode inode) {\n           String ecPolicyName = WritableUtils.readString(din);\n           return dir.getFSNamesystem()\n               .getErasureCodingPolicyManager()\n-              .getByName(ecPolicyName)\n+              .getErasureCodingPolicyByName(ecPolicyName)\n               .getName();\n         }\n       } else if (inode.getParent() != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/35d4f32b32a3ba05332811caf3d245d9c3dcf1a5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java",
                "sha": "0263f2a347faac25b4b13cdce4ca21ac2d47272c",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/35d4f32b32a3ba05332811caf3d245d9c3dcf1a5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java?ref=35d4f32b32a3ba05332811caf3d245d9c3dcf1a5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java",
                "patch": "@@ -255,6 +255,22 @@ public ErasureCodingPolicy getByName(String name) {\n     return ecpi.getPolicy();\n   }\n \n+  /**\n+   * Get a {@link ErasureCodingPolicy} by policy name, including system\n+   * policy, user defined policy and Replication policy.\n+   * @return ecPolicy, or null if not found\n+   */\n+  public ErasureCodingPolicy getErasureCodingPolicyByName(String name) {\n+    final ErasureCodingPolicyInfo ecpi = getPolicyInfoByName(name);\n+    if (ecpi == null) {\n+      if (name.equalsIgnoreCase(ErasureCodeConstants.REPLICATION_POLICY_NAME)) {\n+        return SystemErasureCodingPolicies.getReplicationPolicy();\n+      }\n+      return null;\n+    }\n+    return ecpi.getPolicy();\n+  }\n+\n   /**\n    * Get a {@link ErasureCodingPolicyInfo} by policy name, including system\n    * policy and user defined policy.",
                "raw_url": "https://github.com/apache/hadoop/raw/35d4f32b32a3ba05332811caf3d245d9c3dcf1a5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ErasureCodingPolicyManager.java",
                "sha": "d9f7e9afdc70dd17f85c686ff4ea6b2170b61f1a",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/35d4f32b32a3ba05332811caf3d245d9c3dcf1a5/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml?ref=35d4f32b32a3ba05332811caf3d245d9c3dcf1a5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml",
                "patch": "@@ -1055,5 +1055,24 @@\n       </comparators>\n     </test>\n \n+    <test>\n+      <description>ls: file with replication Policy</description>\n+      <test-commands>\n+        <command>-fs NAMENODE -mkdir -p /ecdir</command>\n+        <ec-admin-command>-fs NAMENODE -setPolicy -path /ecdir -replicate</ec-admin-command>\n+        <command>-fs NAMENODE -touchz /ecdir/file1</command>\n+        <command>-fs NAMENODE -ls -e /</command>\n+      </test-commands>\n+      <cleanup-commands>\n+        <command>-fs NAMENODE -rmdir /ecdir</command>\n+      </cleanup-commands>\n+      <comparators>\n+        <comparator>\n+          <type>RegexpComparator</type>\n+          <expected-output>^drwxr-xr-x( )*-( )*USERNAME( )*supergroup( )*[A-Za-z0-9-]{1,}( )*0( )*[0-9]{4,}-[0-9]{2,}-[0-9]{2,} [0-9]{2,}:[0-9]{2,}( )*/ecdir</expected-output>\n+        </comparator>\n+      </comparators>\n+    </test>\n+\n   </tests>\n </configuration>",
                "raw_url": "https://github.com/apache/hadoop/raw/35d4f32b32a3ba05332811caf3d245d9c3dcf1a5/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml",
                "sha": "2cc08f4aa5b2c454cfb69278a05a64f7fbc437ca",
                "status": "modified"
            }
        ],
        "message": "HDFS-14274. EC: NPE While Listing EC Policy For A Directory Following Replication Policy. Contributed by Ayush Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/00c5ffaee2fb16eaef512a47054c7b9ee7ea2e50",
        "patched_files": [
            "ContentSummaryComputationContext.java",
            "ErasureCodingPolicyManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "testErasureCodingConf.java"
        ]
    },
    "hadoop_384764c": {
        "bug_id": "hadoop_384764c",
        "commit": "https://github.com/apache/hadoop/commit/384764cdeac6490bc47fa0eb7b936baa4c0d3230",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java?ref=384764cdeac6490bc47fa0eb7b936baa4c0d3230",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java",
                "patch": "@@ -329,9 +329,12 @@ public synchronized boolean parentZNodeExists()\n    * This recursively creates the znode as well as all of its parents.\n    */\n   public synchronized void ensureParentZNode()\n-      throws IOException, InterruptedException {\n+      throws IOException, InterruptedException, KeeperException {\n     Preconditions.checkState(!wantToBeInElection,\n         \"ensureParentZNode() may not be called while in the election\");\n+    if (zkClient == null) {\n+      createConnection();\n+    }\n \n     String pathParts[] = znodeWorkingDir.split(\"/\");\n     Preconditions.checkArgument(pathParts.length >= 1 &&",
                "raw_url": "https://github.com/apache/hadoop/raw/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java",
                "sha": "d099ca71ac7ac5e3143751e3efcaea7d98c6a392",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java?ref=384764cdeac6490bc47fa0eb7b936baa4c0d3230",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java",
                "patch": "@@ -269,7 +269,7 @@ private void printUsage() {\n   }\n \n   private int formatZK(boolean force, boolean interactive)\n-      throws IOException, InterruptedException {\n+      throws IOException, InterruptedException, KeeperException {\n     if (elector.parentZNodeExists()) {\n       if (!force && (!interactive || !confirmFormat())) {\n         return ERR_CODE_FORMAT_DENIED;",
                "raw_url": "https://github.com/apache/hadoop/raw/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java",
                "sha": "f66e3c97490c664d703861c2f5889078bfdd8be8",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java?ref=384764cdeac6490bc47fa0eb7b936baa4c0d3230",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java",
                "patch": "@@ -22,8 +22,10 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.ha.ClientBaseWithFixes;\n import org.apache.hadoop.ha.ServiceFailedException;\n+import org.apache.hadoop.service.ServiceStateException;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -304,6 +306,26 @@ private void testCallbackSynchronizationTimingStandby(AdminService as,\n     verify(as, times(1)).transitionToStandby(any());\n   }\n \n+  /**\n+   * Test that active elector service triggers a fatal RM Event when connection\n+   * to ZK fails. YARN-8409\n+   */\n+  @Test\n+  public void testFailureToConnectToZookeeper() throws Exception {\n+    stopServer();\n+    Configuration myConf = new Configuration(conf);\n+    ResourceManager rm = new MockRM(conf);\n+\n+    ActiveStandbyElectorBasedElectorService ees =\n+        new ActiveStandbyElectorBasedElectorService(rm);\n+    try {\n+      ees.init(myConf);\n+      Assert.fail(\"expect failure to connect to Zookeeper\");\n+    } catch (ServiceStateException sse) {\n+      Assert.assertTrue(sse.getMessage().contains(\"ConnectionLoss\"));\n+    }\n+  }\n+\n   private class MockRMWithElector extends MockRM {\n     private long delayMs = 0;\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java",
                "sha": "8c038618c103a17af053cb617d9cef055645f501",
                "status": "modified"
            }
        ],
        "message": "YARN-8409.  Fixed NPE in ActiveStandbyElectorBasedElectorService.\n            Contributed by Chandni Singh",
        "parent": "https://github.com/apache/hadoop/commit/d3fa83a44b01c85f39bfb4deaf2972912ac61ca3",
        "patched_files": [
            "ActiveStandbyElector.java",
            "ZKFailoverController.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestActiveStandbyElector.java",
            "TestZKFailoverController.java",
            "TestRMEmbeddedElector.java"
        ]
    },
    "hadoop_38d5ca2": {
        "bug_id": "hadoop_38d5ca2",
        "commit": "https://github.com/apache/hadoop/commit/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -126,6 +126,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1928. Fixed a race condition in TestAMRMRPCNodeUpdates which caused it\n     to fail occassionally. (Zhijie Shen via vinodkv)\n \n+    YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling\n+    Disconnected event from ZK. (Karthik Kambatla via jianhe)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed12a7bc8acf7df38decfbfa771b2a7dea07b881",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -280,10 +280,9 @@ public String run() throws KeeperException, InterruptedException {\n     }\n   }\n \n-  private void logRootNodeAcls(String prefix) throws KeeperException,\n-      InterruptedException {\n+  private void logRootNodeAcls(String prefix) throws Exception {\n     Stat getStat = new Stat();\n-    List<ACL> getAcls = zkClient.getACL(zkRootNodePath, getStat);\n+    List<ACL> getAcls = getACLWithRetries(zkRootNodePath, getStat);\n \n     StringBuilder builder = new StringBuilder();\n     builder.append(prefix);\n@@ -363,7 +362,7 @@ protected synchronized void storeVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n     byte[] data =\n         ((RMStateVersionPBImpl) CURRENT_VERSION_INFO).getProto().toByteArray();\n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       setDataWithRetries(versionNodePath, data, -1);\n     } else {\n       createWithRetries(versionNodePath, data, zkAcl, CreateMode.PERSISTENT);\n@@ -374,7 +373,7 @@ protected synchronized void storeVersion() throws Exception {\n   protected synchronized RMStateVersion loadVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n \n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       byte[] data = getDataWithRetries(versionNodePath, true);\n       RMStateVersion version =\n           new RMStateVersionPBImpl(RMStateVersionProto.parseFrom(data));\n@@ -442,7 +441,8 @@ private void loadRMSequentialNumberState(RMState rmState) throws Exception {\n   }\n \n   private void loadRMDelegationTokenState(RMState rmState) throws Exception {\n-    List<String> childNodes = zkClient.getChildren(delegationTokensRootPath, true);\n+    List<String> childNodes =\n+        getChildrenWithRetries(delegationTokensRootPath, true);\n     for (String childNodeName : childNodes) {\n       String childNodePath =\n           getNodePath(delegationTokensRootPath, childNodeName);\n@@ -567,7 +567,7 @@ public synchronized void updateApplicationStateInternal(ApplicationId appId,\n     }\n     byte[] appStateData = appStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, appStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, appStateData, zkAcl,\n@@ -610,7 +610,7 @@ public synchronized void updateApplicationAttemptStateInternal(\n     }\n     byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, attemptStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, attemptStateData, zkAcl,\n@@ -661,7 +661,7 @@ protected synchronized void removeRMDelegationTokenState(\n       LOG.debug(\"Removing RMDelegationToken_\"\n           + rmDTIdentifier.getSequenceNumber());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       opList.add(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -677,7 +677,7 @@ protected void updateRMDelegationTokenAndSequenceNumberInternal(\n     String nodeRemovePath =\n         getNodePath(delegationTokensRootPath, DELEGATION_TOKEN_PREFIX\n             + rmDTIdentifier.getSequenceNumber());\n-    if (zkClient.exists(nodeRemovePath, true) == null) {\n+    if (existsWithRetries(nodeRemovePath, true) == null) {\n       // in case znode doesn't exist\n       addStoreOrUpdateOps(\n           opList, rmDTIdentifier, renewDate, latestSequenceNumber, false);\n@@ -760,7 +760,7 @@ protected synchronized void removeRMDTMasterKeyState(\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Removing RMDelegationKey_\" + delegationKey.getKeyId());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       doMultiWithRetries(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -891,6 +891,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private List<ACL> getACLWithRetries(\n+      final String path, final Stat stat) throws Exception {\n+    return new ZKAction<List<ACL>>() {\n+      @Override\n+      public List<ACL> run() throws KeeperException, InterruptedException {\n+        return zkClient.getACL(path, stat);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   private List<String> getChildrenWithRetries(\n       final String path, final boolean watch) throws Exception {\n     return new ZKAction<List<String>>() {\n@@ -901,6 +911,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private Stat existsWithRetries(\n+      final String path, final boolean watch) throws Exception {\n+    return new ZKAction<Stat>() {\n+      @Override\n+      Stat run() throws KeeperException, InterruptedException {\n+        return zkClient.exists(path, watch);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   /**\n    * Helper class that periodically attempts creating a znode to ensure that\n    * this RM continues to be the Active.",
                "raw_url": "https://github.com/apache/hadoop/raw/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "9b15bb21e7e62a1dab06a6799dd5a39621c06a4f",
                "status": "modified"
            }
        ],
        "message": "YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling Disconnected event from ZK. Contributed by Karthik Kambatla.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587776 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/9887d7f2502f814200317fa0e516d3ca29ba5ae4",
        "patched_files": [
            "ZKRMStateStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop_39775dc": {
        "bug_id": "hadoop_39775dc",
        "commit": "https://github.com/apache/hadoop/commit/39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -220,9 +220,6 @@ Release 0.23.3 - UNRELEASED\n     HADOOP-8163. Improve ActiveStandbyElector to provide hooks for\n     fencing old active. (todd)\n \n-    HADOOP-8193. Refactor FailoverController/HAAdmin code to add an abstract\n-    class for \"target\" services. (todd)\n-\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "f3fda7668c5ee52db3dacbecaf62f2f9cfa5b5bd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "patch": "@@ -19,16 +19,11 @@\n \n import java.io.IOException;\n \n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-\n /**\n  * Indicates that the operator has specified an invalid configuration\n  * for fencing methods.\n  */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public class BadFencingConfigurationException extends IOException {\n+class BadFencingConfigurationException extends IOException {\n   private static final long serialVersionUID = 1L;\n \n   public BadFencingConfigurationException(String msg) {",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "sha": "3d3b1ba53cca506a3ef8d28ade71a18c25747777",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 22,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.ha;\n \n import java.io.IOException;\n+import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -50,21 +51,21 @@\n    * allow it to become active, eg because it triggers a log roll\n    * so the standby can learn about new blocks and leave safemode.\n    *\n-   * @param target service to make active\n+   * @param toSvc service to make active\n+   * @param toSvcName name of service to make active\n    * @param forceActive ignore toSvc if it reports that it is not ready\n    * @throws FailoverFailedException if we should avoid failover\n    */\n-  private static void preFailoverChecks(HAServiceTarget target,\n+  private static void preFailoverChecks(HAServiceProtocol toSvc,\n+                                        InetSocketAddress toSvcAddr,\n                                         boolean forceActive)\n       throws FailoverFailedException {\n     HAServiceStatus toSvcStatus;\n-    HAServiceProtocol toSvc;\n \n     try {\n-      toSvc = target.getProxy();\n       toSvcStatus = toSvc.getServiceStatus();\n     } catch (IOException e) {\n-      String msg = \"Unable to get service state for \" + target;\n+      String msg = \"Unable to get service state for \" + toSvcAddr;\n       LOG.error(msg, e);\n       throw new FailoverFailedException(msg, e);\n     }\n@@ -78,7 +79,7 @@ private static void preFailoverChecks(HAServiceTarget target,\n       String notReadyReason = toSvcStatus.getNotReadyReason();\n       if (!forceActive) {\n         throw new FailoverFailedException(\n-            target + \" is not ready to become active: \" +\n+            toSvcAddr + \" is not ready to become active: \" +\n             notReadyReason);\n       } else {\n         LOG.warn(\"Service is not ready to become active, but forcing: \" +\n@@ -102,72 +103,77 @@ private static void preFailoverChecks(HAServiceTarget target,\n    * then try to failback.\n    *\n    * @param fromSvc currently active service\n+   * @param fromSvcAddr addr of the currently active service\n    * @param toSvc service to make active\n+   * @param toSvcAddr addr of the service to make active\n+   * @param fencer for fencing fromSvc\n    * @param forceFence to fence fromSvc even if not strictly necessary\n    * @param forceActive try to make toSvc active even if it is not ready\n    * @throws FailoverFailedException if the failover fails\n    */\n-  public static void failover(HAServiceTarget fromSvc,\n-                              HAServiceTarget toSvc,\n+  public static void failover(HAServiceProtocol fromSvc,\n+                              InetSocketAddress fromSvcAddr,\n+                              HAServiceProtocol toSvc,\n+                              InetSocketAddress toSvcAddr,\n+                              NodeFencer fencer,\n                               boolean forceFence,\n                               boolean forceActive)\n       throws FailoverFailedException {\n-    Preconditions.checkArgument(fromSvc.getFencer() != null,\n-        \"failover requires a fencer\");\n-    preFailoverChecks(toSvc, forceActive);\n+    Preconditions.checkArgument(fencer != null, \"failover requires a fencer\");\n+    preFailoverChecks(toSvc, toSvcAddr, forceActive);\n \n     // Try to make fromSvc standby\n     boolean tryFence = true;\n     try {\n-      HAServiceProtocolHelper.transitionToStandby(fromSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToStandby(fromSvc);\n       // We should try to fence if we failed or it was forced\n       tryFence = forceFence ? true : false;\n     } catch (ServiceFailedException sfe) {\n-      LOG.warn(\"Unable to make \" + fromSvc + \" standby (\" +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr + \" standby (\" +\n           sfe.getMessage() + \")\");\n     } catch (IOException ioe) {\n-      LOG.warn(\"Unable to make \" + fromSvc +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr +\n           \" standby (unable to connect)\", ioe);\n     }\n \n     // Fence fromSvc if it's required or forced by the user\n     if (tryFence) {\n-      if (!fromSvc.getFencer().fence(fromSvc)) {\n+      if (!fencer.fence(fromSvcAddr)) {\n         throw new FailoverFailedException(\"Unable to fence \" +\n-            fromSvc + \". Fencing failed.\");\n+            fromSvcAddr + \". Fencing failed.\");\n       }\n     }\n \n     // Try to make toSvc active\n     boolean failed = false;\n     Throwable cause = null;\n     try {\n-      HAServiceProtocolHelper.transitionToActive(toSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToActive(toSvc);\n     } catch (ServiceFailedException sfe) {\n-      LOG.error(\"Unable to make \" + toSvc + \" active (\" +\n+      LOG.error(\"Unable to make \" + toSvcAddr + \" active (\" +\n           sfe.getMessage() + \"). Failing back.\");\n       failed = true;\n       cause = sfe;\n     } catch (IOException ioe) {\n-      LOG.error(\"Unable to make \" + toSvc +\n+      LOG.error(\"Unable to make \" + toSvcAddr +\n           \" active (unable to connect). Failing back.\", ioe);\n       failed = true;\n       cause = ioe;\n     }\n \n     // We failed to make toSvc active\n     if (failed) {\n-      String msg = \"Unable to failover to \" + toSvc;\n+      String msg = \"Unable to failover to \" + toSvcAddr;\n       // Only try to failback if we didn't fence fromSvc\n       if (!tryFence) {\n         try {\n           // Unconditionally fence toSvc in case it is still trying to\n           // become active, eg we timed out waiting for its response.\n           // Unconditionally force fromSvc to become active since it\n           // was previously active when we initiated failover.\n-          failover(toSvc, fromSvc, true, true);\n+          failover(toSvc, toSvcAddr, fromSvc, fromSvcAddr, fencer, true, true);\n         } catch (FailoverFailedException ffe) {\n-          msg += \". Failback to \" + fromSvc +\n+          msg += \". Failback to \" + fromSvcAddr +\n             \" failed (\" + ffe.getMessage() + \")\";\n           LOG.fatal(msg);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "sha": "c8878e8b73951f0f7a3290757615241c8f989d08",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n+\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configurable;\n@@ -60,6 +62,6 @@\n    * @throws BadFencingConfigurationException if the configuration was\n    *         determined to be invalid only at runtime\n    */\n-  public boolean tryFence(HAServiceTarget target, String args)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String args)\n     throws BadFencingConfigurationException;\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "sha": "d8bda1402fa928ae1001a2a705709aaad03d70fb",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import java.io.IOException;\n import java.io.PrintStream;\n+import java.net.InetSocketAddress;\n import java.util.Map;\n \n import org.apache.commons.cli.Options;\n@@ -27,8 +28,11 @@\n import org.apache.commons.cli.GnuParser;\n import org.apache.commons.cli.ParseException;\n \n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -73,8 +77,6 @@\n   protected PrintStream errOut = System.err;\n   PrintStream out = System.out;\n \n-  protected abstract HAServiceTarget resolveTarget(String string);\n-\n   protected String getUsageString() {\n     return \"Usage: HAAdmin\";\n   }\n@@ -107,7 +109,7 @@ private int transitionToActive(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToActive(proto);\n     return 0;\n   }\n@@ -120,13 +122,14 @@ private int transitionToStandby(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToStandby(proto);\n     return 0;\n   }\n \n   private int failover(final String[] argv)\n       throws IOException, ServiceFailedException {\n+    Configuration conf = getConf();\n     boolean forceFence = false;\n     boolean forceActive = false;\n \n@@ -159,12 +162,29 @@ private int failover(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceTarget fromNode = resolveTarget(args[0]);\n-    HAServiceTarget toNode = resolveTarget(args[1]);\n-    \n+    NodeFencer fencer;\n     try {\n-      FailoverController.failover(fromNode, toNode,\n-          forceFence, forceActive); \n+      fencer = NodeFencer.create(conf);\n+    } catch (BadFencingConfigurationException bfce) {\n+      errOut.println(\"failover: incorrect fencing configuration: \" + \n+          bfce.getLocalizedMessage());\n+      return -1;\n+    }\n+    if (fencer == null) {\n+      errOut.println(\"failover: no fencer configured\");\n+      return -1;\n+    }\n+\n+    InetSocketAddress addr1 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[0]));\n+    InetSocketAddress addr2 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[1]));\n+    HAServiceProtocol proto1 = getProtocol(args[0]);\n+    HAServiceProtocol proto2 = getProtocol(args[1]);\n+\n+    try {\n+      FailoverController.failover(proto1, addr1, proto2, addr2,\n+          fencer, forceFence, forceActive); \n       out.println(\"Failover from \"+args[0]+\" to \"+args[1]+\" successful\");\n     } catch (FailoverFailedException ffe) {\n       errOut.println(\"Failover failed: \" + ffe.getLocalizedMessage());\n@@ -181,7 +201,7 @@ private int checkHealth(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     try {\n       HAServiceProtocolHelper.monitorHealth(proto);\n     } catch (HealthCheckFailedException e) {\n@@ -199,7 +219,7 @@ private int getServiceState(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     out.println(proto.getServiceStatus().getState());\n     return 0;\n   }\n@@ -212,6 +232,16 @@ protected String getServiceAddr(String serviceId) {\n     return serviceId;\n   }\n \n+  /**\n+   * Return a proxy to the specified target service.\n+   */\n+  protected HAServiceProtocol getProtocol(String serviceId)\n+      throws IOException {\n+    String serviceAddr = getServiceAddr(serviceId);\n+    InetSocketAddress addr = NetUtils.createSocketAddr(serviceAddr);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, getConf());\n+  }\n+\n   @Override\n   public int run(String[] argv) throws Exception {\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "sha": "a16ffb4c4004bdf89f821920605e17b78d9fe4a1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java?ref=ea868d3d8b6c5e018eb104a560890c60d30fa269",
                "deletions": 74,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "patch": "@@ -1,74 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n-import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n-\n-/**\n- * Represents a target of the client side HA administration commands.\n- */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public abstract class HAServiceTarget {\n-\n-  /**\n-   * @return the IPC address of the target node.\n-   */\n-  public abstract InetSocketAddress getAddress();\n-\n-  /**\n-   * @return a Fencer implementation configured for this target node\n-   */\n-  public abstract NodeFencer getFencer();\n-  \n-  /**\n-   * @throws BadFencingConfigurationException if the fencing configuration\n-   * appears to be invalid. This is divorced from the above\n-   * {@link #getFencer()} method so that the configuration can be checked\n-   * during the pre-flight phase of failover.\n-   */\n-  public abstract void checkFencingConfigured()\n-      throws BadFencingConfigurationException;\n-  \n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public HAServiceProtocol getProxy(Configuration conf, int timeoutMs)\n-      throws IOException {\n-    Configuration confCopy = new Configuration(conf);\n-    // Lower the timeout so we quickly fail to connect\n-    confCopy.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n-    return new HAServiceProtocolClientSideTranslatorPB(\n-        getAddress(),\n-        confCopy, null, timeoutMs);\n-  }\n-\n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public final HAServiceProtocol getProxy() throws IOException {\n-    return getProxy(new Configuration(), 0); // default conf, timeout\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "sha": "78a2f2e4d98ab1b14b3042af340caa3d1c0d751b",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Matcher;\n@@ -90,14 +91,14 @@ public static NodeFencer create(Configuration conf)\n     return new NodeFencer(conf);\n   }\n \n-  public boolean fence(HAServiceTarget fromSvc) {\n+  public boolean fence(InetSocketAddress serviceAddr) {\n     LOG.info(\"====== Beginning Service Fencing Process... ======\");\n     int i = 0;\n     for (FenceMethodWithArg method : methods) {\n       LOG.info(\"Trying method \" + (++i) + \"/\" + methods.size() +\": \" + method);\n       \n       try {\n-        if (method.method.tryFence(fromSvc, method.arg)) {\n+        if (method.method.tryFence(serviceAddr, method.arg)) {\n           LOG.info(\"====== Fencing successful by method \" + method + \" ======\");\n           return true;\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "sha": "34a2c8b823a3ee2943d18c94431babe22275ef2d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "patch": "@@ -75,8 +75,7 @@ public void checkArgs(String args) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String cmd) {\n-    InetSocketAddress serviceAddr = target.getAddress();\n+  public boolean tryFence(InetSocketAddress serviceAddr, String cmd) {\n     List<String> cmdList = Arrays.asList(cmd.split(\"\\\\s+\"));\n \n     // Create arg list with service as the first argument",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "sha": "ca81f23a1878fa7fea3247b8e197e1150209a53c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "patch": "@@ -79,11 +79,10 @@ public void checkArgs(String argStr) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String argsStr)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String argsStr)\n       throws BadFencingConfigurationException {\n \n     Args args = new Args(argsStr);\n-    InetSocketAddress serviceAddr = target.getAddress();\n     String host = serviceAddr.getHostName();\n     \n     Session session;",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "sha": "00b9a83a572a3ad8a14cf2f7eab8cd013501a2ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java?ref=ea868d3d8b6c5e018eb104a560890c60d30fa269",
                "deletions": 94,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "patch": "@@ -1,94 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n-import org.apache.hadoop.security.AccessControlException;\n-import org.mockito.Mockito;\n-\n-/**\n- * Test-only implementation of {@link HAServiceTarget}, which returns\n- * a mock implementation.\n- */\n-class DummyHAService extends HAServiceTarget {\n-  HAServiceState state;\n-  HAServiceProtocol proxy;\n-  NodeFencer fencer;\n-  InetSocketAddress address;\n-\n-  DummyHAService(HAServiceState state, InetSocketAddress address) {\n-    this.state = state;\n-    this.proxy = makeMock();\n-    this.fencer = Mockito.mock(NodeFencer.class);\n-    this.address = address;\n-  }\n-  \n-  private HAServiceProtocol makeMock() {\n-    return Mockito.spy(new HAServiceProtocol() {\n-      @Override\n-      public void monitorHealth() throws HealthCheckFailedException,\n-          AccessControlException, IOException {\n-      }\n-\n-      @Override\n-      public void transitionToActive() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.ACTIVE;\n-      }\n-\n-      @Override\n-      public void transitionToStandby() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.STANDBY;\n-      }\n-\n-      @Override\n-      public HAServiceStatus getServiceStatus() throws IOException {\n-        HAServiceStatus ret = new HAServiceStatus(state);\n-        if (state == HAServiceState.STANDBY) {\n-          ret.setReadyToBecomeActive();\n-        }\n-        return ret;\n-      }\n-    });\n-  }\n-\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return address;\n-  }\n-\n-  @Override\n-  public HAServiceProtocol getProxy(Configuration conf, int timeout)\n-      throws IOException {\n-    return proxy;\n-  }\n-\n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "sha": "69c4a6fde41141b8c43c94a4efe41348718f9a32",
                "status": "removed"
            },
            {
                "additions": 214,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "changes": 358,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 144,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "patch": "@@ -24,85 +24,124 @@\n import static org.mockito.Mockito.verify;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysSucceedFencer;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysFailFencer;\n import static org.apache.hadoop.ha.TestNodeFencer.setupFencer;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n \n import org.junit.Test;\n-import org.mockito.Mockito;\n-import org.mockito.internal.stubbing.answers.ThrowsException;\n-import org.mockito.stubbing.Answer;\n-\n import static org.junit.Assert.*;\n \n public class TestFailoverController {\n+\n   private InetSocketAddress svc1Addr = new InetSocketAddress(\"svc1\", 1234); \n-  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678);\n+  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678); \n+\n+  private class DummyService implements HAServiceProtocol {\n+    HAServiceState state;\n+\n+    DummyService(HAServiceState state) {\n+      this.state = state;\n+    }\n+\n+    @Override\n+    public void monitorHealth() throws HealthCheckFailedException, IOException {\n+      // Do nothing\n+    }\n+\n+    @Override\n+    public void transitionToActive() throws ServiceFailedException, IOException {\n+      state = HAServiceState.ACTIVE;\n+    }\n \n-  HAServiceStatus STATE_NOT_READY = new HAServiceStatus(HAServiceState.STANDBY)\n-      .setNotReadyToBecomeActive(\"injected not ready\");\n+    @Override\n+    public void transitionToStandby() throws ServiceFailedException, IOException {\n+      state = HAServiceState.STANDBY;\n+    }\n \n+    @Override\n+    public HAServiceStatus getServiceStatus() throws IOException {\n+      HAServiceStatus ret = new HAServiceStatus(state);\n+      if (state == HAServiceState.STANDBY) {\n+        ret.setReadyToBecomeActive();\n+      }\n+      return ret;\n+    }\n+    \n+    private HAServiceState getServiceState() {\n+      return state;\n+    }\n+  }\n+  \n   @Test\n   public void testFailoverAndFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc1, svc2, false, false);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc2, svc1, false, false);\n+    FailoverController.failover(svc2, svc2Addr, svc1, svc1Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromStandbyToStandby() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.STANDBY, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.STANDBY);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n-    FailoverController.failover(svc1, svc2, false, false);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromActiveToActive() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.ACTIVE, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.ACTIVE);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to an already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverWithoutPermission() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc1.proxy).getServiceStatus();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover when access is denied\");\n     } catch (FailoverFailedException ffe) {\n       assertTrue(ffe.getCause().getMessage().contains(\"Access denied\"));\n@@ -112,13 +151,19 @@ public void testFailoverWithoutPermission() throws Exception {\n \n   @Test\n   public void testFailoverToUnreadyService() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doReturn(STATE_NOT_READY).when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        HAServiceStatus ret = new HAServiceStatus(HAServiceState.STANDBY);\n+        ret.setNotReadyToBecomeActive(\"injected not ready\");\n+        return ret;\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to a service that's not ready\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -127,88 +172,95 @@ public void testFailoverToUnreadyService() throws Exception {\n       }\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n \n     // Forcing it means we ignore readyToBecomeActive\n-    FailoverController.failover(svc1, svc2, false, true);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, true);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToUnhealthyServiceFailsAndFailsback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new HealthCheckFailedException(\"Failed!\"))\n-        .when(svc2.proxy).monitorHealth();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void monitorHealth() throws HealthCheckFailedException {\n+        throw new HealthCheckFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to unhealthy service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceSucceeds() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Faulty active prevented failover\");\n     }\n \n     // svc1 still thinks it's active, that's OK, it was fenced\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc1, AlwaysSucceedFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceFencingFailure() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over even though fencing failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFencingFailureDuringFailover() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over even though fencing requested and failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -217,105 +269,115 @@ public void testFencingFailureDuringFailover() throws Exception {\n     // If fencing was requested and it failed we don't try to make\n     // svc2 active anyway, and we don't failback to svc1.\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n   \n+  private HAServiceProtocol getProtocol(String target)\n+      throws IOException {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(target);\n+    Configuration conf = new Configuration();\n+    // Lower the timeout so we quickly fail to connect\n+    conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, conf);\n+  }\n+\n   @Test\n   public void testFailoverFromNonExistantServiceWithFencer() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(null, svc1Addr));\n-    // Getting a proxy to a dead server will throw IOException on call,\n-    // not on creation of the proxy.\n-    HAServiceProtocol errorThrowingProxy = Mockito.mock(HAServiceProtocol.class,\n-        new ThrowsException(new IOException(\"Could not connect to host\")));\n-    Mockito.doReturn(errorThrowingProxy).when(svc1).getProxy();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    HAServiceProtocol svc1 = getProtocol(\"localhost:1234\");\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Non-existant active prevented failover\");\n     }\n \n     // Don't check svc1 because we can't reach it, but that's OK, it's been fenced.\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToNonExistantServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = spy(new DummyHAService(null, svc2Addr));\n-    Mockito.doThrow(new IOException(\"Failed to connect\"))\n-      .when(svc2).getProxy(Mockito.<Configuration>any(),\n-          Mockito.anyInt());\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    HAServiceProtocol svc2 = getProtocol(\"localhost:1234\");\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to a non-existant standby\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToFaultyServiceFailsbackOK() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(HAServiceState.ACTIVE, svc1Addr));\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = spy(new DummyService(HAServiceState.ACTIVE));\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // svc1 went standby then back to active\n-    verify(svc1.proxy).transitionToStandby();\n-    verify(svc1.proxy).transitionToActive();\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    verify(svc1).transitionToStandby();\n+    verify(svc1).transitionToActive();\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeDontFailbackIfActiveWasFenced() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // We failed to failover and did not failback because we fenced\n     // svc1 (we forced it), therefore svc1 and svc2 should be standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n     AlwaysSucceedFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -324,22 +386,25 @@ public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n     // We failed to failover. We did not fence svc1 because it cooperated\n     // and we didn't force it, so we failed back to svc1 and fenced svc2.\n     // Note svc2 still thinks it's active, that's OK, we fenced it.\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc2, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysSucceedFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new IOException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n     AlwaysFailFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -348,30 +413,35 @@ public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n     // We did not fence svc1 because it cooperated and we didn't force it, \n     // we failed to failover so we fenced svc2, we failed to fence svc2\n     // so we did not failback to svc1, ie it's still standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc2, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysFailFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailbackToFaultyServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToActive();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1, svc1Addr, svc2, svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "sha": "6dec32c636de5317a679eb3bc609faa8168e0922",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "patch": "@@ -22,15 +22,14 @@\n import java.io.ByteArrayOutputStream;\n import java.io.IOException;\n import java.io.PrintStream;\n-import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n \n import org.junit.Before;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n import com.google.common.base.Charsets;\n import com.google.common.base.Joiner;\n@@ -41,14 +40,15 @@\n   private HAAdmin tool;\n   private ByteArrayOutputStream errOutBytes = new ByteArrayOutputStream();\n   private String errOutput;\n-\n+  private HAServiceProtocol mockProtocol;\n+  \n   @Before\n   public void setup() throws IOException {\n+    mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new HAAdmin() {\n       @Override\n-      protected HAServiceTarget resolveTarget(String target) {\n-        return new DummyHAService(HAServiceState.STANDBY,\n-            new InetSocketAddress(\"dummy\", 12345));\n+      protected HAServiceProtocol getProtocol(String target) throws IOException {\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(new Configuration());",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "sha": "7f885d8bc2535d2b34eb4fe9e5dd961da12be685",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 28,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "patch": "@@ -26,35 +26,26 @@\n import org.apache.hadoop.conf.Configured;\n import org.junit.Before;\n import org.junit.Test;\n-import org.mockito.Mockito;\n \n import com.google.common.collect.Lists;\n \n public class TestNodeFencer {\n \n-  private HAServiceTarget MOCK_TARGET;\n-  \n-\n   @Before\n   public void clearMockState() {\n     AlwaysSucceedFencer.fenceCalled = 0;\n     AlwaysSucceedFencer.callArgs.clear();\n     AlwaysFailFencer.fenceCalled = 0;\n     AlwaysFailFencer.callArgs.clear();\n-    \n-    MOCK_TARGET = Mockito.mock(HAServiceTarget.class);\n-    Mockito.doReturn(\"my mock\").when(MOCK_TARGET).toString();\n-    Mockito.doReturn(new InetSocketAddress(\"host\", 1234))\n-        .when(MOCK_TARGET).getAddress();\n   }\n \n   @Test\n   public void testSingleFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n   \n@@ -63,7 +54,7 @@ public void testMultipleFencers() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar)\\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // Only one call, since the first fencer succeeds\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n@@ -77,12 +68,12 @@ public void testWhitespaceAndCommentsInConfig()\n         \" # the next one will always fail\\n\" +\n         \" \" + AlwaysFailFencer.class.getName() + \"(foo) # <- fails\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar) \\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysFailFencer.fencedSvc);\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysFailFencer.callArgs.get(0));\n     assertEquals(\"bar\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n@@ -91,41 +82,41 @@ public void testWhitespaceAndCommentsInConfig()\n   public void testArglessFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName());\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(null, AlwaysSucceedFencer.callArgs.get(0));\n   }\n \n   @Test\n   public void testShortNameShell() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"shell(true)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSsh() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUser() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUserPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   public static NodeFencer setupFencer(String confStr)\n@@ -142,12 +133,12 @@ public static NodeFencer setupFencer(String confStr)\n   public static class AlwaysSucceedFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return true;\n@@ -164,12 +155,12 @@ public void checkArgs(String args) {\n   public static class AlwaysFailFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return false;",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "sha": "5508547c0a52daff57ae2400b14a0f30c946d699",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "patch": "@@ -22,7 +22,6 @@\n import java.net.InetSocketAddress;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.util.StringUtils;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -33,9 +32,6 @@\n \n public class TestShellCommandFencer {\n   private ShellCommandFencer fencer = createFencer();\n-  private static final HAServiceTarget TEST_TARGET =\n-      new DummyHAService(HAServiceState.ACTIVE,\n-          new InetSocketAddress(\"host\", 1234));\n   \n   @BeforeClass\n   public static void setupLogSpy() {\n@@ -61,10 +57,11 @@ private static ShellCommandFencer createFencer() {\n    */\n   @Test\n   public void testBasicSuccessFailure() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo\"));\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"exit 1\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo\"));\n+    assertFalse(fencer.tryFence(addr, \"exit 1\"));\n     // bad path should also fail\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"xxxxxxxxxxxx\"));\n+    assertFalse(fencer.tryFence(addr, \"xxxxxxxxxxxx\"));\n   }\n   \n   @Test\n@@ -101,7 +98,8 @@ public void testCheckParensNoArgs() {\n    */\n   @Test\n   public void testStdoutLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello\"));\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo hello: host:1234 hello\"));\n   }\n@@ -112,7 +110,8 @@ public void testStdoutLogging() {\n    */\n   @Test\n   public void testStderrLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello >&2\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello >&2\"));\n     Mockito.verify(ShellCommandFencer.LOG).warn(\n         Mockito.endsWith(\"echo hello >&2: host:1234 hello\"));\n   }\n@@ -123,7 +122,8 @@ public void testStderrLogging() {\n    */\n   @Test\n   public void testConfAsEnvironment() {\n-    fencer.tryFence(TEST_TARGET, \"echo $in_fencing_tests\");\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    fencer.tryFence(addr, \"echo $in_fencing_tests\");\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo $in...ing_tests: host:1234 yessir\"));\n   }\n@@ -136,7 +136,8 @@ public void testConfAsEnvironment() {\n    */\n   @Test(timeout=10000)\n   public void testSubprocessInputIsClosed() {\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"read\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertFalse(fencer.tryFence(addr, \"read\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "sha": "49bae039eccbcc7b7f43a73e64e2084bc66c9a2d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 19,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "patch": "@@ -23,7 +23,6 @@\n \n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.SshFenceByTcpPort.Args;\n import org.apache.log4j.Level;\n import org.junit.Assume;\n@@ -35,25 +34,12 @@\n     ((Log4JLogger)SshFenceByTcpPort.LOG).getLogger().setLevel(Level.ALL);\n   }\n   \n-  private static String TEST_FENCING_HOST = System.getProperty(\n+  private String TEST_FENCING_HOST = System.getProperty(\n       \"test.TestSshFenceByTcpPort.host\", \"localhost\");\n-  private static final String TEST_FENCING_PORT = System.getProperty(\n+  private String TEST_FENCING_PORT = System.getProperty(\n       \"test.TestSshFenceByTcpPort.port\", \"8020\");\n-  private static final String TEST_KEYFILE = System.getProperty(\n+  private final String TEST_KEYFILE = System.getProperty(\n       \"test.TestSshFenceByTcpPort.key\");\n-  \n-  private static final InetSocketAddress TEST_ADDR =\n-    new InetSocketAddress(TEST_FENCING_HOST,\n-      Integer.valueOf(TEST_FENCING_PORT));\n-  private static final HAServiceTarget TEST_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE, TEST_ADDR);\n-  \n-  /**\n-   *  Connect to Google's DNS server - not running ssh!\n-   */\n-  private static final HAServiceTarget UNFENCEABLE_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE,\n-        new InetSocketAddress(\"8.8.8.8\", 1234));\n \n   @Test(timeout=20000)\n   public void testFence() throws BadFencingConfigurationException {\n@@ -63,7 +49,8 @@ public void testFence() throws BadFencingConfigurationException {\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n     assertTrue(fence.tryFence(\n-        TEST_TARGET,\n+        new InetSocketAddress(TEST_FENCING_HOST,\n+                              Integer.valueOf(TEST_FENCING_PORT)),\n         null));\n   }\n \n@@ -78,7 +65,8 @@ public void testConnectTimeout() throws BadFencingConfigurationException {\n     conf.setInt(SshFenceByTcpPort.CONF_CONNECT_TIMEOUT_KEY, 3000);\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n-    assertFalse(fence.tryFence(UNFENCEABLE_TARGET, \"\"));\n+    // Connect to Google's DNS server - not running ssh!\n+    assertFalse(fence.tryFence(new InetSocketAddress(\"8.8.8.8\", 1234), \"\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "sha": "554a7abca5fdd33e027dfbb305c85e2d4a1b9ba3",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.ha.HAAdmin;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -65,9 +65,15 @@ public void setConf(Configuration conf) {\n    * Try to map the given namenode ID to its service address.\n    */\n   @Override\n-  protected HAServiceTarget resolveTarget(String nnId) {\n+  protected String getServiceAddr(String nnId) {\n     HdfsConfiguration conf = (HdfsConfiguration)getConf();\n-    return new NNHAServiceTarget(conf, nameserviceId, nnId);\n+    String serviceAddr = \n+      DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, nnId);\n+    if (serviceAddr == null) {\n+      throw new IllegalArgumentException(\n+          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n+    }\n+    return serviceAddr;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "sha": "13bde2ae53391ee734fd84e669950a2eb1355f4b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java?ref=ea868d3d8b6c5e018eb104a560890c60d30fa269",
                "deletions": 84,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "patch": "@@ -1,84 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hdfs.tools;\n-\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.ha.BadFencingConfigurationException;\n-import org.apache.hadoop.ha.HAServiceTarget;\n-import org.apache.hadoop.ha.NodeFencer;\n-import org.apache.hadoop.hdfs.DFSUtil;\n-import org.apache.hadoop.hdfs.HdfsConfiguration;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n-import org.apache.hadoop.net.NetUtils;\n-\n-/**\n- * One of the NN NameNodes acting as the target of an administrative command\n- * (e.g. failover).\n- */\n-@InterfaceAudience.Private\n-public class NNHAServiceTarget extends HAServiceTarget {\n-\n-  private final InetSocketAddress addr;\n-  private NodeFencer fencer;\n-  private BadFencingConfigurationException fenceConfigError;\n-\n-  public NNHAServiceTarget(HdfsConfiguration conf,\n-      String nsId, String nnId) {\n-    String serviceAddr = \n-      DFSUtil.getNamenodeServiceAddr(conf, nsId, nnId);\n-    if (serviceAddr == null) {\n-      throw new IllegalArgumentException(\n-          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n-    }\n-    this.addr = NetUtils.createSocketAddr(serviceAddr,\n-        NameNode.DEFAULT_PORT);\n-    try {\n-      this.fencer = NodeFencer.create(conf);\n-    } catch (BadFencingConfigurationException e) {\n-      this.fenceConfigError = e;\n-    }\n-  }\n-\n-  /**\n-   * @return the NN's IPC address.\n-   */\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return addr;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-    if (fenceConfigError != null) {\n-      throw fenceConfigError;\n-    }\n-  }\n-  \n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-  \n-  @Override\n-  public String toString() {\n-    return \"NameNode at \" + addr;\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "sha": "9e8c239e7e8782fffff27109a1c2c2f0f8700028",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 12,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.HAServiceStatus;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.NodeFencer;\n \n@@ -80,18 +79,10 @@ private HdfsConfiguration getHAConf() {\n   public void setup() throws IOException {\n     mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new DFSHAAdmin() {\n-\n       @Override\n-      protected HAServiceTarget resolveTarget(String nnId) {\n-        HAServiceTarget target = super.resolveTarget(nnId);\n-        HAServiceTarget spy = Mockito.spy(target);\n-        // OVerride the target to return our mock protocol\n-        try {\n-          Mockito.doReturn(mockProtocol).when(spy).getProxy();\n-        } catch (IOException e) {\n-          throw new AssertionError(e); // mock setup doesn't really throw\n-        }\n-        return spy;\n+      protected HAServiceProtocol getProtocol(String serviceId) throws IOException {\n+        getServiceAddr(serviceId);\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(getHAConf());",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "sha": "c5ba0eb7e589257272defe6615228876daebf439",
                "status": "modified"
            }
        ],
        "message": "Revert HADOOP-8193 from r1304967. Patch introduced some NPEs in a test case.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1305152 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ea868d3d8b6c5e018eb104a560890c60d30fa269",
        "patched_files": [
            "FailoverController.java",
            "DummyHAService.java",
            "NNHAServiceTarget.java",
            "ShellCommandFencer.java",
            "SshFenceByTcpPort.java",
            "HAServiceTarget.java",
            "FenceMethod.java",
            "BadFencingConfigurationException.java",
            "NodeFencer.java",
            "DFSHAAdmin.java",
            "CHANGES.java",
            "HAAdmin.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestHAAdmin.java",
            "TestDFSHAAdmin.java",
            "TestFailoverController.java",
            "TestNodeFencer.java",
            "TestShellCommandFencer.java",
            "TestSshFenceByTcpPort.java"
        ]
    },
    "hadoop_3b00eae": {
        "bug_id": "hadoop_3b00eae",
        "commit": "https://github.com/apache/hadoop/commit/3b00eaea256d252be3361a7d9106b88756fcb9ba",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1211,6 +1211,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-8948. Use GenericTestUtils to set log levels in TestPread and\n     TestReplaceDatanodeOnFailure. (Mingliang Liu via wheat9)\n \n+    HDFS-8932. NPE thrown in NameNode when try to get TotalSyncCount metric\n+    before editLogStream initialization. (Surendra Singh Lilhore via xyao)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "7aadcc60337a497c271f62f09e74f7c23066b0f5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "patch": "@@ -1692,10 +1692,14 @@ private JournalManager createJournal(URI uri) {\n   }\n \n   /**\n-   +   * Return total number of syncs happened on this edit log.\n-   +   * @return long - count\n-   +   */\n+   * Return total number of syncs happened on this edit log.\n+   * @return long - count\n+   */\n   public long getTotalSyncCount() {\n-    return editLogStream.getNumSync();\n+    if (editLogStream != null) {\n+      return editLogStream.getNumSync();\n+    } else {\n+      return 0;\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "sha": "faaea633448cf6f5b16df7025c7e014629cb9dc5",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -7295,7 +7295,12 @@ public long getTotalSyncCount() {\n   @Metric({\"TotalSyncTimes\",\n               \"Total time spend in sync operation on various edit logs\"})\n   public String getTotalSyncTimes() {\n-    return fsImage.editLog.getJournalSet().getSyncTimes();\n+    JournalSet journalSet = fsImage.editLog.getJournalSet();\n+    if (journalSet != null) {\n+      return journalSet.getSyncTimes();\n+    } else {\n+      return \"\";\n+    }\n   }\n }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "3c3ef0b9797cc703fd7f27b11838af6781133c8d",
                "status": "modified"
            }
        ],
        "message": "HDFS-8932. NPE thrown in NameNode when try to get TotalSyncCount metric before editLogStream initialization. Contributed by Surendra Singh Lilhore",
        "parent": "https://github.com/apache/hadoop/commit/66d0c81d8f4e200a5051c8df87be890c9ad8772e",
        "patched_files": [
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_3f16651": {
        "bug_id": "hadoop_3f16651",
        "commit": "https://github.com/apache/hadoop/commit/3f166512afa2564ba1f34512e31476282af862be",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/3f166512afa2564ba1f34512e31476282af862be/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java?ref=3f166512afa2564ba1f34512e31476282af862be",
                "deletions": 1,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java",
                "patch": "@@ -193,7 +193,6 @@ private KeyManagerImpl(OzoneManager om, ScmClient scmClient,\n     this.secretManager = secretManager;\n     this.kmsProvider = kmsProvider;\n \n-    start(conf);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/3f166512afa2564ba1f34512e31476282af862be/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/KeyManagerImpl.java",
                "sha": "354c9075e3e97c7934cf95e227271eb3ead3a151",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/3f166512afa2564ba1f34512e31476282af862be/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java?ref=3f166512afa2564ba1f34512e31476282af862be",
                "deletions": 0,
                "filename": "hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java",
                "patch": "@@ -95,6 +95,7 @@ public void checkIfDeleteServiceisDeletingKeys()\n         new KeyManagerImpl(\n             new ScmBlockLocationTestingClient(null, null, 0),\n             metaMgr, conf, UUID.randomUUID().toString(), null);\n+    keyManager.start(conf);\n     final int keyCount = 100;\n     createAndDeleteKeys(keyManager, keyCount, 1);\n     KeyDeletingService keyDeletingService =\n@@ -117,6 +118,7 @@ public void checkIfDeleteServiceWithFailingSCM()\n         new KeyManagerImpl(\n             new ScmBlockLocationTestingClient(null, null, 1),\n             metaMgr, conf, UUID.randomUUID().toString(), null);\n+    keyManager.start(conf);\n     final int keyCount = 100;\n     createAndDeleteKeys(keyManager, keyCount, 1);\n     KeyDeletingService keyDeletingService =\n@@ -144,6 +146,7 @@ public void checkDeletionForEmptyKey()\n         new KeyManagerImpl(\n             new ScmBlockLocationTestingClient(null, null, 1),\n             metaMgr, conf, UUID.randomUUID().toString(), null);\n+    keyManager.start(conf);\n     final int keyCount = 100;\n     createAndDeleteKeys(keyManager, keyCount, 0);\n     KeyDeletingService keyDeletingService =",
                "raw_url": "https://github.com/apache/hadoop/raw/3f166512afa2564ba1f34512e31476282af862be/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/TestKeyDeletingService.java",
                "sha": "3c707ba1e18bde091d3a4f900eaebfd71a6cac9f",
                "status": "modified"
            }
        ],
        "message": "HDDS-2237. KeyDeletingService throws NPE if it's started too early (#1584)",
        "parent": "https://github.com/apache/hadoop/commit/4510970e2f7728d036c750b596985e5ffa357b60",
        "patched_files": [
            "KeyDeletingService.java",
            "KeyManagerImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestKeyManagerImpl.java",
            "TestKeyDeletingService.java"
        ]
    },
    "hadoop_3f622a1": {
        "bug_id": "hadoop_3f622a1",
        "commit": "https://github.com/apache/hadoop/commit/3f622a143c5fb15fee7e5dded99e4a4136f19810",
        "file": [
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/hadoop/blob/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java?ref=3f622a143c5fb15fee7e5dded99e4a4136f19810",
                "deletions": 10,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java",
                "patch": "@@ -63,6 +63,7 @@\n   public static final String TASK_ID_REGEX = TASK + \"_(\\\\d+)_(\\\\d+)_\" +\n       CharTaskTypeMaps.allTaskTypes + \"_(\\\\d+)\";\n   public static final Pattern taskIdPattern = Pattern.compile(TASK_ID_REGEX);\n+\n   static {\n     idFormat.setGroupingUsed(false);\n     idFormat.setMinimumIntegerDigits(6);\n@@ -72,7 +73,8 @@\n   private TaskType type;\n   \n   /**\n-   * Constructs a TaskID object from given {@link JobID}.  \n+   * Constructs a TaskID object from given {@link JobID}.\n+   *\n    * @param jobId JobID that this tip belongs to \n    * @param type the {@link TaskType} of the task \n    * @param id the tip number\n@@ -88,6 +90,7 @@ public TaskID(JobID jobId, TaskType type, int id) {\n   \n   /**\n    * Constructs a TaskInProgressId object from given parts.\n+   *\n    * @param jtIdentifier jobTracker identifier\n    * @param jobId job number \n    * @param type the TaskType \n@@ -99,6 +102,7 @@ public TaskID(String jtIdentifier, int jobId, TaskType type, int id) {\n \n   /**\n    * Constructs a TaskID object from given {@link JobID}.\n+   *\n    * @param jobId JobID that this tip belongs to\n    * @param isMap whether the tip is a map\n    * @param id the tip number\n@@ -110,6 +114,7 @@ public TaskID(JobID jobId, boolean isMap, int id) {\n \n   /**\n    * Constructs a TaskInProgressId object from given parts.\n+   *\n    * @param jtIdentifier jobTracker identifier\n    * @param jobId job number\n    * @param isMap whether the tip is a map\n@@ -120,23 +125,37 @@ public TaskID(String jtIdentifier, int jobId, boolean isMap, int id) {\n     this(new JobID(jtIdentifier, jobId), isMap, id);\n   }\n   \n+  /**\n+   * Default constructor for Writable. Sets the task type to\n+   * {@link TaskType#REDUCE}, the ID to 0, and the job ID to an empty job ID.\n+   */\n   public TaskID() { \n-    jobId = new JobID();\n+    this(new JobID(), TaskType.REDUCE, 0);\n   }\n   \n-  /** Returns the {@link JobID} object that this tip belongs to */\n+  /**\n+   * Returns the {@link JobID} object that this tip belongs to.\n+   *\n+   * @return the JobID object\n+   */\n   public JobID getJobID() {\n     return jobId;\n   }\n   \n-  /**Returns whether this TaskID is a map ID */\n+  /**\n+   * Returns whether this TaskID is a map ID.\n+   *\n+   * @return whether this TaskID is a map ID\n+   */\n   @Deprecated\n   public boolean isMap() {\n     return type == TaskType.MAP;\n   }\n     \n   /**\n-   * Get the type of the task\n+   * Get the type of the task.\n+   *\n+   * @return the type of the task\n    */\n   public TaskType getTaskType() {\n     return type;\n@@ -151,8 +170,14 @@ public boolean equals(Object o) {\n     return this.type == that.type && this.jobId.equals(that.jobId);\n   }\n \n-  /**Compare TaskInProgressIds by first jobIds, then by tip numbers. Reduces are \n-   * defined as greater then maps.*/\n+  /**\n+   * Compare TaskInProgressIds by first jobIds, then by tip numbers.\n+   * Reducers are defined as greater than mappers.\n+   *\n+   * @param o the TaskID against which to compare\n+   * @return 0 if equal, positive if this TaskID is greater, and negative if\n+   * this TaskID is less\n+   */\n   @Override\n   public int compareTo(ID o) {\n     TaskID that = (TaskID)o;\n@@ -174,6 +199,7 @@ public String toString() {\n \n   /**\n    * Add the unique string to the given builder.\n+   *\n    * @param builder the builder to append to\n    * @return the builder that was passed in\n    */\n@@ -204,7 +230,10 @@ public void write(DataOutput out) throws IOException {\n     WritableUtils.writeEnum(out, type);\n   }\n   \n-  /** Construct a TaskID object from given string \n+  /**\n+   * Construct a TaskID object from given string.\n+   *\n+   * @param str the target string\n    * @return constructed TaskID object or null if the given String is null\n    * @throws IllegalArgumentException if the given string is malformed\n    */\n@@ -224,22 +253,30 @@ public static TaskID forName(String str)\n     throw new IllegalArgumentException(exceptionMsg);\n   }\n   /**\n-   * Gets the character representing the {@link TaskType}\n+   * Gets the character representing the {@link TaskType}.\n+   *\n    * @param type the TaskType\n    * @return the character\n    */\n   public static char getRepresentingCharacter(TaskType type) {\n     return CharTaskTypeMaps.getRepresentingCharacter(type);\n   }\n   /**\n-   * Gets the {@link TaskType} corresponding to the character\n+   * Gets the {@link TaskType} corresponding to the character.\n+   *\n    * @param c the character\n    * @return the TaskType\n    */\n   public static TaskType getTaskType(char c) {\n     return CharTaskTypeMaps.getTaskType(c);\n   }\n   \n+  /**\n+   * Returns a string of characters describing all possible {@link TaskType}\n+   * values\n+   *\n+   * @return a string of all task type characters\n+   */\n   public static String getAllTaskTypes() {\n     return CharTaskTypeMaps.allTaskTypes;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java",
                "sha": "3ddfbe97c8437e73e0336450e61355d4d29a0424",
                "status": "modified"
            },
            {
                "additions": 461,
                "blob_url": "https://github.com/apache/hadoop/blob/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java",
                "changes": 461,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java?ref=3f622a143c5fb15fee7e5dded99e4a4136f19810",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java",
                "patch": "@@ -0,0 +1,461 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapreduce;\n+\n+import org.apache.hadoop.io.DataInputByteBuffer;\n+import org.apache.hadoop.io.DataOutputByteBuffer;\n+import org.apache.hadoop.io.WritableUtils;\n+import org.junit.Test;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Test the {@link TaskID} class.\n+ */\n+public class TestTaskID {\n+  /**\n+   * Test of getJobID method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetJobID() {\n+    JobID jobId = new JobID(\"1234\", 0);\n+    TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);\n+\n+    assertSame(\"TaskID did not store the JobID correctly\",\n+        jobId, taskId.getJobID());\n+\n+    taskId = new TaskID();\n+\n+    assertEquals(\"Job ID was set unexpectedly in default contsructor\",\n+        \"\", taskId.getJobID().getJtIdentifier());\n+  }\n+\n+  /**\n+   * Test of isMap method, of class TaskID.\n+   */\n+  @Test\n+  public void testIsMap() {\n+    JobID jobId = new JobID(\"1234\", 0);\n+\n+    for (TaskType type : TaskType.values()) {\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+\n+      if (type == TaskType.MAP) {\n+        assertTrue(\"TaskID for map task did not correctly identify itself \"\n+            + \"as a map task\", taskId.isMap());\n+      } else {\n+        assertFalse(\"TaskID for \" + type + \" task incorrectly identified \"\n+            + \"itself as a map task\", taskId.isMap());\n+      }\n+    }\n+\n+    TaskID taskId = new TaskID();\n+\n+    assertFalse(\"TaskID of default type incorrectly identified itself as a \"\n+        + \"map task\", taskId.isMap());\n+  }\n+\n+  /**\n+   * Test of getTaskType method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetTaskType_0args() {\n+    JobID jobId = new JobID(\"1234\", 0);\n+\n+    for (TaskType type : TaskType.values()) {\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+\n+      assertEquals(\"TaskID incorrectly reported its type\",\n+          type, taskId.getTaskType());\n+    }\n+\n+    TaskID taskId = new TaskID();\n+\n+    assertEquals(\"TaskID of default type incorrectly reported its type\",\n+        TaskType.REDUCE, taskId.getTaskType());\n+  }\n+\n+  /**\n+   * Test of equals method, of class TaskID.\n+   */\n+  @Test\n+  public void testEquals() {\n+    JobID jobId1 = new JobID(\"1234\", 1);\n+    JobID jobId2 = new JobID(\"2345\", 2);\n+    TaskID taskId1 = new TaskID(jobId1, TaskType.MAP, 0);\n+    TaskID taskId2 = new TaskID(jobId1, TaskType.MAP, 0);\n+\n+    assertTrue(\"The equals() method reported two equal task IDs were not equal\",\n+        taskId1.equals(taskId2));\n+\n+    taskId2 = new TaskID(jobId2, TaskType.MAP, 0);\n+\n+    assertFalse(\"The equals() method reported two task IDs with different \"\n+        + \"job IDs were equal\", taskId1.equals(taskId2));\n+\n+    taskId2 = new TaskID(jobId1, TaskType.MAP, 1);\n+\n+    assertFalse(\"The equals() method reported two task IDs with different IDs \"\n+        + \"were equal\", taskId1.equals(taskId2));\n+\n+    TaskType[] types = TaskType.values();\n+\n+    for (int i = 0; i < types.length; i++) {\n+      for (int j = 0; j < types.length; j++) {\n+        taskId1 = new TaskID(jobId1, types[i], 0);\n+        taskId2 = new TaskID(jobId1, types[j], 0);\n+\n+        if (i == j) {\n+          assertTrue(\"The equals() method reported two equal task IDs were not \"\n+              + \"equal\", taskId1.equals(taskId2));\n+        } else {\n+          assertFalse(\"The equals() method reported two task IDs with \"\n+              + \"different types were equal\", taskId1.equals(taskId2));\n+        }\n+      }\n+    }\n+\n+    assertFalse(\"The equals() method matched against a JobID object\",\n+        taskId1.equals(jobId1));\n+\n+    assertFalse(\"The equals() method matched against a null object\",\n+        taskId1.equals(null));\n+  }\n+\n+  /**\n+   * Test of compareTo method, of class TaskID.\n+   */\n+  @Test\n+  public void testCompareTo() {\n+    JobID jobId = new JobID(\"1234\", 1);\n+    TaskID taskId1 = new TaskID(jobId, TaskType.REDUCE, 0);\n+    TaskID taskId2 = new TaskID(jobId, TaskType.REDUCE, 0);\n+\n+    assertEquals(\"The compareTo() method returned non-zero for two equal \"\n+        + \"task IDs\", 0, taskId1.compareTo(taskId2));\n+\n+    taskId2 = new TaskID(jobId, TaskType.MAP, 1);\n+\n+    assertTrue(\"The compareTo() method did not weigh task type more than task \"\n+        + \"ID\", taskId1.compareTo(taskId2) > 0);\n+\n+    TaskType[] types = TaskType.values();\n+\n+    for (int i = 0; i < types.length; i++) {\n+      for (int j = 0; j < types.length; j++) {\n+        taskId1 = new TaskID(jobId, types[i], 0);\n+        taskId2 = new TaskID(jobId, types[j], 0);\n+\n+        if (i == j) {\n+          assertEquals(\"The compareTo() method returned non-zero for two equal \"\n+              + \"task IDs\", 0, taskId1.compareTo(taskId2));\n+        } else if (i < j) {\n+          assertTrue(\"The compareTo() method did not order \" + types[i]\n+              + \" before \" + types[j], taskId1.compareTo(taskId2) < 0);\n+        } else {\n+          assertTrue(\"The compareTo() method did not order \" + types[i]\n+              + \" after \" + types[j], taskId1.compareTo(taskId2) > 0);\n+        }\n+      }\n+    }\n+\n+    try {\n+      taskId1.compareTo(jobId);\n+      fail(\"The compareTo() method allowed comparison to a JobID object\");\n+    } catch (ClassCastException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      taskId1.compareTo(null);\n+      fail(\"The compareTo() method allowed comparison to a null object\");\n+    } catch (NullPointerException ex) {\n+      // Expected\n+    }\n+  }\n+\n+  /**\n+   * Test of toString method, of class TaskID.\n+   */\n+  @Test\n+  public void testToString() {\n+    JobID jobId = new JobID(\"1234\", 1);\n+\n+    for (TaskType type : TaskType.values()) {\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+      String str = String.format(\"task_1234_0001_%c_000000\",\n+          TaskID.getRepresentingCharacter(type));\n+\n+      assertEquals(\"The toString() method returned the wrong value\",\n+          str, taskId.toString());\n+    }\n+  }\n+\n+  /**\n+   * Test of appendTo method, of class TaskID.\n+   */\n+  @Test\n+  public void testAppendTo() {\n+    JobID jobId = new JobID(\"1234\", 1);\n+    StringBuilder builder = new StringBuilder();\n+\n+    for (TaskType type : TaskType.values()) {\n+      builder.setLength(0);\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+      String str = String.format(\"_1234_0001_%c_000000\",\n+          TaskID.getRepresentingCharacter(type));\n+\n+      assertEquals(\"The appendTo() method appended the wrong value\",\n+          str, taskId.appendTo(builder).toString());\n+    }\n+\n+    try {\n+      new TaskID().appendTo(null);\n+      fail(\"The appendTo() method allowed a null builder\");\n+    } catch (NullPointerException ex) {\n+      // Expected\n+    }\n+  }\n+\n+  /**\n+   * Test of hashCode method, of class TaskID.\n+   */\n+  @Test\n+  public void testHashCode() {\n+    TaskType[] types = TaskType.values();\n+\n+    for (int i = 0; i < types.length; i++) {\n+      JobID jobId = new JobID(\"1234\" + i, i);\n+      TaskID taskId1 = new TaskID(jobId, types[i], i);\n+      TaskID taskId2 = new TaskID(jobId, types[i], i);\n+\n+      assertTrue(\"The hashcode() method gave unequal hash codes for two equal \"\n+          + \"task IDs\", taskId1.hashCode() == taskId2.hashCode());\n+    }\n+  }\n+\n+  /**\n+   * Test of readFields method, of class TaskID.\n+   */\n+  @Test\n+  public void testReadFields() throws Exception {\n+    DataOutputByteBuffer out = new DataOutputByteBuffer();\n+\n+    out.writeInt(0);\n+    out.writeInt(1);\n+    WritableUtils.writeVInt(out, 4);\n+    out.write(new byte[] { 0x31, 0x32, 0x33, 0x34});\n+    WritableUtils.writeEnum(out, TaskType.REDUCE);\n+\n+    DataInputByteBuffer in = new DataInputByteBuffer();\n+\n+    in.reset(out.getData());\n+\n+    TaskID instance = new TaskID();\n+\n+    instance.readFields(in);\n+\n+    assertEquals(\"The readFields() method did not produce the expected task ID\",\n+        \"task_1234_0001_r_000000\", instance.toString());\n+  }\n+\n+  /**\n+   * Test of write method, of class TaskID.\n+   */\n+  @Test\n+  public void testWrite() throws Exception {\n+    JobID jobId = new JobID(\"1234\", 1);\n+    TaskID taskId = new TaskID(jobId, TaskType.JOB_SETUP, 0);\n+    DataOutputByteBuffer out = new DataOutputByteBuffer();\n+\n+    taskId.write(out);\n+\n+    DataInputByteBuffer in = new DataInputByteBuffer();\n+    byte[] buffer = new byte[4];\n+\n+    in.reset(out.getData());\n+\n+    assertEquals(\"The write() method did not write the expected task ID\",\n+        0, in.readInt());\n+    assertEquals(\"The write() method did not write the expected job ID\",\n+        1, in.readInt());\n+    assertEquals(\"The write() method did not write the expected job \"\n+        + \"identifier length\", 4, WritableUtils.readVInt(in));\n+    in.readFully(buffer, 0, 4);\n+    assertEquals(\"The write() method did not write the expected job \"\n+        + \"identifier length\", \"1234\", new String(buffer));\n+    assertEquals(\"The write() method did not write the expected task type\",\n+        TaskType.JOB_SETUP, WritableUtils.readEnum(in, TaskType.class));\n+  }\n+\n+  /**\n+   * Test of forName method, of class TaskID.\n+   */\n+  @Test\n+  public void testForName() {\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_1_0001_m_000000\",\n+        TaskID.forName(\"task_1_0001_m_000\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_23_0002_r_000001\",\n+        TaskID.forName(\"task_23_0002_r_0001\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_345_0003_s_000002\",\n+        TaskID.forName(\"task_345_0003_s_00002\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_6789_0004_c_000003\",\n+        TaskID.forName(\"task_6789_0004_c_000003\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_12345_0005_t_4000000\",\n+        TaskID.forName(\"task_12345_0005_t_4000000\").toString());\n+\n+    try {\n+      TaskID.forName(\"tisk_12345_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"tisk_12345_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"tisk_12345_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"tisk_12345_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_abc_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_abc_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_xyz_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_xyz_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_x_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_x_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_t_jkl\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_t_jkl\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_t\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_t\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"12345_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"12345_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+  }\n+\n+  /**\n+   * Test of getRepresentingCharacter method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetRepresentingCharacter() {\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 'm',\n+        TaskID.getRepresentingCharacter(TaskType.MAP));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 'r',\n+        TaskID.getRepresentingCharacter(TaskType.REDUCE));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 's',\n+        TaskID.getRepresentingCharacter(TaskType.JOB_SETUP));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 'c',\n+        TaskID.getRepresentingCharacter(TaskType.JOB_CLEANUP));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 't',\n+        TaskID.getRepresentingCharacter(TaskType.TASK_CLEANUP));\n+  }\n+\n+  /**\n+   * Test of getTaskType method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetTaskType_char() {\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.MAP,\n+        TaskID.getTaskType('m'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.REDUCE,\n+        TaskID.getTaskType('r'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.JOB_SETUP,\n+        TaskID.getTaskType('s'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.JOB_CLEANUP,\n+        TaskID.getTaskType('c'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.TASK_CLEANUP,\n+        TaskID.getTaskType('t'));\n+    assertNull(\"The getTaskType() method did not return null for an unknown \"\n+        + \"type\", TaskID.getTaskType('x'));\n+  }\n+\n+  /**\n+   * Test of getAllTaskTypes method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetAllTaskTypes() {\n+    assertEquals(\"The getAllTaskTypes method did not return the expected \"\n+        + \"string\", \"(m|r|s|c|t)\", TaskID.getAllTaskTypes());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java",
                "sha": "5531074dce84205e30fd05d7085df8fa63f998c9",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-6535. TaskID default constructor results in NPE on toString(). Contributed by Daniel Templeton",
        "parent": "https://github.com/apache/hadoop/commit/2c268cc9365851f5b02d967d13c8c0cbca850a86",
        "patched_files": [
            "TaskID.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskID.java"
        ]
    },
    "hadoop_3fde0f1": {
        "bug_id": "hadoop_3fde0f1",
        "commit": "https://github.com/apache/hadoop/commit/3fde0f1db599227773c0cd537b33312d368ad4d9",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/3fde0f1db599227773c0cd537b33312d368ad4d9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java?ref=3fde0f1db599227773c0cd537b33312d368ad4d9",
                "deletions": 5,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "patch": "@@ -236,8 +236,8 @@ public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {\n     DecayTask task = new DecayTask(this, timer);\n     timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n \n-    metricsProxy = MetricsProxy.getInstance(ns, numLevels);\n-    metricsProxy.setDelegate(this);\n+    metricsProxy = MetricsProxy.getInstance(ns, numLevels, this);\n+    recomputeScheduleCache();\n   }\n \n   // Load configs\n@@ -680,21 +680,26 @@ public long getTotalCallSnapshot() {\n     private long[] callCountInLastWindowDefault;\n     private ObjectName decayRpcSchedulerInfoBeanName;\n \n-    private MetricsProxy(String namespace, int numLevels) {\n+    private MetricsProxy(String namespace, int numLevels,\n+        DecayRpcScheduler drs) {\n       averageResponseTimeDefault = new double[numLevels];\n       callCountInLastWindowDefault = new long[numLevels];\n+      setDelegate(drs);\n       decayRpcSchedulerInfoBeanName =\n           MBeans.register(namespace, \"DecayRpcScheduler\", this);\n       this.registerMetrics2Source(namespace);\n     }\n \n     public static synchronized MetricsProxy getInstance(String namespace,\n-        int numLevels) {\n+        int numLevels, DecayRpcScheduler drs) {\n       MetricsProxy mp = INSTANCES.get(namespace);\n       if (mp == null) {\n         // We must create one\n-        mp = new MetricsProxy(namespace, numLevels);\n+        mp = new MetricsProxy(namespace, numLevels, drs);\n         INSTANCES.put(namespace, mp);\n+      } else  if (drs != mp.delegate.get()){\n+        // in case of delegate is reclaimed, we should set it again\n+        mp.setDelegate(drs);\n       }\n       return mp;\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/3fde0f1db599227773c0cd537b33312d368ad4d9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "sha": "0a00ca73d9641657892180361a1f5c0ed529f9b2",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/3fde0f1db599227773c0cd537b33312d368ad4d9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestDecayRpcScheduler.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestDecayRpcScheduler.java?ref=3fde0f1db599227773c0cd537b33312d368ad4d9",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestDecayRpcScheduler.java",
                "patch": "@@ -19,19 +19,22 @@\n package org.apache.hadoop.ipc;\n \n import static java.lang.Thread.sleep;\n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n \n import org.junit.Test;\n+\n+import static org.junit.Assert.*;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n+import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.conf.Configuration;\n \n import javax.management.MBeanServer;\n import javax.management.ObjectName;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.PrintStream;\n import java.lang.management.ManagementFactory;\n \n public class TestDecayRpcScheduler {\n@@ -248,4 +251,27 @@ public void testPeriodic() throws InterruptedException {\n       sleep(10);\n     }\n   }\n+\n+  @Test(timeout=60000)\n+  public void testNPEatInitialization() throws InterruptedException {\n+    // redirect the LOG to and check if there is NPE message while initializing\n+    // the DecayRpcScheduler\n+    PrintStream output = System.out;\n+    try {\n+      ByteArrayOutputStream bytes = new ByteArrayOutputStream();\n+      System.setOut(new PrintStream(bytes));\n+\n+      // initializing DefaultMetricsSystem here would set \"monitoring\" flag in\n+      // MetricsSystemImpl to true\n+      DefaultMetricsSystem.initialize(\"NameNode\");\n+      Configuration conf = new Configuration();\n+      scheduler = new DecayRpcScheduler(1, \"ns\", conf);\n+      // check if there is npe in log\n+      assertFalse(bytes.toString().contains(\"NullPointerException\"));\n+    } finally {\n+      //set systout back\n+      System.setOut(output);\n+    }\n+\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/3fde0f1db599227773c0cd537b33312d368ad4d9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ipc/TestDecayRpcScheduler.java",
                "sha": "10ab40ace1f678e51c2044f07bc92d4f210a85f5",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15121. Encounter NullPointerException when using DecayRpcScheduler. Contributed by Tao Jie.",
        "parent": "https://github.com/apache/hadoop/commit/97fe3cc187cb9f773777ca79db6f1c7e4d1d5a68",
        "patched_files": [
            "DecayRpcScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDecayRpcScheduler.java"
        ]
    },
    "hadoop_4002bf0": {
        "bug_id": "hadoop_4002bf0",
        "commit": "https://github.com/apache/hadoop/commit/4002bf0a9e7e3619a3bdcff071f0369cbf2873ad",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/4002bf0a9e7e3619a3bdcff071f0369cbf2873ad/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java?ref=4002bf0a9e7e3619a3bdcff071f0369cbf2873ad",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -146,7 +146,11 @@ private static void prepareTimelineStore(TimelineStore store, int scale)\n       } else if (i == 3) {\n         entities.addEntity(createApplicationTimelineEntity(\n             appId, false, false, false, false, YarnApplicationState.FINISHED,\n-            true));\n+            true, false));\n+      } else if (i == SCALE + 1) {\n+        entities.addEntity(createApplicationTimelineEntity(\n+            appId, false, false, false, false, YarnApplicationState.FINISHED,\n+            false, true));\n       } else {\n         entities.addEntity(createApplicationTimelineEntity(\n             appId, false, false, false, false, YarnApplicationState.FINISHED));\n@@ -497,13 +501,14 @@ private static TimelineEntity createApplicationTimelineEntity(\n       boolean wrongAppId, boolean enableUpdateEvent,\n       YarnApplicationState state) {\n     return createApplicationTimelineEntity(appId, emptyACLs, noAttemptId,\n-        wrongAppId, enableUpdateEvent, state, false);\n+        wrongAppId, enableUpdateEvent, state, false, false);\n   }\n \n   private static TimelineEntity createApplicationTimelineEntity(\n       ApplicationId appId, boolean emptyACLs, boolean noAttemptId,\n       boolean wrongAppId, boolean enableUpdateEvent,\n-      YarnApplicationState state, boolean missingPreemptMetrics) {\n+      YarnApplicationState state, boolean missingPreemptMetrics,\n+      boolean missingQueue) {\n     TimelineEntity entity = new TimelineEntity();\n     entity.setEntityType(ApplicationMetricsConstants.ENTITY_TYPE);\n     if (wrongAppId) {\n@@ -519,7 +524,10 @@ private static TimelineEntity createApplicationTimelineEntity(\n     entityInfo.put(ApplicationMetricsConstants.TYPE_ENTITY_INFO,\n         \"test app type\");\n     entityInfo.put(ApplicationMetricsConstants.USER_ENTITY_INFO, \"user1\");\n-    entityInfo.put(ApplicationMetricsConstants.QUEUE_ENTITY_INFO, \"test queue\");\n+    if (!missingQueue) {\n+      entityInfo.put(ApplicationMetricsConstants.QUEUE_ENTITY_INFO,\n+          \"test queue\");\n+    }\n     entityInfo.put(\n         ApplicationMetricsConstants.UNMANAGED_APPLICATION_ENTITY_INFO, \"false\");\n     entityInfo.put(ApplicationMetricsConstants.APPLICATION_PRIORITY_INFO,",
                "raw_url": "https://github.com/apache/hadoop/raw/4002bf0a9e7e3619a3bdcff071f0369cbf2873ad/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "sha": "ecaaf1e878dc5d5e11f10420d5c47b938f4b673d",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/4002bf0a9e7e3619a3bdcff071f0369cbf2873ad/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/webapp/TestAHSWebServices.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/webapp/TestAHSWebServices.java?ref=4002bf0a9e7e3619a3bdcff071f0369cbf2873ad",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/webapp/TestAHSWebServices.java",
                "patch": "@@ -100,7 +100,7 @@\n   private static ApplicationHistoryClientService historyClientService;\n   private static AHSWebServices ahsWebservice;\n   private static final String[] USERS = new String[] { \"foo\" , \"bar\" };\n-  private static final int MAX_APPS = 5;\n+  private static final int MAX_APPS = 6;\n   private static Configuration conf;\n   private static FileSystem fs;\n   private static final String remoteLogRootDir = \"target/logs/\";\n@@ -364,7 +364,27 @@ public void testAppsQuery() throws Exception {\n     JSONObject apps = json.getJSONObject(\"apps\");\n     assertEquals(\"incorrect number of elements\", 1, apps.length());\n     JSONArray array = apps.getJSONArray(\"app\");\n-    assertEquals(\"incorrect number of elements\", 5, array.length());\n+    assertEquals(\"incorrect number of elements\", MAX_APPS, array.length());\n+  }\n+\n+  @Test\n+  public void testQueueQuery() throws Exception {\n+    WebResource r = resource();\n+    ClientResponse response =\n+        r.path(\"ws\").path(\"v1\").path(\"applicationhistory\").path(\"apps\")\n+            .queryParam(\"queue\", \"test queue\")\n+            .queryParam(\"user.name\", USERS[round])\n+            .accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);\n+    assertResponseStatusCode(Status.OK, response.getStatusInfo());\n+    assertEquals(MediaType.APPLICATION_JSON + \"; \" + JettyUtils.UTF_8,\n+        response.getType().toString());\n+    JSONObject json = response.getEntity(JSONObject.class);\n+    assertEquals(\"incorrect number of elements\", 1, json.length());\n+    JSONObject apps = json.getJSONObject(\"apps\");\n+    assertEquals(\"incorrect number of elements\", 1, apps.length());\n+    JSONArray array = apps.getJSONArray(\"app\");\n+    assertEquals(\"incorrect number of elements\", MAX_APPS - 1,\n+        array.length());\n   }\n \n   @Test\n@@ -414,7 +434,7 @@ public void testMultipleAttempts() throws Exception {\n     JSONObject appAttempts = json.getJSONObject(\"appAttempts\");\n     assertEquals(\"incorrect number of elements\", 1, appAttempts.length());\n     JSONArray array = appAttempts.getJSONArray(\"appAttempt\");\n-    assertEquals(\"incorrect number of elements\", 5, array.length());\n+    assertEquals(\"incorrect number of elements\", MAX_APPS, array.length());\n   }\n \n   @Test\n@@ -471,7 +491,7 @@ public void testMultipleContainers() throws Exception {\n     JSONObject containers = json.getJSONObject(\"containers\");\n     assertEquals(\"incorrect number of elements\", 1, containers.length());\n     JSONArray array = containers.getJSONArray(\"container\");\n-    assertEquals(\"incorrect number of elements\", 5, array.length());\n+    assertEquals(\"incorrect number of elements\", MAX_APPS, array.length());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/4002bf0a9e7e3619a3bdcff071f0369cbf2873ad/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/webapp/TestAHSWebServices.java",
                "sha": "154c68a6a3fb45d7a167cee1b2d920bfc7326675",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4002bf0a9e7e3619a3bdcff071f0369cbf2873ad/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/WebServices.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/WebServices.java?ref=4002bf0a9e7e3619a3bdcff071f0369cbf2873ad",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/WebServices.java",
                "patch": "@@ -192,7 +192,8 @@ public AppsInfo getApps(HttpServletRequest req, HttpServletResponse res,\n         }\n       }\n       if (queueQuery != null && !queueQuery.isEmpty()) {\n-        if (!appReport.getQueue().equals(queueQuery)) {\n+        if (appReport.getQueue() == null || !appReport.getQueue()\n+            .equals(queueQuery)) {\n           continue;\n         }\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/4002bf0a9e7e3619a3bdcff071f0369cbf2873ad/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/WebServices.java",
                "sha": "1399099a28af0e43b47fa22000e6a001531437fe",
                "status": "modified"
            }
        ],
        "message": "YARN-7118. AHS REST API can return NullPointerException. Contributed by Billie Rinaldi.",
        "parent": "https://github.com/apache/hadoop/commit/b133dc5700b66318ac6d80e463b049723c511f37",
        "patched_files": [
            "ApplicationHistoryManagerOnTimelineStore.java",
            "AHSWebServices.java",
            "WebServices.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationHistoryManagerOnTimelineStore.java",
            "TestAHSWebServices.java"
        ]
    },
    "hadoop_417951a": {
        "bug_id": "hadoop_417951a",
        "commit": "https://github.com/apache/hadoop/commit/417951ab587e6905d6ab8c08d4f33063a3f53dae",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/417951ab587e6905d6ab8c08d4f33063a3f53dae/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java?ref=417951ab587e6905d6ab8c08d4f33063a3f53dae",
                "deletions": 0,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "patch": "@@ -787,6 +787,7 @@ public void start() throws IOException {\n     keyManager.start(configuration);\n     omRpcServer.start();\n     try {\n+      httpServer = new OzoneManagerHttpServer(configuration, this);\n       httpServer.start();\n     } catch (Exception ex) {\n       // Allow OM to start as Http Server failure is not fatal.",
                "raw_url": "https://github.com/apache/hadoop/raw/417951ab587e6905d6ab8c08d4f33063a3f53dae/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "sha": "d391516b6ca22a9b6af01aaa1f53e647a40f8201",
                "status": "modified"
            }
        ],
        "message": "HDDS-884. Fix merge issue that causes NPE OzoneManager#httpServer. Contributed by Xiaoyu Yao.",
        "parent": "https://github.com/apache/hadoop/commit/6d522dc05d473d7e52f6e236c4b6bebdb91748eb",
        "patched_files": [
            "OzoneManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOzoneManager.java"
        ]
    },
    "hadoop_42a185b": {
        "bug_id": "hadoop_42a185b",
        "commit": "https://github.com/apache/hadoop/commit/42a185b57d3136a1ac108072132aced21d9f5d17",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7322. Adding a util method in FileUtil for directory listing,\n+    avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7023. Add listCorruptFileBlocks to Filesysem. (Patrick Kling\n     via hairong)\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/CHANGES.txt",
                "sha": "b6d57b4104b866a5cfb9af40fea15f9e617741c6",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -324,7 +324,7 @@ public static boolean copy(File src,\n       if (!dstFS.mkdirs(dst)) {\n         return false;\n       }\n-      File contents[] = src.listFiles();\n+      File contents[] = listFiles(src);\n       for (int i = 0; i < contents.length; i++) {\n         copy(contents[i], dstFS, new Path(dst, contents[i].getName()),\n              deleteSource, conf);\n@@ -486,8 +486,10 @@ public static long getDU(File dir) {\n     } else {\n       size = dir.length();\n       File[] allFiles = dir.listFiles();\n-      for (int i = 0; i < allFiles.length; i++) {\n-        size = size + getDU(allFiles[i]);\n+      if(allFiles != null) {\n+         for (int i = 0; i < allFiles.length; i++) {\n+            size = size + getDU(allFiles[i]);\n+         }\n       }\n       return size;\n     }\n@@ -707,4 +709,23 @@ public static void replaceFile(File src, File target) throws IOException {\n       }\n     }\n   }\n+  \n+  /**\n+   * A wrapper for {@link File#listFiles()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#listFiles() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of files or empty list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static File[] listFiles(File dir) throws IOException {\n+    File[] files = dir.listFiles();\n+    if(files == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return files;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "537959bd7317e49342f72650c5507729f3b5ea0a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "patch": "@@ -276,7 +276,7 @@ public boolean delete(Path p, boolean recursive) throws IOException {\n     if (f.isFile()) {\n       return f.delete();\n     } else if ((!recursive) && f.isDirectory() && \n-        (f.listFiles().length != 0)) {\n+        (FileUtil.listFiles(f).length != 0)) {\n       throw new IOException(\"Directory \" + f.toString() + \" is not empty\");\n     }\n     return FileUtil.fullyDelete(f);",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "sha": "63579980cd8f3ca5d4d3b1b2b6132bf54426cc61",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -117,6 +117,33 @@ private void createFile(File directory, String name, String contents)\n     }\n   }\n \n+  @Test\n+  public void testListFiles() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    File[] files = FileUtil.listFiles(partitioned);\n+    Assert.assertEquals(2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.listFiles(newDir);\n+    Assert.assertEquals(0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.listFiles(newDir);\n+      Assert.fail(\"IOException expected on listFiles() for non-existent dir \"\n+      \t\t+ newDir.toString());\n+    } catch(IOException ioe) {\n+    \t//Expected an IOException\n+    }\n+  }\n+  \n   @After\n   public void tearDown() throws IOException {\n     FileUtil.fullyDelete(del);",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "1c193db3e74118e3d496ff68eec991522f008999",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7322. Adding a util method in FileUtil for directory listing, avoid NPEs on File.listFiles(). Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1127697 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e79d8afcd05722fe369eba919abe4f4205771a41",
        "patched_files": [
            "RawLocalFileSystem.java",
            "CHANGES.java",
            "FileUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop_44809b8": {
        "bug_id": "hadoop_44809b8",
        "commit": "https://github.com/apache/hadoop/commit/44809b80814d5520a73d5609d0f73a13eb2360ac",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=44809b80814d5520a73d5609d0f73a13eb2360ac",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -469,6 +469,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-10027. *Compressor_deflateBytesDirect passes instance instead of\n     jclass to GetStaticObjectField. (Hui Zheng via cnauroth)\n \n+    HADOOP-11724. DistCp throws NPE when the target directory is root.\n+    (Lei Eddy Xu via Yongjun Zhang) \n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2e26b0a48e13b514dd5ee8f9ac6a5e977c9a2e6c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java?ref=44809b80814d5520a73d5609d0f73a13eb2360ac",
                "deletions": 0,
                "filename": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "patch": "@@ -133,6 +133,9 @@ private void cleanupTempFiles(JobContext context) {\n   private void deleteAttemptTempFiles(Path targetWorkPath,\n                                       FileSystem targetFS,\n                                       String jobId) throws IOException {\n+    if (targetWorkPath == null) {\n+      return;\n+    }\n \n     FileStatus[] tempFiles = targetFS.globStatus(\n         new Path(targetWorkPath, \".distcp.tmp.\" + jobId.replaceAll(\"job\",\"attempt\") + \"*\"));",
                "raw_url": "https://github.com/apache/hadoop/raw/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "sha": "9ec57f426aa13944452c5fea73082af0b649caea",
                "status": "modified"
            }
        ],
        "message": "HADOOP-11724. DistCp throws NPE when the target directory is root. (Lei Eddy Xu via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/fc1031af749435dc95efea6745b1b2300ce29446",
        "patched_files": [
            "CopyCommitter.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCopyCommitter.java"
        ]
    },
    "hadoop_44fdf00": {
        "bug_id": "hadoop_44fdf00",
        "commit": "https://github.com/apache/hadoop/commit/44fdf009642ae4e99b15f89ec0ca43834f991ef3",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/44fdf009642ae4e99b15f89ec0ca43834f991ef3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java?ref=44fdf009642ae4e99b15f89ec0ca43834f991ef3",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java",
                "patch": "@@ -664,15 +664,20 @@ public synchronized int getClusterNodeCount() {\n     List<LinkedHashSet<T>> list = new LinkedList<LinkedHashSet<T>>();\n \n     RemoteRequestsTable remoteRequestsTable = getTable(0);\n-    List<ResourceRequestInfo<T>> matchingRequests =\n-        remoteRequestsTable.getMatchingRequests(priority, resourceName,\n-            executionType, capability);\n-    // If no exact match. Container may be larger than what was requested.\n-    // get all resources <= capability. map is reverse sorted.\n-    for (ResourceRequestInfo<T> resReqInfo : matchingRequests) {\n-      if (canFit(resReqInfo.remoteRequest.getCapability(), capability) &&\n-        !resReqInfo.containerRequests.isEmpty()) {\n-        list.add(resReqInfo.containerRequests);\n+\n+    if (null != remoteRequestsTable) {\n+      List<ResourceRequestInfo<T>> matchingRequests =\n+          remoteRequestsTable.getMatchingRequests(priority, resourceName,\n+              executionType, capability);\n+      if (null != matchingRequests) {\n+        // If no exact match. Container may be larger than what was requested.\n+        // get all resources <= capability. map is reverse sorted.\n+        for (ResourceRequestInfo<T> resReqInfo : matchingRequests) {\n+          if (canFit(resReqInfo.remoteRequest.getCapability(), capability) &&\n+              !resReqInfo.containerRequests.isEmpty()) {\n+            list.add(resReqInfo.containerRequests);\n+          }\n+        }\n       }\n     }\n     // no match found",
                "raw_url": "https://github.com/apache/hadoop/raw/44fdf009642ae4e99b15f89ec0ca43834f991ef3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/AMRMClientImpl.java",
                "sha": "3ed43b004b0743930de545746b6df52c8ee96b3c",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/44fdf009642ae4e99b15f89ec0ca43834f991ef3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClient.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClient.java?ref=44fdf009642ae4e99b15f89ec0ca43834f991ef3",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClient.java",
                "patch": "@@ -235,6 +235,20 @@ public static void tearDown() {\n       yarnCluster.stop();\n     }\n   }\n+\n+  @Test (timeout = 60000)\n+  public void testAMRMClientNoMatchingRequests()\n+      throws IOException, YarnException {\n+    AMRMClient<ContainerRequest> amClient =  AMRMClient.createAMRMClient();\n+    amClient.init(conf);\n+    amClient.start();\n+    amClient.registerApplicationMaster(\"Host\", 10000, \"\");\n+\n+    Resource testCapability1 = Resource.newInstance(1024,  2);\n+    List<? extends Collection<ContainerRequest>> matches =\n+        amClient.getMatchingRequests(priority, node, testCapability1);\n+    assertEquals(\"Expected no macthing requests.\", matches.size(), 0);\n+  }\n   \n   @Test (timeout=60000)\n   public void testAMRMClientMatchingFit() throws YarnException, IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/44fdf009642ae4e99b15f89ec0ca43834f991ef3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClient.java",
                "sha": "c4bb3ced94aea9512eb870cf69b1d07e60e9e126",
                "status": "modified"
            }
        ],
        "message": "YARN-5753. fix NPE in AMRMClientImpl.getMatchingRequests() (haibochen via rkanter)",
        "parent": "https://github.com/apache/hadoop/commit/d3bb69a66776e9f410150c4030ddb15981f58fb9",
        "patched_files": [
            "AMRMClient.java",
            "AMRMClientImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAMRMClient.java"
        ]
    },
    "hadoop_4530f45": {
        "bug_id": "hadoop_4530f45",
        "commit": "https://github.com/apache/hadoop/commit/4530f4500d308c9cefbcc5990769c04bd061ad87",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/4530f4500d308c9cefbcc5990769c04bd061ad87/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=4530f4500d308c9cefbcc5990769c04bd061ad87",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -526,9 +526,11 @@ protected void serviceStop() throws Exception {\n       DefaultMetricsSystem.shutdown();\n \n       // Cleanup ResourcePluginManager\n-      ResourcePluginManager rpm = context.getResourcePluginManager();\n-      if (rpm != null) {\n-        rpm.cleanup();\n+      if (null != context) {\n+        ResourcePluginManager rpm = context.getResourcePluginManager();\n+        if (rpm != null) {\n+          rpm.cleanup();\n+        }\n       }\n     } finally {\n       // YARN-3641: NM's services stop get failed shouldn't block the",
                "raw_url": "https://github.com/apache/hadoop/raw/4530f4500d308c9cefbcc5990769c04bd061ad87/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "9eff3a9213e3156aaa56d9c58730ac6ce1c633eb",
                "status": "modified"
            }
        ],
        "message": "YARN-9507. Fix NPE in NodeManager#serviceStop on startup failure. Contributed by Bilwa S T.",
        "parent": "https://github.com/apache/hadoop/commit/21852494815e7314e0873c3963a54457ac2aab28",
        "patched_files": [
            "NodeManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_45a8e8c": {
        "bug_id": "hadoop_45a8e8c",
        "commit": "https://github.com/apache/hadoop/commit/45a8e8c5a46535287de97fd6609c0743eef888ee",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -74,3 +74,5 @@ Release 0.23.3 - Unreleased\n     YARN-63. RMNodeImpl is missing valid transitions from the UNHEALTHY state\n     (Jason Lowe via bobby)\n \n+    YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and\n+    thus causes all containers to be rejected. (vinodkv)",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/CHANGES.txt",
                "sha": "8284215a98abec90778697f810fabb528d00e125",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "patch": "@@ -18,11 +18,14 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords;\n \n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n \n-\n public interface NodeHeartbeatRequest {\n-  public abstract NodeStatus getNodeStatus();\n-  \n-  public abstract void setNodeStatus(NodeStatus status);\n+\n+  NodeStatus getNodeStatus();\n+  void setNodeStatus(NodeStatus status);\n+\n+  MasterKey getLastKnownMasterKey();\n+  void setLastKnownMasterKey(MasterKey secretKey);\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "sha": "9e69680d87f6ed2198fd764ac2b9437dc80194fb",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "patch": "@@ -18,24 +18,25 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb;\n \n-\n import org.apache.hadoop.yarn.api.records.ProtoBase;\n+import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProtoOrBuilder;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl;\n import org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl;\n \n-\n-    \n-public class NodeHeartbeatRequestPBImpl extends ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n+public class NodeHeartbeatRequestPBImpl extends\n+    ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n   NodeHeartbeatRequestProto proto = NodeHeartbeatRequestProto.getDefaultInstance();\n   NodeHeartbeatRequestProto.Builder builder = null;\n   boolean viaProto = false;\n   \n   private NodeStatus nodeStatus = null;\n-  \n+  private MasterKey lastKnownMasterKey = null;\n   \n   public NodeHeartbeatRequestPBImpl() {\n     builder = NodeHeartbeatRequestProto.newBuilder();\n@@ -57,6 +58,10 @@ private void mergeLocalToBuilder() {\n     if (this.nodeStatus != null) {\n       builder.setNodeStatus(convertToProtoFormat(this.nodeStatus));\n     }\n+    if (this.lastKnownMasterKey != null) {\n+      builder\n+        .setLastKnownMasterKey(convertToProtoFormat(this.lastKnownMasterKey));\n+    }\n   }\n \n   private void mergeLocalToProto() {\n@@ -96,6 +101,27 @@ public void setNodeStatus(NodeStatus nodeStatus) {\n     this.nodeStatus = nodeStatus;\n   }\n \n+  @Override\n+  public MasterKey getLastKnownMasterKey() {\n+    NodeHeartbeatRequestProtoOrBuilder p = viaProto ? proto : builder;\n+    if (this.lastKnownMasterKey != null) {\n+      return this.lastKnownMasterKey;\n+    }\n+    if (!p.hasLastKnownMasterKey()) {\n+      return null;\n+    }\n+    this.lastKnownMasterKey = convertFromProtoFormat(p.getLastKnownMasterKey());\n+    return this.lastKnownMasterKey;\n+  }\n+\n+  @Override\n+  public void setLastKnownMasterKey(MasterKey masterKey) {\n+    maybeInitBuilder();\n+    if (masterKey == null) \n+      builder.clearLastKnownMasterKey();\n+    this.lastKnownMasterKey = masterKey;\n+  }\n+\n   private NodeStatusPBImpl convertFromProtoFormat(NodeStatusProto p) {\n     return new NodeStatusPBImpl(p);\n   }\n@@ -104,6 +130,11 @@ private NodeStatusProto convertToProtoFormat(NodeStatus t) {\n     return ((NodeStatusPBImpl)t).getProto();\n   }\n \n+  private MasterKeyPBImpl convertFromProtoFormat(MasterKeyProto p) {\n+    return new MasterKeyPBImpl(p);\n+  }\n \n-\n+  private MasterKeyProto convertToProtoFormat(MasterKey t) {\n+    return ((MasterKeyPBImpl)t).getProto();\n+  }\n }  ",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "sha": "8fcf7f2c147a901117435dfd09262655f2bc75fc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "patch": "@@ -35,6 +35,7 @@ message RegisterNodeManagerResponseProto {\n \n message NodeHeartbeatRequestProto {\n   optional NodeStatusProto node_status = 1;\n+  optional MasterKeyProto last_known_master_key = 2;\n }\n \n message NodeHeartbeatResponseProto {",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "sha": "e4d82c75d61e56c087f51bd90a52da149de00398",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.avro.AvroRuntimeException;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.YarnException;\n@@ -111,10 +112,7 @@ public synchronized void init(Configuration conf) {\n     this.totalResource = recordFactory.newRecordInstance(Resource.class);\n     this.totalResource.setMemory(memoryMb);\n     metrics.addResource(totalResource);\n-    this.tokenKeepAliveEnabled =\n-        conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n-            YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n-            && isSecurityEnabled();\n+    this.tokenKeepAliveEnabled = isTokenKeepAliveEnabled(conf);\n     this.tokenRemovalDelayMs =\n         conf.getInt(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n             YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n@@ -163,10 +161,17 @@ synchronized boolean hasToRebootNode() {\n     return this.hasToRebootNode;\n   }\n \n-  protected boolean isSecurityEnabled() {\n+  private boolean isSecurityEnabled() {\n     return UserGroupInformation.isSecurityEnabled();\n   }\n \n+  @Private\n+  protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n+    return conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n+        YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n+        && isSecurityEnabled();\n+  }\n+\n   protected ResourceTracker getRMClient() {\n     Configuration conf = getConfig();\n     YarnRPC rpc = YarnRPC.create(conf);\n@@ -321,7 +326,11 @@ public void run() {\n             \n             NodeHeartbeatRequest request = recordFactory\n                 .newRecordInstance(NodeHeartbeatRequest.class);\n-            request.setNodeStatus(nodeStatus);            \n+            request.setNodeStatus(nodeStatus);\n+            if (isSecurityEnabled()) {\n+              request.setLastKnownMasterKey(NodeStatusUpdaterImpl.this.context\n+                .getContainerTokenSecretManager().getCurrentKey());\n+            }\n             HeartbeatResponse response =\n               resourceTracker.nodeHeartbeat(request).getHeartbeatResponse();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "sha": "819e22d2146ae18083ee7ee7609b7678351c8407",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "patch": "@@ -92,7 +92,6 @@ public synchronized void setMasterKey(MasterKey masterKeyRecord) {\n         containerId.getApplicationAttemptId().getApplicationId();\n \n     MasterKeyData masterKeyToUse = null;\n-\n     if (this.previousMasterKey != null\n         && keyId == this.previousMasterKey.getMasterKey().getKeyId()) {\n       // A container-launch has come in with a token generated off the last",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "sha": "bc70f26a07e93e1bbc53e26d7f8fbd938348a99f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "patch": "@@ -261,7 +261,7 @@ protected ResourceTracker getRMClient() {\n     }\n     \n     @Override\n-    protected boolean isSecurityEnabled() {\n+    protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n       return true;\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "sha": "41d171f97c034a2be50743f9d9887121ac3644af",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -159,7 +159,7 @@ public synchronized void init(Configuration conf) {\n     DelegationTokenRenewer tokenRenewer = createDelegationTokenRenewer();\n     addService(tokenRenewer);\n \n-    this.containerTokenSecretManager = new RMContainerTokenSecretManager(conf);\n+    this.containerTokenSecretManager = createContainerTokenSecretManager(conf);\n     \n     this.rmContext =\n         new RMContextImpl(this.store, this.rmDispatcher,\n@@ -231,6 +231,11 @@ public synchronized void init(Configuration conf) {\n     super.init(conf);\n   }\n \n+  protected RMContainerTokenSecretManager createContainerTokenSecretManager(\n+      Configuration conf) {\n+    return new RMContainerTokenSecretManager(conf);\n+  }\n+\n   protected EventHandler<SchedulerEvent> createSchedulerEventDispatcher() {\n     return new SchedulerEventDispatcher(this.scheduler);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "e9e5340b80cbd23cc817c6a9708bf2dc3e82f614",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -169,14 +169,14 @@ public RegisterNodeManagerResponse registerNodeManager(\n       return response;\n     }\n \n-    MasterKey nextMasterKeyForNode = null;\n     if (isSecurityEnabled()) {\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getCurrentKey();\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getCurrentKey();\n       regResponse.setMasterKey(nextMasterKeyForNode);\n     }\n \n     RMNode rmNode = new RMNodeImpl(nodeId, rmContext, host, cmPort, httpPort,\n-        resolve(host), capability, nextMasterKeyForNode);\n+        resolve(host), capability);\n \n     RMNode oldNode = this.rmContext.getRMNodes().putIfAbsent(nodeId, rmNode);\n     if (oldNode == null) {\n@@ -266,17 +266,18 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     latestResponse.addAllApplicationsToCleanup(rmNode.getAppsToCleanup());\n     latestResponse.setNodeAction(NodeAction.NORMAL);\n \n-    MasterKey nextMasterKeyForNode = null;\n-\n     // Check if node's masterKey needs to be updated and if the currentKey has\n     // roller over, send it across\n     if (isSecurityEnabled()) {\n+\n       boolean shouldSendMasterKey = false;\n-      MasterKey nodeKnownMasterKey = rmNode.getCurrentMasterKey();\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getNextKey();\n+\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getNextKey();\n       if (nextMasterKeyForNode != null) {\n         // nextMasterKeyForNode can be null if there is no outstanding key that\n         // is in the activation period.\n+        MasterKey nodeKnownMasterKey = request.getLastKnownMasterKey();\n         if (nodeKnownMasterKey.getKeyId() != nextMasterKeyForNode.getKeyId()) {\n           shouldSendMasterKey = true;\n         }\n@@ -290,8 +291,7 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     this.rmContext.getDispatcher().getEventHandler().handle(\n         new RMNodeStatusEvent(nodeId, remoteNodeStatus.getNodeHealthStatus(),\n             remoteNodeStatus.getContainersStatuses(), \n-            remoteNodeStatus.getKeepAliveApplications(), latestResponse,\n-            nextMasterKeyForNode));\n+            remoteNodeStatus.getKeepAliveApplications(), latestResponse));\n \n     nodeHeartBeatResponse.setHeartbeatResponse(latestResponse);\n     return nodeHeartBeatResponse;",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "ed4a021b0db631102cbc075c9d961020362aea4e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n /**\n  * Node managers information on available resources \n@@ -107,6 +106,4 @@\n   public List<ApplicationId> getAppsToCleanup();\n \n   public HeartbeatResponse getLastHeartBeatResponse();\n-  \n-  public MasterKey getCurrentMasterKey();\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "sha": "aafa3dbdefeaf585d509d3b0e3cc4dd2d56df23b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 19,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType;\n@@ -105,8 +104,6 @@\n   private HeartbeatResponse latestHeartBeatResponse = recordFactory\n       .newRecordInstance(HeartbeatResponse.class);\n   \n-  private MasterKey currentMasterKey;\n-\n   private static final StateMachineFactory<RMNodeImpl,\n                                            NodeState,\n                                            RMNodeEventType,\n@@ -167,8 +164,7 @@\n                              RMNodeEvent> stateMachine;\n \n   public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n-      int cmPort, int httpPort, Node node, Resource capability,\n-      MasterKey masterKey) {\n+      int cmPort, int httpPort, Node node, Resource capability) {\n     this.nodeId = nodeId;\n     this.context = context;\n     this.hostName = hostName;\n@@ -178,7 +174,6 @@ public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n     this.nodeAddress = hostName + \":\" + cmPort;\n     this.httpAddress = hostName + \":\" + httpPort;\n     this.node = node;\n-    this.currentMasterKey = masterKey;\n     this.nodeHealthStatus.setIsNodeHealthy(true);\n     this.nodeHealthStatus.setHealthReport(\"Healthy\");\n     this.nodeHealthStatus.setLastHealthReportTime(System.currentTimeMillis());\n@@ -312,17 +307,6 @@ public HeartbeatResponse getLastHeartBeatResponse() {\n       this.readLock.unlock();\n     }\n   }\n-  \n-  @Override\n-  public MasterKey getCurrentMasterKey() {\n-    this.readLock.lock();\n-    try {\n-      return this.currentMasterKey;\n-    } finally {\n-      this.readLock.unlock();\n-    }\n-  }\n-  \n \n   public void handle(RMNodeEvent event) {\n     LOG.debug(\"Processing \" + event.getNodeId() + \" of type \" + event.getType());\n@@ -500,7 +484,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n \n       NodeHealthStatus remoteNodeHealthStatus = \n           statusEvent.getNodeHealthStatus();\n@@ -582,7 +565,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n       NodeHealthStatus remoteNodeHealthStatus = statusEvent.getNodeHealthStatus();\n       rmNode.setNodeHealthStatus(remoteNodeHealthStatus);\n       if (remoteNodeHealthStatus.getIsNodeHealthy()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "sha": "83833b9bdb3c4ac1e2ea1e9fac616bbfccbc09b0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "patch": "@@ -25,25 +25,22 @@\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n public class RMNodeStatusEvent extends RMNodeEvent {\n \n   private final NodeHealthStatus nodeHealthStatus;\n   private final List<ContainerStatus> containersCollection;\n   private final HeartbeatResponse latestResponse;\n   private final List<ApplicationId> keepAliveAppIds;\n-  private final MasterKey currentMasterKey;\n \n   public RMNodeStatusEvent(NodeId nodeId, NodeHealthStatus nodeHealthStatus,\n       List<ContainerStatus> collection, List<ApplicationId> keepAliveAppIds,\n-      HeartbeatResponse latestResponse, MasterKey currentMasterKey) {\n+      HeartbeatResponse latestResponse) {\n     super(nodeId, RMNodeEventType.STATUS_UPDATE);\n     this.nodeHealthStatus = nodeHealthStatus;\n     this.containersCollection = collection;\n     this.keepAliveAppIds = keepAliveAppIds;\n     this.latestResponse = latestResponse;\n-    this.currentMasterKey = currentMasterKey;\n   }\n \n   public NodeHealthStatus getNodeHealthStatus() {\n@@ -61,8 +58,4 @@ public HeartbeatResponse getLatestResponse() {\n   public List<ApplicationId> getKeepAliveAppIds() {\n     return this.keepAliveAppIds;\n   }\n-  \n-  public MasterKey getCurrentMasterKey() {\n-    return this.currentMasterKey;\n-  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "sha": "1285c2bed99d979c34b86cd59238b75bceb41aa8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "patch": "@@ -89,14 +89,17 @@ public void stop() {\n    * Creates a new master-key and sets it as the primary.\n    */\n   @Private\n-  protected void rollMasterKey() {\n+  public void rollMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Rolling master-key for container-tokens\");\n       if (this.currentMasterKey == null) { // Setting up for the first time.\n         this.currentMasterKey = createNewMasterKey();\n       } else {\n         this.nextMasterKey = createNewMasterKey();\n+        LOG.info(\"Going to activate master-key with key-id \"\n+            + this.nextMasterKey.getMasterKey().getKeyId() + \" in \"\n+            + this.activationDelay + \"ms\");\n         this.timer.schedule(new NextKeyActivator(), this.activationDelay);\n       }\n     } finally {\n@@ -122,7 +125,7 @@ public MasterKey getNextKey() {\n    * Activate the new master-key\n    */\n   @Private\n-  protected void activateNextMasterKey() {\n+  public void activateNextMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Activating next master key with id: \"",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "sha": "cc4ccd7e1ebd863edaae6f6f488c0341e520e4df",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "patch": "@@ -35,7 +35,9 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n import org.apache.hadoop.yarn.util.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n \n@@ -46,8 +48,9 @@\n   private final int memory;\n   private final ResourceTrackerService resourceTracker;\n   private final int httpPort = 2;\n+  private MasterKey currentMasterKey;\n \n-  MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n+  public MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n     this.memory = memory;\n     this.resourceTracker = resourceTracker;\n     String[] splits = nodeIdStr.split(\":\");\n@@ -72,21 +75,23 @@ public void containerStatus(Container container) throws Exception {\n     nodeHeartbeat(conts, true);\n   }\n \n-  public NodeId registerNode() throws Exception {\n+  public RegistrationResponse registerNode() throws Exception {\n     RegisterNodeManagerRequest req = Records.newRecord(\n         RegisterNodeManagerRequest.class);\n     req.setNodeId(nodeId);\n     req.setHttpPort(httpPort);\n     Resource resource = Records.newRecord(Resource.class);\n     resource.setMemory(memory);\n     req.setResource(resource);\n-    resourceTracker.registerNodeManager(req);\n-    return nodeId;\n+    RegistrationResponse registrationResponse =\n+        resourceTracker.registerNodeManager(req).getRegistrationResponse();\n+    this.currentMasterKey = registrationResponse.getMasterKey();\n+    return registrationResponse;\n   }\n \n-  public HeartbeatResponse nodeHeartbeat(boolean b) throws Exception {\n+  public HeartbeatResponse nodeHeartbeat(boolean isHealthy) throws Exception {\n     return nodeHeartbeat(new HashMap<ApplicationId, List<ContainerStatus>>(),\n-        b, ++responseId);\n+        isHealthy, ++responseId);\n   }\n \n   public HeartbeatResponse nodeHeartbeat(ApplicationAttemptId attemptId,\n@@ -123,7 +128,15 @@ public HeartbeatResponse nodeHeartbeat(Map<ApplicationId,\n     healthStatus.setLastHealthReportTime(1);\n     status.setNodeHealthStatus(healthStatus);\n     req.setNodeStatus(status);\n-    return resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    req.setLastKnownMasterKey(this.currentMasterKey);\n+    HeartbeatResponse heartbeatResponse =\n+        resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    MasterKey masterKeyFromRM = heartbeatResponse.getMasterKey();\n+    this.currentMasterKey =\n+        (masterKeyFromRM != null\n+            && masterKeyFromRM.getKeyId() != this.currentMasterKey.getKeyId()\n+            ? masterKeyFromRM : this.currentMasterKey);\n+    return heartbeatResponse;\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "sha": "ba999bfb2e094b837d3cb83cc890548543858186",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "patch": "@@ -25,12 +25,11 @@\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.NodeState;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n \n import com.google.common.collect.Lists;\n@@ -188,11 +187,6 @@ public NodeState getState() {\n     public HeartbeatResponse getLastHeartBeatResponse() {\n       return null;\n     }\n-\n-    @Override\n-    public MasterKey getCurrentMasterKey() {\n-      return null;\n-    }\n   };\n \n   private static RMNode buildRMNode(int rack, final Resource perNode, NodeState state, String httpAddr) {",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "sha": "0c56a27ada03143e36ffac9c39f469e27807369e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "patch": "@@ -105,7 +105,7 @@ public Void answer(InvocationOnMock invocation) throws Throwable {\n         new TestSchedulerEventDispatcher());\n     \n     NodeId nodeId = BuilderUtils.newNodeId(\"localhost\", 0);\n-    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);\n+    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null);\n \n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "sha": "2b2decccb6bc4d6d2bb0e3f3b6217ef8d23786fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "patch": "@@ -54,6 +54,7 @@\n import org.w3c.dom.Element;\n import org.w3c.dom.NodeList;\n import org.xml.sax.InputSource;\n+\n import com.google.inject.Guice;\n import com.google.inject.Injector;\n import com.google.inject.servlet.GuiceServletContextListener;\n@@ -145,7 +146,7 @@ public void testNodesDefaultWithUnHealthyNode() throws JSONException,\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm3.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm3.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response =\n@@ -360,7 +361,7 @@ public void testNodesQueryHealthyAndState() throws JSONException, Exception {\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm1.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm1.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"cluster\")",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "sha": "084dcffe4cbb20c550e21ca1452c6a0a904decd9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "patch": "@@ -44,6 +44,12 @@\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n   </dependencies>\n \n   <build>",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "sha": "600c647f9f775e49fb4a4e8e1078435449002a38",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.ipc.RPC;\n-import org.apache.hadoop.ipc.RemoteException;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.SecurityUtil;\n@@ -222,7 +221,7 @@ public void testMaliceUser() throws IOException, InterruptedException {\n     Resource modifiedResource = BuilderUtils.newResource(2048);\n     ContainerTokenIdentifier modifiedIdentifier = new ContainerTokenIdentifier(\n         dummyIdentifier.getContainerID(), dummyIdentifier.getNmHostAddress(),\n-        modifiedResource, Long.MAX_VALUE, 0);\n+        modifiedResource, Long.MAX_VALUE, dummyIdentifier.getMasterKeyId());\n     Token<ContainerTokenIdentifier> modifiedToken = new Token<ContainerTokenIdentifier>(\n         modifiedIdentifier.getBytes(), containerToken.getPassword().array(),\n         new Text(containerToken.getKind()), new Text(containerToken\n@@ -250,19 +249,14 @@ public Void run() {\n               + \"it will indicate RPC success\");\n         } catch (Exception e) {\n           Assert.assertEquals(\n-              java.lang.reflect.UndeclaredThrowableException.class\n-                  .getCanonicalName(), e.getClass().getCanonicalName());\n-          Assert.assertEquals(RemoteException.class.getCanonicalName(), e\n-            .getCause().getClass().getCanonicalName());\n-          Assert.assertEquals(\n-            \"org.apache.hadoop.security.token.SecretManager$InvalidToken\",\n-            ((RemoteException) e.getCause()).getClassName());\n+            java.lang.reflect.UndeclaredThrowableException.class\n+              .getCanonicalName(), e.getClass().getCanonicalName());\n           Assert.assertTrue(e\n             .getCause()\n             .getMessage()\n-            .matches(\n-              \"Given Container container_\\\\d*_\\\\d*_\\\\d\\\\d_\\\\d*\"\n-                  + \" seems to have an illegally generated token.\"));\n+            .contains(\n+              \"DIGEST-MD5: digest response format violation. \"\n+                  + \"Mismatched response.\"));\n         }\n         return null;\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "sha": "1c7933ae275e20cc592feac9b26d687c3f2c8724",
                "status": "modified"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "patch": "@@ -0,0 +1,120 @@\n+/**\n+* Licensed to the Apache Software Foundation (ASF) under one\n+* or more contributor license agreements.  See the NOTICE file\n+* distributed with this work for additional information\n+* regarding copyright ownership.  The ASF licenses this file\n+* to you under the Apache License, Version 2.0 (the\n+* \"License\"); you may not use this file except in compliance\n+* with the License.  You may obtain a copy of the License at\n+*\n+*     http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.hadoop.yarn.server;\n+\n+import java.io.IOException;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.event.Dispatcher;\n+import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNM;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n+import org.junit.Test;\n+\n+public class TestRMNMSecretKeys {\n+\n+  @Test\n+  public void testNMUpdation() throws Exception {\n+    YarnConfiguration conf = new YarnConfiguration();\n+    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,\n+      \"kerberos\");\n+    UserGroupInformation.setConfiguration(conf);\n+    // Default rolling and activation intervals are large enough, no need to\n+    // intervene\n+\n+    final DrainDispatcher dispatcher = new DrainDispatcher();\n+    ResourceManager rm = new ResourceManager(null) {\n+      @Override\n+      protected void doSecureLogin() throws IOException {\n+        // Do nothing.\n+      }\n+\n+      @Override\n+      protected Dispatcher createDispatcher() {\n+        return dispatcher;\n+      }\n+    };\n+    rm.init(conf);\n+    rm.start();\n+\n+    MockNM nm = new MockNM(\"host:1234\", 3072, rm.getResourceTrackerService());\n+    RegistrationResponse registrationResponse = nm.registerNode();\n+    MasterKey masterKey = registrationResponse.getMasterKey();\n+    Assert.assertNotNull(\"Registration should cause a key-update!\", masterKey);\n+    dispatcher.await();\n+\n+    HeartbeatResponse response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"First heartbeat after registration shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert\n+      .assertNull(\n+        \"Even second heartbeat after registration shouldn't get any key updates!\",\n+        response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force a roll-over\n+    RMContainerTokenSecretManager secretManager =\n+        rm.getRMContainerTokenSecretManager();\n+    secretManager.rollMasterKey();\n+\n+    // Heartbeats after roll-over and before activation should be fine.\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNotNull(\n+      \"Heartbeats after roll-over and before activation should not err out.\",\n+      response.getMasterKey());\n+    Assert.assertEquals(\n+      \"Roll-over should have incremented the key-id only by one!\",\n+      masterKey.getKeyId() + 1, response.getMasterKey().getKeyId());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Second heartbeat after roll-over shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force activation\n+    secretManager.activateNextMasterKey();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\"Activation shouldn't cause any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Even second heartbeat after activation shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    rm.stop();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "sha": "9b6024ce3c0621706b5ffce30d59fb58947ed517",
                "status": "added"
            }
        ],
        "message": "YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and thus causes all containers to be rejected. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379550 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/38d003a6db26307cd6544e1ca303c5a521299fb4",
        "patched_files": [
            "NodeStatusUpdaterImpl.java",
            "NodeStatusUpdater.java",
            "NodeHeartbeatRequest.java",
            "ResourceTrackerService.java",
            "MockNodes.java",
            "NodeHeartbeatRequestPBImpl.java",
            "RMContainerTokenSecretManager.java",
            "RMNode.java",
            "ResourceManager.java",
            "CHANGES.java",
            "pom.java",
            "yarn_server_common_service_protos.java",
            "RMNodeStatusEvent.java",
            "MockNM.java",
            "NMContainerTokenSecretManager.java",
            "RMNodeImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java",
            "TestContainerManagerSecurity.java",
            "TestNodeStatusUpdater.java",
            "TestResourceTrackerService.java",
            "TestRMNodeTransitions.java",
            "TestRMWebServicesNodes.java",
            "TestRMNMSecretKeys.java"
        ]
    },
    "hadoop_477ed62": {
        "bug_id": "hadoop_477ed62",
        "commit": "https://github.com/apache/hadoop/commit/477ed62b3fe8db4b07d99479f56a2b997933cb01",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=477ed62b3fe8db4b07d99479f56a2b997933cb01",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -443,6 +443,9 @@ Release 2.4.0 - UNRELEASED\n     apps-killed metrics correctly for killed applications. (Varun Vasudev via\n     vinodkv)\n \n+    YARN-1821. NPE on registerNodeManager if the request has containers for \n+    UnmanagedAMs. (kasha)\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/CHANGES.txt",
                "sha": "a222fed796ef219a5784f065c33fd29eaf1ec68e",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=477ed62b3fe8db4b07d99479f56a2b997933cb01",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -210,14 +210,16 @@ public RegisterNodeManagerResponse registerNodeManager(\n             rmContext.getRMApps().get(appAttemptId.getApplicationId());\n         if (rmApp != null) {\n           RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt.getMasterContainer().getId()\n-              .equals(containerStatus.getContainerId())\n-              && containerStatus.getState() == ContainerState.COMPLETE) {\n-            // sending master container finished event.\n-            RMAppAttemptContainerFinishedEvent evt =\n-                new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                    containerStatus);\n-            rmContext.getDispatcher().getEventHandler().handle(evt);\n+          if (rmAppAttempt != null) {\n+            if (rmAppAttempt.getMasterContainer().getId()\n+                .equals(containerStatus.getContainerId())\n+                && containerStatus.getState() == ContainerState.COMPLETE) {\n+              // sending master container finished event.\n+              RMAppAttemptContainerFinishedEvent evt =\n+                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+                      containerStatus);\n+              rmContext.getDispatcher().getEventHandler().handle(evt);\n+            }\n           }\n         } else {\n           LOG.error(\"Received finished container :\"",
                "raw_url": "https://github.com/apache/hadoop/raw/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "8a2c53958cba746d4fb5e7f0bf30e199235cb4bb",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=477ed62b3fe8db4b07d99479f56a2b997933cb01",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -21,6 +21,8 @@\n import java.io.File;\n import java.io.FileOutputStream;\n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n \n@@ -29,7 +31,11 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n@@ -42,6 +48,7 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n@@ -50,6 +57,8 @@\n import org.junit.After;\n import org.junit.Test;\n \n+import static org.junit.Assert.assertEquals;\n+\n public class TestResourceTrackerService {\n \n   private final static File TEMP_DIR = new File(System.getProperty(\n@@ -457,6 +466,28 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @Test\n+  public void testNodeRegistrationWithContainers() throws Exception {\n+    MockRM rm = new MockRM();\n+    rm.init(new YarnConfiguration());\n+    rm.start();\n+    RMApp app = rm.submitApp(1024);\n+\n+    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n+    nm.nodeHeartbeat(true);\n+\n+    // Register node with some container statuses\n+    ContainerStatus status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+\n+    // The following shouldn't throw NPE\n+    nm.registerNode(Collections.singletonList(status));\n+    assertEquals(\"Incorrect number of nodes\", 1,\n+        rm.getRMContext().getRMNodes().size());\n+  }\n+\n   @Test\n   public void testReconnectNode() throws Exception {\n     final DrainDispatcher dispatcher = new DrainDispatcher();",
                "raw_url": "https://github.com/apache/hadoop/raw/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "eed9ecf9fa7e28f66bcb962fc41c2e842d9c7443",
                "status": "modified"
            }
        ],
        "message": "YARN-1821. NPE on registerNodeManager if the request has containers for UnmanagedAMs (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576525 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b027ef8858e6b8ce26635ffbadc2a461c555ff86",
        "patched_files": [
            "ResourceTrackerService.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_47c41e7": {
        "bug_id": "hadoop_47c41e7",
        "commit": "https://github.com/apache/hadoop/commit/47c41e7ac7e6b905a58550f8899f629c1cf8b138",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java?ref=47c41e7ac7e6b905a58550f8899f629c1cf8b138",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "patch": "@@ -539,9 +539,14 @@ public boolean cancel() {\n    */\n   private boolean skipTokenRenewal(Token<?> token)\n       throws IOException {\n+\n     @SuppressWarnings(\"unchecked\")\n-    Text renewer = ((Token<AbstractDelegationTokenIdentifier>)token).\n-        decodeIdentifier().getRenewer();\n+    AbstractDelegationTokenIdentifier identifier =\n+        ((Token<AbstractDelegationTokenIdentifier>) token).decodeIdentifier();\n+    if (identifier == null) {\n+      return false;\n+    }\n+    Text renewer = identifier.getRenewer();\n     return (renewer != null && renewer.toString().equals(\"\"));\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "sha": "fd12f11c78999a7ba9a95e9e164562c13138a457",
                "status": "modified"
            }
        ],
        "message": "YARN-5048. DelegationTokenRenewer#skipTokenRenewal may throw NPE (Jian He via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/6957e4569996734b1b176e04df5a03d000bed5b7",
        "patched_files": [
            "DelegationTokenRenewer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationTokenRenewer.java"
        ]
    },
    "hadoop_47ef869": {
        "bug_id": "hadoop_47ef869",
        "commit": "https://github.com/apache/hadoop/commit/47ef869fa790dd096b576697c4245d2f3a3193fa",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/47ef869fa790dd096b576697c4245d2f3a3193fa/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt?ref=47ef869fa790dd096b576697c4245d2f3a3193fa",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "patch": "@@ -59,7 +59,7 @@\n \n     HDFS-8023. Erasure Coding: retrieve eraure coding schema for a file from\n     NameNode (vinayakumarb)\n-    \n+\n     HDFS-8074. Define a system-wide default EC schema. (Kai Zheng)\n \n     HDFS-8077. Erasure coding: fix bugs in EC zone and symlinks.\n@@ -110,7 +110,7 @@\n \n     HDFS-8216. TestDFSStripedOutputStream should use BlockReaderTestUtil to \n     create BlockReader. (szetszwo via Zhe Zhang)\n-    \n+\n     HDFS-8212. DistributedFileSystem.createErasureCodingZone should pass schema\n     in FileSystemLinkResolver. (szetszwo via Zhe Zhang)\n \n@@ -172,7 +172,7 @@\n \n     HDFS-8324. Add trace info to DFSClient#getErasureCodingZoneInfo(..) (vinayakumarb via \n     umamahesh)\n-    \n+\n     HDFS-7672. Handle write failure for stripping blocks and refactor the\n     existing code in DFSStripedOutputStream and StripedDataStreamer.  (szetszwo)\n \n@@ -235,3 +235,6 @@\n     (Rakesh R via waltersu4549)\n \n     HDFS-8375. Add cellSize as an XAttr to ECZone. ( Vinayakumar B via zhz).\n+\n+    HDFS-8428. Erasure Coding: Fix the NullPointerException when deleting file.\n+    (Yi Liu via zhz).",
                "raw_url": "https://github.com/apache/hadoop/raw/47ef869fa790dd096b576697c4245d2f3a3193fa/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "sha": "48bc9d6919f6ddbb99732538a01303f30e004f14",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/47ef869fa790dd096b576697c4245d2f3a3193fa/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=47ef869fa790dd096b576697c4245d2f3a3193fa",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3396,7 +3396,7 @@ public void processIncrementalBlockReport(final DatanodeID nodeID,\n     for (ReceivedDeletedBlockInfo rdbi : srdb.getBlocks()) {\n       switch (rdbi.getStatus()) {\n       case DELETED_BLOCK:\n-        removeStoredBlock(storageInfo, getStoredBlock(rdbi.getBlock()), node);\n+        removeStoredBlock(storageInfo, rdbi.getBlock(), node);\n         deleted++;\n         break;\n       case RECEIVED_BLOCK:",
                "raw_url": "https://github.com/apache/hadoop/raw/47ef869fa790dd096b576697c4245d2f3a3193fa/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "fc1396580dcce10f8094e02a59e41e55fe999962",
                "status": "modified"
            }
        ],
        "message": "HDFS-8428. Erasure Coding: Fix the NullPointerException when deleting file. Contributed by Yi Liu.",
        "parent": "https://github.com/apache/hadoop/commit/91c81fdc24709b3caf1f6281c8879ffee08db956",
        "patched_files": [
            "BlockManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_47f711e": {
        "bug_id": "hadoop_47f711e",
        "commit": "https://github.com/apache/hadoop/commit/47f711eebca315804c80012eea5f31275ac25518",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=47f711eebca315804c80012eea5f31275ac25518",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -2825,8 +2825,8 @@ public boolean tryCommit(Resource cluster, ResourceCommitRequest r,\n       // proposal might be outdated if AM failover just finished\n       // and proposal queue was not be consumed in time\n       if (app != null && attemptId.equals(app.getApplicationAttemptId())) {\n-        if (app.accept(cluster, request, updatePending)) {\n-          app.apply(cluster, request, updatePending);\n+        if (app.accept(cluster, request, updatePending)\n+            && app.apply(cluster, request, updatePending)) {\n           LOG.info(\"Allocation proposal accepted\");\n           isSuccess = true;\n         } else{",
                "raw_url": "https://github.com/apache/hadoop/raw/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "bf674a83547f365d4a3cac28b94e043bf14246ab",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=47f711eebca315804c80012eea5f31275ac25518",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -489,7 +489,7 @@ public boolean accept(Resource cluster,\n     return accepted;\n   }\n \n-  public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n+  public boolean apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n       FiCaSchedulerNode> request, boolean updatePending) {\n     boolean reReservation = false;\n \n@@ -502,8 +502,16 @@ public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n             allocation = request.getFirstAllocatedOrReservedContainer();\n         SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n             schedulerContainer = allocation.getAllocatedOrReservedContainer();\n-        RMContainer rmContainer = schedulerContainer.getRmContainer();\n \n+        // Required sanity check - AM can call 'allocate' to update resource\n+        // request without locking the scheduler, hence we need to check\n+        if (updatePending &&\n+            getOutstandingAsksCount(schedulerContainer.getSchedulerRequestKey())\n+                <= 0) {\n+          return false;\n+        }\n+\n+        RMContainer rmContainer = schedulerContainer.getRmContainer();\n         reReservation =\n             (!schedulerContainer.isAllocated()) && (rmContainer.getState()\n                 == RMContainerState.RESERVED);\n@@ -545,7 +553,8 @@ public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n                 containerRequest);\n \n             // If this is from a SchedulingRequest, set allocation tags.\n-            if (containerRequest.getSchedulingRequest() != null) {\n+            if (containerRequest != null\n+                && containerRequest.getSchedulingRequest() != null) {\n               ((RMContainerImpl) rmContainer).setAllocationTags(\n                   containerRequest.getSchedulingRequest().getAllocationTags());\n             }\n@@ -598,6 +607,7 @@ public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n     if (!reReservation) {\n       getCSLeafQueue().apply(cluster, request);\n     }\n+    return true;\n   }\n \n   public boolean unreserve(SchedulerRequestKey schedulerKey,",
                "raw_url": "https://github.com/apache/hadoop/raw/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "32b2cad0ddf89a325f3bc195860f26b9b26ff9d0",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java?ref=47f711eebca315804c80012eea5f31275ac25518",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "patch": "@@ -134,6 +134,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent;\n@@ -170,6 +171,8 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Sets;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n \n public class TestCapacityScheduler extends CapacitySchedulerTestBase {\n   private static final Log LOG = LogFactory.getLog(TestCapacityScheduler.class);\n@@ -4857,4 +4860,54 @@ private void waitforNMRegistered(ResourceScheduler scheduler, int nodecount,\n       }\n     }\n   }\n+\n+  @Test (timeout = 60000)\n+  public void testClearRequestsBeforeApplyTheProposal()\n+      throws Exception {\n+    // init RM & NMs & Nodes\n+    final MockRM rm = new MockRM(new CapacitySchedulerConfiguration());\n+    rm.start();\n+    final MockNM nm = rm.registerNode(\"h1:1234\", 200 * GB);\n+\n+    // submit app\n+    final RMApp app = rm.submitApp(200, \"app\", \"user\");\n+    MockRM.launchAndRegisterAM(app, rm, nm);\n+\n+    // spy capacity scheduler to handle CapacityScheduler#apply\n+    final Priority priority = Priority.newInstance(1);\n+    final CapacityScheduler cs = (CapacityScheduler) rm.getResourceScheduler();\n+    final CapacityScheduler spyCs = Mockito.spy(cs);\n+    Mockito.doAnswer(new Answer<Object>() {\n+      public Object answer(InvocationOnMock invocation) throws Exception {\n+        // clear resource request before applying the proposal for container_2\n+        spyCs.allocate(app.getCurrentAppAttempt().getAppAttemptId(),\n+            Arrays.asList(ResourceRequest.newInstance(priority, \"*\",\n+                Resources.createResource(1 * GB), 0)), null,\n+            Collections.<ContainerId>emptyList(), null, null,\n+            NULL_UPDATE_REQUESTS);\n+        // trigger real apply which can raise NPE before YARN-6629\n+        try {\n+          FiCaSchedulerApp schedulerApp = cs.getApplicationAttempt(\n+              app.getCurrentAppAttempt().getAppAttemptId());\n+          schedulerApp.apply((Resource) invocation.getArguments()[0],\n+              (ResourceCommitRequest) invocation.getArguments()[1],\n+              (Boolean) invocation.getArguments()[2]);\n+          // the proposal of removed request should be rejected\n+          Assert.assertEquals(1, schedulerApp.getLiveContainers().size());\n+        } catch (Throwable e) {\n+          Assert.fail();\n+        }\n+        return null;\n+      }\n+    }).when(spyCs).tryCommit(Mockito.any(Resource.class),\n+        Mockito.any(ResourceCommitRequest.class), Mockito.anyBoolean());\n+\n+    // rm allocates container_2 to reproduce the process that can raise NPE\n+    spyCs.allocate(app.getCurrentAppAttempt().getAppAttemptId(),\n+        Arrays.asList(ResourceRequest.newInstance(priority, \"*\",\n+            Resources.createResource(1 * GB), 1)), null,\n+        Collections.<ContainerId>emptyList(), null, null, NULL_UPDATE_REQUESTS);\n+    spyCs.handle(new NodeUpdateSchedulerEvent(\n+        spyCs.getNode(nm.getNodeId()).getRMNode()));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "sha": "1d2aadcf2a84bd7bd6cfc61eeec1fd3b0a3c474e",
                "status": "modified"
            }
        ],
        "message": "YARN-6629. NPE occurred when container allocation proposal is applied but its resource requests are removed before. (Tao Yang via wangda)\n\nChange-Id: I805880f90b3f6798ec96ed8e8e75755f390a9ad5",
        "parent": "https://github.com/apache/hadoop/commit/cdee0a4f840868d8b8acac15e62da2ab337618c7",
        "patched_files": [
            "CapacityScheduler.java",
            "FiCaSchedulerApp.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_48ab08f": {
        "bug_id": "hadoop_48ab08f",
        "commit": "https://github.com/apache/hadoop/commit/48ab08f1c62696e99d7d94c275b68709c499b7bf",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/48ab08f1c62696e99d7d94c275b68709c499b7bf/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=48ab08f1c62696e99d7d94c275b68709c499b7bf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -280,6 +280,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4051. Remove the empty hadoop-mapreduce-project/assembly/all.xml\n     file (Ravi Prakash via bobby)\n \n+    MAPREDUCE-4117. mapred job -status throws NullPointerException (Devaraj K\n+    via bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/48ab08f1c62696e99d7d94c275b68709c499b7bf/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "719669dd848c568c6a16b048950dd24ac595fe44",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/48ab08f1c62696e99d7d94c275b68709c499b7bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java?ref=48ab08f1c62696e99d7d94c275b68709c499b7bf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "patch": "@@ -509,6 +509,11 @@ String getTaskFailureEventString() throws IOException,\n         lastEvent = event;\n       }\n     }\n+    if (lastEvent == null) {\n+      return \"There are no failed tasks for the job. \"\n+          + \"Job is failed due to some other reason and reason \"\n+          + \"can be found in the logs.\";\n+    }\n     String[] taskAttemptID = lastEvent.getTaskAttemptId().toString().split(\"_\", 2);\n     String taskID = taskAttemptID[1].substring(0, taskAttemptID[1].length()-2);\n     return (\" task \" + taskID + \" failed \" +",
                "raw_url": "https://github.com/apache/hadoop/raw/48ab08f1c62696e99d7d94c275b68709c499b7bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "sha": "51bac982285f65639d4d0ec49bd271c569287d40",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/48ab08f1c62696e99d7d94c275b68709c499b7bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java?ref=48ab08f1c62696e99d7d94c275b68709c499b7bf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "patch": "@@ -0,0 +1,53 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.mapreduce;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.JobStatus.State;\n+import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestJob {\n+  @Test\n+  public void testJobToString() throws IOException, InterruptedException {\n+    Cluster cluster = mock(Cluster.class);\n+    ClientProtocol client = mock(ClientProtocol.class);\n+    when(cluster.getClient()).thenReturn(client);\n+    JobID jobid = new JobID(\"1014873536921\", 6);\n+    JobStatus status = new JobStatus(jobid, 0.0f, 0.0f, 0.0f, 0.0f,\n+        State.FAILED, JobPriority.NORMAL, \"root\", \"TestJobToString\",\n+        \"job file\", \"tracking url\");\n+    when(client.getJobStatus(jobid)).thenReturn(status);\n+    when(client.getTaskReports(jobid, TaskType.MAP)).thenReturn(\n+        new TaskReport[0]);\n+    when(client.getTaskReports(jobid, TaskType.REDUCE)).thenReturn(\n+        new TaskReport[0]);\n+    when(client.getTaskCompletionEvents(jobid, 0, 10)).thenReturn(\n+        new TaskCompletionEvent[0]);\n+    Job job = Job.getInstance(cluster, status, new JobConf());\n+    Assert.assertNotNull(job.toString());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/48ab08f1c62696e99d7d94c275b68709c499b7bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "sha": "110acba20808c16bc06af3ede5b25b69c708a84e",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-4117. mapred job -status throws NullPointerException (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311479 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/706394d03992b394e9f907aff2155df493e4ea4e",
        "patched_files": [
            "Job.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJob.java"
        ]
    },
    "hadoop_48f9577": {
        "bug_id": "hadoop_48f9577",
        "commit": "https://github.com/apache/hadoop/commit/48f95779c1b2b631168235303655e7920abc5ae6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/48f95779c1b2b631168235303655e7920abc5ae6/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=48f95779c1b2b631168235303655e7920abc5ae6",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -452,6 +452,9 @@ Release 0.23.3 - UNRELEASED\n \n     MAPREDUCE-4163. consistently set the bind address (Daryn Sharp via bobby)\n \n+    MAPREDUCE-4048. NullPointerException exception while accessing the\n+    Application Master UI (Devaraj K via bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/48f95779c1b2b631168235303655e7920abc5ae6/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c4046c64b6901ef7c971ea909c42d1fa5763ba49",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/48f95779c1b2b631168235303655e7920abc5ae6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java?ref=48f95779c1b2b631168235303655e7920abc5ae6",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java",
                "patch": "@@ -27,6 +27,8 @@\n import javax.servlet.http.HttpServletResponse;\n \n import org.apache.commons.lang.StringUtils;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.JobACL;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n@@ -47,6 +49,8 @@\n  * This class renders the various pages that the web app supports.\n  */\n public class AppController extends Controller implements AMParams {\n+  private static final Log LOG = LogFactory.getLog(AppController.class);\n+  \n   protected final App app;\n   \n   protected AppController(App app, Configuration conf, RequestContext ctx,\n@@ -220,6 +224,8 @@ public void tasks() {\n             toString().toLowerCase(Locale.US));\n         setTitle(join(tt, \" Tasks for \", $(JOB_ID)));\n       } catch (Exception e) {\n+        LOG.error(\"Failed to render tasks page with task type : \"\n+            + $(TASK_TYPE) + \" for job id : \" + $(JOB_ID), e);\n         badRequest(e.getMessage());\n       }\n     }\n@@ -283,6 +289,8 @@ public void attempts() {\n \n         render(attemptsPage());\n       } catch (Exception e) {\n+        LOG.error(\"Failed to render attempts page with task type : \"\n+            + $(TASK_TYPE) + \" for job id : \" + $(JOB_ID), e);\n         badRequest(e.getMessage());\n       }\n     }\n@@ -316,7 +324,8 @@ public void conf() {\n    */\n   void badRequest(String s) {\n     setStatus(HttpServletResponse.SC_BAD_REQUEST);\n-    setTitle(join(\"Bad request: \", s));\n+    String title = \"Bad request: \";\n+    setTitle((s != null) ? join(title, s) : title);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/48f95779c1b2b631168235303655e7920abc5ae6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java",
                "sha": "da537e5bc71adb18ccbfd1016b6e9af40f8b505e",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/hadoop/blob/48f95779c1b2b631168235303655e7920abc5ae6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java?ref=48f95779c1b2b631168235303655e7920abc5ae6",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java",
                "patch": "@@ -0,0 +1,71 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapreduce.v2.app.webapp;\n+\n+import static org.mockito.Matchers.anyString;\n+import static org.mockito.Matchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import org.apache.commons.lang.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.v2.app.AppContext;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.util.Records;\n+import org.apache.hadoop.yarn.webapp.Controller.RequestContext;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class TestAppController {\n+\n+  private AppController appController;\n+  private RequestContext ctx;\n+\n+  @Before\n+  public void setUp() {\n+    AppContext context = mock(AppContext.class);\n+    when(context.getApplicationID()).thenReturn(\n+        Records.newRecord(ApplicationId.class));\n+    App app = new App(context);\n+    Configuration conf = new Configuration();\n+    ctx = mock(RequestContext.class);\n+    appController = new AppController(app, conf, ctx);\n+  }\n+\n+  @Test\n+  public void testBadRequest() {\n+    String message = \"test string\";\n+    appController.badRequest(message);\n+    verifyExpectations(message);\n+  }\n+\n+  @Test\n+  public void testBadRequestWithNullMessage() {\n+    // It should not throw NullPointerException\n+    appController.badRequest(null);\n+    verifyExpectations(StringUtils.EMPTY);\n+  }\n+\n+  private void verifyExpectations(String message) {\n+    verify(ctx).setStatus(400);\n+    verify(ctx).set(\"app.id\", \"application_0_0000\");\n+    verify(ctx).set(eq(\"rm.web\"), anyString());\n+    verify(ctx).set(\"title\", \"Bad request: \" + message);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/48f95779c1b2b631168235303655e7920abc5ae6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java",
                "sha": "4fcb4755736662a8eb47e90fb23944f633913e7d",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-4048. NullPointerException exception while accessing the Application Master UI (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1334013 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/a70587f368a6519fceb0388c14befec4e97e8293",
        "patched_files": [
            "AppController.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAppController.java"
        ]
    },
    "hadoop_4922394": {
        "bug_id": "hadoop_4922394",
        "commit": "https://github.com/apache/hadoop/commit/492239424a3ace9868b6154f44a0f18fa5318235",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=492239424a3ace9868b6154f44a0f18fa5318235",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -129,6 +129,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-3412. RM tests should use MockRM where possible. (kasha)\n \n+    YARN-3425. NPE from RMNodeLabelsManager.serviceStop when \n+    NodeLabelsManager.serviceInit failed. (Bibin A Chundatt via wangda)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/CHANGES.txt",
                "sha": "f5dc39d2c6c163ad1d0fec1574b19c81cdc9815a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java?ref=492239424a3ace9868b6154f44a0f18fa5318235",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "patch": "@@ -258,7 +258,9 @@ protected void serviceStart() throws Exception {\n   // for UT purpose\n   protected void stopDispatcher() {\n     AsyncDispatcher asyncDispatcher = (AsyncDispatcher) dispatcher;\n-    asyncDispatcher.stop();\n+    if (null != asyncDispatcher) {\n+      asyncDispatcher.stop();\n+    }\n   }\n   \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "sha": "fe3816474e615342ed560a87f0a9ffdaea3ceb40",
                "status": "modified"
            }
        ],
        "message": "YARN-3425. NPE from RMNodeLabelsManager.serviceStop when NodeLabelsManager.serviceInit failed. (Bibin A Chundatt via wangda)",
        "parent": "https://github.com/apache/hadoop/commit/2e79f1c2125517586c165a84e99d3c4d38ca0938",
        "patched_files": [
            "CommonNodeLabelsManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCommonNodeLabelsManager.java"
        ]
    },
    "hadoop_493b0b5": {
        "bug_id": "hadoop_493b0b5",
        "commit": "https://github.com/apache/hadoop/commit/493b0b57601afe3f9ce944d18ca09bdd058d2ce4",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/493b0b57601afe3f9ce944d18ca09bdd058d2ce4/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java?ref=493b0b57601afe3f9ce944d18ca09bdd058d2ce4",
                "deletions": 3,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.hadoop.hdds.protocol.proto\n         .StorageContainerDatanodeProtocolProtos.PipelineReportsProto;\n import org.apache.hadoop.hdds.scm.container.ContainerID;\n+import org.apache.hadoop.hdds.scm.net.InnerNode;\n import org.apache.hadoop.hdds.scm.net.NetConstants;\n import org.apache.hadoop.hdds.scm.net.NetworkTopology;\n import org.apache.hadoop.hdds.scm.net.Node;\n@@ -566,9 +567,20 @@ public DatanodeDetails getNode(String address) {\n       node = clusterMap.getNode(location + NetConstants.PATH_SEPARATOR_STR +\n           address);\n     }\n-    LOG.debug(\"Get node for {} return {}\", address, (node == null ?\n-        \"not found\" : node.getNetworkFullPath()));\n-    return node == null ? null : (DatanodeDetails)node;\n+\n+    if (node != null) {\n+      if (node instanceof InnerNode) {\n+        LOG.warn(\"Get node for {} return {}, it's an inner node, \" +\n+            \"not a datanode\", address, node.getNetworkFullPath());\n+      } else {\n+        LOG.debug(\"Get node for {} return {}\", address,\n+            node.getNetworkFullPath());\n+        return (DatanodeDetails)node;\n+      }\n+    } else {\n+      LOG.warn(\"Cannot find node for {}\", address);\n+    }\n+    return null;\n   }\n \n   private String nodeResolve(String hostname) {",
                "raw_url": "https://github.com/apache/hadoop/raw/493b0b57601afe3f9ce944d18ca09bdd058d2ce4/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "sha": "e4e9d35bdf89c9d44273ec4f26f3f5aea08cdd37",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/493b0b57601afe3f9ce944d18ca09bdd058d2ce4/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java?ref=493b0b57601afe3f9ce944d18ca09bdd058d2ce4",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java",
                "patch": "@@ -290,7 +290,12 @@ public ScmInfo getScmInfo() throws IOException {\n       NodeManager nodeManager = scm.getScmNodeManager();\n       Node client = nodeManager.getNode(clientMachine);\n       List<Node> nodeList = new ArrayList();\n-      nodes.stream().forEach(path -> nodeList.add(nodeManager.getNode(path)));\n+      nodes.stream().forEach(path -> {\n+        DatanodeDetails node = nodeManager.getNode(path);\n+        if (node != null) {\n+          nodeList.add(nodeManager.getNode(path));\n+        }\n+      });\n       List<? extends Node> sortedNodeList = scm.getClusterMap()\n           .sortByDistanceCost(client, nodeList, nodes.size());\n       List<DatanodeDetails> ret = new ArrayList<>();",
                "raw_url": "https://github.com/apache/hadoop/raw/493b0b57601afe3f9ce944d18ca09bdd058d2ce4/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMBlockProtocolServer.java",
                "sha": "1c76070d3bdb44656d4b7e99f2bc0abcb9c21b87",
                "status": "modified"
            },
            {
                "additions": 153,
                "blob_url": "https://github.com/apache/hadoop/blob/493b0b57601afe3f9ce944d18ca09bdd058d2ce4/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java",
                "changes": 153,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java?ref=493b0b57601afe3f9ce944d18ca09bdd058d2ce4",
                "deletions": 0,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java",
                "patch": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.server;\n+\n+import org.apache.hadoop.hdds.HddsConfigKeys;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos;\n+import org.apache.hadoop.hdds.scm.TestUtils;\n+import org.apache.hadoop.hdds.scm.node.NodeManager;\n+import org.apache.hadoop.ozone.protocolPB\n+    .ScmBlockLocationProtocolServerSideTranslatorPB;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Test class for @{@link SCMBlockProtocolServer}.\n+ * */\n+public class TestSCMBlockProtocolServer {\n+  private OzoneConfiguration config;\n+  private SCMBlockProtocolServer server;\n+  private StorageContainerManager scm;\n+  private NodeManager nodeManager;\n+  private ScmBlockLocationProtocolServerSideTranslatorPB service;\n+  private final int nodeCount = 10;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    config = new OzoneConfiguration();\n+    File dir = GenericTestUtils.getRandomizedTestDir();\n+    config.set(HddsConfigKeys.OZONE_METADATA_DIRS, dir.toString());\n+    SCMConfigurator configurator = new SCMConfigurator();\n+    scm = TestUtils.getScm(config, configurator);\n+    scm.start();\n+    scm.exitSafeMode();\n+    // add nodes to scm node manager\n+    nodeManager = scm.getScmNodeManager();\n+    for (int i = 0; i < nodeCount; i++) {\n+      nodeManager.register(TestUtils.randomDatanodeDetails(), null, null);\n+\n+    }\n+    server = scm.getBlockProtocolServer();\n+    service = new ScmBlockLocationProtocolServerSideTranslatorPB(server);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (scm != null) {\n+      scm.stop();\n+      scm.join();\n+    }\n+  }\n+\n+  @Test\n+  public void testSortDatanodes() throws Exception {\n+    List<String> nodes = new ArrayList();\n+    nodeManager.getAllNodes().stream().forEach(\n+        node -> nodes.add(node.getNetworkName()));\n+\n+    // sort normal datanodes\n+    String client;\n+    client = nodes.get(0);\n+    List<DatanodeDetails> datanodeDetails =\n+        server.sortDatanodes(nodes, client);\n+    System.out.println(\"client = \" + client);\n+    datanodeDetails.stream().forEach(\n+        node -> System.out.println(node.toString()));\n+    Assert.assertTrue(datanodeDetails.size() == nodeCount);\n+\n+    // illegal client 1\n+    client += \"X\";\n+    datanodeDetails = server.sortDatanodes(nodes, client);\n+    System.out.println(\"client = \" + client);\n+    datanodeDetails.stream().forEach(\n+        node -> System.out.println(node.toString()));\n+    Assert.assertTrue(datanodeDetails.size() == nodeCount);\n+    // illegal client 2\n+    client = \"/default-rack\";\n+    datanodeDetails = server.sortDatanodes(nodes, client);\n+    System.out.println(\"client = \" + client);\n+    datanodeDetails.stream().forEach(\n+        node -> System.out.println(node.toString()));\n+    Assert.assertTrue(datanodeDetails.size() == nodeCount);\n+\n+    // illegal nodes to sort 1\n+    nodes.add(\"/default-rack\");\n+    ScmBlockLocationProtocolProtos.SortDatanodesRequestProto request =\n+        ScmBlockLocationProtocolProtos.SortDatanodesRequestProto\n+            .newBuilder()\n+            .addAllNodeNetworkName(nodes)\n+            .setClient(client)\n+            .build();\n+    ScmBlockLocationProtocolProtos.SortDatanodesResponseProto resp =\n+        service.sortDatanodes(request);\n+    Assert.assertTrue(resp.getNodeList().size() == nodeCount);\n+    System.out.println(\"client = \" + client);\n+    resp.getNodeList().stream().forEach(\n+        node -> System.out.println(node.getNetworkName()));\n+\n+    // illegal nodes to sort 2\n+    nodes.remove(\"/default-rack\");\n+    nodes.add(nodes.get(0) + \"X\");\n+    request = ScmBlockLocationProtocolProtos.SortDatanodesRequestProto\n+            .newBuilder()\n+            .addAllNodeNetworkName(nodes)\n+            .setClient(client)\n+            .build();\n+    resp = service.sortDatanodes(request);\n+    Assert.assertTrue(resp.getNodeList().size() == nodeCount);\n+    System.out.println(\"client = \" + client);\n+    resp.getNodeList().stream().forEach(\n+        node -> System.out.println(node.getNetworkName()));\n+\n+    // all illegal nodes\n+    nodes.clear();\n+    nodes.add(\"/default-rack\");\n+    nodes.add(\"/default-rack-1\");\n+    nodes.add(\"/default-rack-2\");\n+    request = ScmBlockLocationProtocolProtos.SortDatanodesRequestProto\n+        .newBuilder()\n+        .addAllNodeNetworkName(nodes)\n+        .setClient(client)\n+        .build();\n+    resp = service.sortDatanodes(request);\n+    System.out.println(\"client = \" + client);\n+    Assert.assertTrue(resp.getNodeList().size() == 0);\n+    resp.getNodeList().stream().forEach(\n+        node -> System.out.println(node.getNetworkName()));\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/493b0b57601afe3f9ce944d18ca09bdd058d2ce4/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/server/TestSCMBlockProtocolServer.java",
                "sha": "ba78f4c9df2a33b80e937482df6bd37746dc8088",
                "status": "added"
            }
        ],
        "message": "HDDS-1787. NPE thrown while trying to find DN closest to client. Contributed by Sammi Chen. (#1094)",
        "parent": "https://github.com/apache/hadoop/commit/c5e3ab5a4d80c2650050d54f01abc7b0c6259619",
        "patched_files": [
            "SCMNodeManager.java",
            "SCMBlockProtocolServer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSCMBlockProtocolServer.java",
            "TestSCMNodeManager.java"
        ]
    },
    "hadoop_49c3889": {
        "bug_id": "hadoop_49c3889",
        "commit": "https://github.com/apache/hadoop/commit/49c38898b0be64fc686d039ed2fb2dea1378df02",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=49c38898b0be64fc686d039ed2fb2dea1378df02",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -90,6 +90,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-2857. ConcurrentModificationException in ContainerLogAppender\n     (Mohammad Kamrul Islam via jlowe)\n \n+    YARN-2816. NM fail to start with NPE during container recovery (Zhihai Xu\n+    via jlowe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/CHANGES.txt",
                "sha": "1f8af5592a75afbc17cf5c8d918a486795e4fd7f",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java?ref=49c38898b0be64fc686d039ed2fb2dea1378df02",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java",
                "patch": "@@ -146,6 +146,8 @@ public boolean isNewlyCreated() {\n       throws IOException {\n     ArrayList<RecoveredContainerState> containers =\n         new ArrayList<RecoveredContainerState>();\n+    ArrayList<ContainerId> containersToRemove =\n+              new ArrayList<ContainerId>();\n     LeveldbIterator iter = null;\n     try {\n       iter = new LeveldbIterator(db);\n@@ -165,7 +167,14 @@ public boolean isNewlyCreated() {\n         ContainerId containerId = ConverterUtils.toContainerId(\n             key.substring(CONTAINERS_KEY_PREFIX.length(), idEndPos));\n         String keyPrefix = key.substring(0, idEndPos+1);\n-        containers.add(loadContainerState(containerId, iter, keyPrefix));\n+        RecoveredContainerState rcs = loadContainerState(containerId,\n+            iter, keyPrefix);\n+        // Don't load container without StartContainerRequest\n+        if (rcs.startRequest != null) {\n+          containers.add(rcs);\n+        } else {\n+          containersToRemove.add(containerId);\n+        }\n       }\n     } catch (DBException e) {\n       throw new IOException(e);\n@@ -175,6 +184,19 @@ public boolean isNewlyCreated() {\n       }\n     }\n \n+    // remove container without StartContainerRequest\n+    for (ContainerId containerId : containersToRemove) {\n+      LOG.warn(\"Remove container \" + containerId +\n+          \" with incomplete records\");\n+      try {\n+        removeContainer(containerId);\n+        // TODO: kill and cleanup the leaked container\n+      } catch (IOException e) {\n+        LOG.error(\"Unable to remove container \" + containerId +\n+            \" in store\", e);\n+      }\n+    }\n+\n     return containers;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java",
                "sha": "9d5468845dac4c5bfad8057db8e8fdc98292cce9",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java?ref=49c38898b0be64fc686d039ed2fb2dea1378df02",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java",
                "patch": "@@ -274,6 +274,13 @@ public void testContainerStorage() throws IOException {\n     assertEquals(containerReq, rcs.getStartRequest());\n     assertTrue(rcs.getDiagnostics().isEmpty());\n \n+    // store a new container record without StartContainerRequest\n+    ContainerId containerId1 = ContainerId.newContainerId(appAttemptId, 6);\n+    stateStore.storeContainerLaunched(containerId1);\n+    recoveredContainers = stateStore.loadContainersState();\n+    // check whether the new container record is discarded\n+    assertEquals(1, recoveredContainers.size());\n+\n     // launch the container, add some diagnostics, and verify recovered\n     StringBuilder diags = new StringBuilder();\n     stateStore.storeContainerLaunched(containerId);",
                "raw_url": "https://github.com/apache/hadoop/raw/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java",
                "sha": "f7f43cc5aa495ee994920eb336ef1b0087e20de2",
                "status": "modified"
            }
        ],
        "message": "YARN-2816. NM fail to start with NPE during container recovery. Contributed by Zhihai Xu",
        "parent": "https://github.com/apache/hadoop/commit/3baaa42945fd432e10c547d3ead57e220f64cd57",
        "patched_files": [
            "CHANGES.java",
            "NMLeveldbStateStoreService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNMLeveldbStateStoreService.java"
        ]
    },
    "hadoop_4a114dd": {
        "bug_id": "hadoop_4a114dd",
        "commit": "https://github.com/apache/hadoop/commit/4a114dd67aae83e5bb2d65470166de954acf36a2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -911,6 +911,9 @@ Release 2.6.0 - UNRELEASED\n \n     YARN-2825. Container leak on NM (Jian He via jlowe)\n \n+    YARN-2819. NPE in ATS Timeline Domains when upgrading from 2.4 to 2.6.\n+    (Zhijie Shen via xgong)\n+\n Release 2.5.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/CHANGES.txt",
                "sha": "d4c882719c68977fab8eced897d50d0af87ced4c",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java",
                "patch": "@@ -792,7 +792,8 @@ private TimelineEntities getEntityByTime(byte[] base,\n    * Put a single entity.  If there is an error, add a TimelinePutError to the\n    * given response.\n    */\n-  private void put(TimelineEntity entity, TimelinePutResponse response) {\n+  private void put(TimelineEntity entity, TimelinePutResponse response,\n+      boolean allowEmptyDomainId) {\n     LockMap.CountingReentrantLock<EntityIdentifier> lock =\n         writeLocks.getLock(new EntityIdentifier(entity.getEntityId(),\n             entity.getEntityType()));\n@@ -867,10 +868,18 @@ private void put(TimelineEntity entity, TimelinePutResponse response) {\n                   new EntityIdentifier(relatedEntityId, relatedEntityType));\n               continue;\n             } else {\n+              // This is the existing entity\n               byte[] domainIdBytes = db.get(createDomainIdKey(\n                   relatedEntityId, relatedEntityType, relatedEntityStartTime));\n-              // This is the existing entity\n-              String domainId = new String(domainIdBytes);\n+              // The timeline data created by the server before 2.6 won't have\n+              // the domain field. We assume this timeline data is in the\n+              // default timeline domain.\n+              String domainId = null;\n+              if (domainIdBytes == null) {\n+                domainId = TimelineDataManager.DEFAULT_DOMAIN_ID;\n+              } else {\n+                domainId = new String(domainIdBytes);\n+              }\n               if (!domainId.equals(entity.getDomainId())) {\n                 // in this case the entity will be put, but the relation will be\n                 // ignored\n@@ -923,12 +932,14 @@ private void put(TimelineEntity entity, TimelinePutResponse response) {\n           entity.getEntityType(), revStartTime);\n       if (entity.getDomainId() == null ||\n           entity.getDomainId().length() == 0) {\n-        TimelinePutError error = new TimelinePutError();\n-        error.setEntityId(entity.getEntityId());\n-        error.setEntityType(entity.getEntityType());\n-        error.setErrorCode(TimelinePutError.NO_DOMAIN);\n-        response.addError(error);\n-        return;\n+        if (!allowEmptyDomainId) {\n+          TimelinePutError error = new TimelinePutError();\n+          error.setEntityId(entity.getEntityId());\n+          error.setEntityType(entity.getEntityType());\n+          error.setErrorCode(TimelinePutError.NO_DOMAIN);\n+          response.addError(error);\n+          return;\n+        }\n       } else {\n         writeBatch.put(key, entity.getDomainId().getBytes());\n         writePrimaryFilterEntries(writeBatch, primaryFilters, key,\n@@ -1011,7 +1022,22 @@ public TimelinePutResponse put(TimelineEntities entities) {\n       deleteLock.readLock().lock();\n       TimelinePutResponse response = new TimelinePutResponse();\n       for (TimelineEntity entity : entities.getEntities()) {\n-        put(entity, response);\n+        put(entity, response, false);\n+      }\n+      return response;\n+    } finally {\n+      deleteLock.readLock().unlock();\n+    }\n+  }\n+\n+  @Private\n+  @VisibleForTesting\n+  public TimelinePutResponse putWithNoDomainId(TimelineEntities entities) {\n+    try {\n+      deleteLock.readLock().lock();\n+      TimelinePutResponse response = new TimelinePutResponse();\n+      for (TimelineEntity entity : entities.getEntities()) {\n+        put(entity, response, true);\n       }\n       return response;\n     } finally {",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java",
                "sha": "c4ea9960ad739fd79391c32702d6b9effcb3e163",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java",
                "patch": "@@ -124,6 +124,7 @@ public TimelineEntities getEntities(\n           entities.getEntities().iterator();\n       while (entitiesItr.hasNext()) {\n         TimelineEntity entity = entitiesItr.next();\n+        addDefaultDomainIdIfAbsent(entity);\n         try {\n           // check ACLs\n           if (!timelineACLsManager.checkAccess(\n@@ -161,6 +162,7 @@ public TimelineEntity getEntity(\n     entity =\n         store.getEntity(entityId, entityType, fields);\n     if (entity != null) {\n+      addDefaultDomainIdIfAbsent(entity);\n       // check ACLs\n       if (!timelineACLsManager.checkAccess(\n           callerUGI, ApplicationAccessType.VIEW_APP, entity)) {\n@@ -203,6 +205,7 @@ public TimelineEvents getEvents(\n               eventsOfOneEntity.getEntityId(),\n               eventsOfOneEntity.getEntityType(),\n               EnumSet.of(Field.PRIMARY_FILTERS));\n+          addDefaultDomainIdIfAbsent(entity);\n           // check ACLs\n           if (!timelineACLsManager.checkAccess(\n               callerUGI, ApplicationAccessType.VIEW_APP, entity)) {\n@@ -254,10 +257,12 @@ public TimelinePutResponse postEntities(\n         existingEntity =\n             store.getEntity(entityID.getId(), entityID.getType(),\n                 EnumSet.of(Field.PRIMARY_FILTERS));\n-        if (existingEntity != null &&\n-            !existingEntity.getDomainId().equals(entity.getDomainId())) {\n-          throw new YarnException(\"The domain of the timeline entity \"\n-            + entityID + \" is not allowed to be changed.\");\n+        if (existingEntity != null) {\n+          addDefaultDomainIdIfAbsent(existingEntity);\n+          if (!existingEntity.getDomainId().equals(entity.getDomainId())) {\n+            throw new YarnException(\"The domain of the timeline entity \"\n+              + entityID + \" is not allowed to be changed.\");\n+          }\n         }\n         if (!timelineACLsManager.checkAccess(\n             callerUGI, ApplicationAccessType.MODIFY_APP, entity)) {\n@@ -355,4 +360,11 @@ public TimelineDomains getDomains(String owner,\n     }\n   }\n \n+  private static void addDefaultDomainIdIfAbsent(TimelineEntity entity) {\n+    // be compatible with the timeline data created before 2.6\n+    if (entity.getDomainId() == null) {\n+      entity.setDomainId(DEFAULT_DOMAIN_ID);\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java",
                "sha": "888c28311579e4300429cfa2eb5e150830c1a424",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.yarn.api.records.timeline.TimelineEntities;\n import org.apache.hadoop.yarn.api.records.timeline.TimelineEntity;\n import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.server.records.Version;\n import org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore;\n@@ -160,12 +161,13 @@ private boolean deleteNextEntity(String entityType, byte[] ts)\n   @Test\n   public void testGetEntityTypes() throws IOException {\n     List<String> entityTypes = ((LeveldbTimelineStore)store).getEntityTypes();\n-    assertEquals(5, entityTypes.size());\n-    assertEquals(entityType1, entityTypes.get(0));\n-    assertEquals(entityType2, entityTypes.get(1));\n-    assertEquals(entityType4, entityTypes.get(2));\n-    assertEquals(entityType5, entityTypes.get(3));\n-    assertEquals(entityType7, entityTypes.get(4));\n+    assertEquals(6, entityTypes.size());\n+    assertEquals(\"OLD_ENTITY_TYPE_1\", entityTypes.get(0));\n+    assertEquals(entityType1, entityTypes.get(1));\n+    assertEquals(entityType2, entityTypes.get(2));\n+    assertEquals(entityType4, entityTypes.get(3));\n+    assertEquals(entityType5, entityTypes.get(4));\n+    assertEquals(entityType7, entityTypes.get(5));\n   }\n \n   @Test\n@@ -196,7 +198,7 @@ public void testDeleteEntities() throws IOException, InterruptedException {\n     ((LeveldbTimelineStore)store).discardOldEntities(-123l);\n     assertEquals(2, getEntities(\"type_1\").size());\n     assertEquals(0, getEntities(\"type_2\").size());\n-    assertEquals(4, ((LeveldbTimelineStore)store).getEntityTypes().size());\n+    assertEquals(5, ((LeveldbTimelineStore)store).getEntityTypes().size());\n \n     ((LeveldbTimelineStore)store).discardOldEntities(123l);\n     assertEquals(0, getEntities(\"type_1\").size());\n@@ -327,4 +329,69 @@ public void testGetDomains() throws IOException {\n     super.testGetDomains();\n   }\n \n+  @Test\n+  public void testRelatingToNonExistingEntity() throws IOException {\n+    TimelineEntity entityToStore = new TimelineEntity();\n+    entityToStore.setEntityType(\"TEST_ENTITY_TYPE_1\");\n+    entityToStore.setEntityId(\"TEST_ENTITY_ID_1\");\n+    entityToStore.setDomainId(TimelineDataManager.DEFAULT_DOMAIN_ID);\n+    entityToStore.addRelatedEntity(\"TEST_ENTITY_TYPE_2\", \"TEST_ENTITY_ID_2\");\n+    TimelineEntities entities = new TimelineEntities();\n+    entities.addEntity(entityToStore);\n+    store.put(entities);\n+    TimelineEntity entityToGet =\n+        store.getEntity(\"TEST_ENTITY_ID_2\", \"TEST_ENTITY_TYPE_2\", null);\n+    Assert.assertNotNull(entityToGet);\n+    Assert.assertEquals(\"DEFAULT\", entityToGet.getDomainId());\n+    Assert.assertEquals(\"TEST_ENTITY_TYPE_1\",\n+        entityToGet.getRelatedEntities().keySet().iterator().next());\n+    Assert.assertEquals(\"TEST_ENTITY_ID_1\",\n+        entityToGet.getRelatedEntities().values().iterator().next()\n+            .iterator().next());\n+  }\n+\n+  @Test\n+  public void testRelatingToOldEntityWithoutDomainId() throws IOException {\n+    // New entity is put in the default domain\n+    TimelineEntity entityToStore = new TimelineEntity();\n+    entityToStore.setEntityType(\"NEW_ENTITY_TYPE_1\");\n+    entityToStore.setEntityId(\"NEW_ENTITY_ID_1\");\n+    entityToStore.setDomainId(TimelineDataManager.DEFAULT_DOMAIN_ID);\n+    entityToStore.addRelatedEntity(\"OLD_ENTITY_TYPE_1\", \"OLD_ENTITY_ID_1\");\n+    TimelineEntities entities = new TimelineEntities();\n+    entities.addEntity(entityToStore);\n+    store.put(entities);\n+\n+    TimelineEntity entityToGet =\n+        store.getEntity(\"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entityToGet);\n+    Assert.assertNull(entityToGet.getDomainId());\n+    Assert.assertEquals(\"NEW_ENTITY_TYPE_1\",\n+        entityToGet.getRelatedEntities().keySet().iterator().next());\n+    Assert.assertEquals(\"NEW_ENTITY_ID_1\",\n+        entityToGet.getRelatedEntities().values().iterator().next()\n+            .iterator().next());\n+\n+    // New entity is not put in the default domain\n+    entityToStore = new TimelineEntity();\n+    entityToStore.setEntityType(\"NEW_ENTITY_TYPE_2\");\n+    entityToStore.setEntityId(\"NEW_ENTITY_ID_2\");\n+    entityToStore.setDomainId(\"NON_DEFAULT\");\n+    entityToStore.addRelatedEntity(\"OLD_ENTITY_TYPE_1\", \"OLD_ENTITY_ID_1\");\n+    entities = new TimelineEntities();\n+    entities.addEntity(entityToStore);\n+    TimelinePutResponse response = store.put(entities);\n+    Assert.assertEquals(1, response.getErrors().size());\n+    Assert.assertEquals(TimelinePutError.FORBIDDEN_RELATION,\n+        response.getErrors().get(0).getErrorCode());\n+    entityToGet =\n+        store.getEntity(\"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entityToGet);\n+    Assert.assertNull(entityToGet.getDomainId());\n+    // Still have one related entity\n+    Assert.assertEquals(1, entityToGet.getRelatedEntities().keySet().size());\n+    Assert.assertEquals(1, entityToGet.getRelatedEntities().values()\n+        .iterator().next().size());\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java",
                "sha": "5ebc96b627b5c8104d78d635a512e2f1257a53a4",
                "status": "modified"
            },
            {
                "additions": 152,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java",
                "patch": "@@ -0,0 +1,152 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.timeline;\n+\n+import java.io.File;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileContext;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelineEntities;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelineEntity;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+\n+public class TestTimelineDataManager extends TimelineStoreTestUtils {\n+\n+  private FileContext fsContext;\n+  private File fsPath;\n+  private TimelineDataManager dataManaer;\n+\n+  @Before\n+  public void setup() throws Exception {\n+    fsPath = new File(\"target\", this.getClass().getSimpleName() +\n+        \"-tmpDir\").getAbsoluteFile();\n+    fsContext = FileContext.getLocalFSFileContext();\n+    fsContext.delete(new Path(fsPath.getAbsolutePath()), true);\n+    Configuration conf = new YarnConfiguration();\n+    conf.set(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH,\n+        fsPath.getAbsolutePath());\n+    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_TTL_ENABLE, false);\n+    store = new LeveldbTimelineStore();\n+    store.init(conf);\n+    store.start();\n+    loadTestEntityData();\n+    loadVerificationEntityData();\n+    loadTestDomainData();\n+\n+    TimelineACLsManager aclsManager = new TimelineACLsManager(conf);\n+    dataManaer = new TimelineDataManager(store, aclsManager);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (store != null) {\n+      store.stop();\n+    }\n+    if (fsContext != null) {\n+      fsContext.delete(new Path(fsPath.getAbsolutePath()), true);\n+    }\n+  }\n+\n+  @Test\n+  public void testGetOldEntityWithOutDomainId() throws Exception {\n+    TimelineEntity entity = dataManaer.getEntity(\n+        \"OLD_ENTITY_TYPE_1\", \"OLD_ENTITY_ID_1\", null,\n+        UserGroupInformation.getCurrentUser());\n+    Assert.assertNotNull(entity);\n+    Assert.assertEquals(\"OLD_ENTITY_ID_1\", entity.getEntityId());\n+    Assert.assertEquals(\"OLD_ENTITY_TYPE_1\", entity.getEntityType());\n+    Assert.assertEquals(\n+        TimelineDataManager.DEFAULT_DOMAIN_ID, entity.getDomainId());\n+  }\n+\n+  @Test\n+  public void testGetOldEntitiesWithOutDomainId() throws Exception {\n+    TimelineEntities entities = dataManaer.getEntities(\n+        \"OLD_ENTITY_TYPE_1\", null, null, null, null, null, null, null, null,\n+        UserGroupInformation.getCurrentUser());\n+    Assert.assertEquals(2, entities.getEntities().size());\n+    Assert.assertEquals(\"OLD_ENTITY_ID_2\",\n+        entities.getEntities().get(0).getEntityId());\n+    Assert.assertEquals(\"OLD_ENTITY_TYPE_1\",\n+        entities.getEntities().get(0).getEntityType());\n+    Assert.assertEquals(TimelineDataManager.DEFAULT_DOMAIN_ID,\n+        entities.getEntities().get(0).getDomainId());\n+    Assert.assertEquals(\"OLD_ENTITY_ID_1\",\n+        entities.getEntities().get(1).getEntityId());\n+    Assert.assertEquals(\"OLD_ENTITY_TYPE_1\",\n+        entities.getEntities().get(1).getEntityType());\n+    Assert.assertEquals(TimelineDataManager.DEFAULT_DOMAIN_ID,\n+        entities.getEntities().get(1).getDomainId());\n+  }\n+\n+  @Test\n+  public void testUpdatingOldEntityWithoutDomainId() throws Exception {\n+    // Set the domain to the default domain when updating\n+    TimelineEntity entity = new TimelineEntity();\n+    entity.setEntityType(\"OLD_ENTITY_TYPE_1\");\n+    entity.setEntityId(\"OLD_ENTITY_ID_1\");\n+    entity.setDomainId(TimelineDataManager.DEFAULT_DOMAIN_ID);\n+    entity.addOtherInfo(\"NEW_OTHER_INFO_KEY\", \"NEW_OTHER_INFO_VALUE\");\n+    TimelineEntities entities = new TimelineEntities();\n+    entities.addEntity(entity);\n+    TimelinePutResponse response = dataManaer.postEntities(\n+        entities, UserGroupInformation.getCurrentUser());\n+    Assert.assertEquals(0, response.getErrors().size());\n+    entity = store.getEntity(\"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entity);\n+    // Even in leveldb, the domain is updated to the default domain Id\n+    Assert.assertEquals(\n+        TimelineDataManager.DEFAULT_DOMAIN_ID, entity.getDomainId());\n+    Assert.assertEquals(1, entity.getOtherInfo().size());\n+    Assert.assertEquals(\"NEW_OTHER_INFO_KEY\",\n+        entity.getOtherInfo().keySet().iterator().next());\n+    Assert.assertEquals(\"NEW_OTHER_INFO_VALUE\",\n+        entity.getOtherInfo().values().iterator().next());\n+    \n+    // Set the domain to the non-default domain when updating\n+    entity = new TimelineEntity();\n+    entity.setEntityType(\"OLD_ENTITY_TYPE_1\");\n+    entity.setEntityId(\"OLD_ENTITY_ID_2\");\n+    entity.setDomainId(\"NON_DEFAULT\");\n+    entity.addOtherInfo(\"NEW_OTHER_INFO_KEY\", \"NEW_OTHER_INFO_VALUE\");\n+    entities = new TimelineEntities();\n+    entities.addEntity(entity);\n+    response = dataManaer.postEntities(\n+        entities, UserGroupInformation.getCurrentUser());\n+    Assert.assertEquals(1, response.getErrors().size());\n+    Assert.assertEquals(TimelinePutResponse.TimelinePutError.ACCESS_DENIED,\n+        response.getErrors().get(0).getErrorCode());\n+    entity = store.getEntity(\"OLD_ENTITY_ID_2\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entity);\n+    // In leveldb, the domain Id is still null\n+    Assert.assertNull(entity.getDomainId());\n+    // Updating is not executed\n+    Assert.assertEquals(0, entity.getOtherInfo().size());\n+  }\n+  \n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java",
                "sha": "f74956735a34b2509ebc81bdf657535c5c1f8bc1",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "patch": "@@ -210,6 +210,18 @@ protected void loadTestEntityData() throws IOException {\n     assertEquals(entityId7, response.getErrors().get(0).getEntityId());\n     assertEquals(TimelinePutError.FORBIDDEN_RELATION,\n         response.getErrors().get(0).getErrorCode());\n+\n+    if (store instanceof LeveldbTimelineStore) {\n+      LeveldbTimelineStore leveldb = (LeveldbTimelineStore) store;\n+      entities.setEntities(Collections.singletonList(createEntity(\n+          \"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", 63l, null, null, null, null,\n+          null)));\n+      leveldb.putWithNoDomainId(entities);\n+      entities.setEntities(Collections.singletonList(createEntity(\n+          \"OLD_ENTITY_ID_2\", \"OLD_ENTITY_TYPE_1\", 64l, null, null, null, null,\n+          null)));\n+      leveldb.putWithNoDomainId(entities);\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "sha": "6f15b9245b8d5c0b0643bbe07729b062f87ab150",
                "status": "modified"
            }
        ],
        "message": "YARN-2819. NPE in ATS Timeline Domains when upgrading from 2.4 to 2.6. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/1e215e8ba2e801eb26f16c307daee756d6b2ca66",
        "patched_files": [
            "LeveldbTimelineStore.java",
            "CHANGES.java",
            "TimelineDataManager.java",
            "TimelineStoreTestUtils.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLeveldbTimelineStore.java",
            "TestTimelineDataManager.java"
        ]
    },
    "hadoop_4a4dd27": {
        "bug_id": "hadoop_4a4dd27",
        "commit": "https://github.com/apache/hadoop/commit/4a4dd27571c44b2374d6a909a88bdd04817b0f11",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4a4dd27571c44b2374d6a909a88bdd04817b0f11/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=4a4dd27571c44b2374d6a909a88bdd04817b0f11",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -984,6 +984,9 @@ Trunk (unreleased changes)\n     HADOOP-6229. Attempt to make a directory under an existing file on\n     LocalFileSystem should throw an Exception. (Boris Shkolnik via tomwhite)\n \n+    HADOOP-6243. Fix a NullPointerException in processing deprecated keys.\n+    (Sreekanth Ramakrishnan via yhemanth)\n+\n Release 0.20.1 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4a4dd27571c44b2374d6a909a88bdd04817b0f11/CHANGES.txt",
                "sha": "75917b6261d9ced18865e1e68582512eeefaa204",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/4a4dd27571c44b2374d6a909a88bdd04817b0f11/src/java/core-default.xml",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/core-default.xml?ref=4a4dd27571c44b2374d6a909a88bdd04817b0f11",
                "deletions": 7,
                "filename": "src/java/core-default.xml",
                "patch": "@@ -485,11 +485,4 @@\n     IP address.\n   </description>\n </property>\n-\n-<property>\n-  <name>hadoop.conf.extra.classes</name>\n-  <value>org.apache.hadoop.mapred.JobConf</value>\n-  <final>true</final>\n-</property>\n-\n </configuration>",
                "raw_url": "https://github.com/apache/hadoop/raw/4a4dd27571c44b2374d6a909a88bdd04817b0f11/src/java/core-default.xml",
                "sha": "9a2ae76ee0d27205fa34ff412dd4e6804a41f782",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/4a4dd27571c44b2374d6a909a88bdd04817b0f11/src/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/conf/Configuration.java?ref=4a4dd27571c44b2374d6a909a88bdd04817b0f11",
                "deletions": 48,
                "filename": "src/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -336,6 +336,12 @@ private String handleDeprecation(String name) {\n     }\n     addDefaultResource(\"core-default.xml\");\n     addDefaultResource(\"core-site.xml\");\n+    //Add code for managing deprecated key mapping\n+    //for example\n+    //addDeprecation(\"oldKey1\",new String[]{\"newkey1\",\"newkey2\"});\n+    //adds deprecation for oldKey1 to two new keys(newkey1, newkey2).\n+    //so get or set of oldKey1 will correctly populate/access values of \n+    //newkey1 and newkey2\n   }\n   \n   private Properties properties;\n@@ -1364,56 +1370,8 @@ private void loadResources(Properties properties,\n       loadResource(properties, resource, quiet);\n     }\n     // process for deprecation.\n-    processDeprecation();\n-  }\n-  \n-  /**\n-   * Flag to ensure that the classes mentioned in the value of the property\n-   * <code>hadoop.conf.extra.classes</code> are loaded only once for\n-   * all instances of <code>Configuration</code>\n-   */\n-  private static AtomicBoolean loadedDeprecation = new AtomicBoolean(false);\n-  \n-  private static final String extraConfKey = \"hadoop.conf.extra.classes\";\n-\n-  /**\n-   * adds all the deprecations to the deprecatedKeyMap and updates the values of\n-   * the appropriate keys\n-   */\n-  private void processDeprecation() {\n-    populateDeprecationMapping();\n     processDeprecatedKeys();\n   }\n-  \n-  /**\n-   * Loads all the classes in mapred and hdfs that extend Configuration and that\n-   * have deprecations to be added into deprecatedKeyMap\n-   */\n-  private synchronized void populateDeprecationMapping() {\n-    if (!loadedDeprecation.get()) {\n-      // load classes from mapred and hdfs which extend Configuration and have \n-      // deprecations added in their static blocks\n-      String classnames = substituteVars(properties.getProperty(extraConfKey));\n-      if (classnames == null) {\n-        return;\n-      }\n-      String[] classes = StringUtils.getStrings(classnames);\n-      for (String className : classes) {\n-        try {\n-          Class.forName(className);\n-        } catch (ClassNotFoundException e) {\n-          LOG.warn(className + \" is not in the classpath\");\n-        }\n-      }\n-      // make deprecatedKeyMap unmodifiable in order to prevent changes to \n-      // it in user's code.\n-      deprecatedKeyMap = Collections.unmodifiableMap(deprecatedKeyMap);\n-      // ensure that deprecation processing is done only once for all \n-      // instances of this object\n-      loadedDeprecation.set(true);\n-    }\n-  }\n-\n   /**\n    * Updates the keys that are replacing the deprecated keys and removes the \n    * deprecated keys from memory.",
                "raw_url": "https://github.com/apache/hadoop/raw/4a4dd27571c44b2374d6a909a88bdd04817b0f11/src/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "d2572595842be861335c1e8b82fd1c34766e15f1",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/4a4dd27571c44b2374d6a909a88bdd04817b0f11/src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java?ref=4a4dd27571c44b2374d6a909a88bdd04817b0f11",
                "deletions": 26,
                "filename": "src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java",
                "patch": "@@ -81,30 +81,27 @@ void appendProperty(String name, String val, boolean isFinal)\n     out.write(\"</property>\\n\");\n   }\n   \n-  static class MyConf extends Configuration {\n-    static {\n-      // add deprecation mappings.\n-      Configuration.addDeprecation(\"old.key1\", new String[]{\"new.key1\"});\n-      Configuration.addDeprecation(\"old.key2\", new String[]{\"new.key2\"});\n-      Configuration.addDeprecation(\"old.key3\", new String[]{\"new.key3\"});\n-      Configuration.addDeprecation(\"old.key4\", new String[]{\"new.key4\"});\n-      Configuration.addDeprecation(\"old.key5\", new String[]{\"new.key5\"});\n-      Configuration.addDeprecation(\"old.key6\", new String[]{\"new.key6\"});\n-      Configuration.addDeprecation(\"old.key7\", new String[]{\"new.key7\"});\n-      Configuration.addDeprecation(\"old.key8\", new String[]{\"new.key8\"});\n-      Configuration.addDeprecation(\"old.key9\", new String[]{\"new.key9\"});\n-      Configuration.addDeprecation(\"old.key10\", new String[]{\"new.key10\"});\n-      Configuration.addDeprecation(\"old.key11\", new String[]{\"new.key11\"});\n-      Configuration.addDeprecation(\"old.key12\", new String[]{\"new.key12\"});\n-      Configuration.addDeprecation(\"old.key13\", new String[]{\"new.key13\"});\n-      Configuration.addDeprecation(\"old.key14\", new String[]{\"new.key14\"});\n-      Configuration.addDeprecation(\"old.key15\", new String[]{\"new.key15\"});\n-      Configuration.addDeprecation(\"old.key16\", new String[]{\"new.key16\"});\n-      Configuration.addDeprecation(\"A\", new String[]{\"B\"});\n-      Configuration.addDeprecation(\"C\", new String[]{\"D\"});\n-      Configuration.addDeprecation(\"E\", new String[]{\"F\"});\n-      Configuration.addDeprecation(\"G\", new String[]{\"H\",\"I\"});\n-    }\n+  private void addDeprecationToConfiguration() {\n+    Configuration.addDeprecation(\"old.key1\", new String[]{\"new.key1\"});\n+    Configuration.addDeprecation(\"old.key2\", new String[]{\"new.key2\"});\n+    Configuration.addDeprecation(\"old.key3\", new String[]{\"new.key3\"});\n+    Configuration.addDeprecation(\"old.key4\", new String[]{\"new.key4\"});\n+    Configuration.addDeprecation(\"old.key5\", new String[]{\"new.key5\"});\n+    Configuration.addDeprecation(\"old.key6\", new String[]{\"new.key6\"});\n+    Configuration.addDeprecation(\"old.key7\", new String[]{\"new.key7\"});\n+    Configuration.addDeprecation(\"old.key8\", new String[]{\"new.key8\"});\n+    Configuration.addDeprecation(\"old.key9\", new String[]{\"new.key9\"});\n+    Configuration.addDeprecation(\"old.key10\", new String[]{\"new.key10\"});\n+    Configuration.addDeprecation(\"old.key11\", new String[]{\"new.key11\"});\n+    Configuration.addDeprecation(\"old.key12\", new String[]{\"new.key12\"});\n+    Configuration.addDeprecation(\"old.key13\", new String[]{\"new.key13\"});\n+    Configuration.addDeprecation(\"old.key14\", new String[]{\"new.key14\"});\n+    Configuration.addDeprecation(\"old.key15\", new String[]{\"new.key15\"});\n+    Configuration.addDeprecation(\"old.key16\", new String[]{\"new.key16\"});\n+    Configuration.addDeprecation(\"A\", new String[]{\"B\"});\n+    Configuration.addDeprecation(\"C\", new String[]{\"D\"});\n+    Configuration.addDeprecation(\"E\", new String[]{\"F\"});\n+    Configuration.addDeprecation(\"G\", new String[]{\"H\",\"I\"});\n   }\n   \n   /**\n@@ -123,8 +120,6 @@ void appendProperty(String name, String val, boolean isFinal)\n   public void testDeprecation() throws IOException {\n     out=new BufferedWriter(new FileWriter(CONFIG));\n     startConfig();\n-    appendProperty(\"hadoop.conf.extra.classes\", MyConf.class.getName()\n-        + \",myconf1\");\n     // load keys with default values. Some of them are set to final to\n     // test the precedence order between deprecation and being final\n     appendProperty(\"new.key1\",\"default.value1\",true);\n@@ -145,6 +140,7 @@ public void testDeprecation() throws IOException {\n     appendProperty(\"new.key16\",\"default.value16\");\n     endConfig();\n     Path fileResource = new Path(CONFIG);\n+    addDeprecationToConfiguration();\n     conf.addResource(fileResource);\n     \n     out=new BufferedWriter(new FileWriter(CONFIG2));",
                "raw_url": "https://github.com/apache/hadoop/raw/4a4dd27571c44b2374d6a909a88bdd04817b0f11/src/test/core/org/apache/hadoop/conf/TestConfigurationDeprecation.java",
                "sha": "a5cb59de7ff7899e28dba89b2aa41503d5b74be7",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6243. Fix a NullPointerException in processing deprecated keys. Contributed by Sreekanth Ramakrishnan.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@812455 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/d09ade4d0b64c1f0f629fe9361d511ed905732c6",
        "patched_files": [
            "CHANGES.java",
            "Configuration.java",
            "core-default.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestConfiguration.java",
            "TestConfigurationDeprecation.java"
        ]
    },
    "hadoop_4a9b3c6": {
        "bug_id": "hadoop_4a9b3c6",
        "commit": "https://github.com/apache/hadoop/commit/4a9b3c693def87579298fb59b7df0b8892a3508e",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt?ref=4a9b3c693def87579298fb59b7df0b8892a3508e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "patch": "@@ -18,3 +18,5 @@ HDFS-3773. TestNNWithQJM fails after HDFS-3741. (atm)\n HDFS-3793. Implement genericized format() in QJM (todd)\n \n HDFS-3795. QJM: validate journal dir at startup (todd)\n+\n+HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "sha": "74e443d74d69e12b59b3ac49b5f769f3a7bd7c3b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java?ref=4a9b3c693def87579298fb59b7df0b8892a3508e",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "patch": "@@ -273,6 +273,11 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n     }\n     \n     FileJournalManager.EditLogFile elf = fjm.getLogFile(startTxId);\n+    if (elf == null) {\n+      throw new IllegalStateException(\"No log file to finalize at \" +\n+          \"transaction ID \" + startTxId);\n+    }\n+\n     if (elf.isInProgress()) {\n       // TODO: this is slow to validate when in non-recovery cases\n       // we already know the length here!\n@@ -281,7 +286,7 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n       elf.validateLog();\n       \n       Preconditions.checkState(elf.getLastTxId() == endTxId,\n-          \"Trying to finalize log %s-%s, but current state of log\" +\n+          \"Trying to finalize log %s-%s, but current state of log \" +\n           \"is %s\", startTxId, endTxId, elf);\n       fjm.finalizeLogSegment(startTxId, endTxId);\n     } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "sha": "cf2c11d885db87a04fb4ddbc59f8db25b34bc1fb",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop/blob/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java?ref=4a9b3c693def87579298fb59b7df0b8892a3508e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "patch": "@@ -155,6 +155,60 @@ public void testJournalLocking() throws Exception {\n     journal2.newEpoch(FAKE_NSINFO, 2);\n   }\n   \n+  /**\n+   * Test finalizing a segment after some batch of edits were missed.\n+   * This should fail, since we validate the log before finalization.\n+   */\n+  @Test\n+  public void testFinalizeWhenEditsAreMissed() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    journal.startLogSegment(makeRI(1), 1);\n+    journal.journal(makeRI(2), 1, 3,\n+        QJMTestUtil.createTxnData(1, 3));\n+    \n+    // Try to finalize up to txn 6, even though we only wrote up to txn 3.\n+    try {\n+      journal.finalizeLogSegment(makeRI(3), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+    \n+    // Check that, even if we re-construct the journal by scanning the\n+    // disk, we don't allow finalizing incorrectly.\n+    journal.close();\n+    journal = new Journal(TEST_LOG_DIR, mockErrorReporter);\n+    \n+    try {\n+      journal.finalizeLogSegment(makeRI(4), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+  }\n+  \n+  /**\n+   * Ensure that finalizing a segment which doesn't exist throws the\n+   * appropriate exception.\n+   */\n+  @Test\n+  public void testFinalizeMissingSegment() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    try {\n+      journal.finalizeLogSegment(makeRI(1), 1000, 1001);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"No log file to finalize at transaction ID 1000\", ise);\n+    }\n+  }\n+  \n+  private static RequestInfo makeRI(int serial) {\n+    return new RequestInfo(JID, 1, serial);\n+  }\n+  \n   @Test\n   public void testNamespaceVerification() throws Exception {\n     journal.newEpoch(FAKE_NSINFO, 1);",
                "raw_url": "https://github.com/apache/hadoop/raw/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "sha": "f9539d9bb61c8db495b076a4b32c7523cd71c5a4",
                "status": "modified"
            }
        ],
        "message": "HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/160bfcd6c2e2bc4a4adfa397f0f716430a0832bb",
        "patched_files": [
            "CHANGES.java",
            "Journal.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJournal.java"
        ]
    },
    "hadoop_4b4e9d7": {
        "bug_id": "hadoop_4b4e9d7",
        "commit": "https://github.com/apache/hadoop/commit/4b4e9d741e3a62e8e38925c5c4033931708a81c0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4b4e9d741e3a62e8e38925c5c4033931708a81c0/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=4b4e9d741e3a62e8e38925c5c4033931708a81c0",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -207,6 +207,9 @@ Trunk (unreleased changes)\n     HADOOP-6549. TestDoAsEffectiveUser should use ip address of the host\n      for superuser ip check(jnp via boryas)\n \n+    HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns\n+    null. (hairong)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4b4e9d741e3a62e8e38925c5c4033931708a81c0/CHANGES.txt",
                "sha": "7354c4cec03c5288df758a25a5ba3d2d7bfb07fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/java/org/apache/hadoop/ipc/RPC.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/ipc/RPC.java?ref=4b4e9d741e3a62e8e38925c5c4033931708a81c0",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/ipc/RPC.java",
                "patch": "@@ -244,8 +244,9 @@ public static Object getProxy(Class protocol, long clientVersion,\n    * @param proxy the proxy to be stopped\n    */\n   public static void stopProxy(Object proxy) {\n-    if (proxy!=null) {\n-      getProxyEngine(proxy).stopProxy(proxy);\n+    RpcEngine rpcEngine;\n+    if (proxy!=null && (rpcEngine = getProxyEngine(proxy)) != null) {\n+      rpcEngine.stopProxy(proxy);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/java/org/apache/hadoop/ipc/RPC.java",
                "sha": "36874c511dcca84e6dfa25fd0967d8e5af2a6cab",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/ipc/TestRPC.java?ref=4b4e9d741e3a62e8e38925c5c4033931708a81c0",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "patch": "@@ -39,6 +39,8 @@\n import org.apache.hadoop.security.authorize.Service;\n import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;\n \n+import static org.mockito.Mockito.*;\n+\n /** Unit tests for RPC. */\n public class TestRPC extends TestCase {\n   private static final String ADDRESS = \"0.0.0.0\";\n@@ -392,6 +394,14 @@ public void testNoPings() throws Exception {\n     conf.setBoolean(\"ipc.client.ping\", false);\n     new TestRPC(\"testnoPings\").testCalls(conf);\n   }\n+\n+  /**\n+   * Test stopping a non-registered proxy\n+   * @throws Exception\n+   */\n+  public void testStopNonRegisteredProxy() throws Exception {\n+    RPC.stopProxy(mock(TestProtocol.class));\n+  }\n   \n   public static void main(String[] args) throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "sha": "0bb3f8dc5c0b3c0820081e6e849e2e1f18e2add8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns null. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@911134 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/3a7841aeb8fe1bd861cc4e959f20fbbcb0172f30",
        "patched_files": [
            "CHANGES.java",
            "RPC.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRPC.java"
        ]
    },
    "hadoop_4c1a128": {
        "bug_id": "hadoop_4c1a128",
        "commit": "https://github.com/apache/hadoop/commit/4c1a1287bc58390900ba1c79818d3ba491c4862c",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/4c1a1287bc58390900ba1c79818d3ba491c4862c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=4c1a1287bc58390900ba1c79818d3ba491c4862c",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -995,8 +995,10 @@ public void run() {\n                 getLocalResourcesTracker(LocalResourceVisibility.APPLICATION, user, applicationId);\n               final String diagnostics = \"Failed to download resource \" +\n                   assoc.getResource() + \" \" + e.getCause();\n-              tracker.handle(new ResourceFailedLocalizationEvent(\n-                  assoc.getResource().getRequest(), diagnostics));\n+              if(tracker != null) {\n+                tracker.handle(new ResourceFailedLocalizationEvent(\n+                    assoc.getResource().getRequest(), diagnostics));\n+              }\n               publicRsrc.handle(new ResourceFailedLocalizationEvent(\n                   assoc.getResource().getRequest(), diagnostics));\n               LOG.error(diagnostics);",
                "raw_url": "https://github.com/apache/hadoop/raw/4c1a1287bc58390900ba1c79818d3ba491c4862c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "b5403e8764b1ab67b6ec64ad7dad47493a146778",
                "status": "modified"
            }
        ],
        "message": "YARN-9968. Public Localizer is exiting in NodeManager due to NullPointerException. Contributed by Tarun Parimi",
        "parent": "https://github.com/apache/hadoop/commit/8afabe41e4175551ae71217a45f7c4276f10b7da",
        "patched_files": [
            "ResourceLocalizationService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_4c9c395": {
        "bug_id": "hadoop_4c9c395",
        "commit": "https://github.com/apache/hadoop/commit/4c9c3955ae6cb5c3b67c8127eb7fa5449a0286d9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4c9c3955ae6cb5c3b67c8127eb7fa5449a0286d9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=4c9c3955ae6cb5c3b67c8127eb7fa5449a0286d9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2074,6 +2074,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9157. [OEV and OIV] : Unnecessary parsing for mandatory arguements if\n     '-h' option is specified as the only option (nijel via vinayakumarb)\n \n+    HDFS-9237. NPE at TestDataNodeVolumeFailureToleration#tearDown.\n+    (Brahma Reddy Battula via ozawa)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4c9c3955ae6cb5c3b67c8127eb7fa5449a0286d9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3edbfba980cbe618279524b449571aefddbeccab",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4c9c3955ae6cb5c3b67c8127eb7fa5449a0286d9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java?ref=4c9c3955ae6cb5c3b67c8127eb7fa5449a0286d9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java",
                "patch": "@@ -76,7 +76,9 @@ public void setUp() throws Exception {\n \n   @After\n   public void tearDown() throws Exception {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/4c9c3955ae6cb5c3b67c8127eb7fa5449a0286d9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java",
                "sha": "32a439d1b4835c46013b451ae3626c887188ee13",
                "status": "modified"
            }
        ],
        "message": "HDFS-9237. NPE at TestDataNodeVolumeFailureToleration#tearDown. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/476a251e5efe5e5850671f924e622b587c262653",
        "patched_files": [
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDataNodeVolumeFailureToleration.java"
        ]
    },
    "hadoop_4cf94aa": {
        "bug_id": "hadoop_4cf94aa",
        "commit": "https://github.com/apache/hadoop/commit/4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -656,6 +656,9 @@ Release 2.5.0 - UNRELEASED\n     HDFS-6552. add DN storage to a BlockInfo will not replace the different\n     storage from same DN. (Amir Langer via Arpit Agarwal)\n \n+    HDFS-6551. Rename with OVERWRITE option may throw NPE when the target\n+    file/directory is a reference INode. (jing9)\n+\n   BREAKDOWN OF HDFS-2006 SUBTASKS AND RELATED JIRAS\n \n     HDFS-6299. Protobuf for XAttr and client-side implementation. (Yi Liu via umamahesh)",
                "raw_url": "https://github.com/apache/hadoop/raw/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "f0a84bd7d224030b1d5c5518bd0a997e33a2152d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -44,7 +44,6 @@\n import org.apache.hadoop.fs.XAttrSetFlag;\n import org.apache.hadoop.fs.permission.AclEntry;\n import org.apache.hadoop.fs.permission.AclStatus;\n-import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n@@ -891,9 +890,10 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n     \n     boolean undoRemoveDst = false;\n     INode removedDst = null;\n+    long removedNum = 0;\n     try {\n       if (dstInode != null) { // dst exists remove it\n-        if (removeLastINode(dstIIP) != -1) {\n+        if ((removedNum = removeLastINode(dstIIP)) != -1) {\n           removedDst = dstIIP.getLastINode();\n           undoRemoveDst = true;\n         }\n@@ -933,13 +933,15 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n         long filesDeleted = -1;\n         if (removedDst != null) {\n           undoRemoveDst = false;\n-          BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n-          List<INode> removedINodes = new ChunkedArrayList<INode>();\n-          filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n-              dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes, true)\n-              .get(Quota.NAMESPACE);\n-          getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n-              removedINodes);\n+          if (removedNum > 0) {\n+            BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n+            List<INode> removedINodes = new ChunkedArrayList<INode>();\n+            filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n+                dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes,\n+                true).get(Quota.NAMESPACE);\n+            getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n+                removedINodes);\n+          }\n         }\n \n         if (snapshottableDirs.size() > 0) {",
                "raw_url": "https://github.com/apache/hadoop/raw/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "dc553ab73b96d1c636cdf5fb23a33ebf4ccfec8c",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop/blob/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java?ref=4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "patch": "@@ -171,8 +171,6 @@ public void testRenameFromSDir2NonSDir() throws Exception {\n   private static boolean existsInDiffReport(List<DiffReportEntry> entries,\n       DiffType type, String relativePath) {\n     for (DiffReportEntry entry : entries) {\n-      System.out.println(\"DiffEntry is:\" + entry.getType() + \"\\\"\"\n-          + new String(entry.getRelativePath()) + \"\\\"\");\n       if ((entry.getType() == type)\n           && ((new String(entry.getRelativePath())).compareTo(relativePath) == 0)) {\n         return true;\n@@ -2374,4 +2372,46 @@ public void testAppendFileAfterRenameInSnapshot() throws Exception {\n     // save namespace and restart\n     restartClusterAndCheckImage(true);\n   }\n+\n+  @Test\n+  public void testRenameWithOverWrite() throws Exception {\n+    final Path root = new Path(\"/\");\n+    final Path foo = new Path(root, \"foo\");\n+    final Path file1InFoo = new Path(foo, \"file1\");\n+    final Path file2InFoo = new Path(foo, \"file2\");\n+    final Path file3InFoo = new Path(foo, \"file3\");\n+    DFSTestUtil.createFile(hdfs, file1InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file2InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file3InFoo, 1L, REPL, SEED);\n+    final Path bar = new Path(root, \"bar\");\n+    hdfs.mkdirs(bar);\n+\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n+    // move file1 from foo to bar\n+    final Path fileInBar = new Path(bar, \"file1\");\n+    hdfs.rename(file1InFoo, fileInBar);\n+    // rename bar to newDir\n+    final Path newDir = new Path(root, \"newDir\");\n+    hdfs.rename(bar, newDir);\n+    // move file2 from foo to newDir\n+    final Path file2InNewDir = new Path(newDir, \"file2\");\n+    hdfs.rename(file2InFoo, file2InNewDir);\n+    // move file3 from foo to newDir and rename it to file1, this will overwrite\n+    // the original file1\n+    final Path file1InNewDir = new Path(newDir, \"file1\");\n+    hdfs.rename(file3InFoo, file1InNewDir, Rename.OVERWRITE);\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n+\n+    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\");\n+    LOG.info(\"DiffList is \\n\\\"\" + report.toString() + \"\\\"\");\n+    List<DiffReportEntry> entries = report.getDiffList();\n+    assertEquals(7, entries.size());\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, \"\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, foo.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, bar.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.CREATE, newDir.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file1\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file2\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file3\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "sha": "c88aaf2410e8b34fd0303b6546aeb9249a2e51b5",
                "status": "modified"
            }
        ],
        "message": "HDFS-6551. Rename with OVERWRITE option may throw NPE when the target file/directory is a reference INode. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603612 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/52d18aa217a308e8343ca8b23b5a2dedda77270f",
        "patched_files": [
            "FSDirectory.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSDirectory.java",
            "TestRenameWithSnapshots.java"
        ]
    },
    "hadoop_4d779e0": {
        "bug_id": "hadoop_4d779e0",
        "commit": "https://github.com/apache/hadoop/commit/4d779e088a30f958c9788366e0e251476cb18410",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=4d779e088a30f958c9788366e0e251476cb18410",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -149,3 +149,5 @@ HDFS-2845. SBN should not allow browsing of the file system via web UI. (Bikas S\n HDFS-2742. HA: observed dataloss in replication stress test. (todd via eli)\n \n HDFS-2870. Fix log level for block debug info in processMisReplicatedBlocks (todd)\n+\n+HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect (Bikas Saha via todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "7a4ef27f19513e6bb421cf8ad4d79f49e29b0397",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=4d779e088a30f958c9788366e0e251476cb18410",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -61,6 +61,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n import com.google.common.base.Joiner;\n import com.google.common.collect.Lists;\n@@ -69,6 +71,8 @@\n \n @InterfaceAudience.Private\n public class DFSUtil {\n+  private static final Log LOG = LogFactory.getLog(DFSUtil.class.getName());\n+  \n   private DFSUtil() { /* Hidden constructor */ }\n   private static final ThreadLocal<Random> RANDOM = new ThreadLocal<Random>() {\n     @Override\n@@ -935,9 +939,10 @@ private static String getNameServiceId(Configuration conf, String addressKey) {\n         try {\n           s = NetUtils.createSocketAddr(addr);\n         } catch (Exception e) {\n+          LOG.warn(\"Exception in creating socket address\", e);\n           continue;\n         }\n-        if (matcher.match(s)) {\n+        if (!s.isUnresolved() && matcher.match(s)) {\n           nameserviceId = nsId;\n           namenodeId = nnId;\n           found++;",
                "raw_url": "https://github.com/apache/hadoop/raw/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "c9ccf9f38c7cafd26872165b4890710acc96be21",
                "status": "modified"
            }
        ],
        "message": "HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect. Contributed by Bikas Saha.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1239356 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/048c416beb42ad27cf0e82b144da1d99e50c62b1",
        "patched_files": [
            "DFSUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop_4db4a4a": {
        "bug_id": "hadoop_4db4a4a",
        "commit": "https://github.com/apache/hadoop/commit/4db4a4a165d3936d59f5e67947c6bfbc9c3270a5",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/4db4a4a165d3936d59f5e67947c6bfbc9c3270a5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/CapacitySchedulerPage.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/CapacitySchedulerPage.java?ref=4db4a4a165d3936d59f5e67947c6bfbc9c3270a5",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/CapacitySchedulerPage.java",
                "patch": "@@ -161,10 +161,11 @@ private void renderQueueCapacityInfo(ResponseInfo ri, String label) {\n           .__(\"Configured Capacity:\",\n               capacities.getConfiguredMinResource().toString())\n           .__(\"Configured Max Capacity:\",\n-              capacities.getConfiguredMaxResource().getResource()\n-                  .equals(Resources.none())\n-                      ? \"unlimited\"\n-                      : capacities.getConfiguredMaxResource().toString())\n+              (capacities.getConfiguredMaxResource() == null\n+                  || capacities.getConfiguredMaxResource().getResource()\n+                      .equals(Resources.none()))\n+                          ? \"unlimited\"\n+                          : capacities.getConfiguredMaxResource().toString())\n           .__(\"Effective Capacity:\",\n               appendPercent(capacities.getEffectiveMinResource().toString(),\n                   capacities.getCapacity() / 100))",
                "raw_url": "https://github.com/apache/hadoop/raw/4db4a4a165d3936d59f5e67947c6bfbc9c3270a5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/CapacitySchedulerPage.java",
                "sha": "4933d34abfbaed26eddee3ccbe86fad7d3303bd7",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop/blob/4db4a4a165d3936d59f5e67947c6bfbc9c3270a5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestAbsoluteResourceConfiguration.java",
                "changes": 116,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestAbsoluteResourceConfiguration.java?ref=4db4a4a165d3936d59f5e67947c6bfbc9c3270a5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestAbsoluteResourceConfiguration.java",
                "patch": "@@ -511,4 +511,120 @@ public void testEffectiveResourceAfterReducingClusterResource()\n \n     rm.stop();\n   }\n+\n+  @Test\n+  public void testEffectiveResourceAfterIncreasinClusterResource()\n+      throws Exception {\n+    // create conf with basic queue configuration.\n+    CapacitySchedulerConfiguration csConf = setupComplexQueueConfiguration(\n+        false);\n+    setupComplexMinMaxResourceConfig(csConf);\n+\n+    csConf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,\n+        ResourceScheduler.class);\n+\n+    @SuppressWarnings(\"resource\")\n+    MockRM rm = new MockRM(csConf);\n+    rm.start();\n+\n+    // Add few nodes\n+    rm.registerNode(\"127.0.0.1:1234\", 125 * GB, 20);\n+    rm.registerNode(\"127.0.0.2:1234\", 125 * GB, 20);\n+\n+    // Get queue object to verify min/max resource configuration.\n+    CapacityScheduler cs = (CapacityScheduler) rm.getResourceScheduler();\n+\n+    ParentQueue qA = (ParentQueue) cs.getQueue(QUEUEA);\n+    Assert.assertNotNull(qA);\n+    Assert.assertEquals(\"Min resource configured for QUEUEA is not correct\",\n+        QUEUE_A_MINRES, qA.queueResourceQuotas.getConfiguredMinResource());\n+    Assert.assertEquals(\"Max resource configured for QUEUEA is not correct\",\n+        QUEUE_A_MAXRES, qA.queueResourceQuotas.getConfiguredMaxResource());\n+    Assert.assertEquals(\"Effective Min resource for QUEUEA is not correct\",\n+        QUEUE_A_MINRES, qA.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEA is not correct\",\n+        QUEUE_A_MAXRES, qA.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    ParentQueue qB = (ParentQueue) cs.getQueue(QUEUEB);\n+    Assert.assertNotNull(qB);\n+    Assert.assertEquals(\"Min resource configured for QUEUEB is not correct\",\n+        QUEUE_B_MINRES, qB.queueResourceQuotas.getConfiguredMinResource());\n+    Assert.assertEquals(\"Max resource configured for QUEUEB is not correct\",\n+        QUEUE_B_MAXRES, qB.queueResourceQuotas.getConfiguredMaxResource());\n+    Assert.assertEquals(\"Effective Min resource for QUEUEB is not correct\",\n+        QUEUE_B_MINRES, qB.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEB is not correct\",\n+        QUEUE_B_MAXRES, qB.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    LeafQueue qC = (LeafQueue) cs.getQueue(QUEUEC);\n+    Assert.assertNotNull(qC);\n+    Assert.assertEquals(\"Min resource configured for QUEUEC is not correct\",\n+        QUEUE_C_MINRES, qC.queueResourceQuotas.getConfiguredMinResource());\n+    Assert.assertEquals(\"Max resource configured for QUEUEC is not correct\",\n+        QUEUE_C_MAXRES, qC.queueResourceQuotas.getConfiguredMaxResource());\n+    Assert.assertEquals(\"Effective Min resource for QUEUEC is not correct\",\n+        QUEUE_C_MINRES, qC.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEC is not correct\",\n+        QUEUE_C_MAXRES, qC.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    LeafQueue qA1 = (LeafQueue) cs.getQueue(QUEUEA1);\n+    Assert.assertEquals(\"Effective Min resource for QUEUEA1 is not correct\",\n+        QUEUE_A1_MINRES, qA1.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEA1 is not correct\",\n+        QUEUE_A_MAXRES, qA1.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    LeafQueue qA2 = (LeafQueue) cs.getQueue(QUEUEA2);\n+    Assert.assertEquals(\"Effective Min resource for QUEUEA2 is not correct\",\n+        QUEUE_A2_MINRES, qA2.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEA2 is not correct\",\n+        QUEUE_A_MAXRES, qA2.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    LeafQueue qB1 = (LeafQueue) cs.getQueue(QUEUEB1);\n+    Assert.assertEquals(\"Min resource configured for QUEUEB1 is not correct\",\n+        QUEUE_B1_MINRES, qB1.queueResourceQuotas.getConfiguredMinResource());\n+    Assert.assertEquals(\"Max resource configured for QUEUEB1 is not correct\",\n+        QUEUE_B_MAXRES, qB1.queueResourceQuotas.getConfiguredMaxResource());\n+    Assert.assertEquals(\"Effective Min resource for QUEUEB1 is not correct\",\n+        QUEUE_B1_MINRES, qB1.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEB1 is not correct\",\n+        QUEUE_B_MAXRES, qB1.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    // add new NM.\n+    rm.registerNode(\"127.0.0.3:1234\", 125 * GB, 20);\n+\n+    // There will be no change in effective resource when nodes are added.\n+    // Since configured capacity was based on initial node capacity, a\n+    // re configurations is needed to use this added capacity.\n+    Assert.assertEquals(\"Effective Min resource for QUEUEA is not correct\",\n+        QUEUE_A_MINRES, qA.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEA is not correct\",\n+        QUEUE_A_MAXRES, qA.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    Assert.assertEquals(\"Effective Min resource for QUEUEB is not correct\",\n+        QUEUE_B_MINRES, qB.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEB is not correct\",\n+        QUEUE_B_MAXRES, qB.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    Assert.assertEquals(\"Effective Min resource for QUEUEC is not correct\",\n+        QUEUE_C_MINRES, qC.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEC is not correct\",\n+        QUEUE_C_MAXRES, qC.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    Assert.assertEquals(\"Effective Min resource for QUEUEB1 is not correct\",\n+        QUEUE_B1_MINRES, qB1.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEB1 is not correct\",\n+        QUEUE_B_MAXRES, qB1.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    Assert.assertEquals(\"Effective Min resource for QUEUEA1 is not correct\",\n+        QUEUE_A1_MINRES, qA1.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEA1 is not correct\",\n+        QUEUE_A_MAXRES, qA1.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    Assert.assertEquals(\"Effective Min resource for QUEUEA2 is not correct\",\n+        QUEUE_A2_MINRES, qA2.queueResourceQuotas.getEffectiveMinResource());\n+    Assert.assertEquals(\"Effective Max resource for QUEUEA2 is not correct\",\n+        QUEUE_A_MAXRES, qA2.queueResourceQuotas.getEffectiveMaxResource());\n+\n+    rm.stop();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4db4a4a165d3936d59f5e67947c6bfbc9c3270a5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestAbsoluteResourceConfiguration.java",
                "sha": "5c4a12e7d822984646928588128c7f1f523a0b60",
                "status": "modified"
            }
        ],
        "message": "YARN-7575. NPE in scheduler UI when max-capacity is not configured. Contributed by Sunil G.",
        "parent": "https://github.com/apache/hadoop/commit/daa1cdd062657a47acbf4b23f895860296241199",
        "patched_files": [
            "CapacitySchedulerPage.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAbsoluteResourceConfiguration.java"
        ]
    },
    "hadoop_4ddc5ca": {
        "bug_id": "hadoop_4ddc5ca",
        "commit": "https://github.com/apache/hadoop/commit/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -407,6 +407,9 @@ Release 2.7.0 - UNRELEASED\n     HDFS-7366. BlockInfo should take replication as an short in the constructor.\n     (Li Lu via wheat9)\n \n+    HDFS-7383. DataNode.requestShortCircuitFdsForRead may throw \n+    NullPointerException. (szetszwo via suresh)\n+\n Release 2.6.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "af183793124aeeadca86a7301e2da4c2e975dbcc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -1543,7 +1543,7 @@ public ShortCircuitFdsVersionException(String msg) {\n     \n     try {\n       fis[0] = (FileInputStream)data.getBlockInputStream(blk, 0);\n-      fis[1] = (FileInputStream)data.getMetaDataInputStream(blk).getWrappedStream();\n+      fis[1] = DatanodeUtil.getMetaDataInputStream(blk, data);\n     } catch (ClassCastException e) {\n       LOG.debug(\"requestShortCircuitFdsForRead failed\", e);\n       throw new ShortCircuitFdsUnsupportedException(\"This DataNode's \" +",
                "raw_url": "https://github.com/apache/hadoop/raw/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "adfbaf38a937f58fb29a3f25f72062189ecdf0cc",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hadoop/blob/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java?ref=4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java",
                "patch": "@@ -18,10 +18,15 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n \n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hdfs.protocol.Block;\n+import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi;\n+import org.apache.hadoop.hdfs.server.datanode.fsdataset.LengthInputStream;\n \n /** Provide utility methods for Datanode. */\n @InterfaceAudience.Private\n@@ -114,4 +119,20 @@ public static File idToBlockDir(File root, long blockId) {\n         DataStorage.BLOCK_SUBDIR_PREFIX + d2;\n     return new File(root, path);\n   }\n+\n+  /**\n+   * @return the FileInputStream for the meta data of the given block.\n+   * @throws FileNotFoundException\n+   *           if the file not found.\n+   * @throws ClassCastException\n+   *           if the underlying input stream is not a FileInputStream.\n+   */\n+  public static FileInputStream getMetaDataInputStream(\n+      ExtendedBlock b, FsDatasetSpi<?> data) throws IOException {\n+    final LengthInputStream lin = data.getMetaDataInputStream(b);\n+    if (lin == null) {\n+      throw new FileNotFoundException(\"Meta file for \" + b + \" not found.\");\n+    }\n+    return (FileInputStream)lin.getWrappedStream();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DatanodeUtil.java",
                "sha": "746c3f6948f2774502f9325c17983ed57356dee3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java?ref=4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+import org.apache.hadoop.hdfs.server.datanode.DatanodeUtil;\n import org.apache.hadoop.io.nativeio.NativeIO;\n import org.apache.hadoop.util.Time;\n import org.slf4j.Logger;\n@@ -373,8 +374,7 @@ public void run() {\n         reservedBytes = true;\n         try {\n           blockIn = (FileInputStream)dataset.getBlockInputStream(extBlk, 0);\n-          metaIn = (FileInputStream)dataset.getMetaDataInputStream(extBlk)\n-              .getWrappedStream();\n+          metaIn = DatanodeUtil.getMetaDataInputStream(extBlk, dataset);\n         } catch (ClassCastException e) {\n           LOG.warn(\"Failed to cache \" + key +\n               \": Underlying blocks are not backed by files.\", e);",
                "raw_url": "https://github.com/apache/hadoop/raw/4ddc5cad0a4175f7f5ef9504a7365601dc7e63b4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java",
                "sha": "c6408e6d5ecbaee20d59c55116cd66ea9245c91e",
                "status": "modified"
            }
        ],
        "message": "HDFS-7383. DataNode.requestShortCircuitFdsForRead may throw NullPointerException. Contributed by Tsz Wo Nicholas Sze.",
        "parent": "https://github.com/apache/hadoop/commit/a37a993453c02048a618f71b5b9bc63b5a44dbf6",
        "patched_files": [
            "FsDatasetCache.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetCache.java"
        ]
    },
    "hadoop_50bd067": {
        "bug_id": "hadoop_50bd067",
        "commit": "https://github.com/apache/hadoop/commit/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/CHANGES.txt",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -1138,6 +1138,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-4440. FSAppAttempt#getAllowedLocalityLevelByTime should init the\n     lastScheduler time. (Lin Yiqun via zxu)\n \n+    YARN-4452. NPE when submit Unmanaged application. (Naganarasimha G R\n+    via junping_du)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES\n@@ -1186,6 +1189,9 @@ Release 2.7.3 - UNRELEASED\n \n     YARN-4439. Clarify NMContainerStatus#toString method. (Jian He via xgong)\n \n+    YARN-4452. NPE when submit Unmanaged application. (Naganarasimha G R via\n+    junping_du)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES\n@@ -2040,6 +2046,9 @@ Release 2.6.4 - UNRELEASED\n   YARN-3535. Scheduler must re-request container resources when RMContainer transitions\n   from ALLOCATED to KILLED (rohithsharma and peng.zhang via asuresh)\n \n+  YARN-4452. NPE when submit Unmanaged application. (Naganarasimha G R\n+  via junping_du)\n+\n Release 2.6.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/CHANGES.txt",
                "sha": "9b2397fdd95293f4847a61fbc7277b660ccf6033",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -160,14 +160,16 @@ public void appACLsUpdated(RMApp app, String appViewACLs,\n   public void appAttemptRegistered(RMAppAttempt appAttempt,\n       long registeredTime) {\n     if (publishSystemMetrics) {\n+      ContainerId container = (appAttempt.getMasterContainer() == null) ? null\n+          : appAttempt.getMasterContainer().getId();\n       dispatcher.getEventHandler().handle(\n           new AppAttemptRegisteredEvent(\n               appAttempt.getAppAttemptId(),\n               appAttempt.getHost(),\n               appAttempt.getRpcPort(),\n               appAttempt.getTrackingUrl(),\n               appAttempt.getOriginalTrackingUrl(),\n-              appAttempt.getMasterContainer().getId(),\n+              container,\n               registeredTime));\n     }\n   }\n@@ -176,6 +178,8 @@ public void appAttemptRegistered(RMAppAttempt appAttempt,\n   public void appAttemptFinished(RMAppAttempt appAttempt,\n       RMAppAttemptState appAttemtpState, RMApp app, long finishedTime) {\n     if (publishSystemMetrics) {\n+      ContainerId container = (appAttempt.getMasterContainer() == null) ? null\n+          : appAttempt.getMasterContainer().getId();\n       dispatcher.getEventHandler().handle(\n           new AppAttemptFinishedEvent(\n               appAttempt.getAppAttemptId(),\n@@ -187,7 +191,7 @@ public void appAttemptFinished(RMAppAttempt appAttempt,\n               app.getFinalApplicationStatus(),\n               RMServerUtils.createApplicationAttemptState(appAttemtpState),\n               finishedTime,\n-              appAttempt.getMasterContainer().getId()));\n+              container));\n     }\n   }\n \n@@ -390,9 +394,10 @@ private static TimelineEntity createApplicationEntity(\n         event.getHost());\n     eventInfo.put(AppAttemptMetricsConstants.RPC_PORT_EVENT_INFO,\n         event.getRpcPort());\n-    eventInfo.put(\n-        AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n-        event.getMasterContainerId().toString());\n+    if (event.getMasterContainerId() != null) {\n+      eventInfo.put(AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n+          event.getMasterContainerId().toString());\n+    }\n     tEvent.setEventInfo(eventInfo);\n     entity.addEvent(tEvent);\n     putEntity(entity);\n@@ -417,9 +422,10 @@ private void publishAppAttemptFinishedEvent(AppAttemptFinishedEvent event) {\n         event.getFinalApplicationStatus().toString());\n     eventInfo.put(AppAttemptMetricsConstants.STATE_EVENT_INFO,\n         event.getYarnApplicationAttemptState().toString());\n-    eventInfo.put(\n-        AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n-        event.getMasterContainerId().toString());\n+    if (event.getMasterContainerId() != null) {\n+      eventInfo.put(AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n+          event.getMasterContainerId().toString());\n+    }\n     tEvent.setEventInfo(eventInfo);\n     entity.addEvent(tEvent);\n     putEntity(entity);",
                "raw_url": "https://github.com/apache/hadoop/raw/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "f240660da4695bf3cbce6b7fb7cd07fd05f9a67f",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -256,11 +256,31 @@ public void testPublishApplicationMetrics() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 10000)\n+  public void testPublishAppAttemptMetricsForUnmanagedAM() throws Exception {\n+    ApplicationAttemptId appAttemptId =\n+        ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1);\n+    RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId,true);\n+    metricsPublisher.appAttemptRegistered(appAttempt, Integer.MAX_VALUE + 1L);\n+    RMApp app = mock(RMApp.class);\n+    when(app.getFinalApplicationStatus()).thenReturn(FinalApplicationStatus.UNDEFINED);\n+    metricsPublisher.appAttemptFinished(appAttempt, RMAppAttemptState.FINISHED, app,\n+        Integer.MAX_VALUE + 2L);\n+    TimelineEntity entity = null;\n+    do {\n+      entity =\n+          store.getEntity(appAttemptId.toString(),\n+              AppAttemptMetricsConstants.ENTITY_TYPE,\n+              EnumSet.allOf(Field.class));\n+      // ensure two events are both published before leaving the loop\n+    } while (entity == null || entity.getEvents().size() < 2);\n+  }\n+\n   @Test(timeout = 10000)\n   public void testPublishAppAttemptMetrics() throws Exception {\n     ApplicationAttemptId appAttemptId =\n         ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1);\n-    RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId);\n+    RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId, false);\n     metricsPublisher.appAttemptRegistered(appAttempt, Integer.MAX_VALUE + 1L);\n     RMApp app = mock(RMApp.class);\n     when(app.getFinalApplicationStatus()).thenReturn(FinalApplicationStatus.UNDEFINED);\n@@ -435,15 +455,17 @@ private static RMApp createRMApp(ApplicationId appId) {\n   }\n \n   private static RMAppAttempt createRMAppAttempt(\n-      ApplicationAttemptId appAttemptId) {\n+      ApplicationAttemptId appAttemptId, boolean unmanagedAMAttempt) {\n     RMAppAttempt appAttempt = mock(RMAppAttempt.class);\n     when(appAttempt.getAppAttemptId()).thenReturn(appAttemptId);\n     when(appAttempt.getHost()).thenReturn(\"test host\");\n     when(appAttempt.getRpcPort()).thenReturn(-100);\n-    Container container = mock(Container.class);\n-    when(container.getId())\n-        .thenReturn(ContainerId.newContainerId(appAttemptId, 1));\n-    when(appAttempt.getMasterContainer()).thenReturn(container);\n+    if (!unmanagedAMAttempt) {\n+      Container container = mock(Container.class);\n+      when(container.getId())\n+          .thenReturn(ContainerId.newContainerId(appAttemptId, 1));\n+      when(appAttempt.getMasterContainer()).thenReturn(container);\n+    }\n     when(appAttempt.getDiagnostics()).thenReturn(\"test diagnostics info\");\n     when(appAttempt.getTrackingUrl()).thenReturn(\"test tracking url\");\n     when(appAttempt.getOriginalTrackingUrl()).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop/raw/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "f2b02bcfad78b495f075a27b422e0e64178fec37",
                "status": "modified"
            }
        ],
        "message": "YARN-4452. NPE when submit Unmanaged application. Contributed by Naganarasimha G R.",
        "parent": "https://github.com/apache/hadoop/commit/607473e1d047ccd2a2c3804ae94e04f133af9cc2",
        "patched_files": [
            "CHANGES.java",
            "SystemMetricsPublisher.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_50cb277": {
        "bug_id": "hadoop_50cb277",
        "commit": "https://github.com/apache/hadoop/commit/50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -125,6 +125,10 @@ Trunk (unreleased changes)\n \n     MAPREDUCE-2764. Fix renewal of dfs delegation tokens. (Owen via jitendra)\n \n+    HDFS-2439. Fix NullPointerException in webhdfs when opening a non-existing\n+    file or creating a file without specifying the replication parameter.\n+    (szetszwo)\n+\n Release 0.23.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "24232461fb30fc4f20a266142876386fee6acd81",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java?ref=50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
                "patch": "@@ -18,6 +18,7 @@\n \n package org.apache.hadoop.hdfs;\n \n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.net.HttpURLConnection;\n@@ -107,6 +108,8 @@ private InputStream getInputStream() throws IOException {\n           HftpFileSystem.LOG.debug(\"filelength = \" + filelength);\n         }\n         in = connection.getInputStream();\n+      } catch (FileNotFoundException fnfe) {\n+        throw fnfe;\n       } catch (IOException ioe) {\n         HftpFileSystem.throwIOExceptionFromConnection(connection, ioe);\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/ByteRangeInputStream.java",
                "sha": "e2e8bf37c423849219d62495486bf482094b2b81",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java?ref=50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "patch": "@@ -131,7 +131,7 @@ public Response run() throws IOException, URISyntaxException {\n           fullpath, permission.getFsPermission(), \n           overwrite.getValue() ? EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE)\n               : EnumSet.of(CreateFlag.CREATE),\n-          replication.getValue(), blockSize.getValue(conf), null, b), null);\n+          replication.getValue(conf), blockSize.getValue(conf), null, b), null);\n       try {\n         IOUtils.copyBytes(in, out, b);\n       } finally {",
                "raw_url": "https://github.com/apache/hadoop/raw/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "sha": "aaa1fd0173b8ebbc0dfba29caa147022f42b0d79",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java?ref=50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "patch": "@@ -46,6 +46,7 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.ContentSummary;\n import org.apache.hadoop.fs.Options;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n@@ -119,6 +120,9 @@ private static DatanodeInfo chooseDatanode(final NameNode namenode,\n         || op == PostOpParam.Op.APPEND) {\n       final NamenodeProtocols np = namenode.getRpcServer();\n       final HdfsFileStatus status = np.getFileInfo(path);\n+      if (status == null) {\n+        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n+      }\n       final long len = status.getLen();\n       if (op == GetOpParam.Op.OPEN && (openOffset < 0L || openOffset >= len)) {\n         throw new IOException(\"Offset=\" + openOffset + \" out of the range [0, \"\n@@ -238,6 +242,7 @@ public Response run() throws IOException, URISyntaxException {\n         try {\n \n     final String fullpath = path.getAbsolutePath();\n+    final Configuration conf = (Configuration)context.getAttribute(JspHelper.CURRENT_CONF);\n     final NameNode namenode = (NameNode)context.getAttribute(\"name.node\");\n     final NamenodeProtocols np = namenode.getRpcServer();\n \n@@ -259,7 +264,6 @@ public Response run() throws IOException, URISyntaxException {\n     {\n       final EnumSet<Options.Rename> s = renameOptions.getValue();\n       if (s.isEmpty()) {\n-        @SuppressWarnings(\"deprecation\")\n         final boolean b = np.rename(fullpath, dstPath.getValue());\n         final String js = JsonUtil.toJsonString(\"boolean\", b);\n         return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n@@ -271,7 +275,7 @@ public Response run() throws IOException, URISyntaxException {\n     }\n     case SETREPLICATION:\n     {\n-      final boolean b = np.setReplication(fullpath, replication.getValue());\n+      final boolean b = np.setReplication(fullpath, replication.getValue(conf));\n       final String js = JsonUtil.toJsonString(\"boolean\", b);\n       return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "sha": "b5152b4558a3d0c6154884e788ce96cfdca58d9b",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java?ref=50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java",
                "patch": "@@ -17,6 +17,11 @@\n  */\n package org.apache.hadoop.hdfs.web.resources;\n \n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_REPLICATION_DEFAULT;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_REPLICATION_KEY;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n /** Replication parameter. */\n public class ReplicationParam extends ShortParam {\n   /** Parameter name. */\n@@ -46,4 +51,10 @@ public ReplicationParam(final String str) {\n   public String getName() {\n     return NAME;\n   }\n+\n+  /** @return the value or, if it is null, return the default from conf. */\n+  public short getValue(final Configuration conf) {\n+    return getValue() != null? getValue()\n+        : (short)conf.getInt(DFS_REPLICATION_KEY, DFS_REPLICATION_DEFAULT);\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/ReplicationParam.java",
                "sha": "1eee7ee34d43c6c9f98ef03afb2f377d50c35352",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java?ref=50cb2771e924d2d6d9d04e588cb0a94aefb25b70",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.hdfs.web;\n \n import java.io.BufferedReader;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStreamReader;\n import java.net.HttpURLConnection;\n@@ -28,6 +29,7 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileSystemContractBaseTest;\n import org.apache.hadoop.fs.Path;\n@@ -158,4 +160,16 @@ public void testCaseInsensitive() throws IOException {\n     //check if the command successes.\n     assertTrue(fs.getFileStatus(p).isDirectory());\n   }\n+\n+  public void testOpenNonExistFile() throws IOException {\n+    final Path p = new Path(\"/test/testOpenNonExistFile\");\n+    //open it as a file, should get FileNotFoundException \n+    try {\n+      final FSDataInputStream in = fs.open(p);\n+      in.read();\n+      fail();\n+    } catch(FileNotFoundException fnfe) {\n+      WebHdfsFileSystem.LOG.info(\"This is expected.\", fnfe);\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/50cb2771e924d2d6d9d04e588cb0a94aefb25b70/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "sha": "7cc938709b175b3049e0812d086fdf9618b955cd",
                "status": "modified"
            }
        ],
        "message": "HDFS-2439. Fix NullPointerException in webhdfs when opening a non-existing file or creating a file without specifying the replication parameter.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1183554 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b5e21e7e633a06b9ebb3cbe181cdb4837db99c9c",
        "patched_files": [
            "ByteRangeInputStream.java",
            "ReplicationParam.java",
            "NamenodeWebHdfsMethods.java",
            "CHANGES.java",
            "DatanodeWebHdfsMethods.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestWebHdfsFileSystemContract.java",
            "TestByteRangeInputStream.java"
        ]
    },
    "hadoop_52c1f27": {
        "bug_id": "hadoop_52c1f27",
        "commit": "https://github.com/apache/hadoop/commit/52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -1084,6 +1084,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-12386. RetryPolicies.RETRY_FOREVER should be able to specify a \n     retry interval. (Sunil G via wangda)\n \n+    HADOOP-12252. LocalDirAllocator should not throw NPE with empty string\n+    configuration. (Zhihai Xu)\n+\n   OPTIMIZATIONS\n \n     HADOOP-12051. ProtobufRpcEngine.invoke() should use Exception.toString()",
                "raw_url": "https://github.com/apache/hadoop/raw/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "d5ce38b450444de4b3745a3671d05acd591327b7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java?ref=52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "patch": "@@ -250,9 +250,9 @@ int getCurrentDirectoryIndex() {\n     private int dirNumLastAccessed;\n     private Random dirIndexRandomizer = new Random();\n     private FileSystem localFS;\n-    private DF[] dirDF;\n+    private DF[] dirDF = new DF[0];\n     private String contextCfgItemName;\n-    private String[] localDirs;\n+    private String[] localDirs = new String[0];\n     private String savedLocalDirs = \"\";\n \n     public AllocatorPerContext(String contextCfgItemName) {",
                "raw_url": "https://github.com/apache/hadoop/raw/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "sha": "ccea6e5a2bebfa412f17780735801f2f1b168f01",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java?ref=52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.NoSuchElementException;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.util.DiskChecker.DiskErrorException;\n import org.apache.hadoop.util.Shell;\n \n import org.junit.runner.RunWith;\n@@ -312,7 +313,30 @@ public void testShouldNotthrowNPE() throws Exception {\n     } catch (IOException e) {\n       assertEquals(CONTEXT + \" not configured\", e.getMessage());\n     } catch (NullPointerException e) {\n-      fail(\"Lack of configuration should not have thrown an NPE.\");\n+      fail(\"Lack of configuration should not have thrown a NPE.\");\n+    }\n+\n+    String  NEW_CONTEXT = CONTEXT + \".new\";\n+    conf1.set(NEW_CONTEXT, \"\");\n+    LocalDirAllocator newDirAllocator = new LocalDirAllocator(NEW_CONTEXT);\n+    try {\n+      newDirAllocator.getLocalPathForWrite(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + NEW_CONTEXT +\n+          \" is set to empty string\");\n+    } catch (IOException e) {\n+      assertTrue(e instanceof DiskErrorException);\n+    } catch (NullPointerException e) {\n+      fail(\"Wrong configuration should not have thrown a NPE.\");\n+    }\n+\n+    try {\n+      newDirAllocator.getLocalPathToRead(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + NEW_CONTEXT +\n+          \" is set to empty string\");\n+    } catch (IOException e) {\n+      assertTrue(e instanceof DiskErrorException);\n+    } catch (NullPointerException e) {\n+      fail(\"Wrong configuration should not have thrown a NPE.\");\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "sha": "2e311744f6a5433c4b3dc12a712d69222effbd14",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12252. LocalDirAllocator should not throw NPE with empty string configuration. Contributed by Zhihai Xu",
        "parent": "https://github.com/apache/hadoop/commit/df31c446bfa628bee9fab88addcfec5a13edda30",
        "patched_files": [
            "CHANGES.java",
            "LocalDirAllocator.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalDirAllocator.java"
        ]
    },
    "hadoop_5409908": {
        "bug_id": "hadoop_5409908",
        "commit": "https://github.com/apache/hadoop/commit/5409908026dd791cce62a7d71ae56c92a70a9753",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=5409908026dd791cce62a7d71ae56c92a70a9753",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -120,3 +120,6 @@ HDFS-5535 subtasks:\n \n     HDFS-6029. Secondary NN fails to checkpoint after -rollingUpgrade prepare.\n     (jing9)\n+\n+    HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. (Haohui Mai\n+    via jing9)",
                "raw_url": "https://github.com/apache/hadoop/raw/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "002afcd44b67682655424e7e1ddb1daae4b83cab",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java?ref=5409908026dd791cce62a7d71ae56c92a70a9753",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "patch": "@@ -881,9 +881,12 @@ private void loadFSImage(File imageFile, FSNamesystem target,\n    */\n   private void loadFSImage(File curFile, MD5Hash expectedMd5,\n       FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n+    // BlockPoolId is required when the FsImageLoader loads the rolling upgrade\n+    // information. Make sure the ID is properly set.\n+    target.setBlockPoolId(this.getBlockPoolID());\n+\n     FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);\n     loader.load(curFile);\n-    target.setBlockPoolId(this.getBlockPoolID());\n \n     // Check that the image digest we loaded matches up with what\n     // we expected",
                "raw_url": "https://github.com/apache/hadoop/raw/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "sha": "e79b6c9224665352c3ec945cab6ce37ad4b58b39",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java?ref=5409908026dd791cce62a7d71ae56c92a70a9753",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "patch": "@@ -432,6 +432,32 @@ public void testQuery() throws Exception {\n     }\n   }\n \n+  @Test (timeout = 300000)\n+  public void testQueryAfterRestart() throws IOException, InterruptedException {\n+    final Configuration conf = new Configuration();\n+    MiniDFSCluster cluster = null;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+      cluster.waitActive();\n+      DistributedFileSystem dfs = cluster.getFileSystem();\n+\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      // start rolling upgrade\n+      dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);\n+      queryForPreparation(dfs);\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      dfs.saveNamespace();\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+\n+      cluster.restartNameNodes();\n+      dfs.rollingUpgrade(RollingUpgradeAction.QUERY);\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n+\n   @Test(timeout = 300000)\n   public void testCheckpoint() throws IOException, InterruptedException {\n     final Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop/raw/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "sha": "321f8843d0f6f9091f76a43a50ea0d8ff9405171",
                "status": "modified"
            }
        ],
        "message": "HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1572801 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e9a17c8ce0656a4e5d47401ca22a575c5f5f66db",
        "patched_files": [
            "FSImage.java",
            "CHANGES_HDFS-5535.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRollingUpgrade.java",
            "TestFSImage.java"
        ]
    },
    "hadoop_5426653": {
        "bug_id": "hadoop_5426653",
        "commit": "https://github.com/apache/hadoop/commit/54266538192a558c6d80725c25912005090e14c4",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/54266538192a558c6d80725c25912005090e14c4/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java?ref=54266538192a558c6d80725c25912005090e14c4",
                "deletions": 5,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "patch": "@@ -1438,11 +1438,14 @@ public OzoneManagerHttpServer getHttpServer() {\n           .setNodeType(HddsProtos.NodeType.DATANODE)\n           .setHostname(datanode.getHostName());\n \n-      dnServiceInfoBuilder.addServicePort(ServicePort.newBuilder()\n-          .setType(ServicePort.Type.HTTP)\n-          .setValue(DatanodeDetails.getFromProtoBuf(datanode)\n-              .getPort(DatanodeDetails.Port.Name.REST).getValue())\n-          .build());\n+      if(DatanodeDetails.getFromProtoBuf(datanode)\n+          .getPort(DatanodeDetails.Port.Name.REST) != null) {\n+        dnServiceInfoBuilder.addServicePort(ServicePort.newBuilder()\n+            .setType(ServicePort.Type.HTTP)\n+            .setValue(DatanodeDetails.getFromProtoBuf(datanode)\n+                .getPort(DatanodeDetails.Port.Name.REST).getValue())\n+            .build());\n+      }\n \n       services.add(dnServiceInfoBuilder.build());\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/54266538192a558c6d80725c25912005090e14c4/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "sha": "f41eb88c2f508c51397a6da10b94437b996b67e3",
                "status": "modified"
            }
        ],
        "message": "HDDS-908: NPE in TestOzoneRpcClient.\nContributed by Ajay Kumar.",
        "parent": "https://github.com/apache/hadoop/commit/71e0b0d8005ea1952dc7e582db15c2ac09df7c91",
        "patched_files": [
            "OzoneManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOzoneManager.java"
        ]
    },
    "hadoop_5459b24": {
        "bug_id": "hadoop_5459b24",
        "commit": "https://github.com/apache/hadoop/commit/5459b241c86cc9a26fecca9a06ceaf524e48fed4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=5459b241c86cc9a26fecca9a06ceaf524e48fed4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -520,6 +520,8 @@ Release 2.8.0 - UNRELEASED\n     HDFS-7863. Missing description of some methods and parameters in javadoc of\n     FSDirDeleteOp. (Brahma Reddy Battula via ozawa)\n \n+    HDFS-8043. NPE in MiniDFSCluster teardown. (Brahma Reddy Battula via ozawa)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "30a460fb1e27843d3ce92cb47e4cbc94703bfa3b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java?ref=5459b241c86cc9a26fecca9a06ceaf524e48fed4",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java",
                "patch": "@@ -1744,11 +1744,14 @@ public void shutdown(boolean deleteDfsDir, boolean closeFileSystem) {\n         nameNode = null;\n       }\n     }\n-    if (deleteDfsDir) {\n+    if (base_dir != null) {\n+      if (deleteDfsDir) {\n         base_dir.delete();\n-    } else {\n+      } else {\n         base_dir.deleteOnExit();\n+      }\n     }\n+\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java",
                "sha": "d3eaa6ef6c1831a4a6bb6f172fce9007845329b8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java?ref=5459b241c86cc9a26fecca9a06ceaf524e48fed4",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "patch": "@@ -85,7 +85,9 @@ public void setUpCluster() throws IOException {\n   \n   @After\n   public void tearDownCluster() throws IOException {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @After",
                "raw_url": "https://github.com/apache/hadoop/raw/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "sha": "644d66d6ac97d1eb08b1359ed02cad1f7fef6644",
                "status": "modified"
            }
        ],
        "message": "HDFS-8043. NPE in MiniDFSCluster teardown. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/5112477d9e1f1ebc7d91757924c4bdc6eabc35a9",
        "patched_files": [
            "MiniDFSCluster.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSClientFailover.java",
            "TestMiniDFSCluster.java"
        ]
    },
    "hadoop_552237d": {
        "bug_id": "hadoop_552237d",
        "commit": "https://github.com/apache/hadoop/commit/552237d4a34ab10fa5f9ec7aad7942f2a110993e",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/552237d4a34ab10fa5f9ec7aad7942f2a110993e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStorePerf.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStorePerf.java?ref=552237d4a34ab10fa5f9ec7aad7942f2a110993e",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStorePerf.java",
                "patch": "@@ -91,16 +91,19 @@ public void tearDown() throws Exception {\n     if (appTokenMgr != null) {\n       appTokenMgr.stop();\n     }\n-    curatorTestingServer.stop();\n+    if (curatorTestingServer != null) {\n+      curatorTestingServer.stop();\n+    }\n   }\n \n   private void initStore(String hostPort) {\n     Optional<String> optHostPort = Optional.fromNullable(hostPort);\n     RMContext rmContext = mock(RMContext.class);\n \n     conf = new YarnConfiguration();\n-    conf.set(YarnConfiguration.RM_ZK_ADDRESS,\n-        optHostPort.or(curatorTestingServer.getConnectString()));\n+    conf.set(YarnConfiguration.RM_ZK_ADDRESS, optHostPort\n+        .or((curatorTestingServer == null) ? \"\" : curatorTestingServer\n+            .getConnectString()));\n     conf.set(YarnConfiguration.ZK_RM_STATE_STORE_PARENT_PATH, workingZnode);\n \n     store = new ZKRMStateStore();",
                "raw_url": "https://github.com/apache/hadoop/raw/552237d4a34ab10fa5f9ec7aad7942f2a110993e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStorePerf.java",
                "sha": "bd25defc2235d54937bf0d8508ed1a8ce2f3f44d",
                "status": "modified"
            }
        ],
        "message": "YARN-4880. Running TestZKRMStateStorePerf with real zookeeper cluster throws NPE. Contributed by Sunil G",
        "parent": "https://github.com/apache/hadoop/commit/818d6b799eead13a17a0214172df60a269b046fb",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStorePerf.java"
        ]
    },
    "hadoop_565af87": {
        "bug_id": "hadoop_565af87",
        "commit": "https://github.com/apache/hadoop/commit/565af873d5e9bc4178f1221805765a417f065192",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/565af873d5e9bc4178f1221805765a417f065192/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=565af873d5e9bc4178f1221805765a417f065192",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -1391,6 +1391,9 @@ Release 2.8.0 - UNRELEASED\n     policy instead of using same as active applications ordering policy.\n     (Rohith Sharma K S via jianhe)\n \n+    YARN-4667. RM Admin CLI for refreshNodesResources throws NPE when nothing\n+    is configured. (Naganarasimha G R via devaraj)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/565af873d5e9bc4178f1221805765a417f065192/hadoop-yarn-project/CHANGES.txt",
                "sha": "7ac2b398124960abb1861a5cf8b6654b4a6a0044",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/565af873d5e9bc4178f1221805765a417f065192/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java?ref=565af873d5e9bc4178f1221805765a417f065192",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java",
                "patch": "@@ -69,11 +69,11 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshClusterMaxPriorityRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshClusterMaxPriorityResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesRequest;\n+import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest;\n+import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshQueuesRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshQueuesResponse;\n-import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest;\n-import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshServiceAclsRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshServiceAclsResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshSuperUserGroupsConfigurationRequest;\n@@ -652,7 +652,7 @@ public RefreshNodesResourcesResponse refreshNodesResources(\n         newconf = new DynamicResourceConfiguration(configuration, true);\n       }\n \n-      if (newconf.getNodes().length == 0) {\n+      if (newconf.getNodes() == null || newconf.getNodes().length == 0) {\n         RMAuditLogger.logSuccess(user.getShortUserName(), argName,\n             \"AdminService\");\n         return response;",
                "raw_url": "https://github.com/apache/hadoop/raw/565af873d5e9bc4178f1221805765a417f065192/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java",
                "sha": "fbc2d6f068bd6c8b1bc5d33f38bf24861e3a6567",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/565af873d5e9bc4178f1221805765a417f065192/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMAdminService.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMAdminService.java?ref=565af873d5e9bc4178f1221805765a417f065192",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMAdminService.java",
                "patch": "@@ -45,29 +45,28 @@\n import org.apache.hadoop.security.authorize.AccessControlList;\n import org.apache.hadoop.security.authorize.ProxyUsers;\n import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;\n-import org.apache.hadoop.yarn.LocalConfigurationProvider;\n import org.apache.hadoop.yarn.api.records.DecommissionType;\n import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.conf.HAUtil;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshAdminAclsRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshClusterMaxPriorityRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesRequest;\n+import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshQueuesRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshServiceAclsRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshSuperUserGroupsConfigurationRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshUserToGroupsMappingsRequest;\n-import org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RemoveFromClusterNodeLabelsRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.ReplaceLabelsOnNodeRequest;\n import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration;\n-import org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration;\n-import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.junit.After;\n import org.junit.Assert;\n@@ -176,6 +175,32 @@ public void testAdminRefreshQueuesWithFileSystemBasedConfigurationProvider()\n     Assert.assertTrue(maxAppsAfter != maxAppsBefore);\n   }\n \n+  @Test\n+  public void testAdminRefreshNodesWithoutConfiguration()\n+      throws IOException, YarnException {\n+    configuration.set(YarnConfiguration.RM_CONFIGURATION_PROVIDER_CLASS,\n+        \"org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider\");\n+\n+    // upload default configurations\n+    uploadDefaultConfiguration();\n+\n+    try {\n+      rm = new MockRM(configuration);\n+      rm.init(configuration);\n+      rm.start();\n+    } catch (Exception ex) {\n+      fail(\"Should not get any exceptions\");\n+    }\n+\n+    try {\n+      rm.adminService\n+          .refreshNodesResources(RefreshNodesResourcesRequest.newInstance());\n+    } catch (Exception ex) {\n+      fail(\"Should not get any exceptions even when no configurations \"\n+          + \"are done for node resources refresh\");\n+    }\n+  }\n+\n   @Test\n   public void testAdminRefreshNodesResourcesWithFileSystemBasedConfigurationProvider()\n       throws IOException, YarnException {",
                "raw_url": "https://github.com/apache/hadoop/raw/565af873d5e9bc4178f1221805765a417f065192/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMAdminService.java",
                "sha": "639b95586a13c51ae270de1a9ec871952aa427fe",
                "status": "modified"
            }
        ],
        "message": "YARN-4667. RM Admin CLI for refreshNodesResources throws NPE when nothing\nis configured. Contributed by Naganarasimha G R.",
        "parent": "https://github.com/apache/hadoop/commit/f3bbe0bd020b9efe05d5918ad042d9d4d4b1ca57",
        "patched_files": [
            "AdminService.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMAdminService.java"
        ]
    },
    "hadoop_56996a6": {
        "bug_id": "hadoop_56996a6",
        "commit": "https://github.com/apache/hadoop/commit/56996a685e6201cb186cea866d22418289174574",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=56996a685e6201cb186cea866d22418289174574",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -752,6 +752,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-11927.  Fix \"undefined reference to dlopen\" error when compiling\n     libhadooppipes (Xianyin Xin via Colin P. McCabe)\n \n+    HADOOP-8751. NPE in Token.toString() when Token is constructed using null\n+    identifier. (kanaka kumar avvaru via aajisaka)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "864865fb40caa53f32c5cf51e33e3d1c62cd7c48",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java?ref=56996a685e6201cb186cea866d22418289174574",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "patch": "@@ -70,10 +70,10 @@ public Token(T id, SecretManager<T> mgr) {\n    * @param service the service for this token\n    */\n   public Token(byte[] identifier, byte[] password, Text kind, Text service) {\n-    this.identifier = identifier;\n-    this.password = password;\n-    this.kind = kind;\n-    this.service = service;\n+    this.identifier = (identifier == null)? new byte[0] : identifier;\n+    this.password = (password == null)? new byte[0] : password;\n+    this.kind = (kind == null)? new Text() : kind;\n+    this.service = (service == null)? new Text() : service;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "sha": "bd254e6d8d09535a7fb5bd0ee504643e85ae4547",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java?ref=56996a685e6201cb186cea866d22418289174574",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.security.token.SecretManager;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n+import org.apache.hadoop.security.token.TokenIdentifier;\n import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;\n import org.apache.hadoop.util.Daemon;\n import org.apache.hadoop.util.Time;\n@@ -539,4 +540,18 @@ public void testDelegationKeyEqualAndHash() {\n     Assert.assertEquals(key1, key2);\n     Assert.assertFalse(key2.equals(key3));\n   }\n+\n+  @Test\n+  public void testEmptyToken() throws IOException {\n+    Token<?> token1 = new Token<TokenIdentifier>();\n+\n+    Token<?> token2 = new Token<TokenIdentifier>(new byte[0], new byte[0],\n+        new Text(), new Text());\n+    assertEquals(token1, token2);\n+    assertEquals(token1.encodeToUrlString(), token2.encodeToUrlString());\n+\n+    token2 = new Token<TokenIdentifier>(null, null, null, null);\n+    assertEquals(token1, token2);\n+    assertEquals(token1.encodeToUrlString(), token2.encodeToUrlString());\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "sha": "b41ff15251050a26a9108b4b59ab4248076da619",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8751. NPE in Token.toString() when Token is constructed using null identifier. Contributed by kanaka kumar avvaru.",
        "parent": "https://github.com/apache/hadoop/commit/39077dba2e877420e7470df253f6154f6ecc64ec",
        "patched_files": [
            "Token.java",
            "CHANGES.java",
            "DelegationToken.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestToken.java",
            "TestDelegationToken.java"
        ]
    },
    "hadoop_57ead18": {
        "bug_id": "hadoop_57ead18",
        "commit": "https://github.com/apache/hadoop/commit/57ead18a85e15aef1993f49157cf05aed38f1c87",
        "file": [
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/57ead18a85e15aef1993f49157cf05aed38f1c87/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java?ref=57ead18a85e15aef1993f49157cf05aed38f1c87",
                "deletions": 17,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "patch": "@@ -402,8 +402,13 @@ public int run(String[] argv) throws Exception {\n           }\n         }\n       } else if (listEvents) {\n-        listEvents(getJob(JobID.forName(jobid)), fromEvent, nEvents);\n-        exitCode = 0;\n+        Job job = getJob(JobID.forName(jobid));\n+        if (job == null) {\n+          System.out.println(\"Could not find job \" + jobid);\n+        } else {\n+          listEvents(job, fromEvent, nEvents);\n+          exitCode = 0;\n+        }\n       } else if (listJobs) {\n         listJobs(cluster);\n         exitCode = 0;\n@@ -417,8 +422,13 @@ public int run(String[] argv) throws Exception {\n         listBlacklistedTrackers(cluster);\n         exitCode = 0;\n       } else if (displayTasks) {\n-        displayTasks(getJob(JobID.forName(jobid)), taskType, taskState);\n-        exitCode = 0;\n+        Job job = getJob(JobID.forName(jobid));\n+        if (job == null) {\n+          System.out.println(\"Could not find job \" + jobid);\n+        } else {\n+          displayTasks(getJob(JobID.forName(jobid)), taskType, taskState);\n+          exitCode = 0;\n+        }\n       } else if(killTask) {\n         TaskAttemptID taskID = TaskAttemptID.forName(taskid);\n         Job job = getJob(taskID.getJobID());\n@@ -444,20 +454,24 @@ public int run(String[] argv) throws Exception {\n           exitCode = -1;\n         }\n       } else if (logs) {\n-        try {\n         JobID jobID = JobID.forName(jobid);\n-        TaskAttemptID taskAttemptID = TaskAttemptID.forName(taskid);\n-        LogParams logParams = cluster.getLogParams(jobID, taskAttemptID);\n-        LogCLIHelpers logDumper = new LogCLIHelpers();\n-        logDumper.setConf(getConf());\n-        exitCode = logDumper.dumpAContainersLogs(logParams.getApplicationId(),\n-            logParams.getContainerId(), logParams.getNodeId(),\n-            logParams.getOwner());\n-        } catch (IOException e) {\n-          if (e instanceof RemoteException) {\n-            throw e;\n-          } \n-          System.out.println(e.getMessage());\n+        if (getJob(jobID) == null) {\n+          System.out.println(\"Could not find job \" + jobid);\n+        } else {\n+          try {\n+            TaskAttemptID taskAttemptID = TaskAttemptID.forName(taskid);\n+            LogParams logParams = cluster.getLogParams(jobID, taskAttemptID);\n+            LogCLIHelpers logDumper = new LogCLIHelpers();\n+            logDumper.setConf(getConf());\n+            exitCode = logDumper.dumpAContainersLogs(\n+                    logParams.getApplicationId(), logParams.getContainerId(),\n+                    logParams.getNodeId(), logParams.getOwner());\n+          } catch (IOException e) {\n+            if (e instanceof RemoteException) {\n+              throw e;\n+            }\n+            System.out.println(e.getMessage());\n+          }\n         }\n       }\n     } catch (RemoteException re) {",
                "raw_url": "https://github.com/apache/hadoop/raw/57ead18a85e15aef1993f49157cf05aed38f1c87/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "sha": "4f557922db942ae9602c363154dc05a111233f67",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop/blob/57ead18a85e15aef1993f49157cf05aed38f1c87/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/tools/TestCLI.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/tools/TestCLI.java?ref=57ead18a85e15aef1993f49157cf05aed38f1c87",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/tools/TestCLI.java",
                "patch": "@@ -87,7 +87,7 @@ public void testListAttemptIdsWithInvalidInputs() throws Exception {\n     JobID jobId = JobID.forName(jobIdStr);\n     Cluster mockCluster = mock(Cluster.class);\n     Job job = mock(Job.class);\n-    CLI cli = spy(new CLI());\n+    CLI cli = spy(new CLI(new Configuration()));\n \n     doReturn(mockCluster).when(cli).createCluster();\n     when(mockCluster.getJob(jobId)).thenReturn(job);\n@@ -101,12 +101,18 @@ public void testListAttemptIdsWithInvalidInputs() throws Exception {\n     int retCode_invalidTaskState = cli.run(new String[] { \"-list-attempt-ids\",\n         jobIdStr, \"REDUCE\", \"complete\" });\n \n+    String jobIdStr2 = \"job_1015298225799_0016\";\n+    int retCode_invalidJobId = cli.run(new String[] { \"-list-attempt-ids\",\n+        jobIdStr2, \"MAP\", \"running\" });\n+\n     assertEquals(\"JOB_SETUP is an invalid input,exit code should be -1\", -1,\n         retCode_JOB_SETUP);\n     assertEquals(\"JOB_CLEANUP is an invalid input,exit code should be -1\", -1,\n         retCode_JOB_CLEANUP);\n     assertEquals(\"complete is an invalid input,exit code should be -1\", -1,\n         retCode_invalidTaskState);\n+    assertEquals(\"Non existing job id should be skippted with -1\", -1,\n+        retCode_invalidJobId);\n \n   }\n \n@@ -176,4 +182,34 @@ public void testGetJob() throws Exception {\n       Assert.assertTrue(end - start < ((i + 1) * sleepTime));\n     }\n   }\n+\n+  @Test\n+  public void testListEvents() throws Exception {\n+    Cluster mockCluster = mock(Cluster.class);\n+    CLI cli = spy(new CLI(new Configuration()));\n+    doReturn(mockCluster).when(cli).createCluster();\n+    String jobId1 = \"job_1234654654_001\";\n+    String jobId2 = \"job_1234654656_002\";\n+\n+    Job mockJob1 = mockJob(mockCluster, jobId1, State.RUNNING);\n+\n+    // Check exiting with non existing job\n+    int exitCode = cli.run(new String[]{\"-events\", jobId2, \"0\", \"10\"});\n+    assertEquals(-1, exitCode);\n+  }\n+\n+  @Test\n+  public void testLogs() throws Exception {\n+    Cluster mockCluster = mock(Cluster.class);\n+    CLI cli = spy(new CLI(new Configuration()));\n+    doReturn(mockCluster).when(cli).createCluster();\n+    String jobId1 = \"job_1234654654_001\";\n+    String jobId2 = \"job_1234654656_002\";\n+\n+    Job mockJob1 = mockJob(mockCluster, jobId1, State.SUCCEEDED);\n+\n+    // Check exiting with non existing job\n+    int exitCode = cli.run(new String[]{\"-logs\", jobId2});\n+    assertEquals(-1, exitCode);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/57ead18a85e15aef1993f49157cf05aed38f1c87/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/tools/TestCLI.java",
                "sha": "112f5855b01203c9c06392d9d18227081a1621d4",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6110. JobHistoryServer CLI throws NullPointerException with job ids that do not exist. (Kai Sasaki via gtcarrera9)",
        "parent": "https://github.com/apache/hadoop/commit/e7ed05e4f5b0421e93f2f2cadc5beda3d28b9911",
        "patched_files": [
            "CLI.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCLI.java"
        ]
    },
    "hadoop_5827d16": {
        "bug_id": "hadoop_5827d16",
        "commit": "https://github.com/apache/hadoop/commit/5827d1667cf9f9ef6602db534481a931739480ad",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5827d1667cf9f9ef6602db534481a931739480ad/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=5827d1667cf9f9ef6602db534481a931739480ad",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -952,6 +952,9 @@ Release 2.1.0-beta - 2013-08-22\n     HDFS-5016. Deadlock in pipeline recovery causes Datanode to be marked dead.\n     (suresh)\n \n+    HDFS-5228. The RemoteIterator returned by DistributedFileSystem.listFiles\n+    may throw NullPointerException.  (szetszwo and cnauroth via szetszwo)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop/raw/5827d1667cf9f9ef6602db534481a931739480ad/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "096ead7f9924807cc4ddcc38c6af99de416481b4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5827d1667cf9f9ef6602db534481a931739480ad/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java?ref=5827d1667cf9f9ef6602db534481a931739480ad",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
                "patch": "@@ -713,6 +713,7 @@ public Void next(final FileSystem fs, final Path p)\n   protected RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path p,\n       final PathFilter filter)\n   throws IOException {\n+    final Path absF = fixRelativePart(p);\n     return new RemoteIterator<LocatedFileStatus>() {\n       private DirectoryListing thisListing;\n       private int i;\n@@ -722,7 +723,7 @@ public Void next(final FileSystem fs, final Path p)\n       { // initializer\n         // Fully resolve symlinks in path first to avoid additional resolution\n         // round-trips as we fetch more batches of listings\n-        src = getPathName(resolvePath(p));\n+        src = getPathName(resolvePath(absF));\n         // fetch the first batch of entries in the directory\n         thisListing = dfs.listPaths(src, HdfsFileStatus.EMPTY_NAME, true);\n         statistics.incrementReadOps(1);\n@@ -736,7 +737,7 @@ public boolean hasNext() throws IOException {\n         while (curStat == null && hasNextNoFilter()) {\n           LocatedFileStatus next = \n               ((HdfsLocatedFileStatus)thisListing.getPartialListing()[i++])\n-              .makeQualifiedLocated(getUri(), p);\n+              .makeQualifiedLocated(getUri(), absF);\n           if (filter.accept(next.getPath())) {\n             curStat = next;\n           }",
                "raw_url": "https://github.com/apache/hadoop/raw/5827d1667cf9f9ef6602db534481a931739480ad/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java",
                "sha": "6fb572aa472d76615b79c9b2feb29ad4f1fa19d6",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/5827d1667cf9f9ef6602db534481a931739480ad/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=5827d1667cf9f9ef6602db534481a931739480ad",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -31,8 +31,10 @@\n import java.io.IOException;\n import java.net.URI;\n import java.security.PrivilegedExceptionAction;\n+import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.EnumSet;\n+import java.util.List;\n import java.util.Random;\n \n import org.apache.commons.lang.ArrayUtils;\n@@ -47,9 +49,11 @@\n import org.apache.hadoop.fs.FileChecksum;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n import org.apache.hadoop.fs.MD5MD5CRC32FileChecksum;\n import org.apache.hadoop.fs.Options.ChecksumOpt;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n import org.apache.hadoop.fs.VolumeId;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.hdfs.web.WebHdfsFileSystem;\n@@ -226,7 +230,7 @@ public void testDFSClient() throws Exception {\n       final long millis = Time.now();\n \n       {\n-        DistributedFileSystem dfs = (DistributedFileSystem)cluster.getFileSystem();\n+        final DistributedFileSystem dfs = cluster.getFileSystem();\n         dfs.dfs.getLeaseRenewer().setGraceSleepPeriod(grace);\n         assertFalse(dfs.dfs.getLeaseRenewer().isRunning());\n   \n@@ -326,7 +330,7 @@ public void testDFSClient() throws Exception {\n       }\n \n       {\n-        DistributedFileSystem dfs = (DistributedFileSystem)cluster.getFileSystem();\n+        final DistributedFileSystem dfs = cluster.getFileSystem();\n         assertFalse(dfs.dfs.getLeaseRenewer().isRunning());\n \n         //open and check the file\n@@ -835,4 +839,25 @@ public void testFileCloseStatus() throws IOException {\n     }\n   }\n   \n+  @Test(timeout=60000)\n+  public void testListFiles() throws IOException {\n+    Configuration conf = new HdfsConfiguration();\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).build();\n+    \n+    try {\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+  \n+      final Path relative = new Path(\"relative\");\n+      fs.create(new Path(relative, \"foo\")).close();\n+  \n+      final List<LocatedFileStatus> retVal = new ArrayList<LocatedFileStatus>();\n+      final RemoteIterator<LocatedFileStatus> iter = fs.listFiles(relative, true);\n+      while (iter.hasNext()) {\n+        retVal.add(iter.next());\n+      }\n+      System.out.println(\"retVal = \" + retVal);\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/5827d1667cf9f9ef6602db534481a931739480ad/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "a57ad74561201e0b79f669e7295b573e23234d89",
                "status": "modified"
            }
        ],
        "message": "HDFS-5228. The RemoteIterator returned by DistributedFileSystem.listFiles may throw NullPointerException.  Contributed by szetszwo and cnauroth\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525828 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b838ac89a673c465d96b71c9c5eae8e4e26b8608",
        "patched_files": [
            "DistributedFileSystem.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDistributedFileSystem.java"
        ]
    },
    "hadoop_585ebd8": {
        "bug_id": "hadoop_585ebd8",
        "commit": "https://github.com/apache/hadoop/commit/585ebd873a55bedd2a364d256837f08ada8ba032",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java?ref=585ebd873a55bedd2a364d256837f08ada8ba032",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "patch": "@@ -500,6 +500,11 @@ public Path getPathForLocalization(LocalResourceRequest req,\n \n     Path localPath = new Path(rPath, req.getPath().getName());\n     LocalizedResource rsrc = localrsrc.get(req);\n+    if (rsrc == null) {\n+      LOG.warn(\"Resource \" + req + \" has been removed\"\n+          + \" and will no longer be localized\");\n+      return null;\n+    }\n     rsrc.setLocalPath(localPath);\n     LocalResource lr = LocalResource.newInstance(req.getResource(),\n         req.getType(), req.getVisibility(), req.getSize(),",
                "raw_url": "https://github.com/apache/hadoop/raw/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "sha": "ad24c62828fea4818e0f0f1aec2f306372ca7891",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=585ebd873a55bedd2a364d256837f08ada8ba032",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -885,6 +885,9 @@ public void addResource(LocalizerResourceRequestEvent request) {\n             Path publicDirDestPath =\n                 publicRsrc.getPathForLocalization(key, publicRootPath,\n                     delService);\n+            if (publicDirDestPath == null) {\n+              return;\n+            }\n             if (!publicDirDestPath.getParent().equals(publicRootPath)) {\n               createParentDirs(publicDirDestPath, publicRootPath);\n               if (diskValidator != null) {\n@@ -1175,10 +1178,11 @@ LocalizerHeartbeatResponse processHeartbeat(\n           LocalResourcesTracker tracker = getLocalResourcesTracker(\n               next.getVisibility(), user, applicationId);\n           if (tracker != null) {\n-            ResourceLocalizationSpec resource =\n-                NodeManagerBuilderUtils.newResourceLocalizationSpec(next,\n-                getPathForLocalization(next, tracker));\n-            rsrcs.add(resource);\n+            Path localPath = getPathForLocalization(next, tracker);\n+            if (localPath != null) {\n+              rsrcs.add(NodeManagerBuilderUtils.newResourceLocalizationSpec(\n+                  next, localPath));\n+            }\n           }\n         } catch (IOException e) {\n           LOG.error(\"local path for PRIVATE localization could not be \" +",
                "raw_url": "https://github.com/apache/hadoop/raw/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "d9b887f56decbc4dc493b2dced48b6812570ac0c",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java?ref=585ebd873a55bedd2a364d256837f08ada8ba032",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "patch": "@@ -1717,8 +1717,18 @@ public void testLocalizerHeartbeatWhenAppCleaningUp() throws Exception {\n       assertEquals(\"NM should tell localizer to be LIVE in Heartbeat.\",\n           LocalizerAction.LIVE, response.getLocalizerAction());\n \n-      // Cleanup application.\n+      // Cleanup container.\n       spyService.handle(new ContainerLocalizationCleanupEvent(c, rsrcs));\n+      dispatcher.await();\n+      try {\n+        /*Directly send heartbeat to introduce race as container\n+          is being cleaned up.*/\n+        locRunnerForContainer.processHeartbeat(\n+              Collections.singletonList(rsrcSuccess));\n+      } catch (Exception e) {\n+        fail(\"Exception should not have been thrown on processing heartbeat\");\n+      }\n+      // Cleanup application.\n       spyService.handle(new ApplicationLocalizationEvent(\n           LocalizationEventType.DESTROY_APPLICATION_RESOURCES, app));\n       dispatcher.await();",
                "raw_url": "https://github.com/apache/hadoop/raw/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "sha": "21896ca4c01c44179e7e8506515df3756f1f14c5",
                "status": "modified"
            }
        ],
        "message": "YARN-8649. NPE in localizer hearbeat processing if a container is killed while localizing. Contributed by lujie",
        "parent": "https://github.com/apache/hadoop/commit/bed8cb6979e0460141ed77e3b15d4f18db098a8e",
        "patched_files": [
            "ResourceLocalizationService.java",
            "LocalResourcesTrackerImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalResourcesTrackerImpl.java",
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_58b97c7": {
        "bug_id": "hadoop_58b97c7",
        "commit": "https://github.com/apache/hadoop/commit/58b97c79e34901938d59acc84ed48c1f9344996a",
        "file": [
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 21,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "patch": "@@ -1065,7 +1065,7 @@ private void addKeytabResourceIfSecure(SliderFileSystem fileSystem,\n       LOG.warn(\"No Kerberos principal name specified for \" + service.getName());\n       return;\n     }\n-    if(StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n+    if (StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n       LOG.warn(\"No Kerberos keytab specified for \" + service.getName());\n       return;\n     }\n@@ -1077,27 +1077,31 @@ private void addKeytabResourceIfSecure(SliderFileSystem fileSystem,\n       throw new YarnException(e);\n     }\n \n-    switch (keytabURI.getScheme()) {\n-    case \"hdfs\":\n-      Path keytabOnhdfs = new Path(keytabURI);\n-      if (!fileSystem.getFileSystem().exists(keytabOnhdfs)) {\n-        LOG.warn(service.getName() + \"'s keytab (principalName = \" +\n-            principalName + \") doesn't exist at: \" + keytabOnhdfs);\n-        return;\n+    if (keytabURI.getScheme() != null) {\n+      switch (keytabURI.getScheme()) {\n+      case \"hdfs\":\n+        Path keytabOnhdfs = new Path(keytabURI);\n+        if (!fileSystem.getFileSystem().exists(keytabOnhdfs)) {\n+          LOG.warn(service.getName() + \"'s keytab (principalName = \"\n+              + principalName + \") doesn't exist at: \" + keytabOnhdfs);\n+          return;\n+        }\n+        LocalResource keytabRes = fileSystem.createAmResource(keytabOnhdfs,\n+            LocalResourceType.FILE);\n+        localResource.put(String.format(YarnServiceConstants.KEYTAB_LOCATION,\n+            service.getName()), keytabRes);\n+        LOG.info(\"Adding \" + service.getName() + \"'s keytab for \"\n+            + \"localization, uri = \" + keytabOnhdfs);\n+        break;\n+      case \"file\":\n+        LOG.info(\"Using a keytab from localhost: \" + keytabURI);\n+        break;\n+      default:\n+        LOG.warn(\"Unsupported keytab URI scheme \" + keytabURI);\n+        break;\n       }\n-      LocalResource keytabRes =\n-          fileSystem.createAmResource(keytabOnhdfs, LocalResourceType.FILE);\n-      localResource.put(String.format(YarnServiceConstants.KEYTAB_LOCATION,\n-          service.getName()), keytabRes);\n-      LOG.debug(\"Adding \" + service.getName() + \"'s keytab for \" +\n-          \"localization, uri = \" + keytabOnhdfs);\n-      break;\n-    case \"file\":\n-      LOG.debug(\"Using a keytab from localhost: \" + keytabURI);\n-      break;\n-    default:\n-      LOG.warn(\"Unsupported URI scheme \" + keytabURI);\n-      break;\n+    } else {\n+      LOG.warn(\"Unsupported keytab URI scheme \" + keytabURI);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "sha": "364a94ca3f68b273e4d5a75bf0c4417d88d45ff4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "patch": "@@ -103,4 +103,6 @@\n       + \"expression element name %s specified in placement policy of component \"\n       + \"%s. Expression element names should be a valid constraint name or an \"\n       + \"expression name defined for this component only.\";\n+  String ERROR_KEYTAB_URI_SCHEME_INVALID = \"Unsupported keytab URI scheme: %s\";\n+  String ERROR_KEYTAB_URI_INVALID = \"Invalid keytab URI: %s\";\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "sha": "0e42533504d1fbda30a84808a370729b7fc8f3a5",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "patch": "@@ -29,13 +29,14 @@\n import org.apache.hadoop.registry.client.binding.RegistryUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n-import org.apache.hadoop.yarn.service.api.records.Container;\n-import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.api.records.Artifact;\n import org.apache.hadoop.yarn.service.api.records.Component;\n import org.apache.hadoop.yarn.service.api.records.Configuration;\n+import org.apache.hadoop.yarn.service.api.records.Container;\n+import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n+import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.exceptions.SliderException;\n import org.apache.hadoop.yarn.service.conf.RestApiConstants;\n import org.apache.hadoop.yarn.service.exceptions.RestApiErrorMessages;\n@@ -111,14 +112,7 @@ public static void validateAndResolveService(Service service,\n     }\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n-      if (!StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n-        try {\n-          // validate URI format\n-          new URI(service.getKerberosPrincipal().getKeytab());\n-        } catch (URISyntaxException e) {\n-          throw new IllegalArgumentException(e);\n-        }\n-      }\n+      validateKerberosPrincipal(service.getKerberosPrincipal());\n     }\n \n     // Validate the Docker client config.\n@@ -145,9 +139,8 @@ public static void validateAndResolveService(Service service,\n         throw new IllegalArgumentException(\"Component name collision: \" +\n             comp.getName());\n       }\n-      // If artifact is of type SERVICE (which cannot be filled from\n-      // global), read external service and add its components to this\n-      // service\n+      // If artifact is of type SERVICE (which cannot be filled from global),\n+      // read external service and add its components to this service\n       if (comp.getArtifact() != null && comp.getArtifact().getType() ==\n           Artifact.TypeEnum.SERVICE) {\n         if (StringUtils.isEmpty(comp.getArtifact().getId())) {\n@@ -226,6 +219,25 @@ public static void validateAndResolveService(Service service,\n     }\n   }\n \n+  public static void validateKerberosPrincipal(\n+      KerberosPrincipal kerberosPrincipal) throws IOException {\n+    if (!StringUtils.isEmpty(kerberosPrincipal.getKeytab())) {\n+      try {\n+        // validate URI format\n+        URI keytabURI = new URI(kerberosPrincipal.getKeytab());\n+        if (keytabURI.getScheme() == null) {\n+          throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_KEYTAB_URI_SCHEME_INVALID,\n+              kerberosPrincipal.getKeytab()));\n+        }\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(\n+            String.format(RestApiErrorMessages.ERROR_KEYTAB_URI_INVALID,\n+                e.getLocalizedMessage()));\n+      }\n+    }\n+  }\n+\n   private static void validateDockerClientConfiguration(Service service,\n       org.apache.hadoop.conf.Configuration conf) throws IOException {\n     String dockerClientConfig = service.getDockerClientConfig();",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "sha": "6e62c56f23b7bc1498822046755fc33d3b95686c",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.service.api.records.Artifact;\n import org.apache.hadoop.yarn.service.api.records.Component;\n+import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n import org.apache.hadoop.yarn.service.api.records.PlacementPolicy;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n@@ -45,6 +46,7 @@\n import static org.apache.hadoop.yarn.service.exceptions.RestApiErrorMessages.*;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n \n /**\n  * Test for ServiceApiUtil helper methods.\n@@ -525,4 +527,43 @@ public void testPlacementPolicy() throws IOException {\n       Assert.fail(NO_EXCEPTION_PREFIX + e.getMessage());\n     }\n   }\n+\n+  @Test\n+  public void testKerberosPrincipal() throws IOException {\n+    SliderFileSystem sfs = ServiceTestUtils.initMockFs();\n+    Service app = createValidApplication(\"comp-a\");\n+    KerberosPrincipal kp = new KerberosPrincipal();\n+    kp.setKeytab(\"/some/path\");\n+    kp.setPrincipalName(\"user/_HOST@domain.com\");\n+    app.setKerberosPrincipal(kp);\n+\n+    try {\n+      ServiceApiUtil.validateKerberosPrincipal(app.getKerberosPrincipal());\n+      Assert.fail(EXCEPTION_PREFIX + \"service with invalid keytab URI scheme\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(\n+          String.format(RestApiErrorMessages.ERROR_KEYTAB_URI_SCHEME_INVALID,\n+              kp.getKeytab()),\n+          e.getMessage());\n+    }\n+\n+    kp.setKeytab(\"/ blank / in / paths\");\n+    try {\n+      ServiceApiUtil.validateKerberosPrincipal(app.getKerberosPrincipal());\n+      Assert.fail(EXCEPTION_PREFIX + \"service with invalid keytab\");\n+    } catch (IllegalArgumentException e) {\n+      // strip out the %s at the end of the RestApiErrorMessages string constant\n+      assertTrue(e.getMessage().contains(\n+          RestApiErrorMessages.ERROR_KEYTAB_URI_INVALID.substring(0,\n+              RestApiErrorMessages.ERROR_KEYTAB_URI_INVALID.length() - 2)));\n+    }\n+\n+    kp.setKeytab(\"file:///tmp/a.keytab\");\n+    // now it should succeed\n+    try {\n+      ServiceApiUtil.validateKerberosPrincipal(app.getKerberosPrincipal());\n+    } catch (IllegalArgumentException e) {\n+      Assert.fail(NO_EXCEPTION_PREFIX + e.getMessage());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "sha": "b209bbb3267829c1e829c7f0694b2c284c17adb2",
                "status": "modified"
            }
        ],
        "message": "YARN-8236. Invalid kerberos principal file name cause NPE in native service. Contributed by Gour Saha.",
        "parent": "https://github.com/apache/hadoop/commit/ffb9210dedb79a56075448dc296251896bed49e6",
        "patched_files": [
            "ServiceApiUtil.java",
            "ServiceClient.java",
            "RestApiErrorMessages.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestServiceApiUtil.java",
            "TestServiceClient.java",
            "ServiceClientTest.java"
        ]
    },
    "hadoop_592aaa1": {
        "bug_id": "hadoop_592aaa1",
        "commit": "https://github.com/apache/hadoop/commit/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -573,6 +573,9 @@ Release 0.23.0 - Unreleased\n     HADOOP-7598. Fix smart-apply-patch.sh to handle patching from a sub\n     directory correctly. (Robert Evans via acmurthy) \n \n+    HADOOP-7328. When a serializer class is missing, return null, not throw\n+    an NPE. (Harsh J Chouraria via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "23ac34d30664d2f504e3883127cb50b1959ae0b9",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java?ref=592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
                "deletions": 10,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "patch": "@@ -27,10 +27,10 @@\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.io.serializer.avro.AvroReflectSerialization;\n import org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization;\n import org.apache.hadoop.util.ReflectionUtils;\n-import org.apache.hadoop.util.StringUtils;\n \n /**\n  * <p>\n@@ -50,14 +50,15 @@\n    * <p>\n    * Serializations are found by reading the <code>io.serializations</code>\n    * property from <code>conf</code>, which is a comma-delimited list of\n-   * classnames. \n+   * classnames.\n    * </p>\n    */\n   public SerializationFactory(Configuration conf) {\n     super(conf);\n-    for (String serializerName : conf.getStrings(\"io.serializations\", \n-      new String[]{WritableSerialization.class.getName(), \n-        AvroSpecificSerialization.class.getName(), \n+    for (String serializerName : conf.getStrings(\n+      CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,\n+      new String[]{WritableSerialization.class.getName(),\n+        AvroSpecificSerialization.class.getName(),\n         AvroReflectSerialization.class.getName()})) {\n       add(conf, serializerName);\n     }\n@@ -67,27 +68,35 @@ public SerializationFactory(Configuration conf) {\n   private void add(Configuration conf, String serializationName) {\n     try {\n       Class<? extends Serialization> serializionClass =\n-\t(Class<? extends Serialization>) conf.getClassByName(serializationName);\n+        (Class<? extends Serialization>) conf.getClassByName(serializationName);\n       serializations.add((Serialization)\n-\t  ReflectionUtils.newInstance(serializionClass, getConf()));\n+      ReflectionUtils.newInstance(serializionClass, getConf()));\n     } catch (ClassNotFoundException e) {\n       LOG.warn(\"Serialization class not found: \", e);\n     }\n   }\n \n   public <T> Serializer<T> getSerializer(Class<T> c) {\n-    return getSerialization(c).getSerializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getSerializer(c);\n+    }\n+    return null;\n   }\n \n   public <T> Deserializer<T> getDeserializer(Class<T> c) {\n-    return getSerialization(c).getDeserializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getDeserializer(c);\n+    }\n+    return null;\n   }\n \n   @SuppressWarnings(\"unchecked\")\n   public <T> Serialization<T> getSerialization(Class<T> c) {\n     for (Serialization serialization : serializations) {\n       if (serialization.accept(c)) {\n-\treturn (Serialization<T>) serialization;\n+        return (Serialization<T>) serialization;\n       }\n     }\n     return null;",
                "raw_url": "https://github.com/apache/hadoop/raw/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "sha": "52a0a253bbaa6bb7e7d580f71292e72fd9c28678",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java?ref=592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.io.serializer;\n+\n+import org.junit.Test;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertNotNull;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+\n+public class TestSerializationFactory {\n+\n+  @Test\n+  public void testSerializerAvailability() {\n+    Configuration conf = new Configuration();\n+    SerializationFactory factory = new SerializationFactory(conf);\n+    // Test that a valid serializer class is returned when its present\n+    assertNotNull(\"A valid class must be returned for default Writable Serde\",\n+        factory.getSerializer(Writable.class));\n+    assertNotNull(\"A valid class must be returned for default Writable serDe\",\n+        factory.getDeserializer(Writable.class));\n+    // Test that a null is returned when none can be found.\n+    assertNull(\"A null should be returned if there are no serializers found.\",\n+        factory.getSerializer(TestSerializationFactory.class));\n+    assertNull(\"A null should be returned if there are no deserializers found\",\n+        factory.getDeserializer(TestSerializationFactory.class));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "sha": "18c2637ec5adb644f7a56fb41aef5fe82967e9e2",
                "status": "added"
            }
        ],
        "message": "HADOOP-7328. When a serializer class is missing, return null, not throw an NPE. Contributed by Harsh J Chouraria.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1167363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/fbf0035ee60aea7209725fdf42710c64bd910982",
        "patched_files": [
            "SerializationFactory.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSerializationFactory.java"
        ]
    },
    "hadoop_5991ed9": {
        "bug_id": "hadoop_5991ed9",
        "commit": "https://github.com/apache/hadoop/commit/5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -305,6 +305,9 @@ Branch-2 ( Unreleased changes )\n \n     HDFS-3243. TestParallelRead timing out on jenkins. (Henry Robinson via todd)\n \n+    HDFS-3490. DatanodeWebHdfsMethods throws NullPointerException if\n+    NamenodeRpcAddressParam is not set.  (szetszwo)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "0277f829a0478b82dbc2d4ce236e62a881cc7f94",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java?ref=5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "patch": "@@ -98,6 +98,10 @@ private void init(final UserGroupInformation ugi,\n       LOG.trace(\"HTTP \" + op.getValue().getType() + \": \" + op + \", \" + path\n           + \", ugi=\" + ugi + Param.toSortedString(\", \", parameters));\n     }\n+    if (nnRpcAddr == null) {\n+      throw new IllegalArgumentException(NamenodeRpcAddressParam.NAME\n+          + \" is not specified.\");\n+    }\n \n     //clear content type\n     response.setContentType(null);",
                "raw_url": "https://github.com/apache/hadoop/raw/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java",
                "sha": "eb4afe75f85be5949890a6fdee9882413be3dbc0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java?ref=5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "patch": "@@ -123,7 +123,7 @@ private void init(final UserGroupInformation ugi,\n       final DelegationParam delegation,\n       final UserParam username, final DoAsParam doAsUser,\n       final UriFsPathParam path, final HttpOpParam<?> op,\n-      final Param<?, ?>... parameters) throws IOException {\n+      final Param<?, ?>... parameters) {\n     if (LOG.isTraceEnabled()) {\n       LOG.trace(\"HTTP \" + op.getValue().getType() + \": \" + op + \", \" + path\n           + \", ugi=\" + ugi + \", \" + username + \", \" + doAsUser\n@@ -532,7 +532,7 @@ public Response getRoot(\n           final RenewerParam renewer,\n       @QueryParam(BufferSizeParam.NAME) @DefaultValue(BufferSizeParam.DEFAULT)\n           final BufferSizeParam bufferSize\n-      ) throws IOException, URISyntaxException, InterruptedException {\n+      ) throws IOException, InterruptedException {\n     return get(ugi, delegation, username, doAsUser, ROOT, op,\n         offset, length, renewer, bufferSize);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "sha": "de8f256705b2f3d1f3277af13ad27a997b6b5a64",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java?ref=5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java",
                "patch": "@@ -44,6 +44,10 @@ public String getDomain() {\n \n     @Override\n     InetSocketAddress parse(final String str) {\n+      if (str == null) {\n+        throw new IllegalArgumentException(\"The input string is null: expect \"\n+            + getDomain());\n+      }\n       final int i = str.indexOf(':');\n       if (i < 0) {\n         throw new IllegalArgumentException(\"Failed to parse \\\"\" + str",
                "raw_url": "https://github.com/apache/hadoop/raw/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/InetSocketAddressParam.java",
                "sha": "9879ba3032c35549c6b84d951727095f04c1d4bb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java?ref=5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java",
                "patch": "@@ -59,7 +59,7 @@ public String toString() {\n \n     @Override\n     public String getDomain() {\n-      return \"<\" + NULL + \" | short in radix \" + radix + \">\";\n+      return \"<\" + NULL + \" | long in radix \" + radix + \">\";\n     }\n \n     @Override\n@@ -72,7 +72,7 @@ Long parse(final String str) {\n       }\n     }\n \n-    /** Convert a Short to a String. */ \n+    /** Convert a Long to a String. */ \n     String toString(final Long n) {\n       return n == null? NULL: Long.toString(n, radix);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/LongParam.java",
                "sha": "6f102e1c9f3f7e1d73f74ba30448936b297bd7b4",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java?ref=5991ed9cbd18520040159508ef8bd02b7b3bf5e5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.hdfs.web.resources.DoAsParam;\n import org.apache.hadoop.hdfs.web.resources.GetOpParam;\n import org.apache.hadoop.hdfs.web.resources.HttpOpParam;\n+import org.apache.hadoop.hdfs.web.resources.NamenodeRpcAddressParam;\n import org.apache.hadoop.hdfs.web.resources.PutOpParam;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -351,5 +352,31 @@ public void testResponseCode() throws IOException {\n     {//test append.\n       AppendTestUtil.testAppend(fs, new Path(dir, \"append\"));\n     }\n+\n+    {//test NamenodeRpcAddressParam not set.\n+      final HttpOpParam.Op op = PutOpParam.Op.CREATE;\n+      final URL url = webhdfs.toUrl(op, dir);\n+      HttpURLConnection conn = (HttpURLConnection) url.openConnection();\n+      conn.setRequestMethod(op.getType().toString());\n+      conn.setDoOutput(false);\n+      conn.setInstanceFollowRedirects(false);\n+      conn.connect();\n+      final String redirect = conn.getHeaderField(\"Location\");\n+      conn.disconnect();\n+\n+      //remove NamenodeRpcAddressParam\n+      WebHdfsFileSystem.LOG.info(\"redirect = \" + redirect);\n+      final int i = redirect.indexOf(NamenodeRpcAddressParam.NAME);\n+      final int j = redirect.indexOf(\"&\", i);\n+      String modified = redirect.substring(0, i - 1) + redirect.substring(j);\n+      WebHdfsFileSystem.LOG.info(\"modified = \" + modified);\n+\n+      //connect to datanode\n+      conn = (HttpURLConnection)new URL(modified).openConnection();\n+      conn.setRequestMethod(op.getType().toString());\n+      conn.setDoOutput(op.getDoOutput());\n+      conn.connect();\n+      assertEquals(HttpServletResponse.SC_BAD_REQUEST, conn.getResponseCode());\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/5991ed9cbd18520040159508ef8bd02b7b3bf5e5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java",
                "sha": "61734180cfd66918c98f69d5b2561cc2b4863eeb",
                "status": "modified"
            }
        ],
        "message": "HDFS-3490. DatanodeWebHdfsMethods throws NullPointerException if NamenodeRpcAddressParam is not set. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1348287 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/51d889e369a3f3f3a6ac02791b47e89d0a0312a6",
        "patched_files": [
            "DatanodeWebHdfsMethods.java",
            "NamenodeWebHdfsMethods.java",
            "LongParam.java",
            "CHANGES.java",
            "InetSocketAddressParam.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestWebHdfsFileSystemContract.java",
            "TestLongParam.java"
        ]
    },
    "hadoop_5b322c6": {
        "bug_id": "hadoop_5b322c6",
        "commit": "https://github.com/apache/hadoop/commit/5b322c6a823208bbc64698379340343a72e8160a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5b322c6a823208bbc64698379340343a72e8160a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=5b322c6a823208bbc64698379340343a72e8160a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1169,6 +1169,9 @@ Release 2.7.0 - UNRELEASED\n \n     HDFS-7886. Fix TestFileTruncate falures. (Plamen Jeliazkov and shv)\n \n+    HDFS-7946. TestDataNodeVolumeFailureReporting NPE on Windows. (Xiaoyu Yao\n+    via Arpit Agarwal)\n+\n     BREAKDOWN OF HDFS-7584 SUBTASKS AND RELATED JIRAS\n \n       HDFS-7720. Quota by Storage Type API, tools and ClientNameNode",
                "raw_url": "https://github.com/apache/hadoop/raw/5b322c6a823208bbc64698379340343a72e8160a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "db8741c35c9ee8dfe4ca946e6cebea3524b76f15",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/5b322c6a823208bbc64698379340343a72e8160a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java?ref=5b322c6a823208bbc64698379340343a72e8160a",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java",
                "patch": "@@ -91,6 +91,7 @@ public void tearDown() throws Exception {\n     // been simulated by denying execute access.  This is based on the maximum\n     // number of datanodes and the maximum number of storages per data node used\n     // throughout the tests in this suite.\n+    assumeTrue(!Path.WINDOWS);\n     int maxDataNodes = 3;\n     int maxStoragesPerDataNode = 4;\n     for (int i = 0; i < maxDataNodes; i++) {\n@@ -100,7 +101,9 @@ public void tearDown() throws Exception {\n       }\n     }\n     IOUtils.cleanup(LOG, fs);\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/5b322c6a823208bbc64698379340343a72e8160a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java",
                "sha": "9842f25b5ff0909fb036a0bac0f5ec26c29fb530",
                "status": "modified"
            }
        ],
        "message": "HDFS-7946. TestDataNodeVolumeFailureReporting NPE on Windows. (Contributed by Xiaoyu Yao)",
        "parent": "https://github.com/apache/hadoop/commit/658097d6da1b1aac8e01db459f0c3b456e99652f",
        "patched_files": [
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDataNodeVolumeFailureReporting.java"
        ]
    },
    "hadoop_5c8d907": {
        "bug_id": "hadoop_5c8d907",
        "commit": "https://github.com/apache/hadoop/commit/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java?ref=5c8d90763c52f6bf5224b59738739bd2d1a4b4b8",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "patch": "@@ -586,12 +586,14 @@ public InetAddress getByName(String host) throws UnknownHostException {\n    *       hadoop.security.token.service.use_ip=false \n    */\n   protected static class QualifiedHostResolver implements HostResolver {\n-    private List<String> searchDomains;\n+    private List<String> searchDomains = new ArrayList<>();\n     {\n       ResolverConfig resolverConfig = ResolverConfig.getCurrentConfig();\n-      searchDomains = new ArrayList<>();\n-      for (Name name : resolverConfig.searchPath()) {\n-        searchDomains.add(name.toString());\n+      Name[] names = resolverConfig.searchPath();\n+      if (names != null) {\n+        for (Name name : names) {\n+          searchDomains.add(name.toString());\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "sha": "2313119bfec2eb6a87ed9d259f5b02ae89b374ec",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15764. Addendum patch: Fix NPE in SecurityUtil.",
        "parent": "https://github.com/apache/hadoop/commit/2a5d4315bfe13c12cacc7718537077bf9abb22e2",
        "patched_files": [
            "SecurityUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSecurityUtil.java"
        ]
    },
    "hadoop_5e093f0": {
        "bug_id": "hadoop_5e093f0",
        "commit": "https://github.com/apache/hadoop/commit/5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -290,6 +290,9 @@ Release 2.7.1 - UNRELEASED\n     YARN-3522. Fixed DistributedShell to instantiate TimeLineClient as the\n     correct user. (Zhijie Shen via jianhe)\n \n+    YARN-3537. NPE when NodeManager.serviceInit fails and stopRecoveryStore\n+    invoked (Brahma Reddy Battula via jlowe)\n+\n Release 2.7.0 - 2015-04-20\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/CHANGES.txt",
                "sha": "001396fa80b65a39c4137c42e42bfd58cb14694d",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -178,18 +178,20 @@ private void initAndStartRecoveryStore(Configuration conf)\n   }\n \n   private void stopRecoveryStore() throws IOException {\n-    nmStore.stop();\n-    if (null != context) {\n-      if (context.getDecommissioned() && nmStore.canRecover()) {\n-        LOG.info(\"Removing state store due to decommission\");\n-        Configuration conf = getConfig();\n-        Path recoveryRoot =\n-            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n-        LOG.info(\"Removing state store at \" + recoveryRoot\n-            + \" due to decommission\");\n-        FileSystem recoveryFs = FileSystem.getLocal(conf);\n-        if (!recoveryFs.delete(recoveryRoot, true)) {\n-          LOG.warn(\"Unable to delete \" + recoveryRoot);\n+    if (null != nmStore) {\n+      nmStore.stop();\n+      if (null != context) {\n+        if (context.getDecommissioned() && nmStore.canRecover()) {\n+          LOG.info(\"Removing state store due to decommission\");\n+          Configuration conf = getConfig();\n+          Path recoveryRoot =\n+              new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n+          LOG.info(\"Removing state store at \" + recoveryRoot\n+              + \" due to decommission\");\n+          FileSystem recoveryFs = FileSystem.getLocal(conf);\n+          if (!recoveryFs.delete(recoveryRoot, true)) {\n+            LOG.warn(\"Unable to delete \" + recoveryRoot);\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "6718b53f7b70943808e1f2b1f2c1f14e639ec3f1",
                "status": "modified"
            }
        ],
        "message": "YARN-3537. NPE when NodeManager.serviceInit fails and stopRecoveryStore invoked. Contributed by Brahma Reddy Battula",
        "parent": "https://github.com/apache/hadoop/commit/5ce3a77f3c00aeabcd791c3373dd3c8c25160ce2",
        "patched_files": [
            "NodeManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_606061a": {
        "bug_id": "hadoop_606061a",
        "commit": "https://github.com/apache/hadoop/commit/606061aa147dc6d619d6240b7ea31d8f8f220e5d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/606061aa147dc6d619d6240b7ea31d8f8f220e5d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/FpgaDiscoverer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/FpgaDiscoverer.java?ref=606061aa147dc6d619d6240b7ea31d8f8f220e5d",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/FpgaDiscoverer.java",
                "patch": "@@ -124,6 +124,7 @@ public void initialize(Configuration config) throws YarnException {\n \n     if (allowed == null || allowed.equalsIgnoreCase(\n         YarnConfiguration.AUTOMATICALLY_DISCOVER_GPU_DEVICES)) {\n+      currentFpgaInfo = ImmutableList.copyOf(list);\n       return list;\n     } else if (allowed.matches(\"(\\\\d,)*\\\\d\")){\n       Set<String> minors = Sets.newHashSet(allowed.split(\",\"));\n@@ -134,6 +135,8 @@ public void initialize(Configuration config) throws YarnException {\n         .filter(dev -> minors.contains(String.valueOf(dev.getMinor())))\n         .collect(Collectors.toList());\n \n+      currentFpgaInfo = ImmutableList.copyOf(list);\n+\n       // if the count of user configured is still larger than actual\n       if (list.size() != minors.size()) {\n         LOG.warn(\"We continue although there're mistakes in user's configuration \" +\n@@ -145,8 +148,6 @@ public void initialize(Configuration config) throws YarnException {\n           YarnConfiguration.NM_FPGA_ALLOWED_DEVICES + \":\\\"\" + allowed + \"\\\"\");\n     }\n \n-    currentFpgaInfo = ImmutableList.copyOf(list);\n-\n     return list;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/606061aa147dc6d619d6240b7ea31d8f8f220e5d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/FpgaDiscoverer.java",
                "sha": "180a011b61f59dbda53ebfe0ede0447d2ef1d103",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/hadoop/blob/606061aa147dc6d619d6240b7ea31d8f8f220e5d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/TestFpgaDiscoverer.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/TestFpgaDiscoverer.java?ref=606061aa147dc6d619d6240b7ea31d8f8f220e5d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/TestFpgaDiscoverer.java",
                "patch": "@@ -288,6 +288,39 @@ public void testDiscoveryWhenExternalScriptCannotBeExecuted()\n     }\n   }\n \n+  @Test\n+  public void testCurrentFpgaInfoWhenAllDevicesAreAllowed()\n+      throws YarnException {\n+    conf.set(YarnConfiguration.NM_FPGA_AVAILABLE_DEVICES,\n+        \"acl0/243:0,acl1/244:1\");\n+\n+    fpgaDiscoverer.initialize(conf);\n+    List<FpgaDevice> devices = fpgaDiscoverer.discover();\n+    List<FpgaDevice> currentFpgaInfo = fpgaDiscoverer.getCurrentFpgaInfo();\n+\n+    assertEquals(\"Devices\", devices, currentFpgaInfo);\n+  }\n+\n+  @Test\n+  public void testCurrentFpgaInfoWhenAllowedDevicesDefined()\n+      throws YarnException {\n+    conf.set(YarnConfiguration.NM_FPGA_AVAILABLE_DEVICES,\n+        \"acl0/243:0,acl1/244:1\");\n+    conf.set(YarnConfiguration.NM_FPGA_ALLOWED_DEVICES, \"0\");\n+\n+    fpgaDiscoverer.initialize(conf);\n+    List<FpgaDevice> devices = fpgaDiscoverer.discover();\n+    List<FpgaDevice> currentFpgaInfo = fpgaDiscoverer.getCurrentFpgaInfo();\n+\n+    assertEquals(\"Devices\", devices, currentFpgaInfo);\n+    assertEquals(\"List of devices\", 1, currentFpgaInfo.size());\n+\n+    FpgaDevice device = currentFpgaInfo.get(0);\n+    assertEquals(\"Device id\", \"acl0\", device.getAliasDevName());\n+    assertEquals(\"Minor number\", 0, device.getMinor());\n+    assertEquals(\"Major\", 243, device.getMajor());\n+  }\n+\n   private IntelFpgaOpenclPlugin.InnerShellExecutor mockPuginShell() {\n     IntelFpgaOpenclPlugin.InnerShellExecutor shell = mock(IntelFpgaOpenclPlugin.InnerShellExecutor.class);\n     when(shell.runDiagnose(anyString(),anyInt())).thenReturn(\"\");",
                "raw_url": "https://github.com/apache/hadoop/raw/606061aa147dc6d619d6240b7ea31d8f8f220e5d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/fpga/TestFpgaDiscoverer.java",
                "sha": "6f570c6f63d639b1f99e49798f435220c62fe189",
                "status": "modified"
            }
        ],
        "message": "YARN-9595. FPGA plugin: NullPointerException in FpgaNodeResourceUpdateHandler.updateConfiguredResource(). Contributed by Peter Bacsko.",
        "parent": "https://github.com/apache/hadoop/commit/277e9a835b5b45af8df70b0dca52c03074f0d6b5",
        "patched_files": [
            "FpgaDiscoverer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFpgaDiscoverer.java"
        ]
    },
    "hadoop_60cbcff": {
        "bug_id": "hadoop_60cbcff",
        "commit": "https://github.com/apache/hadoop/commit/60cbcff2f7363e5cc386284981cac67abc965ee7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=60cbcff2f7363e5cc386284981cac67abc965ee7",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -270,6 +270,8 @@ Trunk (Unreleased)\n \n     HDFS-7581. HDFS documentation needs updating post-shell rewrite (aw)\n \n+    HDFS-7606. Fix potential NPE in INodeFile.getBlocks(). (Byron Wong via shv)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "6af1a52324d292c0d3ee474666def12ca8844c75",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java?ref=60cbcff2f7363e5cc386284981cac67abc965ee7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "patch": "@@ -432,7 +432,7 @@ final long getBlockDiskspace() {\n       return snapshotBlocks;\n     // Blocks are not in the current snapshot\n     // Find next snapshot with blocks present or return current file blocks\n-    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(diff.getSnapshotId());\n+    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(snapshot);\n     return (snapshotBlocks == null) ? getBlocks() : snapshotBlocks;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "sha": "e871bdc59b7f262728f3c670a4e75d0c6f8c5c76",
                "status": "modified"
            }
        ],
        "message": "HDFS-7606. Fix potential NPE in INodeFile.getBlocks(). Contributed by Byron Wong.",
        "parent": "https://github.com/apache/hadoop/commit/ec4389cf7270cff4cc96313b4190422ea7c70ced",
        "patched_files": [
            "INodeFile.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestINodeFile.java"
        ]
    },
    "hadoop_6124439": {
        "bug_id": "hadoop_6124439",
        "commit": "https://github.com/apache/hadoop/commit/612443951b1a950d463873d8ecff198b6252c25c",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java?ref=612443951b1a950d463873d8ecff198b6252c25c",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "patch": "@@ -202,8 +202,12 @@ public static String uriToString(URI[] uris){\n   }\n   \n   /**\n-   * \n    * @param str\n+   *          The string array to be parsed into an URI array.\n+   * @return <tt>null</tt> if str is <tt>null</tt>, else the URI array\n+   *         equivalent to str.\n+   * @throws IllegalArgumentException\n+   *           If any string in str violates RFC&nbsp;2396.\n    */\n   public static URI[] stringToURI(String[] str){\n     if (str == null) \n@@ -213,9 +217,8 @@ public static String uriToString(URI[] uris){\n       try{\n         uris[i] = new URI(str[i]);\n       }catch(URISyntaxException ur){\n-        System.out.println(\"Exception in specified URI's \" + StringUtils.stringifyException(ur));\n-        //making sure its asssigned to null in case of an error\n-        uris[i] = null;\n+        throw new IllegalArgumentException(\n+            \"Failed to create uri for \" + str[i], ur);\n       }\n     }\n     return uris;",
                "raw_url": "https://github.com/apache/hadoop/raw/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "sha": "67a8f82d9382cfd19d0c803144248c213359a0be",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java?ref=612443951b1a950d463873d8ecff198b6252c25c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "patch": "@@ -269,6 +269,17 @@ public void testCamelize() {\n     assertEquals(\"Yy\", StringUtils.camelize(\"yY\"));\n     assertEquals(\"Zz\", StringUtils.camelize(\"zZ\"));\n   }\n+  \n+  @Test\n+  public void testStringToURI() {\n+    String[] str = new String[] { \"file://\" };\n+    try {\n+      StringUtils.stringToURI(str);\n+      fail(\"Ignoring URISyntaxException while creating URI from string file://\");\n+    } catch (IllegalArgumentException iae) {\n+      assertEquals(\"Failed to create uri for file://\", iae.getMessage());\n+    }\n+  }\n \n   // Benchmark for StringUtils split\n   public static void main(String []args) {",
                "raw_url": "https://github.com/apache/hadoop/raw/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "sha": "fc90984608fb1a2630184419a840177223e9cb12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/612443951b1a950d463873d8ecff198b6252c25c/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=612443951b1a950d463873d8ecff198b6252c25c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -236,6 +236,9 @@ Branch-2 ( Unreleased changes )\n     HADOOP-8499. Lower min.user.id to 500 for the tests.\n     (Colin Patrick McCabe via eli)\n \n+    MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager\n+    #determineTimestamps (Bhallamudi via bobby)\n+\n Release 2.0.0-alpha - 05-23-2012\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/612443951b1a950d463873d8ecff198b6252c25c/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "db352254f14824b21d8b6f84f401ed03722e859e",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager#determineTimestamps (Bhallamudi via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362052 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0f122c209d7346e7913907dec86aa8cf221dd8f2",
        "patched_files": [
            "StringUtils.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestStringUtils.java"
        ]
    },
    "hadoop_614af50": {
        "bug_id": "hadoop_614af50",
        "commit": "https://github.com/apache/hadoop/commit/614af50625a8495812dce8da59db0e1aef40b1c0",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/614af50625a8495812dce8da59db0e1aef40b1c0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java?ref=614af50625a8495812dce8da59db0e1aef40b1c0",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "patch": "@@ -1040,20 +1040,27 @@ public SchedulerNode getNode(NodeId nodeId) {\n     for (Map.Entry<ApplicationId, ContainerStatus> c : updateExistContainers) {\n       SchedulerApplication<T> app = applications.get(c.getKey());\n       ContainerId containerId = c.getValue().getContainerId();\n+      if (app == null || app.getCurrentAppAttempt() == null) {\n+        continue;\n+      }\n+      RMContainer rmContainer\n+          = app.getCurrentAppAttempt().getRMContainer(containerId);\n+      if (rmContainer == null) {\n+        continue;\n+      }\n+      // exposed ports are already set for the container, skip\n+      if (rmContainer.getExposedPorts() != null &&\n+          rmContainer.getExposedPorts().size() > 0) {\n+        continue;\n+      }\n+\n       String strExposedPorts = c.getValue().getExposedPorts();\n-      Map<String, List<Map<String, String>>> exposedPorts = null;\n       if (null != strExposedPorts && !strExposedPorts.isEmpty()) {\n         Gson gson = new Gson();\n-        exposedPorts = gson.fromJson(strExposedPorts,\n+        Map<String, List<Map<String, String>>> exposedPorts =\n+            gson.fromJson(strExposedPorts,\n             new TypeToken<Map<String, List<Map<String, String>>>>()\n-            {}.getType());\n-      }\n-\n-      RMContainer rmContainer\n-          = app.getCurrentAppAttempt().getRMContainer(containerId);\n-      if (null != rmContainer &&\n-          (null == rmContainer.getExposedPorts()\n-              || rmContainer.getExposedPorts().size() == 0)) {\n+                {}.getType());\n         LOG.info(\"update exist container \" + containerId.getContainerId()\n             + \", strExposedPorts = \" + strExposedPorts);\n         rmContainer.setExposedPorts(exposedPorts);",
                "raw_url": "https://github.com/apache/hadoop/raw/614af50625a8495812dce8da59db0e1aef40b1c0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "sha": "a798b97af5fa4ade6531c71a34c8405973359536",
                "status": "modified"
            }
        ],
        "message": "YARN-9179. Fix NPE in AbstractYarnScheduler#updateNewContainerInfo.",
        "parent": "https://github.com/apache/hadoop/commit/05c84ab01c08d37457bb5aa40df61f45c7ed5fd4",
        "patched_files": [
            "AbstractYarnScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAbstractYarnScheduler.java"
        ]
    },
    "hadoop_6243eab": {
        "bug_id": "hadoop_6243eab",
        "commit": "https://github.com/apache/hadoop/commit/6243eabb48390fffada2418ade5adf9e0766afbe",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=6243eabb48390fffada2418ade5adf9e0766afbe",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1038,9 +1038,9 @@ private LocatedBlock createLocatedBlock(final BlockInfo blk, final long pos)\n     }\n \n     final int numNodes = blocksMap.numNodes(blk);\n-    final boolean isCorrupt = numCorruptNodes != 0 &&\n-        numCorruptNodes == numNodes;\n-    final int numMachines = isCorrupt ? numNodes: numNodes - numCorruptNodes;\n+    final boolean isCorrupt = numCorruptReplicas != 0 &&\n+        numCorruptReplicas == numNodes;\n+    final int numMachines = isCorrupt ? numNodes: numNodes - numCorruptReplicas;\n     final DatanodeStorageInfo[] machines = new DatanodeStorageInfo[numMachines];\n     final byte[] blockIndices = blk.isStriped() ? new byte[numMachines] : null;\n     int j = 0, i = 0;\n@@ -1366,11 +1366,22 @@ public void findAndMarkBlockAsCorrupt(final ExtendedBlock blk,\n           + \" as corrupt because datanode \" + dn + \" (\" + dn.getDatanodeUuid()\n           + \") does not exist\");\n     }\n-    \n+    DatanodeStorageInfo storage = null;\n+    if (storageID != null) {\n+      storage = node.getStorageInfo(storageID);\n+    }\n+    if (storage == null) {\n+      storage = storedBlock.findStorageInfo(node);\n+    }\n+\n+    if (storage == null) {\n+      blockLog.debug(\"BLOCK* findAndMarkBlockAsCorrupt: {} not found on {}\",\n+          blk, dn);\n+      return;\n+    }\n     markBlockAsCorrupt(new BlockToMarkCorrupt(reportedBlock, storedBlock,\n             blk.getGenerationStamp(), reason, Reason.CORRUPTION_REPORTED),\n-        storageID == null ? null : node.getStorageInfo(storageID),\n-        node);\n+        storage, node);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "accfc38f0b3c8e85ff3568efd763737fac498321",
                "status": "modified"
            },
            {
                "additions": 86,
                "blob_url": "https://github.com/apache/hadoop/blob/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java",
                "changes": 87,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java?ref=6243eabb48390fffada2418ade5adf9e0766afbe",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java",
                "patch": "@@ -18,15 +18,22 @@\n \n package org.apache.hadoop.hdfs;\n \n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n import java.io.DataInputStream;\n import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n import java.io.FileOutputStream;\n import java.util.ArrayList;\n+import java.util.HashSet;\n import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.ChecksumException;\n@@ -36,6 +43,8 @@\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs.BlockReportReplica;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n+import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;\n import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.DataNodeTestUtils;\n import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n@@ -167,7 +176,83 @@ public void testArrayOutOfBoundsException() throws Exception {\n       }\n     }\n   }\n-  \n+\n+  @Test\n+  public void testCorruptionWithDiskFailure() throws Exception {\n+    MiniDFSCluster cluster = null;\n+    try {\n+      Configuration conf = new HdfsConfiguration();\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+      cluster.waitActive();\n+      BlockManager bm = cluster.getNamesystem().getBlockManager();\n+      FileSystem fs = cluster.getFileSystem();\n+      final Path FILE_PATH = new Path(\"/tmp.txt\");\n+      final long FILE_LEN = 1L;\n+      DFSTestUtil.createFile(fs, FILE_PATH, FILE_LEN, (short) 3, 1L);\n+\n+      // get the block\n+      final String bpid = cluster.getNamesystem().getBlockPoolId();\n+      File storageDir = cluster.getInstanceStorageDir(0, 0);\n+      File dataDir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);\n+      assertTrue(\"Data directory does not exist\", dataDir.exists());\n+      ExtendedBlock blk = getFirstBlock(cluster.getDataNodes().get(0), bpid);\n+      if (blk == null) {\n+        blk = getFirstBlock(cluster.getDataNodes().get(0), bpid);\n+      }\n+      assertFalse(\"Data directory does not contain any blocks or there was an\" +\n+          \" \" +\n+          \"IO error\", blk == null);\n+      ArrayList<DataNode> datanodes = cluster.getDataNodes();\n+      assertEquals(datanodes.size(), 3);\n+      FSNamesystem ns = cluster.getNamesystem();\n+      //fail the storage on that node which has the block\n+      try {\n+        ns.writeLock();\n+        updateAllStorages(bm);\n+      } finally {\n+        ns.writeUnlock();\n+      }\n+      ns.writeLock();\n+      try {\n+        markAllBlocksAsCorrupt(bm, blk);\n+      } finally {\n+        ns.writeUnlock();\n+      }\n+\n+      // open the file\n+      fs.open(FILE_PATH);\n+\n+      //clean up\n+      fs.delete(FILE_PATH, false);\n+    } finally {\n+      if (cluster != null) { cluster.shutdown(); }\n+    }\n+\n+  }\n+\n+  private void markAllBlocksAsCorrupt(BlockManager bm,\n+                                      ExtendedBlock blk) throws IOException {\n+    for (DatanodeStorageInfo info : bm.getStorages(blk.getLocalBlock())) {\n+      bm.findAndMarkBlockAsCorrupt(\n+          blk, info.getDatanodeDescriptor(), info.getStorageID(), \"STORAGE_ID\");\n+    }\n+  }\n+\n+  private void updateAllStorages(BlockManager bm) {\n+    for (DatanodeDescriptor dd : bm.getDatanodeManager().getDatanodes()) {\n+      Set<DatanodeStorageInfo> setInfos = new HashSet<DatanodeStorageInfo>();\n+      DatanodeStorageInfo[] infos = dd.getStorageInfos();\n+      Random random = new Random();\n+      for (int i = 0; i < infos.length; i++) {\n+        int blkId = random.nextInt(101);\n+        DatanodeStorage storage = new DatanodeStorage(Integer.toString(blkId),\n+            DatanodeStorage.State.FAILED, StorageType.DISK);\n+        infos[i].updateFromStorage(storage);\n+        setInfos.add(infos[i]);\n+      }\n+    }\n+  }\n+\n   private static ExtendedBlock getFirstBlock(DataNode dn, String bpid) {\n     Map<DatanodeStorage, BlockListAsLongs> blockReports =\n         dn.getFSDataset().getBlockReports(bpid);",
                "raw_url": "https://github.com/apache/hadoop/raw/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java",
                "sha": "011baa1ea3ab2ded7fe82c7bb26f08eb3cb2f21b",
                "status": "modified"
            }
        ],
        "message": "HDFS-9958. BlockManager#createLocatedBlocks can throw NPE for corruptBlocks on failed storages. Contributed by Kuhu Shukla",
        "parent": "https://github.com/apache/hadoop/commit/cf2ee45f71f3326c4b5b3367ef534c0277392fc1",
        "patched_files": [
            "BlockManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestFileCorruption.java"
        ]
    },
    "hadoop_6285cbb": {
        "bug_id": "hadoop_6285cbb",
        "commit": "https://github.com/apache/hadoop/commit/6285cbb4990736ed25cea739c421fcfe0cd9d591",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6285cbb4990736ed25cea739c421fcfe0cd9d591/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=6285cbb4990736ed25cea739c421fcfe0cd9d591",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -289,6 +289,9 @@ Trunk (unreleased changes)\n     HADOOP-6991.  Fix SequenceFile::Reader to honor file lengths and call\n     openFile (cdouglas via omalley)\n \n+    HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.\n+    (Aaron T. Myers via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop/raw/6285cbb4990736ed25cea739c421fcfe0cd9d591/CHANGES.txt",
                "sha": "e607a0c1062c4eef025df0df61256d286a37a9af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/6285cbb4990736ed25cea739c421fcfe0cd9d591/src/java/org/apache/hadoop/security/KerberosName.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/KerberosName.java?ref=6285cbb4990736ed25cea739c421fcfe0cd9d591",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/security/KerberosName.java",
                "patch": "@@ -399,9 +399,10 @@ static void printRules() throws IOException {\n   }\n \n   public static void main(String[] args) throws Exception {\n+    setConfiguration(new Configuration());\n     for(String arg: args) {\n       KerberosName name = new KerberosName(arg);\n       System.out.println(\"Name: \" + name + \" to \" + name.getShortName());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/6285cbb4990736ed25cea739c421fcfe0cd9d591/src/java/org/apache/hadoop/security/KerberosName.java",
                "sha": "b533cd22f77d6dbf0aaa0a42edf6338dc6d987c6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.  Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1028938 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0c462b223f151208ff7bd5148cee0e436c23d795",
        "patched_files": [
            "KerberosName.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestKerberosName.java"
        ]
    },
    "hadoop_62deab1": {
        "bug_id": "hadoop_62deab1",
        "commit": "https://github.com/apache/hadoop/commit/62deab17a33cef723d73f8d8b9e37e5bddbc1813",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/62deab17a33cef723d73f8d8b9e37e5bddbc1813/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=62deab17a33cef723d73f8d8b9e37e5bddbc1813",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -682,6 +682,10 @@ public static UserGroupInformation getRemoteUser() throws IOException {\n   @Override\n   public void verifyToken(DelegationTokenIdentifier id, byte[] password)\n       throws IOException {\n+    // during startup namesystem is null, let client retry\n+    if (namesystem == null) {\n+      throw new RetriableException(\"Namenode is in startup mode\");\n+    }\n     namesystem.verifyToken(id, password);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/62deab17a33cef723d73f8d8b9e37e5bddbc1813/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "c3107d5317f9e2fbee53d9cd4e134f209ad8f529",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hadoop/blob/62deab17a33cef723d73f8d8b9e37e5bddbc1813/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=62deab17a33cef723d73f8d8b9e37e5bddbc1813",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -21,12 +21,14 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n+import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.hdfs.web.resources.DoAsParam;\n import org.apache.hadoop.hdfs.web.resources.UserParam;\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.DataOutputBuffer;\n import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.ipc.RetriableException;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;\n import org.apache.hadoop.security.authorize.AuthorizationException;\n@@ -36,9 +38,11 @@\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.TokenIdentifier;\n import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager;\n+import org.apache.hadoop.test.LambdaTestUtils;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n import javax.servlet.ServletContext;\n import javax.servlet.http.HttpServletRequest;\n@@ -371,8 +375,38 @@ public void testGetProxyUgi() throws IOException {\n     }\n   }\n \n+  @Test\n+  public void testGetUgiDuringStartup() throws Exception {\n+    conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n+    ServletContext context = mock(ServletContext.class);\n+    String realUser = \"TheDoctor\";\n+    String user = \"TheNurse\";\n+    conf.set(DFSConfigKeys.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n+    UserGroupInformation.setConfiguration(conf);\n+    HttpServletRequest request;\n \n+    Text ownerText = new Text(user);\n+    DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(\n+        ownerText, ownerText, new Text(realUser));\n+    Token<DelegationTokenIdentifier> token =\n+        new Token<DelegationTokenIdentifier>(dtId,\n+            new DummySecretManager(0, 0, 0, 0));\n+    String tokenString = token.encodeToUrlString();\n \n+    // token with auth-ed user\n+    request = getMockRequest(realUser, null, null);\n+    when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n+        tokenString);\n+\n+    NameNode mockNN = mock(NameNode.class);\n+    Mockito.doCallRealMethod().when(mockNN)\n+        .verifyToken(Mockito.any(), Mockito.any());\n+    when(context.getAttribute(\"name.node\")).thenReturn(mockNN);\n+\n+    LambdaTestUtils.intercept(RetriableException.class,\n+        \"Namenode is in startup mode\",\n+        () -> JspHelper.getUGI(context, request, conf));\n+  }\n \n   private HttpServletRequest getMockRequest(String remoteUser, String user, String doAs) {\n     HttpServletRequest request = mock(HttpServletRequest.class);",
                "raw_url": "https://github.com/apache/hadoop/raw/62deab17a33cef723d73f8d8b9e37e5bddbc1813/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "1aff7669e9823347bf1071ce73e094cc238e9e4d",
                "status": "modified"
            }
        ],
        "message": "HDFS-14647. NPE during secure namenode startup. Contributed by Fengnan Li.",
        "parent": "https://github.com/apache/hadoop/commit/9b8b3acb0a2b87356056c23f3d0f30a97a38cd3d",
        "patched_files": [
            "JspHelper.java",
            "NameNode.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJspHelper.java"
        ]
    },
    "hadoop_63e08ec": {
        "bug_id": "hadoop_63e08ec",
        "commit": "https://github.com/apache/hadoop/commit/63e08ec071852640babea9e39780327a0907712a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java?ref=63e08ec071852640babea9e39780327a0907712a",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java",
                "patch": "@@ -3532,7 +3532,8 @@ static boolean validateAuthUserWithEntityUser(\n   static boolean checkAccess(TimelineReaderManager readerManager,\n       UserGroupInformation ugi, String entityUser) {\n     if (isDisplayEntityPerUserFilterEnabled(readerManager.getConfig())) {\n-      if (!validateAuthUserWithEntityUser(readerManager, ugi, entityUser)) {\n+      if (ugi != null && !validateAuthUserWithEntityUser(readerManager, ugi,\n+          entityUser)) {\n         String userName = ugi.getShortUserName();\n         String msg = \"User \" + userName\n             + \" is not allowed to read TimelineService V2 data.\";",
                "raw_url": "https://github.com/apache/hadoop/raw/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java",
                "sha": "b10b705bb61f0a0818da6adf586ad70bee11f4c6",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java?ref=63e08ec071852640babea9e39780327a0907712a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java",
                "patch": "@@ -88,6 +88,10 @@\n     Assert.assertFalse(TimelineReaderWebServices\n         .validateAuthUserWithEntityUser(manager, null, user1));\n \n+    // true because ugi is null\n+    Assert.assertTrue(\n+        TimelineReaderWebServices.checkAccess(manager, null, user1));\n+\n     // incoming ugi is admin asking for entity owner user1\n     Assert.assertTrue(\n         TimelineReaderWebServices.checkAccess(manager, adminUgi, user1));",
                "raw_url": "https://github.com/apache/hadoop/raw/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java",
                "sha": "6651457ff7592bc96270845f032521d2c2e6dea6",
                "status": "modified"
            }
        ],
        "message": "YARN-8591. [ATSv2] NPE while checking for entity acl in non-secure cluster. Contributed by Rohith Sharma K S.",
        "parent": "https://github.com/apache/hadoop/commit/0857f116b754d83d3c540cd6f989087af24fef27",
        "patched_files": [
            "TimelineReaderWebServices.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestTimelineReaderWebServicesBasicAcl.java",
            "TestTimelineReaderWebServices.java"
        ]
    },
    "hadoop_645d67b": {
        "bug_id": "hadoop_645d67b",
        "commit": "https://github.com/apache/hadoop/commit/645d67bc4f4e29d10ef810386c89e6a7c8c61862",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/645d67bc4f4e29d10ef810386c89e6a7c8c61862/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java?ref=645d67bc4f4e29d10ef810386c89e6a7c8c61862",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java",
                "patch": "@@ -123,8 +123,8 @@ public ThrottledAsyncChecker(final Timer timer,\n       return Optional.empty();\n     }\n \n-    if (completedChecks.containsKey(target)) {\n-      final LastCheckResult<V> result = completedChecks.get(target);\n+    final LastCheckResult<V> result = completedChecks.get(target);\n+    if (result != null) {\n       final long msSinceLastCheck = timer.monotonicNow() - result.completedAt;\n       if (msSinceLastCheck < minMsBetweenChecks) {\n         LOG.debug(\"Skipped checking {}. Time since last check {}ms \" +",
                "raw_url": "https://github.com/apache/hadoop/raw/645d67bc4f4e29d10ef810386c89e6a7c8c61862/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/checker/ThrottledAsyncChecker.java",
                "sha": "032379a4d12a148ef53213015feb3aa5b62bcd4e",
                "status": "modified"
            }
        ],
        "message": "HDFS-14074. DataNode runs async disk checks maybe throws NullPointerException, and DataNode failed to register to NameSpace.  Contributed by guangyi lu.",
        "parent": "https://github.com/apache/hadoop/commit/1524e2e6c52aba966cbbf1d8025ba165688ab9bb",
        "patched_files": [
            "ThrottledAsyncChecker.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestThrottledAsyncChecker.java"
        ]
    },
    "hadoop_6468071": {
        "bug_id": "hadoop_6468071",
        "commit": "https://github.com/apache/hadoop/commit/6468071f137e6d918a7b4799ad54558fa26b25ce",
        "file": [
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/hadoop/blob/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java?ref=6468071f137e6d918a7b4799ad54558fa26b25ce",
                "deletions": 89,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement;\n \n import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableSet;\n import org.apache.commons.collections.IteratorUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -238,110 +239,118 @@ private void validateAndSetSchedulingRequest(SchedulingRequest\n           \"Only GUARANTEED execution type is supported.\");\n     }\n \n-    PlacementConstraint constraint =\n-        newSchedulingRequest.getPlacementConstraint();\n-\n-    // We only accept SingleConstraint\n-    PlacementConstraint.AbstractConstraint ac = constraint.getConstraintExpr();\n-    if (!(ac instanceof PlacementConstraint.SingleConstraint)) {\n-      throwExceptionWithMetaInfo(\n-          \"Only accepts \" + PlacementConstraint.SingleConstraint.class.getName()\n-              + \" as constraint-expression. Rejecting the new added \"\n-              + \"constraint-expression.class=\" + ac.getClass().getName());\n-    }\n-\n-    PlacementConstraint.SingleConstraint singleConstraint =\n-        (PlacementConstraint.SingleConstraint) ac;\n-\n-    // Make sure it is an anti-affinity request (actually this implementation\n-    // should be able to support both affinity / anti-affinity without much\n-    // effort. Considering potential test effort required. Limit to\n-    // anti-affinity to intra-app and scope is node.\n-    if (!singleConstraint.getScope().equals(PlacementConstraints.NODE)) {\n-      throwExceptionWithMetaInfo(\n-          \"Only support scope=\" + PlacementConstraints.NODE\n-              + \"now. PlacementConstraint=\" + singleConstraint);\n-    }\n-\n-    if (singleConstraint.getMinCardinality() != 0\n-        || singleConstraint.getMaxCardinality() != 0) {\n-      throwExceptionWithMetaInfo(\n-          \"Only support anti-affinity, which is: minCardinality=0, \"\n-              + \"maxCardinality=1\");\n-    }\n-\n-    Set<PlacementConstraint.TargetExpression> targetExpressionSet =\n-        singleConstraint.getTargetExpressions();\n-    if (targetExpressionSet == null || targetExpressionSet.isEmpty()) {\n-      throwExceptionWithMetaInfo(\n-          \"TargetExpression should not be null or empty\");\n-    }\n-\n-    // Set node partition\n+    // Node partition\n     String nodePartition = null;\n-\n     // Target allocation tags\n     Set<String> targetAllocationTags = null;\n \n-    for (PlacementConstraint.TargetExpression targetExpression : targetExpressionSet) {\n-      // Handle node partition\n-      if (targetExpression.getTargetType().equals(\n-          PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE)) {\n-        // For node attribute target, we only support Partition now. And once\n-        // YARN-3409 is merged, we will support node attribute.\n-        if (!targetExpression.getTargetKey().equals(NODE_PARTITION)) {\n-          throwExceptionWithMetaInfo(\"When TargetType=\"\n-              + PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE\n-              + \" only \" + NODE_PARTITION + \" is accepted as TargetKey.\");\n-        }\n+    PlacementConstraint constraint =\n+        newSchedulingRequest.getPlacementConstraint();\n \n-        if (nodePartition != null) {\n-          // This means we have duplicated node partition entry inside placement\n-          // constraint, which might be set by mistake.\n-          throwExceptionWithMetaInfo(\n-              \"Only one node partition targetExpression is allowed\");\n-        }\n+    if (constraint != null) {\n+      // We only accept SingleConstraint\n+      PlacementConstraint.AbstractConstraint ac = constraint\n+          .getConstraintExpr();\n+      if (!(ac instanceof PlacementConstraint.SingleConstraint)) {\n+        throwExceptionWithMetaInfo(\"Only accepts \"\n+            + PlacementConstraint.SingleConstraint.class.getName()\n+                + \" as constraint-expression. Rejecting the new added \"\n+            + \"constraint-expression.class=\" + ac.getClass().getName());\n+      }\n \n-        Set<String> values = targetExpression.getTargetValues();\n-        if (values == null || values.isEmpty()) {\n-          nodePartition = RMNodeLabelsManager.NO_LABEL;\n-          continue;\n-        }\n+      PlacementConstraint.SingleConstraint singleConstraint =\n+          (PlacementConstraint.SingleConstraint) ac;\n+\n+      // Make sure it is an anti-affinity request (actually this implementation\n+      // should be able to support both affinity / anti-affinity without much\n+      // effort. Considering potential test effort required. Limit to\n+      // anti-affinity to intra-app and scope is node.\n+      if (!singleConstraint.getScope().equals(PlacementConstraints.NODE)) {\n+        throwExceptionWithMetaInfo(\n+            \"Only support scope=\" + PlacementConstraints.NODE\n+                + \"now. PlacementConstraint=\" + singleConstraint);\n+      }\n \n-        if (values.size() > 1) {\n-          throwExceptionWithMetaInfo(\"Inside one targetExpression, we only \"\n-              + \"support affinity to at most one node partition now\");\n-        }\n+      if (singleConstraint.getMinCardinality() != 0\n+          || singleConstraint.getMaxCardinality() != 0) {\n+        throwExceptionWithMetaInfo(\n+            \"Only support anti-affinity, which is: minCardinality=0, \"\n+                + \"maxCardinality=1\");\n+      }\n \n-        nodePartition = values.iterator().next();\n-      } else if (targetExpression.getTargetType().equals(\n-          PlacementConstraint.TargetExpression.TargetType.ALLOCATION_TAG)) {\n-        // Handle allocation tags\n-        if (targetAllocationTags != null) {\n-          // This means we have duplicated AllocationTag expressions entries\n-          // inside placement constraint, which might be set by mistake.\n-          throwExceptionWithMetaInfo(\n-              \"Only one AllocationTag targetExpression is allowed\");\n-        }\n+      Set<PlacementConstraint.TargetExpression> targetExpressionSet =\n+          singleConstraint.getTargetExpressions();\n+      if (targetExpressionSet == null || targetExpressionSet.isEmpty()) {\n+        throwExceptionWithMetaInfo(\n+            \"TargetExpression should not be null or empty\");\n+      }\n \n-        if (targetExpression.getTargetValues() == null || targetExpression\n-            .getTargetValues().isEmpty()) {\n-          throwExceptionWithMetaInfo(\"Failed to find allocation tags from \"\n-              + \"TargetExpressions or couldn't find self-app target.\");\n+      for (PlacementConstraint.TargetExpression targetExpression :\n+          targetExpressionSet) {\n+        // Handle node partition\n+        if (targetExpression.getTargetType().equals(\n+            PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE)) {\n+          // For node attribute target, we only support Partition now. And once\n+          // YARN-3409 is merged, we will support node attribute.\n+          if (!targetExpression.getTargetKey().equals(NODE_PARTITION)) {\n+            throwExceptionWithMetaInfo(\"When TargetType=\"\n+                + PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE\n+                + \" only \" + NODE_PARTITION + \" is accepted as TargetKey.\");\n+          }\n+\n+          if (nodePartition != null) {\n+            // This means we have duplicated node partition entry\n+            // inside placement constraint, which might be set by mistake.\n+            throwExceptionWithMetaInfo(\n+                \"Only one node partition targetExpression is allowed\");\n+          }\n+\n+          Set<String> values = targetExpression.getTargetValues();\n+          if (values == null || values.isEmpty()) {\n+            nodePartition = RMNodeLabelsManager.NO_LABEL;\n+            continue;\n+          }\n+\n+          if (values.size() > 1) {\n+            throwExceptionWithMetaInfo(\"Inside one targetExpression, we only \"\n+                + \"support affinity to at most one node partition now\");\n+          }\n+\n+          nodePartition = values.iterator().next();\n+        } else if (targetExpression.getTargetType().equals(\n+            PlacementConstraint.TargetExpression.TargetType.ALLOCATION_TAG)) {\n+          // Handle allocation tags\n+          if (targetAllocationTags != null) {\n+            // This means we have duplicated AllocationTag expressions entries\n+            // inside placement constraint, which might be set by mistake.\n+            throwExceptionWithMetaInfo(\n+                \"Only one AllocationTag targetExpression is allowed\");\n+          }\n+\n+          if (targetExpression.getTargetValues() == null ||\n+              targetExpression.getTargetValues().isEmpty()) {\n+            throwExceptionWithMetaInfo(\"Failed to find allocation tags from \"\n+                + \"TargetExpressions or couldn't find self-app target.\");\n+          }\n+\n+          targetAllocationTags = new HashSet<>(\n+              targetExpression.getTargetValues());\n         }\n+      }\n \n-        targetAllocationTags = new HashSet<>(\n-            targetExpression.getTargetValues());\n+      if (targetAllocationTags == null) {\n+        // That means we don't have ALLOCATION_TAG specified\n+        throwExceptionWithMetaInfo(\n+            \"Couldn't find target expression with type == ALLOCATION_TAG,\"\n+                + \" it is required to include one and only one target\"\n+                + \" expression with type == ALLOCATION_TAG\");\n       }\n     }\n \n+    // If this scheduling request doesn't contain a placement constraint,\n+    // we set allocation tags an empty set.\n     if (targetAllocationTags == null) {\n-      // That means we don't have ALLOCATION_TAG specified\n-      throwExceptionWithMetaInfo(\n-          \"Couldn't find target expression with type == ALLOCATION_TAG, it is \"\n-              + \"required to include one and only one target expression with \"\n-              + \"type == ALLOCATION_TAG\");\n-\n+      targetAllocationTags = ImmutableSet.of();\n     }\n \n     if (nodePartition == null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java",
                "sha": "2b610f2fe769ce0e5969ce1f86d29ded199868db",
                "status": "modified"
            },
            {
                "additions": 84,
                "blob_url": "https://github.com/apache/hadoop/blob/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java?ref=6468071f137e6d918a7b4799ad54558fa26b25ce",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java",
                "patch": "@@ -18,8 +18,15 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity;\n \n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest;\n+import org.apache.hadoop.yarn.api.records.ExecutionType;\n+import org.apache.hadoop.yarn.api.records.ExecutionTypeRequest;\n+import org.apache.hadoop.yarn.api.records.SchedulingRequest;\n+import org.apache.hadoop.yarn.api.resource.PlacementConstraint;\n+import org.apache.hadoop.yarn.api.resource.PlacementConstraints;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TargetApplicationsNamespace;\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n@@ -39,6 +46,8 @@\n import org.junit.Before;\n import org.junit.Test;\n \n+import static org.apache.hadoop.yarn.api.resource.PlacementConstraints.PlacementTargets.*;\n+\n public class TestSchedulingRequestContainerAllocation {\n   private final int GB = 1024;\n \n@@ -393,4 +402,79 @@ public RMNodeLabelsManager createNodeLabelManager() {\n     Assert.assertTrue(caughtException);\n     rm1.close();\n   }\n+\n+  @Test\n+  public void testSchedulingRequestWithNullConstraint() throws Exception {\n+    Configuration csConf = TestUtils.getConfigurationWithMultipleQueues(\n+        new Configuration());\n+    csConf.set(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_HANDLER,\n+        YarnConfiguration.SCHEDULER_RM_PLACEMENT_CONSTRAINTS_HANDLER);\n+\n+    // inject node label manager\n+    MockRM rm1 = new MockRM(csConf) {\n+      @Override\n+      public RMNodeLabelsManager createNodeLabelManager() {\n+        return mgr;\n+      }\n+    };\n+\n+    rm1.getRMContext().setNodeLabelManager(mgr);\n+    rm1.start();\n+\n+    // 4 NMs.\n+    MockNM[] nms = new MockNM[4];\n+    RMNode[] rmNodes = new RMNode[4];\n+    for (int i = 0; i < 4; i++) {\n+      nms[i] = rm1.registerNode(\"192.168.0.\" + i + \":1234\", 10 * GB);\n+      rmNodes[i] = rm1.getRMContext().getRMNodes().get(nms[i].getNodeId());\n+    }\n+\n+    // app1 -> c\n+    RMApp app1 = rm1.submitApp(1 * GB, \"app\", \"user\", null, \"c\");\n+    MockAM am1 = MockRM.launchAndRegisterAM(app1, rm1, nms[0]);\n+\n+    CapacityScheduler cs = (CapacityScheduler) rm1.getResourceScheduler();\n+\n+    PlacementConstraint constraint = PlacementConstraints\n+        .targetNotIn(\"node\", allocationTag(\"t1\"))\n+        .build();\n+    SchedulingRequest sc = SchedulingRequest\n+        .newInstance(0, Priority.newInstance(1),\n+            ExecutionTypeRequest.newInstance(ExecutionType.GUARANTEED),\n+            ImmutableSet.of(\"t1\"),\n+            ResourceSizing.newInstance(1, Resource.newInstance(1024, 1)),\n+            constraint);\n+    AllocateRequest request = AllocateRequest.newBuilder()\n+        .schedulingRequests(ImmutableList.of(sc)).build();\n+    am1.allocate(request);\n+\n+    for (int i = 0; i < 4; i++) {\n+      cs.handle(new NodeUpdateSchedulerEvent(rmNodes[i]));\n+    }\n+\n+    FiCaSchedulerApp schedApp = cs.getApplicationAttempt(\n+        am1.getApplicationAttemptId());\n+    Assert.assertEquals(2, schedApp.getLiveContainers().size());\n+\n+\n+    // Send another request with null placement constraint,\n+    // ensure there is no NPE while handling this request.\n+    sc = SchedulingRequest\n+        .newInstance(1, Priority.newInstance(1),\n+            ExecutionTypeRequest.newInstance(ExecutionType.GUARANTEED),\n+            ImmutableSet.of(\"t2\"),\n+            ResourceSizing.newInstance(2, Resource.newInstance(1024, 1)),\n+            null);\n+    AllocateRequest request1 = AllocateRequest.newBuilder()\n+        .schedulingRequests(ImmutableList.of(sc)).build();\n+    am1.allocate(request1);\n+\n+    for (int i = 0; i < 4; i++) {\n+      cs.handle(new NodeUpdateSchedulerEvent(rmNodes[i]));\n+    }\n+\n+    Assert.assertEquals(4, schedApp.getLiveContainers().size());\n+\n+    rm1.close();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java",
                "sha": "f23fd8f06f3b2628392252f99b6cf75beba8dd01",
                "status": "modified"
            }
        ],
        "message": "YARN-8367. Fix NPE in SingleConstraintAppPlacementAllocator when placement constraint in SchedulingRequest is null. Contributed by Weiwei Yang.",
        "parent": "https://github.com/apache/hadoop/commit/d1e2b8098078af4af31392ed7f2fa350a7d1c3b2",
        "patched_files": [
            "SingleConstraintAppPlacementAllocator.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSchedulingRequestContainerAllocation.java",
            "TestSingleConstraintAppPlacementAllocator.java"
        ]
    },
    "hadoop_648ba4a": {
        "bug_id": "hadoop_648ba4a",
        "commit": "https://github.com/apache/hadoop/commit/648ba4a36b568879f59b9b89dd58576773751caa",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/648ba4a36b568879f59b9b89dd58576773751caa/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=648ba4a36b568879f59b9b89dd58576773751caa",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -265,6 +265,9 @@ Release 2.0.3-alpha - Unreleased\n \n     HDFS-3964. Make NN log of fs.defaultFS debug rather than info. (eli)\n \n+    HDFS-3992. Method org.apache.hadoop.hdfs.TestHftpFileSystem.tearDown()\n+    sometimes throws NPEs. (Ivan A. Veselovsky via atm)\n+\n Release 2.0.2-alpha - 2012-09-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/648ba4a36b568879f59b9b89dd58576773751caa/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "931dea087408d9b23b477708e12b16fa1de234fc",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/648ba4a36b568879f59b9b89dd58576773751caa/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java?ref=648ba4a36b568879f59b9b89dd58576773751caa",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java",
                "patch": "@@ -102,9 +102,15 @@ public static void setUp() throws IOException {\n   \n   @AfterClass\n   public static void tearDown() throws IOException {\n-    hdfs.close();\n-    hftpFs.close();\n-    cluster.shutdown();\n+    if (hdfs != null) {\n+      hdfs.close();\n+    }\n+    if (hftpFs != null) {\n+      hftpFs.close();\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/648ba4a36b568879f59b9b89dd58576773751caa/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpFileSystem.java",
                "sha": "6cb0ad1ce83701d03dc77d8f30137f686637148a",
                "status": "modified"
            }
        ],
        "message": "HDFS-3992. Method org.apache.hadoop.hdfs.TestHftpFileSystem.tearDown() sometimes throws NPEs. Contributed by Ivan A. Veselovsky.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1391763 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/1ced82cc812b830cf755d2d300351ea92a0dc9a2",
        "patched_files": [
            "CHANGES.java",
            "HftpFileSystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestHftpFileSystem.java"
        ]
    },
    "hadoop_65b308f": {
        "bug_id": "hadoop_65b308f",
        "commit": "https://github.com/apache/hadoop/commit/65b308f7834c0770c7e062def0a67bf9a0e065e8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=65b308f7834c0770c7e062def0a67bf9a0e065e8",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -59,6 +59,9 @@ Release 2.1.0-alpha - Unreleased\n     YARN-79. Implement close on all clients to YARN so that RPC clients don't\n     throw exceptions on shut-down. (Vinod Kumar Vavilapalli)\n \n+    YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that\n+    NMs don't get NPEs on startup errors. (Devaraj K via vinodkv)\n+\n Release 0.23.4 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/CHANGES.txt",
                "sha": "8106a37f4a93cbc674e5f820b46b8ae0308f07c7",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java?ref=65b308f7834c0770c7e062def0a67bf9a0e065e8",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "patch": "@@ -79,16 +79,18 @@ public void init(Configuration conf) {\n \n   @Override\n   public void stop() {\n-    sched.shutdown();\n-    boolean isShutdown = false;\n-    try {\n-      isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n-    } catch (InterruptedException e) {\n-      sched.shutdownNow();\n-      isShutdown = true;\n-    }\n-    if (!isShutdown) {\n-      sched.shutdownNow();\n+    if (sched != null) {\n+      sched.shutdown();\n+      boolean isShutdown = false;\n+      try {\n+        isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n+      } catch (InterruptedException e) {\n+        sched.shutdownNow();\n+        isShutdown = true;\n+      }\n+      if (!isShutdown) {\n+        sched.shutdownNow();\n+      }\n     }\n     super.stop();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "sha": "7ec634b0ebe59ffd65ce3de334d67a3902be4e6e",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java?ref=65b308f7834c0770c7e062def0a67bf9a0e065e8",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "patch": "@@ -183,6 +183,24 @@ public void testDelayedDelete() {\n     verify(mockSched).schedule(any(Runnable.class), eq(10800l),\n         eq(TimeUnit.SECONDS));\n   }\n+  \n+  @Test\n+  public void testStop() throws Exception {\n+    NonAggregatingLogHandler aggregatingLogHandler = \n+        new NonAggregatingLogHandler(null, null, null);\n+\n+    // It should not throw NullPointerException\n+    aggregatingLogHandler.stop();\n+\n+    NonAggregatingLogHandlerWithMockExecutor logHandler = \n+        new NonAggregatingLogHandlerWithMockExecutor(null, null, null);\n+    logHandler.init(new Configuration());\n+    logHandler.stop();\n+    verify(logHandler.mockSched).shutdown();\n+    verify(logHandler.mockSched)\n+        .awaitTermination(eq(10l), eq(TimeUnit.SECONDS));\n+    verify(logHandler.mockSched).shutdownNow();\n+  }\n \n   private class NonAggregatingLogHandlerWithMockExecutor extends\n       NonAggregatingLogHandler {",
                "raw_url": "https://github.com/apache/hadoop/raw/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "sha": "36251e47e129c85380a287a665c8ca475ad71f27",
                "status": "modified"
            }
        ],
        "message": "YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that NMs don't get NPEs on startup errors. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1380954 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ab74b1addeaf36359f4dd300471e2b185792bcd5",
        "patched_files": [
            "CHANGES.java",
            "NonAggregatingLogHandler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNonAggregatingLogHandler.java"
        ]
    },
    "hadoop_6604131": {
        "bug_id": "hadoop_6604131",
        "commit": "https://github.com/apache/hadoop/commit/660413165aa25815bbba66ac2195b0ae17184844",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/660413165aa25815bbba66ac2195b0ae17184844/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=660413165aa25815bbba66ac2195b0ae17184844",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -620,13 +620,17 @@ private void setAppCollectorsMapToResponse(\n     Map<ApplicationId, RMApp> rmApps = rmContext.getRMApps();\n     // Set collectors for all running apps on this node.\n     for (ApplicationId appId : runningApps) {\n-      AppCollectorData appCollectorData = rmApps.get(appId).getCollectorData();\n-      if (appCollectorData != null) {\n-        liveAppCollectorsMap.put(appId, appCollectorData);\n-      } else {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Collector for applicaton: \" + appId +\n-              \" hasn't registered yet!\");\n+      RMApp app = rmApps.get(appId);\n+      if (app != null) {\n+        AppCollectorData appCollectorData = rmApps.get(appId)\n+            .getCollectorData();\n+        if (appCollectorData != null) {\n+          liveAppCollectorsMap.put(appId, appCollectorData);\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Collector for applicaton: \" + appId +\n+                \" hasn't registered yet!\");\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/660413165aa25815bbba66ac2195b0ae17184844/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "cc47e02cb19d53a1f149c594262db56682d1352b",
                "status": "modified"
            }
        ],
        "message": "YARN-6801. NPE in RM while setting collectors map in NodeHeartbeatResponse. Contributed by Vrushali C.",
        "parent": "https://github.com/apache/hadoop/commit/ac7f52df83d2b4758e7debe9416be7db0ec69d2b",
        "patched_files": [
            "ResourceTrackerService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_687ce1a": {
        "bug_id": "hadoop_687ce1a",
        "commit": "https://github.com/apache/hadoop/commit/687ce1a5fca2d58a781e7382bf0333a16d39839d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt?ref=687ce1a5fca2d58a781e7382bf0333a16d39839d",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "patch": "@@ -59,3 +59,6 @@ fs-encryption (Unreleased)\n   OPTIMIZATIONS\n \n   BUG FIXES\n+\n+    HDFS-6733. Creating encryption zone results in NPE when\n+    KeyProvider is null. (clamb)",
                "raw_url": "https://github.com/apache/hadoop/raw/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "sha": "c447d49b52ceab9ba1ad9515a622bb17810bf5ff",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=687ce1a5fca2d58a781e7382bf0333a16d39839d",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -8448,6 +8448,11 @@ void createEncryptionZone(final String src, String keyNameArg)\n     String keyName = keyNameArg;\n     boolean success = false;\n     try {\n+      if (provider == null) {\n+        throw new IOException(\n+            \"Can't create an encryption zone for \" + src +\n+            \" since no key provider is available.\");\n+      }\n       if (keyName == null || keyName.isEmpty()) {\n         keyName = UUID.randomUUID().toString();\n         createNewKey(keyName, src);",
                "raw_url": "https://github.com/apache/hadoop/raw/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "9b7cb1995839e75c07298a7dc62e449b1192b541",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java?ref=687ce1a5fca2d58a781e7382bf0333a16d39839d",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "patch": "@@ -68,6 +68,7 @@\n   private MiniDFSCluster cluster;\n   private HdfsAdmin dfsAdmin;\n   private DistributedFileSystem fs;\n+  private File testRootDir;\n \n   protected FileSystemTestWrapper fsWrapper;\n   protected FileContextTestWrapper fcWrapper;\n@@ -78,14 +79,14 @@ public void setup() throws IOException {\n     fsHelper = new FileSystemTestHelper();\n     // Set up java key store\n     String testRoot = fsHelper.getTestRootDir();\n-    File testRootDir = new File(testRoot).getAbsoluteFile();\n+    testRootDir = new File(testRoot).getAbsoluteFile();\n     conf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n         JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n     );\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n     Logger.getLogger(EncryptionZoneManager.class).setLevel(Level.TRACE);\n     fs = cluster.getFileSystem();\n-    fsWrapper = new FileSystemTestWrapper(cluster.getFileSystem());\n+    fsWrapper = new FileSystemTestWrapper(fs);\n     fcWrapper = new FileContextTestWrapper(\n         FileContext.getFileContext(cluster.getURI(), conf));\n     dfsAdmin = new HdfsAdmin(cluster.getURI(), conf);\n@@ -429,4 +430,25 @@ public void testCipherSuiteNegotiation() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 120000)\n+  public void testCreateEZWithNoProvider() throws Exception {\n+\n+    final Configuration clusterConf = cluster.getConfiguration(0);\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"\");\n+    cluster.restartNameNode(true);\n+    /* Test failure of create EZ on a directory that doesn't exist. */\n+    final Path zone1 = new Path(\"/zone1\");\n+    /* Normal creation of an EZ */\n+    fsWrapper.mkdir(zone1, FsPermission.getDirDefault(), true);\n+    try {\n+      dfsAdmin.createEncryptionZone(zone1, null);\n+      fail(\"expected exception\");\n+    } catch (IOException e) {\n+      assertExceptionContains(\"since no key provider is available\", e);\n+    }\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n+        JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n+    );\n+    cluster.restartNameNode(true);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "sha": "421396bdf3873e51886b7e34eaad6a0037162d89",
                "status": "modified"
            }
        ],
        "message": "HDFS-6733. Creating encryption zone results in NPE when KeyProvider is null. (clamb)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612843 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/69b75fca7aec5f5cbf79bc7db3915119cef69e65",
        "patched_files": [
            "CHANGES-fs-encryption.java",
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestEncryptionZones.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_6981b14": {
        "bug_id": "hadoop_6981b14",
        "commit": "https://github.com/apache/hadoop/commit/6981b14003d3ff99fd719515ac08d748fc5f44bd",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -509,8 +509,6 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n-    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n-\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4c01ec63631384b08b4b0ab66dbb7a2226b41f61",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -540,7 +540,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if (UserGroupInformation.isSecurityEnabled()) {\n+    if(UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -558,7 +558,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.verifyToken(id, token.getPassword());\n+        nn.getNamesystem().verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "c0da8779fd354d13c394ddf75046f8315df5b240",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5464,11 +5464,21 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+  \n   @Override\n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n-\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "571bde80b7469124187525f0d6e184c117aea607",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,7 +51,6 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n-import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -79,7 +78,6 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n-import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1285,18 +1283,7 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t);\n   }\n-\n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-\n+  \n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "d69328565e623774d335e906df61d5be62cab16e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "44b0437d131646de1483dcf1da4787c6e4c18c38",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -70,7 +69,6 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n-    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -81,8 +79,6 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n-    when(context.getAttribute(\n-        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "c7fafdb13bc8b2bdf26bf5ed53b3353552975e3c",
                "status": "modified"
            }
        ],
        "message": "Revert HDFS-3654. TestJspHelper#testGetUgi fails with NPE.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362759 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e2253b539e5d18b7006e8645a573659ca3e77699",
        "patched_files": [
            "FSNamesystem.java",
            "NameNode.java",
            "JspHelper.java",
            "CHANGES.java",
            "NameNodeHttpServer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJspHelper.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_69b903b": {
        "bug_id": "hadoop_69b903b",
        "commit": "https://github.com/apache/hadoop/commit/69b903bbd8e2dafac6b2cb1d748ea666b6f877cf",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/69b903bbd8e2dafac6b2cb1d748ea666b6f877cf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java?ref=69b903bbd8e2dafac6b2cb1d748ea666b6f877cf",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "patch": "@@ -803,8 +803,12 @@ void register(NamespaceInfo nsInfo) throws IOException {\n         sleepAndLogInterrupts(1000, \"connecting to server\");\n       }\n     }\n-    \n-    LOG.info(\"Block pool \" + this + \" successfully registered with NN\");\n+\n+    if (bpRegistration == null) {\n+      throw new IOException(\"DN shut down before block pool registered\");\n+    }\n+\n+    LOG.info(this + \" successfully registered with NN\");\n     bpos.registrationSucceeded(this, bpRegistration);\n \n     // reset lease id whenever registered to NN.",
                "raw_url": "https://github.com/apache/hadoop/raw/69b903bbd8e2dafac6b2cb1d748ea666b6f877cf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "sha": "83b0460e07d997427d204da44e3893b4e9298c29",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/69b903bbd8e2dafac6b2cb1d748ea666b6f877cf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDatanodeRegister.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDatanodeRegister.java?ref=69b903bbd8e2dafac6b2cb1d748ea666b6f877cf",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDatanodeRegister.java",
                "patch": "@@ -26,9 +26,20 @@\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.ArrayList;\n+import java.util.Collections;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+\n+import com.google.common.collect.Lists;\n+\n+import org.junit.Assert;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;\n import org.apache.hadoop.hdfs.server.common.IncorrectVersionException;\n@@ -118,4 +129,37 @@ public void testDifferentLayoutVersions() throws Exception {\n       fail(\"Should not fail to retrieve NS info from DN with different layout version\");\n     }\n   }\n+\n+  @Test\n+  public void testDNShutdwonBeforeRegister() throws Exception {\n+    final InetSocketAddress nnADDR = new InetSocketAddress(\n+        \"localhost\", 5020);\n+    Configuration conf = new HdfsConfiguration();\n+    conf.set(DFSConfigKeys.DFS_DATANODE_ADDRESS_KEY, \"0.0.0.0:0\");\n+    conf.set(DFSConfigKeys.DFS_DATANODE_HTTP_ADDRESS_KEY, \"0.0.0.0:0\");\n+    conf.set(DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_KEY, \"0.0.0.0:0\");\n+    FileSystem.setDefaultUri(conf,\n+        \"hdfs://\" + nnADDR.getHostName() + \":\" + nnADDR.getPort());\n+    ArrayList<StorageLocation> locations = new ArrayList<>();\n+    DataNode dn = new DataNode(conf, locations, null, null);\n+    BPOfferService bpos = new BPOfferService(\"test_ns\",\n+        Lists.newArrayList(\"nn0\"), Lists.newArrayList(nnADDR),\n+        Collections.<InetSocketAddress>nCopies(1, null), dn);\n+    DatanodeProtocolClientSideTranslatorPB fakeDnProt =\n+        mock(DatanodeProtocolClientSideTranslatorPB.class);\n+    when(fakeDnProt.versionRequest()).thenReturn(fakeNsInfo);\n+\n+    BPServiceActor localActor = new BPServiceActor(\"test\", \"test\",\n+        INVALID_ADDR, null, bpos);\n+    localActor.setNameNode(fakeDnProt);\n+    try {\n+      NamespaceInfo nsInfo = localActor.retrieveNamespaceInfo();\n+      bpos.setNamespaceInfo(nsInfo);\n+      localActor.stop();\n+      localActor.register(nsInfo);\n+    } catch (IOException e) {\n+      Assert.assertEquals(\"DN shut down before block pool registered\",\n+          e.getMessage());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/69b903bbd8e2dafac6b2cb1d748ea666b6f877cf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDatanodeRegister.java",
                "sha": "95a361a65dd26dd3e87fe5b5d540090f911e8229",
                "status": "modified"
            }
        ],
        "message": "HDFS-14372. NPE while DN is shutting down. Contributed by lujie.",
        "parent": "https://github.com/apache/hadoop/commit/e424392a62418fad401fe80bf6517e375911c08c",
        "patched_files": [
            "BPServiceActor.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDatanodeRegister.java"
        ]
    },
    "hadoop_69f7277": {
        "bug_id": "hadoop_69f7277",
        "commit": "https://github.com/apache/hadoop/commit/69f7277625b86a30a5964285d05dac4ba982e795",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/69f7277625b86a30a5964285d05dac4ba982e795/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java?ref=69f7277625b86a30a5964285d05dac4ba982e795",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java",
                "patch": "@@ -25,6 +25,7 @@\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+import com.google.common.base.Supplier;\n import java.io.File;\n import java.io.FileInputStream;\n import java.io.IOException;\n@@ -96,6 +97,7 @@\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.util.Clock;\n import org.apache.hadoop.yarn.util.SystemClock;\n import org.junit.Test;\n@@ -1196,6 +1198,12 @@ public void testSpeculative() throws Exception {\n     TaskAttempt task1Attempt2 = t1it.next();\n     TaskAttempt task2Attempt = mapTask2.getAttempts().values().iterator().next();\n \n+    // wait for the second task attempt to be assigned.\n+    GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+      @Override public Boolean get() {\n+        return task1Attempt2.getAssignedContainerID() != null;\n+      }\n+    }, 10, 10000);\n     ContainerId t1a2contId = task1Attempt2.getAssignedContainerID();\n \n     LOG.info(t1a2contId.toString());",
                "raw_url": "https://github.com/apache/hadoop/raw/69f7277625b86a30a5964285d05dac4ba982e795/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java",
                "sha": "3ede2e92d514ce2e01bc0356719a9ff9a1e732e4",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6768. TestRecovery.testSpeculative failed with NPE. Contributed by Haibo Chen",
        "parent": "https://github.com/apache/hadoop/commit/6742fb6e68d349055f985eb640d845e689d75384",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "TestRecovery.java"
        ]
    },
    "hadoop_6a7c030": {
        "bug_id": "hadoop_6a7c030",
        "commit": "https://github.com/apache/hadoop/commit/6a7c0306bd9a600a943abb61606aaaf4f9beafee",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6a7c0306bd9a600a943abb61606aaaf4f9beafee/hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hdfs/CHANGES.txt?ref=6a7c0306bd9a600a943abb61606aaaf4f9beafee",
                "deletions": 0,
                "filename": "hdfs/CHANGES.txt",
                "patch": "@@ -951,6 +951,9 @@ Trunk (unreleased changes)\n     HDFS-2196. Make ant build system work with hadoop-common JAR generated\n     by Maven. (Alejandro Abdelnur via tomwhite)\n \n+    HDFS-2245. Fix a NullPointerException in BlockManager.chooseTarget(..).\n+    (szetszwo)\n+\n   BREAKDOWN OF HDFS-1073 SUBTASKS\n \n     HDFS-1521. Persist transaction ID on disk between NN restarts.",
                "raw_url": "https://github.com/apache/hadoop/raw/6a7c0306bd9a600a943abb61606aaaf4f9beafee/hdfs/CHANGES.txt",
                "sha": "ff0ba15b050cb1f618d6bcf9d6b0348278b347a8",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/6a7c0306bd9a600a943abb61606aaaf4f9beafee/hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=6a7c0306bd9a600a943abb61606aaaf4f9beafee",
                "deletions": 6,
                "filename": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1221,12 +1221,13 @@ private boolean computeReplicationWorkForBlock(Block block, int priority) {\n     final DatanodeDescriptor targets[] = blockplacement.chooseTarget(\n         src, numOfReplicas, client, excludedNodes, blocksize);\n     if (targets.length < minReplication) {\n-      throw new IOException(\"File \" + src + \" could only be replicated to \" +\n-                            targets.length + \" nodes, instead of \" +\n-                            minReplication + \". There are \"\n-                            + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n-                            + \" datanode(s) running but \"+excludedNodes.size() +\n-                            \" node(s) are excluded in this operation.\");\n+      throw new IOException(\"File \" + src + \" could only be replicated to \"\n+          + targets.length + \" nodes instead of minReplication (=\"\n+          + minReplication + \").  There are \"\n+          + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n+          + \" datanode(s) running and \"\n+          + (excludedNodes == null? \"no\": excludedNodes.size())\n+          + \" node(s) are excluded in this operation.\");\n     }\n     return targets;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/6a7c0306bd9a600a943abb61606aaaf4f9beafee/hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "f60530b4cc40b36c154b06a7d740b48216925864",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/6a7c0306bd9a600a943abb61606aaaf4f9beafee/hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java?ref=6a7c0306bd9a600a943abb61606aaaf4f9beafee",
                "deletions": 0,
                "filename": "hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java",
                "patch": "@@ -404,6 +404,36 @@ public void testFileCreationError2() throws IOException {\n     }\n   }\n \n+  /** test addBlock(..) when replication<min and excludeNodes==null. */\n+  public void testFileCreationError3() throws IOException {\n+    System.out.println(\"testFileCreationError3 start\");\n+    Configuration conf = new HdfsConfiguration();\n+    // create cluster\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+    DistributedFileSystem dfs = null;\n+    try {\n+      cluster.waitActive();\n+      dfs = (DistributedFileSystem)cluster.getFileSystem();\n+      DFSClient client = dfs.dfs;\n+\n+      // create a new file.\n+      final Path f = new Path(\"/foo.txt\");\n+      createFile(dfs, f, 3);\n+      try {\n+        cluster.getNameNode().addBlock(f.toString(), \n+            client.clientName, null, null);\n+        fail();\n+      } catch(IOException ioe) {\n+        FileSystem.LOG.info(\"GOOD!\", ioe);\n+      }\n+\n+      System.out.println(\"testFileCreationError3 successful\");\n+    } finally {\n+      IOUtils.closeStream(dfs);\n+      cluster.shutdown();\n+    }\n+  }\n+\n   /**\n    * Test that file leases are persisted across namenode restarts.\n    * This test is currently not triggered because more HDFS work is ",
                "raw_url": "https://github.com/apache/hadoop/raw/6a7c0306bd9a600a943abb61606aaaf4f9beafee/hdfs/src/test/hdfs/org/apache/hadoop/hdfs/TestFileCreation.java",
                "sha": "d2dfd7fc65e9aa9599969a6370322e602f9de383",
                "status": "modified"
            }
        ],
        "message": "HDFS-2245. Fix a NullPointerException in BlockManager.chooseTarget(..).\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1156490 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/cde987996ae727154b5081bf0a76e10c7c236118",
        "patched_files": [
            "BlockManager.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFileCreation.java"
        ]
    },
    "hadoop_6aa6345": {
        "bug_id": "hadoop_6aa6345",
        "commit": "https://github.com/apache/hadoop/commit/6aa63452b358cf0ba8687ec89b5d6c3a99649bc5",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/6aa63452b358cf0ba8687ec89b5d6c3a99649bc5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageHandler.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageHandler.java?ref=6aa63452b358cf0ba8687ec89b5d6c3a99649bc5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageHandler.java",
                "patch": "@@ -85,10 +85,15 @@ public void channelRead0(ChannelHandlerContext ctx, HttpRequest request)\n     }\n \n     QueryStringDecoder decoder = new QueryStringDecoder(request.getUri());\n+    // check path. throw exception if path doesn't start with WEBHDFS_PREFIX\n+    String path = getPath(decoder);\n     final String op = getOp(decoder);\n+    // check null op\n+    if (op == null) {\n+      throw new IllegalArgumentException(\"Param op must be specified.\");\n+    }\n \n     final String content;\n-    String path = getPath(decoder);\n     switch (op) {\n     case \"GETFILESTATUS\":\n       content = image.getFileStatus(path);",
                "raw_url": "https://github.com/apache/hadoop/raw/6aa63452b358cf0ba8687ec89b5d6c3a99649bc5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/FSImageHandler.java",
                "sha": "9b0031a3a2eaaa5ca0957b28a775a0c51b8065a3",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/6aa63452b358cf0ba8687ec89b5d6c3a99649bc5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewer.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewer.java?ref=6aa63452b358cf0ba8687ec89b5d6c3a99649bc5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewer.java",
                "patch": "@@ -624,6 +624,25 @@ public void testWebImageViewer() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testWebImageViewerNullOp() throws Exception {\n+    WebImageViewer viewer = new WebImageViewer(\n+        NetUtils.createSocketAddr(\"localhost:0\"));\n+    try {\n+      viewer.initServer(originalFsimage.getAbsolutePath());\n+      int port = viewer.getPort();\n+\n+      // null op\n+      URL url = new URL(\"http://localhost:\" + port +\n+          \"/webhdfs/v1/\");\n+      // should get HTTP_BAD_REQUEST. NPE gets HTTP_INTERNAL_ERROR\n+      verifyHttpResponseCode(HttpURLConnection.HTTP_BAD_REQUEST, url);\n+    } finally {\n+      // shutdown the viewer\n+      viewer.close();\n+    }\n+  }\n+\n   @Test\n   public void testWebImageViewerSecureMode() throws Exception {\n     Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop/raw/6aa63452b358cf0ba8687ec89b5d6c3a99649bc5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewer.java",
                "sha": "1895ada79d10a97969daf381b640cb8d4ab044e7",
                "status": "modified"
            }
        ],
        "message": "HDFS-14242. OIV WebImageViewer: NPE when param op is not specified. Contributed by Siyao Meng.\n\nSigned-off-by: Wei-Chiu Chuang <weichiu@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/d3de8e162be7933be496343db4a807b6b5ca04cd",
        "patched_files": [
            "OfflineImageViewer.java",
            "FSImageHandler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOfflineImageViewer.java"
        ]
    },
    "hadoop_6bb741f": {
        "bug_id": "hadoop_6bb741f",
        "commit": "https://github.com/apache/hadoop/commit/6bb741ff0ef208a8628bc64d6537999d4cd67955",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java?ref=6bb741ff0ef208a8628bc64d6537999d4cd67955",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n@@ -242,7 +243,8 @@ private void setDecomissionedNMs() {\n     for (final String host : excludeList) {\n       NodeId nodeId = createUnknownNodeId(host);\n       RMNodeImpl rmNode = new RMNodeImpl(nodeId,\n-          rmContext, host, -1, -1, new UnknownNode(host), null, null);\n+          rmContext, host, -1, -1, new UnknownNode(host),\n+          Resource.newInstance(0, 0), \"unknown\");\n       rmContext.getInactiveRMNodes().put(nodeId, rmNode);\n       rmNode.handle(new RMNodeEvent(nodeId, RMNodeEventType.DECOMMISSION));\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java",
                "sha": "7d69f930cc6562f13d937e24d12a8068789c3ae7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java?ref=6bb741ff0ef208a8628bc64d6537999d4cd67955",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "patch": "@@ -256,4 +256,8 @@ public long getMemory() {\n   public int getvCores() {\n     return vCores;\n   }\n+\n+  public String getVersion() {\n+    return version;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "sha": "32cdb1b195beeaf53af5aab7ffbe3875ee74fb04",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java?ref=6bb741ff0ef208a8628bc64d6537999d4cd67955",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java",
                "patch": "@@ -40,6 +40,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.io.FileUtils;\n import org.apache.commons.logging.Log;\n@@ -103,6 +104,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n@@ -1957,6 +1959,9 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n       rm1.start();\n       MockNM nm1 = rm1.registerNode(\"localhost:1234\", 8000);\n       MockNM nm2 = rm1.registerNode(\"host2:1234\", 8000);\n+      Resource expectedCapability =\n+          Resource.newInstance(nm1.getMemory(), nm1.getvCores());\n+      String expectedVersion = nm1.getVersion();\n       Assert\n           .assertEquals(0,\n               ClusterMetrics.getMetrics().getNumDecommisionedNMs());\n@@ -1978,6 +1983,7 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n       Assert\n           .assertEquals(2,\n               ClusterMetrics.getMetrics().getNumDecommisionedNMs());\n+      verifyNodesAfterDecom(rm1, 2, expectedCapability, expectedVersion);\n       rm1.stop();\n       rm1 = null;\n       Assert\n@@ -1991,6 +1997,7 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n       Assert\n           .assertEquals(2,\n               ClusterMetrics.getMetrics().getNumDecommisionedNMs());\n+      verifyNodesAfterDecom(rm2, 2, Resource.newInstance(0, 0), \"unknown\");\n     } finally {\n       if (rm1 != null) {\n         rm1.stop();\n@@ -2001,6 +2008,18 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n     }\n   }\n \n+  private void verifyNodesAfterDecom(MockRM rm, int numNodes,\n+                                     Resource expectedCapability,\n+                                     String expectedVersion) {\n+    ConcurrentMap<NodeId, RMNode> inactiveRMNodes =\n+        rm.getRMContext().getInactiveRMNodes();\n+    Assert.assertEquals(numNodes, inactiveRMNodes.size());\n+    for (RMNode rmNode : inactiveRMNodes.values()) {\n+      Assert.assertEquals(expectedCapability, rmNode.getTotalCapability());\n+      Assert.assertEquals(expectedVersion, rmNode.getNodeManagerVersion());\n+    }\n+  }\n+\n   // Test Delegation token is renewed synchronously so that recover events\n   // can be processed before any other external incoming events, specifically\n   // the ContainerFinished event on NM re-registraton.",
                "raw_url": "https://github.com/apache/hadoop/raw/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java",
                "sha": "a98a124057593e55aba42aaeac259159768b53b4",
                "status": "modified"
            }
        ],
        "message": "YARN-5837. NPE when getting node status of a decommissioned node after an RM restart. Contributed by Robert Kanter",
        "parent": "https://github.com/apache/hadoop/commit/0c0ab102ab392ba07ed2aa8d8a67eef4c20cad9b",
        "patched_files": [
            "NodesListManager.java",
            "MockNM.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNodesListManager.java",
            "TestRMRestart.java"
        ]
    },
    "hadoop_6d7eedf": {
        "bug_id": "hadoop_6d7eedf",
        "commit": "https://github.com/apache/hadoop/commit/6d7eedfd28cc1712690db2f6ca8a281b0901ee28",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/6d7eedfd28cc1712690db2f6ca8a281b0901ee28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=6d7eedfd28cc1712690db2f6ca8a281b0901ee28",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -437,9 +437,11 @@\n           RMAppAttemptState.FAILED,\n           EnumSet.of(\n               RMAppAttemptEventType.LAUNCHED,\n+              RMAppAttemptEventType.LAUNCH_FAILED,\n               RMAppAttemptEventType.EXPIRE,\n               RMAppAttemptEventType.KILL,\n               RMAppAttemptEventType.FAIL,\n+              RMAppAttemptEventType.REGISTERED,\n               RMAppAttemptEventType.UNREGISTERED,\n               RMAppAttemptEventType.STATUS_UPDATE,\n               RMAppAttemptEventType.CONTAINER_ALLOCATED))\n@@ -1203,10 +1205,16 @@ public RMAppAttemptState transition(RMAppAttemptImpl appAttempt,\n       }\n \n       // Set the masterContainer\n-      appAttempt.setMasterContainer(amContainerAllocation.getContainers()\n-          .get(0));\n+      Container amContainer = amContainerAllocation.getContainers().get(0);\n       RMContainerImpl rmMasterContainer = (RMContainerImpl)appAttempt.scheduler\n-          .getRMContainer(appAttempt.getMasterContainer().getId());\n+          .getRMContainer(amContainer.getId());\n+      //while one NM is removed, the scheduler will clean the container,the\n+      //following CONTAINER_FINISHED event will handle the cleaned container.\n+      //so just return RMAppAttemptState.SCHEDULED\n+      if (rmMasterContainer == null) {\n+        return RMAppAttemptState.SCHEDULED;\n+      }\n+      appAttempt.setMasterContainer(amContainer);\n       rmMasterContainer.setAMContainer(true);\n       // The node set in NMTokenSecrentManager is used for marking whether the\n       // NMToken has been issued for this node to the AM.",
                "raw_url": "https://github.com/apache/hadoop/raw/6d7eedfd28cc1712690db2f6ca8a281b0901ee28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "03039daae967d5706bfc0bd1909a1088ec2b5f38",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hadoop/blob/6d7eedfd28cc1712690db2f6ca8a281b0901ee28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java?ref=6d7eedfd28cc1712690db2f6ca8a281b0901ee28",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "patch": "@@ -986,7 +986,7 @@ public void testLaunchedAtFinalSaving() {\n   public void testAttemptAddedAtFinalSaving() {\n     submitApplicationAttempt();\n \n-    // SUBNITED->FINAL_SAVING\n+    // SUBMITTED->FINAL_SAVING\n     applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n                    .getAppAttemptId(), RMAppAttemptEventType.KILL));\n     assertEquals(RMAppAttemptState.FINAL_SAVING,\n@@ -999,6 +999,56 @@ public void testAttemptAddedAtFinalSaving() {\n                    applicationAttempt.getAppAttemptState());\n   }\n \n+  @Test(timeout = 10000)\n+  public void testAttemptRegisteredAtFailed() {\n+    Container amContainer = allocateApplicationAttempt();\n+    launchApplicationAttempt(amContainer);\n+\n+    //send CONTAINER_FINISHED event\n+    NodeId anyNodeId = NodeId.newInstance(\"host\", 1234);\n+    applicationAttempt.handle(new RMAppAttemptContainerFinishedEvent(\n+        applicationAttempt.getAppAttemptId(), BuilderUtils.newContainerStatus(\n+        amContainer.getId(), ContainerState.COMPLETE, \"\", 0,\n+        amContainer.getResource()), anyNodeId));\n+    assertEquals(RMAppAttemptState.FINAL_SAVING,\n+        applicationAttempt.getAppAttemptState());\n+\n+    sendAttemptUpdateSavedEvent(applicationAttempt);\n+    assertEquals(RMAppAttemptState.FAILED,\n+        applicationAttempt.getAppAttemptState());\n+\n+    //send REGISTERED event\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.REGISTERED));\n+\n+    assertEquals(RMAppAttemptState.FAILED,\n+        applicationAttempt.getAppAttemptState());\n+  }\n+\n+  @Test\n+  public void testAttemptLaunchFailedAtFailed() {\n+    Container amContainer = allocateApplicationAttempt();\n+    launchApplicationAttempt(amContainer);\n+    //send CONTAINER_FINISHED event\n+    NodeId anyNodeId = NodeId.newInstance(\"host\", 1234);\n+    applicationAttempt.handle(new RMAppAttemptContainerFinishedEvent(\n+        applicationAttempt.getAppAttemptId(), BuilderUtils.newContainerStatus(\n+        amContainer.getId(), ContainerState.COMPLETE, \"\", 0,\n+        amContainer.getResource()), anyNodeId));\n+    assertEquals(RMAppAttemptState.FINAL_SAVING,\n+        applicationAttempt.getAppAttemptState());\n+    sendAttemptUpdateSavedEvent(applicationAttempt);\n+    assertEquals(RMAppAttemptState.FAILED,\n+        applicationAttempt.getAppAttemptState());\n+\n+    //send LAUNCH_FAILED event\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED));\n+\n+    assertEquals(RMAppAttemptState.FAILED,\n+        applicationAttempt.getAppAttemptState());\n+  }\n+\n   @Test\n   public void testAMCrashAtAllocated() {\n     Container amContainer = allocateApplicationAttempt();\n@@ -1598,6 +1648,34 @@ public void testFailedToFailed() {\n     assertTrue(found);\n   }\n \n+  @Test\n+  public void testContainerRemovedBeforeAllocate() {\n+    scheduleApplicationAttempt();\n+\n+    // Mock the allocation of AM container\n+    Container container = mock(Container.class);\n+    Resource resource = BuilderUtils.newResource(2048, 1);\n+    when(container.getId()).thenReturn(\n+        BuilderUtils.newContainerId(applicationAttempt.getAppAttemptId(), 1));\n+    when(container.getResource()).thenReturn(resource);\n+    Allocation allocation = mock(Allocation.class);\n+    when(allocation.getContainers()).\n+        thenReturn(Collections.singletonList(container));\n+    when(scheduler.allocate(any(ApplicationAttemptId.class), any(List.class),\n+        any(List.class), any(List.class), any(List.class), any(List.class),\n+        any(ContainerUpdates.class))).\n+        thenReturn(allocation);\n+\n+    //container removed, so return null\n+    when(scheduler.getRMContainer(container.getId())).\n+        thenReturn(null);\n+\n+    applicationAttempt.handle(\n+        new RMAppAttemptEvent(applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.CONTAINER_ALLOCATED));\n+    assertEquals(RMAppAttemptState.SCHEDULED,\n+        applicationAttempt.getAppAttemptState());\n+  }\n \n   @SuppressWarnings(\"deprecation\")\n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/6d7eedfd28cc1712690db2f6ca8a281b0901ee28/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "sha": "faecdb4660715a24918b036a029a2f2e4ab1e271",
                "status": "modified"
            }
        ],
        "message": "YARN-9194. Invalid event: REGISTERED and LAUNCH_FAILED at FAILED, and NullPointerException happens in RM while shutdown a NM. (lujie via wangda)\n\nChange-Id: I4359f59a73a278a941f4bb9d106dd38c9cb471fe",
        "parent": "https://github.com/apache/hadoop/commit/0a46baecd31c485d1ea4e567c29c47bfba0b092e",
        "patched_files": [
            "RMAppAttemptImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMAppAttemptTransitions.java"
        ]
    },
    "hadoop_6d96a28": {
        "bug_id": "hadoop_6d96a28",
        "commit": "https://github.com/apache/hadoop/commit/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -87,7 +87,10 @@ Trunk (unreleased changes)\n     HDFS-3037. TestMulitipleNNDataBlockScanner#testBlockScannerAfterRestart is\n     racy. (atm)\n \n-    HDFS-2966 TestNameNodeMetrics tests can fail under load. (stevel)\n+    HDFS-2966. TestNameNodeMetrics tests can fail under load. (stevel)\n+\n+    HDFS-3067. NPE in DFSInputStream.readBuffer if read is repeated on\n+    corrupted block. (Henry Robinson via atm)\n \n Release 0.23.3 - UNRELEASED \n ",
                "raw_url": "https://github.com/apache/hadoop/raw/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3cc339a596e7986ab09a071236546d5c0b042b5b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -538,7 +538,9 @@ public synchronized int read(byte buf[], int off, int len) throws IOException {\n       int retries = 2;\n       while (retries > 0) {\n         try {\n-          if (pos > blockEnd) {\n+          // currentNode can be left as null if previous read had a checksum\n+          // error on the same block. See HDFS-3067\n+          if (pos > blockEnd || currentNode == null) {\n             currentNode = blockSeekTo(pos);\n           }\n           int realLen = (int) Math.min(len, (blockEnd - pos + 1L));",
                "raw_url": "https://github.com/apache/hadoop/raw/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "71c8a500a83b1fa157204dd18a14c16863585a6d",
                "status": "modified"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/hadoop/blob/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java?ref=6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "patch": "@@ -48,6 +48,7 @@\n import org.apache.hadoop.fs.FileChecksum;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.UnresolvedLinkException;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n import org.apache.hadoop.hdfs.protocol.Block;\n@@ -64,6 +65,7 @@\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.ipc.RpcPayloadHeader.RpcKind;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.mockito.internal.stubbing.answers.ThrowsException;\n import org.mockito.invocation.InvocationOnMock;\n import org.mockito.stubbing.Answer;\n@@ -649,5 +651,52 @@ public void testClientDNProtocolTimeout() throws IOException {\n       server.stop();\n     }\n   }\n+\n+  /**\n+   * Test that checksum failures are recovered from by the next read on the same\n+   * DFSInputStream. Corruption information is not persisted from read call to\n+   * read call, so the client should expect consecutive calls to behave the same\n+   * way. See HDFS-3067.\n+   */\n+  public void testRetryOnChecksumFailure()\n+      throws UnresolvedLinkException, IOException {\n+    HdfsConfiguration conf = new HdfsConfiguration();\n+    MiniDFSCluster cluster =\n+      new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n+\n+    try {\n+      final short REPL_FACTOR = 1;\n+      final long FILE_LENGTH = 512L;\n+      cluster.waitActive();\n+      FileSystem fs = cluster.getFileSystem();\n+\n+      Path path = new Path(\"/corrupted\");\n+\n+      DFSTestUtil.createFile(fs, path, FILE_LENGTH, REPL_FACTOR, 12345L);\n+      DFSTestUtil.waitReplication(fs, path, REPL_FACTOR);\n+\n+      ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, path);\n+      int blockFilesCorrupted = cluster.corruptBlockOnDataNodes(block);\n+      assertEquals(\"All replicas not corrupted\", REPL_FACTOR,\n+          blockFilesCorrupted);\n+\n+      InetSocketAddress nnAddr =\n+        new InetSocketAddress(\"localhost\", cluster.getNameNodePort());\n+      DFSClient client = new DFSClient(nnAddr, conf);\n+      DFSInputStream dis = client.open(path.toString());\n+      byte[] arr = new byte[(int)FILE_LENGTH];\n+      for (int i = 0; i < 2; ++i) {\n+        try {\n+          dis.read(arr, 0, (int)FILE_LENGTH);\n+          fail(\"Expected ChecksumException not thrown\");\n+        } catch (Exception ex) {\n+          GenericTestUtils.assertExceptionContains(\n+              \"Checksum error\", ex);\n+        }\n+      }\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "sha": "1e39b9a40d1854b90379763f0ac23c94a4e83d7a",
                "status": "modified"
            }
        ],
        "message": "HDFS-3067. NPE in DFSInputStream.readBuffer if read is repeated on corrupted block. Contributed by Henry Robinson.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301182 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/1177d4edc29f839b8df1038c4fbf37f56f56a2a0",
        "patched_files": [
            "DFSInputStream.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSClientRetries.java"
        ]
    },
    "hadoop_6efb8c9": {
        "bug_id": "hadoop_6efb8c9",
        "commit": "https://github.com/apache/hadoop/commit/6efb8c9c6018ec688eeba8c61c220009ec350a44",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/6efb8c9c6018ec688eeba8c61c220009ec350a44/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java?ref=6efb8c9c6018ec688eeba8c61c220009ec350a44",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java",
                "patch": "@@ -270,9 +270,9 @@ static void initSecureConf(Configuration conf) throws Exception {\n   public static void destroy() throws Exception {\n     if (kdc != null) {\n       kdc.stop();\n-    }\n-    FileUtil.fullyDelete(baseDir);\n+      FileUtil.fullyDelete(baseDir);\n       KeyStoreTestUtil.cleanupSSLConfig(keystoresDir, sslConfDir);\n+    }\n   }\n \n   /* create a file with a length of <code>fileLen</code> */",
                "raw_url": "https://github.com/apache/hadoop/raw/6efb8c9c6018ec688eeba8c61c220009ec350a44/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java",
                "sha": "22b48ad12f7e36b8f0522a2d8fa1953f4451a066",
                "status": "modified"
            }
        ],
        "message": "HDFS-11135. The tests in TestBalancer run fails due to NPE. Contributed By Yiqun Lin",
        "parent": "https://github.com/apache/hadoop/commit/79448d4ab1f298e50565b161e6605d994ee5e058",
        "patched_files": [
            "Balancer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBalancer.java"
        ]
    },
    "hadoop_7039b77": {
        "bug_id": "hadoop_7039b77",
        "commit": "https://github.com/apache/hadoop/commit/7039b776c64cd0b1c444d27ba2ae118b5a812ab2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7039b776c64cd0b1c444d27ba2ae118b5a812ab2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=7039b776c64cd0b1c444d27ba2ae118b5a812ab2",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -310,6 +310,9 @@ Release 2.4.0 - UNRELEASED\n     HDFS-5843. DFSClient.getFileChecksum() throws IOException if checksum is \n     disabled. (Laurent Goujon via jing9)\n \n+    HDFS-5856. DataNode.checkDiskError might throw NPE.\n+    (Josh Elser via suresh)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/7039b776c64cd0b1c444d27ba2ae118b5a812ab2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "1fb5c1c190ce0d90159261c81bcb4b9c535923ac",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/7039b776c64cd0b1c444d27ba2ae118b5a812ab2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=7039b776c64cd0b1c444d27ba2ae118b5a812ab2",
                "deletions": 7,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -36,6 +36,7 @@\n import java.net.URI;\n import java.net.UnknownHostException;\n import java.nio.channels.ClosedByInterruptException;\n+import java.nio.channels.ClosedChannelException;\n import java.nio.channels.SocketChannel;\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n@@ -51,7 +52,6 @@\n \n import javax.management.ObjectName;\n \n-\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n@@ -1324,12 +1324,7 @@ public void shutdown() {\n   protected void checkDiskError(Exception e ) throws IOException {\n     \n     LOG.warn(\"checkDiskError: exception: \", e);\n-    if (e instanceof SocketException || e instanceof SocketTimeoutException\n-    \t  || e instanceof ClosedByInterruptException \n-    \t  || e.getMessage().startsWith(\"An established connection was aborted\")\n-    \t  || e.getMessage().startsWith(\"Broken pipe\")\n-    \t  || e.getMessage().startsWith(\"Connection reset\")\n-    \t  || e.getMessage().contains(\"java.nio.channels.SocketChannel\")) {\n+    if (isNetworkRelatedException(e)) {\n       LOG.info(\"Not checking disk as checkDiskError was called on a network\" +\n       \t\t\" related exception\");\t\n       return;\n@@ -1342,6 +1337,28 @@ protected void checkDiskError(Exception e ) throws IOException {\n     }\n   }\n   \n+  /**\n+   * Check if the provided exception looks like it's from a network error\n+   * @param e the exception from a checkDiskError call\n+   * @return true if this exception is network related, false otherwise\n+   */\n+  protected boolean isNetworkRelatedException(Exception e) {\n+    if (e instanceof SocketException \n+        || e instanceof SocketTimeoutException\n+        || e instanceof ClosedChannelException \n+        || e instanceof ClosedByInterruptException) {\n+      return true;\n+    }\n+    \n+    String msg = e.getMessage();\n+    \n+    return null != msg \n+        && (msg.startsWith(\"An established connection was aborted\")\n+            || msg.startsWith(\"Broken pipe\")\n+            || msg.startsWith(\"Connection reset\")\n+            || msg.contains(\"java.nio.channels.SocketChannel\"));\n+  }\n+  \n   /**\n    *  Check if there is a disk failure and if so, handle the error\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/7039b776c64cd0b1c444d27ba2ae118b5a812ab2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "6bf98b694659ebe8fa0d9406500f9c5c0735b4b1",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/7039b776c64cd0b1c444d27ba2ae118b5a812ab2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java?ref=7039b776c64cd0b1c444d27ba2ae118b5a812ab2",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "patch": "@@ -18,12 +18,16 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n import java.io.DataOutputStream;\n import java.io.File;\n import java.net.InetSocketAddress;\n import java.net.Socket;\n+import java.net.SocketException;\n+import java.net.SocketTimeoutException;\n+import java.nio.channels.ClosedChannelException;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -196,4 +200,16 @@ public void testLocalDirs() throws Exception {\n       }\n     }\n   }\n+  \n+  @Test\n+  public void testNetworkErrorsIgnored() {\n+    DataNode dn = cluster.getDataNodes().iterator().next();\n+    \n+    assertTrue(dn.isNetworkRelatedException(new SocketException()));\n+    assertTrue(dn.isNetworkRelatedException(new SocketTimeoutException()));\n+    assertTrue(dn.isNetworkRelatedException(new ClosedChannelException()));\n+    assertTrue(dn.isNetworkRelatedException(new Exception(\"Broken pipe foo bar\")));\n+    assertFalse(dn.isNetworkRelatedException(new Exception()));\n+    assertFalse(dn.isNetworkRelatedException(new Exception(\"random problem\")));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/7039b776c64cd0b1c444d27ba2ae118b5a812ab2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDiskError.java",
                "sha": "e36005bafc6f486614b39c887d88315d9f797572",
                "status": "modified"
            }
        ],
        "message": "HDFS-5856. DataNode.checkDiskError might throw NPE. Contributed by Josh Elser.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1563064 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ed097d24fe789844079a650eba05587e2973b158",
        "patched_files": [
            "DataNode.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDiskError.java"
        ]
    },
    "hadoop_708fa8b": {
        "bug_id": "hadoop_708fa8b",
        "commit": "https://github.com/apache/hadoop/commit/708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/ApplicationEntityReader.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/ApplicationEntityReader.java?ref=708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
                "deletions": 17,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/ApplicationEntityReader.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.Get;\n import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType;\n import org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader.Field;\n@@ -85,24 +86,10 @@ protected Result getResult(Configuration hbaseConf, Connection conn)\n   }\n \n   @Override\n-  protected Iterable<Result> getResults(Configuration hbaseConf,\n+  protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn) throws IOException {\n-    // If getEntities() is called for an application, there can be at most\n-    // one entity. If the entity passes the filter, it is returned. Otherwise,\n-    // an empty set is returned.\n-    byte[] rowKey = ApplicationRowKey.getRowKey(clusterId, userId, flowId,\n-        flowRunId, appId);\n-    Get get = new Get(rowKey);\n-    get.setMaxVersions(Integer.MAX_VALUE);\n-    Result result = table.getResult(hbaseConf, conn, get);\n-    TimelineEntity entity = parseEntity(result);\n-    Set<Result> set;\n-    if (entity != null) {\n-      set = Collections.singleton(result);\n-    } else {\n-      set = Collections.emptySet();\n-    }\n-    return set;\n+    throw new UnsupportedOperationException(\n+        \"we don't support multiple apps query\");\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/ApplicationEntityReader.java",
                "sha": "d5b5d636ee84d629fa19a387896e3860d34b5230",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java?ref=708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.filter.PageFilter;\n import org.apache.hadoop.yarn.api.records.timelineservice.FlowActivityEntity;\n@@ -88,18 +89,22 @@ public FlowActivityEntityReader(String userId, String clusterId,\n     augmentParams(hbaseConf, conn);\n \n     NavigableSet<TimelineEntity> entities = new TreeSet<>();\n-    Iterable<Result> results = getResults(hbaseConf, conn);\n-    for (Result result : results) {\n-      TimelineEntity entity = parseEntity(result);\n-      if (entity == null) {\n-        continue;\n-      }\n-      entities.add(entity);\n-      if (entities.size() == limit) {\n-        break;\n+    ResultScanner results = getResults(hbaseConf, conn);\n+    try {\n+      for (Result result : results) {\n+        TimelineEntity entity = parseEntity(result);\n+        if (entity == null) {\n+          continue;\n+        }\n+        entities.add(entity);\n+        if (entities.size() == limit) {\n+          break;\n+        }\n       }\n+      return entities;\n+    } finally {\n+      results.close();\n     }\n-    return entities;\n   }\n \n   @Override\n@@ -123,7 +128,7 @@ protected Result getResult(Configuration hbaseConf, Connection conn)\n   }\n \n   @Override\n-  protected Iterable<Result> getResults(Configuration hbaseConf,\n+  protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn) throws IOException {\n     Scan scan = new Scan();\n     scan.setRowPrefixFilter(FlowActivityRowKey.getRowKeyPrefix(clusterId));",
                "raw_url": "https://github.com/apache/hadoop/raw/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowActivityEntityReader.java",
                "sha": "e68ca17b398fd93438af0b30af9c17de6f670a12",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowRunEntityReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowRunEntityReader.java?ref=708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowRunEntityReader.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.Get;\n import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.yarn.api.records.timelineservice.FlowRunEntity;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n import org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader.Field;\n@@ -96,7 +97,7 @@ protected Result getResult(Configuration hbaseConf, Connection conn)\n   }\n \n   @Override\n-  protected Iterable<Result> getResults(Configuration hbaseConf,\n+  protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn) throws IOException {\n     throw new UnsupportedOperationException(\n         \"multiple entity query is not supported\");\n@@ -110,14 +111,14 @@ protected TimelineEntity parseEntity(Result result) throws IOException {\n     flowRun.setRunId(flowRunId);\n \n     // read the start time\n-    Long startTime = (Long)FlowRunColumn.MIN_START_TIME.readResult(result);\n+    Number startTime = (Number)FlowRunColumn.MIN_START_TIME.readResult(result);\n     if (startTime != null) {\n-      flowRun.setStartTime(startTime);\n+      flowRun.setStartTime(startTime.longValue());\n     }\n     // read the end time if available\n-    Long endTime = (Long)FlowRunColumn.MAX_END_TIME.readResult(result);\n+    Number endTime = (Number)FlowRunColumn.MAX_END_TIME.readResult(result);\n     if (endTime != null) {\n-      flowRun.setMaxEndTime(endTime);\n+      flowRun.setMaxEndTime(endTime.longValue());\n     }\n \n     // read the flow version",
                "raw_url": "https://github.com/apache/hadoop/raw/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/FlowRunEntityReader.java",
                "sha": "b5d7ae5e7fc74cade3fb0d684d3b58bbac4dc5ab",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/GenericEntityReader.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/GenericEntityReader.java?ref=708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/GenericEntityReader.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.Get;\n import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.hbase.client.Scan;\n import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n@@ -176,7 +177,7 @@ protected Result getResult(Configuration hbaseConf, Connection conn)\n   }\n \n   @Override\n-  protected Iterable<Result> getResults(Configuration hbaseConf,\n+  protected ResultScanner getResults(Configuration hbaseConf,\n       Connection conn) throws IOException {\n     // Scan through part of the table to find the entities belong to one app\n     // and one type",
                "raw_url": "https://github.com/apache/hadoop/raw/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/GenericEntityReader.java",
                "sha": "396a02b5aa3f251a12e906fe448888c493b42cdc",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineEntityReader.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineEntityReader.java?ref=708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineEntityReader.java",
                "patch": "@@ -25,9 +25,12 @@\n import java.util.Set;\n import java.util.TreeSet;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hbase.client.Connection;\n import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetric;\n import org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader.Field;\n@@ -40,6 +43,7 @@\n  * entities that are being requested.\n  */\n abstract class TimelineEntityReader {\n+  private static final Log LOG = LogFactory.getLog(TimelineEntityReader.class);\n   protected final boolean singleEntityRead;\n \n   protected String userId;\n@@ -131,6 +135,11 @@ public TimelineEntity readEntity(Configuration hbaseConf, Connection conn)\n     augmentParams(hbaseConf, conn);\n \n     Result result = getResult(hbaseConf, conn);\n+    if (result == null || result.isEmpty()) {\n+      // Could not find a matching row.\n+      LOG.info(\"Cannot find matching entity of type \" + entityType);\n+      return null;\n+    }\n     return parseEntity(result);\n   }\n \n@@ -145,18 +154,22 @@ public TimelineEntity readEntity(Configuration hbaseConf, Connection conn)\n     augmentParams(hbaseConf, conn);\n \n     NavigableSet<TimelineEntity> entities = new TreeSet<>();\n-    Iterable<Result> results = getResults(hbaseConf, conn);\n-    for (Result result : results) {\n-      TimelineEntity entity = parseEntity(result);\n-      if (entity == null) {\n-        continue;\n-      }\n-      entities.add(entity);\n-      if (entities.size() > limit) {\n-        entities.pollLast();\n+    ResultScanner results = getResults(hbaseConf, conn);\n+    try {\n+      for (Result result : results) {\n+        TimelineEntity entity = parseEntity(result);\n+        if (entity == null) {\n+          continue;\n+        }\n+        entities.add(entity);\n+        if (entities.size() > limit) {\n+          entities.pollLast();\n+        }\n       }\n+      return entities;\n+    } finally {\n+      results.close();\n     }\n-    return entities;\n   }\n \n   /**\n@@ -184,9 +197,9 @@ protected abstract Result getResult(Configuration hbaseConf, Connection conn)\n       throws IOException;\n \n   /**\n-   * Fetches an iterator for {@link Result} instances for a multi-entity read.\n+   * Fetches a {@link ResultScanner} for a multi-entity read.\n    */\n-  protected abstract Iterable<Result> getResults(Configuration hbaseConf,\n+  protected abstract ResultScanner getResults(Configuration hbaseConf,\n       Connection conn) throws IOException;\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/TimelineEntityReader.java",
                "sha": "93be2db7e64cbaded22574f5cf471c5f2f184ec4",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hadoop/blob/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesFlowRun.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesFlowRun.java?ref=708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesFlowRun.java",
                "patch": "@@ -59,6 +59,7 @@\n import com.sun.jersey.api.client.Client;\n import com.sun.jersey.api.client.ClientResponse;\n import com.sun.jersey.api.client.GenericType;\n+import com.sun.jersey.api.client.ClientResponse.Status;\n import com.sun.jersey.api.client.config.ClientConfig;\n import com.sun.jersey.api.client.config.DefaultClientConfig;\n import com.sun.jersey.client.urlconnection.HttpURLConnectionFactory;\n@@ -281,6 +282,16 @@ private static boolean verifyMetrics(\n     return false;\n   }\n \n+  private static void verifyHttpResponse(Client client, URI uri,\n+      Status status) {\n+    ClientResponse resp =\n+        client.resource(uri).accept(MediaType.APPLICATION_JSON)\n+        .type(MediaType.APPLICATION_JSON).get(ClientResponse.class);\n+    assertNotNull(resp);\n+    assertTrue(\"Response from server should have been \" + status,\n+        resp.getClientResponseStatus().equals(status));\n+  }\n+\n   @Test\n   public void testGetFlowRun() throws Exception {\n     Client client = createClient();\n@@ -354,6 +365,35 @@ public void testGetFlows() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testGetFlowRunNotPresent() throws Exception {\n+    Client client = createClient();\n+    try {\n+      URI uri = URI.create(\"http://localhost:\" + serverPort + \"/ws/v2/\" +\n+          \"timeline/flowrun/cluster1/flow_name/1002345678929?userid=user1\");\n+      verifyHttpResponse(client, uri, Status.NOT_FOUND);\n+    } finally {\n+      client.destroy();\n+    }\n+  }\n+\n+  @Test\n+  public void testGetFlowsNotPresent() throws Exception {\n+    Client client = createClient();\n+    try {\n+      URI uri = URI.create(\"http://localhost:\" + serverPort + \"/ws/v2/\" +\n+          \"timeline/flows/cluster2\");\n+      ClientResponse resp = getResponse(client, uri);\n+      Set<FlowActivityEntity> entities =\n+          resp.getEntity(new GenericType<Set<FlowActivityEntity>>(){});\n+      assertEquals(MediaType.APPLICATION_JSON_TYPE, resp.getType());\n+      assertNotNull(entities);\n+      assertEquals(0, entities.size());\n+    } finally {\n+      client.destroy();\n+    }\n+  }\n+\n   @After\n   public void stop() throws Exception {\n     if (server != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesFlowRun.java",
                "sha": "e359f789618f7324f9f0f188eb3a9022d3e30e9d",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java?ref=708fa8b1ae85b6efda318368bc0c0ba02d4958c8",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java",
                "patch": "@@ -249,11 +249,7 @@ public void testWriteApplicationToHBase() throws Exception {\n       TimelineEntity e1 = hbr.getEntity(user, cluster, flow, runid, appId,\n           entity.getType(), entity.getId(),\n           EnumSet.of(TimelineReader.Field.ALL));\n-      Set<TimelineEntity> es1 = hbr.getEntities(user, cluster, flow, runid,\n-          appId, entity.getType(), null, null, null, null, null, null, null,\n-          null, null, null, null, EnumSet.of(TimelineReader.Field.ALL));\n       assertNotNull(e1);\n-      assertEquals(1, es1.size());\n \n       // verify attributes\n       assertEquals(appId, e1.getId());\n@@ -610,18 +606,9 @@ public void testEvents() throws IOException {\n       TimelineEntity e2 = hbr.getEntity(user, cluster, null, null, appName,\n           entity.getType(), entity.getId(),\n           EnumSet.of(TimelineReader.Field.ALL));\n-      Set<TimelineEntity> es1 = hbr.getEntities(user, cluster, flow, runid,\n-          appName, entity.getType(), null, null, null, null, null, null, null,\n-          null, null, null, null, EnumSet.of(TimelineReader.Field.ALL));\n-      Set<TimelineEntity> es2 = hbr.getEntities(user, cluster, null, null,\n-          appName, entity.getType(), null, null, null, null, null, null, null,\n-          null, null, null, null, EnumSet.of(TimelineReader.Field.ALL));\n       assertNotNull(e1);\n       assertNotNull(e2);\n       assertEquals(e1, e2);\n-      assertEquals(1, es1.size());\n-      assertEquals(1, es2.size());\n-      assertEquals(es1, es2);\n \n       // check the events\n       NavigableSet<TimelineEvent> events = e1.getEvents();",
                "raw_url": "https://github.com/apache/hadoop/raw/708fa8b1ae85b6efda318368bc0c0ba02d4958c8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java",
                "sha": "3b0921b05fb2e12a85ffaccfdf7b6a0546f1983d",
                "status": "modified"
            }
        ],
        "message": "YARN-4210. HBase reader throws NPE if Get returns no rows (Varun Saxena via vrushali)",
        "parent": "https://github.com/apache/hadoop/commit/da2b7bd08e27673945cccc391b1ad17e8f22abf1",
        "patched_files": [
            "FlowActivityEntityReader.java",
            "TimelineEntityReader.java",
            "ApplicationEntityReader.java",
            "FlowRunEntityReader.java",
            "GenericEntityReader.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestHBaseTimelineStorage.java",
            "TestTimelineReaderWebServicesFlowRun.java"
        ]
    },
    "hadoop_710a869": {
        "bug_id": "hadoop_710a869",
        "commit": "https://github.com/apache/hadoop/commit/710a8693e57235d283e704c983722079c8fd6f38",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -227,6 +227,9 @@ Release 2.5.0 - UNRELEASED\n     YARN-2128. FairScheduler: Incorrect calculation of amResource usage.\n     (Wei Yan via kasha)\n \n+    YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. (Wangda Tan\n+    via jianhe)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/CHANGES.txt",
                "sha": "401fb102bf47b883c2c57fbf85ee5e61fd4281bb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -327,7 +327,7 @@ protected static void validateConfigs(Configuration conf) {\n    * RMActiveServices handles all the Active services in the RM.\n    */\n   @Private\n-  class RMActiveServices extends CompositeService {\n+  public class RMActiveServices extends CompositeService {\n \n     private DelegationTokenRenewer delegationTokenRenewer;\n     private EventHandler<SchedulerEvent> schedulerDispatcher;\n@@ -526,11 +526,9 @@ protected void createPolicyMonitors() {\n                   (PreemptableResourceScheduler) scheduler));\n           for (SchedulingEditPolicy policy : policies) {\n             LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n-            policy.init(conf, rmContext.getDispatcher().getEventHandler(),\n-                (PreemptableResourceScheduler) scheduler);\n             // periodically check whether we need to take action to guarantee\n             // constraints\n-            SchedulingMonitor mon = new SchedulingMonitor(policy);\n+            SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);\n             addService(mon);\n           }\n         } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "77de2090a00479282f863f110299a592f5c0d77e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "patch": "@@ -21,6 +21,8 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.service.AbstractService;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler;\n \n import com.google.common.annotations.VisibleForTesting;\n \n@@ -34,18 +36,29 @@\n   private Thread checkerThread;\n   private volatile boolean stopped;\n   private long monitorInterval;\n+  private RMContext rmContext;\n \n-  public SchedulingMonitor(SchedulingEditPolicy scheduleEditPolicy) {\n+  public SchedulingMonitor(RMContext rmContext,\n+      SchedulingEditPolicy scheduleEditPolicy) {\n     super(\"SchedulingMonitor (\" + scheduleEditPolicy.getPolicyName() + \")\");\n     this.scheduleEditPolicy = scheduleEditPolicy;\n-    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n+    this.rmContext = rmContext;\n   }\n \n   public long getMonitorInterval() {\n     return monitorInterval;\n   }\n+  \n+  @VisibleForTesting\n+  public synchronized SchedulingEditPolicy getSchedulingEditPolicy() {\n+    return scheduleEditPolicy;\n+  }\n \n+  @SuppressWarnings(\"unchecked\")\n   public void serviceInit(Configuration conf) throws Exception {\n+    scheduleEditPolicy.init(conf, rmContext.getDispatcher().getEventHandler(),\n+        (PreemptableResourceScheduler) rmContext.getScheduler());\n+    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n     super.serviceInit(conf);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "sha": "1682f7d8612602446bc52b157945b5901a6a1a11",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -165,6 +165,11 @@ public void init(Configuration config,\n     observeOnly = config.getBoolean(OBSERVE_ONLY, false);\n     rc = scheduler.getResourceCalculator();\n   }\n+  \n+  @VisibleForTesting\n+  public ResourceCalculator getResourceCalculator() {\n+    return rc;\n+  }\n \n   @Override\n   public void editSchedule(){",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "sha": "6d1516158b5aad26990a58d562def6b0ca5df243",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "patch": "@@ -571,4 +571,8 @@ public void clearQueueMetrics(RMApp app) {\n       .getSchedulerApplications().get(app.getApplicationId()).getQueue()\n       .getMetrics().clearQueueMetrics();\n   }\n+  \n+  public RMActiveServices getRMActiveService() {\n+    return activeServices;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "sha": "67eac76e5f335d4df535ec2c888619ec677e9ae9",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -17,6 +17,25 @@\n  */\n package org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity;\n \n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.argThat;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n import java.util.ArrayList;\n import java.util.Comparator;\n import java.util.Deque;\n@@ -27,12 +46,16 @@\n import java.util.TreeSet;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.service.Service;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor;\n import org.apache.hadoop.yarn.server.resourcemanager.resource.Priority;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEvent;\n@@ -52,17 +75,6 @@\n import org.mockito.ArgumentCaptor;\n import org.mockito.ArgumentMatcher;\n \n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n-import static org.junit.Assert.*;\n-import static org.mockito.Mockito.*;\n-\n public class TestProportionalCapacityPreemptionPolicy {\n \n   static final long TS = 3141592653L;\n@@ -424,6 +436,36 @@ public void testContainerOrdering(){\n     assert containers.get(4).equals(rm5);\n \n   }\n+  \n+  @Test\n+  public void testPolicyInitializeAfterSchedulerInitialized() {\n+    Configuration conf = new Configuration();\n+    conf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n+        ProportionalCapacityPreemptionPolicy.class.getCanonicalName());\n+    conf.setBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, true);\n+    \n+    @SuppressWarnings(\"resource\")\n+    MockRM rm = new MockRM(conf);\n+    rm.init(conf);\n+    \n+    // ProportionalCapacityPreemptionPolicy should be initialized after\n+    // CapacityScheduler initialized. We will \n+    // 1) find SchedulingMonitor from RMActiveService's service list, \n+    // 2) check if ResourceCalculator in policy is null or not. \n+    // If it's not null, we can come to a conclusion that policy initialized\n+    // after scheduler got initialized\n+    for (Service service : rm.getRMActiveService().getServices()) {\n+      if (service instanceof SchedulingMonitor) {\n+        ProportionalCapacityPreemptionPolicy policy =\n+            (ProportionalCapacityPreemptionPolicy) ((SchedulingMonitor) service)\n+                .getSchedulingEditPolicy();\n+        assertNotNull(policy.getResourceCalculator());\n+        return;\n+      }\n+    }\n+    \n+    fail(\"Failed to find SchedulingMonitor service, please check what happened\");\n+  }\n \n   static class IsPreemptionRequestFor\n       extends ArgumentMatcher<ContainerPreemptEvent> {",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "sha": "d0a80eb20bb320d8b1c1ec7009743049bc09eb3f",
                "status": "modified"
            }
        ],
        "message": "YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. Contributed by Wangda Tan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601964 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
        "patched_files": [
            "ProportionalCapacityPreemptionPolicy.java",
            "CHANGES.java",
            "ResourceManager.java",
            "MockRM.java",
            "SchedulingMonitor.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSchedulingMonitor.java",
            "TestResourceManager.java",
            "TestProportionalCapacityPreemptionPolicy.java"
        ]
    },
    "hadoop_74748ec": {
        "bug_id": "hadoop_74748ec",
        "commit": "https://github.com/apache/hadoop/commit/74748ec62570f92d57dbad3ba4cca47402990db5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1695,6 +1695,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-2788. Normalize resource requests in FifoScheduler\n     appropriately. (Ahmed Radwan via acmurthy) \n \n+    MAPREDUCE-2693. Fix NPE in job-blacklisting. (Hitesh Shah via acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "22917ec8c83d7882471017e315f2646fb1a47dea",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "changes": 160,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 37,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "patch": "@@ -509,18 +509,6 @@ void addMap(ContainerRequestEvent event) {\n         request = new ContainerRequest(event, PRIORITY_FAST_FAIL_MAP);\n       } else {\n         for (String host : event.getHosts()) {\n-          //host comes from data splitLocations which are hostnames. Containers\n-          // use IP addresses.\n-          //TODO Temporary fix for locality. Use resolvers from h-common. \n-          // Cache to make this more efficient ?\n-          InetAddress addr = null;\n-          try {\n-            addr = InetAddress.getByName(host);\n-          } catch (UnknownHostException e) {\n-            LOG.warn(\"Unable to resolve host to IP for host [: \" + host + \"]\");\n-          }\n-          if (addr != null) //Fallback to host if resolve fails.\n-            host = addr.getHostAddress();\n           LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n           if (list == null) {\n             list = new LinkedList<TaskAttemptId>();\n@@ -557,26 +545,101 @@ private void assign(List<Container> allocatedContainers) {\n       while (it.hasNext()) {\n         Container allocated = it.next();\n         LOG.info(\"Assigning container \" + allocated);\n-        ContainerRequest assigned = assign(allocated);\n-          \n-        if (assigned != null) {\n-          // Update resource requests\n-          decContainerReq(assigned);\n+        \n+        // check if allocated container meets memory requirements \n+        // and whether we have any scheduled tasks that need \n+        // a container to be assigned\n+        boolean isAssignable = true;\n+        Priority priority = allocated.getPriority();\n+        if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+            || PRIORITY_MAP.equals(priority)) {\n+          if (allocated.getResource().getMemory() < mapResourceReqt\n+              || maps.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a map as either \"\n+                + \" container memory less than required \" + mapResourceReqt\n+                + \" or no pending map tasks - maps.isEmpty=\" \n+                + maps.isEmpty()); \n+            isAssignable = false; \n+          }\n+        } \n+        else if (PRIORITY_REDUCE.equals(priority)) {\n+          if (allocated.getResource().getMemory() < reduceResourceReqt\n+              || reduces.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a reduce as either \"\n+                + \" container memory less than required \" + reduceResourceReqt\n+                + \" or no pending reduce tasks - reduces.isEmpty=\" \n+                + reduces.isEmpty()); \n+            isAssignable = false;\n+          }\n+        }          \n+        \n+        boolean blackListed = false;         \n+        ContainerRequest assigned = null;\n+        \n+        if (isAssignable) {\n+          // do not assign if allocated container is on a  \n+          // blacklisted host\n+          blackListed = isNodeBlacklisted(allocated.getNodeId().getHost());\n+          if (blackListed) {\n+            // we need to request for a new container \n+            // and release the current one\n+            LOG.info(\"Got allocated container on a blacklisted \"\n+                + \" host. Releasing container \" + allocated);\n+\n+            // find the request matching this allocated container \n+            // and replace it with a new one \n+            ContainerRequest toBeReplacedReq = \n+                getContainerReqToReplace(allocated);\n+            if (toBeReplacedReq != null) {\n+              LOG.info(\"Placing a new container request for task attempt \" \n+                  + toBeReplacedReq.attemptID);\n+              ContainerRequest newReq = \n+                  getFilteredContainerRequest(toBeReplacedReq);\n+              decContainerReq(toBeReplacedReq);\n+              if (toBeReplacedReq.attemptID.getTaskId().getTaskType() ==\n+                  TaskType.MAP) {\n+                maps.put(newReq.attemptID, newReq);\n+              }\n+              else {\n+                reduces.put(newReq.attemptID, newReq);\n+              }\n+              addContainerReq(newReq);\n+            }\n+            else {\n+              LOG.info(\"Could not map allocated container to a valid request.\"\n+                  + \" Releasing allocated container \" + allocated);\n+            }\n+          }\n+          else {\n+            assigned = assign(allocated);\n+            if (assigned != null) {\n+              // Update resource requests\n+              decContainerReq(assigned);\n \n-          // send the container-assigned event to task attempt\n-          eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n-              assigned.attemptID, allocated));\n+              // send the container-assigned event to task attempt\n+              eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n+                  assigned.attemptID, allocated));\n \n-          assignedRequests.add(allocated.getId(), assigned.attemptID);\n-          \n-          LOG.info(\"Assigned container (\" + allocated + \") \" +\n-              \" to task \" + assigned.attemptID +\n-              \" on node \" + allocated.getNodeId().toString());\n-        } else {\n-          //not assigned to any request, release the container\n-          LOG.info(\"Releasing unassigned and invalid container \" + allocated\n-              + \". RM has gone crazy, someone go look!\"\n-              + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+              assignedRequests.add(allocated.getId(), assigned.attemptID);\n+\n+              LOG.info(\"Assigned container (\" + allocated + \") \" +\n+                  \" to task \" + assigned.attemptID +\n+                  \" on node \" + allocated.getNodeId().toString());\n+            }\n+            else {\n+              //not assigned to any request, release the container\n+              LOG.info(\"Releasing unassigned and invalid container \" \n+                  + allocated + \". RM has gone crazy, someone go look!\"\n+                  + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+            }\n+          }\n+        }\n+        \n+        // release container if it was blacklisted \n+        // or if we could not assign it \n+        if (blackListed || assigned == null) {\n           containersReleased++;\n           release(allocated.getId());\n         }\n@@ -604,12 +667,37 @@ private ContainerRequest assign(Container allocated) {\n       return assigned;\n     }\n     \n+    private ContainerRequest getContainerReqToReplace(Container allocated) {\n+      Priority priority = allocated.getPriority();\n+      ContainerRequest toBeReplaced = null;\n+      if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+          || PRIORITY_MAP.equals(priority)) {\n+        // allocated container was for a map\n+        String host = allocated.getNodeId().getHost();\n+        LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n+        if (list != null && list.size() > 0) {\n+          TaskAttemptId tId = list.removeLast();\n+          if (maps.containsKey(tId)) {\n+            toBeReplaced = maps.remove(tId);\n+          }\n+        }\n+        else {\n+          TaskAttemptId tId = maps.keySet().iterator().next();\n+          toBeReplaced = maps.remove(tId);          \n+        }        \n+      }\n+      else if (PRIORITY_REDUCE.equals(priority)) {\n+        TaskAttemptId tId = reduces.keySet().iterator().next();\n+        toBeReplaced = reduces.remove(tId);    \n+      }\n+      return toBeReplaced;\n+    }\n+    \n     \n     private ContainerRequest assignToFailedMap(Container allocated) {\n       //try to assign to earlierFailedMaps if present\n       ContainerRequest assigned = null;\n-      while (assigned == null && earlierFailedMaps.size() > 0 && \n-          allocated.getResource().getMemory() >= mapResourceReqt) {\n+      while (assigned == null && earlierFailedMaps.size() > 0) {\n         TaskAttemptId tId = earlierFailedMaps.removeFirst();\n         if (maps.containsKey(tId)) {\n           assigned = maps.remove(tId);\n@@ -627,8 +715,7 @@ private ContainerRequest assignToFailedMap(Container allocated) {\n     private ContainerRequest assignToReduce(Container allocated) {\n       ContainerRequest assigned = null;\n       //try to assign to reduces if present\n-      if (assigned == null && reduces.size() > 0\n-          && allocated.getResource().getMemory() >= reduceResourceReqt) {\n+      if (assigned == null && reduces.size() > 0) {\n         TaskAttemptId tId = reduces.keySet().iterator().next();\n         assigned = reduces.remove(tId);\n         LOG.info(\"Assigned to reduce\");\n@@ -640,9 +727,8 @@ private ContainerRequest assignToMap(Container allocated) {\n     //try to assign to maps if present \n       //first by host, then by rack, followed by *\n       ContainerRequest assigned = null;\n-      while (assigned == null && maps.size() > 0\n-          && allocated.getResource().getMemory() >= mapResourceReqt) {\n-        String host = getHost(allocated.getNodeId().toString());\n+      while (assigned == null && maps.size() > 0) {\n+        String host = allocated.getNodeId().getHost();\n         LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n         while (list != null && list.size() > 0) {\n           LOG.info(\"Host matched to the request list \" + host);",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "sha": "fc45b8f11b285a1bb0fc475c184f61b8e1c65175",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 11,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.hadoop.mapreduce.v2.app.rm;\n \n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -63,7 +65,7 @@\n   //Key->ResourceName (e.g., hostname, rackname, *)\n   //Value->Map\n   //Key->Resource Capability\n-  //Value->ResourceReqeust\n+  //Value->ResourceRequest\n   private final Map<Priority, Map<String, Map<Resource, ResourceRequest>>>\n   remoteRequestsTable =\n       new TreeMap<Priority, Map<String, Map<Resource, ResourceRequest>>>();\n@@ -87,14 +89,22 @@ public RMContainerRequestor(ClientService clientService, AppContext context) {\n     final String[] racks;\n     //final boolean earlierAttemptFailed;\n     final Priority priority;\n+    \n     public ContainerRequest(ContainerRequestEvent event, Priority priority) {\n-      this.attemptID = event.getAttemptID();\n-      this.capability = event.getCapability();\n-      this.hosts = event.getHosts();\n-      this.racks = event.getRacks();\n-      //this.earlierAttemptFailed = event.getEarlierAttemptFailed();\n+      this(event.getAttemptID(), event.getCapability(), event.getHosts(),\n+          event.getRacks(), priority);\n+    }\n+    \n+    public ContainerRequest(TaskAttemptId attemptID,\n+        Resource capability, String[] hosts, String[] racks, \n+        Priority priority) {\n+      this.attemptID = attemptID;\n+      this.capability = capability;\n+      this.hosts = hosts;\n+      this.racks = racks;\n       this.priority = priority;\n     }\n+    \n   }\n \n   @Override\n@@ -149,14 +159,37 @@ protected void containerFailedOnHost(String hostName) {\n       //remove all the requests corresponding to this hostname\n       for (Map<String, Map<Resource, ResourceRequest>> remoteRequests \n           : remoteRequestsTable.values()){\n-        //remove from host\n-        Map<Resource, ResourceRequest> reqMap = remoteRequests.remove(hostName);\n+        //remove from host if no pending allocations\n+        boolean foundAll = true;\n+        Map<Resource, ResourceRequest> reqMap = remoteRequests.get(hostName);\n         if (reqMap != null) {\n           for (ResourceRequest req : reqMap.values()) {\n-            ask.remove(req);\n+            if (!ask.remove(req)) {\n+              foundAll = false;\n+            }\n+            else {\n+              // if ask already sent to RM, we can try and overwrite it if possible.\n+              // send a new ask to RM with numContainers\n+              // specified for the blacklisted host to be 0.\n+              ResourceRequest zeroedRequest = BuilderUtils.newResourceRequest(req);\n+              zeroedRequest.setNumContainers(0);\n+              // to be sent to RM on next heartbeat\n+              ask.add(zeroedRequest);\n+            }\n           }\n+          // if all requests were still in ask queue\n+          // we can remove this request\n+          if (foundAll) {\n+            remoteRequests.remove(hostName);\n+          }     \n         }\n-        //TODO: remove from rack\n+        // TODO handling of rack blacklisting\n+        // Removing from rack should be dependent on no. of failures within the rack \n+        // Blacklisting a rack on the basis of a single node's blacklisting \n+        // may be overly aggressive. \n+        // Node failures could be co-related with other failures on the same rack \n+        // but we probably need a better approach at trying to decide how and when \n+        // to blacklist a rack\n       }\n     } else {\n       nodeFailures.put(hostName, failures);\n@@ -171,7 +204,9 @@ protected void addContainerReq(ContainerRequest req) {\n     // Create resource requests\n     for (String host : req.hosts) {\n       // Data-local\n-      addResourceRequest(req.priority, host, req.capability);\n+      if (!isNodeBlacklisted(host)) {\n+        addResourceRequest(req.priority, host, req.capability);\n+      }      \n     }\n \n     // Nothing Rack-local for now\n@@ -234,6 +269,14 @@ private void decResourceRequest(Priority priority, String resourceName,\n     Map<String, Map<Resource, ResourceRequest>> remoteRequests =\n       this.remoteRequestsTable.get(priority);\n     Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);\n+    if (reqMap == null) {\n+      // as we modify the resource requests by filtering out blacklisted hosts \n+      // when they are added, this value may be null when being \n+      // decremented\n+      LOG.debug(\"Not decrementing resource as \" + resourceName\n+          + \" is not present in request table\");\n+      return;\n+    }\n     ResourceRequest remoteRequest = reqMap.get(capability);\n \n     LOG.info(\"BEFORE decResourceRequest:\" + \" applicationId=\" + applicationId.getId()\n@@ -267,4 +310,23 @@ protected void release(ContainerId containerId) {\n     release.add(containerId);\n   }\n   \n+  protected boolean isNodeBlacklisted(String hostname) {\n+    if (!nodeBlacklistingEnabled) {\n+      return false;\n+    }\n+    return blacklistedNodes.contains(hostname);\n+  }\n+  \n+  protected ContainerRequest getFilteredContainerRequest(ContainerRequest orig) {\n+    ArrayList<String> newHosts = new ArrayList<String>();\n+    for (String host : orig.hosts) {\n+      if (!isNodeBlacklisted(host)) {\n+        newHosts.add(host);      \n+      }\n+    }\n+    String[] hosts = newHosts.toArray(new String[newHosts.size()]);\n+    ContainerRequest newReq = new ContainerRequest(orig.attemptID, orig.capability,\n+        hosts, orig.racks, orig.priority); \n+    return newReq;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "sha": "cfedde2229aadb4355bc2ea93f25d3d13d83e858",
                "status": "modified"
            },
            {
                "additions": 121,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "changes": 121,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobReport;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n@@ -44,6 +45,7 @@\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptContainerAssignedEvent;\n import org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl;\n+import org.apache.hadoop.mapreduce.v2.app.rm.ContainerFailedEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator;\n import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\n@@ -478,6 +480,105 @@ public void testReportedAppProgressWithOnlyMaps() throws Exception {\n     Assert.assertEquals(100.0f, app.getProgress(), 0.0);\n   }\n \n+  @Test\n+  public void testBlackListedNodes() throws Exception {\n+    \n+    LOG.info(\"Running testBlackListedNodes\");\n+\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\n+    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\n+    \n+    MyResourceManager rm = new MyResourceManager(conf);\n+    rm.start();\n+    DrainDispatcher dispatcher = (DrainDispatcher) rm.getRMContext()\n+        .getDispatcher();\n+\n+    // Submit the application\n+    RMApp app = rm.submitApp(1024);\n+    dispatcher.await();\n+\n+    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\n+    amNodeManager.nodeHeartbeat(true);\n+    dispatcher.await();\n+\n+    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt()\n+        .getAppAttemptId();\n+    rm.sendAMLaunched(appAttemptId);\n+    dispatcher.await();\n+    \n+    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getReport()).thenReturn(\n+        MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING,\n+            0, 0, 0, 0, 0, 0, \"jobfile\"));\n+    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf,\n+        appAttemptId, mockJob);\n+\n+    // add resources to scheduler\n+    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\n+    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\n+    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\n+    dispatcher.await();\n+\n+    // create the container request\n+    ContainerRequestEvent event1 = createReq(jobId, 1, 1024,\n+        new String[] { \"h1\" });\n+    allocator.sendRequest(event1);\n+\n+    // send 1 more request with different resource req\n+    ContainerRequestEvent event2 = createReq(jobId, 2, 1024,\n+        new String[] { \"h2\" });\n+    allocator.sendRequest(event2);\n+\n+    // send another request with different resource and priority\n+    ContainerRequestEvent event3 = createReq(jobId, 3, 1024,\n+        new String[] { \"h3\" });\n+    allocator.sendRequest(event3);\n+\n+    // this tells the scheduler about the requests\n+    // as nodes are not added, no allocations\n+    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\n+\n+    // Send events to blacklist nodes h1 and h2\n+    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h1\", false);            \n+    allocator.sendFailure(f1);\n+    ContainerFailedEvent f2 = createFailEvent(jobId, 1, \"h2\", false);            \n+    allocator.sendFailure(f2);\n+\n+    // update resources in scheduler\n+    nodeManager1.nodeHeartbeat(true); // Node heartbeat\n+    nodeManager2.nodeHeartbeat(true); // Node heartbeat\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    // mark h1/h2 as bad nodes\n+    nodeManager1.nodeHeartbeat(false);\n+    nodeManager2.nodeHeartbeat(false);\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    nodeManager3.nodeHeartbeat(true); // Node heartbeat\n+    assigned = allocator.schedule();    \n+    dispatcher.await();\n+        \n+    Assert.assertTrue(\"No of assignments must be 3\", assigned.size() == 3);\n+    \n+    // validate that all containers are assigned to h3\n+    for (TaskAttemptContainerAssignedEvent assig : assigned) {\n+      Assert.assertTrue(\"Assigned container host not correct\", \"h3\".equals(assig\n+          .getContainer().getNodeId().getHost()));\n+    }\n+  }\n+  \n   private static class MyFifoScheduler extends FifoScheduler {\n \n     public MyFifoScheduler(RMContext rmContext) {\n@@ -534,6 +635,19 @@ private ContainerRequestEvent createReq(JobId jobId, int taskAttemptId,\n         new String[] { NetworkTopology.DEFAULT_RACK });\n   }\n \n+  private ContainerFailedEvent createFailEvent(JobId jobId, int taskAttemptId,\n+      String host, boolean reduce) {\n+    TaskId taskId;\n+    if (reduce) {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\n+    } else {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\n+    }\n+    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId,\n+        taskAttemptId);\n+    return new ContainerFailedEvent(attemptId, host);    \n+  }\n+  \n   private void checkAssignments(ContainerRequestEvent[] requests,\n       List<TaskAttemptContainerAssignedEvent> assignments,\n       boolean checkHostMatch) {\n@@ -653,6 +767,10 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n       }\n     }\n \n+    public void sendFailure(ContainerFailedEvent f) {\n+      super.handle(f);\n+    }\n+    \n     // API to be used by tests\n     public List<TaskAttemptContainerAssignedEvent> schedule() {\n       // run the scheduler\n@@ -672,6 +790,7 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n     protected void startAllocatorThread() {\n       // override to NOT start thread\n     }\n+        \n   }\n \n   public static void main(String[] args) throws Exception {\n@@ -681,5 +800,7 @@ public static void main(String[] args) throws Exception {\n     t.testMapReduceScheduling();\n     t.testReportedAppProgress();\n     t.testReportedAppProgressWithOnlyMaps();\n+    t.testBlackListedNodes();\n   }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "sha": "dfbae8092c0b8f0e576ea16ce911cf9eb47e4e40",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2693. Fix NPE in job-blacklisting. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186529 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ba66ca6856bb4fc8bba786a59051517f19b15d8f",
        "patched_files": [
            "CHANGES.java",
            "RMContainerRequestor.java",
            "RMContainerAllocator.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMContainerAllocator.java"
        ]
    },
    "hadoop_7545ce6": {
        "bug_id": "hadoop_7545ce6",
        "commit": "https://github.com/apache/hadoop/commit/7545ce6636066a05763744a817878e03ee87f456",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=7545ce6636066a05763744a817878e03ee87f456",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -780,6 +780,9 @@ Release 2.8.0 - UNRELEASED\n     (Larry McCay via cnauroth)\n \n   IMPROVEMENTS\n+    \n+    HADOOP-12831. LocalFS/FSOutputSummer NPEs in constructor if bytes per checksum \n+    set to 0 (Mingliang Liu via gtcarrera9)\n \n     HADOOP-12458. Retries is typoed to spell Retires in parts of\n     hadoop-yarn and hadoop-common",
                "raw_url": "https://github.com/apache/hadoop/raw/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "9f952211c09ef471a77b06717bdb2af0d3b14933",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java?ref=7545ce6636066a05763744a817878e03ee87f456",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.nio.channels.ClosedChannelException;\n import java.util.Arrays;\n \n+import com.google.common.base.Preconditions;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n@@ -61,6 +62,9 @@ public void setConf(Configuration conf) {\n     if (conf != null) {\n       bytesPerChecksum = conf.getInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY,\n \t\t                     LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_DEFAULT);\n+      Preconditions.checkState(bytesPerChecksum > 0,\n+          \"bytes per checksum should be positive but was %s\",\n+          bytesPerChecksum);\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java",
                "sha": "c19be3d188b6196fd55e644e83f3aaf0ee494c73",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java?ref=7545ce6636066a05763744a817878e03ee87f456",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java",
                "patch": "@@ -228,6 +228,29 @@ public void testRenameFileIntoDirFile() throws Exception {\n   }\n \n \n+  @Test\n+  public void testSetConf() {\n+    Configuration conf = new Configuration();\n+\n+    conf.setInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY, 0);\n+    try {\n+      localFs.setConf(conf);\n+      fail(\"Should have failed because zero bytes per checksum is invalid\");\n+    } catch (IllegalStateException ignored) {\n+    }\n+\n+    conf.setInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY, -1);\n+    try {\n+      localFs.setConf(conf);\n+      fail(\"Should have failed because negative bytes per checksum is invalid\");\n+    } catch (IllegalStateException ignored) {\n+    }\n+\n+    conf.setInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY, 512);\n+    localFs.setConf(conf);\n+\n+  }\n+\n   void verifyRename(Path srcPath, Path dstPath, boolean dstIsDir)\n       throws Exception { \n     localFs.delete(srcPath,true);",
                "raw_url": "https://github.com/apache/hadoop/raw/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java",
                "sha": "923d21967c097db341eb7f437b1c62e7e89641cf",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12831. LocalFS/FSOutputSummer NPEs in constructor if bytes per checksum set to 0 (Mingliang Liu via gtcarrera9)",
        "parent": "https://github.com/apache/hadoop/commit/03cfb454fe5a1351e283e4678ad1b432ed231485",
        "patched_files": [
            "CHANGES.java",
            "ChecksumFileSystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestChecksumFileSystem.java"
        ]
    },
    "hadoop_766544c": {
        "bug_id": "hadoop_766544c",
        "commit": "https://github.com/apache/hadoop/commit/766544c0b008da9e78bcea6285b2c478653df75a",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/766544c0b008da9e78bcea6285b2c478653df75a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java?ref=766544c0b008da9e78bcea6285b2c478653df75a",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "patch": "@@ -105,7 +105,7 @@ private void launch() throws IOException, YarnException {\n     connect();\n     ContainerId masterContainerID = masterContainer.getId();\n     ApplicationSubmissionContext applicationContext =\n-      application.getSubmissionContext();\n+        application.getSubmissionContext();\n     LOG.info(\"Setting up container \" + masterContainer\n         + \" for AM \" + application.getAppAttemptId());\n     ContainerLaunchContext launchContext =\n@@ -189,6 +189,10 @@ private ContainerLaunchContext createAMContainerLaunchContext(\n     ContainerLaunchContext container =\n         applicationMasterContext.getAMContainerSpec();\n \n+    if (container == null){\n+      throw new IOException(containerID +\n+            \" has been cleaned before launched\");\n+    }\n     // Finalize the container\n     setupTokens(container, containerID);\n     // set the flow context optionally for timeline service v.2\n@@ -305,11 +309,7 @@ public void run() {\n         handler.handle(new RMAppAttemptEvent(application.getAppAttemptId(),\n             RMAppAttemptEventType.LAUNCHED, System.currentTimeMillis()));\n       } catch(Exception ie) {\n-        String message = \"Error launching \" + application.getAppAttemptId()\n-            + \". Got exception: \" + StringUtils.stringifyException(ie);\n-        LOG.info(message);\n-        handler.handle(new RMAppAttemptEvent(application\n-            .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED, message));\n+        onAMLaunchFailed(masterContainer.getId(), ie);\n       }\n       break;\n     case CLEANUP:\n@@ -344,4 +344,13 @@ private void parseAndThrowException(Throwable t) throws YarnException,\n       throw (IOException) t;\n     }\n   }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  protected void onAMLaunchFailed(ContainerId containerId, Exception ie) {\n+    String message = \"Error launching \" + application.getAppAttemptId()\n+            + \". Got exception: \" + StringUtils.stringifyException(ie);\n+    LOG.info(message);\n+    handler.handle(new RMAppAttemptEvent(application\n+           .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED, message));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/766544c0b008da9e78bcea6285b2c478653df75a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "sha": "0bedb528dc4cb2db444ffcda4da0274f052ea00d",
                "status": "modified"
            },
            {
                "additions": 68,
                "blob_url": "https://github.com/apache/hadoop/blob/766544c0b008da9e78bcea6285b2c478653df75a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationMasterLauncher.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationMasterLauncher.java?ref=766544c0b008da9e78bcea6285b2c478653df75a",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationMasterLauncher.java",
                "patch": "@@ -24,12 +24,14 @@\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.concurrent.TimeoutException;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.DataOutputBuffer;\n import org.apache.hadoop.security.Credentials;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.api.ApplicationConstants;\n import org.apache.hadoop.yarn.api.ContainerManagementProtocol;\n import org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse;\n@@ -73,6 +75,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n import org.apache.hadoop.yarn.server.utils.AMRMClientUtils;\n@@ -83,6 +86,9 @@\n import org.junit.Assert;\n import org.junit.Test;\n \n+import com.google.common.base.Supplier;\n+\n+import static org.junit.Assert.fail;\n import static org.mockito.Matchers.any;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n@@ -216,10 +222,14 @@ public void testAMLaunchAndCleanup() throws Exception {\n     // kick the scheduling\n     nm1.nodeHeartbeat(true);\n \n-    int waitCount = 0;\n-    while (containerManager.launched == false && waitCount++ < 20) {\n-      LOG.info(\"Waiting for AM Launch to happen..\");\n-      Thread.sleep(1000);\n+    try {\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override public Boolean get() {\n+          return containerManager.launched;\n+        }\n+      }, 100, 200 * 100);\n+    } catch (TimeoutException e) {\n+      fail(\"timed out while waiting for AM Launch to happen.\");\n     }\n     Assert.assertTrue(containerManager.launched);\n \n@@ -233,7 +243,7 @@ public void testAMLaunchAndCleanup() throws Exception {\n         .getMasterContainer().getId()\n         .toString(), containerManager.containerIdAtContainerManager);\n     Assert.assertEquals(nm1.getNodeId().toString(),\n-      containerManager.nmHostAtContainerManager);\n+        containerManager.nmHostAtContainerManager);\n     Assert.assertEquals(YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS,\n         containerManager.maxAppAttempts);\n \n@@ -246,17 +256,63 @@ public void testAMLaunchAndCleanup() throws Exception {\n     nm1.nodeHeartbeat(attempt.getAppAttemptId(), 1, ContainerState.COMPLETE);\n     rm.waitForState(am.getApplicationAttemptId(), RMAppAttemptState.FINISHED);\n \n-    waitCount = 0;\n-    while (containerManager.cleanedup == false && waitCount++ < 20) {\n-      LOG.info(\"Waiting for AM Cleanup to happen..\");\n-      Thread.sleep(1000);\n+    try {\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override public Boolean get() {\n+          return containerManager.cleanedup;\n+        }\n+      }, 100, 200 * 100);\n+    } catch (TimeoutException e) {\n+      fail(\"timed out while waiting for AM cleanup to happen.\");\n     }\n     Assert.assertTrue(containerManager.cleanedup);\n \n     rm.waitForState(am.getApplicationAttemptId(), RMAppAttemptState.FINISHED);\n     rm.stop();\n   }\n \n+  @Test\n+  public void testAMCleanupBeforeLaunch() throws Exception {\n+    MockRM rm = new MockRM();\n+    rm.start();\n+    MockNM nm1 = rm.registerNode(\"127.0.0.1:1234\", 5120);\n+    RMApp app = rm.submitApp(2000);\n+    // kick the scheduling\n+    nm1.nodeHeartbeat(true);\n+    RMAppAttempt attempt = app.getCurrentAppAttempt();\n+\n+    try {\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override public Boolean get() {\n+          return attempt.getMasterContainer() != null;\n+        }\n+      }, 10, 200 * 100);\n+    } catch (TimeoutException e) {\n+      fail(\"timed out while waiting for AM Launch to happen.\");\n+    }\n+\n+    //send kill before launch\n+    rm.killApp(app.getApplicationId());\n+    rm.waitForState(app.getApplicationId(), RMAppState.KILLED);\n+    //Launch after kill\n+    AMLauncher launcher = new AMLauncher(rm.getRMContext(),\n+            attempt, AMLauncherEventType.LAUNCH, rm.getConfig()) {\n+        @Override\n+        public void onAMLaunchFailed(ContainerId containerId, Exception e) {\n+          Assert.assertFalse(\"NullPointerException happens \"\n+                 + \" while launching \" + containerId,\n+                   e instanceof NullPointerException);\n+        }\n+        @Override\n+        protected ContainerManagementProtocol getContainerMgrProxy(\n+            ContainerId containerId) {\n+          return new MyContainerManagerImpl();\n+        }\n+    };\n+    launcher.run();\n+    rm.stop();\n+  }\n+\n   @Test\n   public void testRetriesOnFailures() throws Exception {\n     final ContainerManagementProtocol mockProxy =\n@@ -303,7 +359,7 @@ protected YarnRPC getYarnRPC() {\n     rm.drainEvents();\n \n     MockRM.waitForState(app.getCurrentAppAttempt(),\n-      RMAppAttemptState.LAUNCHED, 500);\n+        RMAppAttemptState.LAUNCHED, 500);\n   }\n \n \n@@ -337,9 +393,9 @@ public void testallocateBeforeAMRegistration() throws Exception {\n \n     AllocateResponse amrs = null;\n     try {\n-        amrs = am.allocate(new ArrayList<ResourceRequest>(),\n+      amrs = am.allocate(new ArrayList<ResourceRequest>(),\n           new ArrayList<ContainerId>());\n-        Assert.fail();\n+      Assert.fail();\n     } catch (ApplicationMasterNotRegisteredException e) {\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/766544c0b008da9e78bcea6285b2c478653df75a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationMasterLauncher.java",
                "sha": "e518b9083af6ec7eef3e4aa89116af3c1adaa43c",
                "status": "modified"
            }
        ],
        "message": "YARN-7786. NullPointerException while launching ApplicationMaster. Contributed by lujie",
        "parent": "https://github.com/apache/hadoop/commit/85585f9eef26baded5b3ef7fcc3283aa37fc362d",
        "patched_files": [
            "ApplicationMasterLauncher.java",
            "AMLauncher.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationMasterLauncher.java"
        ]
    },
    "hadoop_76b94c2": {
        "bug_id": "hadoop_76b94c2",
        "commit": "https://github.com/apache/hadoop/commit/76b94c274fe9775efcfd51c676d80c88a4f7fdb9",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/76b94c274fe9775efcfd51c676d80c88a4f7fdb9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java?ref=76b94c274fe9775efcfd51c676d80c88a4f7fdb9",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "patch": "@@ -377,9 +377,21 @@ public void setDelegate(FairCallQueue<? extends Schedulable> obj) {\n       this.revisionNumber++;\n     }\n \n+    /**\n+     * Fetch the current call queue from the weak reference delegate. If there\n+     * is no delegate, or the delegate is empty, this will return null.\n+     */\n+    private FairCallQueue<? extends Schedulable> getCallQueue() {\n+      WeakReference<FairCallQueue<? extends Schedulable>> ref = this.delegate;\n+      if (ref == null) {\n+        return null;\n+      }\n+      return ref.get();\n+    }\n+\n     @Override\n     public int[] getQueueSizes() {\n-      FairCallQueue<? extends Schedulable> obj = this.delegate.get();\n+      FairCallQueue<? extends Schedulable> obj = getCallQueue();\n       if (obj == null) {\n         return new int[]{};\n       }\n@@ -389,7 +401,7 @@ public void setDelegate(FairCallQueue<? extends Schedulable> obj) {\n \n     @Override\n     public long[] getOverflowedCalls() {\n-      FairCallQueue<? extends Schedulable> obj = this.delegate.get();\n+      FairCallQueue<? extends Schedulable> obj = getCallQueue();\n       if (obj == null) {\n         return new long[]{};\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/76b94c274fe9775efcfd51c676d80c88a4f7fdb9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "sha": "b4e953948c657b8d443c0af6a31d603a5b7fd051",
                "status": "modified"
            }
        ],
        "message": "HADOOP-16345. Fix a potential NPE when instantiating FairCallQueue metrics. Contributed by Erik Krogen.",
        "parent": "https://github.com/apache/hadoop/commit/4e38dafde4dce8cd8c368783a291e830f06e1def",
        "patched_files": [
            "FairCallQueue.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFairCallQueue.java"
        ]
    },
    "hadoop_773f0d1": {
        "bug_id": "hadoop_773f0d1",
        "commit": "https://github.com/apache/hadoop/commit/773f0d1519715e3ddf77c139998cc12d7447da66",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java?ref=773f0d1519715e3ddf77c139998cc12d7447da66",
                "deletions": 2,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java",
                "patch": "@@ -95,15 +95,30 @@ private VolumeInfo(Builder b) throws IOException {\n     this.usage = new VolumeUsage(root, b.conf);\n   }\n \n-  public long getCapacity() {\n-    return configuredCapacity < 0 ? usage.getCapacity() : configuredCapacity;\n+  public long getCapacity() throws IOException {\n+    if (configuredCapacity < 0) {\n+      if (usage == null) {\n+        throw new IOException(\"Volume Usage thread is not running. This error\" +\n+            \" is usually seen during DataNode shutdown.\");\n+      }\n+      return usage.getCapacity();\n+    }\n+    return configuredCapacity;\n   }\n \n   public long getAvailable() throws IOException {\n+    if (usage == null) {\n+      throw new IOException(\"Volume Usage thread is not running. This error \" +\n+          \"is usually seen during DataNode shutdown.\");\n+    }\n     return usage.getAvailable();\n   }\n \n   public long getScmUsed() throws IOException {\n+    if (usage == null) {\n+      throw new IOException(\"Volume Usage thread is not running. This error \" +\n+          \"is usually seen during DataNode shutdown.\");\n+    }\n     return usage.getScmUsed();\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeInfo.java",
                "sha": "0de9f18cf981f6036d551a3bfd4d1ecc593b2046",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java?ref=773f0d1519715e3ddf77c139998cc12d7447da66",
                "deletions": 4,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "patch": "@@ -372,18 +372,21 @@ public void shutdown() {\n       for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n         hddsVolume = entry.getValue();\n         VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n-        long scmUsed = 0;\n-        long remaining = 0;\n+        long scmUsed;\n+        long remaining;\n+        long capacity;\n         failed = false;\n         try {\n           scmUsed = volumeInfo.getScmUsed();\n           remaining = volumeInfo.getAvailable();\n+          capacity = volumeInfo.getCapacity();\n         } catch (IOException ex) {\n           LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n-              \"storage location {}\", volumeInfo.getRootDir());\n+              \"storage location {}\", volumeInfo.getRootDir(), ex);\n           // reset scmUsed and remaining if df/du failed.\n           scmUsed = 0;\n           remaining = 0;\n+          capacity = 0;\n           failed = true;\n         }\n \n@@ -392,7 +395,7 @@ public void shutdown() {\n         builder.setStorageLocation(volumeInfo.getRootDir())\n             .setId(hddsVolume.getStorageID())\n             .setFailed(failed)\n-            .setCapacity(hddsVolume.getCapacity())\n+            .setCapacity(capacity)\n             .setRemaining(remaining)\n             .setScmUsed(scmUsed)\n             .setStorageType(hddsVolume.getStorageType());",
                "raw_url": "https://github.com/apache/hadoop/raw/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "sha": "d30dd89ee05c29a658a37911a9301ef2e9a7c5b6",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java?ref=773f0d1519715e3ddf77c139998cc12d7447da66",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.mockito.Mockito;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.Properties;\n import java.util.UUID;\n \n@@ -134,12 +135,14 @@ public void testShutdown() throws Exception{\n         scmUsedFile.exists());\n \n     try {\n-      // Volume.getAvailable() should fail with NullPointerException as usage\n-      // is shutdown.\n+      // Volume.getAvailable() should fail with IOException\n+      // as usage thread is shutdown.\n       volume.getAvailable();\n       fail(\"HddsVolume#shutdown test failed\");\n     } catch (Exception ex){\n-      assertTrue(ex instanceof NullPointerException);\n+      assertTrue(ex instanceof IOException);\n+      assertTrue(ex.getMessage().contains(\n+          \"Volume Usage thread is not running.\"));\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestHddsVolume.java",
                "sha": "6b467622738bbcfad11ff6b0cb71268f7d362852",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java?ref=773f0d1519715e3ddf77c139998cc12d7447da66",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java",
                "patch": "@@ -222,8 +222,10 @@ public void testShutdown() throws Exception {\n         // getAvailable() should throw null pointer exception as usage is null.\n         volume.getAvailable();\n         fail(\"Volume shutdown failed.\");\n-      } catch (NullPointerException ex) {\n+      } catch (IOException ex) {\n         // Do Nothing. Exception is expected.\n+        assertTrue(ex.getMessage().contains(\n+            \"Volume Usage thread is not running.\"));\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/volume/TestVolumeSet.java",
                "sha": "7bb8a43d7b4c64e44b7340de30046cccfc85e5dc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java?ref=773f0d1519715e3ddf77c139998cc12d7447da66",
                "deletions": 1,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java",
                "patch": "@@ -99,7 +99,7 @@ public static void shutdown() {\n     }\n   }\n \n-  @Test\n+  @Test(timeout = 300_000L)\n   public void testPipelineFail() throws InterruptedException, IOException,\n       TimeoutException {\n     Assert.assertEquals(ratisContainer1.getPipeline().getPipelineState(),\n@@ -118,6 +118,7 @@ public void testPipelineFail() throws InterruptedException, IOException,\n         pipelineManager.getPipeline(ratisContainer2.getPipeline().getId())\n             .getPipelineState());\n     // Now restart the datanode and make sure that a new pipeline is created.\n+    cluster.setWaitForClusterToBeReadyTimeout(300000);\n     cluster.restartHddsDatanode(dnToFail, true);\n     ContainerWithPipeline ratisContainer3 =\n         containerManager.allocateContainer(RATIS, THREE, \"testOwner\");",
                "raw_url": "https://github.com/apache/hadoop/raw/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/pipeline/TestNodeFailure.java",
                "sha": "618cd8e3a12ab92156224a9e390a5630d05a70aa",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java?ref=773f0d1519715e3ddf77c139998cc12d7447da66",
                "deletions": 0,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java",
                "patch": "@@ -66,6 +66,14 @@ static Builder newBuilder(OzoneConfiguration conf) {\n    */\n   void waitForClusterToBeReady() throws TimeoutException, InterruptedException;\n \n+  /**\n+   * Sets the timeout value after which\n+   * {@link MiniOzoneCluster#waitForClusterToBeReady} times out.\n+   *\n+   * @param timeoutInMs timeout value in milliseconds\n+   */\n+  void setWaitForClusterToBeReadyTimeout(int timeoutInMs);\n+\n   /**\n    * Waits/blocks till the cluster is out of chill mode.\n    *",
                "raw_url": "https://github.com/apache/hadoop/raw/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneCluster.java",
                "sha": "15bf8d099ab8a803e4defd28f9faa679f21a78e3",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java?ref=773f0d1519715e3ddf77c139998cc12d7447da66",
                "deletions": 1,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java",
                "patch": "@@ -90,6 +90,9 @@\n   private final OzoneManager ozoneManager;\n   private final List<HddsDatanodeService> hddsDatanodes;\n \n+  // Timeout for the cluster to be ready\n+  private int waitForClusterToBeReadyTimeout = 60000; // 1 min\n+\n   /**\n    * Creates a new MiniOzoneCluster.\n    *\n@@ -122,7 +125,18 @@ public void waitForClusterToBeReady()\n           isReady? \"Cluster is ready\" : \"Waiting for cluster to be ready\",\n           healthy, hddsDatanodes.size());\n       return isReady;\n-    }, 1000, 60 * 1000); //wait for 1 min.\n+    }, 1000, waitForClusterToBeReadyTimeout);\n+  }\n+\n+  /**\n+   * Sets the timeout value after which\n+   * {@link MiniOzoneClusterImpl#waitForClusterToBeReady} times out.\n+   *\n+   * @param timeoutInMs timeout value in milliseconds\n+   */\n+  @Override\n+  public void setWaitForClusterToBeReadyTimeout(int timeoutInMs) {\n+    waitForClusterToBeReadyTimeout = timeoutInMs;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/773f0d1519715e3ddf77c139998cc12d7447da66/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneClusterImpl.java",
                "sha": "6c0f408123a4adf9bf466d860776e567f87598d4",
                "status": "modified"
            }
        ],
        "message": "HDDS-754. VolumeInfo#getScmUsed throws NPE.\nContributed by Hanisha Koneru.",
        "parent": "https://github.com/apache/hadoop/commit/e33b61f3351c09b00717f6eef32ff7d24345d06e",
        "patched_files": [
            "HddsVolume.java",
            "VolumeSet.java",
            "VolumeInfo.java",
            "MiniOzoneClusterImpl.java",
            "MiniOzoneCluster.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeFailure.java",
            "TestVolumeSet.java",
            "TestMiniOzoneCluster.java",
            "TestHddsVolume.java"
        ]
    },
    "hadoop_77721f3": {
        "bug_id": "hadoop_77721f3",
        "commit": "https://github.com/apache/hadoop/commit/77721f39e26b630352a1f4087524a3fbd21ff06e",
        "file": [
            {
                "additions": 110,
                "blob_url": "https://github.com/apache/hadoop/blob/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 179,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java?ref=77721f39e26b630352a1f4087524a3fbd21ff06e",
                "deletions": 69,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -40,6 +40,7 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.Date;\n import java.util.EnumMap;\n import java.util.HashMap;\n import java.util.Iterator;\n@@ -851,81 +852,121 @@ void spawnAutoRenewalThreadForUserCreds(boolean force) {\n     }\n \n     //spawn thread only if we have kerb credentials\n-    Thread t = new Thread(new Runnable() {\n+    KerberosTicket tgt = getTGT();\n+    if (tgt == null) {\n+      return;\n+    }\n+    String cmd = conf.get(\"hadoop.kerberos.kinit.command\", \"kinit\");\n+    long nextRefresh = getRefreshTime(tgt);\n+    Thread t =\n+        new Thread(new AutoRenewalForUserCredsRunnable(tgt, cmd, nextRefresh));\n+    t.setDaemon(true);\n+    t.setName(\"TGT Renewer for \" + getUserName());\n+    t.start();\n+  }\n+\n+  @VisibleForTesting\n+  class AutoRenewalForUserCredsRunnable implements Runnable {\n+    private KerberosTicket tgt;\n+    private RetryPolicy rp;\n+    private String kinitCmd;\n+    private long nextRefresh;\n+    private boolean runRenewalLoop = true;\n+\n+    AutoRenewalForUserCredsRunnable(KerberosTicket tgt, String kinitCmd,\n+        long nextRefresh){\n+      this.tgt = tgt;\n+      this.kinitCmd = kinitCmd;\n+      this.nextRefresh = nextRefresh;\n+      this.rp = null;\n+    }\n+\n+    public void setRunRenewalLoop(boolean runRenewalLoop) {\n+      this.runRenewalLoop = runRenewalLoop;\n+    }\n \n-      @Override\n-      public void run() {\n-        String cmd = conf.get(\"hadoop.kerberos.kinit.command\", \"kinit\");\n-        KerberosTicket tgt = getTGT();\n-        if (tgt == null) {\n+    @Override\n+    public void run() {\n+      do {\n+        try {\n+          long now = Time.now();\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Current time is \" + now);\n+            LOG.debug(\"Next refresh is \" + nextRefresh);\n+          }\n+          if (now < nextRefresh) {\n+            Thread.sleep(nextRefresh - now);\n+          }\n+          String output = Shell.execCommand(kinitCmd, \"-R\");\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Renewed ticket. kinit output: {}\", output);\n+          }\n+          reloginFromTicketCache();\n+          tgt = getTGT();\n+          if (tgt == null) {\n+            LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n+                getUserName());\n+            return;\n+          }\n+          nextRefresh = Math.max(getRefreshTime(tgt),\n+              now + kerberosMinSecondsBeforeRelogin);\n+          metrics.renewalFailures.set(0);\n+          rp = null;\n+        } catch (InterruptedException ie) {\n+          LOG.warn(\"Terminating renewal thread\");\n           return;\n-        }\n-        long nextRefresh = getRefreshTime(tgt);\n-        RetryPolicy rp = null;\n-        while (true) {\n+        } catch (IOException ie) {\n+          metrics.renewalFailuresTotal.incr();\n+          final long now = Time.now();\n+\n+          if (tgt.isDestroyed()) {\n+            LOG.error(\"TGT is destroyed. Aborting renew thread for {}.\",\n+                getUserName());\n+            return;\n+          }\n+\n+          long tgtEndTime;\n+          // As described in HADOOP-15593 we need to handle the case when\n+          // tgt.getEndTime() throws NPE because of JDK issue JDK-8147772\n+          // NPE is only possible if this issue is not fixed in the JDK\n+          // currently used\n           try {\n-            long now = Time.now();\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Current time is \" + now);\n-              LOG.debug(\"Next refresh is \" + nextRefresh);\n-            }\n-            if (now < nextRefresh) {\n-              Thread.sleep(nextRefresh - now);\n-            }\n-            String output = Shell.execCommand(cmd, \"-R\");\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Renewed ticket. kinit output: {}\", output);\n-            }\n-            reloginFromTicketCache();\n-            tgt = getTGT();\n-            if (tgt == null) {\n-              LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n-                  getUserName());\n-              return;\n-            }\n-            nextRefresh = Math.max(getRefreshTime(tgt),\n-              now + kerberosMinSecondsBeforeRelogin);\n-            metrics.renewalFailures.set(0);\n-            rp = null;\n-          } catch (InterruptedException ie) {\n-            LOG.warn(\"Terminating renewal thread\");\n+            tgtEndTime = tgt.getEndTime().getTime();\n+          } catch (NullPointerException npe) {\n+            LOG.error(\"NPE thrown while getting KerberosTicket endTime. \"\n+                + \"Aborting renew thread for {}.\", getUserName());\n+            return;\n+          }\n+\n+          LOG.warn(\"Exception encountered while running the renewal \"\n+                  + \"command for {}. (TGT end time:{}, renewalFailures: {},\"\n+                  + \"renewalFailuresTotal: {})\", getUserName(), tgtEndTime,\n+              metrics.renewalFailures.value(),\n+              metrics.renewalFailuresTotal.value(), ie);\n+          if (rp == null) {\n+            // Use a dummy maxRetries to create the policy. The policy will\n+            // only be used to get next retry time with exponential back-off.\n+            // The final retry time will be later limited within the\n+            // tgt endTime in getNextTgtRenewalTime.\n+            rp = RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2,\n+                kerberosMinSecondsBeforeRelogin, TimeUnit.MILLISECONDS);\n+          }\n+          try {\n+            nextRefresh = getNextTgtRenewalTime(tgtEndTime, now, rp);\n+          } catch (Exception e) {\n+            LOG.error(\"Exception when calculating next tgt renewal time\", e);\n+            return;\n+          }\n+          metrics.renewalFailures.incr();\n+          // retry until close enough to tgt endTime.\n+          if (now > nextRefresh) {\n+            LOG.error(\"TGT is expired. Aborting renew thread for {}.\",\n+                getUserName());\n             return;\n-          } catch (IOException ie) {\n-            metrics.renewalFailuresTotal.incr();\n-            final long tgtEndTime = tgt.getEndTime().getTime();\n-            LOG.warn(\"Exception encountered while running the renewal \"\n-                    + \"command for {}. (TGT end time:{}, renewalFailures: {},\"\n-                    + \"renewalFailuresTotal: {})\", getUserName(), tgtEndTime,\n-                metrics.renewalFailures, metrics.renewalFailuresTotal, ie);\n-            final long now = Time.now();\n-            if (rp == null) {\n-              // Use a dummy maxRetries to create the policy. The policy will\n-              // only be used to get next retry time with exponential back-off.\n-              // The final retry time will be later limited within the\n-              // tgt endTime in getNextTgtRenewalTime.\n-              rp = RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2,\n-                  kerberosMinSecondsBeforeRelogin, TimeUnit.MILLISECONDS);\n-            }\n-            try {\n-              nextRefresh = getNextTgtRenewalTime(tgtEndTime, now, rp);\n-            } catch (Exception e) {\n-              LOG.error(\"Exception when calculating next tgt renewal time\", e);\n-              return;\n-            }\n-            metrics.renewalFailures.incr();\n-            // retry until close enough to tgt endTime.\n-            if (now > nextRefresh) {\n-              LOG.error(\"TGT is expired. Aborting renew thread for {}.\",\n-                  getUserName());\n-              return;\n-            }\n           }\n         }\n-      }\n-    });\n-    t.setDaemon(true);\n-    t.setName(\"TGT Renewer for \" + getUserName());\n-    t.start();\n+      } while (runRenewalLoop);\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "6ce72edb8e20b66882256a98a0a944c4ce67ba79",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hadoop/blob/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java?ref=77721f39e26b630352a1f4087524a3fbd21ff06e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java",
                "patch": "@@ -47,6 +47,7 @@\n \n import javax.security.auth.Subject;\n import javax.security.auth.kerberos.KerberosPrincipal;\n+import javax.security.auth.kerberos.KerberosTicket;\n import javax.security.auth.kerberos.KeyTab;\n import javax.security.auth.login.AppConfigurationEntry;\n import javax.security.auth.login.LoginContext;\n@@ -61,6 +62,7 @@\n import java.util.Collection;\n import java.util.ConcurrentModificationException;\n import java.util.Date;\n+import java.util.HashSet;\n import java.util.LinkedHashSet;\n import java.util.Set;\n import java.util.concurrent.Callable;\n@@ -88,7 +90,10 @@\n import static org.junit.Assert.assertSame;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n+import static org.mockito.Mockito.atLeastOnce;\n+import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n \n public class TestUserGroupInformation {\n@@ -1211,4 +1216,37 @@ public UserGroupInformation run() throws Exception {\n     barrier.await();\n     assertSame(testUgi1.getSubject(), blockingLookup.get().getSubject());\n   }\n+\n+  @Test\n+  public void testKerberosTicketIsDestroyedChecked() throws Exception {\n+    // Create UserGroupInformation\n+    GenericTestUtils.setLogLevel(UserGroupInformation.LOG, Level.DEBUG);\n+    Set<User> users = new HashSet<>();\n+    users.add(new User(\"Foo\"));\n+    Subject subject =\n+        new Subject(true, users, new HashSet<>(), new HashSet<>());\n+    UserGroupInformation ugi = spy(new UserGroupInformation(subject));\n+\n+    // throw IOException in the middle of the autoRenewalForUserCreds\n+    doThrow(new IOException()).when(ugi).reloginFromTicketCache();\n+\n+    // Create and destroy the KerberosTicket, so endTime will be null\n+    Date d = new Date();\n+    KerberosPrincipal kp = new KerberosPrincipal(\"Foo\");\n+    KerberosTicket tgt = spy(new KerberosTicket(new byte[]{}, kp, kp, new\n+        byte[]{}, 0, null, d, d, d, d, null));\n+    tgt.destroy();\n+\n+    // run AutoRenewalForUserCredsRunnable with this\n+    UserGroupInformation.AutoRenewalForUserCredsRunnable userCredsRunnable =\n+        ugi.new AutoRenewalForUserCredsRunnable(tgt,\n+            Boolean.toString(Boolean.TRUE), 100);\n+\n+    // Set the runnable to not to run in a loop\n+    userCredsRunnable.setRunRenewalLoop(false);\n+    // there should be no exception when calling this\n+    userCredsRunnable.run();\n+    // isDestroyed should be called at least once\n+    Mockito.verify(tgt, atLeastOnce()).isDestroyed();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java",
                "sha": "011e930e50c7ea98d76f8f867a2a0cb0c423f0a7",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15593.  Fixed NPE in UGI spawnAutoRenewalThreadForUserCreds.\n               Contributed by Gabor Bota",
        "parent": "https://github.com/apache/hadoop/commit/40fad32824d2f8f960c779d78357e62103453da0",
        "patched_files": [
            "UserGroupInformation.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_778a4a2": {
        "bug_id": "hadoop_778a4a2",
        "commit": "https://github.com/apache/hadoop/commit/778a4a24be176382a5704f709c00bdfcfe6ddc8c",
        "file": [
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 55,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "patch": "@@ -694,62 +694,66 @@ public void requestContainers(long count) {\n       // composite constraints then this AND-ed composite constraint is not\n       // used.\n       PlacementConstraint finalConstraint = null;\n-      for (org.apache.hadoop.yarn.service.api.records.PlacementConstraint\n-          yarnServiceConstraint : placementPolicy.getConstraints()) {\n-        List<TargetExpression> targetExpressions = new ArrayList<>();\n-        // Currently only intra-application allocation tags are supported.\n-        if (!yarnServiceConstraint.getTargetTags().isEmpty()) {\n-          targetExpressions.add(PlacementTargets.allocationTag(\n-              yarnServiceConstraint.getTargetTags().toArray(new String[0])));\n-        }\n-        // Add all node attributes\n-        for (Map.Entry<String, List<String>> attribute : yarnServiceConstraint\n-            .getNodeAttributes().entrySet()) {\n-          targetExpressions.add(PlacementTargets.nodeAttribute(\n-              attribute.getKey(), attribute.getValue().toArray(new String[0])));\n-        }\n-        // Add all node partitions\n-        if (!yarnServiceConstraint.getNodePartitions().isEmpty()) {\n-          targetExpressions\n-              .add(PlacementTargets.nodePartition(yarnServiceConstraint\n-                  .getNodePartitions().toArray(new String[0])));\n-        }\n-        PlacementConstraint constraint = null;\n-        switch (yarnServiceConstraint.getType()) {\n-        case AFFINITY:\n-          constraint = PlacementConstraints\n-              .targetIn(yarnServiceConstraint.getScope().getValue(),\n-                  targetExpressions.toArray(new TargetExpression[0]))\n-              .build();\n-          break;\n-        case ANTI_AFFINITY:\n-          constraint = PlacementConstraints\n-              .targetNotIn(yarnServiceConstraint.getScope().getValue(),\n-                  targetExpressions.toArray(new TargetExpression[0]))\n-              .build();\n-          break;\n-        case AFFINITY_WITH_CARDINALITY:\n-          constraint = PlacementConstraints.targetCardinality(\n-              yarnServiceConstraint.getScope().name().toLowerCase(),\n-              yarnServiceConstraint.getMinCardinality() == null ? 0\n-                  : yarnServiceConstraint.getMinCardinality().intValue(),\n-              yarnServiceConstraint.getMaxCardinality() == null\n-                  ? Integer.MAX_VALUE\n-                  : yarnServiceConstraint.getMaxCardinality().intValue(),\n-              targetExpressions.toArray(new TargetExpression[0])).build();\n-          break;\n-        }\n-        // The default AND-ed final composite constraint\n-        if (finalConstraint != null) {\n-          finalConstraint = PlacementConstraints\n-              .and(constraint.getConstraintExpr(),\n-                  finalConstraint.getConstraintExpr())\n-              .build();\n-        } else {\n-          finalConstraint = constraint;\n+      if (placementPolicy != null) {\n+        for (org.apache.hadoop.yarn.service.api.records.PlacementConstraint\n+            yarnServiceConstraint : placementPolicy.getConstraints()) {\n+          List<TargetExpression> targetExpressions = new ArrayList<>();\n+          // Currently only intra-application allocation tags are supported.\n+          if (!yarnServiceConstraint.getTargetTags().isEmpty()) {\n+            targetExpressions.add(PlacementTargets.allocationTag(\n+                yarnServiceConstraint.getTargetTags().toArray(new String[0])));\n+          }\n+          // Add all node attributes\n+          for (Map.Entry<String, List<String>> attribute : yarnServiceConstraint\n+              .getNodeAttributes().entrySet()) {\n+            targetExpressions\n+                .add(PlacementTargets.nodeAttribute(attribute.getKey(),\n+                    attribute.getValue().toArray(new String[0])));\n+          }\n+          // Add all node partitions\n+          if (!yarnServiceConstraint.getNodePartitions().isEmpty()) {\n+            targetExpressions\n+                .add(PlacementTargets.nodePartition(yarnServiceConstraint\n+                    .getNodePartitions().toArray(new String[0])));\n+          }\n+          PlacementConstraint constraint = null;\n+          switch (yarnServiceConstraint.getType()) {\n+          case AFFINITY:\n+            constraint = PlacementConstraints\n+                .targetIn(yarnServiceConstraint.getScope().getValue(),\n+                    targetExpressions.toArray(new TargetExpression[0]))\n+                .build();\n+            break;\n+          case ANTI_AFFINITY:\n+            constraint = PlacementConstraints\n+                .targetNotIn(yarnServiceConstraint.getScope().getValue(),\n+                    targetExpressions.toArray(new TargetExpression[0]))\n+                .build();\n+            break;\n+          case AFFINITY_WITH_CARDINALITY:\n+            constraint = PlacementConstraints.targetCardinality(\n+                yarnServiceConstraint.getScope().name().toLowerCase(),\n+                yarnServiceConstraint.getMinCardinality() == null ? 0\n+                    : yarnServiceConstraint.getMinCardinality().intValue(),\n+                yarnServiceConstraint.getMaxCardinality() == null\n+                    ? Integer.MAX_VALUE\n+                    : yarnServiceConstraint.getMaxCardinality().intValue(),\n+                targetExpressions.toArray(new TargetExpression[0])).build();\n+            break;\n+          }\n+          // The default AND-ed final composite constraint\n+          if (finalConstraint != null) {\n+            finalConstraint = PlacementConstraints\n+                .and(constraint.getConstraintExpr(),\n+                    finalConstraint.getConstraintExpr())\n+                .build();\n+          } else {\n+            finalConstraint = constraint;\n+          }\n+          LOG.debug(\"[COMPONENT {}] Placement constraint: {}\",\n+              componentSpec.getName(),\n+              constraint.getConstraintExpr().toString());\n         }\n-        LOG.debug(\"[COMPONENT {}] Placement constraint: {}\",\n-            componentSpec.getName(), constraint.getConstraintExpr().toString());\n       }\n       ResourceSizing resourceSizing = ResourceSizing.newInstance((int) count,\n           resource);",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "sha": "a1ee7964b83ea9bd2efde2d1b685d14797fa6374",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "patch": "@@ -91,6 +91,14 @@\n \n   String ERROR_QUICKLINKS_FOR_COMP_INVALID = \"Quicklinks specified at\"\n       + \" component level, needs corresponding values set at service level\";\n+  // Note: %sin is not a typo. Constraint name is optional so the error messages\n+  // below handle that scenario by adding a space if name is specified.\n+  String ERROR_PLACEMENT_POLICY_CONSTRAINT_TYPE_NULL = \"Type not specified \"\n+      + \"for constraint %sin placement policy of component %s.\";\n+  String ERROR_PLACEMENT_POLICY_CONSTRAINT_SCOPE_NULL = \"Scope not specified \"\n+      + \"for constraint %sin placement policy of component %s.\";\n+  String ERROR_PLACEMENT_POLICY_CONSTRAINT_TAGS_NULL = \"Tag(s) not specified \"\n+      + \"for constraint %sin placement policy of component %s.\";\n   String ERROR_PLACEMENT_POLICY_TAG_NAME_NOT_SAME = \"Invalid target tag %s \"\n       + \"specified in placement policy of component %s. For now, target tags \"\n       + \"support self reference only. Specifying anything other than its \"",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "sha": "1d2d719d32badd9b1e9e5aa40e4722833cb2c1e4",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.yarn.service.api.records.Configuration;\n import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n+import org.apache.hadoop.yarn.service.api.records.PlacementPolicy;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n import org.apache.hadoop.yarn.service.exceptions.SliderException;\n import org.apache.hadoop.yarn.service.conf.RestApiConstants;\n@@ -314,9 +315,28 @@ public static void validateNameFormat(String name,\n   private static void validatePlacementPolicy(List<Component> components,\n       Set<String> componentNames) {\n     for (Component comp : components) {\n-      if (comp.getPlacementPolicy() != null) {\n-        for (PlacementConstraint constraint : comp.getPlacementPolicy()\n+      PlacementPolicy placementPolicy = comp.getPlacementPolicy();\n+      if (placementPolicy != null) {\n+        for (PlacementConstraint constraint : placementPolicy\n             .getConstraints()) {\n+          if (constraint.getType() == null) {\n+            throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TYPE_NULL,\n+              constraint.getName() == null ? \"\" : constraint.getName() + \" \",\n+              comp.getName()));\n+          }\n+          if (constraint.getScope() == null) {\n+            throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_SCOPE_NULL,\n+              constraint.getName() == null ? \"\" : constraint.getName() + \" \",\n+              comp.getName()));\n+          }\n+          if (constraint.getTargetTags().isEmpty()) {\n+            throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TAGS_NULL,\n+              constraint.getName() == null ? \"\" : constraint.getName() + \" \",\n+              comp.getName()));\n+          }\n           for (String targetTag : constraint.getTargetTags()) {\n             if (!comp.getName().equals(targetTag)) {\n               throw new IllegalArgumentException(String.format(",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "sha": "6101bf01363814d65773ba85e5804e456b9b7dfc",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "patch": "@@ -25,6 +25,8 @@\n import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n import org.apache.hadoop.yarn.service.api.records.PlacementPolicy;\n+import org.apache.hadoop.yarn.service.api.records.PlacementScope;\n+import org.apache.hadoop.yarn.service.api.records.PlacementType;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.exceptions.RestApiErrorMessages;\n@@ -503,13 +505,48 @@ public void testPlacementPolicy() throws IOException {\n     PlacementPolicy pp = new PlacementPolicy();\n     PlacementConstraint pc = new PlacementConstraint();\n     pc.setName(\"CA1\");\n-    pc.setTargetTags(Collections.singletonList(\"comp-invalid\"));\n     pp.setConstraints(Collections.singletonList(pc));\n     comp.setPlacementPolicy(pp);\n \n     try {\n       ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n-      Assert.fail(EXCEPTION_PREFIX + \"service with empty placement\");\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with no type\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(String.format(\n+          RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TYPE_NULL,\n+          \"CA1 \", \"comp-a\"), e.getMessage());\n+    }\n+\n+    // Set the type\n+    pc.setType(PlacementType.ANTI_AFFINITY);\n+\n+    try {\n+      ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with no scope\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(String.format(\n+          RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_SCOPE_NULL,\n+          \"CA1 \", \"comp-a\"), e.getMessage());\n+    }\n+\n+    // Set the scope\n+    pc.setScope(PlacementScope.NODE);\n+\n+    try {\n+      ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with no tag(s)\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(String.format(\n+          RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TAGS_NULL,\n+          \"CA1 \", \"comp-a\"), e.getMessage());\n+    }\n+\n+    // Set a target tag - but an invalid one\n+    pc.setTargetTags(Collections.singletonList(\"comp-invalid\"));\n+\n+    try {\n+      ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with invalid tag name\");\n     } catch (IllegalArgumentException e) {\n       assertEquals(\n           String.format(\n@@ -518,9 +555,10 @@ public void testPlacementPolicy() throws IOException {\n           e.getMessage());\n     }\n \n+    // Set valid target tags now\n     pc.setTargetTags(Collections.singletonList(\"comp-a\"));\n \n-    // now it should succeed\n+    // Finally it should succeed\n     try {\n       ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n     } catch (IllegalArgumentException e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "sha": "243c6b3a618145458a5eb37c7d3aa52d1e65c995",
                "status": "modified"
            }
        ],
        "message": "YARN-8350. NPE in service AM related to placement policy. Contributed by Gour Saha",
        "parent": "https://github.com/apache/hadoop/commit/96eefcc84aacc4cc82ad7e3e72c5bdad56f4a7b7",
        "patched_files": [
            "ServiceApiUtil.java",
            "Component.java",
            "RestApiErrorMessages.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestServiceApiUtil.java",
            "TestComponent.java"
        ]
    },
    "hadoop_78b05fd": {
        "bug_id": "hadoop_78b05fd",
        "commit": "https://github.com/apache/hadoop/commit/78b05fde6c41f7a6b2dc2d99b435d1d83242590c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java?ref=78b05fde6c41f7a6b2dc2d99b435d1d83242590c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
                "patch": "@@ -533,6 +533,9 @@ private static Object toJsonMap(\n \n   public static String toJsonString(\n       SnapshottableDirectoryStatus[] snapshottableDirectoryList) {\n+    if (snapshottableDirectoryList == null) {\n+      return toJsonString(\"SnapshottableDirectoryList\", null);\n+    }\n     Object[] a = new Object[snapshottableDirectoryList.length];\n     for (int i = 0; i < snapshottableDirectoryList.length; i++) {\n       a[i] = toJsonMap(snapshottableDirectoryList[i]);",
                "raw_url": "https://github.com/apache/hadoop/raw/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
                "sha": "43a252b1c3ed5c78875c32e6d02978372f8c932f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java?ref=78b05fde6c41f7a6b2dc2d99b435d1d83242590c",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "patch": "@@ -755,14 +755,16 @@ public void testWebHdfsSnapshottableDirectoryList() throws Exception {\n       final Path bar = new Path(\"/bar\");\n       dfs.mkdirs(foo);\n       dfs.mkdirs(bar);\n+      SnapshottableDirectoryStatus[] statuses =\n+          webHdfs.getSnapshottableDirectoryList();\n+      Assert.assertNull(statuses);\n       dfs.allowSnapshot(foo);\n       dfs.allowSnapshot(bar);\n       Path file0 = new Path(foo, \"file0\");\n       DFSTestUtil.createFile(dfs, file0, 100, (short) 1, 0);\n       Path file1 = new Path(bar, \"file1\");\n       DFSTestUtil.createFile(dfs, file1, 100, (short) 1, 0);\n-      SnapshottableDirectoryStatus[] statuses =\n-          webHdfs.getSnapshottableDirectoryList();\n+      statuses = webHdfs.getSnapshottableDirectoryList();\n       SnapshottableDirectoryStatus[] dfsStatuses =\n           dfs.getSnapshottableDirListing();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "sha": "224735546b9d0b5b772b1b0ad4398a2fb5832060",
                "status": "modified"
            }
        ],
        "message": "HDFS-13280. WebHDFS: Fix NPE in get snasphottable directory list call. Contributed by Lokesh Jain.",
        "parent": "https://github.com/apache/hadoop/commit/e71bc00a471422ddb26dd54e706f09f0fe09925c",
        "patched_files": [
            "JsonUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestWebHDFS.java",
            "TestJsonUtil.java"
        ]
    },
    "hadoop_7b25fb9": {
        "bug_id": "hadoop_7b25fb9",
        "commit": "https://github.com/apache/hadoop/commit/7b25fb949bf6f02df997beeca7df46c9e84c8d96",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/7b25fb949bf6f02df997beeca7df46c9e84c8d96/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java?ref=7b25fb949bf6f02df997beeca7df46c9e84c8d96",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hdfs.server.federation.resolver;\n \n+import static org.apache.hadoop.hdfs.client.HdfsClientConfigKeys.DFS_NAMESERVICES;\n+import static org.apache.hadoop.hdfs.client.HdfsClientConfigKeys.DeprecatedKeys.DFS_NAMESERVICE_ID;\n import static org.apache.hadoop.hdfs.server.federation.router.RBFConfigKeys.DFS_ROUTER_DEFAULT_NAMESERVICE;\n import static org.apache.hadoop.hdfs.server.federation.router.RBFConfigKeys.FEDERATION_MOUNT_TABLE_MAX_CACHE_SIZE;\n import static org.apache.hadoop.hdfs.server.federation.router.RBFConfigKeys.FEDERATION_MOUNT_TABLE_MAX_CACHE_SIZE_DEFAULT;\n@@ -42,7 +44,6 @@\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n-import org.apache.hadoop.HadoopIllegalArgumentException;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSUtil;\n@@ -149,14 +150,25 @@ private void registerCacheExternal() {\n    * @param conf Configuration for this resolver.\n    */\n   private void initDefaultNameService(Configuration conf) {\n-    try {\n-      this.defaultNameService = conf.get(\n-          DFS_ROUTER_DEFAULT_NAMESERVICE,\n-          DFSUtil.getNamenodeNameServiceId(conf));\n-    } catch (HadoopIllegalArgumentException e) {\n-      LOG.error(\"Cannot find default name service, setting it to the first\");\n+    this.defaultNameService = conf.get(\n+        DFS_ROUTER_DEFAULT_NAMESERVICE,\n+        DFSUtil.getNamenodeNameServiceId(conf));\n+\n+    if (defaultNameService == null) {\n+      LOG.warn(\n+          \"{} and {} is not set. Fallback to {} as the default name service.\",\n+          DFS_ROUTER_DEFAULT_NAMESERVICE, DFS_NAMESERVICE_ID, DFS_NAMESERVICES);\n       Collection<String> nsIds = DFSUtilClient.getNameServiceIds(conf);\n-      this.defaultNameService = nsIds.iterator().next();\n+      if (nsIds.isEmpty()) {\n+        this.defaultNameService = \"\";\n+      } else {\n+        this.defaultNameService = nsIds.iterator().next();\n+      }\n+    }\n+\n+    if (this.defaultNameService.equals(\"\")) {\n+      LOG.warn(\"Default name service is not set.\");\n+    } else {\n       LOG.info(\"Default name service: {}\", this.defaultNameService);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/7b25fb949bf6f02df997beeca7df46c9e84c8d96/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/resolver/MountTableResolver.java",
                "sha": "c264de32d2e3274ed9ba25041d3e99326c223541",
                "status": "modified"
            },
            {
                "additions": 82,
                "blob_url": "https://github.com/apache/hadoop/blob/7b25fb949bf6f02df997beeca7df46c9e84c8d96/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestInitializeMountTableResolver.java",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestInitializeMountTableResolver.java?ref=7b25fb949bf6f02df997beeca7df46c9e84c8d96",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestInitializeMountTableResolver.java",
                "patch": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.resolver;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.Test;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMESERVICE_ID;\n+import static org.apache.hadoop.hdfs.client.HdfsClientConfigKeys.DFS_NAMESERVICES;\n+import static org.apache.hadoop.hdfs.server.federation.router.RBFConfigKeys.DFS_ROUTER_DEFAULT_NAMESERVICE;\n+import static org.junit.Assert.assertEquals;\n+\n+/**\n+ * Test {@link MountTableResolver} initialization.\n+ */\n+public class TestInitializeMountTableResolver {\n+\n+  @Test\n+  public void testDefaultNameserviceIsMissing() {\n+    Configuration conf = new Configuration();\n+    MountTableResolver mountTable = new MountTableResolver(conf);\n+    assertEquals(\"\", mountTable.getDefaultNamespace());\n+  }\n+\n+  @Test\n+  public void testDefaultNameserviceWithEmptyString() {\n+    Configuration conf = new Configuration();\n+    conf.set(DFS_ROUTER_DEFAULT_NAMESERVICE, \"\");\n+    MountTableResolver mountTable = new MountTableResolver(conf);\n+    assertEquals(\"\", mountTable.getDefaultNamespace());\n+  }\n+\n+  @Test\n+  public void testRouterDefaultNameservice() {\n+    Configuration conf = new Configuration();\n+    conf.set(DFS_ROUTER_DEFAULT_NAMESERVICE, \"router_ns\"); // this is priority\n+    conf.set(DFS_NAMESERVICE_ID, \"ns_id\");\n+    conf.set(DFS_NAMESERVICES, \"nss\");\n+    MountTableResolver mountTable = new MountTableResolver(conf);\n+    assertEquals(\"router_ns\", mountTable.getDefaultNamespace());\n+  }\n+\n+  @Test\n+  public void testNameserviceID() {\n+    Configuration conf = new Configuration();\n+    conf.set(DFS_NAMESERVICE_ID, \"ns_id\"); // this is priority\n+    conf.set(DFS_NAMESERVICES, \"nss\");\n+    MountTableResolver mountTable = new MountTableResolver(conf);\n+    assertEquals(\"ns_id\", mountTable.getDefaultNamespace());\n+  }\n+\n+  @Test\n+  public void testSingleNameservices() {\n+    Configuration conf = new Configuration();\n+    conf.set(DFS_NAMESERVICES, \"ns1\");\n+    MountTableResolver mountTable = new MountTableResolver(conf);\n+    assertEquals(\"ns1\", mountTable.getDefaultNamespace());\n+  }\n+\n+  @Test\n+  public void testMultipleNameservices() {\n+    Configuration conf = new Configuration();\n+    conf.set(DFS_NAMESERVICES, \"ns1,ns2\");\n+    MountTableResolver mountTable = new MountTableResolver(conf);\n+    assertEquals(\"ns1\", mountTable.getDefaultNamespace());\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/7b25fb949bf6f02df997beeca7df46c9e84c8d96/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/resolver/TestInitializeMountTableResolver.java",
                "sha": "5db7531c4dddb3530dbda9192c82261ad1e86ce7",
                "status": "added"
            }
        ],
        "message": "HDFS-13743. RBF: Router throws NullPointerException due to the invalid initialization of MountTableResolver. Contributed by Takanobu Asanuma.",
        "parent": "https://github.com/apache/hadoop/commit/e6873dfde057e63ce5efa91f3061db3ee1b2e236",
        "patched_files": [
            "MountTableResolver.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestInitializeMountTableResolver.java",
            "TestMountTableResolver.java"
        ]
    },
    "hadoop_7ba5bba": {
        "bug_id": "hadoop_7ba5bba",
        "commit": "https://github.com/apache/hadoop/commit/7ba5bbac02b688f68a8d23671a1e869234b4cebe",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=7ba5bbac02b688f68a8d23671a1e869234b4cebe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -338,6 +338,9 @@ Trunk (Unreleased)\n \n     HDFS-8412. Fix the test failures in HTTPFS. (umamahesh)\n \n+    HDFS-8627. NPE thrown if unable to fetch token from Namenode\n+    (J.Andreina via vinayakumarb)\n+\n Release 2.8.0 - UNRELEASED\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop/raw/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "b065f98e73256032a5a524bf67bcb749cce1733f",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java?ref=7ba5bbac02b688f68a8d23671a1e869234b4cebe",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java",
                "patch": "@@ -176,14 +176,17 @@ static void saveDelegationToken(Configuration conf, FileSystem fs,\n                                   final String renewer, final Path tokenFile)\n           throws IOException {\n     Token<?> token = fs.getDelegationToken(renewer);\n-\n-    Credentials cred = new Credentials();\n-    cred.addToken(token.getKind(), token);\n-    cred.writeTokenStorageFile(tokenFile, conf);\n-\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Fetched token \" + fs.getUri() + \" for \" + token.getService()\n-              + \" into \" + tokenFile);\n+    if (null != token) {\n+      Credentials cred = new Credentials();\n+      cred.addToken(token.getKind(), token);\n+      cred.writeTokenStorageFile(tokenFile, conf);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Fetched token \" + fs.getUri() + \" for \" +\n+            token.getService() + \" into \" + tokenFile);\n+      }\n+    } else {\n+      System.err.println(\"ERROR: Failed to fetch token from \" + fs.getUri());\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java",
                "sha": "803402dc942b777a33cbd71ba3faa6cad6592472",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java?ref=7ba5bbac02b688f68a8d23671a1e869234b4cebe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "patch": "@@ -90,4 +90,19 @@ public void expectedTokenIsRetrievedFromHttp() throws Exception {\n     DelegationTokenFetcher.cancelTokens(conf, p);\n     Assert.assertEquals(testToken, FakeRenewer.getLastCanceled());\n   }\n+\n+  /**\n+   * If token returned is null, saveDelegationToken should not\n+   * throw nullPointerException\n+   */\n+  @Test\n+  public void testReturnedTokenIsNull() throws Exception {\n+    WebHdfsFileSystem fs = mock(WebHdfsFileSystem.class);\n+    doReturn(null).when(fs).getDelegationToken(anyString());\n+    Path p = new Path(f.getRoot().getAbsolutePath(), tokenFile);\n+    DelegationTokenFetcher.saveDelegationToken(conf, fs, null, p);\n+    // When Token returned is null, TokenFile should not exist\n+    Assert.assertFalse(p.getFileSystem(conf).exists(p));\n+\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "sha": "ab3933bb6865d3c36ddbd7b32df6b2f158738937",
                "status": "modified"
            }
        ],
        "message": "HDFS-8627. NPE thrown if unable to fetch token from Namenode (Contributed by J.Andreina)",
        "parent": "https://github.com/apache/hadoop/commit/6d99017f38f5a158b5cb65c74688b4c833e4e35f",
        "patched_files": [
            "DelegationTokenFetcher.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationTokenFetcher.java"
        ]
    },
    "hadoop_7c14f15": {
        "bug_id": "hadoop_7c14f15",
        "commit": "https://github.com/apache/hadoop/commit/7c14f1557b6097c193a22f4b690de6e602d311b5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt?ref=7c14f1557b6097c193a22f4b690de6e602d311b5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "patch": "@@ -329,3 +329,6 @@ Branch-2802 Snapshot (Unreleased)\n \n   HDFS-4758. Disallow nested snapshottable directories and unwrap\n   RemoteException.  (szetszwo)\n+\n+  HDFS-4781. Fix a NullPointerException when listing .snapshot under\n+  a non-existing directory.  (szetszwo)",
                "raw_url": "https://github.com/apache/hadoop/raw/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "sha": "083d972fe55b5363fff6fb45ae176a6513601f38",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=7c14f1557b6097c193a22f4b690de6e602d311b5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1588,7 +1588,8 @@ private HdfsFileStatus getFileInfo4DotSnapshot(String src)\n         src.length() - HdfsConstants.DOT_SNAPSHOT_DIR.length()));\n     \n     final INode node = this.getINode(dirPath);\n-    if (node.isDirectory()\n+    if (node != null\n+        && node.isDirectory()\n         && node.asDirectory() instanceof INodeDirectorySnapshottable) {\n       return new HdfsFileStatus(0, true, 0, 0, 0, 0, null, null, null, null,\n           HdfsFileStatus.EMPTY_NAME, -1L);",
                "raw_url": "https://github.com/apache/hadoop/raw/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "4462099be9e0398f7bad7df668887fbf1b027c67",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java?ref=7c14f1557b6097c193a22f4b690de6e602d311b5",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "patch": "@@ -313,12 +313,12 @@ public INodeSymlink asSymlink() {\n    * children.\n    * \n    * 1.3 The current inode is a {@link FileWithSnapshot}.\n-   * Call {@link INode#recordModification(Snapshot)} to capture the \n-   * current states. Mark the INode as deleted.\n+   * Call recordModification(..) to capture the current states.\n+   * Mark the INode as deleted.\n    * \n    * 1.4 The current inode is a {@link INodeDirectoryWithSnapshot}.\n-   * Call {@link INode#recordModification(Snapshot)} to capture the \n-   * current states. Destroy files/directories created after the latest snapshot \n+   * Call recordModification(..) to capture the current states. \n+   * Destroy files/directories created after the latest snapshot \n    * (i.e., the inodes stored in the created list of the latest snapshot).\n    * Recursively clean remaining children. \n    *",
                "raw_url": "https://github.com/apache/hadoop/raw/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "sha": "541f591113a0cacf819bd87a891188795779baf4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java?ref=7c14f1557b6097c193a22f4b690de6e602d311b5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java",
                "patch": "@@ -111,7 +111,7 @@ private int searchChildren(byte[] name) {\n    * Remove the specified child from this directory.\n    * \n    * @param child the child inode to be removed\n-   * @param latest See {@link INode#recordModification(Snapshot)}.\n+   * @param latest See {@link INode#recordModification(Snapshot, INodeMap)}.\n    */\n   public boolean removeChild(INode child, Snapshot latest,\n       final INodeMap inodeMap) throws QuotaExceededException {",
                "raw_url": "https://github.com/apache/hadoop/raw/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java",
                "sha": "1dc1016498864ee8b6418c89da73fe0e2d51702e",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java?ref=7c14f1557b6097c193a22f4b690de6e602d311b5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java",
                "patch": "@@ -22,6 +22,8 @@\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n+import java.io.FileNotFoundException;\n+\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSTestUtil;\n@@ -238,6 +240,18 @@ public void testSnapshotPathINodes() throws Exception {\n     final INode last = nodesInPath.getLastINode();\n     assertEquals(last.getFullPathName(), sub1.toString());\n     assertFalse(last instanceof INodeFileWithSnapshot);\n+    \n+    String[] invalidPathComponent = {\"invalidDir\", \"foo\", \".snapshot\", \"bar\"};\n+    Path invalidPath = new Path(invalidPathComponent[0]);\n+    for(int i = 1; i < invalidPathComponent.length; i++) {\n+      invalidPath = new Path(invalidPath, invalidPathComponent[i]);\n+      try {\n+        hdfs.getFileStatus(invalidPath);\n+        Assert.fail();\n+      } catch(FileNotFoundException fnfe) {\n+        System.out.println(\"The exception is expected: \" + fnfe);\n+      }\n+    }\n   }\n   \n   /** ",
                "raw_url": "https://github.com/apache/hadoop/raw/7c14f1557b6097c193a22f4b690de6e602d311b5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestSnapshotPathINodes.java",
                "sha": "721a0fd3f659c7fdeeb656baee3374d9e4cccb08",
                "status": "modified"
            }
        ],
        "message": "HDFS-4781. Fix a NullPointerException when listing .snapshot under a non-existing directory.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1478135 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/faca77f227a52907ed278014c3a6f65f0a3e0ea1",
        "patched_files": [
            "INodeDirectory.java",
            "FSDirectory.java",
            "INode.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSnapshotPathINodes.java",
            "TestFSDirectory.java",
            "TestINode.java"
        ]
    },
    "hadoop_7d06806": {
        "bug_id": "hadoop_7d06806",
        "commit": "https://github.com/apache/hadoop/commit/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=7d06806dfdeb3252ac0defe23e8c468eabfa8b5e",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -1273,8 +1273,6 @@ synchronized void transitionToStandby(boolean initialize)\n   protected void serviceStart() throws Exception {\n     if (this.rmContext.isHAEnabled()) {\n       transitionToStandby(false);\n-    } else {\n-      transitionToActive();\n     }\n \n     startWepApp();\n@@ -1284,6 +1282,11 @@ protected void serviceStart() throws Exception {\n       WebAppUtils.setRMWebAppPort(conf, port);\n     }\n     super.serviceStart();\n+\n+    // Non HA case, start after RM services are started.\n+    if (!this.rmContext.isHAEnabled()) {\n+      transitionToActive();\n+    }\n   }\n   \n   protected void doSecureLogin() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "05745ec272e1f72d924376688782c1bc4a71495b",
                "status": "modified"
            }
        ],
        "message": "YARN-6827. [ATS1/1.5] NPE exception while publishing recovering applications into ATS during RM restart. Contributed by Rohith Sharma K S.",
        "parent": "https://github.com/apache/hadoop/commit/c6d7d3eb059c7539db7d00586e181ec44da13557",
        "patched_files": [
            "ResourceManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java"
        ]
    },
    "hadoop_7d6792e": {
        "bug_id": "hadoop_7d6792e",
        "commit": "https://github.com/apache/hadoop/commit/7d6792e5d2001e1a83ae75a4777324a3b01c557a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/7d6792e5d2001e1a83ae75a4777324a3b01c557a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java?ref=7d6792e5d2001e1a83ae75a4777324a3b01c557a",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java",
                "patch": "@@ -188,7 +188,7 @@ public String getErasureCodingPolicyName(INode inode) {\n           String ecPolicyName = WritableUtils.readString(din);\n           return dir.getFSNamesystem()\n               .getErasureCodingPolicyManager()\n-              .getEnabledPolicyByName(ecPolicyName)\n+              .getByName(ecPolicyName)\n               .getName();\n         }\n       } else if (inode.getParent() != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/7d6792e5d2001e1a83ae75a4777324a3b01c557a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ContentSummaryComputationContext.java",
                "sha": "2e7c78a205b4084dc52d07fe35db6285f76dd6b6",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hadoop/blob/7d6792e5d2001e1a83ae75a4777324a3b01c557a/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml?ref=7d6792e5d2001e1a83ae75a4777324a3b01c557a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml",
                "patch": "@@ -1034,5 +1034,26 @@\n       </comparators>\n     </test>\n \n+    <test>\n+      <description>ls: file with disabled EC Policy</description>\n+      <test-commands>\n+        <command>-fs NAMENODE -mkdir -p /ecdir</command>\n+        <ec-admin-command>-fs NAMENODE -setPolicy -path /ecdir -policy XOR-2-1-1024k</ec-admin-command>\n+        <command>-fs NAMENODE -touchz /ecdir/file1</command>\n+        <ec-admin-command>-fs NAMENODE -disablePolicy -policy XOR-2-1-1024k</ec-admin-command>\n+        <command>-fs NAMENODE -ls -e /ecdir</command>\n+      </test-commands>\n+      <cleanup-commands>\n+        <command>-fs NAMENODE -rmdir /ecdir</command>\n+        <ec-admin-command>-fs NAMENODE -enablePolicy -policy XOR-2-1-1024k</ec-admin-command>\n+      </cleanup-commands>\n+      <comparators>\n+        <comparator>\n+          <type>RegexpComparator</type>\n+          <expected-output>^-rw-r--r--( )*1( )*USERNAME( )*supergroup( )*[A-Za-z0-9-]{1,}( )*0( )*[0-9]{4,}-[0-9]{2,}-[0-9]{2,} [0-9]{2,}:[0-9]{2,}( )*/ecdir/file1</expected-output>\n+        </comparator>\n+      </comparators>\n+    </test>\n+\n   </tests>\n </configuration>",
                "raw_url": "https://github.com/apache/hadoop/raw/7d6792e5d2001e1a83ae75a4777324a3b01c557a/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/testErasureCodingConf.xml",
                "sha": "c280eca42acbd7dd4fc9c8b6020ca44b0f911820",
                "status": "modified"
            }
        ],
        "message": "HDFS-14218. EC: Ls -e throw NPE when directory ec policy is disabled. Contributed by Ayush Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/0dd35e218fd4d6c660fd064e893be3112c546c9f",
        "patched_files": [
            "ContentSummaryComputationContext.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "testErasureCodingConf.java"
        ]
    },
    "hadoop_7dafee1": {
        "bug_id": "hadoop_7dafee1",
        "commit": "https://github.com/apache/hadoop/commit/7dafee11d865c7c121c7886ac66aa9d088ea13f7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7dafee11d865c7c121c7886ac66aa9d088ea13f7/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=7dafee11d865c7c121c7886ac66aa9d088ea13f7",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -683,6 +683,9 @@ Release 2.8.0 - UNRELEASED\n     MAPREDUCE-6589. TestTaskLog outputs a log under directory other than\n     target/test-dir. (aajisaka)\n \n+    MAPREDUCE-6593. TestJobHistoryEventHandler.testTimelineEventHandling fails\n+    on trunk because of NPE. (Naganarasimha G R via aajisaka)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/7dafee11d865c7c121c7886ac66aa9d088ea13f7/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d04ce5daedd9520541f8f8840fa29a20d3c3388d",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/7dafee11d865c7c121c7886ac66aa9d088ea13f7/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java?ref=7dafee11d865c7c121c7886ac66aa9d088ea13f7",
                "deletions": 6,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java",
                "patch": "@@ -18,6 +18,7 @@\n \n package org.apache.hadoop.mapreduce.jobhistory;\n \n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n import static org.mockito.Mockito.mock;\n@@ -31,8 +32,6 @@\n import java.io.IOException;\n import java.util.HashMap;\n \n-import org.junit.Assert;\n-\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n@@ -67,14 +66,14 @@\n import org.apache.hadoop.yarn.api.records.timeline.TimelineEntity;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;\n-import org.junit.After;\n-import org.junit.AfterClass;\n-import static org.junit.Assert.assertFalse;\n-import org.junit.BeforeClass;\n import org.apache.hadoop.yarn.server.MiniYARNCluster;\n import org.apache.hadoop.yarn.server.timeline.TimelineStore;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n@@ -854,6 +853,9 @@ public JHEvenHandlerForTest(AppContext context, int startCount, boolean mockHist\n \n   @Override\n   protected void serviceStart() {\n+    if (timelineClient != null) {\n+      timelineClient.start();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/7dafee11d865c7c121c7886ac66aa9d088ea13f7/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java",
                "sha": "8ca386ed703bc2bb92c811e64ad164bf88570138",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6593. TestJobHistoryEventHandler.testTimelineEventHandling fails on trunk because of NPE. Contributed by Naganarasimha G R.",
        "parent": "https://github.com/apache/hadoop/commit/db99e30f670cb5d73d5ec79671026eeeaf337d3d",
        "patched_files": [
            "CHANGES.java",
            "JobHistoryEventHandler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJobHistoryEventHandler.java"
        ]
    },
    "hadoop_7ffb994": {
        "bug_id": "hadoop_7ffb994",
        "commit": "https://github.com/apache/hadoop/commit/7ffb9943b8838a3bb56684e0722db40d800743a2",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=7ffb9943b8838a3bb56684e0722db40d800743a2",
                "deletions": 19,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -1036,7 +1036,6 @@ LocalizerHeartbeatResponse processHeartbeat(\n         List<LocalResourceStatus> remoteResourceStatuses) {\n       LocalizerHeartbeatResponse response =\n         recordFactory.newRecordInstance(LocalizerHeartbeatResponse.class);\n-\n       String user = context.getUser();\n       ApplicationId applicationId =\n           context.getContainerId().getApplicationAttemptId().getApplicationId();\n@@ -1059,14 +1058,19 @@ LocalizerHeartbeatResponse processHeartbeat(\n           LOG.error(\"Unknown resource reported: \" + req);\n           continue;\n         }\n+        LocalResourcesTracker tracker =\n+            getLocalResourcesTracker(req.getVisibility(), user, applicationId);\n+        if (tracker == null) {\n+          // This is likely due to a race between heartbeat and\n+          // app cleaning up.\n+          continue;\n+        }\n         switch (stat.getStatus()) {\n           case FETCH_SUCCESS:\n             // notify resource\n             try {\n-            getLocalResourcesTracker(req.getVisibility(), user, applicationId)\n-              .handle(\n-                new ResourceLocalizedEvent(req, stat.getLocalPath().toPath(),\n-                    stat.getLocalSize()));\n+              tracker.handle(new ResourceLocalizedEvent(req,\n+                  stat.getLocalPath().toPath(), stat.getLocalSize()));\n             } catch (URISyntaxException e) { }\n \n             // unlocking the resource and removing it from scheduled resource\n@@ -1080,9 +1084,8 @@ LocalizerHeartbeatResponse processHeartbeat(\n             final String diagnostics = stat.getException().toString();\n             LOG.warn(req + \" failed: \" + diagnostics);\n             fetchFailed = true;\n-            getLocalResourcesTracker(req.getVisibility(), user, applicationId)\n-              .handle(new ResourceFailedLocalizationEvent(\n-                  req, diagnostics));\n+            tracker.handle(new ResourceFailedLocalizationEvent(req,\n+                diagnostics));\n \n             // unlocking the resource and removing it from scheduled resource\n             // list\n@@ -1092,9 +1095,8 @@ LocalizerHeartbeatResponse processHeartbeat(\n           default:\n             LOG.info(\"Unknown status: \" + stat.getStatus());\n             fetchFailed = true;\n-            getLocalResourcesTracker(req.getVisibility(), user, applicationId)\n-              .handle(new ResourceFailedLocalizationEvent(\n-                  req, stat.getException().getMessage()));\n+            tracker.handle(new ResourceFailedLocalizationEvent(req,\n+                stat.getException().getMessage()));\n             break;\n         }\n       }\n@@ -1114,10 +1116,14 @@ LocalizerHeartbeatResponse processHeartbeat(\n       LocalResource next = findNextResource();\n       if (next != null) {\n         try {\n-          ResourceLocalizationSpec resource =\n-              NodeManagerBuilderUtils.newResourceLocalizationSpec(next,\n-                getPathForLocalization(next));\n-          rsrcs.add(resource);\n+          LocalResourcesTracker tracker = getLocalResourcesTracker(\n+              next.getVisibility(), user, applicationId);\n+          if (tracker != null) {\n+            ResourceLocalizationSpec resource =\n+                NodeManagerBuilderUtils.newResourceLocalizationSpec(next,\n+                getPathForLocalization(next, tracker));\n+            rsrcs.add(resource);\n+          }\n         } catch (IOException e) {\n           LOG.error(\"local path for PRIVATE localization could not be \" +\n             \"found. Disks might have failed.\", e);\n@@ -1136,14 +1142,12 @@ LocalizerHeartbeatResponse processHeartbeat(\n       return response;\n     }\n \n-    private Path getPathForLocalization(LocalResource rsrc) throws IOException,\n-        URISyntaxException {\n+    private Path getPathForLocalization(LocalResource rsrc,\n+        LocalResourcesTracker tracker) throws IOException, URISyntaxException {\n       String user = context.getUser();\n       ApplicationId appId =\n           context.getContainerId().getApplicationAttemptId().getApplicationId();\n       LocalResourceVisibility vis = rsrc.getVisibility();\n-      LocalResourcesTracker tracker =\n-          getLocalResourcesTracker(vis, user, appId);\n       String cacheDirectory = null;\n       if (vis == LocalResourceVisibility.PRIVATE) {// PRIVATE Only\n         cacheDirectory = getUserFileCachePath(user);",
                "raw_url": "https://github.com/apache/hadoop/raw/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "71971c757deb8f7f0db4f51fba24d5ad9a4d2e4b",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/hadoop/blob/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java?ref=7ffb9943b8838a3bb56684e0722db40d800743a2",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "patch": "@@ -22,6 +22,7 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Matchers.anyInt;\n import static org.mockito.Matchers.anyLong;\n@@ -147,7 +148,6 @@\n import org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n-import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -1482,6 +1482,114 @@ public void testPublicResourceInitializesLocalDir() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 20000)\n+  @SuppressWarnings(\"unchecked\")\n+  public void testLocalizerHeartbeatWhenAppCleaningUp() throws Exception {\n+    conf.set(YarnConfiguration.NM_LOCAL_DIRS,\n+        lfs.makeQualified(new Path(basedir, 0 + \"\")).toString());\n+    // Start dispatcher.\n+    DrainDispatcher dispatcher = new DrainDispatcher();\n+    dispatcher.init(conf);\n+    dispatcher.start();\n+    dispatcher.register(ApplicationEventType.class, mock(EventHandler.class));\n+    dispatcher.register(ContainerEventType.class, mock(EventHandler.class));\n+\n+    DummyExecutor exec = new DummyExecutor();\n+    LocalDirsHandlerService dirsHandler = new LocalDirsHandlerService();\n+    dirsHandler.init(conf);\n+    // Start resource localization service.\n+    ResourceLocalizationService rawService = new ResourceLocalizationService(\n+        dispatcher, exec, mock(DeletionService.class), dirsHandler, nmContext);\n+    ResourceLocalizationService spyService = spy(rawService);\n+    doReturn(mockServer).when(spyService).createServer();\n+    doReturn(lfs).when(spyService).\n+        getLocalFileContext(isA(Configuration.class));\n+    try {\n+      spyService.init(conf);\n+      spyService.start();\n+\n+      // Init application resources.\n+      final Application app = mock(Application.class);\n+      final ApplicationId appId = BuilderUtils.newApplicationId(1234567890L, 3);\n+      when(app.getUser()).thenReturn(\"user0\");\n+      when(app.getAppId()).thenReturn(appId);\n+      when(app.toString()).thenReturn(appId.toString());\n+      spyService.handle(new ApplicationLocalizationEvent(\n+          LocalizationEventType.INIT_APPLICATION_RESOURCES, app));\n+      dispatcher.await();\n+\n+      // Initialize localizer.\n+      Random r = new Random();\n+      long seed = r.nextLong();\n+      System.out.println(\"SEED: \" + seed);\n+      r.setSeed(seed);\n+      final Container c = getMockContainer(appId, 46, \"user0\");\n+      FSDataOutputStream out =\n+          new FSDataOutputStream(new DataOutputBuffer(), null);\n+      doReturn(out).when(spylfs).createInternal(isA(Path.class),\n+          isA(EnumSet.class), isA(FsPermission.class), anyInt(), anyShort(),\n+          anyLong(), isA(Progressable.class), isA(ChecksumOpt.class),\n+          anyBoolean());\n+      final LocalResource resource1 = getAppMockedResource(r);\n+      final LocalResource resource2 = getAppMockedResource(r);\n+\n+      // Send localization requests for container.\n+      // 2 resources generated with APPLICATION visibility.\n+      final LocalResourceRequest req1 = new LocalResourceRequest(resource1);\n+      final LocalResourceRequest req2 = new LocalResourceRequest(resource2);\n+      Map<LocalResourceVisibility, Collection<LocalResourceRequest>> rsrcs =\n+          new HashMap<LocalResourceVisibility,\n+              Collection<LocalResourceRequest>>();\n+      List<LocalResourceRequest> appResourceList = Arrays.asList(req1, req2);\n+      rsrcs.put(LocalResourceVisibility.APPLICATION, appResourceList);\n+      spyService.handle(new ContainerLocalizationRequestEvent(c, rsrcs));\n+      dispatcher.await();\n+\n+      // Wait for localization to begin.\n+      exec.waitForLocalizers(1);\n+      final String containerIdStr = c.getContainerId().toString();\n+      LocalizerRunner locRunnerForContainer =\n+          spyService.getLocalizerRunner(containerIdStr);\n+      // Heartbeats from container localizer\n+      LocalResourceStatus rsrcSuccess = mock(LocalResourceStatus.class);\n+      LocalizerStatus stat = mock(LocalizerStatus.class);\n+      when(stat.getLocalizerId()).thenReturn(containerIdStr);\n+      when(rsrcSuccess.getResource()).thenReturn(resource1);\n+      when(rsrcSuccess.getLocalSize()).thenReturn(4344L);\n+      when(rsrcSuccess.getLocalPath()).thenReturn(getPath(\"/some/path\"));\n+      when(rsrcSuccess.getStatus()).\n+          thenReturn(ResourceStatusType.FETCH_SUCCESS);\n+      when(stat.getResources()).\n+          thenReturn(Collections.<LocalResourceStatus>emptyList());\n+\n+      // First heartbeat which schedules first resource.\n+      LocalizerHeartbeatResponse response = spyService.heartbeat(stat);\n+      assertEquals(\"NM should tell localizer to be LIVE in Heartbeat.\",\n+          LocalizerAction.LIVE, response.getLocalizerAction());\n+\n+      // Cleanup application.\n+      spyService.handle(new ContainerLocalizationCleanupEvent(c, rsrcs));\n+      spyService.handle(new ApplicationLocalizationEvent(\n+          LocalizationEventType.DESTROY_APPLICATION_RESOURCES, app));\n+      dispatcher.await();\n+      try {\n+        // Directly send heartbeat to introduce race as app is being cleaned up.\n+        locRunnerForContainer.processHeartbeat(\n+            Collections.singletonList(rsrcSuccess));\n+      } catch (Exception e) {\n+        fail(\"Exception should not have been thrown on processing heartbeat\");\n+      }\n+      // Send another heartbeat.\n+      response = spyService.heartbeat(stat);\n+      assertEquals(\"NM should tell localizer to DIE in Heartbeat.\",\n+          LocalizerAction.DIE, response.getLocalizerAction());\n+      exec.setStopLocalization();\n+    } finally {\n+      spyService.stop();\n+      dispatcher.stop();\n+    }\n+  }\n+\n   @Test(timeout=20000)\n   @SuppressWarnings(\"unchecked\") // mocked generics\n   public void testFailedPublicResource() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "sha": "13ba2c12c37113519887db0a98769504bc59f00c",
                "status": "modified"
            }
        ],
        "message": "YARN-4355. NPE while processing localizer heartbeat. Contributed by Varun Saxena & Jonathan Hung.",
        "parent": "https://github.com/apache/hadoop/commit/43aef303bf9b71293b00c7ed6e8807d15274ca95",
        "patched_files": [
            "ResourceLocalizationService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_8064849": {
        "bug_id": "hadoop_8064849",
        "commit": "https://github.com/apache/hadoop/commit/80648492036d7f0aaa72082a875efc7cba500782",
        "file": [
            {
                "additions": 161,
                "blob_url": "https://github.com/apache/hadoop/blob/80648492036d7f0aaa72082a875efc7cba500782/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java",
                "changes": 161,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java?ref=80648492036d7f0aaa72082a875efc7cba500782",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java",
                "patch": "@@ -0,0 +1,161 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.io.compress;\n+\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.util.Random;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.DataInputBuffer;\n+import org.apache.hadoop.io.DataOutputBuffer;\n+import org.apache.hadoop.io.RandomDatum;\n+import org.apache.hadoop.io.compress.zlib.ZlibFactory;\n+import org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionLevel;\n+import org.apache.hadoop.io.compress.zlib.ZlibCompressor.CompressionStrategy;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import junit.framework.TestCase;\n+\n+public class TestCompressionStreamReuse extends TestCase {\n+  private static final Log LOG = LogFactory\n+      .getLog(TestCompressionStreamReuse.class);\n+\n+  private Configuration conf = new Configuration();\n+  private int count = 10000;\n+  private int seed = new Random().nextInt();\n+\n+  public void testBZip2Codec() throws IOException {\n+    resetStateTest(conf, seed, count,\n+        \"org.apache.hadoop.io.compress.BZip2Codec\");\n+  }\n+\n+  public void testGzipCompressStreamReuse() throws IOException {\n+    resetStateTest(conf, seed, count,\n+        \"org.apache.hadoop.io.compress.GzipCodec\");\n+  }\n+\n+  public void testGzipCompressStreamReuseWithParam() throws IOException {\n+    Configuration conf = new Configuration(this.conf);\n+    ZlibFactory\n+        .setCompressionLevel(conf, CompressionLevel.BEST_COMPRESSION);\n+    ZlibFactory.setCompressionStrategy(conf,\n+        CompressionStrategy.HUFFMAN_ONLY);\n+    resetStateTest(conf, seed, count,\n+        \"org.apache.hadoop.io.compress.GzipCodec\");\n+  }\n+\n+  private static void resetStateTest(Configuration conf, int seed, int count,\n+      String codecClass) throws IOException {\n+    // Create the codec\n+    CompressionCodec codec = null;\n+    try {\n+      codec = (CompressionCodec) ReflectionUtils.newInstance(conf\n+          .getClassByName(codecClass), conf);\n+    } catch (ClassNotFoundException cnfe) {\n+      throw new IOException(\"Illegal codec!\");\n+    }\n+    LOG.info(\"Created a Codec object of type: \" + codecClass);\n+\n+    // Generate data\n+    DataOutputBuffer data = new DataOutputBuffer();\n+    RandomDatum.Generator generator = new RandomDatum.Generator(seed);\n+    for (int i = 0; i < count; ++i) {\n+      generator.next();\n+      RandomDatum key = generator.getKey();\n+      RandomDatum value = generator.getValue();\n+\n+      key.write(data);\n+      value.write(data);\n+    }\n+    LOG.info(\"Generated \" + count + \" records\");\n+\n+    // Compress data\n+    DataOutputBuffer compressedDataBuffer = new DataOutputBuffer();\n+    DataOutputStream deflateOut = new DataOutputStream(\n+        new BufferedOutputStream(compressedDataBuffer));\n+    CompressionOutputStream deflateFilter = codec\n+        .createOutputStream(deflateOut);\n+    deflateFilter.write(data.getData(), 0, data.getLength());\n+    deflateFilter.finish();\n+    deflateFilter.flush();\n+    LOG.info(\"Finished compressing data\");\n+\n+    // reset deflator\n+    deflateFilter.resetState();\n+    LOG.info(\"Finished reseting deflator\");\n+\n+    // re-generate data\n+    data.reset();\n+    generator = new RandomDatum.Generator(seed);\n+    for (int i = 0; i < count; ++i) {\n+      generator.next();\n+      RandomDatum key = generator.getKey();\n+      RandomDatum value = generator.getValue();\n+\n+      key.write(data);\n+      value.write(data);\n+    }\n+    DataInputBuffer originalData = new DataInputBuffer();\n+    DataInputStream originalIn = new DataInputStream(\n+        new BufferedInputStream(originalData));\n+    originalData.reset(data.getData(), 0, data.getLength());\n+\n+    // re-compress data\n+    compressedDataBuffer.reset();\n+    deflateOut = new DataOutputStream(new BufferedOutputStream(\n+        compressedDataBuffer));\n+    deflateFilter = codec.createOutputStream(deflateOut);\n+\n+    deflateFilter.write(data.getData(), 0, data.getLength());\n+    deflateFilter.finish();\n+    deflateFilter.flush();\n+    LOG.info(\"Finished re-compressing data\");\n+\n+    // De-compress data\n+    DataInputBuffer deCompressedDataBuffer = new DataInputBuffer();\n+    deCompressedDataBuffer.reset(compressedDataBuffer.getData(), 0,\n+        compressedDataBuffer.getLength());\n+    CompressionInputStream inflateFilter = codec\n+        .createInputStream(deCompressedDataBuffer);\n+    DataInputStream inflateIn = new DataInputStream(\n+        new BufferedInputStream(inflateFilter));\n+\n+    // Check\n+    for (int i = 0; i < count; ++i) {\n+      RandomDatum k1 = new RandomDatum();\n+      RandomDatum v1 = new RandomDatum();\n+      k1.readFields(originalIn);\n+      v1.readFields(originalIn);\n+\n+      RandomDatum k2 = new RandomDatum();\n+      RandomDatum v2 = new RandomDatum();\n+      k2.readFields(inflateIn);\n+      v2.readFields(inflateIn);\n+      assertTrue(\n+          \"original and compressed-then-decompressed-output not equal\",\n+          k1.equals(k2) && v1.equals(v2));\n+    }\n+    LOG.info(\"SUCCESS! Completed checking \" + count + \" records\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/80648492036d7f0aaa72082a875efc7cba500782/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCompressionStreamReuse.java",
                "sha": "2c285944539da847caa071b80c54e4fa01314872",
                "status": "added"
            }
        ],
        "message": "HADOOP-8419. Fixed GzipCode NPE reset for IBM JDK. (Yu Li via eyang)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431740 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "TestCompressionStreamReuse.java"
        ]
    },
    "hadoop_81ec909": {
        "bug_id": "hadoop_81ec909",
        "commit": "https://github.com/apache/hadoop/commit/81ec90941160cc5f75370ee45af22c6b47c3c94b",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/81ec90941160cc5f75370ee45af22c6b47c3c94b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/AdminHelper.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/AdminHelper.java?ref=81ec90941160cc5f75370ee45af22c6b47c3c94b",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/AdminHelper.java",
                "patch": "@@ -65,8 +65,14 @@ static DistributedFileSystem getDFS(URI uri, Configuration conf)\n    * When it's a known error, pretty-print the error and squish the stack trace.\n    */\n   static String prettifyException(Exception e) {\n-    return e.getClass().getSimpleName() + \": \"\n-        + e.getLocalizedMessage().split(\"\\n\")[0];\n+    if (e.getLocalizedMessage() != null) {\n+      return e.getClass().getSimpleName() + \": \"\n+          + e.getLocalizedMessage().split(\"\\n\")[0];\n+    } else if (e.getStackTrace() != null && e.getStackTrace().length > 0) {\n+      return e.getClass().getSimpleName() + \" at \" + e.getStackTrace()[0];\n+    } else {\n+      return e.getClass().getSimpleName();\n+    }\n   }\n \n   static TableListing getOptionDescriptionListing() {",
                "raw_url": "https://github.com/apache/hadoop/raw/81ec90941160cc5f75370ee45af22c6b47c3c94b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/AdminHelper.java",
                "sha": "9cb646b38f6f7515d97f95e6ba52d37f2fa90ce9",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/hadoop/blob/81ec90941160cc5f75370ee45af22c6b47c3c94b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestAdminHelper.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestAdminHelper.java?ref=81ec90941160cc5f75370ee45af22c6b47c3c94b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestAdminHelper.java",
                "patch": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.tools;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Test class to test Admin Helper.\n+ */\n+public class TestAdminHelper {\n+\n+  @Test\n+  public void prettifyExceptionWithNpe() {\n+    String pretty = AdminHelper.prettifyException(new NullPointerException());\n+    Assert.assertTrue(\n+        \"Prettified exception message doesn't contain the required exception \"\n+            + \"message\",\n+        pretty.startsWith(\"NullPointerException at org.apache.hadoop.hdfs.tools\"\n+            + \".TestAdminHelper.prettifyExceptionWithNpe\"));\n+  }\n+\n+  @Test\n+  public void prettifyException() {\n+\n+    String pretty = AdminHelper.prettifyException(\n+        new IllegalArgumentException(\"Something is wrong\",\n+            new IllegalArgumentException(\"Something is illegal\")));\n+\n+    Assert.assertEquals(\n+        \"IllegalArgumentException: Something is wrong\",\n+        pretty);\n+\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/81ec90941160cc5f75370ee45af22c6b47c3c94b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestAdminHelper.java",
                "sha": "f99ef0186ccecdc32d35ba5990376b5540443518",
                "status": "added"
            }
        ],
        "message": "HDFS-14078. Admin helper fails to prettify NullPointerExceptions. Contributed by Elek, Marton.\n\nSigned-off-by: Wei-Chiu Chuang <weichiu@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/71edfce187e78d63f2ee623fda1d77d1f3b1a7a6",
        "patched_files": [
            "AdminHelper.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAdminHelper.java"
        ]
    },
    "hadoop_8201ed8": {
        "bug_id": "hadoop_8201ed8",
        "commit": "https://github.com/apache/hadoop/commit/8201ed8009e5f04c49568a8133635d47fcde3989",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java?ref=8201ed8009e5f04c49568a8133635d47fcde3989",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java",
                "patch": "@@ -122,9 +122,10 @@ protected void serviceInit(Configuration conf) throws Exception {\n     private List<String> racks;\n     private Priority priority;\n     private long allocationRequestId;\n-    private boolean relaxLocality;\n+    private boolean relaxLocality = true;\n     private String nodeLabelsExpression;\n-    private ExecutionTypeRequest executionTypeRequest;\n+    private ExecutionTypeRequest executionTypeRequest =\n+        ExecutionTypeRequest.newInstance();\n     \n     /**\n      * Instantiates a {@link ContainerRequest} with the given constraints and",
                "raw_url": "https://github.com/apache/hadoop/raw/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java",
                "sha": "60e305f4ff49cfb2fb0665195a44dab0be41a893",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java?ref=8201ed8009e5f04c49568a8133635d47fcde3989",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java",
                "patch": "@@ -51,8 +51,10 @@ public void testOpportunisticAndGuaranteedRequests() {\n \n     Resource capability = Resource.newInstance(1024, 1);\n     ContainerRequest request =\n-        new ContainerRequest(capability, new String[] {\"host1\", \"host2\"},\n-            new String[] {\"/rack2\"}, Priority.newInstance(1));\n+        ContainerRequest.newBuilder().capability(capability)\n+            .nodes(new String[] { \"host1\", \"host2\" })\n+            .racks(new String[] { \"/rack2\" }).priority(Priority.newInstance(1))\n+            .build();\n     client.addContainerRequest(request);\n     verifyResourceRequest(client, request, \"host1\", true);\n     verifyResourceRequest(client, request, \"host2\", true);",
                "raw_url": "https://github.com/apache/hadoop/raw/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java",
                "sha": "96035394ec7c2b225f123e73b528b08dd2715c36",
                "status": "modified"
            }
        ],
        "message": "YARN-6756. ContainerRequest#executionTypeRequest causes NPE. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/cf93d60d3f032000e5b78a08d320793d78799f3d",
        "patched_files": [
            "AMRMClient.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAMRMClientContainerRequest.java",
            "TestAMRMClient.java"
        ]
    },
    "hadoop_8253106": {
        "bug_id": "hadoop_8253106",
        "commit": "https://github.com/apache/hadoop/commit/825310608bc98977646390c38b4f8f0404486d69",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/825310608bc98977646390c38b4f8f0404486d69/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java?ref=825310608bc98977646390c38b4f8f0404486d69",
                "deletions": 1,
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java",
                "patch": "@@ -32,6 +32,8 @@\n import org.apache.hadoop.hdds.scm.XceiverClientGrpc;\n import org.apache.hadoop.hdds.scm.XceiverClientSpi;\n import org.apache.hadoop.hdds.scm.pipeline.Pipeline;\n+import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n+import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.token.Token;\n@@ -45,6 +47,7 @@\n import org.junit.rules.Timeout;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n+import org.mockito.Mockito;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -133,7 +136,7 @@ public void testCreateOzoneContainer() throws Exception {\n           OzoneConfigKeys.DFS_CONTAINER_IPC_RANDOM_PORT, false);\n \n       DatanodeDetails dn = TestUtils.randomDatanodeDetails();\n-      container = new OzoneContainer(dn, conf, null);\n+      container = new OzoneContainer(dn, conf, getContext(dn));\n       //Setting scmId, as we start manually ozone container.\n       container.getDispatcher().setScmId(UUID.randomUUID().toString());\n       container.start();\n@@ -206,4 +209,13 @@ public static void createContainerForTesting(XceiverClientSpi client,\n     Assert.assertNotNull(response);\n     Assert.assertTrue(request.getTraceID().equals(response.getTraceID()));\n   }\n+\n+  private StateContext getContext(DatanodeDetails datanodeDetails) {\n+    DatanodeStateMachine stateMachine = Mockito.mock(\n+        DatanodeStateMachine.class);\n+    StateContext context = Mockito.mock(StateContext.class);\n+    Mockito.when(stateMachine.getDatanodeDetails()).thenReturn(datanodeDetails);\n+    Mockito.when(context.getParent()).thenReturn(stateMachine);\n+    return context;\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/825310608bc98977646390c38b4f8f0404486d69/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestSecureOzoneContainer.java",
                "sha": "22243008992a7e2bb40f2a5ac51a4b799bafc936",
                "status": "modified"
            }
        ],
        "message": "HDDS-873. Fix TestSecureOzoneContainer NPE after HDDS-837. Contributed by Xiaoyu Yao.",
        "parent": "https://github.com/apache/hadoop/commit/7e2770699ce95235732c4c9c0a564cba8d35d1c9",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "TestSecureOzoneContainer.java"
        ]
    },
    "hadoop_825365d": {
        "bug_id": "hadoop_825365d",
        "commit": "https://github.com/apache/hadoop/commit/825365d1d0e22d2021208371dcb313c927f51ce3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/825365d1d0e22d2021208371dcb313c927f51ce3/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=825365d1d0e22d2021208371dcb313c927f51ce3",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -201,6 +201,9 @@ Release 2.5.0 - UNRELEASED\n     YARN-2117. Fixed the issue that secret file reader is potentially not\n     closed in TimelineAuthenticationFilterInitializer. (Chen He via zjshen)\n \n+    YARN-2121. Fixed NPE handling in Timeline Server's TimelineAuthenticator.\n+    (Zhijie Shen via vinodkv)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/825365d1d0e22d2021208371dcb313c927f51ce3/hadoop-yarn-project/CHANGES.txt",
                "sha": "f32978745e7f6532310c937ac3d8be265aeee4b5",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/825365d1d0e22d2021208371dcb313c927f51ce3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java?ref=825365d1d0e22d2021208371dcb313c927f51ce3",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "patch": "@@ -35,13 +35,15 @@\n import org.apache.hadoop.security.authentication.client.KerberosAuthenticator;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.yarn.api.records.timeline.TimelineDelegationTokenResponse;\n+import org.apache.hadoop.yarn.security.client.TimelineAuthenticationConsts;\n import org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier;\n import org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation;\n-import org.apache.hadoop.yarn.security.client.TimelineAuthenticationConsts;\n import org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n \n+import com.google.common.annotations.VisibleForTesting;\n+\n /**\n  * A <code>KerberosAuthenticator</code> subclass that fallback to\n  * {@link TimelineAuthenticationConsts}.\n@@ -77,9 +79,15 @@ public static void injectDelegationToken(Map<String, String> params,\n     }\n   }\n \n-  private boolean hasDelegationToken(URL url) {\n-    return url.getQuery().contains(\n-        TimelineAuthenticationConsts.DELEGATION_PARAM + \"=\");\n+  @Private\n+  @VisibleForTesting\n+  boolean hasDelegationToken(URL url) {\n+    if (url.getQuery() == null) {\n+      return false;\n+    } else {\n+      return url.getQuery().contains(\n+          TimelineAuthenticationConsts.DELEGATION_PARAM + \"=\");\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/825365d1d0e22d2021208371dcb313c927f51ce3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineAuthenticator.java",
                "sha": "25333c7551b014e424045bebaf839ba3a7baf5dd",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hadoop/blob/825365d1d0e22d2021208371dcb313c927f51ce3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java?ref=825365d1d0e22d2021208371dcb313c927f51ce3",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "patch": "@@ -0,0 +1,40 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.client.api.impl;\n+\n+import java.net.URL;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestTimelineAuthenticator {\n+\n+  @Test\n+  public void testHasDelegationTokens() throws Exception {\n+    TimelineAuthenticator authenticator = new TimelineAuthenticator();\n+    Assert.assertFalse(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource\")));\n+    Assert.assertFalse(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?other=xxxx\")));\n+    Assert.assertTrue(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?delegation=yyyy\")));\n+    Assert.assertTrue(authenticator.hasDelegationToken(new URL(\n+        \"http://localhost:8/resource?other=xxxx&delegation=yyyy\")));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/825365d1d0e22d2021208371dcb313c927f51ce3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineAuthenticator.java",
                "sha": "19aaa88533f24514a224645387244cc421a3db77",
                "status": "added"
            }
        ],
        "message": "YARN-2121. Fixed NPE handling in Timeline Server's TimelineAuthenticator. Contributed by Zhijie Shen.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601000 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/08b4aa699a40e6095736f344d3ff59bec37c7e6e",
        "patched_files": [
            "CHANGES.java",
            "TimelineAuthenticator.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestTimelineAuthenticator.java"
        ]
    },
    "hadoop_831a3ff": {
        "bug_id": "hadoop_831a3ff",
        "commit": "https://github.com/apache/hadoop/commit/831a3ffd6ef49214b08cb30329494418703be0f9",
        "file": [
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hadoop/blob/831a3ffd6ef49214b08cb30329494418703be0f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java?ref=831a3ffd6ef49214b08cb30329494418703be0f9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java",
                "patch": "@@ -482,6 +482,63 @@ private static void matchMetrics(Map<Long, Number> m1, Map<Long, Number> m2) {\n     }\n   }\n \n+\n+  @Test\n+  public void testWriteNullApplicationToHBase() throws Exception {\n+    TimelineEntities te = new TimelineEntities();\n+    ApplicationEntity entity = new ApplicationEntity();\n+    String appId = \"application_1000178881110_2002\";\n+    entity.setId(appId);\n+    long cTime = 1425016501000L;\n+    entity.setCreatedTime(cTime);\n+\n+    // add the info map in Timeline Entity\n+    Map<String, Object> infoMap = new HashMap<String, Object>();\n+    infoMap.put(\"infoMapKey1\", \"infoMapValue1\");\n+    infoMap.put(\"infoMapKey2\", 10);\n+    entity.addInfo(infoMap);\n+\n+    te.addEntity(entity);\n+    HBaseTimelineWriterImpl hbi = null;\n+    try {\n+      Configuration c1 = util.getConfiguration();\n+      hbi = new HBaseTimelineWriterImpl(c1);\n+      hbi.init(c1);\n+      hbi.start();\n+      String cluster = \"cluster_check_null_application\";\n+      String user = \"user1check_null_application\";\n+      //set the flow name to null\n+      String flow = null;\n+      String flowVersion = \"AB7822C10F1111\";\n+      long runid = 1002345678919L;\n+      hbi.write(cluster, user, flow, flowVersion, runid, appId, te);\n+      hbi.stop();\n+\n+      // retrieve the row\n+      Scan scan = new Scan();\n+      scan.setStartRow(Bytes.toBytes(cluster));\n+      Connection conn = ConnectionFactory.createConnection(c1);\n+      ResultScanner resultScanner = new ApplicationTable()\n+          .getResultScanner(c1, conn, scan);\n+\n+      assertTrue(resultScanner != null);\n+      // try to iterate over results\n+      int count = 0;\n+      for (Result rr = resultScanner.next(); rr != null;\n+          rr = resultScanner.next()) {\n+         count++;\n+      }\n+      // there should be no rows written\n+      // no exceptions thrown during write\n+      assertEquals(0, count);\n+    } finally {\n+      if (hbi != null) {\n+        hbi.stop();\n+        hbi.close();\n+      }\n+    }\n+  }\n+\n   @Test\n   public void testWriteApplicationToHBase() throws Exception {\n     TimelineEntities te = new TimelineEntities();",
                "raw_url": "https://github.com/apache/hadoop/raw/831a3ffd6ef49214b08cb30329494418703be0f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorage.java",
                "sha": "68135a01a61ee25bc86cfb24a1e031f84bd644a4",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/831a3ffd6ef49214b08cb30329494418703be0f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseTimelineWriterImpl.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseTimelineWriterImpl.java?ref=831a3ffd6ef49214b08cb30329494418703be0f9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseTimelineWriterImpl.java",
                "patch": "@@ -118,6 +118,15 @@ public TimelineWriteResponse write(String clusterId, String userId,\n       TimelineEntities data) throws IOException {\n \n     TimelineWriteResponse putStatus = new TimelineWriteResponse();\n+    // defensive coding to avoid NPE during row key construction\n+    if ((flowName == null) || (appId == null) || (clusterId == null)\n+        || (userId == null)) {\n+      LOG.warn(\"Found null for one of: flowName=\" + flowName + \" appId=\" + appId\n+          + \" userId=\" + userId + \" clusterId=\" + clusterId\n+          + \" . Not proceeding with writing to hbase\");\n+      return putStatus;\n+    }\n+\n     for (TimelineEntity te : data.getEntities()) {\n \n       // a set can have at most 1 null",
                "raw_url": "https://github.com/apache/hadoop/raw/831a3ffd6ef49214b08cb30329494418703be0f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/HBaseTimelineWriterImpl.java",
                "sha": "fe4671fda958194d5667497c9041de8b84581971",
                "status": "modified"
            }
        ],
        "message": "YARN-5097. NPE in Separator.joinEncoded() (Vrushali C via sjlee)",
        "parent": "https://github.com/apache/hadoop/commit/a1b6d7456fa37b7b418dd08178ea73ed5c1de124",
        "patched_files": [
            "HBaseTimelineWriterImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestHBaseTimelineStorage.java"
        ]
    },
    "hadoop_83798f1": {
        "bug_id": "hadoop_83798f1",
        "commit": "https://github.com/apache/hadoop/commit/83798f15f8602ef580a7885876de114b2425da89",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java?ref=83798f15f8602ef580a7885876de114b2425da89",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java",
                "patch": "@@ -136,6 +136,10 @@\n   }\n \n   public void resourceLocalizationFailed(LocalResourceRequest request) {\n+    // Skip null request when localization failed for running container\n+    if (request == null) {\n+      return;\n+    }\n     pendingResources.remove(request);\n     resourcesFailedToBeLocalized.add(request);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java",
                "sha": "745f8a88fb935d700a70b334553e8626026d3f30",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java?ref=83798f15f8602ef580a7885876de114b2425da89",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java",
                "patch": "@@ -524,6 +524,27 @@ public void testDockerContainerLocalizationFailureAtDone() throws Exception {\n     }\n   }\n   \n+  @Test\n+  @SuppressWarnings(\"unchecked\")\n+  public void testLocalizationFailureWhileRunning()\n+      throws Exception {\n+    WrappedContainer wc = null;\n+    try {\n+      wc = new WrappedContainer(6, 314159265358979L, 4344, \"yak\");\n+      wc.initContainer();\n+      wc.localizeResources();\n+      wc.launchContainer();\n+      reset(wc.localizerBus);\n+      assertEquals(ContainerState.RUNNING, wc.c.getContainerState());\n+      // Now in RUNNING, handle ContainerResourceFailedEvent, cause NPE before\n+      wc.handleContainerResourceFailedEvent();\n+    } finally {\n+      if (wc != null) {\n+        wc.finished();\n+      }\n+    }\n+  }\n+\n   @Test\n   @SuppressWarnings(\"unchecked\") // mocked generic\n   public void testCleanupOnKillRequest() throws Exception {\n@@ -1400,6 +1421,11 @@ public void resourceFailedContainer() {\n       drainDispatcherEvents();\n     }\n \n+    public void handleContainerResourceFailedEvent() {\n+      c.handle(new ContainerResourceFailedEvent(cId, null, null));\n+      drainDispatcherEvents();\n+    }\n+\n     // Localize resources \n     // Skip some resources so as to consider them failed\n     public Map<Path, List<String>> doLocalizeResources(",
                "raw_url": "https://github.com/apache/hadoop/raw/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java",
                "sha": "c32ff1a66be6f721b537578546b6af1a34f12ac7",
                "status": "modified"
            }
        ],
        "message": "YARN-7511. NPE in ContainerLocalizer when localization failed for running container. Contributed by Tao Yang",
        "parent": "https://github.com/apache/hadoop/commit/55669515f626eb5b1f3ba25095f3e306c243d899",
        "patched_files": [
            "ResourceSet.java",
            "Container.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestContainer.java"
        ]
    },
    "hadoop_84c35ac": {
        "bug_id": "hadoop_84c35ac",
        "commit": "https://github.com/apache/hadoop/commit/84c35ac6c4a76c31d9bf9c87b87ed29394564611",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml",
                "patch": "@@ -117,8 +117,15 @@\n \n   <!-- Object cast is based on the event type -->\n   <Match>\n-    <Class name=\"org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher$ApplicationEventHandler\" />\n-     <Bug pattern=\"BC_UNCONFIRMED_CAST\" />\n+    <Class name=\"org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher\" />\n+    <Method name=\"publishApplicationEvent\" />\n+    <Bug pattern=\"BC_UNCONFIRMED_CAST\" />\n+  </Match>\n+\n+  <Match>\n+    <Class name=\"org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher\" />\n+    <Method name=\"publishLocalizationEvent\" />\n+    <Bug pattern=\"BC_UNCONFIRMED_CAST\" />\n   </Match>\n \n   <Match>",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml",
                "sha": "08c6ba28c766401de336d98b5c306f6103ff461a",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java",
                "patch": "@@ -17,22 +17,23 @@\n  */\n package org.apache.hadoop.yarn.api.records.timelineservice;\n \n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.yarn.util.TimelineServiceHelper;\n-import org.codehaus.jackson.annotate.JsonSetter;\n-\n-import javax.xml.bind.annotation.XmlAccessType;\n-import javax.xml.bind.annotation.XmlAccessorType;\n-import javax.xml.bind.annotation.XmlElement;\n-import javax.xml.bind.annotation.XmlRootElement;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Map;\n import java.util.NavigableSet;\n import java.util.Set;\n import java.util.TreeSet;\n \n+import javax.xml.bind.annotation.XmlAccessType;\n+import javax.xml.bind.annotation.XmlAccessorType;\n+import javax.xml.bind.annotation.XmlElement;\n+import javax.xml.bind.annotation.XmlRootElement;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.yarn.util.TimelineServiceHelper;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+\n /**\n  * The basic timeline entity data structure for timeline service v2. Timeline\n  * entity objects are not thread safe and should not be accessed concurrently.\n@@ -564,6 +565,10 @@ protected TimelineEntity getReal() {\n   }\n \n   public String toString() {\n-    return identifier.toString();\n+    if (real == null) {\n+      return identifier.toString();\n+    } else {\n+      return real.toString();\n+    }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java",
                "sha": "7ce8279e5f496f4858713282347008c11f833a14",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 18,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java",
                "patch": "@@ -429,9 +429,8 @@ protected void putObjects(String path, MultivaluedMap<String, String> params,\n         URI uri = constructResURI(getConfig(), timelineServiceAddress, true);\n         putObjects(uri, path, params, obj);\n         needRetry = false;\n-      } catch (Exception e) {\n-        // TODO only handle exception for timelineServiceAddress being updated.\n-        // skip retry for other exceptions.\n+      } catch (IOException e) {\n+        // handle exception for timelineServiceAddress being updated.\n         checkRetryWithSleep(retries, e);\n         retries--;\n       }\n@@ -458,29 +457,27 @@ private int verifyRestEndPointAvailable() throws YarnException {\n    * @param retries\n    * @param e\n    */\n-  private void checkRetryWithSleep(int retries, Exception e) throws\n-      YarnException, IOException {\n+  private void checkRetryWithSleep(int retries, IOException e)\n+      throws YarnException, IOException {\n     if (retries > 0) {\n       try {\n         Thread.sleep(this.serviceRetryInterval);\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n+        throw new YarnException(\"Interrupted while retrying to connect to ATS\");\n       }\n     } else {\n-      LOG.error(\"TimelineClient has reached to max retry times :\" +\n-          this.maxServiceRetries + \" for service address: \" +\n-          timelineServiceAddress);\n-      if (e instanceof YarnException) {\n-        throw (YarnException)e;\n-      } else if (e instanceof IOException) {\n-        throw (IOException)e;\n-      } else {\n-        throw new YarnException(e);\n-      }\n+      StringBuilder msg =\n+          new StringBuilder(\"TimelineClient has reached to max retry times : \");\n+      msg.append(this.maxServiceRetries);\n+      msg.append(\" for service address: \");\n+      msg.append(timelineServiceAddress);\n+      LOG.error(msg.toString());\n+      throw new IOException(msg.toString(), e);\n     }\n   }\n \n-  private void putObjects(\n+  protected void putObjects(\n       URI base, String path, MultivaluedMap<String, String> params, Object obj)\n           throws IOException, YarnException {\n     ClientResponse resp;\n@@ -636,17 +633,19 @@ private Object operateDelegationToken(\n \n   /**\n    * Poll TimelineServiceAddress for maximum of retries times if it is null.\n+   *\n    * @param retries\n    * @return the left retry times\n+   * @throws IOException\n    */\n-  private int pollTimelineServiceAddress(int retries) {\n+  private int pollTimelineServiceAddress(int retries) throws YarnException {\n     while (timelineServiceAddress == null && retries > 0) {\n       try {\n         Thread.sleep(this.serviceRetryInterval);\n       } catch (InterruptedException e) {\n         Thread.currentThread().interrupt();\n+        throw new YarnException(\"Interrupted while trying to connect ATS\");\n       }\n-      // timelineServiceAddress = getTimelineServiceAddress();\n       retries--;\n     }\n     return retries;",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java",
                "sha": "ef8838e1a1fe349afda5d4349a881fbb0439f2ab",
                "status": "modified"
            },
            {
                "additions": 81,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java",
                "changes": 91,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.client.api.impl;\n \n import java.io.IOException;\n+import java.net.URI;\n import java.util.ArrayList;\n import java.util.List;\n \n@@ -34,23 +35,33 @@\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.TestName;\n \n public class TestTimelineClientV2Impl {\n   private static final Log LOG =\n       LogFactory.getLog(TestTimelineClientV2Impl.class);\n   private TestV2TimelineClient client;\n   private static long TIME_TO_SLEEP = 150;\n+  private static final String EXCEPTION_MSG = \"Exception in the content\";\n \n   @Before\n   public void setup() {\n-    YarnConfiguration conf = new YarnConfiguration();\n+    conf = new YarnConfiguration();\n     conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\n     conf.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 1.0f);\n     conf.setInt(YarnConfiguration.NUMBER_OF_ASYNC_ENTITIES_TO_MERGE, 3);\n-    client = createTimelineClient(conf);\n+    if (!currTestName.getMethodName()\n+        .contains(\"testRetryOnConnectionFailure\")) {\n+      client = createTimelineClient(conf);\n+    }\n   }\n \n+  @Rule\n+  public TestName currTestName = new TestName();\n+  private YarnConfiguration conf;\n+\n   private TestV2TimelineClient createTimelineClient(YarnConfiguration conf) {\n     ApplicationId id = ApplicationId.newInstance(0, 0);\n     TestV2TimelineClient client = new TestV2TimelineClient(id);\n@@ -59,9 +70,34 @@ private TestV2TimelineClient createTimelineClient(YarnConfiguration conf) {\n     return client;\n   }\n \n-  private class TestV2TimelineClient extends TimelineClientImpl {\n+  private class TestV2TimelineClientForExceptionHandling\n+      extends TimelineClientImpl {\n+    public TestV2TimelineClientForExceptionHandling(ApplicationId id) {\n+      super(id);\n+    }\n+\n+    protected boolean throwYarnException;\n+\n+    public void setThrowYarnException(boolean throwYarnException) {\n+      this.throwYarnException = throwYarnException;\n+    }\n+\n+    @Override\n+    protected void putObjects(URI base, String path,\n+        MultivaluedMap<String, String> params, Object obj)\n+            throws IOException, YarnException {\n+      if (throwYarnException) {\n+        throw new YarnException(EXCEPTION_MSG);\n+      } else {\n+        throw new IOException(\n+            \"Failed to get the response from the timeline server.\");\n+      }\n+    }\n+  }\n+\n+  private class TestV2TimelineClient\n+      extends TestV2TimelineClientForExceptionHandling {\n     private boolean sleepBeforeReturn;\n-    private boolean throwException;\n \n     private List<TimelineEntities> publishedEntities;\n \n@@ -75,10 +111,6 @@ public void setSleepBeforeReturn(boolean sleepBeforeReturn) {\n       this.sleepBeforeReturn = sleepBeforeReturn;\n     }\n \n-    public void setThrowException(boolean throwException) {\n-      this.throwException = throwException;\n-    }\n-\n     public int getNumOfTimelineEntitiesPublished() {\n       return publishedEntities.size();\n     }\n@@ -91,7 +123,7 @@ public TestV2TimelineClient(ApplicationId id) {\n     protected void putObjects(String path,\n         MultivaluedMap<String, String> params, Object obj)\n             throws IOException, YarnException {\n-      if (throwException) {\n+      if (throwYarnException) {\n         throw new YarnException(\"ActualException\");\n       }\n       publishedEntities.add((TimelineEntities) obj);\n@@ -105,6 +137,45 @@ protected void putObjects(String path,\n     }\n   }\n \n+  @Test\n+  public void testExceptionMultipleRetry() {\n+    TestV2TimelineClientForExceptionHandling client =\n+        new TestV2TimelineClientForExceptionHandling(\n+            ApplicationId.newInstance(0, 0));\n+    int maxRetries = 2;\n+    conf.setInt(YarnConfiguration.TIMELINE_SERVICE_CLIENT_MAX_RETRIES,\n+        maxRetries);\n+    client.init(conf);\n+    client.start();\n+    client.setTimelineServiceAddress(\"localhost:12345\");\n+    try {\n+      client.putEntities(new TimelineEntity());\n+    } catch (IOException e) {\n+      Assert.fail(\"YARN exception is expected\");\n+    } catch (YarnException e) {\n+      Throwable cause = e.getCause();\n+      Assert.assertTrue(\"IOException is expected\",\n+          cause instanceof IOException);\n+      Assert.assertTrue(\"YARN exception is expected\",\n+          cause.getMessage().contains(\n+              \"TimelineClient has reached to max retry times : \" + maxRetries));\n+    }\n+\n+    client.setThrowYarnException(true);\n+    try {\n+      client.putEntities(new TimelineEntity());\n+    } catch (IOException e) {\n+      Assert.fail(\"YARN exception is expected\");\n+    } catch (YarnException e) {\n+      Throwable cause = e.getCause();\n+      Assert.assertTrue(\"YARN exception is expected\",\n+          cause instanceof YarnException);\n+      Assert.assertTrue(\"YARN exception is expected\",\n+          cause.getMessage().contains(EXCEPTION_MSG));\n+    }\n+    client.stop();\n+  }\n+\n   @Test\n   public void testPostEntities() throws Exception {\n     try {\n@@ -189,7 +260,7 @@ public void testSyncCall() throws Exception {\n \n   @Test\n   public void testExceptionCalls() throws Exception {\n-    client.setThrowException(true);\n+    client.setThrowYarnException(true);\n     try {\n       client.putEntitiesAsync(generateEntity(\"1\"));\n     } catch (YarnException e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java",
                "sha": "71dafdc8461cd58cd88df07c7b6f4926010bc852",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java",
                "patch": "@@ -69,4 +69,12 @@\n \n   public static final String ALLOCATED_HOST_HTTP_ADDRESS_ENTITY_INFO =\n       \"YARN_CONTAINER_ALLOCATED_HOST_HTTP_ADDRESS\";\n+\n+  // Event of this type will be emitted by NM.\n+  public static final String LOCALIZATION_START_EVENT_TYPE =\n+      \"YARN_NM_CONTAINER_LOCALIZATION_STARTED\";\n+\n+  // Event of this type will be emitted by NM.\n+  public static final String LOCALIZATION_FINISHED_EVENT_TYPE =\n+      \"YARN_NM_CONTAINER_LOCALIZATION_FINISHED\";\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java",
                "sha": "eadb5b792d928c3f33c3b7d7726618660ca3aa19",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "patch": "@@ -55,7 +55,6 @@\n import org.apache.hadoop.yarn.api.records.NodeLabel;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceUtilization;\n-import org.apache.hadoop.yarn.client.api.TimelineClient;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n@@ -89,6 +88,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitor;\n import org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics;\n import org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider;\n+import org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher;\n import org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n import org.apache.hadoop.yarn.util.resource.Resources;\n@@ -983,9 +983,11 @@ private void updateTimelineClientsAddress(\n                 LOG.debug(\"Sync a new collector address: \" + collectorAddr +\n                     \" for application: \" + appId + \" from RM.\");\n               }\n-              TimelineClient client = application.getTimelineClient();\n-              if (client != null) {\n-                client.setTimelineServiceAddress(collectorAddr);\n+              NMTimelinePublisher nmTimelinePublisher =\n+                  context.getNMTimelinePublisher();\n+              if (nmTimelinePublisher != null) {\n+                nmTimelinePublisher.setTimelineServiceAddress(\n+                    application.getAppId(), collectorAddr);\n               }\n             }\n           }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "sha": "6e0e7601a01c550d3f78ac28da23af8580e78e25",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java",
                "patch": "@@ -29,7 +29,6 @@\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.service.CompositeService;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n-import org.apache.hadoop.yarn.client.api.TimelineClient;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.ipc.YarnRPC;\n@@ -42,6 +41,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.NodeManager;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application;\n+import org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher;\n \n /**\n  * Service that handles collector information. It is used only if the timeline\n@@ -116,10 +116,10 @@ public ReportNewCollectorInfoResponse reportNewCollectorInfo(\n         String collectorAddr = collector.getCollectorAddr();\n         newCollectorsMap.put(appId, collectorAddr);\n         // set registered collector address to TimelineClient.\n-        TimelineClient client =\n-            context.getApplications().get(appId).getTimelineClient();\n-        if (client != null) {\n-          client.setTimelineServiceAddress(collectorAddr);\n+        NMTimelinePublisher nmTimelinePublisher =\n+            context.getNMTimelinePublisher();\n+        if (nmTimelinePublisher != null) {\n+          nmTimelinePublisher.setTimelineServiceAddress(appId, collectorAddr);\n         }\n       }\n       ((NodeManager.NMContext)context).addRegisteredCollectors(",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java",
                "sha": "d667c0ee2463a0cc1d47aa899462cc70d2c4ce47",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java",
                "patch": "@@ -22,7 +22,6 @@\n \n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n-import org.apache.hadoop.yarn.client.api.TimelineClient;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n \n@@ -41,7 +40,4 @@\n   String getFlowVersion();\n \n   long getFlowRunId();\n-  \n-  TimelineClient getTimelineClient();\n-\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java",
                "sha": "aee0862ae813fbcb96158dd7b84d4ad5fe51b2b8",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 17,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                "patch": "@@ -58,6 +58,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppFinishedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService;\n+import org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n import org.apache.hadoop.yarn.state.InvalidStateTransitionException;\n import org.apache.hadoop.yarn.state.MultipleArcTransition;\n@@ -83,7 +84,6 @@\n   private final ReadLock readLock;\n   private final WriteLock writeLock;\n   private final Context context;\n-  private TimelineClient timelineClient;\n \n   private static final Log LOG = LogFactory.getLog(ApplicationImpl.class);\n \n@@ -143,7 +143,7 @@ public ApplicationImpl(Dispatcher dispatcher, String user,\n       }\n       this.flowContext = flowContext;\n       if (YarnConfiguration.systemMetricsPublisherEnabled(conf)) {\n-        createAndStartTimelineClient(conf);\n+        context.getNMTimelinePublisher().createTimelineClient(appId);\n       }\n     }\n   }\n@@ -175,13 +175,6 @@ public long getFlowRunId() {\n     }\n   }\n \n-  private void createAndStartTimelineClient(Configuration conf) {\n-    // create and start timeline client\n-    this.timelineClient = TimelineClient.createTimelineClient(appId);\n-    timelineClient.init(conf);\n-    timelineClient.start();\n-  }\n-\n   @Override\n   public String getUser() {\n     return user.toString();\n@@ -192,11 +185,6 @@ public ApplicationId getAppId() {\n     return appId;\n   }\n   \n-  @Override\n-  public TimelineClient getTimelineClient() {\n-    return timelineClient;\n-  }\n-\n   @Override\n   public ApplicationState getApplicationState() {\n     this.readLock.lock();\n@@ -575,9 +563,10 @@ public void transition(ApplicationImpl app, ApplicationEvent event) {\n         registeredCollectors.remove(app.getAppId());\n       }\n       // stop timelineClient when application get finished.\n-      TimelineClient timelineClient = app.getTimelineClient();\n-      if (timelineClient != null) {\n-        timelineClient.stop();\n+      NMTimelinePublisher nmTimelinePublisher =\n+          app.context.getNMTimelinePublisher();\n+      if (nmTimelinePublisher != null) {\n+        nmTimelinePublisher.stopTimelineClient(app.getAppId());\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                "sha": "22779bb4b3869350f70315fbde4d4e0027ae59cd",
                "status": "modified"
            },
            {
                "additions": 119,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java",
                "changes": 210,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 91,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java",
                "patch": "@@ -18,8 +18,10 @@\n \n package org.apache.hadoop.yarn.server.nodemanager.timelineservice;\n \n+import java.io.IOException;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -29,7 +31,6 @@\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.timelineservice.ContainerEntity;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n@@ -41,16 +42,15 @@\n import org.apache.hadoop.yarn.event.AsyncDispatcher;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.metrics.ContainerMetricsConstants;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationContainerFinishedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.ContainerMetric;\n import org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree;\n import org.apache.hadoop.yarn.util.timeline.TimelineUtils;\n@@ -72,22 +72,19 @@\n \n   private String httpAddress;\n \n+  protected final Map<ApplicationId, TimelineClient> appToClientMap;\n+\n   public NMTimelinePublisher(Context context) {\n     super(NMTimelinePublisher.class.getName());\n     this.context = context;\n+    appToClientMap = new ConcurrentHashMap<>();\n   }\n \n   @Override\n   protected void serviceInit(Configuration conf) throws Exception {\n     dispatcher = new AsyncDispatcher();\n     dispatcher.register(NMTimelineEventType.class,\n         new ForwardingEventHandler());\n-    dispatcher\n-        .register(ContainerEventType.class, new ContainerEventHandler());\n-    dispatcher.register(ApplicationEventType.class,\n-        new ApplicationEventHandler());\n-    dispatcher.register(LocalizationEventType.class,\n-        new LocalizationEventDispatcher());\n     addIfService(dispatcher);\n     super.serviceInit(conf);\n   }\n@@ -112,7 +109,6 @@ protected void handleNMTimelineEvent(NMTimelineEvent event) {\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void reportContainerResourceUsage(Container container, Long pmemUsage,\n       Float cpuUsagePercentPerCore) {\n     if (pmemUsage != ResourceCalculatorProcessTree.UNAVAILABLE ||\n@@ -133,15 +129,32 @@ public void reportContainerResourceUsage(Container container, Long pmemUsage,\n             Math.round(cpuUsagePercentPerCore));\n         entity.addMetric(cpuMetric);\n       }\n-      dispatcher.getEventHandler()\n-          .handle(new TimelinePublishEvent(entity, container.getContainerId()\n-              .getApplicationAttemptId().getApplicationId()));\n+      ApplicationId appId = container.getContainerId().getApplicationAttemptId()\n+          .getApplicationId();\n+      try {\n+        // no need to put it as part of publisher as timeline client already has\n+        // Queuing concept\n+        TimelineClient timelineClient = getTimelineClient(appId);\n+        if (timelineClient != null) {\n+          timelineClient.putEntitiesAsync(entity);\n+        } else {\n+          LOG.error(\"Seems like client has been removed before the container\"\n+              + \" metric could be published for \" + container.getContainerId());\n+        }\n+      } catch (IOException | YarnException e) {\n+        LOG.error(\"Failed to publish Container metrics for container \"\n+            + container.getContainerId(), e);\n+      }\n     }\n   }\n \n-  private void publishContainerCreatedEvent(ContainerEntity entity,\n-      ContainerId containerId, Resource resource, Priority priority,\n-      long timestamp) {\n+  @SuppressWarnings(\"unchecked\")\n+  private void publishContainerCreatedEvent(ContainerEvent event) {\n+    ContainerId containerId = event.getContainerID();\n+    ContainerEntity entity = createContainerEntity(containerId);\n+    Container container = context.getContainers().get(containerId);\n+    Resource resource = container.getResource();\n+\n     Map<String, Object> entityInfo = new HashMap<String, Object>();\n     entityInfo.put(ContainerMetricsConstants.ALLOCATED_MEMORY_ENTITY_INFO,\n         resource.getMemory());\n@@ -152,21 +165,23 @@ private void publishContainerCreatedEvent(ContainerEntity entity,\n     entityInfo.put(ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO,\n         nodeId.getPort());\n     entityInfo.put(ContainerMetricsConstants.ALLOCATED_PRIORITY_ENTITY_INFO,\n-        priority.toString());\n+        container.getPriority().toString());\n     entityInfo.put(\n         ContainerMetricsConstants.ALLOCATED_HOST_HTTP_ADDRESS_ENTITY_INFO,\n         httpAddress);\n     entity.setInfo(entityInfo);\n \n     TimelineEvent tEvent = new TimelineEvent();\n     tEvent.setId(ContainerMetricsConstants.CREATED_EVENT_TYPE);\n-    tEvent.setTimestamp(timestamp);\n+    tEvent.setTimestamp(event.getTimestamp());\n \n     entity.addEvent(tEvent);\n-    entity.setCreatedTime(timestamp);\n-    putEntity(entity, containerId.getApplicationAttemptId().getApplicationId());\n+    entity.setCreatedTime(event.getTimestamp());\n+    dispatcher.getEventHandler().handle(new TimelinePublishEvent(entity,\n+        containerId.getApplicationAttemptId().getApplicationId()));\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   private void publishContainerFinishedEvent(ContainerStatus containerStatus,\n       long timeStamp) {\n     ContainerId containerId = containerStatus.getContainerId();\n@@ -186,7 +201,38 @@ private void publishContainerFinishedEvent(ContainerStatus containerStatus,\n     tEvent.setInfo(eventInfo);\n \n     entity.addEvent(tEvent);\n-    putEntity(entity, containerId.getApplicationAttemptId().getApplicationId());\n+\n+    dispatcher.getEventHandler().handle(new TimelinePublishEvent(entity,\n+        containerId.getApplicationAttemptId().getApplicationId()));\n+  }\n+\n+  private void publishContainerLocalizationEvent(\n+      ContainerLocalizationEvent event, String eventType) {\n+    Container container = event.getContainer();\n+    ContainerId containerId = container.getContainerId();\n+    TimelineEntity entity = createContainerEntity(containerId);\n+\n+    TimelineEvent tEvent = new TimelineEvent();\n+    tEvent.setId(eventType);\n+    tEvent.setTimestamp(event.getTimestamp());\n+    entity.addEvent(tEvent);\n+\n+    ApplicationId appId =\n+        container.getContainerId().getApplicationAttemptId().getApplicationId();\n+    try {\n+      // no need to put it as part of publisher as timeline client already has\n+      // Queuing concept\n+      TimelineClient timelineClient = getTimelineClient(appId);\n+      if (timelineClient != null) {\n+        timelineClient.putEntitiesAsync(entity);\n+      } else {\n+        LOG.error(\"Seems like client has been removed before the event could be\"\n+            + \" published for \" + container.getContainerId());\n+      }\n+    } catch (IOException | YarnException e) {\n+      LOG.error(\"Failed to publish Container metrics for container \"\n+          + container.getContainerId(), e);\n+    }\n   }\n \n   private static ContainerEntity createContainerEntity(\n@@ -207,23 +253,33 @@ private void putEntity(TimelineEntity entity, ApplicationId appId) {\n         LOG.debug(\"Publishing the entity \" + entity + \", JSON-style content: \"\n             + TimelineUtils.dumpTimelineRecordtoJSON(entity));\n       }\n-      TimelineClient timelineClient =\n-          context.getApplications().get(appId).getTimelineClient();\n-      timelineClient.putEntities(entity);\n+      TimelineClient timelineClient = getTimelineClient(appId);\n+      if (timelineClient != null) {\n+        timelineClient.putEntities(entity);\n+      } else {\n+        LOG.error(\"Seems like client has been removed before the entity \"\n+            + \"could be published for \" + entity);\n+      }\n     } catch (Exception e) {\n       LOG.error(\"Error when publishing entity \" + entity, e);\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void publishApplicationEvent(ApplicationEvent event) {\n     // publish only when the desired event is received\n     switch (event.getType()) {\n     case INIT_APPLICATION:\n     case FINISH_APPLICATION:\n-    case APPLICATION_CONTAINER_FINISHED:\n     case APPLICATION_LOG_HANDLING_FAILED:\n-      dispatcher.getEventHandler().handle(event);\n+      // TODO need to be handled in future,\n+      // not sure to publish under which entity\n+      break;\n+    case APPLICATION_CONTAINER_FINISHED:\n+      // this is actually used to publish the container Event\n+      ApplicationContainerFinishedEvent evnt =\n+          (ApplicationContainerFinishedEvent) event;\n+      publishContainerFinishedEvent(evnt.getContainerStatus(),\n+          event.getTimestamp());\n       break;\n \n     default:\n@@ -235,12 +291,11 @@ public void publishApplicationEvent(ApplicationEvent event) {\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void publishContainerEvent(ContainerEvent event) {\n     // publish only when the desired event is received\n     switch (event.getType()) {\n     case INIT_CONTAINER:\n-      dispatcher.getEventHandler().handle(event);\n+      publishContainerCreatedEvent(event);\n       break;\n \n     default:\n@@ -253,15 +308,17 @@ public void publishContainerEvent(ContainerEvent event) {\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void publishLocalizationEvent(LocalizationEvent event) {\n     // publish only when the desired event is received\n     switch (event.getType()) {\n     case CONTAINER_RESOURCES_LOCALIZED:\n+      publishContainerLocalizationEvent((ContainerLocalizationEvent) event,\n+          ContainerMetricsConstants.LOCALIZATION_FINISHED_EVENT_TYPE);\n+      break;\n     case INIT_CONTAINER_RESOURCES:\n-      dispatcher.getEventHandler().handle(event);\n+      publishContainerLocalizationEvent((ContainerLocalizationEvent) event,\n+          ContainerMetricsConstants.LOCALIZATION_START_EVENT_TYPE);\n       break;\n-\n     default:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(event.getType()\n@@ -272,64 +329,6 @@ public void publishLocalizationEvent(LocalizationEvent event) {\n     }\n   }\n \n-  private class ApplicationEventHandler implements\n-      EventHandler<ApplicationEvent> {\n-    @Override\n-    public void handle(ApplicationEvent event) {\n-      switch (event.getType()) {\n-      case APPLICATION_CONTAINER_FINISHED:\n-        // this is actually used to publish the container Event\n-        ApplicationContainerFinishedEvent evnt =\n-            (ApplicationContainerFinishedEvent) event;\n-        publishContainerFinishedEvent(evnt.getContainerStatus(),\n-            event.getTimestamp());\n-        break;\n-      default:\n-        LOG.error(\"Seems like event type is captured only in \"\n-            + \"publishApplicationEvent method and not handled here\");\n-        break;\n-      }\n-    }\n-  }\n-\n-  private class ContainerEventHandler implements EventHandler<ContainerEvent> {\n-    @Override\n-    public void handle(ContainerEvent event) {\n-      ContainerId containerId = event.getContainerID();\n-      Container container = context.getContainers().get(containerId);\n-      long timestamp = event.getTimestamp();\n-      ContainerEntity entity = createContainerEntity(containerId);\n-\n-      switch (event.getType()) {\n-      case INIT_CONTAINER:\n-        publishContainerCreatedEvent(entity, containerId,\n-            container.getResource(), container.getPriority(), timestamp);\n-        break;\n-      default:\n-        LOG.error(\"Seems like event type is captured only in \"\n-            + \"publishContainerEvent method and not handled here\");\n-        break;\n-      }\n-    }\n-  }\n-\n-  private static final class LocalizationEventDispatcher implements\n-      EventHandler<LocalizationEvent> {\n-    @Override\n-    public void handle(LocalizationEvent event) {\n-      switch (event.getType()) {\n-      case INIT_CONTAINER_RESOURCES:\n-      case CONTAINER_RESOURCES_LOCALIZED:\n-        // TODO after priority based flush jira is finished\n-        break;\n-      default:\n-        LOG.error(\"Seems like event type is captured only in \"\n-            + \"publishLocalizationEvent method and not handled here\");\n-        break;\n-      }\n-    }\n-  }\n-\n   /**\n    * EventHandler implementation which forward events to NMMetricsPublisher.\n    * Making use of it, NMMetricsPublisher can avoid to have a public handle\n@@ -363,4 +362,33 @@ public TimelineEntity getTimelineEntityToPublish() {\n       return entityToPublish;\n     }\n   }\n+\n+  public void createTimelineClient(ApplicationId appId) {\n+    if (!appToClientMap.containsKey(appId)) {\n+      TimelineClient timelineClient =\n+          TimelineClient.createTimelineClient(appId);\n+      timelineClient.init(getConfig());\n+      timelineClient.start();\n+      appToClientMap.put(appId, timelineClient);\n+    }\n+  }\n+\n+  public void stopTimelineClient(ApplicationId appId) {\n+    TimelineClient client = appToClientMap.remove(appId);\n+    if (client != null) {\n+      client.stop();\n+    }\n+  }\n+\n+  public void setTimelineServiceAddress(ApplicationId appId,\n+      String collectorAddr) {\n+    TimelineClient client = appToClientMap.get(appId);\n+    if (client != null) {\n+      client.setTimelineServiceAddress(collectorAddr);\n+    }\n+  }\n+\n+  private TimelineClient getTimelineClient(ApplicationId appId) {\n+    return appToClientMap.get(appId);\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java",
                "sha": "4d3dafdc06889bbd05c4373b7198d638d2cb605c",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java",
                "patch": "@@ -20,14 +20,12 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n-import static org.mockito.Matchers.any;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.util.Iterator;\n import java.util.Map.Entry;\n-import java.util.concurrent.ConcurrentMap;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n@@ -39,7 +37,6 @@\n import org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree;\n import org.junit.Assert;\n@@ -53,20 +50,23 @@\n   public void testContainerResourceUsage() {\n     Context context = mock(Context.class);\n     @SuppressWarnings(\"unchecked\")\n-    ConcurrentMap<ApplicationId, Application> map = mock(ConcurrentMap.class);\n-    Application aApp = mock(Application.class);\n-    when(map.get(any(ApplicationId.class))).thenReturn(aApp);\n-    DummyTimelineClient timelineClient = new DummyTimelineClient();\n-    when(aApp.getTimelineClient()).thenReturn(timelineClient);\n-    when(context.getApplications()).thenReturn(map);\n+    final DummyTimelineClient timelineClient = new DummyTimelineClient();\n     when(context.getNodeId()).thenReturn(NodeId.newInstance(\"localhost\", 0));\n     when(context.getHttpPort()).thenReturn(0);\n-    NMTimelinePublisher publisher = new NMTimelinePublisher(context);\n+    NMTimelinePublisher publisher = new NMTimelinePublisher(context) {\n+      public void createTimelineClient(ApplicationId appId) {\n+        if (!appToClientMap.containsKey(appId)) {\n+          appToClientMap.put(appId, timelineClient);\n+        }\n+      }\n+    };\n     publisher.init(new Configuration());\n     publisher.start();\n+    ApplicationId appId = ApplicationId.newInstance(0, 1);\n+    publisher.createTimelineClient(appId);\n     Container aContainer = mock(Container.class);\n     when(aContainer.getContainerId()).thenReturn(ContainerId.newContainerId(\n-        ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1),\n+        ApplicationAttemptId.newInstance(appId, 1),\n         0L));\n     publisher.reportContainerResourceUsage(aContainer, 1024L, 8F);\n     verifyPublishedResourceUsageMetrics(timelineClient, 1024L, 8);\n@@ -141,7 +141,7 @@ private void verifyPublishedResourceUsageMetrics(\n     private TimelineEntity[] lastPublishedEntities;\n \n     @Override\n-    public void putEntities(TimelineEntity... entities)\n+    public void putEntitiesAsync(TimelineEntity... entities)\n         throws IOException, YarnException {\n       this.lastPublishedEntities = entities;\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java",
                "sha": "4aa28d2dbe7e96fd1e9fba92239de581bd8bcce8",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java",
                "patch": "@@ -101,9 +101,4 @@ public String getFlowVersion() {\n   public long getFlowRunId() {\n     return flowRunId;\n   }\n-  \n-  @Override\n-  public TimelineClient getTimelineClient() {\n-    return timelineClient;\n-  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java",
                "sha": "c98304001a17c2af13f1958189c3cca4e5f7fbbd",
                "status": "modified"
            }
        ],
        "message": "YARN-4711. NM is going down with NPE's due to single thread processing of events by Timeline client (Naganarasimha G R via sjlee)",
        "parent": "https://github.com/apache/hadoop/commit/6f6cc647d6e77f6cc4c66e0534f8c73bc1612a1b",
        "patched_files": [
            "findbugs-exclude.java",
            "ContainerMetricsConstants.java",
            "NodeStatusUpdaterImpl.java",
            "NMTimelinePublisher.java",
            "ApplicationImpl.java",
            "NMCollectorService.java",
            "MockApp.java",
            "TimelineEntity.java",
            "Application.java",
            "TimelineClientImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplication.java",
            "TestTimelineClientV2Impl.java",
            "TestNMTimelinePublisher.java"
        ]
    },
    "hadoop_84dfae2": {
        "bug_id": "hadoop_84dfae2",
        "commit": "https://github.com/apache/hadoop/commit/84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -222,6 +222,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1957. Consider the max capacity of the queue when computing the ideal\n     capacity for preemption. (Carlo Curino via cdouglas)\n \n+    YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and\n+    attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/CHANGES.txt",
                "sha": "d82cd482a1fb2a3384b3b6a8bfc544f343ee50ae",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java?ref=84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "patch": "@@ -360,7 +360,8 @@ private FiCaSchedulerNode getNode(NodeId nodeId) {\n     return nodes.get(nodeId);\n   }\n \n-  private synchronized void addApplication(ApplicationId applicationId,\n+  @VisibleForTesting\n+  public synchronized void addApplication(ApplicationId applicationId,\n       String queue, String user) {\n     SchedulerApplication application =\n         new SchedulerApplication(DEFAULT_QUEUE, user);\n@@ -372,7 +373,8 @@ private synchronized void addApplication(ApplicationId applicationId,\n         .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n   }\n \n-  private synchronized void\n+  @VisibleForTesting\n+  public synchronized void\n       addApplicationAttempt(ApplicationAttemptId appAttemptId,\n           boolean transferStateFromPreviousAttempt) {\n     SchedulerApplication application =\n@@ -458,6 +460,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n         .entrySet()) {\n       FiCaSchedulerApp application =\n           (FiCaSchedulerApp) e.getValue().getCurrentAppAttempt();\n+      if (application == null) {\n+        continue;\n+      }\n       LOG.debug(\"pre-assignContainers\");\n       application.showRequests();\n       synchronized (application) {\n@@ -497,6 +502,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n     for (SchedulerApplication application : applications.values()) {\n       FiCaSchedulerApp attempt =\n           (FiCaSchedulerApp) application.getCurrentAppAttempt();\n+      if (attempt == null) {\n+        continue;\n+      }\n       attempt.setHeadroom(Resources.subtract(clusterResource, usedResource));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "sha": "21fcdecf4f9ef9fa2ee555996ba746252012e656",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java?ref=84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n+import org.apache.hadoop.yarn.util.resource.Resources;\n import org.apache.log4j.Level;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n@@ -66,7 +67,7 @@\n   \n   private final int GB = 1024;\n   private static YarnConfiguration conf;\n-  \n+\n   @BeforeClass\n   public static void setup() {\n     conf = new YarnConfiguration();\n@@ -213,6 +214,32 @@ public void test() throws Exception {\n     rm.stop();\n   }\n \n+  @Test\n+  public void testNodeUpdateBeforeAppAttemptInit() throws Exception {\n+    FifoScheduler scheduler = new FifoScheduler();\n+    MockRM rm = new MockRM(conf);\n+    scheduler.reinitialize(conf, rm.getRMContext());\n+\n+    RMNode node = MockNodes.newNodeInfo(1,\n+            Resources.createResource(1024, 4), 1, \"127.0.0.1\");\n+    scheduler.handle(new NodeAddedSchedulerEvent(node));\n+\n+    ApplicationId appId = ApplicationId.newInstance(0, 1);\n+    scheduler.addApplication(appId, \"queue1\", \"user1\");\n+\n+    NodeUpdateSchedulerEvent updateEvent = new NodeUpdateSchedulerEvent(node);\n+    try {\n+      scheduler.handle(updateEvent);\n+    } catch (NullPointerException e) {\n+        Assert.fail();\n+    }\n+\n+    ApplicationAttemptId attId = ApplicationAttemptId.newInstance(appId, 1);\n+    scheduler.addApplicationAttempt(attId, false);\n+\n+    rm.stop();\n+  }\n+\n   private void testMinimumAllocation(YarnConfiguration conf, int testAlloc)\n       throws Exception {\n     MockRM rm = new MockRM(conf);",
                "raw_url": "https://github.com/apache/hadoop/raw/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "sha": "fcd5041e4257d4cc1ce64ae90278aba09576f14d",
                "status": "modified"
            }
        ],
        "message": "YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594476 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/cf7dddb603b762e0808d77b7ddefa73274c09156",
        "patched_files": [
            "CHANGES.java",
            "FifoScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFifoScheduler.java"
        ]
    },
    "hadoop_855d529": {
        "bug_id": "hadoop_855d529",
        "commit": "https://github.com/apache/hadoop/commit/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -1181,6 +1181,9 @@ Release 2.7.2 - UNRELEASED\n     YARN-4320. TestJobHistoryEventHandler fails as AHS in MiniYarnCluster no longer\n     binds to default port 8188. (Varun Saxena via ozawa)\n \n+    YARN-4354. Public resource localization fails with NPE. (Jason Lowe via \n+    junping_du)\n+\n Release 2.7.1 - 2015-07-06\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/CHANGES.txt",
                "sha": "747c7290128fa6e806e74810daaf885e9b86c06c",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "patch": "@@ -185,14 +185,22 @@ public synchronized void handle(ResourceEvent event) {\n       break;\n     }\n \n+    if (rsrc == null) {\n+      LOG.warn(\"Received \" + event.getType() + \" event for request \" + req\n+          + \" but localized resource is missing\");\n+      return;\n+    }\n     rsrc.handle(event);\n \n     // Remove the resource if its downloading and its reference count has\n     // become 0 after RELEASE. This maybe because a container was killed while\n     // localizing and no other container is referring to the resource.\n+    // NOTE: This should NOT be done for public resources since the\n+    //       download is not associated with a container-specific localizer.\n     if (event.getType() == ResourceEventType.RELEASE) {\n       if (rsrc.getState() == ResourceState.DOWNLOADING &&\n-          rsrc.getRefCount() <= 0) {\n+          rsrc.getRefCount() <= 0 &&\n+          rsrc.getRequest().getVisibility() != LocalResourceVisibility.PUBLIC) {\n         removeResource(req);\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "sha": "38fffe6c6cf1df17f1d11fbf278aba155b39d803",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java",
                "patch": "@@ -141,12 +141,12 @@ public void test() {\n       tracker.handle(rel21Event);\n \n       dispatcher.await();\n-      verifyTrackedResourceCount(tracker, 1);\n+      verifyTrackedResourceCount(tracker, 2);\n \n       // Verify resource with non zero ref count is not removed.\n       Assert.assertEquals(2, lr1.getRefCount());\n       Assert.assertFalse(tracker.remove(lr1, mockDelService));\n-      verifyTrackedResourceCount(tracker, 1);\n+      verifyTrackedResourceCount(tracker, 2);\n \n       // Localize resource1\n       ResourceLocalizedEvent rle =\n@@ -161,7 +161,7 @@ public void test() {\n \n       // Verify resources in state LOCALIZED with ref-count=0 is removed.\n       Assert.assertTrue(tracker.remove(lr1, mockDelService));\n-      verifyTrackedResourceCount(tracker, 0);\n+      verifyTrackedResourceCount(tracker, 1);\n     } finally {\n       if (dispatcher != null) {\n         dispatcher.stop();\n@@ -899,6 +899,56 @@ public void testResourcePresentInGoodDir() throws IOException {\n     }\n   }\n \n+  @Test\n+  @SuppressWarnings(\"unchecked\")\n+  public void testReleaseWhileDownloading() throws Exception {\n+    String user = \"testuser\";\n+    DrainDispatcher dispatcher = null;\n+    try {\n+      Configuration conf = new Configuration();\n+      dispatcher = createDispatcher(conf);\n+      EventHandler<LocalizerEvent> localizerEventHandler =\n+          mock(EventHandler.class);\n+      EventHandler<LocalizerEvent> containerEventHandler =\n+          mock(EventHandler.class);\n+      dispatcher.register(LocalizerEventType.class, localizerEventHandler);\n+      dispatcher.register(ContainerEventType.class, containerEventHandler);\n+\n+      ContainerId cId = BuilderUtils.newContainerId(1, 1, 1, 1);\n+      LocalizerContext lc = new LocalizerContext(user, cId, null);\n+\n+      LocalResourceRequest req =\n+          createLocalResourceRequest(user, 1, 1, LocalResourceVisibility.PUBLIC);\n+      LocalizedResource lr = createLocalizedResource(req, dispatcher);\n+      ConcurrentMap<LocalResourceRequest, LocalizedResource> localrsrc =\n+          new ConcurrentHashMap<LocalResourceRequest, LocalizedResource>();\n+      localrsrc.put(req, lr);\n+      LocalResourcesTracker tracker =\n+          new LocalResourcesTrackerImpl(user, null, dispatcher, localrsrc,\n+              false, conf, new NMNullStateStoreService(), null);\n+\n+      // request the resource\n+      ResourceEvent reqEvent =\n+          new ResourceRequestEvent(req, LocalResourceVisibility.PUBLIC, lc);\n+      tracker.handle(reqEvent);\n+\n+      // release the resource\n+      ResourceEvent relEvent = new ResourceReleaseEvent(req, cId);\n+      tracker.handle(relEvent);\n+\n+      // download completing after release\n+      ResourceLocalizedEvent rle =\n+          new ResourceLocalizedEvent(req, new Path(\"file:///tmp/r1\"), 1);\n+      tracker.handle(rle);\n+\n+      dispatcher.await();\n+    } finally {\n+      if (dispatcher != null) {\n+        dispatcher.stop();\n+      }\n+    }\n+  }\n+\n   private boolean createdummylocalizefile(Path path) {\n     boolean ret = false;\n     File file = new File(path.toUri().getRawPath().toString());",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java",
                "sha": "21ceaa4f9810820f89ff19803eb0f46606b25166",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "patch": "@@ -487,8 +487,8 @@ public void testResourceRelease() throws Exception {\n         Assert.assertEquals(\"Incorrect reference count\", 0, lr.getRefCount());\n         pubRsrcs.remove(lr.getRequest());\n       }\n-      Assert.assertEquals(2, pubRsrcs.size());\n-      Assert.assertEquals(0, pubRsrcCount);\n+      Assert.assertEquals(0, pubRsrcs.size());\n+      Assert.assertEquals(2, pubRsrcCount);\n \n       appRsrcCount = 0;\n       for (LocalizedResource lr : appTracker) {",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "sha": "c14ec7f45deadc7cee1ddd43f049fa4d3e5f0525",
                "status": "modified"
            }
        ],
        "message": "YARN-4354. Public resource localization fails with NPE. Contributed by Jason Lowe.",
        "parent": "https://github.com/apache/hadoop/commit/c753617a48bffed491b9ca7a5ca6b3d2df5721bf",
        "patched_files": [
            "CHANGES.java",
            "ResourceLocalizationService.java",
            "LocalResourcesTrackerImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalResourcesTrackerImpl.java",
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_85cc644": {
        "bug_id": "hadoop_85cc644",
        "commit": "https://github.com/apache/hadoop/commit/85cc644f92e0e55293993c43e381f161bca5ce32",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/85cc644f92e0e55293993c43e381f161bca5ce32/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=85cc644f92e0e55293993c43e381f161bca5ce32",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -197,6 +197,9 @@ Trunk (Unreleased)\n     MAPREDUCE-6343. JobConf.parseMaximumHeapSizeMB() fails to parse value \n     greater than 2GB expressed in bytes. (Hao Xia via kasha)\n \n+    MAPREDUCE-6396. TestPipeApplication fails by NullPointerException.\n+    (Brahma Reddy Battula via aajisaka)\n+\n   BREAKDOWN OF MAPREDUCE-2841 (NATIVE TASK) SUBTASKS\n \n     MAPREDUCE-5985. native-task: Fix build on macosx. Contributed by",
                "raw_url": "https://github.com/apache/hadoop/raw/85cc644f92e0e55293993c43e381f161bca5ce32/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "a9f5053a3e53e6ca4c815bf6c64183145d50856c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/85cc644f92e0e55293993c43e381f161bca5ce32/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/pipes/TestPipeApplication.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/pipes/TestPipeApplication.java?ref=85cc644f92e0e55293993c43e381f161bca5ce32",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/pipes/TestPipeApplication.java",
                "patch": "@@ -36,6 +36,7 @@\n \n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.FsConstants;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.RawLocalFileSystem;\n import org.apache.hadoop.io.BooleanWritable;\n@@ -94,7 +95,7 @@ public void testRunner() throws Exception {\n       CombineOutputCollector<IntWritable, Text> output = new CombineOutputCollector<IntWritable, Text>(\n               new Counters.Counter(), new Progress());\n       FileSystem fs = new RawLocalFileSystem();\n-      fs.setConf(conf);\n+      fs.initialize(FsConstants.LOCAL_FS_URI, conf);\n       Writer<IntWritable, Text> wr = new Writer<IntWritable, Text>(conf, fs.create(\n               new Path(workSpace + File.separator + \"outfile\")), IntWritable.class,\n               Text.class, null, null, true);\n@@ -176,7 +177,7 @@ public void testApplication() throws Throwable {\n       FakeCollector output = new FakeCollector(new Counters.Counter(),\n               new Progress());\n       FileSystem fs = new RawLocalFileSystem();\n-      fs.setConf(conf);\n+      fs.initialize(FsConstants.LOCAL_FS_URI, conf);\n       Writer<IntWritable, Text> wr = new Writer<IntWritable, Text>(conf, fs.create(\n               new Path(workSpace.getAbsolutePath() + File.separator + \"outfile\")),\n               IntWritable.class, Text.class, null, null, true);",
                "raw_url": "https://github.com/apache/hadoop/raw/85cc644f92e0e55293993c43e381f161bca5ce32/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/pipes/TestPipeApplication.java",
                "sha": "22c5f41dd78718ded1190f439d54975af3b67e45",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6396. TestPipeApplication fails by NullPointerException. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/75a2560e51387ea31ef4609ef434475bbbc628f7",
        "patched_files": [
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestPipeApplication.java"
        ]
    },
    "hadoop_883f682": {
        "bug_id": "hadoop_883f682",
        "commit": "https://github.com/apache/hadoop/commit/883f68222a9cfd06f79a8fcd75ec9fef00abc035",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "patch": "@@ -54,7 +54,9 @@\n     GPU(\"--module-gpu\"),\n     FPGA(\"--module-fpga\"),\n     LIST_AS_USER(\"\"), // no CLI switch supported yet.\n-    ADD_NUMA_PARAMS(\"\"); // no CLI switch supported yet.\n+    ADD_NUMA_PARAMS(\"\"), // no CLI switch supported yet.\n+    REMOVE_DOCKER_CONTAINER(\"--remove-docker-container\"),\n+    INSPECT_DOCKER_CONTAINER(\"--inspect-docker-container\");\n \n     private final String option;\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "sha": "92a82e8fbcdab790b80b82d5b5b585fcb508f3d6",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.security.Credentials;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor;\n@@ -384,7 +385,7 @@ private String runDockerVolumeCommand(DockerVolumeCommand dockerVolumeCommand,\n       Container container) throws ContainerExecutionException {\n     try {\n       String commandFile = dockerClient.writeCommandToTempFile(\n-          dockerVolumeCommand, container, nmContext);\n+          dockerVolumeCommand, container.getContainerId(), nmContext);\n       PrivilegedOperation privOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n       privOp.appendArgs(commandFile);\n@@ -734,6 +735,7 @@ private String getUserIdInfo(String userName)\n   public void launchContainer(ContainerRuntimeContext ctx)\n       throws ContainerExecutionException {\n     Container container = ctx.getContainer();\n+    ContainerId containerId = container.getContainerId();\n     Map<String, String> environment = container.getLaunchContext()\n         .getEnvironment();\n     String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n@@ -750,7 +752,7 @@ public void launchContainer(ContainerRuntimeContext ctx)\n \n     validateImageName(imageName);\n \n-    String containerIdStr = container.getContainerId().toString();\n+    String containerIdStr = containerId.toString();\n     String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n     String dockerRunAsUser = runAsUser;\n     Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n@@ -908,7 +910,7 @@ public void launchContainer(ContainerRuntimeContext ctx)\n     }\n \n     String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n-        container, nmContext);\n+        containerId, nmContext);\n     PrivilegedOperation launchOp = buildLaunchOp(ctx,\n         commandFile, runCommand);\n \n@@ -927,8 +929,8 @@ public void launchContainer(ContainerRuntimeContext ctx)\n   @Override\n   public void relaunchContainer(ContainerRuntimeContext ctx)\n       throws ContainerExecutionException {\n-    Container container = ctx.getContainer();\n-    String containerIdStr = container.getContainerId().toString();\n+    ContainerId containerId = ctx.getContainer().getContainerId();\n+    String containerIdStr = containerId.toString();\n     // Check to see if the container already exists for relaunch\n     DockerCommandExecutor.DockerContainerStatus containerStatus =\n         DockerCommandExecutor.getContainerStatus(containerIdStr, conf,\n@@ -937,7 +939,7 @@ public void relaunchContainer(ContainerRuntimeContext ctx)\n         DockerCommandExecutor.isStartable(containerStatus)) {\n       DockerStartCommand startCommand = new DockerStartCommand(containerIdStr);\n       String commandFile = dockerClient.writeCommandToTempFile(startCommand,\n-          container, nmContext);\n+          containerId, nmContext);\n       PrivilegedOperation launchOp = buildLaunchOp(ctx, commandFile,\n           startCommand);\n \n@@ -1042,12 +1044,13 @@ public void reapContainer(ContainerRuntimeContext ctx)\n   // ipAndHost[1] contains the hostname.\n   @Override\n   public String[] getIpAndHost(Container container) {\n-    String containerId = container.getContainerId().toString();\n+    ContainerId containerId = container.getContainerId();\n+    String containerIdStr = containerId.toString();\n     DockerInspectCommand inspectCommand =\n-        new DockerInspectCommand(containerId).getIpAndHost();\n+        new DockerInspectCommand(containerIdStr).getIpAndHost();\n     try {\n       String commandFile = dockerClient.writeCommandToTempFile(inspectCommand,\n-          container, nmContext);\n+          containerId, nmContext);\n       PrivilegedOperation privOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n       privOp.appendArgs(commandFile);",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "sha": "ec1d055668de701b39fcae1ff8209edce3382c19",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerExecutionException;\n import org.slf4j.Logger;\n@@ -104,9 +103,9 @@ public String writeCommandToTempFile(DockerCommand cmd, String filePrefix)\n     }\n   }\n \n-  public String writeCommandToTempFile(DockerCommand cmd, Container container,\n-      Context nmContext) throws ContainerExecutionException {\n-    ContainerId containerId = container.getContainerId();\n+  public String writeCommandToTempFile(DockerCommand cmd,\n+      ContainerId containerId, Context nmContext)\n+      throws ContainerExecutionException {\n     String filePrefix = containerId.toString();\n     ApplicationId appId = containerId.getApplicationAttemptId()\n         .getApplicationId();",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java",
                "sha": "7bd4546fc8d1f4bf39e3fc5e5cb8118de0beaf50",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java",
                "patch": "@@ -22,7 +22,12 @@\n \n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerExecutionException;\n \n import java.util.ArrayList;\n import java.util.Collections;\n@@ -104,4 +109,31 @@ public void setClientConfigDir(String clientConfigDir) {\n       addCommandArguments(\"docker-config\", clientConfigDir);\n     }\n   }\n+\n+  /**\n+   * Prepare the privileged operation object that will be used to invoke\n+   * the container-executor.\n+   *\n+   * @param dockerCommand Specific command to be run by docker.\n+   * @param containerName\n+   * @param env\n+   * @param conf\n+   * @param nmContext\n+   * @return Returns the PrivilegedOperation object to be used.\n+   * @throws ContainerExecutionException\n+   */\n+  public PrivilegedOperation preparePrivilegedOperation(\n+      DockerCommand dockerCommand, String containerName, Map<String,\n+      String> env, Configuration conf, Context nmContext)\n+      throws ContainerExecutionException {\n+    DockerClient dockerClient = new DockerClient(conf);\n+    String commandFile =\n+        dockerClient.writeCommandToTempFile(dockerCommand,\n+        ContainerId.fromString(containerName),\n+        nmContext);\n+    PrivilegedOperation dockerOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n+    dockerOp.appendArgs(commandFile);\n+    return dockerOp;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java",
                "sha": "366457d11e4376e6be5166a9502046d99163efb2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java",
                "patch": "@@ -17,7 +17,6 @@\n package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n@@ -80,14 +79,9 @@ public static String executeDockerCommand(DockerCommand dockerCommand,\n       PrivilegedOperationExecutor privilegedOperationExecutor,\n       boolean disableFailureLogging, Context nmContext)\n       throws ContainerExecutionException {\n-    DockerClient dockerClient = new DockerClient(conf);\n-    String commandFile =\n-        dockerClient.writeCommandToTempFile(dockerCommand,\n-        nmContext.getContainers().get(ContainerId.fromString(containerId)),\n-        nmContext);\n-    PrivilegedOperation dockerOp = new PrivilegedOperation(\n-        PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n-    dockerOp.appendArgs(commandFile);\n+    PrivilegedOperation dockerOp = dockerCommand.preparePrivilegedOperation(\n+        dockerCommand, containerId, env, conf, nmContext);\n+\n     if (disableFailureLogging) {\n       dockerOp.disableFailureLogging();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java",
                "sha": "8a4888cb2cd72a503f8e8877bbe2b1674cece5aa",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java",
                "patch": "@@ -20,12 +20,19 @@\n \n package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker;\n \n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+\n+import java.util.Map;\n+\n /**\n  * Encapsulates the docker inspect command and its command\n  * line arguments.\n  */\n public class DockerInspectCommand extends DockerCommand {\n   private static final String INSPECT_COMMAND = \"inspect\";\n+  private String commandArguments;\n \n   public DockerInspectCommand(String containerName) {\n     super(INSPECT_COMMAND);\n@@ -34,6 +41,7 @@ public DockerInspectCommand(String containerName) {\n \n   public DockerInspectCommand getContainerStatus() {\n     super.addCommandArguments(\"format\", \"{{.State.Status}}\");\n+    this.commandArguments = \"--format={{.State.Status}}\";\n     return this;\n   }\n \n@@ -43,6 +51,17 @@ public DockerInspectCommand getIpAndHost() {\n     // cannot parse the arguments correctly.\n     super.addCommandArguments(\"format\", \"{{range(.NetworkSettings.Networks)}}\"\n         + \"{{.IPAddress}},{{end}}{{.Config.Hostname}}\");\n+    this.commandArguments = \"--format={{range(.NetworkSettings.Networks)}}\"\n+        + \"{{.IPAddress}},{{end}}{{.Config.Hostname}}\";\n     return this;\n   }\n+  @Override\n+  public PrivilegedOperation preparePrivilegedOperation(\n+      DockerCommand dockerCommand, String containerName, Map<String,\n+      String> env, Configuration conf, Context nmContext) {\n+    PrivilegedOperation dockerOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.INSPECT_DOCKER_CONTAINER);\n+    dockerOp.appendArgs(commandArguments, containerName);\n+    return dockerOp;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java",
                "sha": "3ed9c185243aac9f06ace74d423de29f84f6ec90",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java",
                "patch": "@@ -16,6 +16,12 @@\n  */\n package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker;\n \n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+\n+import java.util.Map;\n+\n /**\n  * Encapsulates the docker rm command and its command\n  * line arguments.\n@@ -27,4 +33,14 @@ public DockerRmCommand(String containerName) {\n     super(RM_COMMAND);\n     super.addCommandArguments(\"name\", containerName);\n   }\n+\n+  @Override\n+  public PrivilegedOperation preparePrivilegedOperation(\n+      DockerCommand dockerCommand, String containerName, Map<String,\n+      String> env, Configuration conf, Context nmContext) {\n+    PrivilegedOperation dockerOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.REMOVE_DOCKER_CONTAINER);\n+    dockerOp.appendArgs(containerName);\n+    return dockerOp;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java",
                "sha": "3a02982d0d02b6192cad121d6a0dded84e90726f",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "patch": "@@ -1332,6 +1332,34 @@ int run_docker(const char *command_file) {\n   return exit_code;\n }\n \n+int exec_docker_command(char *docker_command, char **argv,\n+    int argc, int optind) {\n+  int i;\n+  char* docker_binary = get_docker_binary(&CFG);\n+  size_t command_size = argc - optind + 2;\n+\n+  char **args = alloc_and_clear_memory(command_size + 1, sizeof(char));\n+  args[0] = docker_binary;\n+  args[1] = docker_command;\n+  for(i = 2; i < command_size; i++) {\n+    args[i] = (char *) argv[i];\n+  }\n+  args[i] = NULL;\n+\n+  execvp(docker_binary, args);\n+\n+  // will only get here if execvp fails\n+  fprintf(ERRORFILE, \"Couldn't execute the container launch with args %s - %s\\n\",\n+      docker_binary, strerror(errno));\n+  fflush(LOGFILE);\n+  fflush(ERRORFILE);\n+\n+  free(docker_binary);\n+  free(args);\n+\n+  return DOCKER_RUN_FAILED;\n+}\n+\n int create_script_paths(const char *work_dir,\n   const char *script_name, const char *cred_file,\n   char** script_file_dest, char** cred_file_dest,",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "sha": "6b4ec0c8c02fa7e40b29a1716fd69b46a09da655",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "patch": "@@ -47,7 +47,9 @@ enum operations {\n   RUN_AS_USER_DELETE = 9,\n   RUN_AS_USER_LAUNCH_DOCKER_CONTAINER = 10,\n   RUN_DOCKER = 11,\n-  RUN_AS_USER_LIST = 12\n+  RUN_AS_USER_LIST = 12,\n+  REMOVE_DOCKER_CONTAINER = 13,\n+  INSPECT_DOCKER_CONTAINER = 14\n };\n \n #define NM_GROUP_KEY \"yarn.nodemanager.linux-container-executor.group\"\n@@ -263,6 +265,12 @@ int is_docker_support_enabled();\n  */\n int run_docker(const char *command_file);\n \n+/**\n+ * Run a docker command without a command file\n+ */\n+int exec_docker_command(char *docker_command, char **argv,\n+    int argc, int optind);\n+\n /*\n  * Compile the regex_str and determine if the input string matches.\n  * Return 0 on match, 1 of non-match.",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "sha": "47c422191e2787a43ae9c2f9c3f40cdfdbf98e8a",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "patch": "@@ -36,7 +36,7 @@ static void display_usage(FILE *stream) {\n   fprintf(stream,\n     \"Usage: container-executor --checksetup\\n\"\n     \"       container-executor --mount-cgroups <hierarchy> \"\n-    \"<controller=path>...\\n\" );\n+    \"<controller=path>\\n\" );\n \n   if(is_tc_support_enabled()) {\n     fprintf(stream,\n@@ -52,10 +52,15 @@ static void display_usage(FILE *stream) {\n \n   if(is_docker_support_enabled()) {\n     fprintf(stream,\n-      \"       container-executor --run-docker <command-file>\\n\");\n+      \"       container-executor --run-docker <command-file>\\n\"\n+      \"       container-executor --remove-docker-container <container_id>\\n\"\n+      \"       container-executor --inspect-docker-container <container_id>\\n\");\n   } else {\n     fprintf(stream,\n-      \"[DISABLED] container-executor --run-docker <command-file>\\n\");\n+      \"[DISABLED] container-executor --run-docker <command-file>\\n\"\n+      \"[DISABLED] container-executor --remove-docker-container <container_id>\\n\"\n+      \"[DISABLED] container-executor --inspect-docker-container \"\n+      \"<format> ... <container_id>\\n\");\n   }\n \n   fprintf(stream,\n@@ -331,6 +336,36 @@ static int validate_arguments(int argc, char **argv , int *operation) {\n     }\n   }\n \n+  if (strcmp(\"--remove-docker-container\", argv[1]) == 0) {\n+    if(is_docker_support_enabled()) {\n+      if (argc != 3) {\n+        display_usage(stdout);\n+        return INVALID_ARGUMENT_NUMBER;\n+      }\n+      optind++;\n+      *operation = REMOVE_DOCKER_CONTAINER;\n+      return 0;\n+    } else {\n+        display_feature_disabled_message(\"docker\");\n+        return FEATURE_DISABLED;\n+    }\n+  }\n+\n+  if (strcmp(\"--inspect-docker-container\", argv[1]) == 0) {\n+    if(is_docker_support_enabled()) {\n+      if (argc != 4) {\n+        display_usage(stdout);\n+        return INVALID_ARGUMENT_NUMBER;\n+      }\n+      optind++;\n+      *operation = INSPECT_DOCKER_CONTAINER;\n+      return 0;\n+    } else {\n+        display_feature_disabled_message(\"docker\");\n+        return FEATURE_DISABLED;\n+    }\n+  }\n+\n   /* Now we have to validate 'run as user' operations that don't use\n     a 'long option' - we should fix this at some point. The validation/argument\n     parsing here is extensive enough that it done in a separate function */\n@@ -561,6 +596,12 @@ int main(int argc, char **argv) {\n   case RUN_DOCKER:\n     exit_code = run_docker(cmd_input.docker_command_file);\n     break;\n+  case REMOVE_DOCKER_CONTAINER:\n+    exit_code = exec_docker_command(\"rm\", argv, argc, optind);\n+    break;\n+  case INSPECT_DOCKER_CONTAINER:\n+    exit_code = exec_docker_command(\"inspect\", argv, argc, optind);\n+    break;\n   case RUN_AS_USER_INITIALIZE_CONTAINER:\n     exit_code = set_user(cmd_input.run_as_user_name);\n     if (exit_code != 0) {",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "sha": "c54fd3ea900f5e7a397c948c2bb90b08eee5b49d",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 16,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java",
                "patch": "@@ -153,14 +153,14 @@ public void testExecuteDockerRm() throws Exception {\n         env, configuration, mockExecutor, false, nmContext);\n     List<PrivilegedOperation> ops = MockPrivilegedOperationCaptor\n         .capturePrivilegedOperations(mockExecutor, 1, true);\n-    List<String> dockerCommands = getValidatedDockerCommands(ops);\n+    PrivilegedOperation privOp = ops.get(0);\n+    List<String> args = privOp.getArguments();\n     assertEquals(1, ops.size());\n-    assertEquals(PrivilegedOperation.OperationType.RUN_DOCKER_CMD.name(),\n-        ops.get(0).getOperationType().name());\n-    assertEquals(3, dockerCommands.size());\n-    assertEquals(\"[docker-command-execution]\", dockerCommands.get(0));\n-    assertEquals(\"  docker-command=rm\", dockerCommands.get(1));\n-    assertEquals(\"  name=\" + MOCK_CONTAINER_ID, dockerCommands.get(2));\n+    assertEquals(PrivilegedOperation.OperationType.\n+        REMOVE_DOCKER_CONTAINER.name(),\n+        privOp.getOperationType().name());\n+    assertEquals(1, args.size());\n+    assertEquals(MOCK_CONTAINER_ID, args.get(0));\n   }\n \n   @Test\n@@ -188,16 +188,15 @@ public void testExecuteDockerInspectStatus() throws Exception {\n         env, configuration, mockExecutor, false, nmContext);\n     List<PrivilegedOperation> ops = MockPrivilegedOperationCaptor\n         .capturePrivilegedOperations(mockExecutor, 1, true);\n-    List<String> dockerCommands = getValidatedDockerCommands(ops);\n+    PrivilegedOperation privOp = ops.get(0);\n+    List<String> args = privOp.getArguments();\n     assertEquals(1, ops.size());\n-    assertEquals(PrivilegedOperation.OperationType.RUN_DOCKER_CMD.name(),\n-        ops.get(0).getOperationType().name());\n-    assertEquals(4, dockerCommands.size());\n-    assertEquals(\"[docker-command-execution]\", dockerCommands.get(0));\n-    assertEquals(\"  docker-command=inspect\", dockerCommands.get(1));\n-    assertEquals(\"  format={{.State.Status}}\", dockerCommands.get(2));\n-    assertEquals(\"  name=\" + MOCK_CONTAINER_ID, dockerCommands.get(3));\n-\n+    assertEquals(PrivilegedOperation.OperationType.\n+        INSPECT_DOCKER_CONTAINER.name(),\n+        privOp.getOperationType().name());\n+    assertEquals(2, args.size());\n+    assertEquals(\"--format={{.State.Status}}\", args.get(0));\n+    assertEquals(MOCK_CONTAINER_ID, args.get(1));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java",
                "sha": "50d00bb518c107ca3ff21148e8059313f87141cb",
                "status": "modified"
            }
        ],
        "message": "YARN-8209.  Fixed NPE in Yarn Service deletion.\n            Contributed by Eric Badger",
        "parent": "https://github.com/apache/hadoop/commit/19ae588fde9930c042cdb2848b8a1a0ff514b575",
        "patched_files": [
            "DockerClient.java",
            "DockerCommandExecutor.java",
            "container-executor.java",
            "DockerCommand.java",
            "DockerRmCommand.java",
            "DockerInspectCommand.java",
            "main.java",
            "DockerLinuxContainerRuntime.java",
            "PrivilegedOperation.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDockerCommandExecutor.java",
            "TestDockerRmCommand.java",
            "TestDockerInspectCommand.java"
        ]
    },
    "hadoop_8854c21": {
        "bug_id": "hadoop_8854c21",
        "commit": "https://github.com/apache/hadoop/commit/8854c210bdf8aba763b2a0f0327729f315a066d5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=8854c210bdf8aba763b2a0f0327729f315a066d5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -3114,6 +3114,9 @@ Release 0.23.9 - UNRELEASED\n \n   BUG FIXES\n \n+    HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue.\n+    (Plamen Jeliazkov and Ravi Prakash via shv)\n+\n Release 0.23.8 - 2013-06-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "9bc76f3d5078b72e219dfc0f9dcc468276a052a6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=8854c210bdf8aba763b2a0f0327729f315a066d5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -458,7 +458,8 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n-      String fileName = ((BlockInfo)block).getBlockCollection().getName();\n+      BlockCollection bc = ((BlockInfo) block).getBlockCollection();\n+      String fileName = (bc == null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: == live:, d: == decommissioned c: == corrupt e: == excess",
                "raw_url": "https://github.com/apache/hadoop/raw/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "966c59656742edeb7431b031687a716c42c8f8b7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java?ref=8854c210bdf8aba763b2a0f0327729f315a066d5",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "patch": "@@ -24,11 +24,8 @@\n import java.io.FileInputStream;\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.util.Random;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeys;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSConfigKeys;",
                "raw_url": "https://github.com/apache/hadoop/raw/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "sha": "c0775a62504d43d5526e674c082c2017edb8f8b0",
                "status": "modified"
            }
        ],
        "message": "HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue. Contributed by Plamen Jeliazkov and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490433 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/d46e1608626c64400d2b6c7693a4c035783c55b4",
        "patched_files": [
            "BlockManager.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestMetaSave.java"
        ]
    },
    "hadoop_8a6e354": {
        "bug_id": "hadoop_8a6e354",
        "commit": "https://github.com/apache/hadoop/commit/8a6e3541226fb1b6798cedecc56f1f160012becf",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java?ref=8a6e3541226fb1b6798cedecc56f1f160012becf",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java",
                "patch": "@@ -104,6 +104,7 @@ public DiskBalancer(String dataNodeUUID,\n     scheduler = Executors.newSingleThreadExecutor();\n     lock = new ReentrantLock();\n     workMap = new ConcurrentHashMap<>();\n+    this.planID = \"\";  // to keep protobuf happy.\n     this.isDiskBalancerEnabled = conf.getBoolean(\n         DFSConfigKeys.DFS_DISK_BALANCER_ENABLED,\n         DFSConfigKeys.DFS_DISK_BALANCER_ENABLED_DEFAULT);\n@@ -223,7 +224,9 @@ public void cancelPlan(String planID) throws DiskBalancerException {\n     lock.lock();\n     try {\n       checkDiskBalancerEnabled();\n-      if ((this.planID == null) || (!this.planID.equals(planID))) {\n+      if (this.planID == null ||\n+          !this.planID.equals(planID) ||\n+          this.planID.isEmpty()) {\n         LOG.error(\"Disk Balancer - No such plan. Cancel plan failed. PlanID: \" +\n             planID);\n         throw new DiskBalancerException(\"No such plan.\",",
                "raw_url": "https://github.com/apache/hadoop/raw/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java",
                "sha": "5a1fb9ec40877bec539bf1ae2f2260bc3db03e73",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java?ref=8a6e3541226fb1b6798cedecc56f1f160012becf",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java",
                "patch": "@@ -63,7 +63,7 @@ public void execute(CommandLine cmd) throws Exception {\n     String nodeAddress = nodeName;\n \n     // if the string is not name:port format use the default port.\n-    if (!nodeName.matches(\"^.*:\\\\d$\")) {\n+    if (!nodeName.matches(\"[^\\\\:]+:[0-9]{2,5}\")) {\n       int defaultIPC = NetUtils.createSocketAddr(\n           getConf().getTrimmed(DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_KEY,\n               DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_DEFAULT)).getPort();",
                "raw_url": "https://github.com/apache/hadoop/raw/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java",
                "sha": "fac1e51bfa9886ea8d4dbc1f94a941c82cd6be23",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java?ref=8a6e3541226fb1b6798cedecc56f1f160012becf",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.diskbalancer.connectors.ClusterConnector;\n import org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory;\n import org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster;\n@@ -351,4 +352,28 @@ public void testHelpCommand() throws Exception {\n     }\n     return outputs;\n   }\n+\n+  /**\n+   * Making sure that we can query the node without having done a submit.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiskBalancerQueryWithoutSubmit() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    conf.setBoolean(DFSConfigKeys.DFS_DISK_BALANCER_ENABLED, true);\n+    final int numDatanodes = 2;\n+    MiniDFSCluster miniDFSCluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(numDatanodes).build();\n+    try {\n+      miniDFSCluster.waitActive();\n+      DataNode dataNode = miniDFSCluster.getDataNodes().get(0);\n+      final String queryArg = String.format(\"-query localhost:%d\", dataNode\n+          .getIpcPort());\n+      final String cmdLine = String.format(\"hdfs diskbalancer %s\",\n+          queryArg);\n+      runCommand(cmdLine);\n+    } finally {\n+      miniDFSCluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java",
                "sha": "b0821e2e8d22bc3d26deeb7e045bd9be42155cab",
                "status": "modified"
            }
        ],
        "message": "HDFS-10552. DiskBalancer \"-query\" results in NPE if no plan for the node. Contributed by Anu Engineer.",
        "parent": "https://github.com/apache/hadoop/commit/e8de28181a3ed0053d5cd5f196434739880ee978",
        "patched_files": [
            "QueryCommand.java",
            "DiskBalancer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDiskBalancerCommand.java",
            "TestDiskBalancer.java"
        ]
    },
    "hadoop_8c7f6b2": {
        "bug_id": "hadoop_8c7f6b2",
        "commit": "https://github.com/apache/hadoop/commit/8c7f6b2d4df2e5ca7b766db68213b778d28f198b",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/8c7f6b2d4df2e5ca7b766db68213b778d28f198b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java?ref=8c7f6b2d4df2e5ca7b766db68213b778d28f198b",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "patch": "@@ -350,8 +350,10 @@ static void handleComponentInstanceRelaunch(ComponentInstance compInstance,\n         // record in ATS\n         LOG.info(\"Publishing component instance status {} {} \",\n             event.getContainerId(), containerState);\n+        int exitStatus = failureBeforeLaunch || event.getStatus() == null ?\n+            ContainerExitStatus.INVALID : event.getStatus().getExitStatus();\n         compInstance.serviceTimelinePublisher.componentInstanceFinished(\n-            event.getContainerId(), event.getStatus().getExitStatus(),\n+            event.getContainerId(), exitStatus,\n             containerState, containerDiag);\n       }\n \n@@ -366,8 +368,10 @@ static void handleComponentInstanceRelaunch(ComponentInstance compInstance,\n \n       if (compInstance.timelineServiceEnabled) {\n         // record in ATS\n+        int exitStatus = failureBeforeLaunch || event.getStatus() == null ?\n+            ContainerExitStatus.INVALID : event.getStatus().getExitStatus();\n         compInstance.serviceTimelinePublisher.componentInstanceFinished(\n-            event.getContainerId(), event.getStatus().getExitStatus(),\n+            event.getContainerId(), exitStatus,\n             containerState, containerDiag);\n       }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/8c7f6b2d4df2e5ca7b766db68213b778d28f198b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "sha": "66c298d337ca9a8da98370edb2de3c878a1455db",
                "status": "modified"
            }
        ],
        "message": "YARN-9197.  Add safe guard against NPE for component instance failure.\n            Contributed by kyungwan nam",
        "parent": "https://github.com/apache/hadoop/commit/dacc1a759e3ba3eca000cbacc6145b231253b174",
        "patched_files": [
            "ComponentInstance.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestComponentInstance.java"
        ]
    },
    "hadoop_8ed0d4b": {
        "bug_id": "hadoop_8ed0d4b",
        "commit": "https://github.com/apache/hadoop/commit/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -896,6 +896,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-4171. Fix findbugs warnings in YARN-1197 branch. (Wangda Tan via jianhe)\n \n+    YARN-4152. NodeManager crash with NPE when LogAggregationService#stopContainer called for \n+    absent container. (Bibin A Chundatt via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/CHANGES.txt",
                "sha": "0a0a65c69b537072a5f4dd06113f116ae00333ad",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java?ref=8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java",
                "patch": "@@ -56,6 +56,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.LogHandler;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppFinishedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent;\n@@ -423,8 +424,14 @@ private void stopContainer(ContainerId containerId, int exitCode) {\n           + \", did it fail to start?\");\n       return;\n     }\n-    ContainerType containerType = context.getContainers().get(\n-        containerId).getContainerTokenIdentifier().getContainerType();\n+    Container container = context.getContainers().get(containerId);\n+    if (null == container) {\n+      LOG.warn(\"Log aggregation cannot be started for \" + containerId\n+          + \", as its an absent container\");\n+      return;\n+    }\n+    ContainerType containerType =\n+        container.getContainerTokenIdentifier().getContainerType();\n     aggregator.startContainerLogAggregation(\n         new ContainerLogContext(containerId, containerType, exitCode));\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java",
                "sha": "f64685da543a93bc2464f533cdc678e5257f7bcd",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java?ref=8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "patch": "@@ -1509,6 +1509,25 @@ public void testFailedOrKilledContainerPolicy() throws Exception {\n     verifyLogAggFinishEvent(appId);\n   }\n \n+  @Test(timeout = 50000)\n+  public void testLogAggregationAbsentContainer() throws Exception {\n+    ApplicationId appId = createApplication();\n+    LogAggregationService logAggregationService =\n+        createLogAggregationService(appId,\n+            FailedOrKilledContainerLogAggregationPolicy.class, null);\n+    ApplicationAttemptId appAttemptId1 =\n+        BuilderUtils.newApplicationAttemptId(appId, 1);\n+    ContainerId containerId = BuilderUtils.newContainerId(appAttemptId1, 2l);\n+    try {\n+      logAggregationService.handle(new LogHandlerContainerFinishedEvent(\n+          containerId, 100));\n+      assertTrue(\"Should skip when null containerID\", true);\n+    } catch (Exception e) {\n+      Assert.assertFalse(\"Exception not expected should skip null containerid\",\n+          true);\n+    }\n+  }\n+\n   @Test (timeout = 50000)\n   @SuppressWarnings(\"unchecked\")\n   public void testAMOnlyContainerPolicy() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "sha": "0b33634450bcf952c878009ba7ec2329947c3bb8",
                "status": "modified"
            }
        ],
        "message": "YARN-4152. NodeManager crash with NPE when LogAggregationService#stopContainer called for absent container. (Bibin A Chundatt via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/06d1c9033effcd2b1ea54e87229d5478d85732ca",
        "patched_files": [
            "LogAggregationService.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLogAggregationService.java"
        ]
    },
    "hadoop_8f9661d": {
        "bug_id": "hadoop_8f9661d",
        "commit": "https://github.com/apache/hadoop/commit/8f9661da4823bfbb243e430252ec1bb5780ecbfc",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -509,6 +509,10 @@ Release 0.23.0 - Unreleased\n     HADOOP-7360. Preserve relative paths that do not contain globs in FsShell.\n     (Daryn Sharp and Kihwal Lee via szetszwo)\n \n+    HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the\n+    destination directory does not exist.  (John George and Daryn Sharp\n+    via szetszwo)\n+\n   OPTIMIZATIONS\n   \n     HADOOP-7333. Performance improvement in PureJavaCrc32. (Eric Caspole",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "0b0e1beb76248ba3b27554efd8cb2bd9b239c27d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "patch": "@@ -55,6 +55,7 @@\n   protected int exitCode = 0;\n   protected int numErrors = 0;\n   protected boolean recursive = false;\n+  private int depth = 0;\n   protected ArrayList<Exception> exceptions = new ArrayList<Exception>();\n \n   private static final Log LOG = LogFactory.getLog(Command.class);\n@@ -86,6 +87,10 @@ protected boolean isRecursive() {\n     return recursive;\n   }\n \n+  protected int getDepth() {\n+    return depth;\n+  }\n+  \n   /** \n    * Execute the command on the input path\n    * \n@@ -269,6 +274,7 @@ protected void processArgument(PathData item) throws IOException {\n   protected void processPathArgument(PathData item) throws IOException {\n     // null indicates that the call is not via recursion, ie. there is\n     // no parent directory that was expanded\n+    depth = 0;\n     processPaths(null, item);\n   }\n   \n@@ -326,7 +332,12 @@ protected void processPath(PathData item) throws IOException {\n    *  @throws IOException if anything goes wrong...\n    */\n   protected void recursePath(PathData item) throws IOException {\n-    processPaths(item, item.getDirectoryContents());\n+    try {\n+      depth++;\n+      processPaths(item, item.getDirectoryContents());\n+    } finally {\n+      depth--;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "sha": "b24d47e02b1027b5decd17b36d9ab8a71b589875",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 26,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "patch": "@@ -20,13 +20,18 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.util.LinkedList;\n \n+import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathIsDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIsNotDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathNotFoundException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n+import org.apache.hadoop.io.IOUtils;\n \n /**\n  * Provides: argument processing to ensure the destination is valid\n@@ -106,51 +111,136 @@ protected void processArguments(LinkedList<PathData> args)\n   }\n \n   @Override\n-  protected void processPaths(PathData parent, PathData ... items)\n+  protected void processPathArgument(PathData src)\n   throws IOException {\n+    if (src.stat.isDirectory() && src.fs.equals(dst.fs)) {\n+      PathData target = getTargetPath(src);\n+      String srcPath = src.fs.makeQualified(src.path).toString();\n+      String dstPath = dst.fs.makeQualified(target.path).toString();\n+      if (dstPath.equals(srcPath)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"are identical\");\n+        e.setTargetPath(dstPath.toString());\n+        throw e;\n+      }\n+      if (dstPath.startsWith(srcPath+Path.SEPARATOR)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"is a subdirectory of itself\");\n+        e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+    }\n+    super.processPathArgument(src);\n+  }\n+\n+  @Override\n+  protected void processPath(PathData src) throws IOException {\n+    processPath(src, getTargetPath(src));\n+  }\n+  \n+  /**\n+   * Called with a source and target destination pair\n+   * @param src for the operation\n+   * @param target for the operation\n+   * @throws IOException if anything goes wrong\n+   */\n+  protected void processPath(PathData src, PathData dst) throws IOException {\n+    if (src.stat.isSymlink()) {\n+      // TODO: remove when FileContext is supported, this needs to either\n+      // copy the symlink or deref the symlink\n+      throw new PathOperationException(src.toString());        \n+    } else if (src.stat.isFile()) {\n+      copyFileToTarget(src, dst);\n+    } else if (src.stat.isDirectory() && !isRecursive()) {\n+      throw new PathIsDirectoryException(src.toString());\n+    }\n+  }\n+\n+  @Override\n+  protected void recursePath(PathData src) throws IOException {\n     PathData savedDst = dst;\n     try {\n       // modify dst as we descend to append the basename of the\n       // current directory being processed\n-      if (parent != null) dst = dst.getPathDataForChild(parent);\n-      super.processPaths(parent, items);\n+      dst = getTargetPath(src);\n+      if (dst.exists) {\n+        if (!dst.stat.isDirectory()) {\n+          throw new PathIsNotDirectoryException(dst.toString());\n+        }\n+      } else {\n+        if (!dst.fs.mkdirs(dst.path)) {\n+          // too bad we have no clue what failed\n+          PathIOException e = new PathIOException(dst.toString());\n+          e.setOperation(\"mkdir\");\n+          throw e;\n+        }    \n+        dst.refreshStatus(); // need to update stat to know it exists now\n+      }      \n+      super.recursePath(src);\n     } finally {\n       dst = savedDst;\n     }\n   }\n   \n-  @Override\n-  protected void processPath(PathData src) throws IOException {\n+  protected PathData getTargetPath(PathData src) throws IOException {\n     PathData target;\n-    // if the destination is a directory, make target a child path,\n-    // else use the destination as-is\n-    if (dst.exists && dst.stat.isDirectory()) {\n+    // on the first loop, the dst may be directory or a file, so only create\n+    // a child path if dst is a dir; after recursion, it's always a dir\n+    if ((getDepth() > 0) || (dst.exists && dst.stat.isDirectory())) {\n       target = dst.getPathDataForChild(src);\n     } else {\n       target = dst;\n     }\n-    if (target.exists && !overwrite) {\n+    return target;\n+  }\n+  \n+  /**\n+   * Copies the source file to the target.\n+   * @param src item to copy\n+   * @param target where to copy the item\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyFileToTarget(PathData src, PathData target) throws IOException {\n+    copyStreamToTarget(src.fs.open(src.path), target);\n+  }\n+  \n+  /**\n+   * Copies the stream contents to a temporary file.  If the copy is\n+   * successful, the temporary file will be renamed to the real path,\n+   * else the temporary file will be deleted.\n+   * @param in the input stream for the copy\n+   * @param target where to store the contents of the stream\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyStreamToTarget(InputStream in, PathData target)\n+  throws IOException {\n+    if (target.exists && (target.stat.isDirectory() || !overwrite)) {\n       throw new PathExistsException(target.toString());\n     }\n-\n-    try { \n-      // invoke processPath with both a source and resolved target\n-      processPath(src, target);\n-    } catch (PathIOException e) {\n-      // add the target unless it already has one\n-      if (e.getTargetPath() == null) {\n+    PathData tempFile = null;\n+    try {\n+      tempFile = target.createTempFile(target+\"._COPYING_\");\n+      FSDataOutputStream out = target.fs.create(tempFile.path, true);\n+      IOUtils.copyBytes(in, out, getConf(), true);\n+      // the rename method with an option to delete the target is deprecated\n+      if (target.exists && !target.fs.delete(target.path, false)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(target.toString());\n+        e.setOperation(\"delete\");\n+        throw e;\n+      }\n+      if (!tempFile.fs.rename(tempFile.path, target.path)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(tempFile.toString());\n+        e.setOperation(\"rename\");\n         e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+      tempFile = null;\n+    } finally {\n+      if (tempFile != null) {\n+        tempFile.fs.delete(tempFile.path, false);\n       }\n-      throw e;\n     }\n   }\n-\n-  /**\n-   * Called with a source and target destination pair\n-   * @param src for the operation\n-   * @param target for the operation\n-   * @throws IOException if anything goes wrong\n-   */\n-  protected abstract void processPath(PathData src, PathData target)\n-  throws IOException;\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "sha": "6b3b40389f9f72528cdc248ee23af4af559df101",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 73,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "patch": "@@ -26,13 +26,7 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.fs.ChecksumFileSystem;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileUtil;\n-import org.apache.hadoop.fs.LocalFileSystem;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n-import org.apache.hadoop.io.IOUtils;\n \n /** Various commands for copy files */\n @InterfaceAudience.Private\n@@ -95,18 +89,10 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"f\");\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n+      // should have a -r option\n+      setRecursive(true);\n       getRemoteDestination(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      if (!FileUtil.copy(src.fs, src.path, target.fs, target.path, false, overwrite, getConf())) {\n-        // we have no idea what the error is...  FileUtils masks it and in\n-        // some cases won't even report an error\n-        throw new PathIOException(src.toString());\n-      }\n-    }\n   }\n   \n   /** \n@@ -126,7 +112,6 @@ protected void processPath(PathData src, PathData target)\n      * It must be at least three characters long, required by\n      * {@link java.io.File#createTempFile(String, String, File)}.\n      */\n-    private static final String COPYTOLOCAL_PREFIX = \"_copyToLocal_\";\n     private boolean copyCrc;\n     private boolean verifyChecksum;\n \n@@ -144,7 +129,7 @@ protected void processOptions(LinkedList<String> args)\n     }\n \n     @Override\n-    protected void processPath(PathData src, PathData target)\n+    protected void copyFileToTarget(PathData src, PathData target)\n     throws IOException {\n       src.fs.setVerifyChecksum(verifyChecksum);\n \n@@ -153,41 +138,10 @@ protected void processPath(PathData src, PathData target)\n         copyCrc = false;\n       }      \n \n-      if (src.stat.isFile()) {\n-        // copy the file and maybe its crc\n-        copyFileToLocal(src, target);\n-        if (copyCrc) {\n-          copyFileToLocal(src.getChecksumFile(), target.getChecksumFile());\n-        }\n-      } else if (src.stat.isDirectory()) {\n-        // create the remote directory structure locally\n-        if (!target.toFile().mkdirs()) {\n-          throw new PathIOException(target.toString());\n-        }\n-      } else {\n-        throw new PathOperationException(src.toString());\n-      }\n-    }\n-\n-    private void copyFileToLocal(PathData src, PathData target)\n-    throws IOException {\n-      File targetFile = target.toFile();\n-      File tmpFile = FileUtil.createLocalTempFile(\n-          targetFile, COPYTOLOCAL_PREFIX, true);\n-      // too bad we can't tell exactly why it failed...\n-      if (!FileUtil.copy(src.fs, src.path, tmpFile, false, getConf())) {\n-        PathIOException e = new PathIOException(src.toString());\n-        e.setOperation(\"copy\");\n-        e.setTargetPath(tmpFile.toString());\n-        throw e;\n-      }\n-\n-      // too bad we can't tell exactly why it failed...\n-      if (!tmpFile.renameTo(targetFile)) {\n-        PathIOException e = new PathIOException(tmpFile.toString());\n-        e.setOperation(\"rename\");\n-        e.setTargetPath(targetFile.toString());\n-        throw e;\n+      super.copyFileToTarget(src, target);\n+      if (copyCrc) {\n+        // should we delete real file if crc copy fails?\n+        super.copyFileToTarget(src.getChecksumFile(), target.getChecksumFile());\n       }\n     }\n   }\n@@ -208,6 +162,8 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n       getRemoteDestination(args);\n+      // should have a -r option\n+      setRecursive(true);\n     }\n \n     // commands operating on local paths have no need for glob expansion\n@@ -223,30 +179,11 @@ protected void processArguments(LinkedList<PathData> args)\n     throws IOException {\n       // NOTE: this logic should be better, mimics previous implementation\n       if (args.size() == 1 && args.get(0).toString().equals(\"-\")) {\n-        if (dst.exists && !overwrite) {\n-          throw new PathExistsException(dst.toString());\n-        }\n-        copyFromStdin();\n+        copyStreamToTarget(System.in, getTargetPath(args.get(0)));\n         return;\n       }\n       super.processArguments(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      target.fs.copyFromLocalFile(false, overwrite, src.path, target.path);\n-    }\n-\n-    /** Copies from stdin to the destination file. */\n-    protected void copyFromStdin() throws IOException {\n-      FSDataOutputStream out = dst.fs.create(dst.path); \n-      try {\n-        IOUtils.copyBytes(System.in, out, getConf(), false);\n-      } finally {\n-        out.close();\n-      }\n-    }\n   }\n \n   public static class CopyFromLocal extends Put {",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "sha": "066e5fdb899df7f517fb508451caf3a86f7caf68",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "patch": "@@ -182,6 +182,19 @@ public PathData getChecksumFile() throws IOException {\n     return new PathData(srcFs.getRawFileSystem(), srcPath.toString());\n   }\n \n+  /**\n+   * Returns a temporary file for this PathData with the given extension.\n+   * The file will be deleted on exit.\n+   * @param extension for the temporary file\n+   * @return PathData\n+   * @throws IOException shouldn't happen\n+   */\n+  public PathData createTempFile(String extension) throws IOException {\n+    PathData tmpFile = new PathData(fs, uri+\"._COPYING_\");\n+    fs.deleteOnExit(tmpFile.path);\n+    return tmpFile;\n+  }\n+\n   /**\n    * Returns a list of PathData objects of the items contained in the given\n    * directory.",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "sha": "a3c88f1f2af2736442e7db320919298356c681b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the destination directory does not exist.  Contributed by John George and Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195760 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/bb8fd6a2670d00d562673f32a55d3d0dd7aaa69c",
        "patched_files": [
            "PathData.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestPathData.java"
        ]
    },
    "hadoop_902c6ea": {
        "bug_id": "hadoop_902c6ea",
        "commit": "https://github.com/apache/hadoop/commit/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -409,6 +409,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-3082. Non thread safe access to systemCredentials in NodeHeartbeatResponse\n     processing. (Anubhav Dhoot via ozawa)\n \n+    YARN-3088. LinuxContainerExecutor.deleteAsUser can throw NPE if native\n+    executor returns an error (Eric Payne via jlowe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/CHANGES.txt",
                "sha": "872f16e75c2d75120ed5d57f1afdadf3a8a656df",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -392,18 +392,23 @@ public void deleteAsUser(String user, Path dir, Path... baseDirs) {\n     verifyUsernamePattern(user);\n     String runAsUser = getRunAsUser(user);\n \n+    String dirString = dir == null ? \"\" : dir.toUri().getPath();\n+\n     List<String> command = new ArrayList<String>(\n         Arrays.asList(containerExecutorExe,\n                     runAsUser,\n                     user,\n                     Integer.toString(Commands.DELETE_AS_USER.getValue()),\n-                    dir == null ? \"\" : dir.toUri().getPath()));\n+                    dirString));\n+    List<String> pathsToDelete = new ArrayList<String>();\n     if (baseDirs == null || baseDirs.length == 0) {\n       LOG.info(\"Deleting absolute path : \" + dir);\n+      pathsToDelete.add(dirString);\n     } else {\n       for (Path baseDir : baseDirs) {\n         Path del = dir == null ? baseDir : new Path(baseDir, dir);\n         LOG.info(\"Deleting path : \" + del);\n+        pathsToDelete.add(del.toString());\n         command.add(baseDir.toUri().getPath());\n       }\n     }\n@@ -419,7 +424,7 @@ public void deleteAsUser(String user, Path dir, Path... baseDirs) {\n       }\n     } catch (IOException e) {\n       int exitCode = shExec.getExitCode();\n-      LOG.error(\"DeleteAsUser for \" + dir.toUri().getPath()\n+      LOG.error(\"DeleteAsUser for \" + StringUtils.join(\" \", pathsToDelete)\n           + \" returned with exit code: \" + exitCode, e);\n       LOG.error(\"Output from LinuxContainerExecutor's deleteAsUser follows:\");\n       logOutput(shExec.getOutput());",
                "raw_url": "https://github.com/apache/hadoop/raw/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "d6e6894974d6d6fe8dc304d3e4a8ebd96852e318",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop/blob/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -319,10 +319,57 @@ public void testDeleteAsUser() throws IOException {\n     String cmd = String.valueOf(\n         LinuxContainerExecutor.Commands.DELETE_AS_USER.getValue());\n     Path dir = new Path(\"/tmp/testdir\");\n-    \n+    Path testFile = new Path(\"testfile\");\n+    Path baseDir0 = new Path(\"/grid/0/BaseDir\");\n+    Path baseDir1 = new Path(\"/grid/1/BaseDir\");\n+\n+    mockExec.deleteAsUser(appSubmitter, dir);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"/tmp/testdir\"),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\"),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, testFile, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, testFile.toString(), baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n+\n+    File f = new File(\"./src/test/resources/mock-container-executer-with-error\");\n+    if (!FileUtil.canExecute(f)) {\n+      FileUtil.setExecutable(f, true);\n+    }\n+    String executorPath = f.getAbsolutePath();\n+    Configuration conf = new Configuration();\n+    conf.set(YarnConfiguration.NM_LINUX_CONTAINER_EXECUTOR_PATH, executorPath);\n+    mockExec.setConf(conf);\n+\n     mockExec.deleteAsUser(appSubmitter, dir);\n     assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n         appSubmitter, cmd, \"/tmp/testdir\"),\n         readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\"),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, testFile, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, testFile.toString(), baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "98ab8e0631add31b032ed0db78c1a7b8bb212e01",
                "status": "modified"
            }
        ],
        "message": "YARN-3088. LinuxContainerExecutor.deleteAsUser can throw NPE if native executor returns an error. Contributed by Eric Payne",
        "parent": "https://github.com/apache/hadoop/commit/2b0fa20f69417326a92beac10ffa072db2616e73",
        "patched_files": [
            "LinuxContainerExecutor.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java",
            "TestLinuxContainerExecutorWithMocks.java"
        ]
    },
    "hadoop_91357c2": {
        "bug_id": "hadoop_91357c2",
        "commit": "https://github.com/apache/hadoop/commit/91357c22ef0fb89652dd2898cf488b47234452ce",
        "file": [
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/hadoop/blob/91357c22ef0fb89652dd2898cf488b47234452ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/UpgradeComponentsFinder.java",
                "changes": 104,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/UpgradeComponentsFinder.java?ref=91357c22ef0fb89652dd2898cf488b47234452ce",
                "deletions": 48,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/UpgradeComponentsFinder.java",
                "patch": "@@ -100,55 +100,63 @@\n       targetDef.getComponents().forEach(component -> {\n         Component currentComp = currentDef.getComponent(component.getName());\n \n-        if (!Objects.equals(currentComp.getName(), component.getName())) {\n+        if (currentComp != null) {\n+          if (!Objects.equals(currentComp.getName(), component.getName())) {\n+            throw new UnsupportedOperationException(\n+                \"changes to component name not supported by upgrade\");\n+          }\n+\n+          if (!Objects.equals(currentComp.getDependencies(),\n+              component.getDependencies())) {\n+            throw new UnsupportedOperationException(\n+                \"changes to component dependencies not supported by upgrade\");\n+          }\n+\n+          if (!Objects.equals(currentComp.getReadinessCheck(),\n+              component.getReadinessCheck())) {\n+            throw new UnsupportedOperationException(\n+                \"changes to component readiness check not supported by \"\n+                    + \"upgrade\");\n+          }\n+\n+          if (!Objects.equals(currentComp.getResource(),\n+              component.getResource())) {\n+\n+            throw new UnsupportedOperationException(\n+                \"changes to component resource not supported by upgrade\");\n+          }\n+\n+          if (!Objects.equals(currentComp.getRunPrivilegedContainer(),\n+              component.getRunPrivilegedContainer())) {\n+            throw new UnsupportedOperationException(\n+                \"changes to run privileged container not supported by upgrade\");\n+          }\n+\n+          if (!Objects.equals(currentComp.getPlacementPolicy(),\n+              component.getPlacementPolicy())) {\n+            throw new UnsupportedOperationException(\n+                \"changes to component placement policy not supported by \"\n+                    + \"upgrade\");\n+          }\n+\n+          if (!Objects.equals(currentComp.getQuicklinks(),\n+              component.getQuicklinks())) {\n+            throw new UnsupportedOperationException(\n+                \"changes to component quick links not supported by upgrade\");\n+          }\n+\n+          if (!Objects.equals(currentComp.getArtifact(),\n+              component.getArtifact()) || !Objects.equals(\n+              currentComp.getLaunchCommand(), component.getLaunchCommand())\n+              || !Objects.equals(currentComp.getConfiguration(),\n+              component.getConfiguration())) {\n+            targetComps.add(component);\n+          }\n+        } else{\n           throw new UnsupportedOperationException(\n-              \"changes to component name not supported by upgrade\");\n-        }\n-\n-        if (!Objects.equals(currentComp.getDependencies(),\n-            component.getDependencies())) {\n-          throw new UnsupportedOperationException(\n-              \"changes to component dependencies not supported by upgrade\");\n-        }\n-\n-        if (!Objects.equals(currentComp.getReadinessCheck(),\n-            component.getReadinessCheck())) {\n-          throw new UnsupportedOperationException(\n-              \"changes to component readiness check not supported by upgrade\");\n-        }\n-\n-        if (!Objects.equals(currentComp.getResource(),\n-            component.getResource())) {\n-          throw new UnsupportedOperationException(\n-              \"changes to component resource not supported by upgrade\");\n-        }\n-\n-\n-        if (!Objects.equals(currentComp.getRunPrivilegedContainer(),\n-            component.getRunPrivilegedContainer())) {\n-          throw new UnsupportedOperationException(\n-              \"changes to run privileged container not supported by upgrade\");\n-        }\n-\n-        if (!Objects.equals(currentComp.getPlacementPolicy(),\n-            component.getPlacementPolicy())) {\n-          throw new UnsupportedOperationException(\n-              \"changes to component placement policy not supported by upgrade\");\n-        }\n-\n-        if (!Objects.equals(currentComp.getQuicklinks(),\n-            component.getQuicklinks())) {\n-          throw new UnsupportedOperationException(\n-              \"changes to component quick links not supported by upgrade\");\n-        }\n-\n-        if (!Objects.equals(currentComp.getArtifact(),\n-            component.getArtifact()) ||\n-            !Objects.equals(currentComp.getLaunchCommand(),\n-                component.getLaunchCommand()) ||\n-            !Objects.equals(currentComp.getConfiguration(),\n-                component.getConfiguration())) {\n-          targetComps.add(component);\n+              \"addition/deletion of components not supported by upgrade. \"\n+                  + \"Could not find component \" + component.getName() + \" in \"\n+                  + \"current service definition.\");\n         }\n       });\n       return targetComps;",
                "raw_url": "https://github.com/apache/hadoop/raw/91357c22ef0fb89652dd2898cf488b47234452ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/UpgradeComponentsFinder.java",
                "sha": "19ff6db2a4e10125e8a1c2c2b1a3076ca66ad41f",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/91357c22ef0fb89652dd2898cf488b47234452ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestDefaultUpgradeComponentsFinder.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestDefaultUpgradeComponentsFinder.java?ref=91357c22ef0fb89652dd2898cf488b47234452ce",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestDefaultUpgradeComponentsFinder.java",
                "patch": "@@ -23,8 +23,11 @@\n import org.junit.Test;\n \n import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n \n+import static org.junit.Assert.assertEquals;\n+\n /**\n  * Tests for {@link UpgradeComponentsFinder.DefaultUpgradeComponentsFinder}.\n  */\n@@ -40,11 +43,34 @@ public void testServiceArtifactChange() {\n     targetDef.getComponents().forEach(x -> x.setArtifact(\n         TestServiceManager.createTestArtifact(\"v1\")));\n \n-    Assert.assertEquals(\"all components need upgrade\",\n+    assertEquals(\"all components need upgrade\",\n         targetDef.getComponents(), finder.findTargetComponentSpecs(currentDef,\n             targetDef));\n   }\n \n+  @Test\n+  public void testServiceUpgradeWithNewComponentAddition() {\n+    Service currentDef = ServiceTestUtils.createExampleApplication();\n+    Service targetDef = ServiceTestUtils.createExampleApplication();\n+    Iterator<Component> targetComponentsIter =\n+        targetDef.getComponents().iterator();\n+    Component firstComponent = targetComponentsIter.next();\n+    firstComponent.setName(\"newComponentA\");\n+\n+    try {\n+      finder.findTargetComponentSpecs(currentDef, targetDef);\n+      Assert.fail(\"Expected error since component does not exist in service \"\n+          + \"definition\");\n+    } catch (UnsupportedOperationException usoe) {\n+      assertEquals(\n+          \"addition/deletion of components not supported by upgrade. Could \"\n+              + \"not find component newComponentA in current service \"\n+              + \"definition.\",\n+          usoe.getMessage());\n+      //Expected\n+    }\n+  }\n+\n   @Test\n   public void testComponentArtifactChange() {\n     Service currentDef = TestServiceManager.createBaseDef(\"test\");\n@@ -56,7 +82,7 @@ public void testComponentArtifactChange() {\n     List<Component> expected = new ArrayList<>();\n     expected.add(targetDef.getComponents().get(0));\n \n-    Assert.assertEquals(\"single components needs upgrade\",\n+    assertEquals(\"single components needs upgrade\",\n         expected, finder.findTargetComponentSpecs(currentDef,\n             targetDef));\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/91357c22ef0fb89652dd2898cf488b47234452ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestDefaultUpgradeComponentsFinder.java",
                "sha": "b0a01b39e26122cda348432100a6bad0594c3e77",
                "status": "modified"
            }
        ],
        "message": "YARN-8300.  Fixed NPE in DefaultUpgradeComponentsFinder.\n            Contributed by Suma Shivaprasad",
        "parent": "https://github.com/apache/hadoop/commit/6b67161ca33881d5f3d7606a57786e9c960759a1",
        "patched_files": [
            "UpgradeComponentsFinder.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDefaultUpgradeComponentsFinder.java"
        ]
    },
    "hadoop_91649c3": {
        "bug_id": "hadoop_91649c3",
        "commit": "https://github.com/apache/hadoop/commit/91649c34aa272f648a03bb31aa6eb464959c7c0f",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/91649c34aa272f648a03bb31aa6eb464959c7c0f/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java?ref=91649c34aa272f648a03bb31aa6eb464959c7c0f",
                "deletions": 2,
                "filename": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "patch": "@@ -528,7 +528,9 @@ boolean processKey(String key) throws IOException {\n         // traverse the parent tree structure of this key until we get the\n         // immediate child of the input directory.\n         Path immediateChildPath = getImmediateChildPath(keyPath.getParent());\n-        addSubDirStatus(immediateChildPath);\n+        if (immediateChildPath != null) {\n+          addSubDirStatus(immediateChildPath);\n+        }\n       }\n       return true;\n     }\n@@ -565,7 +567,7 @@ void addSubDirStatus(Path dirPath) throws FileNotFoundException {\n     Path getImmediateChildPath(Path keyPath) {\n       Path path = keyPath;\n       Path parent = path.getParent();\n-      while (parent != null && !parent.isRoot()) {\n+      while (parent != null) {\n         if (pathToKey(parent).equals(pathToKey(f))) {\n           return path;\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/91649c34aa272f648a03bb31aa6eb464959c7c0f/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "sha": "eb9d100beb21623285a74589dc644dd0390eb94a",
                "status": "modified"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/hadoop/blob/91649c34aa272f648a03bb31aa6eb464959c7c0f/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java?ref=91649c34aa272f648a03bb31aa6eb464959c7c0f",
                "deletions": 0,
                "filename": "hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java",
                "patch": "@@ -47,6 +47,7 @@\n import org.junit.rules.Timeout;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n /**\n@@ -192,6 +193,71 @@ public void testListStatus() throws Exception {\n         3, fileStatuses.length);\n   }\n \n+  /**\n+   * Tests listStatus operation on root directory.\n+   */\n+  @Test\n+  public void testListStatusOnRoot() throws Exception {\n+    Path root = new Path(\"/\");\n+    Path dir1 = new Path(root, \"dir1\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path dir2 = new Path(root, \"dir2\");\n+    fs.mkdirs(dir12);\n+    fs.mkdirs(dir2);\n+\n+    // ListStatus on root should return dir1 (even though /dir1 key does not\n+    // exist) and dir2 only. dir12 is not an immediate child of root and\n+    // hence should not be listed.\n+    FileStatus[] fileStatuses = o3fs.listStatus(root);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that dir12 is not included in the result of the listStatus on root\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertFalse(fileStatus1.equals(dir12.toString()));\n+    assertFalse(fileStatus2.equals(dir12.toString()));\n+  }\n+\n+  /**\n+   * Tests listStatus on a path with subdirs.\n+   */\n+  @Test\n+  public void testListStatusOnSubDirs() throws Exception {\n+    // Create the following key structure\n+    //      /dir1/dir11/dir111\n+    //      /dir1/dir12\n+    //      /dir1/dir12/file121\n+    //      /dir2\n+    // ListStatus on /dir1 should return all its immediated subdirs only\n+    // which are /dir1/dir11 and /dir1/dir12. Super child files/dirs\n+    // (/dir1/dir12/file121 and /dir1/dir11/dir111) should not be returned by\n+    // listStatus.\n+    Path dir1 = new Path(\"/dir1\");\n+    Path dir11 = new Path(dir1, \"dir11\");\n+    Path dir111 = new Path(dir11, \"dir111\");\n+    Path dir12 = new Path(dir1, \"dir12\");\n+    Path file121 = new Path(dir12, \"file121\");\n+    Path dir2 = new Path(\"/dir2\");\n+    fs.mkdirs(dir111);\n+    fs.mkdirs(dir12);\n+    ContractTestUtils.touch(fs, file121);\n+    fs.mkdirs(dir2);\n+\n+    FileStatus[] fileStatuses = o3fs.listStatus(dir1);\n+    assertEquals(\"FileStatus should return only the immediate children\", 2,\n+        fileStatuses.length);\n+\n+    // Verify that the two children of /dir1 returned by listStatus operation\n+    // are /dir1/dir11 and /dir1/dir12.\n+    String fileStatus1 = fileStatuses[0].getPath().toUri().getPath();\n+    String fileStatus2 = fileStatuses[1].getPath().toUri().getPath();\n+    assertTrue(fileStatus1.equals(dir11.toString()) ||\n+        fileStatus1.equals(dir12.toString()));\n+    assertTrue(fileStatus2.equals(dir11.toString()) ||\n+        fileStatus2.equals(dir12.toString()));\n+  }\n+\n   private KeyInfo getKey(Path keyPath, boolean isDirectory)\n       throws IOException, OzoneException {\n     String key = o3fs.pathToKey(keyPath);",
                "raw_url": "https://github.com/apache/hadoop/raw/91649c34aa272f648a03bb31aa6eb464959c7c0f/hadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java",
                "sha": "e1ba2e78632167b766e7088a1c81d1a8c5b4b1e2",
                "status": "modified"
            }
        ],
        "message": "HDDS-1013. NPE while listing directories.",
        "parent": "https://github.com/apache/hadoop/commit/1ab69a9543df555b878951e66e3da13485e7f6d5",
        "patched_files": [
            "OzoneFileSystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOzoneFileSystem.java"
        ]
    },
    "hadoop_92b53c4": {
        "bug_id": "hadoop_92b53c4",
        "commit": "https://github.com/apache/hadoop/commit/92b53c40f070bbfe65c736f6f3eca721b9d227f5",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/92b53c40f070bbfe65c736f6f3eca721b9d227f5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java?ref=92b53c40f070bbfe65c736f6f3eca721b9d227f5",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "patch": "@@ -273,11 +273,19 @@ static DatanodeInfo chooseDatanode(final NameNode namenode,\n       for (String host : StringUtils\n           .getTrimmedStringCollection(excludeDatanodes)) {\n         int idx = host.indexOf(\":\");\n-        if (idx != -1) {          \n-          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n-              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n+        Node excludeNode = null;\n+        if (idx != -1) {\n+          excludeNode = bm.getDatanodeManager().getDatanodeByXferAddr(\n+             host.substring(0, idx), Integer.parseInt(host.substring(idx + 1)));\n         } else {\n-          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n+          excludeNode = bm.getDatanodeManager().getDatanodeByHost(host);\n+        }\n+\n+        if (excludeNode != null) {\n+          excludes.add(excludeNode);\n+        } else {\n+          LOG.debug(\"DataNode {} was requested to be excluded, \"\n+                + \"but it was not found.\", host);\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/92b53c40f070bbfe65c736f6f3eca721b9d227f5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "sha": "1e8d1a5a001371f8f53549a46d870dbe6abb9a00",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/92b53c40f070bbfe65c736f6f3eca721b9d227f5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java?ref=92b53c40f070bbfe65c736f6f3eca721b9d227f5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java",
                "patch": "@@ -239,6 +239,29 @@ public void testExcludeDataNodes() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testExcludeWrongDataNode() throws Exception {\n+    final Configuration conf = WebHdfsTestUtil.createConf();\n+    final String[] racks = {RACK0};\n+    final String[] hosts = {\"DataNode1\"};\n+    final int nDataNodes = hosts.length;\n+\n+    final MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n+        .hosts(hosts).numDataNodes(nDataNodes).racks(racks).build();\n+    try {\n+      cluster.waitActive();\n+      final NameNode namenode = cluster.getNameNode();\n+      NamenodeWebHdfsMethods.chooseDatanode(\n+          namenode, \"/path\", PutOpParam.Op.CREATE, 0,\n+          DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT,\n+          \"DataNode2\", LOCALHOST, null);\n+    } catch (Exception e) {\n+      Assert.fail(\"Failed to exclude DataNode2\" + e.getMessage());\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n+\n   @Test\n   public void testChooseDatanodeBeforeNamesystemInit() throws Exception {\n     NameNode nn = mock(NameNode.class);",
                "raw_url": "https://github.com/apache/hadoop/raw/92b53c40f070bbfe65c736f6f3eca721b9d227f5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java",
                "sha": "e009bc6032ccb6350f20d495f58298b3d75cfc74",
                "status": "modified"
            }
        ],
        "message": "HDFS-14216. NullPointerException happens in NamenodeWebHdfs. Contributed by lujie.",
        "parent": "https://github.com/apache/hadoop/commit/a868f59d523e1391b719507a76c1aa9fd58278b5",
        "patched_files": [
            "NamenodeWebHdfsMethods.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestWebHdfsDataLocality.java"
        ]
    },
    "hadoop_936e0df": {
        "bug_id": "hadoop_936e0df",
        "commit": "https://github.com/apache/hadoop/commit/936e0df0d344f13eea97fe624b154e8356cdea7c",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/936e0df0d344f13eea97fe624b154e8356cdea7c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=936e0df0d344f13eea97fe624b154e8356cdea7c",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -3132,7 +3132,7 @@ public synchronized String getClusterId() {\n   @Override // DataNodeMXBean\n   public String getDiskBalancerStatus() {\n     try {\n-      return this.diskBalancer.queryWorkStatus().toJsonString();\n+      return getDiskBalancer().queryWorkStatus().toJsonString();\n     } catch (IOException ex) {\n       LOG.debug(\"Reading diskbalancer Status failed. ex:{}\", ex);\n       return \"\";\n@@ -3510,7 +3510,7 @@ public void submitDiskBalancerPlan(String planID, long planVersion,\n           DiskBalancerException.Result.DATANODE_STATUS_NOT_REGULAR);\n     }\n \n-    this.diskBalancer.submitPlan(planID, planVersion, planFile, planData,\n+    getDiskBalancer().submitPlan(planID, planVersion, planFile, planData,\n             skipDateCheck);\n   }\n \n@@ -3522,7 +3522,7 @@ public void submitDiskBalancerPlan(String planID, long planVersion,\n   public void cancelDiskBalancePlan(String planID) throws\n       IOException {\n     checkSuperuserPrivilege();\n-    this.diskBalancer.cancelPlan(planID);\n+    getDiskBalancer().cancelPlan(planID);\n   }\n \n   /**\n@@ -3533,7 +3533,7 @@ public void cancelDiskBalancePlan(String planID) throws\n   @Override\n   public DiskBalancerWorkStatus queryDiskBalancerPlan() throws IOException {\n     checkSuperuserPrivilege();\n-    return this.diskBalancer.queryWorkStatus();\n+    return getDiskBalancer().queryWorkStatus();\n   }\n \n   /**\n@@ -3550,9 +3550,9 @@ public String getDiskBalancerSetting(String key) throws IOException {\n     Preconditions.checkNotNull(key);\n     switch (key) {\n     case DiskBalancerConstants.DISKBALANCER_VOLUME_NAME:\n-      return this.diskBalancer.getVolumeNames();\n+      return getDiskBalancer().getVolumeNames();\n     case DiskBalancerConstants.DISKBALANCER_BANDWIDTH :\n-      return Long.toString(this.diskBalancer.getBandwidth());\n+      return Long.toString(getDiskBalancer().getBandwidth());\n     default:\n       LOG.error(\"Disk Balancer - Unknown key in get balancer setting. Key: {}\",\n           key);\n@@ -3606,4 +3606,11 @@ public String getSlowDisks() {\n     }\n     return volumeInfoList;\n   }\n+\n+  private DiskBalancer getDiskBalancer() throws IOException {\n+    if (this.diskBalancer == null) {\n+      throw new IOException(\"DiskBalancer is not initialized\");\n+    }\n+    return this.diskBalancer;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/936e0df0d344f13eea97fe624b154e8356cdea7c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "7df92f6083cc92d6efc5888dea3dd61bbe135160",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/936e0df0d344f13eea97fe624b154e8356cdea7c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancer.java?ref=936e0df0d344f13eea97fe624b154e8356cdea7c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancer.java",
                "patch": "@@ -109,6 +109,11 @@ public void testDiskBalancerNameNodeConnectivity() throws Exception {\n           .getFsVolumeReferences()) {\n         assertEquals(ref.size(), dbDnNode.getVolumeCount());\n       }\n+\n+      // Shutdown the DN first, to verify that calling diskbalancer APIs on\n+      // uninitialized DN doesn't NPE\n+      dnNode.shutdown();\n+      assertEquals(\"\", dnNode.getDiskBalancerStatus());\n     } finally {\n       cluster.shutdown();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/936e0df0d344f13eea97fe624b154e8356cdea7c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/TestDiskBalancer.java",
                "sha": "e7896944f64875ddc68084f7a57ef220bfa7a3c2",
                "status": "modified"
            }
        ],
        "message": "HDFS-13721. NPE in DataNode due to uninitialized DiskBalancer.",
        "parent": "https://github.com/apache/hadoop/commit/ba683204498c97654be4727ab9e128c433a45498",
        "patched_files": [
            "DataNode.java",
            "DiskBalancer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDiskBalancer.java"
        ]
    },
    "hadoop_9478484": {
        "bug_id": "hadoop_9478484",
        "commit": "https://github.com/apache/hadoop/commit/94784848456a92a6502f3a3c0074e44fba4b19c9",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/94784848456a92a6502f3a3c0074e44fba4b19c9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java?ref=94784848456a92a6502f3a3c0074e44fba4b19c9",
                "deletions": 7,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "patch": "@@ -206,11 +206,6 @@ public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {\n     this.backOffResponseTimeThresholds =\n         parseBackOffResponseTimeThreshold(ns, conf, numLevels);\n \n-    // Setup delay timer\n-    Timer timer = new Timer();\n-    DecayTask task = new DecayTask(this, timer);\n-    timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n-\n     // Setup response time metrics\n     responseTimeTotalInCurrWindow = new AtomicLongArray(numLevels);\n     responseTimeCountInCurrWindow = new AtomicLongArray(numLevels);\n@@ -223,6 +218,11 @@ public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {\n     Preconditions.checkArgument(topUsersCount > 0,\n         \"the number of top users for scheduler metrics must be at least 1\");\n \n+    // Setup delay timer\n+    Timer timer = new Timer();\n+    DecayTask task = new DecayTask(this, timer);\n+    timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n+\n     MetricsProxy prox = MetricsProxy.getInstance(ns, numLevels);\n     prox.setDelegate(this);\n     prox.registerMetrics2Source(ns);\n@@ -821,9 +821,10 @@ private void addTopNCallerSummary(MetricsRecordBuilder rb) {\n     final int topCallerCount = 10;\n     TopN topNCallers = getTopCallers(topCallerCount);\n     Map<Object, Integer> decisions = scheduleCacheRef.get();\n-    for (int i=0; i < topNCallers.size(); i++) {\n+    final int actualCallerCount = topNCallers.size();\n+    for (int i = 0; i < actualCallerCount; i++) {\n       NameValuePair entry =  topNCallers.poll();\n-      String topCaller = \"Top.\" + (topCallerCount - i) + \".\" +\n+      String topCaller = \"Top.\" + (actualCallerCount - i) + \".\" +\n           \"Caller(\" + entry.getName() + \")\";\n       String topCallerVolume = topCaller + \".Volume\";\n       String topCallerPriority = topCaller + \".Priority\";",
                "raw_url": "https://github.com/apache/hadoop/raw/94784848456a92a6502f3a3c0074e44fba4b19c9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "sha": "3443d0394ad75a44e8506398762fcb622e727f05",
                "status": "modified"
            }
        ],
        "message": "HADOOP-13159. Fix potential NPE in Metrics2 source for DecayRpcScheduler. Contributed by Xiaoyu Yao.",
        "parent": "https://github.com/apache/hadoop/commit/0c6726e20d9503589b21123f30757ddfbd405dde",
        "patched_files": [
            "DecayRpcScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDecayRpcScheduler.java"
        ]
    },
    "hadoop_951c98f": {
        "bug_id": "hadoop_951c98f",
        "commit": "https://github.com/apache/hadoop/commit/951c98f89059d64fda8456366f680eff4a7a6785",
        "file": [
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/hadoop/blob/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 86,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=951c98f89059d64fda8456366f680eff4a7a6785",
                "deletions": 28,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -2771,18 +2771,28 @@ public ResourceUsage getClusterResourceUsage() {\n         .getContainersToKill().isEmpty()) {\n       list = new ArrayList<>();\n       for (RMContainer rmContainer : csAssignment.getContainersToKill()) {\n-        list.add(getSchedulerContainer(rmContainer, false));\n+        SchedulerContainer schedulerContainer =\n+            getSchedulerContainer(rmContainer, false);\n+        if (schedulerContainer != null) {\n+          list.add(schedulerContainer);\n+        }\n       }\n     }\n \n     if (csAssignment.getExcessReservation() != null) {\n       if (null == list) {\n         list = new ArrayList<>();\n       }\n-      list.add(\n-          getSchedulerContainer(csAssignment.getExcessReservation(), false));\n+      SchedulerContainer schedulerContainer =\n+          getSchedulerContainer(csAssignment.getExcessReservation(), false);\n+      if (schedulerContainer != null) {\n+        list.add(schedulerContainer);\n+      }\n     }\n \n+    if (list != null && list.isEmpty()) {\n+      list = null;\n+    }\n     return list;\n   }\n \n@@ -2867,11 +2877,15 @@ public boolean attemptAllocationOnNode(SchedulerApplicationAttempt appAttempt,\n       ((RMContainerImpl)rmContainer).setAllocationTags(\n           new HashSet<>(schedulingRequest.getAllocationTags()));\n \n-      allocated = new ContainerAllocationProposal<>(\n-          getSchedulerContainer(rmContainer, true),\n-          null, null, NodeType.NODE_LOCAL, NodeType.NODE_LOCAL,\n-          SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n-          resource);\n+      SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n+          schedulerContainer = getSchedulerContainer(rmContainer, true);\n+      if (schedulerContainer == null) {\n+        allocated = null;\n+      } else {\n+        allocated = new ContainerAllocationProposal<>(schedulerContainer,\n+            null, null, NodeType.NODE_LOCAL, NodeType.NODE_LOCAL,\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY, resource);\n+      }\n     }\n \n     if (null != allocated) {\n@@ -2901,33 +2915,49 @@ public boolean attemptAllocationOnNode(SchedulerApplicationAttempt appAttempt,\n           csAssignment.getAssignmentInformation().getAllocationDetails();\n       if (!allocations.isEmpty()) {\n         RMContainer rmContainer = allocations.get(0).rmContainer;\n-        allocated = new ContainerAllocationProposal<>(\n-            getSchedulerContainer(rmContainer, true),\n-            getSchedulerContainersToRelease(csAssignment),\n-            getSchedulerContainer(csAssignment.getFulfilledReservedContainer(),\n-                false), csAssignment.getType(),\n-            csAssignment.getRequestLocalityType(),\n-            csAssignment.getSchedulingMode() != null ?\n-                csAssignment.getSchedulingMode() :\n-                SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n-            csAssignment.getResource());\n+        SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n+            schedulerContainer = getSchedulerContainer(rmContainer, true);\n+        if (schedulerContainer == null) {\n+          allocated = null;\n+          // Decrease unconfirmed resource if app is alive\n+          FiCaSchedulerApp app = getApplicationAttempt(\n+              rmContainer.getApplicationAttemptId());\n+          if (app != null) {\n+            app.decUnconfirmedRes(rmContainer.getAllocatedResource());\n+          }\n+        } else {\n+          allocated = new ContainerAllocationProposal<>(schedulerContainer,\n+              getSchedulerContainersToRelease(csAssignment),\n+              getSchedulerContainer(\n+                  csAssignment.getFulfilledReservedContainer(), false),\n+              csAssignment.getType(), csAssignment.getRequestLocalityType(),\n+              csAssignment.getSchedulingMode() != null ?\n+                  csAssignment.getSchedulingMode() :\n+                  SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n+              csAssignment.getResource());\n+        }\n       }\n \n       // Reserved something\n       List<AssignmentInformation.AssignmentDetails> reservation =\n           csAssignment.getAssignmentInformation().getReservationDetails();\n       if (!reservation.isEmpty()) {\n         RMContainer rmContainer = reservation.get(0).rmContainer;\n-        reserved = new ContainerAllocationProposal<>(\n-            getSchedulerContainer(rmContainer, false),\n-            getSchedulerContainersToRelease(csAssignment),\n-            getSchedulerContainer(csAssignment.getFulfilledReservedContainer(),\n-                false), csAssignment.getType(),\n-            csAssignment.getRequestLocalityType(),\n-            csAssignment.getSchedulingMode() != null ?\n-                csAssignment.getSchedulingMode() :\n-                SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n-            csAssignment.getResource());\n+        SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n+            schedulerContainer = getSchedulerContainer(rmContainer, false);\n+        if (schedulerContainer == null) {\n+          reserved = null;\n+        } else {\n+          reserved = new ContainerAllocationProposal<>(schedulerContainer,\n+              getSchedulerContainersToRelease(csAssignment),\n+              getSchedulerContainer(\n+                  csAssignment.getFulfilledReservedContainer(), false),\n+              csAssignment.getType(), csAssignment.getRequestLocalityType(),\n+              csAssignment.getSchedulingMode() != null ?\n+                  csAssignment.getSchedulingMode() :\n+                  SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n+              csAssignment.getResource());\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "e604b81e7f9495e4513eaa9670188f81803ee055",
                "status": "modified"
            },
            {
                "additions": 83,
                "blob_url": "https://github.com/apache/hadoop/blob/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java?ref=951c98f89059d64fda8456366f680eff4a7a6785",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java",
                "patch": "@@ -56,8 +56,11 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SimpleCandidateNodeSet;\n import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;\n import org.apache.hadoop.yarn.util.resource.Resources;\n import org.junit.Assert;\n@@ -843,6 +846,86 @@ private ResourceCommitRequest createAllocateFromReservedProposal(\n     return new ResourceCommitRequest(allocateProposals, null, null);\n   }\n \n+  @Test(timeout = 30000)\n+  public void testReturnNullWhenGetSchedulerContainer() throws Exception {\n+    // disable async-scheduling for simulating complex scenario\n+    Configuration disableAsyncConf = new Configuration(conf);\n+    disableAsyncConf.setBoolean(\n+        CapacitySchedulerConfiguration.SCHEDULE_ASYNCHRONOUSLY_ENABLE, false);\n+\n+    // init RM & NMs\n+    final MockRM rm = new MockRM(disableAsyncConf);\n+    rm.start();\n+    final MockNM nm1 = rm.registerNode(\"192.168.0.1:1234\", 8 * GB);\n+    final MockNM nm2 = rm.registerNode(\"192.168.0.2:2234\", 8 * GB);\n+    rm.drainEvents();\n+    CapacityScheduler cs = (CapacityScheduler) rm.getRMContext().getScheduler();\n+    SchedulerNode sn1 = cs.getSchedulerNode(nm1.getNodeId());\n+    RMNode rmNode1 = cs.getNode(nm1.getNodeId()).getRMNode();\n+    SchedulerNode sn2 = cs.getSchedulerNode(nm2.getNodeId());\n+\n+    // launch app1-am on nm1\n+    RMApp app1 = rm.submitApp(1 * GB, \"app1\", \"user\", null, false, \"default\",\n+        YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS, null, null, true, true);\n+    MockAM am1 = MockRM.launchAndRegisterAM(app1, rm, nm1);\n+\n+    // app2 asks 1 * 1G container\n+    am1.allocate(ImmutableList.of(ResourceRequest\n+        .newInstance(Priority.newInstance(0), \"*\",\n+            Resources.createResource(1 * GB), 1)), null);\n+    RMContainer amContainer = cs.getRMContainer(\n+        ContainerId.newContainerId(am1.getApplicationAttemptId(), 1));\n+\n+    // spy CapacityScheduler\n+    final CapacityScheduler spyCs = Mockito.spy(cs);\n+    // hook CapacityScheduler#submitResourceCommitRequest\n+    List<CSAssignment> assignmentSnapshots = new ArrayList<>();\n+    Mockito.doAnswer(new Answer<Object>() {\n+      public Boolean answer(InvocationOnMock invocation) throws Exception {\n+        CSAssignment assignment = (CSAssignment) invocation.getArguments()[1];\n+        if (cs.getNode(nm1.getNodeId()) != null) {\n+          // decommission nm1 for first allocation on nm1\n+          cs.getRMContext().getDispatcher().getEventHandler().handle(\n+              new RMNodeEvent(nm1.getNodeId(), RMNodeEventType.DECOMMISSION));\n+          rm.drainEvents();\n+          Assert.assertEquals(NodeState.DECOMMISSIONED, rmNode1.getState());\n+          Assert.assertNull(cs.getNode(nm1.getNodeId()));\n+          assignmentSnapshots.add(assignment);\n+        } else {\n+          // add am container on nm1 to containersToKill\n+          // for second allocation on nm2\n+          assignment.setContainersToKill(ImmutableList.of(amContainer));\n+        }\n+        // check no NPE in actual submit, before YARN-8233 will throw NPE\n+        cs.submitResourceCommitRequest((Resource) invocation.getArguments()[0],\n+            assignment);\n+        return false;\n+      }\n+    }).when(spyCs).submitResourceCommitRequest(Mockito.any(Resource.class),\n+        Mockito.any(CSAssignment.class));\n+\n+    // allocation on nm1, test return null when get scheduler container\n+    CandidateNodeSet<FiCaSchedulerNode> candidateNodeSet =\n+        new SimpleCandidateNodeSet(sn1);\n+    spyCs.allocateContainersToNode(candidateNodeSet, false);\n+    // make sure unconfirmed resource is decreased correctly\n+    Assert.assertTrue(spyCs.getApplicationAttempt(am1.getApplicationAttemptId())\n+        .hasPendingResourceRequest(RMNodeLabelsManager.NO_LABEL,\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY));\n+\n+    // allocation on nm2,\n+    // test return null when get scheduler container to release\n+    candidateNodeSet =\n+        new SimpleCandidateNodeSet(sn2);\n+    spyCs.allocateContainersToNode(candidateNodeSet, false);\n+    // make sure unconfirmed resource is decreased correctly\n+    Assert.assertTrue(spyCs.getApplicationAttempt(am1.getApplicationAttemptId())\n+        .hasPendingResourceRequest(RMNodeLabelsManager.NO_LABEL,\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY));\n+\n+    rm.stop();\n+  }\n+\n   private void keepNMHeartbeat(List<MockNM> mockNMs, int interval) {\n     if (nmHeartbeatThread != null) {\n       nmHeartbeatThread.setShouldStop();",
                "raw_url": "https://github.com/apache/hadoop/raw/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java",
                "sha": "67c504d3df102c7b58ae8b520cd8b42bd627770b",
                "status": "modified"
            }
        ],
        "message": "YARN-8233. NPE in CapacityScheduler#tryCommit when handling allocate/reserve proposal whose allocatedOrReservedContainer is null. Contributed by Tao Yang.",
        "parent": "https://github.com/apache/hadoop/commit/ba1f9d66d94ed0b85084d7c40c09a87478b3a05a",
        "patched_files": [
            "CapacityScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacitySchedulerAsyncScheduling.java",
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_95790bb": {
        "bug_id": "hadoop_95790bb",
        "commit": "https://github.com/apache/hadoop/commit/95790bb7e5f59a53cd54bc4c7c7fd93d17173e55",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/95790bb7e5f59a53cd54bc4c7c7fd93d17173e55/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java?ref=95790bb7e5f59a53cd54bc4c7c7fd93d17173e55",
                "deletions": 0,
                "filename": "hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "patch": "@@ -128,6 +128,9 @@ public static URI getKeyProviderUri(UserGroupInformation ugi,\n \n   public static KeyProvider getKeyProvider(final Configuration conf,\n       final URI serverProviderUri) throws IOException{\n+    if (serverProviderUri == null) {\n+      throw new IOException(\"KMS serverProviderUri is not configured.\");\n+    }\n     return KMSUtil.createKeyProviderFromUri(conf, serverProviderUri);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/95790bb7e5f59a53cd54bc4c7c7fd93d17173e55/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "sha": "6be77709d4bc882163f3dbb53c2c46bc97fa3e4f",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/hadoop/blob/95790bb7e5f59a53cd54bc4c7c7fd93d17173e55/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java?ref=95790bb7e5f59a53cd54bc4c7c7fd93d17173e55",
                "deletions": 0,
                "filename": "hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.client.rpc;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Test class for {@link OzoneKMSUtil}.\n+ * */\n+public class TestOzoneKMSUtil {\n+  private OzoneConfiguration config;\n+\n+  @Before\n+  public void setUp() {\n+    config = new OzoneConfiguration();\n+    config.setBoolean(OzoneConfigKeys.OZONE_SECURITY_ENABLED_KEY, true);\n+  }\n+\n+  @Test\n+  public void getKeyProvider() {\n+    try {\n+      OzoneKMSUtil.getKeyProvider(config, null);\n+      fail(\"Expected IOException.\");\n+    } catch (IOException ioe) {\n+      assertEquals(ioe.getMessage(), \"KMS serverProviderUri is \" +\n+          \"not configured.\");\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/95790bb7e5f59a53cd54bc4c7c7fd93d17173e55/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "sha": "49fb5e335110f74f7204b33121110bb3b4fa88f7",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/95790bb7e5f59a53cd54bc4c7c7fd93d17173e55/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java?ref=95790bb7e5f59a53cd54bc4c7c7fd93d17173e55",
                "deletions": 1,
                "filename": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "patch": "@@ -59,7 +59,13 @@ public URI getKeyProviderUri() throws IOException {\n   @Override\n   public DelegationTokenIssuer[] getAdditionalTokenIssuers()\n       throws IOException {\n-    KeyProvider keyProvider = getKeyProvider();\n+    KeyProvider keyProvider;\n+    try {\n+      keyProvider = getKeyProvider();\n+    } catch (IOException ioe) {\n+      LOG.error(\"Error retrieving KeyProvider.\", ioe);\n+      return null;\n+    }\n     if (keyProvider instanceof DelegationTokenIssuer) {\n       return new DelegationTokenIssuer[]{(DelegationTokenIssuer)keyProvider};\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/95790bb7e5f59a53cd54bc4c7c7fd93d17173e55/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "sha": "983c5a9d46568c4a2fdd43741224339c019af16f",
                "status": "modified"
            }
        ],
        "message": "HDDS-1430. NPE if secure ozone if KMS uri is not defined. Contributed by Ajay Kumar. (#752)",
        "parent": "https://github.com/apache/hadoop/commit/59816dff94435bc9bfbc1e6f7419698fef08f7aa",
        "patched_files": [
            "OzoneFileSystem.java",
            "OzoneKMSUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOzoneKMSUtil.java"
        ]
    },
    "hadoop_95a87ca": {
        "bug_id": "hadoop_95a87ca",
        "commit": "https://github.com/apache/hadoop/commit/95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -434,6 +434,9 @@ Release 2.3.0 - UNRELEASED\n     HADOOP-10093. hadoop-env.cmd sets HADOOP_CLIENT_OPTS with a max heap size\n     that is too small. (Shanyu Zhao via cnauroth)\n \n+    HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows().\n+    (Enis Soztutar via cnauroth)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "4fd41a876c51b5258033fc4e71d5f2656c402207",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java?ref=95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "patch": "@@ -431,6 +431,9 @@ private String validateFiles(String files, Configuration conf)\n     if (!Shell.WINDOWS) {\n       return args;\n     }\n+    if (args == null) {\n+      return null;\n+    }\n     List<String> newArgs = new ArrayList<String>(args.length);\n     for (int i=0; i < args.length; i++) {\n       String prop = null;",
                "raw_url": "https://github.com/apache/hadoop/raw/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "sha": "678185553939200900c73b1ac6952a05c7c07816",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java?ref=95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "patch": "@@ -282,4 +282,12 @@ private void assertDOptionParsing(String[] args,\n       Arrays.toString(remainingArgs) + Arrays.toString(expectedRemainingArgs),\n       expectedRemainingArgs, remainingArgs);\n   }\n+\n+  /** Test passing null as args. Some classes still call\n+   * Tool interface from java passing null.\n+   */\n+  public void testNullArgs() throws IOException {\n+    GenericOptionsParser parser = new GenericOptionsParser(conf, null);\n+    parser.getRemainingArgs();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "sha": "48a419b3a52f7b72889e8820fb2561630d73531d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows(). Contributed by Enis Soztutar.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/fe67e30bc2794e7ff073cf938ee80eba805d1e69",
        "patched_files": [
            "GenericOptionsParser.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestGenericOptionsParser.java"
        ]
    },
    "hadoop_960940e": {
        "bug_id": "hadoop_960940e",
        "commit": "https://github.com/apache/hadoop/commit/960940e0e08f7839775f2d8a352b444d104d36b4",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=960940e0e08f7839775f2d8a352b444d104d36b4",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -790,13 +790,24 @@ protected synchronized int readWithStrategy(ReaderStrategy strategy)\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occurred.\n           reportCheckSumFailure(corruptedBlocks,\n-              currentLocatedBlock.getLocations().length, false);\n+              getCurrentBlockLocationsLength(), false);\n         }\n       }\n     }\n     return -1;\n   }\n \n+  protected int getCurrentBlockLocationsLength() {\n+    int len = 0;\n+    if (currentLocatedBlock == null) {\n+      DFSClient.LOG.info(\"Found null currentLocatedBlock. pos={}, \"\n+          + \"blockEnd={}, fileLength={}\", pos, blockEnd, getFileLength());\n+    } else {\n+      len = currentLocatedBlock.getLocations().length;\n+    }\n+    return len;\n+  }\n+\n   /**\n    * Read the entire buffer.\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "b38e6299030abeb41c6e45780a4e5c97ce52e0aa",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java?ref=960940e0e08f7839775f2d8a352b444d104d36b4",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.hdfs;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.fs.ReadOption;\n import org.apache.hadoop.hdfs.protocol.BlockType;\n@@ -160,7 +161,8 @@ protected ThreadPoolExecutor getStripedReadsThreadPool(){\n    * When seeking into a new block group, create blockReader for each internal\n    * block in the group.\n    */\n-  private synchronized void blockSeekTo(long target) throws IOException {\n+  @VisibleForTesting\n+  synchronized void blockSeekTo(long target) throws IOException {\n     if (target >= getFileLength()) {\n       throw new IOException(\"Attempted to read past end of file\");\n     }\n@@ -400,8 +402,8 @@ protected synchronized int readWithStrategy(ReaderStrategy strategy)\n       } finally {\n         // Check if need to report block replicas corruption either read\n         // was successful or ChecksumException occurred.\n-        reportCheckSumFailure(corruptedBlocks,\n-            currentLocatedBlock.getLocations().length, true);\n+        reportCheckSumFailure(corruptedBlocks, getCurrentBlockLocationsLength(),\n+            true);\n       }\n     }\n     return -1;",
                "raw_url": "https://github.com/apache/hadoop/raw/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
                "sha": "f3b16e09812eb21fed5e148b112421d95ac70ff1",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java?ref=960940e0e08f7839775f2d8a352b444d104d36b4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.io.erasurecode.ErasureCoderOptions;\n import org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory;\n import org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -51,7 +52,12 @@\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY;\n import static org.junit.Assert.assertArrayEquals;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.spy;\n \n public class TestDFSStripedInputStream {\n \n@@ -504,4 +510,23 @@ public void testIdempotentClose() throws Exception {\n       in.close();\n     }\n   }\n+\n+  @Test\n+  public void testReadFailToGetCurrentBlock() throws Exception {\n+    DFSTestUtil.writeFile(cluster.getFileSystem(), filePath, \"test\");\n+    try (DFSStripedInputStream in = (DFSStripedInputStream) fs.getClient()\n+        .open(filePath.toString())) {\n+      final DFSStripedInputStream spy = spy(in);\n+      final String msg = \"Injected exception for testReadNPE\";\n+      doThrow(new IOException(msg)).when(spy).blockSeekTo(anyLong());\n+      assertNull(in.getCurrentBlock());\n+      try {\n+        spy.read();\n+        fail(\"read should have failed\");\n+      } catch (IOException expected) {\n+        LOG.info(\"Exception caught\", expected);\n+        GenericTestUtils.assertExceptionContains(msg, expected);\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "sha": "cdebee0dc8da76b4eec2a8826ea536bde584833a",
                "status": "modified"
            }
        ],
        "message": "HDFS-13539. DFSStripedInputStream NPE when reportCheckSumFailure.",
        "parent": "https://github.com/apache/hadoop/commit/fc5d49c202354c6f39b33ea3f80f38e85794c6b3",
        "patched_files": [
            "DFSInputStream.java",
            "DFSStripedInputStream.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSStripedInputStream.java",
            "TestDFSInputStream.java"
        ]
    },
    "hadoop_9630621": {
        "bug_id": "hadoop_9630621",
        "commit": "https://github.com/apache/hadoop/commit/9630621be76182b26755e732f822f29f72454ed8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java?ref=9630621be76182b26755e732f822f29f72454ed8",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java",
                "patch": "@@ -174,7 +174,8 @@ public void deleteVolume(String volumeName) throws IOException {\n     public boolean hasNext() {\n       if(!currentIterator.hasNext()) {\n         currentIterator = getNextListOfVolumes(\n-            currentValue.getName()).iterator();\n+            currentValue != null ? currentValue.getName() : null)\n+            .iterator();\n       }\n       return currentIterator.hasNext();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/ObjectStore.java",
                "sha": "1ed41f5c8f543dcfa81e5289b79e920cf4fe86c0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneBucket.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneBucket.java?ref=9630621be76182b26755e732f822f29f72454ed8",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneBucket.java",
                "patch": "@@ -277,7 +277,8 @@ public void deleteKey(String key) throws IOException {\n     public boolean hasNext() {\n       if(!currentIterator.hasNext()) {\n         currentIterator = getNextListOfKeys(\n-            currentValue.getName()).iterator();\n+            currentValue != null ? currentValue.getName() : null)\n+            .iterator();\n       }\n       return currentIterator.hasNext();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneBucket.java",
                "sha": "a5dd1d093ffb32dc71d76bbe8c4acac360956e04",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneVolume.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneVolume.java?ref=9630621be76182b26755e732f822f29f72454ed8",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneVolume.java",
                "patch": "@@ -247,7 +247,8 @@ public void deleteBucket(String bucketName) throws IOException {\n     public boolean hasNext() {\n       if(!currentIterator.hasNext()) {\n         currentIterator = getNextListOfBuckets(\n-            currentValue.getName()).iterator();\n+            currentValue != null ? currentValue.getName() : null)\n+            .iterator();\n       }\n       return currentIterator.hasNext();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/ozone/client/OzoneVolume.java",
                "sha": "bf5005766245166d0e02d9fc3b2237577d675e96",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClient.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClient.java?ref=9630621be76182b26755e732f822f29f72454ed8",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClient.java",
                "patch": "@@ -409,7 +409,7 @@ public void testDeleteKey()\n   }\n \n   @Test\n-  public void listVolumeTest() throws IOException, OzoneException {\n+  public void testListVolume() throws IOException, OzoneException {\n     String volBase = \"vol-\" + RandomStringUtils.randomNumeric(3);\n     //Create 10 volume vol-<random>-a-0-<random> to vol-<random>-a-9-<random>\n     String volBaseNameA = volBase + \"-a-\";\n@@ -448,7 +448,7 @@ public void listVolumeTest() throws IOException, OzoneException {\n   }\n \n   @Test\n-  public void listBucketTest()\n+  public void testListBucket()\n       throws IOException, OzoneException {\n     String volumeA = \"vol-a-\" + RandomStringUtils.randomNumeric(5);\n     String volumeB = \"vol-b-\" + RandomStringUtils.randomNumeric(5);\n@@ -522,7 +522,19 @@ public void listBucketTest()\n   }\n \n   @Test\n-  public void listKeyTest()\n+  public void testListBucketsOnEmptyVolume()\n+      throws IOException, OzoneException {\n+    String volume = \"vol-\" + RandomStringUtils.randomNumeric(5);\n+    store.createVolume(volume);\n+    OzoneVolume vol = store.getVolume(volume);\n+    Iterator<OzoneBucket> buckets = vol.listBuckets(\"\");\n+    while(buckets.hasNext()) {\n+      Assert.fail();\n+    }\n+  }\n+\n+  @Test\n+  public void testListKey()\n       throws IOException, OzoneException {\n     String volumeA = \"vol-a-\" + RandomStringUtils.randomNumeric(5);\n     String volumeB = \"vol-b-\" + RandomStringUtils.randomNumeric(5);\n@@ -656,6 +668,21 @@ public void listKeyTest()\n     Assert.assertFalse(volABucketBIter.hasNext());\n   }\n \n+  @Test\n+  public void testListKeyOnEmptyBucket()\n+      throws IOException, OzoneException {\n+    String volume = \"vol-\" + RandomStringUtils.randomNumeric(5);\n+    String bucket = \"buc-\" + RandomStringUtils.randomNumeric(5);\n+    store.createVolume(volume);\n+    OzoneVolume vol = store.getVolume(volume);\n+    vol.createBucket(bucket);\n+    OzoneBucket buc = vol.getBucket(bucket);\n+    Iterator<OzoneKey> keys = buc.listKeys(\"\");\n+    while(keys.hasNext()) {\n+      Assert.fail();\n+    }\n+  }\n+\n   /**\n    * Close OzoneClient and shutdown MiniOzoneCluster.\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/9630621be76182b26755e732f822f29f72454ed8/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneRpcClient.java",
                "sha": "0d8864ede58a350e436154603447758be2304f4e",
                "status": "modified"
            }
        ],
        "message": "HDFS-12610. Ozone: OzoneClient: RpcClient list calls throw NPE when iterating over empty list. Contributed by Nandakumar.",
        "parent": "https://github.com/apache/hadoop/commit/c3ef3810111933e48d4624d145a4f12413af162c",
        "patched_files": [
            "OzoneVolume.java",
            "ObjectStore.java",
            "OzoneBucket.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOzoneRpcClient.java"
        ]
    },
    "hadoop_9678020": {
        "bug_id": "hadoop_9678020",
        "commit": "https://github.com/apache/hadoop/commit/9678020e59cf073f74cce70ac57d1f6869349a36",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=9678020e59cf073f74cce70ac57d1f6869349a36",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -813,6 +813,8 @@ Release 2.3.0 - UNRELEASED\n     HDFS-5074. Allow starting up from an fsimage checkpoint in the middle of a\n     segment. (Todd Lipcon via atm)\n \n+    HDFS-4201. NPE in BPServiceActor#sendHeartBeat. (jxiang via cmccabe)\n+\n Release 2.2.0 - 2013-10-13\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "ee9b964724cead83d407f35b7c57096567db2885",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=9678020e59cf073f74cce70ac57d1f6869349a36",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -274,12 +274,22 @@ DataNode getDataNode() {\n   synchronized void verifyAndSetNamespaceInfo(NamespaceInfo nsInfo) throws IOException {\n     if (this.bpNSInfo == null) {\n       this.bpNSInfo = nsInfo;\n-      \n+      boolean success = false;\n+\n       // Now that we know the namespace ID, etc, we can pass this to the DN.\n       // The DN can now initialize its local storage if we are the\n       // first BP to handshake, etc.\n-      dn.initBlockPool(this);\n-      return;\n+      try {\n+        dn.initBlockPool(this);\n+        success = true;\n+      } finally {\n+        if (!success) {\n+          // The datanode failed to initialize the BP. We need to reset\n+          // the namespace info so that other BPService actors still have\n+          // a chance to set it, and re-initialize the datanode.\n+          this.bpNSInfo = null;\n+        }\n+      }\n     } else {\n       checkNSEquality(bpNSInfo.getBlockPoolID(), nsInfo.getBlockPoolID(),\n           \"Blockpool ID\");",
                "raw_url": "https://github.com/apache/hadoop/raw/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "e646be9a650bd13d88caf1a6c51cbeda26d47173",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop/blob/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java?ref=9678020e59cf073f74cce70ac57d1f6869349a36",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "patch": "@@ -25,7 +25,9 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -294,6 +296,47 @@ public void testPickActiveNameNode() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test datanode block pool initialization error handling.\n+   * Failure in initializing a block pool should not cause NPE.\n+   */\n+  @Test\n+  public void testBPInitErrorHandling() throws Exception {\n+    final DataNode mockDn = Mockito.mock(DataNode.class);\n+    Mockito.doReturn(true).when(mockDn).shouldRun();\n+    Configuration conf = new Configuration();\n+    File dnDataDir = new File(\n+      new File(TEST_BUILD_DATA, \"testBPInitErrorHandling\"), \"data\");\n+    conf.set(DFS_DATANODE_DATA_DIR_KEY, dnDataDir.toURI().toString());\n+    Mockito.doReturn(conf).when(mockDn).getConf();\n+    Mockito.doReturn(new DNConf(conf)).when(mockDn).getDnConf();\n+    Mockito.doReturn(DataNodeMetrics.create(conf, \"fake dn\")).\n+      when(mockDn).getMetrics();\n+    final AtomicInteger count = new AtomicInteger();\n+    Mockito.doAnswer(new Answer<Void>() {\n+      @Override\n+      public Void answer(InvocationOnMock invocation) throws Throwable {\n+        if (count.getAndIncrement() == 0) {\n+          throw new IOException(\"faked initBlockPool exception\");\n+        }\n+        // The initBlockPool is called again. Now mock init is done.\n+        Mockito.doReturn(mockFSDataset).when(mockDn).getFSDataset();\n+        return null;\n+      }\n+    }).when(mockDn).initBlockPool(Mockito.any(BPOfferService.class));\n+    BPOfferService bpos = setupBPOSForNNs(mockDn, mockNN1, mockNN2);\n+    bpos.start();\n+    try {\n+      waitForInitialization(bpos);\n+      List<BPServiceActor> actors = bpos.getBPServiceActors();\n+      assertEquals(1, actors.size());\n+      BPServiceActor actor = actors.get(0);\n+      waitForBlockReport(actor.getNameNodeProxy());\n+    } finally {\n+      bpos.stop();\n+    }\n+  }\n+\n   private void waitForOneToFail(final BPOfferService bpos)\n       throws Exception {\n     GenericTestUtils.waitFor(new Supplier<Boolean>() {\n@@ -311,6 +354,11 @@ public Boolean get() {\n    */\n   private BPOfferService setupBPOSForNNs(\n       DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n+    return setupBPOSForNNs(mockDn, nns);\n+  }\n+\n+  private BPOfferService setupBPOSForNNs(DataNode mockDn,\n+      DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n     // Set up some fake InetAddresses, then override the connectToNN\n     // function to return the corresponding proxies.\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "sha": "c5cb6c77f129e8e7a61507403342d8fab2b89505",
                "status": "modified"
            }
        ],
        "message": "HDFS-4201. NPE in BPServiceActor#sendHeartBeat (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550269 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ed4d318d681eaa59bf4e9048c13175260da5a719",
        "patched_files": [
            "BPOfferService.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBPOfferService.java"
        ]
    },
    "hadoop_9849c8b": {
        "bug_id": "hadoop_9849c8b",
        "commit": "https://github.com/apache/hadoop/commit/9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -926,6 +926,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-4201. AMBlacklist does not work for minicluster. (Jun Gong via zxu)\n \n+    YARN-4230. RM crashes with NPE when increasing container resource if there is no headroom left.\n+    (Meng Ding via jianhe)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881/hadoop-yarn-project/CHANGES.txt",
                "sha": "9dfbc9fd5a1b941de58ed5aca2e8442064be9551",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/IncreaseContainerAllocator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/IncreaseContainerAllocator.java?ref=9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/IncreaseContainerAllocator.java",
                "patch": "@@ -323,8 +323,8 @@ public CSAssignment assignContainers(Resource clusterResource,\n           }\n         }\n \n-        // We already allocated something\n-        if (!assigned.getSkipped()) {\n+        // We may have allocated something\n+        if (assigned != null && !assigned.getSkipped()) {\n           break;\n         }\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/allocator/IncreaseContainerAllocator.java",
                "sha": "16cf6d3259796ccd891ebde8fe6dbbee1db8ca7e",
                "status": "modified"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/hadoop/blob/9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestContainerResizing.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestContainerResizing.java?ref=9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestContainerResizing.java",
                "patch": "@@ -284,6 +284,78 @@ public RMNodeLabelsManager createNodeLabelManager() {\n     rm1.close();\n   }\n \n+  @Test\n+  public void testIncreaseRequestWithNoHeadroomLeft() throws Exception {\n+    /**\n+     * Application has two containers running, try to increase one of them, the\n+     * requested amount exceeds user's headroom for the queue.\n+     */\n+    MockRM rm1 = new MockRM() {\n+      @Override\n+      public RMNodeLabelsManager createNodeLabelManager() {\n+        return mgr;\n+      }\n+    };\n+    rm1.start();\n+    MockNM nm1 = rm1.registerNode(\"h1:1234\", 8 * GB);\n+\n+    // app1 -> a1\n+    RMApp app1 = rm1.submitApp(1 * GB, \"app\", \"user\", null, \"default\");\n+    MockAM am1 = MockRM.launchAndRegisterAM(app1, rm1, nm1);\n+\n+    FiCaSchedulerApp app = getFiCaSchedulerApp(rm1, app1.getApplicationId());\n+\n+    // Allocate 1 container\n+    am1.allocate(\n+        Arrays.asList(ResourceRequest.newInstance(Priority.newInstance(1), \"*\",\n+                Resources.createResource(2 * GB), 1)),\n+        null);\n+    ContainerId containerId2 =\n+        ContainerId.newContainerId(am1.getApplicationAttemptId(), 2);\n+    Assert.assertTrue(rm1.waitForState(nm1, containerId2,\n+            RMContainerState.ALLOCATED, 10 * 1000));\n+    // Acquire them, and NM report RUNNING\n+    am1.allocate(null, null);\n+    sentRMContainerLaunched(rm1, containerId2);\n+\n+    // am1 asks to change container2 from 2GB to 8GB, which will exceed user\n+    // limit\n+    am1.sendContainerResizingRequest(Arrays.asList(\n+            ContainerResourceChangeRequest\n+                .newInstance(containerId2, Resources.createResource(8 * GB))),\n+        null);\n+\n+    checkPendingResource(rm1, \"default\", 6 * GB, null);\n+    Assert.assertEquals(6 * GB,\n+        app.getAppAttemptResourceUsage().getPending().getMemory());\n+\n+    // NM1 do 1 heartbeats\n+    CapacityScheduler cs = (CapacityScheduler) rm1.getResourceScheduler();\n+    RMNode rmNode1 = rm1.getRMContext().getRMNodes().get(nm1.getNodeId());\n+    cs.handle(new NodeUpdateSchedulerEvent(rmNode1));\n+\n+    RMContainer rmContainer1 = app.getLiveContainersMap().get(containerId2);\n+\n+    /* Check reservation statuses */\n+    // Increase request should *NOT* be reserved as it exceeds user limit\n+    Assert.assertFalse(rmContainer1.hasIncreaseReservation());\n+    Assert.assertTrue(app.getReservedContainers().isEmpty());\n+    Assert.assertNull(cs.getNode(nm1.getNodeId()).getReservedContainer());\n+    // Pending resource will not be changed since it's not satisfied\n+    checkPendingResource(rm1, \"default\", 6 * GB, null);\n+    Assert.assertEquals(6 * GB,\n+        app.getAppAttemptResourceUsage().getPending().getMemory());\n+    // Queue/user/application's usage will *NOT* be updated\n+    checkUsedResource(rm1, \"default\", 3 * GB, null);\n+    Assert.assertEquals(3 * GB, ((LeafQueue) cs.getQueue(\"default\"))\n+            .getUser(\"user\").getUsed().getMemory());\n+    Assert.assertEquals(3 * GB,\n+        app.getAppAttemptResourceUsage().getUsed().getMemory());\n+    Assert.assertEquals(0 * GB,\n+        app.getAppAttemptResourceUsage().getReserved().getMemory());\n+    rm1.close();\n+  }\n+\n   @Test\n   public void testExcessiveReservationWhenCancelIncreaseRequest()\n       throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/9849c8b3865c7c9c9be81ae0ef8f29caa1d5f881/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestContainerResizing.java",
                "sha": "672af645577590a6117ffdf2e4c866e9dfaad6d4",
                "status": "modified"
            }
        ],
        "message": "YARN-4230. RM crashes with NPE when increasing container resource if there is no headroom left. Contributed by Meng Ding",
        "parent": "https://github.com/apache/hadoop/commit/e617cf6dd13f2bb5d7cbb15ee2cdb260ecd46cd3",
        "patched_files": [
            "CHANGES.java",
            "IncreaseContainerAllocator.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerResizing.java"
        ]
    },
    "hadoop_990aa34": {
        "bug_id": "hadoop_990aa34",
        "commit": "https://github.com/apache/hadoop/commit/990aa34de23c625163745ebc338483065d955bbe",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/990aa34de23c625163745ebc338483065d955bbe/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java?ref=990aa34de23c625163745ebc338483065d955bbe",
                "deletions": 5,
                "filename": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
                "patch": "@@ -1194,8 +1194,8 @@ private ContainerState checkContainer(ContainerAccessType accessType)\n         container.downloadAttributes(getInstrumentedContext());\n         currentKnownContainerState = ContainerState.Unknown;\n       } catch (StorageException ex) {\n-        if (ex.getErrorCode().equals(\n-            StorageErrorCode.RESOURCE_NOT_FOUND.toString())) {\n+        if (StorageErrorCode.RESOURCE_NOT_FOUND.toString()\n+            .equals(ex.getErrorCode())) {\n           currentKnownContainerState = ContainerState.DoesntExist;\n         } else {\n           throw ex;\n@@ -1596,7 +1596,7 @@ public void storeEmptyFolder(String key, PermissionStatus permissionStatus)\n       if (t != null && t instanceof StorageException) {\n         StorageException se = (StorageException) t;\n         // If we got this exception, the blob should have already been created\n-        if (!se.getErrorCode().equals(\"LeaseIdMissing\")) {\n+        if (!\"LeaseIdMissing\".equals(se.getErrorCode())) {\n           throw new AzureException(e);\n         }\n       } else {\n@@ -2427,7 +2427,7 @@ private void safeDelete(CloudBlobWrapper blob, SelfRenewingLease lease) throws S\n       // 2. It got there after one-or-more retries THEN\n       // we swallow the exception.\n       if (e.getErrorCode() != null &&\n-          e.getErrorCode().equals(\"BlobNotFound\") &&\n+          \"BlobNotFound\".equals(e.getErrorCode()) &&\n           operationContext.getRequestResults().size() > 1 &&\n           operationContext.getRequestResults().get(0).getException() != null) {\n         LOG.debug(\"Swallowing delete exception on retry: {}\", e.getMessage());\n@@ -2478,7 +2478,7 @@ public boolean delete(String key) throws IOException {\n       Throwable t = e.getCause();\n       if(t != null && t instanceof StorageException) {\n         StorageException se = (StorageException) t;\n-        if(se.getErrorCode().equals((\"LeaseIdMissing\"))){\n+        if (\"LeaseIdMissing\".equals(se.getErrorCode())){\n           SelfRenewingLease lease = null;\n           try {\n             lease = acquireLease(key);",
                "raw_url": "https://github.com/apache/hadoop/raw/990aa34de23c625163745ebc338483065d955bbe/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
                "sha": "5fa964afd8fbceb086a5e7738e527cb7f4e6d134",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/990aa34de23c625163745ebc338483065d955bbe/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java?ref=990aa34de23c625163745ebc338483065d955bbe",
                "deletions": 2,
                "filename": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java",
                "patch": "@@ -82,7 +82,7 @@ public SelfRenewingLease(CloudBlobWrapper blobWrapper)\n         // Throw again if we don't want to keep waiting.\n         // We expect it to be that the lease is already present,\n         // or in some cases that the blob does not exist.\n-        if (!e.getErrorCode().equals(\"LeaseAlreadyPresent\")) {\n+        if (!\"LeaseAlreadyPresent\".equals(e.getErrorCode())) {\n           LOG.info(\n             \"Caught exception when trying to get lease on blob \"\n             + blobWrapper.getUri().toString() + \". \" + e.getMessage());\n@@ -119,7 +119,7 @@ public void free() throws StorageException {\n     try {\n       blobWrapper.getBlob().releaseLease(accessCondition);\n     } catch (StorageException e) {\n-      if (e.getErrorCode().equals(\"BlobNotFound\")) {\n+      if (\"BlobNotFound\".equals(e.getErrorCode())) {\n \n         // Don't do anything -- it's okay to free a lease\n         // on a deleted file. The delete freed the lease",
                "raw_url": "https://github.com/apache/hadoop/raw/990aa34de23c625163745ebc338483065d955bbe/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/SelfRenewingLease.java",
                "sha": "00d5e99c20fedd7b3738949bc46e64b1321b8de9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/990aa34de23c625163745ebc338483065d955bbe/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java?ref=990aa34de23c625163745ebc338483065d955bbe",
                "deletions": 3,
                "filename": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java",
                "patch": "@@ -20,9 +20,9 @@\n \n import static org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.KEY_CHECK_BLOCK_MD5;\n import static org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.KEY_STORE_BLOB_MD5;\n+import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n import static org.junit.Assume.assumeNotNull;\n \n@@ -130,8 +130,8 @@ private void testStoreBlobMd5(boolean expectMd5Stored) throws Exception {\n       }\n       StorageException cause = (StorageException)ex.getCause();\n       assertNotNull(cause);\n-      assertTrue(\"Unexpected cause: \" + cause,\n-          cause.getErrorCode().equals(StorageErrorCodeStrings.INVALID_MD5));\n+      assertEquals(\"Unexpected cause: \" + cause,\n+          StorageErrorCodeStrings.INVALID_MD5, cause.getErrorCode());\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/990aa34de23c625163745ebc338483065d955bbe/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestBlobDataValidation.java",
                "sha": "ea17b62c19333511f20618ae68adbed4dba80d55",
                "status": "modified"
            }
        ],
        "message": "HADOOP-14609. NPE in AzureNativeFileSystemStore.checkContainer() if StorageException lacks an error code. Contributed by Steve Loughran",
        "parent": "https://github.com/apache/hadoop/commit/e9d8bdfdf576340196843dae92551cc36a87e95f",
        "patched_files": [
            "SelfRenewingLease.java",
            "AzureNativeFileSystemStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlobDataValidation.java"
        ]
    },
    "hadoop_993a76f": {
        "bug_id": "hadoop_993a76f",
        "commit": "https://github.com/apache/hadoop/commit/993a76f2dd1ca75194b857f01bebba548b963318",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/993a76f2dd1ca75194b857f01bebba548b963318/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt?ref=993a76f2dd1ca75194b857f01bebba548b963318",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "patch": "@@ -207,3 +207,6 @@ Branch-2802 Snapshot (Unreleased)\n \n   HDFS-4616. Update the FilesDeleted metric while deleting file/dir in the\n   current tree.  (Jing Zhao via szetszwo)\n+\n+  HDFS-4627. Fix FSImageFormat#Loader NPE and synchronization issues.\n+  (Jing Zhao via suresh)",
                "raw_url": "https://github.com/apache/hadoop/raw/993a76f2dd1ca75194b857f01bebba548b963318/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-2802.txt",
                "sha": "a02b36088c79dc1b671d5932b7a52c8509c280b0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/993a76f2dd1ca75194b857f01bebba548b963318/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java?ref=993a76f2dd1ca75194b857f01bebba548b963318",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
                "patch": "@@ -561,7 +561,7 @@ INodeWithAdditionalFields loadINode(final byte[] localName, boolean isSnapshotIN\n       \n       // read blocks\n       BlockInfo[] blocks = null;\n-      if (numBlocks > 0) {\n+      if (numBlocks >= 0) {\n         blocks = new BlockInfo[numBlocks];\n         for (int j = 0; j < numBlocks; j++) {\n           blocks[j] = new BlockInfo(replication);\n@@ -660,7 +660,7 @@ private void loadFilesUnderConstruction(DataInputStream in,\n               ((INodeFileWithSnapshot)oldnode).getDiffs());\n         }\n \n-        fsDir.unprotectedReplaceINodeFile(path, oldnode, cons);\n+        fsDir.replaceINodeFile(path, oldnode, cons);\n         namesystem.leaseManager.addLease(cons.getClientName(), path); \n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/993a76f2dd1ca75194b857f01bebba548b963318/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java",
                "sha": "61e0ae4e61d3eed9abb131cf9b9b9a662f1328bf",
                "status": "modified"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/hadoop/blob/993a76f2dd1ca75194b857f01bebba548b963318/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "changes": 69,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java?ref=993a76f2dd1ca75194b857f01bebba548b963318",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.hdfs.server.namenode;\n \n+import static org.junit.Assert.assertEquals;\n+\n import java.io.File;\n import java.io.IOException;\n import java.util.ArrayList;\n@@ -26,13 +28,16 @@\n \n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.hdfs.DFSTestUtil;\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream;\n import org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag;\n+import org.apache.hadoop.hdfs.protocol.HdfsConstants.SafeModeAction;\n import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n import org.apache.hadoop.hdfs.server.namenode.NNStorage.NameNodeFile;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotTestHelper;\n@@ -305,4 +310,68 @@ public void testSaveLoadImageWithAppending() throws Exception {\n     // compare two dumped tree\n     SnapshotTestHelper.compareDumpedTreeInFile(fsnBefore, fsnAfter);\n   }\n+  \n+  /**\n+   * Test the fsimage loading while there is file under construction.\n+   */\n+  @Test (timeout=60000)\n+  public void testLoadImageWithAppending() throws Exception {\n+    Path sub1 = new Path(dir, \"sub1\");\n+    Path sub1file1 = new Path(sub1, \"sub1file1\");\n+    Path sub1file2 = new Path(sub1, \"sub1file2\");\n+    DFSTestUtil.createFile(hdfs, sub1file1, BLOCKSIZE, REPLICATION, seed);\n+    DFSTestUtil.createFile(hdfs, sub1file2, BLOCKSIZE, REPLICATION, seed);\n+    \n+    // 1. create snapshot s0\n+    hdfs.allowSnapshot(dir.toString());\n+    hdfs.createSnapshot(dir, \"s0\");\n+    \n+    // 2. create snapshot s1 before appending sub1file1 finishes\n+    HdfsDataOutputStream out = appendFileWithoutClosing(sub1file1, BLOCKSIZE);\n+    out.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));      \n+    \n+    // save namespace and restart cluster\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+    hdfs.saveNamespace();\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+    \n+    cluster.shutdown();\n+    cluster = new MiniDFSCluster.Builder(conf).format(false)\n+        .numDataNodes(REPLICATION).build();\n+    cluster.waitActive();\n+    fsn = cluster.getNamesystem();\n+    hdfs = cluster.getFileSystem();\n+  }\n+  \n+  /**\n+   * Test fsimage loading when 1) there is an empty file loaded from fsimage,\n+   * and 2) there is later an append operation to be applied from edit log.\n+   */\n+  @Test\n+  public void testLoadImageWithEmptyFile() throws Exception {\n+    // create an empty file\n+    Path file = new Path(dir, \"file\");\n+    FSDataOutputStream out = hdfs.create(file);\n+    out.close();\n+    \n+    // save namespace\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+    hdfs.saveNamespace();\n+    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+    \n+    // append to the empty file\n+    out = hdfs.append(file);\n+    out.write(1);\n+    out.close();\n+    \n+    // restart cluster\n+    cluster.shutdown();\n+    cluster = new MiniDFSCluster.Builder(conf).format(false)\n+        .numDataNodes(REPLICATION).build();\n+    cluster.waitActive();\n+    hdfs = cluster.getFileSystem();\n+    \n+    FileStatus status = hdfs.getFileStatus(file);\n+    assertEquals(1, status.getLen());\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/993a76f2dd1ca75194b857f01bebba548b963318/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithSnapshot.java",
                "sha": "5d22836b6c5fb7903178a31bccf0f442287c0a8d",
                "status": "modified"
            }
        ],
        "message": "HDFS-4627. Fix FSImageFormat#Loader NPE and synchronization issues. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1460389 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ca7c588185cbc982c7929e743535bf4fcab41beb",
        "patched_files": [
            "FSImageFormat.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSImageWithSnapshot.java"
        ]
    },
    "hadoop_99c8c58": {
        "bug_id": "hadoop_99c8c58",
        "commit": "https://github.com/apache/hadoop/commit/99c8c5839b65666e6099116e4d7024e0eb4682b9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=99c8c5839b65666e6099116e4d7024e0eb4682b9",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -933,6 +933,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-12186. ActiveStandbyElector shouldn't call monitorLockNodeAsync\n     multiple times (zhihai xu via vinayakumarb)\n \n+    HADOOP-12117. Potential NPE from Configuration#loadProperty with\n+    allowNullValueProperties set. (zhihai xu via vinayakumarb)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "5d11db9c82b0658666a999c1ed020029e03896d9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java?ref=99c8c5839b65666e6099116e4d7024e0eb4682b9",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -2735,14 +2735,14 @@ private void overlay(Properties to, Properties from) {\n       to.put(entry.getKey(), entry.getValue());\n     }\n   }\n-  \n+\n   private void loadProperty(Properties properties, String name, String attr,\n       String value, boolean finalParameter, String[] source) {\n     if (value != null || allowNullValueProperties) {\n+      if (value == null) {\n+        value = DEFAULT_STRING_CHECK;\n+      }\n       if (!finalParameters.contains(attr)) {\n-        if (value==null && allowNullValueProperties) {\n-          value = DEFAULT_STRING_CHECK;\n-        }\n         properties.setProperty(attr, value);\n         if(source != null) {\n           updatingResource.put(attr, source);",
                "raw_url": "https://github.com/apache/hadoop/raw/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "0b4542966c478799f0fa0097550bcf2c3b470276",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java?ref=99c8c5839b65666e6099116e4d7024e0eb4682b9",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java",
                "patch": "@@ -42,13 +42,15 @@\n \n import junit.framework.TestCase;\n import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n \n import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.conf.Configuration.IntegerRanges;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.net.NetUtils;\n import static org.apache.hadoop.util.PlatformName.IBM_JAVA;\n+\n import org.codehaus.jackson.map.ObjectMapper;\n import org.mockito.Mockito;\n \n@@ -1511,6 +1513,19 @@ public void run() {\n     // it's expected behaviour.\n   }\n \n+  public void testNullValueProperties() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.setAllowNullValueProperties(true);\n+    out = new BufferedWriter(new FileWriter(CONFIG));\n+    startConfig();\n+    appendProperty(\"attr\", \"value\", true);\n+    appendProperty(\"attr\", \"\", true);\n+    endConfig();\n+    Path fileResource = new Path(CONFIG);\n+    conf.addResource(fileResource);\n+    assertEquals(\"value\", conf.get(\"attr\"));\n+  }\n+\n   public static void main(String[] argv) throws Exception {\n     junit.textui.TestRunner.main(new String[]{\n       TestConfiguration.class.getName()",
                "raw_url": "https://github.com/apache/hadoop/raw/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java",
                "sha": "a0397414ce1eaa0cb91bf98230f63a7f5d21e69e",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12117. Potential NPE from Configuration#loadProperty with allowNullValueProperties set. (Contributed by zhihai xu)",
        "parent": "https://github.com/apache/hadoop/commit/af63427c6d7d2fc251eafb1f152b7a90c5bd07e5",
        "patched_files": [
            "CHANGES.java",
            "Configuration.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestConfiguration.java"
        ]
    },
    "hadoop_9a10b4e": {
        "bug_id": "hadoop_9a10b4e",
        "commit": "https://github.com/apache/hadoop/commit/9a10b4e773ac937b59b458343457bbbd686d7f1e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=9a10b4e773ac937b59b458343457bbbd686d7f1e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -335,6 +335,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4128. AM Recovery expects all attempts of a completed task to\n     also be completed. (Bikas Saha via bobby)\n \n+    MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node\n+    updates. (Jason Lowe via sseth)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d0eaa60a7342be1d2e8012c739814d903a2851de",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=9a10b4e773ac937b59b458343457bbbd686d7f1e",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -1118,13 +1118,12 @@ private Resource assignOffSwitchContainers(Resource clusterResource, SchedulerNo\n   boolean canAssign(SchedulerApp application, Priority priority, \n       SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n-    // Reserved... \n-    if (reservedContainer != null) {\n-      return true;\n-    }\n-    \n     // Clearly we need containers for this application...\n     if (type == NodeType.OFF_SWITCH) {\n+      if (reservedContainer != null) {\n+        return true;\n+      }\n+\n       // 'Delay' off-switch\n       ResourceRequest offSwitchRequest = \n           application.getResourceRequest(priority, RMNode.ANY);",
                "raw_url": "https://github.com/apache/hadoop/raw/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "2256799f9b55e4212bc184560789dcf12bcfb178",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop/blob/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java?ref=9a10b4e773ac937b59b458343457bbbd686d7f1e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "patch": "@@ -926,6 +926,103 @@ public void testReservation() throws Exception {\n     assertEquals(4*GB, a.getMetrics().getAllocatedMB());\n   }\n   \n+  @Test\n+  public void testStolenReservedContainer() throws Exception {\n+    // Manipulate queue 'a'\n+    LeafQueue a = stubLeafQueue((LeafQueue)queues.get(A));\n+    //unset maxCapacity\n+    a.setMaxCapacity(1.0f);\n+\n+    // Users\n+    final String user_0 = \"user_0\";\n+    final String user_1 = \"user_1\";\n+\n+    // Submit applications\n+    final ApplicationAttemptId appAttemptId_0 =\n+        TestUtils.getMockApplicationAttemptId(0, 0);\n+    SchedulerApp app_0 =\n+        new SchedulerApp(appAttemptId_0, user_0, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_0, user_0, A);\n+\n+    final ApplicationAttemptId appAttemptId_1 =\n+        TestUtils.getMockApplicationAttemptId(1, 0);\n+    SchedulerApp app_1 =\n+        new SchedulerApp(appAttemptId_1, user_1, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_1, user_1, A);\n+\n+    // Setup some nodes\n+    String host_0 = \"host_0\";\n+    SchedulerNode node_0 = TestUtils.getMockNode(host_0, DEFAULT_RACK, 0, 4*GB);\n+    String host_1 = \"host_1\";\n+    SchedulerNode node_1 = TestUtils.getMockNode(host_1, DEFAULT_RACK, 0, 4*GB);\n+\n+    final int numNodes = 3;\n+    Resource clusterResource = Resources.createResource(numNodes * (4*GB));\n+    when(csContext.getNumClusterNodes()).thenReturn(numNodes);\n+\n+    // Setup resource-requests\n+    Priority priority = TestUtils.createMockPriority(1);\n+    app_0.updateResourceRequests(Collections.singletonList(\n+            TestUtils.createResourceRequest(RMNodeImpl.ANY, 2*GB, 1, priority,\n+                recordFactory)));\n+\n+    // Setup app_1 to request a 4GB container on host_0 and\n+    // another 4GB container anywhere.\n+    ArrayList<ResourceRequest> appRequests_1 =\n+        new ArrayList<ResourceRequest>(4);\n+    appRequests_1.add(TestUtils.createResourceRequest(host_0, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(DEFAULT_RACK, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(RMNodeImpl.ANY, 4*GB, 2,\n+        priority, recordFactory));\n+    app_1.updateResourceRequests(appRequests_1);\n+\n+    // Start testing...\n+\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(2*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+    assertEquals(0*GB, a.getMetrics().getAvailableMB());\n+\n+    // Now, reservation should kick in for app_1\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(6*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(2*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+\n+    // node_1 heartbeats in and gets the DEFAULT_RACK request for app_1\n+    a.assignContainers(clusterResource, node_1);\n+    assertEquals(10*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_1.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(6*GB, a.getMetrics().getAllocatedMB());\n+\n+    // Now free 1 container from app_0 and try to assign to node_0\n+    a.completedContainer(clusterResource, app_0, node_0,\n+        app_0.getLiveContainers().iterator().next(), null, RMContainerEventType.KILL);\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(8*GB, a.getUsedResources().getMemory());\n+    assertEquals(0*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(8*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(8*GB, a.getMetrics().getAllocatedMB());\n+  }\n+\n   @Test\n   public void testReservationExchange() throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "sha": "8be9b20193e1b19eb3e99808b94dc5b15b8325b6",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node updates. (Contributed by Jason Lowe)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1325991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/5a20d446cf2a947b37fd5856a7e1fe6c21547557",
        "patched_files": [
            "LeafQueue.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop_9d382a4": {
        "bug_id": "hadoop_9d382a4",
        "commit": "https://github.com/apache/hadoop/commit/9d382a41743831fbcfecd302ead02095f36b7f59",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9d382a41743831fbcfecd302ead02095f36b7f59/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=9d382a41743831fbcfecd302ead02095f36b7f59",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -744,6 +744,8 @@ Release 2.4.0 - UNRELEASED\n     HDFS-5756. hadoopRzOptionsSetByteBufferPool does not accept NULL argument,\n     contrary to docs. (cmccabe via wang)\n \n+    HDFS-5747. Fix NPEs in BlockManager. (Arpit Agarwal)\n+\n   BREAKDOWN OF HDFS-2832 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4985. Add storage type to the protocol and expose it in block report",
                "raw_url": "https://github.com/apache/hadoop/raw/9d382a41743831fbcfecd302ead02095f36b7f59/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "af95486f1e8220955073a84f69feeca9fced908d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/9d382a41743831fbcfecd302ead02095f36b7f59/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java?ref=9d382a41743831fbcfecd302ead02095f36b7f59",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "patch": "@@ -324,12 +324,14 @@ void addReplicaIfNotPresent(DatanodeStorageInfo storage,\n     Iterator<ReplicaUnderConstruction> it = replicas.iterator();\n     while (it.hasNext()) {\n       ReplicaUnderConstruction r = it.next();\n-      if(r.getExpectedStorageLocation() == storage) {\n+      DatanodeStorageInfo expectedLocation = r.getExpectedStorageLocation();\n+      if(expectedLocation == storage) {\n         // Record the gen stamp from the report\n         r.setGenerationStamp(block.getGenerationStamp());\n         return;\n-      } else if (r.getExpectedStorageLocation().getDatanodeDescriptor() ==\n-          storage.getDatanodeDescriptor()) {\n+      } else if (expectedLocation != null &&\n+                 expectedLocation.getDatanodeDescriptor() ==\n+                     storage.getDatanodeDescriptor()) {\n \n         // The Datanode reported that the block is on a different storage\n         // than the one chosen by BlockPlacementPolicy. This can occur as",
                "raw_url": "https://github.com/apache/hadoop/raw/9d382a41743831fbcfecd302ead02095f36b7f59/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfoUnderConstruction.java",
                "sha": "1161077f49ddffb2f3294549d3f9322cf319f6fb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/9d382a41743831fbcfecd302ead02095f36b7f59/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=9d382a41743831fbcfecd302ead02095f36b7f59",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -547,8 +547,8 @@ private void startCommonServices(Configuration conf) throws IOException {\n   }\n   \n   private void stopCommonServices() {\n-    if(namesystem != null) namesystem.close();\n     if(rpcServer != null) rpcServer.stop();\n+    if(namesystem != null) namesystem.close();\n     if (pauseMonitor != null) pauseMonitor.stop();\n     if (plugins != null) {\n       for (ServicePlugin p : plugins) {",
                "raw_url": "https://github.com/apache/hadoop/raw/9d382a41743831fbcfecd302ead02095f36b7f59/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "eb3755bdc4fda9bebbbf35ea7da30826fec2f3fa",
                "status": "modified"
            }
        ],
        "message": "HDFS-5747. Fix NPEs in BlockManager. (Arpit Agarwal)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1557289 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/6608b75829992e84d265dec84b6cb52f041b454a",
        "patched_files": [
            "BlockInfoUnderConstruction.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockInfoUnderConstruction.java"
        ]
    },
    "hadoop_9e35571": {
        "bug_id": "hadoop_9e35571",
        "commit": "https://github.com/apache/hadoop/commit/9e355719653c5e7b48b601090634882e4f29a743",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=9e355719653c5e7b48b601090634882e4f29a743",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -782,6 +782,8 @@ Release 2.6.0 - UNRELEASED\n     HDFS-7096. Fix TestRpcProgramNfs3 to use DFS_ENCRYPTION_KEY_PROVIDER_URI\n     (clamb via cmccabe)\n \n+    HDFS-7046. HA NN can NPE upon transition to active. (kihwal)\n+\n     BREAKDOWN OF HDFS-6134 AND HADOOP-10150 SUBTASKS AND RELATED JIRAS\n   \n       HDFS-6387. HDFS CLI admin tool for creating & deleting an",
                "raw_url": "https://github.com/apache/hadoop/raw/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "8c45d699c08a6fd5bc2abdea5733dd93667440bc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=9e355719653c5e7b48b601090634882e4f29a743",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1156,8 +1156,9 @@ void startActiveServices() throws IOException {\n       cacheManager.startMonitorThread();\n       blockManager.getDatanodeManager().setShouldSendCachingCommands(true);\n     } finally {\n-      writeUnlock();\n       startingActiveService = false;\n+      checkSafeMode();\n+      writeUnlock();\n     }\n   }\n \n@@ -5570,6 +5571,9 @@ private void checkMode() {\n       // Have to have write-lock since leaving safemode initializes\n       // repl queues, which requires write lock\n       assert hasWriteLock();\n+      if (inTransitionToActive()) {\n+        return;\n+      }\n       // if smmthread is already running, the block threshold must have been \n       // reached before, there is no need to enter the safe mode again\n       if (smmthread == null && needEnter()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "4dc278667d1261e5128d1cc6029d6ab4dc7c05ff",
                "status": "modified"
            }
        ],
        "message": "HDFS-7046. HA NN can NPE upon transition to active. Contributed by\nKihwal Lee.",
        "parent": "https://github.com/apache/hadoop/commit/adf0b67a7104bd457b20c95ff78dd48753dcd699",
        "patched_files": [
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_9ee891a": {
        "bug_id": "hadoop_9ee891a",
        "commit": "https://github.com/apache/hadoop/commit/9ee891aa90333bf18cba412400daa5834f15c41d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=9ee891aa90333bf18cba412400daa5834f15c41d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -777,6 +777,8 @@ Release 2.6.0 - UNRELEASED\n     HADOOP-10925. Compilation fails in native link0 function on Windows.\n     (cnauroth)\n \n+    HADOOP-11077. NPE if hosts not specified in ProxyUsers. (gchanan via tucu)\n+\n Release 2.5.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "b0150873cd83fb2408295ae0fdae1dc07f74ce99",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java?ref=9ee891aa90333bf18cba412400daa5834f15c41d",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "patch": "@@ -123,7 +123,7 @@ public void authorize(UserGroupInformation user,\n     MachineList MachineList = proxyHosts.get(\n         getProxySuperuserIpConfKey(realUser.getShortUserName()));\n \n-    if(!MachineList.includes(remoteAddress)) {\n+    if(MachineList == null || !MachineList.includes(remoteAddress)) {\n       throw new AuthorizationException(\"Unauthorized connection for super-user: \"\n           + realUser.getUserName() + \" from IP \" + remoteAddress);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "sha": "b36ac80717ec567e4579006b5aae08f4ee0b7e2e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java?ref=9ee891aa90333bf18cba412400daa5834f15c41d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "patch": "@@ -478,6 +478,21 @@ public void testProxyUsersWithCustomPrefix() throws Exception {\n     assertNotAuthorized(proxyUserUgi, \"1.2.3.5\");\n   }\n \n+  @Test\n+  public void testNoHostsForUsers() throws Exception {\n+    Configuration conf = new Configuration(false);\n+    conf.set(\"y.\" + REAL_USER_NAME + \".users\",\n+      StringUtils.join(\",\", Arrays.asList(AUTHORIZED_PROXY_USER_NAME)));\n+    ProxyUsers.refreshSuperUserGroupsConfiguration(conf, \"y\");\n+\n+    UserGroupInformation realUserUgi = UserGroupInformation\n+      .createRemoteUser(REAL_USER_NAME);\n+    UserGroupInformation proxyUserUgi = UserGroupInformation.createProxyUserForTesting(\n+      AUTHORIZED_PROXY_USER_NAME, realUserUgi, GROUP_NAMES);\n+\n+    // IP doesn't matter\n+    assertNotAuthorized(proxyUserUgi, \"1.2.3.4\");\n+  }\n \n   private void assertNotAuthorized(UserGroupInformation proxyUgi, String host) {\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "sha": "8ff4bfb10884566dad9e426f3abce3f8d1949a49",
                "status": "modified"
            }
        ],
        "message": "HADOOP-11077. NPE if hosts not specified in ProxyUsers. (gchanan via tucu)",
        "parent": "https://github.com/apache/hadoop/commit/bbff44cb03d0150f990acc3b77170893241cc282",
        "patched_files": [
            "CHANGES.java",
            "ProxyUsers.java",
            "DefaultImpersonationProvider.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestProxyUsers.java",
            "TestDefaultImpersonationProvider.java"
        ]
    },
    "hadoop_9f2b77a": {
        "bug_id": "hadoop_9f2b77a",
        "commit": "https://github.com/apache/hadoop/commit/9f2b77aee496b0636aabafa61f13903f28bd86fe",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -715,6 +715,9 @@ Release 0.23.1 - Unreleased\n \n     MAPREDUCE-3813. Added a cache for resolved racks. (vinodkv via acmurthy)   \n \n+    MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps\n+    but no reduces. (Robert Joseph Evans via vinodkv)\n+\n Release 0.23.0 - 2011-11-01 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "050af54fcebc18df5a1efc456ce94a44380010c8",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 7,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "patch": "@@ -85,18 +85,21 @@ private static Path getOutputPath(TaskAttemptContext context) {\n    */\n   @Private\n   Path getJobAttemptPath(JobContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getJobAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getJobAttemptPath(context, out);\n   }\n \n   @Private\n   Path getTaskAttemptPath(TaskAttemptContext context) throws IOException {\n-    return getTaskAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : getTaskAttemptPath(context, out);\n   }\n \n   private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOException {\n     Path workPath = FileOutputFormat.getWorkOutputPath(context.getJobConf());\n-    if(workPath == null) {\n+    if(workPath == null && out != null) {\n       return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n       .getTaskAttemptPath(context, out);\n     }\n@@ -110,14 +113,17 @@ private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOE\n    * @return the path where the output of a committed task is stored until\n    * the entire job is committed.\n    */\n+  @Private\n   Path getCommittedTaskPath(TaskAttemptContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getCommittedTaskPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getCommittedTaskPath(context, out);\n   }\n \n   public Path getWorkPath(TaskAttemptContext context, Path outputPath) \n   throws IOException {\n-    return getTaskAttemptPath(context, outputPath);\n+    return outputPath == null ? null : getTaskAttemptPath(context, outputPath);\n   }\n   \n   @Override\n@@ -156,6 +162,7 @@ public void abortJob(JobContext context, int runState)\n     getWrapped(context).abortJob(context, state);\n   }\n   \n+  @Override\n   public void setupTask(TaskAttemptContext context) throws IOException {\n     getWrapped(context).setupTask(context);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "sha": "a6190d2060d4c3ee0fc1934f86235ac51366e7ca",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 27,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "patch": "@@ -495,36 +495,40 @@ public boolean isRecoverySupported() {\n   @Override\n   public void recoverTask(TaskAttemptContext context)\n       throws IOException {\n-    context.progress();\n-    TaskAttemptID attemptId = context.getTaskAttemptID();\n-    int previousAttempt = getAppAttemptId(context) - 1;\n-    if (previousAttempt < 0) {\n-      throw new IOException (\"Cannot recover task output for first attempt...\");\n-    }\n-    \n-    Path committedTaskPath = getCommittedTaskPath(context);\n-    Path previousCommittedTaskPath = getCommittedTaskPath(\n-        previousAttempt, context);\n-    FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n-    \n-    LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n-        + \" into \" + committedTaskPath);\n-    if (fs.exists(previousCommittedTaskPath)) {\n-      if(fs.exists(committedTaskPath)) {\n-        if(!fs.delete(committedTaskPath, true)) {\n-          throw new IOException(\"Could not delete \"+committedTaskPath);\n-        }\n+    if(hasOutputPath()) {\n+      context.progress();\n+      TaskAttemptID attemptId = context.getTaskAttemptID();\n+      int previousAttempt = getAppAttemptId(context) - 1;\n+      if (previousAttempt < 0) {\n+        throw new IOException (\"Cannot recover task output for first attempt...\");\n       }\n-      //Rename can fail if the parent directory does not yet exist.\n-      Path committedParent = committedTaskPath.getParent();\n-      fs.mkdirs(committedParent);\n-      if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n-        throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n-            \" to \" + committedTaskPath);\n+\n+      Path committedTaskPath = getCommittedTaskPath(context);\n+      Path previousCommittedTaskPath = getCommittedTaskPath(\n+          previousAttempt, context);\n+      FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n+\n+      LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n+          + \" into \" + committedTaskPath);\n+      if (fs.exists(previousCommittedTaskPath)) {\n+        if(fs.exists(committedTaskPath)) {\n+          if(!fs.delete(committedTaskPath, true)) {\n+            throw new IOException(\"Could not delete \"+committedTaskPath);\n+          }\n+        }\n+        //Rename can fail if the parent directory does not yet exist.\n+        Path committedParent = committedTaskPath.getParent();\n+        fs.mkdirs(committedParent);\n+        if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n+          throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n+              \" to \" + committedTaskPath);\n+        }\n+        LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n+      } else {\n+        LOG.warn(attemptId+\" had no output to recover.\");\n       }\n-      LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n     } else {\n-      LOG.warn(attemptId+\" had no output to recover.\");\n+      LOG.warn(\"Output Path is null in recoverTask()\");\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "sha": "7bad09f303977f66b49e7aad180c0e1d4268703e",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "patch": "@@ -104,7 +104,9 @@ public void testRecovery() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     Path jobTempDir1 = committer.getCommittedTaskPath(tContext);\n     File jtd1 = new File(jobTempDir1.toUri().getPath());\n     assertTrue(jtd1.exists());\n@@ -188,7 +190,9 @@ public void testCommitter() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n@@ -214,14 +218,38 @@ public void testMapFileOutputCommitter() throws Exception {\n     writeMapFileOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n     validateMapFileOutputContent(FileSystem.get(conf), outDir);\n     FileUtil.fullyDelete(new File(outDir.toString()));\n   }\n   \n+  public void testMapOnlyNoOutput() throws Exception {\n+    JobConf conf = new JobConf();\n+    //This is not set on purpose. FileOutputFormat.setOutputPath(conf, outDir);\n+    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\n+    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n+    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n+    FileOutputCommitter committer = new FileOutputCommitter();    \n+    \n+    // setup\n+    committer.setupJob(jContext);\n+    committer.setupTask(tContext);\n+    \n+    if(committer.needsTaskCommit(tContext)) {\n+      // do commit\n+      committer.commitTask(tContext);\n+    }\n+    committer.commitJob(jContext);\n+\n+    // validate output\n+    FileUtil.fullyDelete(new File(outDir.toString()));\n+  }\n+  \n   public void testAbort() throws IOException, InterruptedException {\n     JobConf conf = new JobConf();\n     FileOutputFormat.setOutputPath(conf, outDir);",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "sha": "0859571d1f2bde79064403fd1d5023d368f1826d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps but no reduces. Contributed by Robert Joseph Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241217 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/c6282df3e59eb1e5481158184c344034872d2a89",
        "patched_files": [
            "FileOutputCommitter.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFileOutputCommitter.java"
        ]
    },
    "hadoop_9f6cf60": {
        "bug_id": "hadoop_9f6cf60",
        "commit": "https://github.com/apache/hadoop/commit/9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -55,6 +55,9 @@ Release 2.2.0 - UNRELEASED\n \n   BUG FIXES\n \n+    YARN-1128. FifoPolicy.computeShares throws NPE on empty list of Schedulables\n+    (Karthik Kambatla via Sandy Ryza)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c/hadoop-yarn-project/CHANGES.txt",
                "sha": "4b41f79b6975cd0632254abb65bf1c05f2ca0e3c",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java?ref=9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.Serializable;\n import java.util.Collection;\n import java.util.Comparator;\n+import java.util.Iterator;\n \n import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.classification.InterfaceStability.Unstable;\n@@ -73,6 +74,10 @@ public int compare(Schedulable s1, Schedulable s2) {\n   @Override\n   public void computeShares(Collection<? extends Schedulable> schedulables,\n       Resource totalResources) {\n+    if (schedulables.isEmpty()) {\n+      return;\n+    }\n+\n     Schedulable earliest = null;\n     for (Schedulable schedulable : schedulables) {\n       if (earliest == null ||",
                "raw_url": "https://github.com/apache/hadoop/raw/9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/FifoPolicy.java",
                "sha": "3451cfea4c50b68dab286eaa030dfd95eea9ef57",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hadoop/blob/9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java?ref=9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java",
                "patch": "@@ -0,0 +1,57 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies;\n+\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy;\n+import org.apache.hadoop.yarn.util.resource.Resources;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+\n+public class TestEmptyQueues {\n+  private Collection<? extends Schedulable> schedulables;\n+\n+  @Before\n+  public void setup() {\n+    schedulables = new ArrayList<Schedulable>();\n+  }\n+\n+  private void testComputeShares(SchedulingPolicy policy) {\n+    policy.computeShares(schedulables, Resources.none());\n+  }\n+\n+  @Test (timeout = 1000)\n+  public void testFifoPolicy() {\n+    testComputeShares(SchedulingPolicy.getInstance(FifoPolicy.class));\n+  }\n+\n+  @Test (timeout = 1000)\n+  public void testFairSharePolicy() {\n+    testComputeShares(SchedulingPolicy.getInstance(FairSharePolicy.class));\n+  }\n+\n+  @Test (timeout = 1000)\n+  public void testDRFPolicy() {\n+    testComputeShares(\n+        SchedulingPolicy.getInstance(DominantResourceFairnessPolicy.class));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/9f6cf60ae04360ffa9202d0b34ccc29d41faaa3c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/policies/TestEmptyQueues.java",
                "sha": "4636c5bbd8def9014b3db16d0333a7fc9af98942",
                "status": "added"
            }
        ],
        "message": "YARN-1128. FifoPolicy.computeShares throws NPE on empty list of Schedulables (Karthik Kambatla via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525313 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ab22245dc84a6fd84cf97b65adc92684db5743d9",
        "patched_files": [
            "CHANGES.java",
            "FifoPolicy.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestEmptyQueues.java"
        ]
    },
    "hadoop_a0ad975": {
        "bug_id": "hadoop_a0ad975",
        "commit": "https://github.com/apache/hadoop/commit/a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -348,6 +348,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2456. Possible livelock in CapacityScheduler when RM is recovering apps.\n     (Jian He via xgong)\n \n+    YARN-2542. Fixed NPE when retrieving ApplicationReport from TimeLineServer.\n+    (Zhijie Shen via jianhe)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc/hadoop-yarn-project/CHANGES.txt",
                "sha": "06d94caaf45ceec99880a7c148b7cb6adfb066dc",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java?ref=a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java",
                "patch": "@@ -463,9 +463,15 @@ private void printApplicationReport(String applicationId)\n       appReportStr.println(appReport.getHost());\n       appReportStr.print(\"\\tAggregate Resource Allocation : \");\n \n-      ApplicationResourceUsageReport usageReport = appReport.getApplicationResourceUsageReport();\n-      appReportStr.print(usageReport.getMemorySeconds() + \" MB-seconds, \");\n-      appReportStr.println(usageReport.getVcoreSeconds() + \" vcore-seconds\");\n+      ApplicationResourceUsageReport usageReport =\n+          appReport.getApplicationResourceUsageReport();\n+      if (usageReport != null) {\n+        //completed app report in the timeline server doesn't have usage report\n+        appReportStr.print(usageReport.getMemorySeconds() + \" MB-seconds, \");\n+        appReportStr.println(usageReport.getVcoreSeconds() + \" vcore-seconds\");\n+      } else {\n+        appReportStr.println(\"N/A\");\n+      }\n       appReportStr.print(\"\\tDiagnostics : \");\n       appReportStr.print(appReport.getDiagnostics());\n     } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java",
                "sha": "a847cd5c52876228a4709276e3c5056e426377de",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop/blob/a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java?ref=a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc",
                "deletions": 38,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java",
                "patch": "@@ -86,44 +86,48 @@ public void setup() {\n   \n   @Test\n   public void testGetApplicationReport() throws Exception {\n-    ApplicationCLI cli = createAndGetAppCLI();\n-    ApplicationId applicationId = ApplicationId.newInstance(1234, 5);\n-    ApplicationResourceUsageReport usageReport = \n-        ApplicationResourceUsageReport.newInstance(\n-            2, 0, null, null, null, 123456, 4567);\n-    ApplicationReport newApplicationReport = ApplicationReport.newInstance(\n-        applicationId, ApplicationAttemptId.newInstance(applicationId, 1),\n-        \"user\", \"queue\", \"appname\", \"host\", 124, null,\n-        YarnApplicationState.FINISHED, \"diagnostics\", \"url\", 0, 0,\n-        FinalApplicationStatus.SUCCEEDED, usageReport, \"N/A\", 0.53789f, \"YARN\",\n-        null);\n-    when(client.getApplicationReport(any(ApplicationId.class))).thenReturn(\n-        newApplicationReport);\n-    int result = cli.run(new String[] { \"application\", \"-status\", applicationId.toString() });\n-    assertEquals(0, result);\n-    verify(client).getApplicationReport(applicationId);\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n-    PrintWriter pw = new PrintWriter(baos);\n-    pw.println(\"Application Report : \");\n-    pw.println(\"\\tApplication-Id : application_1234_0005\");\n-    pw.println(\"\\tApplication-Name : appname\");\n-    pw.println(\"\\tApplication-Type : YARN\");\n-    pw.println(\"\\tUser : user\");\n-    pw.println(\"\\tQueue : queue\");\n-    pw.println(\"\\tStart-Time : 0\");\n-    pw.println(\"\\tFinish-Time : 0\");\n-    pw.println(\"\\tProgress : 53.79%\");\n-    pw.println(\"\\tState : FINISHED\");\n-    pw.println(\"\\tFinal-State : SUCCEEDED\");\n-    pw.println(\"\\tTracking-URL : N/A\");\n-    pw.println(\"\\tRPC Port : 124\");\n-    pw.println(\"\\tAM Host : host\");\n-    pw.println(\"\\tAggregate Resource Allocation : 123456 MB-seconds, 4567 vcore-seconds\");\n-    pw.println(\"\\tDiagnostics : diagnostics\");\n-    pw.close();\n-    String appReportStr = baos.toString(\"UTF-8\");\n-    Assert.assertEquals(appReportStr, sysOutStream.toString());\n-    verify(sysOut, times(1)).println(isA(String.class));\n+    for (int i = 0; i < 2; ++i) {\n+      ApplicationCLI cli = createAndGetAppCLI();\n+      ApplicationId applicationId = ApplicationId.newInstance(1234, 5);\n+      ApplicationResourceUsageReport usageReport = i == 0 ? null :\n+          ApplicationResourceUsageReport.newInstance(\n+              2, 0, null, null, null, 123456, 4567);\n+      ApplicationReport newApplicationReport = ApplicationReport.newInstance(\n+          applicationId, ApplicationAttemptId.newInstance(applicationId, 1),\n+          \"user\", \"queue\", \"appname\", \"host\", 124, null,\n+          YarnApplicationState.FINISHED, \"diagnostics\", \"url\", 0, 0,\n+          FinalApplicationStatus.SUCCEEDED, usageReport, \"N/A\", 0.53789f, \"YARN\",\n+          null);\n+      when(client.getApplicationReport(any(ApplicationId.class))).thenReturn(\n+          newApplicationReport);\n+      int result = cli.run(new String[] { \"application\", \"-status\", applicationId.toString() });\n+      assertEquals(0, result);\n+      verify(client, times(1 + i)).getApplicationReport(applicationId);\n+      ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+      PrintWriter pw = new PrintWriter(baos);\n+      pw.println(\"Application Report : \");\n+      pw.println(\"\\tApplication-Id : application_1234_0005\");\n+      pw.println(\"\\tApplication-Name : appname\");\n+      pw.println(\"\\tApplication-Type : YARN\");\n+      pw.println(\"\\tUser : user\");\n+      pw.println(\"\\tQueue : queue\");\n+      pw.println(\"\\tStart-Time : 0\");\n+      pw.println(\"\\tFinish-Time : 0\");\n+      pw.println(\"\\tProgress : 53.79%\");\n+      pw.println(\"\\tState : FINISHED\");\n+      pw.println(\"\\tFinal-State : SUCCEEDED\");\n+      pw.println(\"\\tTracking-URL : N/A\");\n+      pw.println(\"\\tRPC Port : 124\");\n+      pw.println(\"\\tAM Host : host\");\n+      pw.println(\"\\tAggregate Resource Allocation : \" +\n+          (i == 0 ? \"N/A\" : \"123456 MB-seconds, 4567 vcore-seconds\"));\n+      pw.println(\"\\tDiagnostics : diagnostics\");\n+      pw.close();\n+      String appReportStr = baos.toString(\"UTF-8\");\n+      Assert.assertEquals(appReportStr, sysOutStream.toString());\n+      sysOutStream.reset();\n+      verify(sysOut, times(1 + i)).println(isA(String.class));\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/a0ad975ea1e70f9532cf6cb6c1d9d92736ca0ebc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java",
                "sha": "980517fdb8daa69f40861ba058f0beecdbd453ec",
                "status": "modified"
            }
        ],
        "message": "YARN-2542. Fixed NPE when retrieving ApplicationReport from TimeLineServer. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/e65ae575a059a426c4c38fdabe22a31eabbb349e",
        "patched_files": [
            "ApplicationCLI.java",
            "CHANGES.java",
            "YarnCLI.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestYarnCLI.java"
        ]
    },
    "hadoop_a0b0383": {
        "bug_id": "hadoop_a0b0383",
        "commit": "https://github.com/apache/hadoop/commit/a0b0383677c037d4492907e5f119eb0e72390faf",
        "file": [
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop/blob/a0b0383677c037d4492907e5f119eb0e72390faf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java?ref=a0b0383677c037d4492907e5f119eb0e72390faf",
                "deletions": 29,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "patch": "@@ -589,20 +589,21 @@ public void testRemoveVolumeBeingWritten() throws Exception {\n     // Will write and remove on dn0.\n     final ExtendedBlock eb = new ExtendedBlock(BLOCK_POOL_IDS[0], 0);\n     final CountDownLatch startFinalizeLatch = new CountDownLatch(1);\n-    final CountDownLatch brReceivedLatch = new CountDownLatch(1);\n-    final CountDownLatch volRemovedLatch = new CountDownLatch(1);\n+    final CountDownLatch blockReportReceivedLatch = new CountDownLatch(1);\n+    final CountDownLatch volRemoveStartedLatch = new CountDownLatch(1);\n+    final CountDownLatch volRemoveCompletedLatch = new CountDownLatch(1);\n     class BlockReportThread extends Thread {\n       public void run() {\n         // Lets wait for the volume remove process to start\n         try {\n-          volRemovedLatch.await();\n+          volRemoveStartedLatch.await();\n         } catch (Exception e) {\n           LOG.info(\"Unexpected exception when waiting for vol removal:\", e);\n         }\n         LOG.info(\"Getting block report\");\n         dataset.getBlockReports(eb.getBlockPoolId());\n         LOG.info(\"Successfully received block report\");\n-        brReceivedLatch.countDown();\n+        blockReportReceivedLatch.countDown();\n       }\n     }\n \n@@ -623,7 +624,7 @@ public void run() {\n           }\n \n           // Lets wait for the other thread finish getting block report\n-          brReceivedLatch.await();\n+          blockReportReceivedLatch.await();\n \n           dataset.finalizeBlock(eb);\n           LOG.info(\"FinalizeBlock finished\");\n@@ -633,34 +634,58 @@ public void run() {\n       }\n     }\n \n-    ResponderThread res = new ResponderThread();\n-    res.start();\n+    class VolRemoveThread extends Thread {\n+      public void run() {\n+        try {\n+          Set<File> volumesToRemove = new HashSet<>();\n+          volumesToRemove.add(StorageLocation.parse(\n+              dataset.getVolume(eb).getBasePath()).getFile());\n+          /**\n+           * TODO: {@link FsDatasetImpl#removeVolumes(Set, boolean)} is throwing\n+           * IllegalMonitorStateException when there is a parallel reader/writer\n+           * to the volume. Remove below exception handling block after fixing\n+           * HDFS-10830.\n+           */\n+          LOG.info(\"Removing volume \" + volumesToRemove);\n+          dataset.removeVolumes(volumesToRemove, true);\n+          volRemoveCompletedLatch.countDown();\n+          LOG.info(\"Removed volume \" + volumesToRemove);\n+        } catch (Exception e) {\n+          LOG.info(\"Unexpected issue while removing volume: \", e);\n+          volRemoveCompletedLatch.countDown();\n+        }\n+      }\n+    }\n+\n+    // Start the volume write operation\n+    ResponderThread responderThread = new ResponderThread();\n+    responderThread.start();\n     startFinalizeLatch.await();\n \n-    // Verify if block report can be received\n-    // when volume is being removed\n-    final BlockReportThread brt = new BlockReportThread();\n-    brt.start();\n+    // Start the block report get operation\n+    final BlockReportThread blockReportThread = new BlockReportThread();\n+    blockReportThread.start();\n \n-    Set<File> volumesToRemove = new HashSet<>();\n-    volumesToRemove.add(\n-        StorageLocation.parse(dataset.getVolume(eb).getBasePath()).getFile());\n-    /**\n-     * TODO: {@link FsDatasetImpl#removeVolumes(Set, boolean)} is throwing\n-     * IllegalMonitorStateException when there is a parallel reader/writer\n-     * to the volume. Remove below try/catch block after fixing HDFS-10830.\n-     */\n-    try {\n-      LOG.info(\"Removing volume \" + volumesToRemove);\n-      dataset.removeVolumes(volumesToRemove, true);\n-    } catch (Exception e) {\n-      LOG.info(\"Unexpected issue while removing volume: \", e);\n-    } finally {\n-      volRemovedLatch.countDown();\n-    }\n+    // Start the volume remove operation\n+    VolRemoveThread volRemoveThread = new VolRemoveThread();\n+    volRemoveThread.start();\n+\n+    // Let volume write and remove operation be\n+    // blocked for few seconds\n+    Thread.sleep(2000);\n+\n+    // Signal block report receiver and volume writer\n+    // thread to complete their operations so that vol\n+    // remove can proceed\n+    volRemoveStartedLatch.countDown();\n+\n+    // Verify if block report can be received\n+    // when volume is in use and also being removed\n+    blockReportReceivedLatch.await();\n \n-    LOG.info(\"Volumes removed\");\n-    brReceivedLatch.await();\n+    // Verify if volume can be removed safely when there\n+    // are read/write operation in-progress\n+    volRemoveCompletedLatch.await();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/a0b0383677c037d4492907e5f119eb0e72390faf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "sha": "b3f04d24365914ed0b9215a36d456476d6b73f00",
                "status": "modified"
            }
        ],
        "message": " Addendum patch for HDFS-9781. FsDatasetImpl#getBlockReports can occasionally throw NullPointerException. Contributed by Manoj Govindassamy.",
        "parent": "https://github.com/apache/hadoop/commit/d4d076876a8d0002bd3a73491d8459d11cb4896c",
        "patched_files": [
            "FsDatasetImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_a196ee9": {
        "bug_id": "hadoop_a196ee9",
        "commit": "https://github.com/apache/hadoop/commit/a196ee9362a1b35e5de20ee519f7c544ab1588e1",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java?ref=a196ee9362a1b35e5de20ee519f7c544ab1588e1",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer;\n import org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext;\n import org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext;\n-import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -97,6 +96,8 @@ public Integer call() {\n       List<String> logDirs = dirsHandler.getLogDirs();\n       List<String> containerLocalDirs = getContainerLocalDirs(localDirs);\n       List<String> containerLogDirs = getContainerLogDirs(logDirs);\n+      List<String> filecacheDirs = getNMFilecacheDirs(localDirs);\n+      List<String> userLocalDirs = getUserLocalDirs(localDirs);\n \n       if (!dirsHandler.areDisksHealthy()) {\n         ret = ContainerExitStatus.DISKS_FAILED;\n@@ -114,6 +115,8 @@ public Integer call() {\n           .setContainerWorkDir(containerWorkDir)\n           .setLocalDirs(localDirs)\n           .setLogDirs(logDirs)\n+          .setFilecacheDirs(filecacheDirs)\n+          .setUserLocalDirs(userLocalDirs)\n           .setContainerLocalDirs(containerLocalDirs)\n           .setContainerLogDirs(containerLogDirs)\n           .build());",
                "raw_url": "https://github.com/apache/hadoop/raw/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java",
                "sha": "6a0761a5d24245b694fd0e3c1f682e4c7cbe0cc4",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop/blob/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java?ref=a196ee9362a1b35e5de20ee519f7c544ab1588e1",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java",
                "patch": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.event.Dispatcher;\n+import org.apache.hadoop.yarn.event.InlineDispatcher;\n+import org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n+import org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext;\n+import org.apache.hadoop.yarn.server.nodemanager.recovery.NMNullStateStoreService;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyString;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+/** Unit tests for relaunching containers. */\n+public class TestContainerRelaunch {\n+\n+  @Test\n+  public void testRelaunchContext() throws Exception {\n+    Configuration conf = new Configuration();\n+\n+    Context mockContext = mock(Context.class);\n+    doReturn(new NMNullStateStoreService()).when(mockContext).getNMStateStore();\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationAttemptId appAttemptId =\n+        ApplicationAttemptId.newInstance(appId, 1);\n+    ContainerId cid = ContainerId.newContainerId(appAttemptId, 1);\n+    Application mockApp = mock(Application.class);\n+    doReturn(appId).when(mockApp).getAppId();\n+    Container mockContainer = mock(Container.class);\n+    doReturn(\"somebody\").when(mockContainer).getUser();\n+    doReturn(cid).when(mockContainer).getContainerId();\n+    doReturn(\"/foo\").when(mockContainer).getWorkDir();\n+    doReturn(\"/bar\").when(mockContainer).getLogDir();\n+    LocalDirsHandlerService mockDirsHandler =\n+        mock(LocalDirsHandlerService.class);\n+    doReturn(true).when(mockDirsHandler).isGoodLocalDir(any(String.class));\n+    doReturn(true).when(mockDirsHandler).isGoodLogDir(anyString());\n+    doReturn(true).when(mockDirsHandler).areDisksHealthy();\n+    doReturn(new Path(\"/some/file\")).when(mockDirsHandler)\n+        .getLocalPathForRead(anyString());\n+    Dispatcher dispatcher = new InlineDispatcher();\n+    ContainerExecutor mockExecutor = mock(ContainerExecutor.class);\n+    ContainerRelaunch cr = new ContainerRelaunch(mockContext, conf, dispatcher,\n+        mockExecutor, mockApp, mockContainer, mockDirsHandler, null);\n+    int result = cr.call();\n+    assertEquals(\"relaunch failed\", 0, result);\n+    ArgumentCaptor<ContainerStartContext> captor =\n+        ArgumentCaptor.forClass(ContainerStartContext.class);\n+    verify(mockExecutor).launchContainer(captor.capture());\n+    ContainerStartContext csc = captor.getValue();\n+    assertNotNull(\"app ID null\", csc.getAppId());\n+    assertNotNull(\"container null\", csc.getContainer());\n+    assertNotNull(\"container local dirs null\", csc.getContainerLocalDirs());\n+    assertNotNull(\"container log dirs null\", csc.getContainerLogDirs());\n+    assertNotNull(\"work dir null\", csc.getContainerWorkDir());\n+    assertNotNull(\"filecache dirs null\", csc.getFilecacheDirs());\n+    assertNotNull(\"local dirs null\", csc.getLocalDirs());\n+    assertNotNull(\"localized resources null\", csc.getLocalizedResources());\n+    assertNotNull(\"log dirs null\", csc.getLogDirs());\n+    assertNotNull(\"script path null\", csc.getNmPrivateContainerScriptPath());\n+    assertNotNull(\"tokens path null\", csc.getNmPrivateTokensPath());\n+    assertNotNull(\"user null\", csc.getUser());\n+    assertNotNull(\"user local dirs null\", csc.getUserLocalDirs());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java",
                "sha": "95f706c3e079e46e23ae72977367f83a78ab650e",
                "status": "added"
            }
        ],
        "message": "YARN-7890. NPE during container relaunch. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/60656bcecadd80e28c81bc943b44abf13d20abae",
        "patched_files": [
            "ContainerRelaunch.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerRelaunch.java"
        ]
    },
    "hadoop_a1c3868": {
        "bug_id": "hadoop_a1c3868",
        "commit": "https://github.com/apache/hadoop/commit/a1c3868c4f027adcb814b30d842e60d1f94326ea",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/a1c3868c4f027adcb814b30d842e60d1f94326ea/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java?ref=a1c3868c4f027adcb814b30d842e60d1f94326ea",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "patch": "@@ -296,6 +296,7 @@ public void testGetBlockType() {\n     preferredBlockSize = 128*1024*1024;\n     INodeFile inf = createINodeFile(replication, preferredBlockSize);\n     assertEquals(inf.getBlockType(), CONTIGUOUS);\n+    ErasureCodingPolicyManager.getInstance().init(new Configuration());\n     INodeFile striped = createStripedINodeFile(preferredBlockSize);\n     assertEquals(striped.getBlockType(), STRIPED);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/a1c3868c4f027adcb814b30d842e60d1f94326ea/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "sha": "4674bd7308313e73740df7602f28fa7c05e85eb0",
                "status": "modified"
            }
        ],
        "message": "HDFS-13287. TestINodeFile#testGetBlockType results in NPE when run alone. Contributed by Virajith Jalaparti.\n\nSigned-off-by: Wei-Chiu Chuang <weichiu@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/f9ee97de98e1371a2760286070e339a1fd7c5fde",
        "patched_files": [
            "INodeFile.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestINodeFile.java"
        ]
    },
    "hadoop_a24d12b": {
        "bug_id": "hadoop_a24d12b",
        "commit": "https://github.com/apache/hadoop/commit/a24d12bdecee655ff00829906e1c57656f0fea7c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a24d12bdecee655ff00829906e1c57656f0fea7c/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=a24d12bdecee655ff00829906e1c57656f0fea7c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -89,6 +89,9 @@ Trunk (unreleased changes)\n     MAPREDUCE-3664. Federation Documentation has incorrect configuration example.\n     (Brandon Li via jitendra)\n \n+    MAPREDUCE-1740. NPE in getMatchingLevelForNodes when node locations are \n+    variable depth (ahmed via tucu) [IMPORTANT: this is dead code in trunk]\n+\n Release 0.23.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a24d12bdecee655ff00829906e1c57656f0fea7c/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d5993698def544f85b305cd8afee5516bf4389a4",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/a24d12bdecee655ff00829906e1c57656f0fea7c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java?ref=a24d12bdecee655ff00829906e1c57656f0fea7c",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java",
                "patch": "@@ -1596,16 +1596,37 @@ public synchronized Task obtainNewReduceTask(TaskTrackerStatus tts,\n   \n   // returns the (cache)level at which the nodes matches\n   private int getMatchingLevelForNodes(Node n1, Node n2) {\n+    return getMatchingLevelForNodes(n1, n2, this.maxLevel);\n+  }\n+\n+  static int getMatchingLevelForNodes(Node n1, Node n2, int maxLevel) {\n     int count = 0;\n+\n+    // In the case that the two nodes are at different levels in the\n+    // node heirarchy, walk upwards on the deeper one until the\n+    // levels are equal. Each of these counts as \"distance\" since it\n+    // assumedly is going through another rack.\n+    int level1=n1.getLevel(), level2=n2.getLevel();\n+    while(n1!=null && level1>level2) {\n+      n1 = n1.getParent();\n+      level1--;\n+      count++;\n+    }\n+    while(n2!=null && level2>level1) {\n+      n2 = n2.getParent();\n+      level2--;\n+      count++;\n+    }\n+\n     do {\n-      if (n1.equals(n2)) {\n-        return count;\n+      if (n1.equals(n2) || count >= maxLevel) {\n+        return Math.min(count, maxLevel);\n       }\n       ++count;\n       n1 = n1.getParent();\n       n2 = n2.getParent();\n     } while (n1 != null);\n-    return this.maxLevel;\n+    return maxLevel;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/a24d12bdecee655ff00829906e1c57656f0fea7c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobInProgress.java",
                "sha": "9f92707077b56ee905db372e676d2e8b71678827",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hadoop/blob/a24d12bdecee655ff00829906e1c57656f0fea7c/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java?ref=a24d12bdecee655ff00829906e1c57656f0fea7c",
                "deletions": 24,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java",
                "patch": "@@ -29,10 +29,9 @@\n import java.util.Map;\n import java.util.Set;\n \n-import junit.extensions.TestSetup;\n-import junit.framework.Test;\n-import junit.framework.TestCase;\n-import junit.framework.TestSuite;\n+import static org.junit.Assert.*;\n+import org.junit.Test;\n+import org.junit.BeforeClass;\n import static org.mockito.Mockito.*;\n \n import org.apache.commons.logging.Log;\n@@ -47,11 +46,13 @@\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n import org.apache.hadoop.mapreduce.split.JobSplit.TaskSplitMetaInfo;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n+import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.net.StaticMapping;\n \n @SuppressWarnings(\"deprecation\")\n-public class TestJobInProgress extends TestCase {\n+public class TestJobInProgress {\n   static final Log LOG = LogFactory.getLog(TestJobInProgress.class);\n \n   static FakeJobTracker jobTracker;\n@@ -75,25 +76,21 @@\n   static int numUniqueHosts = hosts.length;\n   static int clusterSize = trackers.length;\n \n-  public static Test suite() {\n-    TestSetup setup = new TestSetup(new TestSuite(TestJobInProgress.class)) {\n-      protected void setUp() throws Exception {\n-        JobConf conf = new JobConf();\n-        conf.set(JTConfig.JT_IPC_ADDRESS, \"localhost:0\");\n-        conf.set(JTConfig.JT_HTTP_ADDRESS, \"0.0.0.0:0\");\n-        conf.setClass(\"topology.node.switch.mapping.impl\", \n-            StaticMapping.class, DNSToSwitchMapping.class);\n-        jobTracker = new FakeJobTracker(conf, new FakeClock(), trackers);\n-        // Set up the Topology Information\n-        for (int i = 0; i < hosts.length; i++) {\n-          StaticMapping.addNodeToRack(hosts[i], racks[i]);\n-        }\n-        for (String s: trackers) {\n-          FakeObjectUtilities.establishFirstContact(jobTracker, s);\n-        }\n-      }\n-    };\n-    return setup;\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    JobConf conf = new JobConf();\n+    conf.set(JTConfig.JT_IPC_ADDRESS, \"localhost:0\");\n+    conf.set(JTConfig.JT_HTTP_ADDRESS, \"0.0.0.0:0\");\n+    conf.setClass(\"topology.node.switch.mapping.impl\",\n+                  StaticMapping.class, DNSToSwitchMapping.class);\n+    jobTracker = new FakeJobTracker(conf, new FakeClock(), trackers);\n+    // Set up the Topology Information\n+    for (int i = 0; i < hosts.length; i++) {\n+      StaticMapping.addNodeToRack(hosts[i], racks[i]);\n+    }\n+    for (String s: trackers) {\n+      FakeObjectUtilities.establishFirstContact(jobTracker, s);\n+    }\n   }\n \n   static class MyFakeJobInProgress extends FakeJobInProgress {\n@@ -157,6 +154,7 @@ public TaskAttemptID findAndRunNewTask(boolean isMap,\n     }\n   }\n \n+  //@Test\n   public void testPendingMapTaskCount() throws Exception {\n \n     int numMaps = 4;\n@@ -259,6 +257,7 @@ static void testRunningTaskCount(boolean speculation)  throws Exception {\n \n   }\n \n+  //@Test\n   public void testRunningTaskCount() throws Exception {\n     // test with spec = false \n     testRunningTaskCount(false);\n@@ -287,6 +286,7 @@ static void checkTaskCounts(JobInProgress jip, int runningMaps,\n     assertEquals(pendingReduces, jip.pendingReduces());\n   }\n \n+  //@Test\n   public void testJobSummary() throws Exception {\n     int numMaps = 2;\n     int numReds = 2;\n@@ -341,4 +341,35 @@ public void testJobSummary() throws Exception {\n     assertEquals(\"firstReduceTaskLaunchTime\", 3,\n         jspy.getFirstTaskLaunchTimes().get(TaskType.REDUCE).longValue());\n   }\n+\n+  @Test\n+  public void testLocality() throws Exception {\n+    NetworkTopology nt = new NetworkTopology();\n+\n+    Node r1n1 = new NodeBase(\"/default/rack1/node1\");\n+    nt.add(r1n1);\n+    Node r1n2 = new NodeBase(\"/default/rack1/node2\");\n+    nt.add(r1n2);\n+\n+    Node r2n3 = new NodeBase(\"/default/rack2/node3\");\n+    nt.add(r2n3);\n+\n+    Node r2n4 = new NodeBase(\"/default/rack2/s1/node4\");\n+    nt.add(r2n4);\n+\n+    LOG.debug(\"r1n1 parent: \" + r1n1.getParent() + \"\\n\" +\n+              \"r1n2 parent: \" + r1n2.getParent() + \"\\n\" +\n+              \"r2n3 parent: \" + r2n3.getParent() + \"\\n\" +\n+              \"r2n4 parent: \" + r2n4.getParent());\n+\n+    // Same host\n+    assertEquals(0, JobInProgress.getMatchingLevelForNodes(r1n1, r1n1, 3));\n+    // Same rack\n+    assertEquals(1, JobInProgress.getMatchingLevelForNodes(r1n1, r1n2, 3));\n+    // Different rack\n+    assertEquals(2, JobInProgress.getMatchingLevelForNodes(r1n1, r2n3, 3));\n+    // Different rack at different depth\n+    assertEquals(3, JobInProgress.getMatchingLevelForNodes(r1n1, r2n4, 3));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a24d12bdecee655ff00829906e1c57656f0fea7c/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobInProgress.java",
                "sha": "ea100aab08f5e0308763313332365114c0464c68",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1740. NPE in getMatchingLevelForNodes when node locations are variable depth (ahmed via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1303076 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/40a8293d36cbae0fc20abe046a35f229df149f46",
        "patched_files": [
            "JobInProgress.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJobInProgress.java"
        ]
    },
    "hadoop_a446ad2": {
        "bug_id": "hadoop_a446ad2",
        "commit": "https://github.com/apache/hadoop/commit/a446ad2c26359beb2b5367195de4257fbae648c6",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -481,6 +481,8 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4237. TestNodeStatusUpdater can fail if localhost has a domain\n     associated with it (bobby)\n \n+    MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "ff4664675d44bb97f2aff87288a3d2c7127d3307",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "patch": "@@ -21,22 +21,28 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n import org.junit.AfterClass;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import static org.mockito.Mockito.*;\n \n public class TestRMNMInfo {\n   private static final Log LOG = LogFactory.getLog(TestRMNMInfo.class);\n@@ -116,14 +122,47 @@ public void testRMNMInfo() throws Exception {\n               n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n       Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n       Assert.assertNotNull(n.get(\"HealthReport\"));\n-      Assert.assertNotNull(n.get(\"NumContainersMB\"));\n+      Assert.assertNotNull(n.get(\"NumContainers\"));\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected number of used containers\",\n-              0, n.get(\"NumContainersMB\").getValueAsInt());\n+              0, n.get(\"NumContainers\").getValueAsInt());\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected amount of used memory\",\n               0, n.get(\"UsedMemoryMB\").getValueAsInt());\n       Assert.assertNotNull(n.get(\"AvailableMemoryMB\"));\n     }\n   }\n+  \n+  @Test\n+  public void testRMNMInfoMissmatch() throws Exception {\n+    RMContext rmc = mock(RMContext.class);\n+    ResourceScheduler rms = mock(ResourceScheduler.class);\n+    ConcurrentMap<NodeId, RMNode> map = new ConcurrentHashMap<NodeId, RMNode>();\n+    RMNode node = MockNodes.newNodeInfo(1, MockNodes.newResource(4 * 1024));\n+    map.put(node.getNodeID(), node);\n+    when(rmc.getRMNodes()).thenReturn(map);\n+    \n+    RMNMInfo rmInfo = new RMNMInfo(rmc,rms);\n+    String liveNMs = rmInfo.getLiveNodeManagers();\n+    ObjectMapper mapper = new ObjectMapper();\n+    JsonNode jn = mapper.readTree(liveNMs);\n+    Assert.assertEquals(\"Unexpected number of live nodes:\",\n+                                               1, jn.size());\n+    Iterator<JsonNode> it = jn.iterator();\n+    while (it.hasNext()) {\n+      JsonNode n = it.next();\n+      Assert.assertNotNull(n.get(\"HostName\"));\n+      Assert.assertNotNull(n.get(\"Rack\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\",\n+              n.get(\"State\").getValueAsText().contains(\"RUNNING\"));\n+      Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be Healthy\",\n+              n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n+      Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n+      Assert.assertNotNull(n.get(\"HealthReport\"));\n+      Assert.assertNull(n.get(\"NumContainers\"));\n+      Assert.assertNull(n.get(\"UsedMemoryMB\"));\n+      Assert.assertNull(n.get(\"AvailableMemoryMB\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "sha": "4ee485644d91ad5b1fcffde9846301c36d237353",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "patch": "@@ -93,10 +93,12 @@ public String getLiveNodeManagers() {\n                         ni.getNodeHealthStatus().getLastHealthReportTime());\n         info.put(\"HealthReport\",\n                         ni.getNodeHealthStatus().getHealthReport());\n-        info.put(\"NumContainersMB\", report.getNumContainers());\n-        info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n-        info.put(\"AvailableMemoryMB\",\n-                                report.getAvailableResource().getMemory());\n+        if(report != null) {\n+          info.put(\"NumContainers\", report.getNumContainers());\n+          info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n+          info.put(\"AvailableMemoryMB\",\n+              report.getAvailableResource().getMemory());\n+        }\n \n         nodesInfo.add(info);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "sha": "0db42e40ec0e08d72413048956baf0393062157d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1337363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/f092e9fc8a2b58c755ecfc6828cc3e2af624b90b",
        "patched_files": [
            "CHANGES.java",
            "RMNMInfo.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMNMInfo.java"
        ]
    },
    "hadoop_a4bae51": {
        "bug_id": "hadoop_a4bae51",
        "commit": "https://github.com/apache/hadoop/commit/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -273,6 +273,8 @@ Trunk (Unreleased)\n     HDFS-4785. Concat operation does not remove concatenated files from\n     InodeMap. (suresh)\n \n+    HDFS-4784. NPE in FSDirectory.resolvePath(). (Brandon Li via suresh)\n+\n   BREAKDOWN OF HADOOP-8562 and HDFS-3602 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4145. Merge hdfs cmd line scripts from branch-1-win. (David Lao,",
                "raw_url": "https://github.com/apache/hadoop/raw/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4329d145cb8f54eb93bcf59ea28f76aa48002736",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1090,7 +1090,7 @@ int unprotectedDelete(String src, BlocksMapUpdateInfo collectedBlocks,\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedDelete: \"\n           +src+\" is removed\");\n     }\n-    remvoedAllFromInodesFromMap(targetNode);\n+    removeAllFromInodesFromMap(targetNode);\n     return filesRemoved;\n   }\n   \n@@ -1783,14 +1783,14 @@ private final void removeFromInodeMap(INode inode) {\n   }\n   \n   /** Remove all the inodes under given inode from the map */\n-  private void remvoedAllFromInodesFromMap(INode inode) {\n+  private void removeAllFromInodesFromMap(INode inode) {\n     removeFromInodeMap(inode);\n     if (!inode.isDirectory()) {\n       return;\n     }\n     INodeDirectory dir = (INodeDirectory) inode;\n     for (INode child : dir.getChildrenList()) {\n-      remvoedAllFromInodesFromMap(child);\n+      removeAllFromInodesFromMap(child);\n     }\n     dir.clearChildren();\n   }\n@@ -2258,14 +2258,18 @@ static String resolvePath(String src, byte[][] pathComponents, FSDirectory fsd)\n     try {\n       id = Long.valueOf(inodeId);\n     } catch (NumberFormatException e) {\n-      throw new FileNotFoundException(\n-          \"File for given inode path does not exist: \" + src);\n+      throw new FileNotFoundException(\"Invalid inode path: \" + src);\n     }\n     if (id == INodeId.ROOT_INODE_ID && pathComponents.length == 4) {\n       return Path.SEPARATOR;\n     }\n+    INode inode = fsd.getInode(id);\n+    if (inode == null) {\n+      throw new FileNotFoundException(\n+          \"File for given inode path does not exist: \" + src);\n+    }\n     StringBuilder path = id == INodeId.ROOT_INODE_ID ? new StringBuilder()\n-        : new StringBuilder(fsd.getInode(id).getFullPathName());\n+        : new StringBuilder(inode.getFullPathName());\n     for (int i = 4; i < pathComponents.length; i++) {\n       path.append(Path.SEPARATOR).append(DFSUtil.bytes2String(pathComponents[i]));\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "a3150a8b20eff2ab7bb2beb9b0699e8f923348fd",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java?ref=a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "patch": "@@ -909,6 +909,17 @@ public void testInodePath() throws FileNotFoundException {\n     components = INode.getPathComponents(testPath);\n     resolvedPath = FSDirectory.resolvePath(testPath, components, fsd);\n     assertEquals(testPath, resolvedPath);\n+    \n+    // Test path with nonexistent(deleted or wrong id) inode\n+    Mockito.doReturn(null).when(fsd).getInode(Mockito.anyLong());\n+    testPath = \"/.reserved/.inodes/1234\";\n+    components = INode.getPathComponents(testPath);\n+    try {\n+      String realPath = FSDirectory.resolvePath(testPath, components, fsd);\n+      fail(\"Path should not be resolved:\" + realPath);\n+    } catch (IOException e) {\n+      assertTrue(e instanceof FileNotFoundException);\n+    }\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "sha": "3b6491f6e5b4a08d5f5f0f430cb73caff9b9b0c7",
                "status": "modified"
            }
        ],
        "message": "HDFS-4784. NPE in FSDirectory.resolvePath(). Contributed by Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1478276 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/03ba436d42418226a5edb754f5119fe69039c8b8",
        "patched_files": [
            "INodeFile.java",
            "FSDirectory.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestINodeFile.java",
            "TestFSDirectory.java"
        ]
    },
    "hadoop_a4f62a2": {
        "bug_id": "hadoop_a4f62a2",
        "commit": "https://github.com/apache/hadoop/commit/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2308,6 +2308,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9387. Fix namenodeUri parameter parsing in NNThroughputBenchmark.\n     (Mingliang Liu via xyao)\n \n+    HDFS-9421. NNThroughputBenchmark replication test NPE with -namenode option.\n+    (Mingliang Liu via xyao)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "d76253a908a73012071f3b8a5eb38ff395cd2cb3",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java?ref=a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "patch": "@@ -106,9 +106,9 @@\n  * By default the refresh is never called.</li>\n  * <li>-keepResults do not clean up the name-space after execution.</li>\n  * <li>-useExisting do not recreate the name-space, use existing data.</li>\n- * <li>-namenode will run the test against a namenode in another\n- * process or on another host. If you use this option, the namenode\n- * must have dfs.namenode.fs-limits.min-block-size set to 16.</li>\n+ * <li>-namenode will run the test (except {@link ReplicationStats}) against a\n+ * namenode in another process or on another host. If you use this option,\n+ * the namenode must have dfs.namenode.fs-limits.min-block-size set to 16.</li>\n  * </ol>\n  * \n  * The benchmark first generates inputs for each thread so that the\n@@ -126,8 +126,9 @@\n   private static final String GENERAL_OPTIONS_USAGE = \n     \"     [-keepResults] | [-logLevel L] | [-UGCacheRefreshCount G] |\" +\n     \" [-namenode <namenode URI>]\\n\" +\n-    \"     If using -namenode, set the namenode's\" +\n-    \"         dfs.namenode.fs-limits.min-block-size to 16.\";\n+    \"     If using -namenode, set the namenode's \" +\n+    \"dfs.namenode.fs-limits.min-block-size to 16. Replication test does not \" +\n+        \"support -namenode.\";\n \n   static Configuration config;\n   static NameNode nameNode;\n@@ -1471,13 +1472,22 @@ public int run(String[] aArgs) throws Exception {\n         ops.add(opStat);\n       }\n       if(runAll || ReplicationStats.OP_REPLICATION_NAME.equals(type)) {\n-        opStat = new ReplicationStats(args);\n-        ops.add(opStat);\n+        if (namenodeUri != null || args.contains(\"-namenode\")) {\n+          LOG.warn(\"The replication test is ignored as it does not support \" +\n+              \"standalone namenode in another process or on another host. \" +\n+              \"Please run replication test without -namenode argument.\");\n+        } else {\n+          opStat = new ReplicationStats(args);\n+          ops.add(opStat);\n+        }\n       }\n       if(runAll || CleanAllStats.OP_CLEAN_NAME.equals(type)) {\n         opStat = new CleanAllStats(args);\n         ops.add(opStat);\n       }\n+      if (ops.isEmpty()) {\n+        printUsage();\n+      }\n \n       if (namenodeUri == null) {\n         nameNode = NameNode.createNameNode(argv, config);\n@@ -1501,8 +1511,6 @@ public int run(String[] aArgs) throws Exception {\n             DFSTestUtil.getRefreshUserMappingsProtocolProxy(config, nnUri);\n         getBlockPoolId(dfs);\n       }\n-      if(ops.size() == 0)\n-        printUsage();\n       // run each benchmark\n       for(OperationStatsBase op : ops) {\n         LOG.info(\"Starting benchmark: \" + op.getOpName());",
                "raw_url": "https://github.com/apache/hadoop/raw/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "sha": "affbe2f9af54d24dadaa4e31b5f018aee1efc084",
                "status": "modified"
            }
        ],
        "message": "HDFS-9421. NNThroughputBenchmark replication test NPE with -namenode option. Contributed by Mingliang Liu.",
        "parent": "https://github.com/apache/hadoop/commit/2701f2d2558f3ade879539f3f7bedf749709f2f1",
        "patched_files": [
            "NNThroughputBenchmark.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNNThroughputBenchmark.java"
        ]
    },
    "hadoop_a583a40": {
        "bug_id": "hadoop_a583a40",
        "commit": "https://github.com/apache/hadoop/commit/a583a40693f5c56c40b39fd12cfa0bb7174fc526",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=a583a40693f5c56c40b39fd12cfa0bb7174fc526",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -316,6 +316,8 @@ Release 2.8.0 - UNRELEASED\n     YARN-3343. Increased TestCapacitySchedulerNodeLabelUpdate#testNodeUpdate\n     timeout. (Rohith Sharmaks via jianhe)\n \n+    YARN-3582. NPE in WebAppProxyServlet. (jian he via xgong)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/CHANGES.txt",
                "sha": "97b7ee4df5db98fcf21db191ca504bf1ac5221af",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java?ref=a583a40693f5c56c40b39fd12cfa0bb7174fc526",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "patch": "@@ -248,8 +248,11 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       final String remoteUser = req.getRemoteUser();\n       final String pathInfo = req.getPathInfo();\n \n-      String[] parts = pathInfo.split(\"/\", 3);\n-      if(parts.length < 2) {\n+      String[] parts = null;\n+      if (pathInfo != null) {\n+        parts = pathInfo.split(\"/\", 3);\n+      }\n+      if(parts == null || parts.length < 2) {\n         LOG.warn(\"{} gave an invalid proxy path {}\", remoteUser,  pathInfo);\n         notFound(resp, \"Your path appears to be formatted incorrectly.\");\n         return;",
                "raw_url": "https://github.com/apache/hadoop/raw/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "sha": "d45beb68d777b59ce4dfdc0917073d788679956e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java?ref=a583a40693f5c56c40b39fd12cfa0bb7174fc526",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java",
                "patch": "@@ -131,6 +131,13 @@ public void testWebAppProxyServlet() throws Exception {\n \n     // wrong url\n     try {\n+      // wrong url without app ID\n+      URL emptyUrl = new URL(\"http://localhost:\" + proxyPort + \"/proxy\");\n+      HttpURLConnection emptyProxyConn = (HttpURLConnection) emptyUrl\n+          .openConnection();\n+      emptyProxyConn.connect();;\n+      assertEquals(HttpURLConnection.HTTP_NOT_FOUND, emptyProxyConn.getResponseCode());\n+\n       // wrong url. Set wrong app ID\n       URL wrongUrl = new URL(\"http://localhost:\" + proxyPort + \"/proxy/app\");\n       HttpURLConnection proxyConn = (HttpURLConnection) wrongUrl",
                "raw_url": "https://github.com/apache/hadoop/raw/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java",
                "sha": "2a2ca2ca3d2f57b3d8d0540c8590fb8dcc401ba8",
                "status": "modified"
            }
        ],
        "message": "YARN-3582. NPE in WebAppProxyServlet. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/90b384564875bb353224630e501772b46d4ca9c5",
        "patched_files": [
            "WebAppProxyServlet.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestWebAppProxyServlet.java"
        ]
    },
    "hadoop_a749ba0": {
        "bug_id": "hadoop_a749ba0",
        "commit": "https://github.com/apache/hadoop/commit/a749ba0ceaa843aa83146b6bea19e031c8dc3296",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/a749ba0ceaa843aa83146b6bea19e031c8dc3296/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java?ref=a749ba0ceaa843aa83146b6bea19e031c8dc3296",
                "deletions": 2,
                "filename": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java",
                "patch": "@@ -369,8 +369,12 @@ private Path computeSourceRootPath(FileStatus sourceStatus,\n       boolean specialHandling = (options.getSourcePaths().size() == 1 && !targetPathExists) ||\n           options.shouldSyncFolder() || options.shouldOverwrite();\n \n-      return specialHandling && sourceStatus.isDirectory() ? sourceStatus.getPath() :\n-          sourceStatus.getPath().getParent();\n+      if ((specialHandling && sourceStatus.isDirectory()) ||\n+          sourceStatus.getPath().isRoot()) {\n+        return sourceStatus.getPath();\n+      } else {\n+        return sourceStatus.getPath().getParent();\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/a749ba0ceaa843aa83146b6bea19e031c8dc3296/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/SimpleCopyListing.java",
                "sha": "3f52203d32fd2c201bff87943ef307dae710cce1",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hadoop/blob/a749ba0ceaa843aa83146b6bea19e031c8dc3296/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java?ref=a749ba0ceaa843aa83146b6bea19e031c8dc3296",
                "deletions": 0,
                "filename": "hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.hadoop.tools;\n \n+import static org.hamcrest.core.Is.is;\n+\n import java.io.IOException;\n import java.io.OutputStream;\n import java.net.URI;\n@@ -32,6 +34,8 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.util.ToolRunner;\n+import org.junit.Assert;\n+import org.junit.Test;\n \n /**\n  * A JUnit test for copying files recursively.\n@@ -201,4 +205,34 @@ public void testPreserveUserNonEmptyDirWithUpdate() throws Exception {\n     testPreserveUserHelper(srcfiles, dstfiles, true, true, true);\n   }\n \n+  @Test\n+  public void testSourceRoot() throws Exception {\n+    MiniDFSCluster cluster = null;\n+    Configuration conf = new Configuration();\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+      cluster.waitActive();\n+      FileSystem fs = cluster.getFileSystem();\n+\n+      String rootStr = fs.makeQualified(new Path(\"/\")).toString();\n+\n+      // Case 1. The target does not exist.\n+\n+      String tgtStr = fs.makeQualified(new Path(\"/nodir\")).toString();\n+      String[] args = new String[]{ rootStr, tgtStr };\n+      Assert.assertThat(ToolRunner.run(conf, new DistCp(), args), is(0));\n+\n+      // Case 2. The target exists.\n+\n+      Path tgtPath2 = new Path(\"/dir\");\n+      assertTrue(fs.mkdirs(tgtPath2));\n+      String tgtStr2 = fs.makeQualified(tgtPath2).toString();\n+      String[] args2 = new String[]{ rootStr, tgtStr2 };\n+      Assert.assertThat(ToolRunner.run(conf, new DistCp(), args2), is(0));\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/a749ba0ceaa843aa83146b6bea19e031c8dc3296/hadoop-tools/hadoop-distcp/src/test/java/org/apache/hadoop/tools/TestDistCpSystem.java",
                "sha": "cd8656074c85843c77b7c753dbe12494c4ecb855",
                "status": "modified"
            }
        ],
        "message": "HDFS-9670. DistCp throws NPE when source is root. (John Zhuge via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/4838b735f0d472765f402fe6b1c8b6ce85b9fbf1",
        "patched_files": [
            "SimpleCopyListing.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDistCpSystem.java"
        ]
    },
    "hadoop_a7e450c": {
        "bug_id": "hadoop_a7e450c",
        "commit": "https://github.com/apache/hadoop/commit/a7e450c7cc0adef3ff832368a2a041bb51396945",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=a7e450c7cc0adef3ff832368a2a041bb51396945",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -234,6 +234,10 @@ Trunk (Unreleased)\n     HADOOP-8815. RandomDatum needs to override hashCode().\n     (Brandon Li via suresh)\n \n+    HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the\n+    required context item is not configured\n+    (Brahma Reddy Battula via harsh)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "5b4066b80f22163dc57d54001f70fd088847f806",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java?ref=a7e450c7cc0adef3ff832368a2a041bb51396945",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "patch": "@@ -265,6 +265,9 @@ public AllocatorPerContext(String contextCfgItemName) {\n     private synchronized void confChanged(Configuration conf) \n         throws IOException {\n       String newLocalDirs = conf.get(contextCfgItemName);\n+      if (null == newLocalDirs) {\n+        throw new IOException(contextCfgItemName + \" not configured\");\n+      }\n       if (!newLocalDirs.equals(savedLocalDirs)) {\n         localDirs = StringUtils.getTrimmedStrings(newLocalDirs);\n         localFS = FileSystem.getLocal(conf);",
                "raw_url": "https://github.com/apache/hadoop/raw/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "sha": "16a1c99b5c0ca566a6bce500f9ecdc44063a5bff",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java?ref=a7e450c7cc0adef3ff832368a2a041bb51396945",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "patch": "@@ -293,6 +293,23 @@ public void testLocalPathForWriteDirCreation() throws IOException {\n     }\n   }\n \n+  /*\n+   * Test when mapred.local.dir not configured and called\n+   * getLocalPathForWrite\n+   */\n+  @Test\n+  public void testShouldNotthrowNPE() throws Exception {\n+    Configuration conf1 = new Configuration();\n+    try {\n+      dirAllocator.getLocalPathForWrite(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + CONTEXT + \" is not set\");\n+    } catch (IOException e) {\n+      assertEquals(CONTEXT + \" not configured\", e.getMessage());\n+    } catch (NullPointerException e) {\n+      fail(\"Lack of configuration should not have thrown an NPE.\");\n+    }\n+  }\n+\n   /** Test no side effect files are left over. After creating a temp\n    * temp file, remove both the temp file and its parent. Verify that\n    * no files or directories are left over as can happen when File objects",
                "raw_url": "https://github.com/apache/hadoop/raw/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "sha": "7a4618e650b26f9419ce26022863a3a8c45027ae",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the required context item is not configured. Contributed by Brahma Reddy Battula. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1389799 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/d50e06d9e0232dccf5681ae7dda5996046e7eb1c",
        "patched_files": [
            "CHANGES.java",
            "LocalDirAllocator.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalDirAllocator.java"
        ]
    },
    "hadoop_a94b6a0": {
        "bug_id": "hadoop_a94b6a0",
        "commit": "https://github.com/apache/hadoop/commit/a94b6a0529a87577142aa16a53cf54d4dd4596ba",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a94b6a0529a87577142aa16a53cf54d4dd4596ba/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=a94b6a0529a87577142aa16a53cf54d4dd4596ba",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7342. Add an utility API in FileUtil for JDK File.list\n+    avoid NPEs on File.list() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7322. Adding a util method in FileUtil for directory listing,\n     avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/a94b6a0529a87577142aa16a53cf54d4dd4596ba/CHANGES.txt",
                "sha": "16152a6c9f688f892ae747b874e7f6f7105ac83b",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=a94b6a0529a87577142aa16a53cf54d4dd4596ba",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -728,4 +728,23 @@ public static void replaceFile(File src, File target) throws IOException {\n     }\n     return files;\n   }  \n+  \n+  /**\n+   * A wrapper for {@link File#list()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#list() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of file names or empty string list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static String[] list(File dir) throws IOException {\n+    String[] fileNames = dir.list();\n+    if(fileNames == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return fileNames;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "86956e368c079cfa9e221c8317550404fe53a29e",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=a94b6a0529a87577142aa16a53cf54d4dd4596ba",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -143,6 +143,33 @@ public void testListFiles() throws IOException {\n     \t//Expected an IOException\n     }\n   }\n+\n+  @Test\n+  public void testListAPI() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    String[] files = FileUtil.list(partitioned);\n+    Assert.assertEquals(\"Unexpected number of pre-existing files\", 2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.list(newDir);\n+    Assert.assertEquals(\"New directory unexpectedly contains files\", 0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.list(newDir);\n+      Assert.fail(\"IOException expected on list() for non-existent dir \"\n+          + newDir.toString());\n+    } catch(IOException ioe) {\n+      //Expected an IOException\n+    }\n+  }\n   \n   @After\n   public void tearDown() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "65c43435b75b9fb79d8be4e247a5f8175da3bf22",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7342. Add an utility API in FileUtil for JDK File.list avoid NPEs on File.list().  Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1131330 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/babd19de331c875a1dffee908617c07c3e1eb31b",
        "patched_files": [
            "CHANGES.java",
            "FileUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop_a9dc5cd": {
        "bug_id": "hadoop_a9dc5cd",
        "commit": "https://github.com/apache/hadoop/commit/a9dc5cd7069f721e8c55794b877026ba02537167",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a9dc5cd7069f721e8c55794b877026ba02537167",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -890,6 +890,9 @@ Release 2.7.0 - UNRELEASED\n     HDFS-7756. Restore method signature for LocatedBlock#getLocations(). (Ted\n     Yu via yliu)\n \n+    HDFS-7744. Fix potential NPE in DFSInputStream after setDropBehind or\n+    setReadahead is called (cmccabe)\n+\n Release 2.6.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "446c6a3a50d827872020128d58d3cca5f0105284",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=a9dc5cd7069f721e8c55794b877026ba02537167",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -580,10 +580,7 @@ private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {\n     }\n \n     // Will be getting a new BlockReader.\n-    if (blockReader != null) {\n-      blockReader.close();\n-      blockReader = null;\n-    }\n+    closeCurrentBlockReader();\n \n     //\n     // Connect to best DataNode for desired Block, with potential offset\n@@ -686,10 +683,7 @@ public void accept(ByteBuffer k, Object v) {\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n-    if (blockReader != null) {\n-      blockReader.close();\n-      blockReader = null;\n-    }\n+    closeCurrentBlockReader();\n     super.close();\n   }\n \n@@ -1649,6 +1643,7 @@ private void closeCurrentBlockReader() {\n       DFSClient.LOG.error(\"error closing blockReader\", e);\n     }\n     blockReader = null;\n+    blockEnd = -1;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "618f0407dfe968a1c73a10094d36e3b13af07761",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java?ref=a9dc5cd7069f721e8c55794b877026ba02537167",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java",
                "patch": "@@ -369,4 +369,34 @@ public void testNoFadviseAfterWriteThenRead() throws Exception {\n       }\n     }\n   }\n+\n+  @Test(timeout=120000)\n+  public void testSeekAfterSetDropBehind() throws Exception {\n+    // start a cluster\n+    LOG.info(\"testSeekAfterSetDropBehind\");\n+    Configuration conf = new HdfsConfiguration();\n+    MiniDFSCluster cluster = null;\n+    String TEST_PATH = \"/test\";\n+    int TEST_PATH_LEN = MAX_TEST_FILE_LEN;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1)\n+          .build();\n+      cluster.waitActive();\n+      FileSystem fs = cluster.getFileSystem();\n+      createHdfsFile(fs, new Path(TEST_PATH), TEST_PATH_LEN, false);\n+      // verify that we can seek after setDropBehind\n+      FSDataInputStream fis = fs.open(new Path(TEST_PATH));\n+      try {\n+        Assert.assertTrue(fis.read() != -1); // create BlockReader\n+        fis.setDropBehind(false); // clear BlockReader\n+        fis.seek(2); // seek\n+      } finally {\n+        fis.close();\n+      }\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java",
                "sha": "709554a0ea42f1c241c5773d0efff58e73b98fc1",
                "status": "modified"
            }
        ],
        "message": "HDFS-7744. Fix potential NPE in DFSInputStream after setDropBehind or setReadahead is called (cmccabe)",
        "parent": "https://github.com/apache/hadoop/commit/260b5e32c427d54c8c74b9f84432700317d1f282",
        "patched_files": [
            "DFSInputStream.java",
            "CHANGES.java",
            "CachingStrategy.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java",
            "TestCachingStrategy.java"
        ]
    },
    "hadoop_aa5ec85": {
        "bug_id": "hadoop_aa5ec85",
        "commit": "https://github.com/apache/hadoop/commit/aa5ec85f7fd2dc6ac568a88716109bab8df8be19",
        "file": [
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/aa5ec85f7fd2dc6ac568a88716109bab8df8be19/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java?ref=aa5ec85f7fd2dc6ac568a88716109bab8df8be19",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java",
                "patch": "@@ -134,11 +134,13 @@ public LocatedBlockBuilder newLocatedBlocks(int maxValue) {\n   class ProvidedBlocksBuilder extends LocatedBlockBuilder {\n \n     private ShadowDatanodeInfoWithStorage pending;\n+    private boolean hasProvidedLocations;\n \n     ProvidedBlocksBuilder(int maxBlocks) {\n       super(maxBlocks);\n       pending = new ShadowDatanodeInfoWithStorage(\n           providedDescriptor, storageId);\n+      hasProvidedLocations = false;\n     }\n \n     @Override\n@@ -154,6 +156,7 @@ LocatedBlock newLocatedBlock(ExtendedBlock eb,\n         types[i] = storages[i].getStorageType();\n         if (StorageType.PROVIDED.equals(storages[i].getStorageType())) {\n           locs[i] = pending;\n+          hasProvidedLocations = true;\n         } else {\n           locs[i] = new DatanodeInfoWithStorage(\n               storages[i].getDatanodeDescriptor(), sids[i], types[i]);\n@@ -165,25 +168,28 @@ LocatedBlock newLocatedBlock(ExtendedBlock eb,\n     @Override\n     LocatedBlocks build(DatanodeDescriptor client) {\n       // TODO: to support multiple provided storages, need to pass/maintain map\n-      // set all fields of pending DatanodeInfo\n-      List<String> excludedUUids = new ArrayList<String>();\n-      for (LocatedBlock b: blocks) {\n-        DatanodeInfo[] infos = b.getLocations();\n-        StorageType[] types = b.getStorageTypes();\n-\n-        for (int i = 0; i < types.length; i++) {\n-          if (!StorageType.PROVIDED.equals(types[i])) {\n-            excludedUUids.add(infos[i].getDatanodeUuid());\n+      if (hasProvidedLocations) {\n+        // set all fields of pending DatanodeInfo\n+        List<String> excludedUUids = new ArrayList<String>();\n+        for (LocatedBlock b : blocks) {\n+          DatanodeInfo[] infos = b.getLocations();\n+          StorageType[] types = b.getStorageTypes();\n+\n+          for (int i = 0; i < types.length; i++) {\n+            if (!StorageType.PROVIDED.equals(types[i])) {\n+              excludedUUids.add(infos[i].getDatanodeUuid());\n+            }\n           }\n         }\n-      }\n \n-      DatanodeDescriptor dn = providedDescriptor.choose(client, excludedUUids);\n-      if (dn == null) {\n-        dn = providedDescriptor.choose(client);\n+        DatanodeDescriptor dn =\n+                providedDescriptor.choose(client, excludedUUids);\n+        if (dn == null) {\n+          dn = providedDescriptor.choose(client);\n+        }\n+        pending.replaceInternal(dn);\n       }\n \n-      pending.replaceInternal(dn);\n       return new LocatedBlocks(\n           flen, isUC, blocks, last, lastComplete, feInfo, ecPolicy);\n     }\n@@ -278,7 +284,8 @@ DatanodeStorageInfo createProvidedStorage(DatanodeStorage ds) {\n \n     DatanodeDescriptor choose(DatanodeDescriptor client) {\n       // exact match for now\n-      DatanodeDescriptor dn = dns.get(client.getDatanodeUuid());\n+      DatanodeDescriptor dn = client != null ?\n+              dns.get(client.getDatanodeUuid()) : null;\n       if (null == dn) {\n         dn = chooseRandom();\n       }\n@@ -288,7 +295,8 @@ DatanodeDescriptor choose(DatanodeDescriptor client) {\n     DatanodeDescriptor choose(DatanodeDescriptor client,\n         List<String> excludedUUids) {\n       // exact match for now\n-      DatanodeDescriptor dn = dns.get(client.getDatanodeUuid());\n+      DatanodeDescriptor dn = client != null ?\n+              dns.get(client.getDatanodeUuid()) : null;\n \n       if (null == dn || excludedUUids.contains(client.getDatanodeUuid())) {\n         dn = null;",
                "raw_url": "https://github.com/apache/hadoop/raw/aa5ec85f7fd2dc6ac568a88716109bab8df8be19/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/ProvidedStorageMap.java",
                "sha": "518b7e9ccfa14c0aeac3f0a6f4e2d7a9ce03c497",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/aa5ec85f7fd2dc6ac568a88716109bab8df8be19/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeProvidedImplementation.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeProvidedImplementation.java?ref=aa5ec85f7fd2dc6ac568a88716109bab8df8be19",
                "deletions": 17,
                "filename": "hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeProvidedImplementation.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.StorageType;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockFormatProvider;\n@@ -69,6 +70,10 @@\n   final Path BLOCKFILE = new Path(NNDIRPATH, \"blocks.csv\");\n   final String SINGLEUSER = \"usr1\";\n   final String SINGLEGROUP = \"grp1\";\n+  private final int numFiles = 10;\n+  private final String filePrefix = \"file\";\n+  private final String fileSuffix = \".dat\";\n+  private final int baseFileLen = 1024;\n \n   Configuration conf;\n   MiniDFSCluster cluster;\n@@ -114,15 +119,16 @@ public void setSeed() throws Exception {\n     }\n \n     // create 10 random files under BASE\n-    for (int i=0; i < 10; i++) {\n-      File newFile = new File(new Path(NAMEPATH, \"file\" + i).toUri());\n+    for (int i=0; i < numFiles; i++) {\n+      File newFile = new File(\n+          new Path(NAMEPATH, filePrefix + i + fileSuffix).toUri());\n       if(!newFile.exists()) {\n         try {\n           LOG.info(\"Creating \" + newFile.toString());\n           newFile.createNewFile();\n           Writer writer = new OutputStreamWriter(\n               new FileOutputStream(newFile.getAbsolutePath()), \"utf-8\");\n-          for(int j=0; j < 10*i; j++) {\n+          for(int j=0; j < baseFileLen*i; j++) {\n             writer.write(\"0\");\n           }\n           writer.flush();\n@@ -161,29 +167,30 @@ void createImage(TreeWalk t, Path out,\n \n   void startCluster(Path nspath, int numDatanodes,\n       StorageType[] storageTypes,\n-      StorageType[][] storageTypesPerDatanode)\n+      StorageType[][] storageTypesPerDatanode,\n+      boolean doFormat)\n       throws IOException {\n     conf.set(DFS_NAMENODE_NAME_DIR_KEY, nspath.toString());\n \n     if (storageTypesPerDatanode != null) {\n       cluster = new MiniDFSCluster.Builder(conf)\n-          .format(false)\n-          .manageNameDfsDirs(false)\n+          .format(doFormat)\n+          .manageNameDfsDirs(doFormat)\n           .numDataNodes(numDatanodes)\n           .storageTypes(storageTypesPerDatanode)\n           .build();\n     } else if (storageTypes != null) {\n       cluster = new MiniDFSCluster.Builder(conf)\n-          .format(false)\n-          .manageNameDfsDirs(false)\n+          .format(doFormat)\n+          .manageNameDfsDirs(doFormat)\n           .numDataNodes(numDatanodes)\n           .storagesPerDatanode(storageTypes.length)\n           .storageTypes(storageTypes)\n           .build();\n     } else {\n       cluster = new MiniDFSCluster.Builder(conf)\n-          .format(false)\n-          .manageNameDfsDirs(false)\n+          .format(doFormat)\n+          .manageNameDfsDirs(doFormat)\n           .numDataNodes(numDatanodes)\n           .build();\n     }\n@@ -195,7 +202,8 @@ public void testLoadImage() throws Exception {\n     final long seed = r.nextLong();\n     LOG.info(\"NAMEPATH: \" + NAMEPATH);\n     createImage(new RandomTreeWalk(seed), NNDIRPATH, FixedBlockResolver.class);\n-    startCluster(NNDIRPATH, 0, new StorageType[] {StorageType.PROVIDED}, null);\n+    startCluster(NNDIRPATH, 0, new StorageType[] {StorageType.PROVIDED},\n+        null, false);\n \n     FileSystem fs = cluster.getFileSystem();\n     for (TreePath e : new RandomTreeWalk(seed)) {\n@@ -220,7 +228,8 @@ public void testBlockLoad() throws Exception {\n         SingleUGIResolver.class, UGIResolver.class);\n     createImage(new FSTreeWalk(NAMEPATH, conf), NNDIRPATH,\n         FixedBlockResolver.class);\n-    startCluster(NNDIRPATH, 1, new StorageType[] {StorageType.PROVIDED}, null);\n+    startCluster(NNDIRPATH, 1, new StorageType[] {StorageType.PROVIDED},\n+        null, false);\n   }\n \n   @Test(timeout=500000)\n@@ -232,10 +241,10 @@ public void testDefaultReplication() throws Exception {\n     // make the last Datanode with only DISK\n     startCluster(NNDIRPATH, 3, null,\n         new StorageType[][] {\n-          {StorageType.PROVIDED},\n-          {StorageType.PROVIDED},\n-          {StorageType.DISK}}\n-        );\n+            {StorageType.PROVIDED},\n+            {StorageType.PROVIDED},\n+            {StorageType.DISK}},\n+        false);\n     // wait for the replication to finish\n     Thread.sleep(50000);\n \n@@ -290,7 +299,8 @@ public void testBlockRead() throws Exception {\n         FsUGIResolver.class, UGIResolver.class);\n     createImage(new FSTreeWalk(NAMEPATH, conf), NNDIRPATH,\n         FixedBlockResolver.class);\n-    startCluster(NNDIRPATH, 3, new StorageType[] {StorageType.PROVIDED}, null);\n+    startCluster(NNDIRPATH, 3, new StorageType[] {StorageType.PROVIDED},\n+        null, false);\n     FileSystem fs = cluster.getFileSystem();\n     Thread.sleep(2000);\n     int count = 0;\n@@ -342,4 +352,30 @@ public void testBlockRead() throws Exception {\n       }\n     }\n   }\n+\n+  private BlockLocation[] createFile(Path path, short replication,\n+      long fileLen, long blockLen) throws IOException {\n+    FileSystem fs = cluster.getFileSystem();\n+    //create a sample file that is not provided\n+    DFSTestUtil.createFile(fs, path, false, (int) blockLen,\n+        fileLen, blockLen, replication, 0, true);\n+    return fs.getFileBlockLocations(path, 0, fileLen);\n+  }\n+\n+  @Test\n+  public void testClusterWithEmptyImage() throws IOException {\n+    // start a cluster with 2 datanodes without any provided storage\n+    startCluster(NNDIRPATH, 2, null,\n+        new StorageType[][] {\n+            {StorageType.DISK},\n+            {StorageType.DISK}},\n+        true);\n+    assertTrue(cluster.isClusterUp());\n+    assertTrue(cluster.isDataNodeUp());\n+\n+    BlockLocation[] locations = createFile(new Path(\"/testFile1.dat\"),\n+        (short) 2, 1024*1024, 1024*1024);\n+    assertEquals(1, locations.length);\n+    assertEquals(2, locations[0].getHosts().length);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/aa5ec85f7fd2dc6ac568a88716109bab8df8be19/hadoop-tools/hadoop-fs2img/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeProvidedImplementation.java",
                "sha": "5062439fcb75e50df27c2a0da1d079ab0482b600",
                "status": "modified"
            }
        ],
        "message": "HDFS-11663. [READ] Fix NullPointerException in ProvidedBlocksBuilder",
        "parent": "https://github.com/apache/hadoop/commit/1108cb76917debf0a8541d5130e015883eb521af",
        "patched_files": [
            "ProvidedStorageMap.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeProvidedImplementation.java"
        ]
    },
    "hadoop_aa7614c": {
        "bug_id": "hadoop_aa7614c",
        "commit": "https://github.com/apache/hadoop/commit/aa7614c5f36060a3630b742590845da7efd85caa",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/aa7614c5f36060a3630b742590845da7efd85caa/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java?ref=aa7614c5f36060a3630b742590845da7efd85caa",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.ozone.container.common.states.endpoint;\n \n import com.google.common.base.Preconditions;\n+import com.google.protobuf.Descriptors;\n import com.google.protobuf.GeneratedMessage;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n@@ -125,9 +126,14 @@ public void setDatanodeDetailsProto(DatanodeDetailsProto\n    */\n   private void addReports(SCMHeartbeatRequestProto.Builder requestBuilder) {\n     for (GeneratedMessage report : context.getAllAvailableReports()) {\n-      requestBuilder.setField(\n-          SCMHeartbeatRequestProto.getDescriptor().findFieldByName(\n-              report.getDescriptorForType().getName()), report);\n+      String reportName = report.getDescriptorForType().getFullName();\n+      for (Descriptors.FieldDescriptor descriptor :\n+          SCMHeartbeatRequestProto.getDescriptor().getFields()) {\n+        String heartbeatFieldName = descriptor.getMessageType().getFullName();\n+        if (heartbeatFieldName.equals(reportName)) {\n+          requestBuilder.setField(descriptor, report);\n+        }\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/aa7614c5f36060a3630b742590845da7efd85caa/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/HeartbeatEndpointTask.java",
                "sha": "1ee6375a562f0f7c5f34e64a92168aa6227eeda3",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hadoop/blob/aa7614c5f36060a3630b742590845da7efd85caa/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java?ref=aa7614c5f36060a3630b742590845da7efd85caa",
                "deletions": 0,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java",
                "patch": "@@ -18,13 +18,25 @@\n package org.apache.hadoop.ozone.container.common.report;\n \n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import com.google.protobuf.Descriptors;\n import com.google.protobuf.GeneratedMessage;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.proto\n+    .StorageContainerDatanodeProtocolProtos.ContainerReportsProto;\n+import org.apache.hadoop.hdds.protocol.proto\n+    .StorageContainerDatanodeProtocolProtos.NodeReportProto;\n+import org.apache.hadoop.hdds.protocol.proto\n+    .StorageContainerDatanodeProtocolProtos.SCMHeartbeatRequestProto;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.util.concurrent.HadoopExecutors;\n import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n+import java.util.Random;\n+import java.util.UUID;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeUnit;\n \n@@ -103,4 +115,71 @@ public void testPublishReport() throws InterruptedException {\n \n   }\n \n+  @Test\n+  public void testAddingReportToHeartbeat() {\n+    Configuration conf = new OzoneConfiguration();\n+    ReportPublisherFactory factory = new ReportPublisherFactory(conf);\n+    ReportPublisher nodeReportPublisher = factory.getPublisherFor(\n+        NodeReportProto.class);\n+    ReportPublisher containerReportPubliser = factory.getPublisherFor(\n+        ContainerReportsProto.class);\n+    GeneratedMessage nodeReport = nodeReportPublisher.getReport();\n+    GeneratedMessage containerReport = containerReportPubliser.getReport();\n+    SCMHeartbeatRequestProto.Builder heartbeatBuilder =\n+        SCMHeartbeatRequestProto.newBuilder();\n+    heartbeatBuilder.setDatanodeDetails(\n+        getDatanodeDetails().getProtoBufMessage());\n+    addReport(heartbeatBuilder, nodeReport);\n+    addReport(heartbeatBuilder, containerReport);\n+    SCMHeartbeatRequestProto heartbeat = heartbeatBuilder.build();\n+    Assert.assertTrue(heartbeat.hasNodeReport());\n+    Assert.assertTrue(heartbeat.hasContainerReport());\n+  }\n+\n+  /**\n+   * Get a datanode details.\n+   *\n+   * @return DatanodeDetails\n+   */\n+  private static DatanodeDetails getDatanodeDetails() {\n+    String uuid = UUID.randomUUID().toString();\n+    Random random = new Random();\n+    String ipAddress =\n+        random.nextInt(256) + \".\" + random.nextInt(256) + \".\" + random\n+            .nextInt(256) + \".\" + random.nextInt(256);\n+\n+    DatanodeDetails.Port containerPort = DatanodeDetails.newPort(\n+        DatanodeDetails.Port.Name.STANDALONE, 0);\n+    DatanodeDetails.Port ratisPort = DatanodeDetails.newPort(\n+        DatanodeDetails.Port.Name.RATIS, 0);\n+    DatanodeDetails.Port restPort = DatanodeDetails.newPort(\n+        DatanodeDetails.Port.Name.REST, 0);\n+    DatanodeDetails.Builder builder = DatanodeDetails.newBuilder();\n+    builder.setUuid(uuid)\n+        .setHostName(\"localhost\")\n+        .setIpAddress(ipAddress)\n+        .addPort(containerPort)\n+        .addPort(ratisPort)\n+        .addPort(restPort);\n+    return builder.build();\n+  }\n+\n+  /**\n+   * Adds the report to heartbeat.\n+   *\n+   * @param requestBuilder builder to which the report has to be added.\n+   * @param report the report to be added.\n+   */\n+  private static void addReport(SCMHeartbeatRequestProto.Builder requestBuilder,\n+                          GeneratedMessage report) {\n+    String reportName = report.getDescriptorForType().getFullName();\n+    for (Descriptors.FieldDescriptor descriptor :\n+        SCMHeartbeatRequestProto.getDescriptor().getFields()) {\n+      String heartbeatFieldName = descriptor.getMessageType().getFullName();\n+      if (heartbeatFieldName.equals(reportName)) {\n+        requestBuilder.setField(descriptor, report);\n+      }\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/aa7614c5f36060a3630b742590845da7efd85caa/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/report/TestReportPublisher.java",
                "sha": "5fd9cf6047998dc920f4d62aa9fc509d5101f282",
                "status": "modified"
            }
        ],
        "message": "HDDS-158. DatanodeStateMachine endPoint task throws NullPointerException. Contributed by Nanda Kumar.",
        "parent": "https://github.com/apache/hadoop/commit/b3612dd90c419c6ac5d59ab0793b4adec40ef5c6",
        "patched_files": [
            "ReportPublisher.java",
            "HeartbeatEndpointTask.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestReportPublisher.java"
        ]
    },
    "hadoop_aac5472": {
        "bug_id": "hadoop_aac5472",
        "commit": "https://github.com/apache/hadoop/commit/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -100,6 +100,9 @@ Trunk (unreleased changes)\n     HADOOP-7131. Exceptions thrown by Text methods should include the causing\n     exception. (Uma Maheswara Rao G via todd)\n \n+    HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased().\n+    (Kan Zhang via jitendra)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/CHANGES.txt",
                "sha": "ead2d3f88a9ba1f5b25e5526666fa9d823086f02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/UserGroupInformation.java?ref=aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -811,8 +811,8 @@ private boolean hasSufficientTimeElapsed(long now) {\n    * Did the login happen via keytab\n    * @return true or false\n    */\n-  public synchronized static boolean isLoginKeytabBased() {\n-    return loginUser.isKeytab;\n+  public synchronized static boolean isLoginKeytabBased() throws IOException {\n+    return getLoginUser().isKeytab;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "085ce61719eefec5cd06738b846867a1335f08cd",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased(). Contributed by Kan Zhang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1079068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/69fe37a007045c3a3cf3b2b410e1ad14717fdb76",
        "patched_files": [
            "UserGroupInformation.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_ab11085": {
        "bug_id": "hadoop_ab11085",
        "commit": "https://github.com/apache/hadoop/commit/ab11085b81353e1617875deb10f3c8e2a8b91a1e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/ab11085b81353e1617875deb10f3c8e2a8b91a1e/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=ab11085b81353e1617875deb10f3c8e2a8b91a1e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -626,6 +626,10 @@ Release 2.7.2 - UNRELEASED\n     MAPREDUCE-5982. Task attempts that fail from the ASSIGNED state can\n     disappear (Chang Li via jlowe)\n \n+    MAPREDUCE-6492. AsyncDispatcher exit with NPE on\n+    TaskAttemptImpl#sendJHStartEventForAssignedFailTask (Bibin A Chundatt via\n+    jlowe)\n+\n Release 2.7.1 - 2015-07-06 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ab11085b81353e1617875deb10f3c8e2a8b91a1e/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "da5ee0bd41ee602421ac0bd497c684ec374cb705",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ab11085b81353e1617875deb10f3c8e2a8b91a1e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java?ref=ab11085b81353e1617875deb10f3c8e2a8b91a1e",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
                "patch": "@@ -1486,7 +1486,9 @@ private static JobCounterUpdateEvent createJobCounterUpdateEventTAKilled(\n \n   private static void\n       sendJHStartEventForAssignedFailTask(TaskAttemptImpl taskAttempt) {\n-    TaskAttemptContainerLaunchedEvent event;\n+    if (null == taskAttempt.container) {\n+      return;\n+    }\n     taskAttempt.launchTime = taskAttempt.clock.getTime();\n \n     InetSocketAddress nodeHttpInetAddr =",
                "raw_url": "https://github.com/apache/hadoop/raw/ab11085b81353e1617875deb10f3c8e2a8b91a1e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
                "sha": "db4f585a8d3a414d6c3b6646f4a6d4dfed9622b0",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/ab11085b81353e1617875deb10f3c8e2a8b91a1e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java?ref=ab11085b81353e1617875deb10f3c8e2a8b91a1e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "patch": "@@ -774,6 +774,14 @@ public void testAppDiognosticEventOnUnassignedTask() throws Exception {\n     assertFalse(\n         \"InternalError occurred trying to handle TA_DIAGNOSTICS_UPDATE on assigned task\",\n         eventHandler.internalError);\n+    try {\n+      taImpl.handle(new TaskAttemptEvent(attemptId,\n+          TaskAttemptEventType.TA_KILL));\n+      Assert.assertTrue(\"No exception on UNASSIGNED STATE KILL event\", true);\n+    } catch (Exception e) {\n+      Assert.assertFalse(\n+          \"Exception not expected for UNASSIGNED STATE KILL event\", true);\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/ab11085b81353e1617875deb10f3c8e2a8b91a1e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "sha": "1ed8098988a56397d0ea568072db917ee5b0e30d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6492. AsyncDispatcher exit with NPE on TaskAttemptImpl#sendJHStartEventForAssignedFailTask. Contributed by Bibin A Chundatt",
        "parent": "https://github.com/apache/hadoop/commit/e5992ef4df63fbc6a6b8e357b32c647e7837c662",
        "patched_files": [
            "TaskAttempt.java",
            "TaskAttemptImpl.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskAttempt.java"
        ]
    },
    "hadoop_ac5ae00": {
        "bug_id": "hadoop_ac5ae00",
        "commit": "https://github.com/apache/hadoop/commit/ac5ae0065a127ac150a887fa6c6f3cffd86ef733",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/ac5ae0065a127ac150a887fa6c6f3cffd86ef733/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=ac5ae0065a127ac150a887fa6c6f3cffd86ef733",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -2282,12 +2282,14 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n       if (memBlockInfo.getGenerationStamp() != diskGS) {\n         File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, \n             memBlockInfo.getGenerationStamp());\n-        if (memMetaFile.exists()) {\n-          if (memMetaFile.compareTo(diskMetaFile) != 0) {\n-            LOG.warn(\"Metadata file in memory \"\n-                + memMetaFile.getAbsolutePath()\n-                + \" does not match file found by scan \"\n-                + (diskMetaFile == null? null: diskMetaFile.getAbsolutePath()));\n+        if (fileIoProvider.exists(vol, memMetaFile)) {\n+          String warningPrefix = \"Metadata file in memory \"\n+              + memMetaFile.getAbsolutePath()\n+              + \" does not match file found by scan \";\n+          if (!diskMetaFileExists) {\n+            LOG.warn(warningPrefix + \"null\");\n+          } else if (memMetaFile.compareTo(diskMetaFile) != 0) {\n+            LOG.warn(warningPrefix + diskMetaFile.getAbsolutePath());\n           }\n         } else {\n           // Metadata file corresponding to block in memory is missing",
                "raw_url": "https://github.com/apache/hadoop/raw/ac5ae0065a127ac150a887fa6c6f3cffd86ef733/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "aff19ce97e0c008722ad6a33bd6a38ae91220d76",
                "status": "modified"
            }
        ],
        "message": "HDFS-11476. Fix NPE in FsDatasetImpl#checkAndUpdate. Contributed by Xiaobing Zhou.",
        "parent": "https://github.com/apache/hadoop/commit/2148b83993fd8ce73bcbc7677c57ee5028a59cd4",
        "patched_files": [
            "FsDatasetImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_acdf911": {
        "bug_id": "hadoop_acdf911",
        "commit": "https://github.com/apache/hadoop/commit/acdf911c014e6820866f3451c7ae09163119337c",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/acdf911c014e6820866f3451c7ae09163119337c/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java?ref=acdf911c014e6820866f3451c7ae09163119337c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
                "patch": "@@ -1635,6 +1635,7 @@ private ContentSummary aggregateContentSummary(\n     long quota = 0;\n     long spaceConsumed = 0;\n     long spaceQuota = 0;\n+    String ecPolicy = \"\";\n \n     for (ContentSummary summary : summaries) {\n       length += summary.getLength();\n@@ -1643,6 +1644,11 @@ private ContentSummary aggregateContentSummary(\n       quota += summary.getQuota();\n       spaceConsumed += summary.getSpaceConsumed();\n       spaceQuota += summary.getSpaceQuota();\n+      // We return from the first response as we assume that the EC policy\n+      // of each sub-cluster is same.\n+      if (ecPolicy.isEmpty()) {\n+        ecPolicy = summary.getErasureCodingPolicy();\n+      }\n     }\n \n     ContentSummary ret = new ContentSummary.Builder()\n@@ -1652,6 +1658,7 @@ private ContentSummary aggregateContentSummary(\n         .quota(quota)\n         .spaceConsumed(spaceConsumed)\n         .spaceQuota(spaceQuota)\n+        .erasureCodingPolicy(ecPolicy)\n         .build();\n     return ret;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/acdf911c014e6820866f3451c7ae09163119337c/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterClientProtocol.java",
                "sha": "2a8714601138c3f0ec98b2610544496c27413e23",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/acdf911c014e6820866f3451c7ae09163119337c/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java?ref=acdf911c014e6820866f3451c7ae09163119337c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.DirectoryListing;\n import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;\n@@ -229,6 +230,21 @@ public void testProxyRenameFiles() throws IOException, InterruptedException {\n     testRename2(getRouterContext(), filename1, renamedFile, false);\n   }\n \n+  @Test\n+  public void testGetContentSummaryEc() throws Exception {\n+    DistributedFileSystem routerDFS =\n+        (DistributedFileSystem) getRouterFileSystem();\n+    Path dir = new Path(\"/\");\n+    String expectedECPolicy = \"RS-6-3-1024k\";\n+    try {\n+      routerDFS.setErasureCodingPolicy(dir, expectedECPolicy);\n+      assertEquals(expectedECPolicy,\n+          routerDFS.getContentSummary(dir).getErasureCodingPolicy());\n+    } finally {\n+      routerDFS.unsetErasureCodingPolicy(dir);\n+    }\n+  }\n+\n   @Test\n   public void testSubclusterDown() throws Exception {\n     final int totalFiles = 6;",
                "raw_url": "https://github.com/apache/hadoop/raw/acdf911c014e6820866f3451c7ae09163119337c/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterRpcMultiDestination.java",
                "sha": "3d941bbf4b907f42ac6641fb7c9e7227bcc3df02",
                "status": "modified"
            }
        ],
        "message": "HDFS-14224. RBF: NPE in getContentSummary() for getEcPolicy() in case of multiple destinations. Contributed by Ayush Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/8b9b58b58ac91599ada33e6501995d06d8758a3f",
        "patched_files": [
            "RouterClientProtocol.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRouterRpcMultiDestination.java"
        ]
    },
    "hadoop_ad53c52": {
        "bug_id": "hadoop_ad53c52",
        "commit": "https://github.com/apache/hadoop/commit/ad53c520630847834b3e68b7f41aa88ee10b3300",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ad53c520630847834b3e68b7f41aa88ee10b3300/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=ad53c520630847834b3e68b7f41aa88ee10b3300",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -1000,6 +1000,8 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-12600. FileContext and AbstractFileSystem should be annotated as a\n     Stable interface. (cnauroth)\n \n+    HADOOP-12618. Fix NPE in TestSequenceFile. (Brahma Reddy Battula via umamahesh)\n+\n   OPTIMIZATIONS\n \n     HADOOP-11785. Reduce the number of listStatus operation in distcp",
                "raw_url": "https://github.com/apache/hadoop/raw/ad53c520630847834b3e68b7f41aa88ee10b3300/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "6bd626702fe211e4f52ee978cf7b9c8d292a9532",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/ad53c520630847834b3e68b7f41aa88ee10b3300/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java?ref=ad53c520630847834b3e68b7f41aa88ee10b3300",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java",
                "patch": "@@ -130,6 +130,7 @@ public void compressedSeqFileTest(CompressionCodec codec) throws Exception {\n     }\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   private void writeTest(FileSystem fs, int count, int seed, Path file, \n                                 CompressionType compressionType, CompressionCodec codec)\n     throws IOException {\n@@ -150,6 +151,7 @@ private void writeTest(FileSystem fs, int count, int seed, Path file,\n     writer.close();\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   private void readTest(FileSystem fs, int count, int seed, Path file)\n     throws IOException {\n     LOG.debug(\"reading \" + count + \" records\");\n@@ -216,6 +218,7 @@ private void sortTest(FileSystem fs, int count, int megabytes,\n     LOG.info(\"done sorting \" + count + \" debug\");\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   private void checkSort(FileSystem fs, int count, int seed, Path file)\n     throws IOException {\n     LOG.info(\"sorting \" + count + \" records in memory for debug\");\n@@ -253,6 +256,7 @@ private void checkSort(FileSystem fs, int count, int seed, Path file)\n     LOG.debug(\"sucessfully checked \" + count + \" records\");\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   private void mergeTest(FileSystem fs, int count, int seed, Path file, \n                                 CompressionType compressionType,\n                                 boolean fast, int factor, int megabytes)\n@@ -375,6 +379,7 @@ public void testSequenceFileMetadata() throws Exception {\n   }\n   \n   \n+  @SuppressWarnings(\"deprecation\")\n   private SequenceFile.Metadata readMetadata(FileSystem fs, Path file)\n     throws IOException {\n     LOG.info(\"reading file: \" + file.toString());\n@@ -384,6 +389,7 @@ public void testSequenceFileMetadata() throws Exception {\n     return meta;\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   private void writeMetadataTest(FileSystem fs, int count, int seed, Path file, \n                                         CompressionType compressionType, CompressionCodec codec, SequenceFile.Metadata metadata)\n     throws IOException {\n@@ -413,6 +419,7 @@ private void sortMetadataTest(FileSystem fs, Path unsortedFile, Path sortedFile,\n     sorter.sort(new Path[] { unsortedFile }, sortedFile, false);\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testClose() throws IOException {\n     Configuration conf = new Configuration();\n@@ -470,6 +477,7 @@ public void testClose() throws IOException {\n    * Test that makes sure the FileSystem passed to createWriter\n    * @throws Exception\n    */\n+  @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testCreateUsesFsArg() throws Exception {\n     FileSystem fs = FileSystem.getLocal(conf);\n@@ -499,6 +507,7 @@ public boolean isClosed() {\n     }\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testCloseForErroneousSequenceFile()\n     throws IOException {\n@@ -555,6 +564,7 @@ public void testInitZeroLengthSequenceFile() throws IOException {\n    * already created\n    * @throws IOException\n    */\n+  @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testCreateWriterOnExistingFile() throws IOException {\n     Configuration conf = new Configuration();\n@@ -568,6 +578,7 @@ public void testCreateWriterOnExistingFile() throws IOException {\n         CompressionType.NONE, null, new Metadata());\n   }\n \n+  @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testRecursiveSeqFileCreate() throws IOException {\n     FileSystem fs = FileSystem.getLocal(conf);\n@@ -661,7 +672,7 @@ public static void main(String[] args) throws Exception {\n     Path file = null;\n     int seed = new Random().nextInt();\n \n-    String usage = \"Usage: SequenceFile \" +\n+    String usage = \"Usage: testsequencefile \" +\n       \"[-count N] \" + \n       \"[-seed #] [-check] [-compressType <NONE|RECORD|BLOCK>] \" + \n       \"-codec <compressionCodec> \" + \n@@ -751,7 +762,9 @@ public static void main(String[] args) throws Exception {\n         test.checkSort(fs, count, seed, file);\n       }\n     } finally {\n-      fs.close();\n+      if (fs != null) {\n+        fs.close();\n+      }\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/ad53c520630847834b3e68b7f41aa88ee10b3300/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestSequenceFile.java",
                "sha": "4cb4e130692c786fb561c125fc56780437061bc1",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12618. Fix NPE in TestSequenceFile. (Brahma Reddy Battula via umamahesh)",
        "parent": "https://github.com/apache/hadoop/commit/a5e2e1ecb06a3942903cb79f61f0f4bb02480f19",
        "patched_files": [
            "SequenceFile.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSequenceFile.java"
        ]
    },
    "hadoop_adca1a7": {
        "bug_id": "hadoop_adca1a7",
        "commit": "https://github.com/apache/hadoop/commit/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -1301,6 +1301,10 @@ private CSAssignment allocateContainerOnSingleNode(\n     if (reservedContainer != null) {\n       FiCaSchedulerApp reservedApplication = getCurrentAttemptForContainer(\n           reservedContainer.getContainerId());\n+      if (reservedApplication == null) {\n+        LOG.error(\"Trying to schedule for a finished app, please double check.\");\n+        return null;\n+      }\n \n       // Try to fulfill the reservation\n       LOG.info(",
                "raw_url": "https://github.com/apache/hadoop/raw/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "8de363140fb8e943f5b5590765cf76154d2c58bd",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -1201,7 +1201,14 @@ public boolean accept(Resource cluster,\n             allocation.getSchedulingMode(), null);\n \n         // Deduct resources that we can release\n-        Resource usedResource = Resources.clone(getUser(username).getUsed(p));\n+        User user = getUser(username);\n+        if (user == null) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"User \" + username + \" has been removed!\");\n+          }\n+          return false;\n+        }\n+        Resource usedResource = Resources.clone(user.getUsed(p));\n         Resources.subtractFrom(usedResource,\n             request.getTotalReleasedResource());\n \n@@ -1406,6 +1413,12 @@ Resource computeUserLimitAndSetHeadroom(FiCaSchedulerApp application,\n       SchedulingMode schedulingMode, Resource userLimit) {\n     String user = application.getUser();\n     User queueUser = getUser(user);\n+    if (queueUser == null) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"User \" + user + \" has been removed!\");\n+      }\n+      return Resources.none();\n+    }\n \n     // Compute user limit respect requested labels,\n     // TODO, need consider headroom respect labels also\n@@ -1500,6 +1513,12 @@ protected boolean canAssignToUser(Resource clusterResource,\n     try {\n       readLock.lock();\n       User user = getUser(userName);\n+      if (user == null) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"User \" + userName + \" has been removed!\");\n+        }\n+        return false;\n+      }\n \n       currentResourceLimits.setAmountNeededUnreserve(Resources.none());\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "ac1a26ccef2581dbba85444aa927b65a873d2aba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -322,6 +322,11 @@ private boolean commonCheckContainerAllocation(\n     RMContainer reservedContainerOnNode =\n         schedulerContainer.getSchedulerNode().getReservedContainer();\n     if (reservedContainerOnNode != null) {\n+      // adding NP check as this proposal could not be allocated from reserved\n+      // container in async-scheduling mode\n+      if (allocation.getAllocateFromReservedContainer() == null) {\n+        return false;\n+      }\n       RMContainer fromReservedContainer =\n           allocation.getAllocateFromReservedContainer().getRmContainer();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "776a7e98af299a70cf9adfae8e6a813b9271c8e3",
                "status": "modified"
            }
        ],
        "message": "YARN-7591. NPE in async-scheduling mode of CapacityScheduler. (Tao Yang via wangda)\n\nChange-Id: I46689e530550ee0a6ac7a29786aab2cc1bdf314f",
        "parent": "https://github.com/apache/hadoop/commit/a8316df8c05a7b3d1a5577174b838711a49ef971",
        "patched_files": [
            "LeafQueue.java",
            "CapacityScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLeafQueue.java",
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_ade6d9a": {
        "bug_id": "hadoop_ade6d9a",
        "commit": "https://github.com/apache/hadoop/commit/ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -846,6 +846,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-3716. Purger should remove stale fsimage ckpt files\n     (J.Andreina via vinayakumarb)\n \n+    HDFS-8463. Calling DFSInputStream.seekToNewSource just after stream creation\n+    causes NullPointerException (Masatake Iwasaki via kihwal)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "bb65105c5eb991745e736bf038465b30f3c17ab8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -1533,6 +1533,9 @@ private boolean seekToBlockSource(long targetPos)\n    */\n   @Override\n   public synchronized boolean seekToNewSource(long targetPos) throws IOException {\n+    if (currentNode == null) {\n+      return seekToBlockSource(targetPos);\n+    }\n     boolean markedDead = deadNodes.containsKey(currentNode);\n     addToDeadNodes(currentNode);\n     DatanodeInfo oldNode = currentNode;",
                "raw_url": "https://github.com/apache/hadoop/raw/ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "6563d7b87c4a82711972ac7c8a7cb30931cbd4c8",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSInputStream.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSInputStream.java?ref=ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSInputStream.java",
                "patch": "@@ -18,6 +18,8 @@\n package org.apache.hadoop.hdfs;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n import static org.hamcrest.CoreMatchers.equalTo;\n \n import java.io.File;\n@@ -28,6 +30,7 @@\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;\n+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.net.unix.DomainSocket;\n import org.apache.hadoop.net.unix.TemporarySocketDirectory;\n import org.junit.Assume;\n@@ -111,4 +114,26 @@ public void testSkipWithLocalBlockReader() throws IOException {\n     }\n   }\n \n+  @Test(timeout=60000)\n+  public void testSeekToNewSource() throws IOException {\n+    Configuration conf = new Configuration();\n+    MiniDFSCluster cluster =\n+        new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+    DistributedFileSystem fs = cluster.getFileSystem();\n+    Path path = new Path(\"/testfile\");\n+    DFSTestUtil.createFile(fs, path, 1024, (short) 3, 0);\n+    DFSInputStream fin = fs.dfs.open(\"/testfile\");\n+    try {\n+      fin.seekToNewSource(100);\n+      assertEquals(100, fin.getPos());\n+      DatanodeInfo firstNode = fin.getCurrentDatanode();\n+      assertNotNull(firstNode);\n+      fin.seekToNewSource(100);\n+      assertEquals(100, fin.getPos());\n+      assertFalse(firstNode.equals(fin.getCurrentDatanode()));\n+    } finally {\n+      fin.close();\n+      cluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSInputStream.java",
                "sha": "26412c817c0c460385c2fbbb172015f1d3ef01ab",
                "status": "modified"
            }
        ],
        "message": "HDFS-8463. Calling DFSInputStream.seekToNewSource just after stream creation causes NullPointerException. Contributed by Masatake Iwasaki.",
        "parent": "https://github.com/apache/hadoop/commit/ebd797c48fe236b404cf3a125ac9d1f7714e291e",
        "patched_files": [
            "DFSInputStream.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java"
        ]
    },
    "hadoop_aed836e": {
        "bug_id": "hadoop_aed836e",
        "commit": "https://github.com/apache/hadoop/commit/aed836efbff775d95899d05ff947f1048df8cf19",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java?ref=aed836efbff775d95899d05ff947f1048df8cf19",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.hadoop.yarn.server.federation.policies.FederationPolicyInitializationContext;\n import org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils;\n import org.apache.hadoop.yarn.server.federation.policies.dao.WeightedPolicyInfo;\n+import org.apache.hadoop.yarn.server.federation.policies.exceptions.FederationPolicyException;\n import org.apache.hadoop.yarn.server.federation.policies.exceptions.FederationPolicyInitializationException;\n import org.apache.hadoop.yarn.server.federation.store.records.SubClusterId;\n import org.apache.hadoop.yarn.server.federation.store.records.SubClusterIdInfo;\n@@ -95,7 +96,10 @@ public SubClusterId getHomeSubcluster(\n         }\n       }\n     }\n-\n+    if (chosen == null) {\n+      throw new FederationPolicyException(\n+          \"Zero Active Subcluster with weight 1.\");\n+    }\n     return chosen.toId();\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java",
                "sha": "fa5eb4be2cfd5f5f4cc625d0548b28d66caab925",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java?ref=aed836efbff775d95899d05ff947f1048df8cf19",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java",
                "patch": "@@ -17,6 +17,8 @@\n \n package org.apache.hadoop.yarn.server.federation.policies.router;\n \n+import static org.junit.Assert.fail;\n+\n import java.util.HashMap;\n import java.util.Map;\n \n@@ -103,4 +105,33 @@ public void testLoadIsRespected() throws YarnException {\n     Assert.assertEquals(\"sc05\", chosen.getId());\n   }\n \n+  @Test\n+  public void testIfNoSubclustersWithWeightOne() {\n+    setPolicy(new LoadBasedRouterPolicy());\n+    setPolicyInfo(new WeightedPolicyInfo());\n+    Map<SubClusterIdInfo, Float> routerWeights = new HashMap<>();\n+    Map<SubClusterIdInfo, Float> amrmWeights = new HashMap<>();\n+    // update subcluster with weight 0\n+    SubClusterIdInfo sc = new SubClusterIdInfo(String.format(\"sc%02d\", 0));\n+    SubClusterInfo federationSubClusterInfo = SubClusterInfo.newInstance(\n+        sc.toId(), null, null, null, null, -1, SubClusterState.SC_RUNNING, -1,\n+        generateClusterMetricsInfo(0));\n+    getActiveSubclusters().clear();\n+    getActiveSubclusters().put(sc.toId(), federationSubClusterInfo);\n+    routerWeights.put(sc, 0.0f);\n+    amrmWeights.put(sc, 0.0f);\n+    getPolicyInfo().setRouterPolicyWeights(routerWeights);\n+    getPolicyInfo().setAMRMPolicyWeights(amrmWeights);\n+\n+    try {\n+      FederationPoliciesTestUtil.initializePolicyContext(getPolicy(),\n+          getPolicyInfo(), getActiveSubclusters());\n+      ((FederationRouterPolicy) getPolicy())\n+          .getHomeSubcluster(getApplicationSubmissionContext(), null);\n+      fail();\n+    } catch (YarnException ex) {\n+      Assert.assertTrue(\n+          ex.getMessage().contains(\"Zero Active Subcluster with weight 1\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java",
                "sha": "58f1b9947bd81377978b0b00620c385c6e7cfa43",
                "status": "modified"
            }
        ],
        "message": "YARN-8897. LoadBasedRouterPolicy throws NPE in case of sub cluster unavailability. Contributed by Bilwa S T.",
        "parent": "https://github.com/apache/hadoop/commit/babc946d4017e9c385d19a8e6f7f1ecd5080d619",
        "patched_files": [
            "LoadBasedRouterPolicy.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLoadBasedRouterPolicy.java"
        ]
    },
    "hadoop_af61f4a": {
        "bug_id": "hadoop_af61f4a",
        "commit": "https://github.com/apache/hadoop/commit/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1384,6 +1384,9 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-3023. Fixed clients to display queue state correctly. (Ravi\n     Prakash via acmurthy) \n \n+    MAPREDUCE-2970. Fixed NPEs in corner cases with different configurations\n+    for mapreduce.framework.name. (Venu Gopala Rao via vinodkv)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "36a5f24c037c8d3caf8b79c48d22df7ff361a02d",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 13,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "patch": "@@ -41,8 +41,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n-import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n+import org.apache.hadoop.security.token.Token;\n \n /**\n  * Provides a way to access information about the map/reduce cluster.\n@@ -68,30 +68,41 @@\n   }\n   \n   public Cluster(Configuration conf) throws IOException {\n-    this.conf = conf;\n-    this.ugi = UserGroupInformation.getCurrentUser();\n-    for (ClientProtocolProvider provider : ServiceLoader.load(ClientProtocolProvider.class)) {\n-      ClientProtocol clientProtocol = provider.create(conf);\n-      if (clientProtocol != null) {\n-        clientProtocolProvider = provider;\n-        client = clientProtocol;\n-        break;\n-      }\n-    }\n+    this(null, conf);\n   }\n \n   public Cluster(InetSocketAddress jobTrackAddr, Configuration conf) \n       throws IOException {\n     this.conf = conf;\n     this.ugi = UserGroupInformation.getCurrentUser();\n-    for (ClientProtocolProvider provider : ServiceLoader.load(ClientProtocolProvider.class)) {\n-      ClientProtocol clientProtocol = provider.create(jobTrackAddr, conf);\n+    initialize(jobTrackAddr, conf);\n+  }\n+  \n+  private void initialize(InetSocketAddress jobTrackAddr, Configuration conf)\n+      throws IOException {\n+\n+    for (ClientProtocolProvider provider : ServiceLoader\n+        .load(ClientProtocolProvider.class)) {\n+      ClientProtocol clientProtocol = null;\n+      if (jobTrackAddr == null) {\n+        clientProtocol = provider.create(conf);\n+      } else {\n+        clientProtocol = provider.create(jobTrackAddr, conf);\n+      }\n+\n       if (clientProtocol != null) {\n         clientProtocolProvider = provider;\n         client = clientProtocol;\n         break;\n       }\n     }\n+\n+    if (null == clientProtocolProvider || null == client) {\n+      throw new IOException(\n+          \"Cannot initialize Cluster. Please check your configuration for \"\n+              + MRConfig.FRAMEWORK_NAME\n+              + \" and the correspond server addresses.\");\n+    }\n   }\n \n   ClientProtocol getClient() {",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "sha": "33d5f81b4fc3b5120fc28035823f38a43da848d3",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "patch": "@@ -0,0 +1,59 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.hadoop.mapreduce;\r\n+\r\n+import java.io.IOException;\r\n+\r\n+import junit.framework.TestCase;\r\n+\r\n+import org.apache.hadoop.conf.Configuration;\r\n+import org.apache.hadoop.mapred.YARNRunner;\r\n+import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\r\n+import org.junit.Test;\r\n+\r\n+public class TestYarnClientProtocolProvider extends TestCase {\r\n+\r\n+  @Test\r\n+  public void testClusterWithYarnClientProtocolProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration(false);\r\n+    Cluster cluster = null;\r\n+\r\n+    try {\r\n+      cluster = new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with out any framework name\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf = new Configuration();\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n+      cluster = new Cluster(conf);\r\n+      ClientProtocol client = cluster.getClient();\r\n+      assertTrue(client instanceof YARNRunner);\r\n+    } catch (IOException e) {\r\n+\r\n+    } finally {\r\n+      if (cluster != null) {\r\n+        cluster.close();\r\n+      }\r\n+    }\r\n+  }\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "sha": "2bc9030bf85ea491055dbced5260c7f3b728459f",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "patch": "@@ -43,20 +43,24 @@ public ClientProtocol create(Configuration conf) throws IOException {\n     String tracker = conf.get(JTConfig.JT_IPC_ADDRESS, \"local\");\n     if (!\"local\".equals(tracker)) {\n       return createRPCProxy(JobTracker.getAddress(conf), conf);\n+    } else {\n+      throw new IOException(\"Invalid \\\"\" + JTConfig.JT_IPC_ADDRESS\n+          + \"\\\" configuration value for JobTracker: \\\"\"\n+          + tracker + \"\\\"\");\n     }\n-    return null;\n   }\n \n   @Override\n-  public ClientProtocol create(InetSocketAddress addr, Configuration conf) throws IOException {\n+  public ClientProtocol create(InetSocketAddress addr, Configuration conf)\n+      throws IOException {\n     return createRPCProxy(addr, conf);\n   }\n-  \n+\n   private ClientProtocol createRPCProxy(InetSocketAddress addr,\n       Configuration conf) throws IOException {\n     return (ClientProtocol) RPC.getProxy(ClientProtocol.class,\n-      ClientProtocol.versionID, addr, UserGroupInformation.getCurrentUser(),\n-      conf, NetUtils.getSocketFactory(conf, ClientProtocol.class));\n+        ClientProtocol.versionID, addr, UserGroupInformation.getCurrentUser(),\n+        conf, NetUtils.getSocketFactory(conf, ClientProtocol.class));\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "sha": "d12132c68d24c5b0cc793c05818cb7aa016f95b5",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "patch": "@@ -37,11 +37,16 @@ public ClientProtocol create(Configuration conf) throws IOException {\n     if (framework != null && !framework.equals(\"local\")) {\n       return null;\n     }\n-    if (\"local\".equals(conf.get(JTConfig.JT_IPC_ADDRESS, \"local\"))) {\n+    String tracker = conf.get(JTConfig.JT_IPC_ADDRESS, \"local\");\n+    if (\"local\".equals(tracker)) {\n       conf.setInt(\"mapreduce.job.maps\", 1);\n       return new LocalJobRunner(conf);\n+    } else {\n+\n+      throw new IOException(\"Invalid \\\"\" + JTConfig.JT_IPC_ADDRESS\n+          + \"\\\" configuration value for LocalJobRunner : \\\"\"\n+          + tracker + \"\\\"\");\n     }\n-    return null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "sha": "d09b222ee9b2ed9a6972d139e0da0fa66792c420",
                "status": "modified"
            },
            {
                "additions": 99,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "changes": 99,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "patch": "@@ -0,0 +1,99 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.hadoop.mapreduce;\r\n+\r\n+import java.io.IOException;\r\n+\r\n+import junit.framework.TestCase;\r\n+\r\n+import org.apache.hadoop.conf.Configuration;\r\n+import org.apache.hadoop.mapred.LocalJobRunner;\r\n+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\r\n+import org.junit.Test;\r\n+\r\n+public class TestClientProtocolProviderImpls extends TestCase {\r\n+\r\n+  @Test\r\n+  public void testClusterWithLocalClientProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration();\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"incorrect\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with incorrect framework name\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"127.0.0.1:0\");\r\n+\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster with Local Framework name should use local JT address\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n+      Cluster cluster = new Cluster(conf);\r\n+      assertTrue(cluster.getClient() instanceof LocalJobRunner);\r\n+      cluster.close();\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+  }\r\n+\r\n+  @Test\r\n+  public void testClusterWithJTClientProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration();\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"incorrect\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with incorrect framework name\");\r\n+\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"classic\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster with classic Framework name shouldnot use local JT address\");\r\n+\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf = new Configuration();\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"classic\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"127.0.0.1:0\");\r\n+      Cluster cluster = new Cluster(conf);\r\n+      cluster.close();\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+  }\r\n+\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "sha": "a9044e24308133a4d0902e600a3a34fbaa9e93b5",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-2970. Fixed NPEs in corner cases with different configurations for mapreduce.framework.name. Contributed by Venu Gopala Rao.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173534 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/01fbb0fb4502dfa6bd8f76a4dfe7dfd0033e7d62",
        "patched_files": [
            "YarnClientProtocolProvider.java",
            "JobTrackerClientProtocolProvider.java",
            "CHANGES.java",
            "Cluster.java",
            "LocalClientProtocolProvider.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestClientProtocolProviderImpls.java",
            "TestCluster.java",
            "TestYarnClientProtocolProvider.java"
        ]
    },
    "hadoop_b061215": {
        "bug_id": "hadoop_b061215",
        "commit": "https://github.com/apache/hadoop/commit/b061215ecfebe476bf58f70788113d1af816f553",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java",
                "patch": "@@ -20,6 +20,7 @@\n import static org.apache.hadoop.fs.CommonConfigurationKeys.FS_CLIENT_TOPOLOGY_RESOLUTION_ENABLED;\n import static org.apache.hadoop.fs.CommonConfigurationKeys.FS_CLIENT_TOPOLOGY_RESOLUTION_ENABLED_DEFAULT;\n \n+import java.io.IOException;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.List;\n@@ -238,7 +239,7 @@ public ByteArrayManager getByteArrayManager() {\n     return byteArrayManager;\n   }\n \n-  public int getNetworkDistance(DatanodeInfo datanodeInfo) {\n+  public int getNetworkDistance(DatanodeInfo datanodeInfo) throws IOException {\n     // If applications disable the feature or the client machine can't\n     // resolve its network location, clientNode will be set to null.\n     if (clientNode == null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java",
                "sha": "ad1b3595cbd5b8efdc29f11b4a88bcfe394cc9d3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java",
                "patch": "@@ -550,7 +550,11 @@ public static String dateToIso8601String(Date date) {\n   private static final Map<String, Boolean> localAddrMap = Collections\n       .synchronizedMap(new HashMap<String, Boolean>());\n \n-  public static boolean isLocalAddress(InetSocketAddress targetAddr) {\n+  public static boolean isLocalAddress(InetSocketAddress targetAddr)\n+      throws IOException {\n+    if (targetAddr.isUnresolved()) {\n+      throw new IOException(\"Unresolved host: \" + targetAddr);\n+    }\n     InetAddress addr = targetAddr.getAddress();\n     Boolean cached = localAddrMap.get(addr.getHostAddress());\n     if (cached != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java",
                "sha": "2edd7554e838ae738401b97b620d70d1f2de9e24",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 18,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "patch": "@@ -357,28 +357,32 @@ public BlockReader build() throws IOException {\n       return reader;\n     }\n     final ShortCircuitConf scConf = conf.getShortCircuitConf();\n-    if (scConf.isShortCircuitLocalReads() && allowShortCircuitLocalReads) {\n-      if (clientContext.getUseLegacyBlockReaderLocal()) {\n-        reader = getLegacyBlockReaderLocal();\n-        if (reader != null) {\n-          LOG.trace(\"{}: returning new legacy block reader local.\", this);\n-          return reader;\n+    try {\n+      if (scConf.isShortCircuitLocalReads() && allowShortCircuitLocalReads) {\n+        if (clientContext.getUseLegacyBlockReaderLocal()) {\n+          reader = getLegacyBlockReaderLocal();\n+          if (reader != null) {\n+            LOG.trace(\"{}: returning new legacy block reader local.\", this);\n+            return reader;\n+          }\n+        } else {\n+          reader = getBlockReaderLocal();\n+          if (reader != null) {\n+            LOG.trace(\"{}: returning new block reader local.\", this);\n+            return reader;\n+          }\n         }\n-      } else {\n-        reader = getBlockReaderLocal();\n+      }\n+      if (scConf.isDomainSocketDataTraffic()) {\n+        reader = getRemoteBlockReaderFromDomain();\n         if (reader != null) {\n-          LOG.trace(\"{}: returning new block reader local.\", this);\n+          LOG.trace(\"{}: returning new remote block reader using UNIX domain \"\n+              + \"socket on {}\", this, pathInfo.getPath());\n           return reader;\n         }\n       }\n-    }\n-    if (scConf.isDomainSocketDataTraffic()) {\n-      reader = getRemoteBlockReaderFromDomain();\n-      if (reader != null) {\n-        LOG.trace(\"{}: returning new remote block reader using UNIX domain \"\n-            + \"socket on {}\", this, pathInfo.getPath());\n-        return reader;\n-      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Block read failed. Getting remote block reader using TCP\", e);\n     }\n     Preconditions.checkState(!DFSInputStream.tcpReadsDisabledForTesting,\n         \"TCP reads were disabled for testing, but we failed to \" +\n@@ -469,7 +473,7 @@ private BlockReader getLegacyBlockReaderLocal() throws IOException {\n     return null;\n   }\n \n-  private BlockReader getBlockReaderLocal() throws InvalidToken {\n+  private BlockReader getBlockReaderLocal() throws IOException {\n     LOG.trace(\"{}: trying to construct a BlockReaderLocal for short-circuit \"\n         + \" reads.\", this);\n     if (pathInfo == null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "sha": "e83c8ae92b2747bd1b451c83c2356af325b46bc6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java",
                "patch": "@@ -133,7 +133,8 @@ public DomainSocketFactory(ShortCircuitConf conf) {\n    *\n    * @return             Information about the socket path.\n    */\n-  public PathInfo getPathInfo(InetSocketAddress addr, ShortCircuitConf conf) {\n+  public PathInfo getPathInfo(InetSocketAddress addr, ShortCircuitConf conf)\n+      throws IOException {\n     // If there is no domain socket path configured, we can't use domain\n     // sockets.\n     if (conf.getDomainSocketPath().isEmpty()) return PathInfo.NOT_CONFIGURED;",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java",
                "sha": "760e920c232b5241f0c8c5d92ef86331846f12de",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java",
                "patch": "@@ -28,6 +28,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.InetSocketAddress;\n import java.nio.channels.ClosedByInterruptException;\n import java.util.Arrays;\n import java.util.HashMap;\n@@ -53,6 +54,7 @@\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.PerDatanodeVisitorInfo;\n import org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.Visitor;\n+import org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory;\n import org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache;\n import org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo;\n import org.apache.hadoop.io.IOUtils;\n@@ -68,6 +70,7 @@\n import org.junit.Test;\n \n import com.google.common.util.concurrent.Uninterruptibles;\n+import org.junit.rules.ExpectedException;\n import org.junit.rules.Timeout;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -79,6 +82,9 @@\n   @Rule\n   public final Timeout globalTimeout = new Timeout(180000);\n \n+  @Rule\n+  public ExpectedException thrown = ExpectedException.none();\n+\n   @Before\n   public void init() {\n     DomainSocket.disableBindPathValidation();\n@@ -144,6 +150,33 @@ public void testFallbackFromShortCircuitToUnixDomainTraffic()\n     sockDir.close();\n   }\n \n+  /**\n+   * Test the case where address passed to DomainSocketFactory#getPathInfo is\n+   * unresolved. In such a case an exception should be thrown.\n+   */\n+  @Test(timeout=60000)\n+  public void testGetPathInfoWithUnresolvedHost() throws Exception {\n+    TemporarySocketDirectory sockDir = new TemporarySocketDirectory();\n+\n+    Configuration conf =\n+        createShortCircuitConf(\"testGetPathInfoWithUnresolvedHost\", sockDir);\n+    conf.set(DFS_CLIENT_CONTEXT,\n+        \"testGetPathInfoWithUnresolvedHost_Context\");\n+    conf.setBoolean(DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC, true);\n+\n+    DfsClientConf.ShortCircuitConf shortCircuitConf =\n+        new DfsClientConf.ShortCircuitConf(conf);\n+    DomainSocketFactory domainSocketFactory =\n+        new DomainSocketFactory(shortCircuitConf);\n+    InetSocketAddress targetAddr =\n+        InetSocketAddress.createUnresolved(\"random\", 32456);\n+\n+    thrown.expect(IOException.class);\n+    thrown.expectMessage(\"Unresolved host: \" + targetAddr);\n+    domainSocketFactory.getPathInfo(targetAddr, shortCircuitConf);\n+    sockDir.close();\n+  }\n+\n   /**\n    * Test the case where we have multiple threads waiting on the\n    * ShortCircuitCache delivering a certain ShortCircuitReplica.",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java",
                "sha": "6b04b14f49a77e0333a871d5a63de540741f8255",
                "status": "modified"
            }
        ],
        "message": "HDFS-11701. NPE from Unresolved Host causes permanent DFSInputStream failures. Contributed by Lokesh Jain.",
        "parent": "https://github.com/apache/hadoop/commit/456705a07c8b80658950acc99f23086244c6b20f",
        "patched_files": [
            "DFSUtilClient.java",
            "BlockReaderFactory.java",
            "ClientContext.java",
            "DomainSocketFactory.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockReaderFactory.java"
        ]
    },
    "hadoop_b06cc16": {
        "bug_id": "hadoop_b06cc16",
        "commit": "https://github.com/apache/hadoop/commit/b06cc16f7df63766531721a55280061949cab9b4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=b06cc16f7df63766531721a55280061949cab9b4",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -413,6 +413,8 @@ Release 2.4.0 - UNRELEASED\n     YARN-1768. Fixed error message being too verbose when killing a non-existent\n     application\n     \n+    YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and\n+    Karthik Kambatla via kasha)\n \n Release 2.3.1 - UNRELEASED\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/CHANGES.txt",
                "sha": "9286a7ead8254cf637f41654d9c50c6958e7e261",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=b06cc16f7df63766531721a55280061949cab9b4",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -611,9 +611,6 @@ protected synchronized void addApplication(ApplicationId applicationId,\n     RMApp rmApp = rmContext.getRMApps().get(applicationId);\n     FSLeafQueue queue = assignToQueue(rmApp, queueName, user);\n     if (queue == null) {\n-      rmContext.getDispatcher().getEventHandler().handle(\n-          new RMAppRejectedEvent(applicationId,\n-              \"Application rejected by queue placement policy\"));\n       return;\n     }\n \n@@ -679,27 +676,43 @@ protected synchronized void addApplicationAttempt(\n         new RMAppAttemptEvent(applicationAttemptId,\n             RMAppAttemptEventType.ATTEMPT_ADDED));\n   }\n-  \n+\n+  /**\n+   * Helper method that attempts to assign the app to a queue. The method is\n+   * responsible to call the appropriate event-handler if the app is rejected.\n+   */\n   @VisibleForTesting\n   FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n     FSLeafQueue queue = null;\n+    String appRejectMsg = null;\n+\n     try {\n       QueuePlacementPolicy placementPolicy = allocConf.getPlacementPolicy();\n       queueName = placementPolicy.assignAppToQueue(queueName, user);\n       if (queueName == null) {\n-        return null;\n+        appRejectMsg = \"Application rejected by queue placement policy\";\n+      } else {\n+        queue = queueMgr.getLeafQueue(queueName, true);\n+        if (queue == null) {\n+          appRejectMsg = queueName + \" is not a leaf queue\";\n+        }\n       }\n-      queue = queueMgr.getLeafQueue(queueName, true);\n-    } catch (IOException ex) {\n-      LOG.error(\"Error assigning app to queue, rejecting\", ex);\n+    } catch (IOException ioe) {\n+      appRejectMsg = \"Error assigning app to queue \" + queueName;\n     }\n-    \n+\n+    if (appRejectMsg != null && rmApp != null) {\n+      LOG.error(appRejectMsg);\n+      rmContext.getDispatcher().getEventHandler().handle(\n+          new RMAppRejectedEvent(rmApp.getApplicationId(), appRejectMsg));\n+      return null;\n+    }\n+\n     if (rmApp != null) {\n       rmApp.setQueue(queue.getName());\n     } else {\n-      LOG.warn(\"Couldn't find RM app to set queue name on\");\n+      LOG.error(\"Couldn't find RM app to set queue name on\");\n     }\n-    \n     return queue;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "3cdff7f405563bc14e3c7ac7b5522ca977a25a54",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=b06cc16f7df63766531721a55280061949cab9b4",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -704,6 +704,22 @@ public void testAssignToQueue() throws Exception {\n     assertEquals(rmApp2.getQueue(), queue2.getName());\n     assertEquals(\"root.notdefault\", rmApp2.getQueue());\n   }\n+\n+  @Test\n+  public void testAssignToNonLeafQueueReturnsNull() throws Exception {\n+    conf.set(FairSchedulerConfiguration.USER_AS_DEFAULT_QUEUE, \"true\");\n+    scheduler.reinitialize(conf, resourceManager.getRMContext());\n+\n+    scheduler.getQueueManager().getLeafQueue(\"root.child1.granchild\", true);\n+    scheduler.getQueueManager().getLeafQueue(\"root.child2\", true);\n+\n+    RMApp rmApp1 = new MockRMApp(0, 0, RMAppState.NEW);\n+    RMApp rmApp2 = new MockRMApp(1, 1, RMAppState.NEW);\n+\n+    // Trying to assign to non leaf queue would return null\n+    assertNull(scheduler.assignToQueue(rmApp1, \"root.child1\", \"tintin\"));\n+    assertNotNull(scheduler.assignToQueue(rmApp2, \"root.child2\", \"snowy\"));\n+  }\n   \n   @Test\n   public void testQueuePlacementWithPolicy() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "d1052bb165e6ae3798697208b56814da897157ba",
                "status": "modified"
            }
        ],
        "message": "YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and Karthik Kambatla via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575415 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b1f87bbabd38c90f55c833db70f82d89eae15de6",
        "patched_files": [
            "FairScheduler.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop_b098281": {
        "bug_id": "hadoop_b098281",
        "commit": "https://github.com/apache/hadoop/commit/b09828145432c8d986ac8f05ec33608d8e611328",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b09828145432c8d986ac8f05ec33608d8e611328/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java?ref=b09828145432c8d986ac8f05ec33608d8e611328",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
                "patch": "@@ -565,6 +565,10 @@ private static QuotaUsage getQuotaUsageInt(FSDirectory fsd, INodesInPath iip)\n     fsd.readLock();\n     try {\n       INode targetNode = iip.getLastINode();\n+      if (targetNode == null) {\n+        throw new FileNotFoundException(\n+            \"File/Directory does not exist: \" + iip.getPath());\n+      }\n       QuotaUsage usage = null;\n       if (targetNode.isDirectory()) {\n         DirectoryWithQuotaFeature feature =",
                "raw_url": "https://github.com/apache/hadoop/raw/b09828145432c8d986ac8f05ec33608d8e611328/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirStatAndListingOp.java",
                "sha": "01de2360fa8e48b6ac297c59f373ac295cb301d6",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/b09828145432c8d986ac8f05ec33608d8e611328/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java?ref=b09828145432c8d986ac8f05ec33608d8e611328",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java",
                "patch": "@@ -28,6 +28,7 @@\n import static org.junit.Assert.fail;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.OutputStream;\n import java.io.PrintStream;\n@@ -332,6 +333,13 @@ public void testQuotaCommands() throws Exception {\n     // 14a: set quota on a non-existent directory\n     Path nonExistentPath = new Path(dir, \"test1\");\n     assertFalse(dfs.exists(nonExistentPath));\n+    try {\n+      compareQuotaUsage(null, dfs, nonExistentPath);\n+      fail(\"Expected FileNotFoundException\");\n+    } catch (FileNotFoundException fnfe) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"File/Directory does not exist: \" + nonExistentPath, fnfe);\n+    }\n     args = new String[]{\"-setQuota\", \"1\", nonExistentPath.toString()};\n     runCommand(admin, args, true);\n     runCommand(admin, true, \"-setSpaceQuota\", \"1g\", // for space quota",
                "raw_url": "https://github.com/apache/hadoop/raw/b09828145432c8d986ac8f05ec33608d8e611328/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java",
                "sha": "1c4855fc7010db36fa8259f88b8385dd68d1e4ed",
                "status": "modified"
            }
        ],
        "message": "HDFS-13816. dfs.getQuotaUsage() throws NPE on non-existent dir instead of FileNotFoundException. Contributed by Vinayakumar B.",
        "parent": "https://github.com/apache/hadoop/commit/23b441c2253bcb3be5229d3c5eb1e165369070c0",
        "patched_files": [
            "FSDirStatAndListingOp.java",
            "Quota.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestQuota.java"
        ]
    },
    "hadoop_b2af6c7": {
        "bug_id": "hadoop_b2af6c7",
        "commit": "https://github.com/apache/hadoop/commit/b2af6c70245703aa6d2ad1beceacc2aa2fcde5c0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b2af6c70245703aa6d2ad1beceacc2aa2fcde5c0/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=b2af6c70245703aa6d2ad1beceacc2aa2fcde5c0",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -253,6 +253,9 @@ Release 2.1.1-beta - UNRELEASED\n \n     MAPREDUCE-5475. MRClientService does not verify ACLs properly (jlowe)\n \n+    MAPREDUCE-5414. TestTaskAttempt fails in JDK7 with NPE (Nemon Lou via \n+    devaraj)\n+\n Release 2.1.0-beta - 2013-08-22\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b2af6c70245703aa6d2ad1beceacc2aa2fcde5c0/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "13f0079fc3c4b3ec7c06a736953a4ea3d63de510",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/b2af6c70245703aa6d2ad1beceacc2aa2fcde5c0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java?ref=b2af6c70245703aa6d2ad1beceacc2aa2fcde5c0",
                "deletions": 6,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "patch": "@@ -343,7 +343,7 @@ public void testLaunchFailedWhileKilling() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), null);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n@@ -399,7 +399,7 @@ public void testContainerCleanedWhileRunning() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.2\", 0);\n@@ -456,7 +456,7 @@ public void testContainerCleanedWhileCommitting() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n@@ -516,7 +516,7 @@ public void testDoubleTooManyFetchFailure() throws Exception {\n     TaskAttemptImpl taImpl =\n       new MapTaskAttemptImpl(taskId, 1, eventHandler, jobFile, 1,\n           splits, jobConf, taListener,\n-          mock(Token.class), new Credentials(),\n+          new Token(), new Credentials(),\n           new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n@@ -582,7 +582,7 @@ public void testAppDiognosticEventOnUnassignedTask() throws Exception {\n \n     TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler,\n         jobFile, 1, splits, jobConf, taListener,\n-        mock(Token.class), new Credentials(), new SystemClock(), appCtx);\n+        new Token(), new Credentials(), new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n     ContainerId contId = ContainerId.newInstance(appAttemptId, 3);\n@@ -631,7 +631,7 @@ public void testAppDiognosticEventOnNewTask() throws Exception {\n \n     TaskAttemptImpl taImpl = new MapTaskAttemptImpl(taskId, 1, eventHandler,\n         jobFile, 1, splits, jobConf, taListener,\n-        mock(Token.class), new Credentials(), new SystemClock(), appCtx);\n+        new Token(), new Credentials(), new SystemClock(), appCtx);\n \n     NodeId nid = NodeId.newInstance(\"127.0.0.1\", 0);\n     ContainerId contId = ContainerId.newInstance(appAttemptId, 3);",
                "raw_url": "https://github.com/apache/hadoop/raw/b2af6c70245703aa6d2ad1beceacc2aa2fcde5c0/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
                "sha": "1129c2fcfc4469f14a9ec29499d3e417758fc6ed",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5414. TestTaskAttempt fails in JDK7 with NPE. Contributed by Nemon Lou.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1520964 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/4f2bf68b73df98f1bb5f80e6e192bd03b935b03b",
        "patched_files": [
            "TaskAttempt.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskAttempt.java"
        ]
    },
    "hadoop_b2ed6ae": {
        "bug_id": "hadoop_b2ed6ae",
        "commit": "https://github.com/apache/hadoop/commit/b2ed6ae73197990a950ce71ece80c0f23221c384",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -587,19 +587,22 @@ private static ContainerReport convertToContainerReport(\n         }\n       }\n     }\n-    NodeId allocatedNode = NodeId.newInstance(allocatedHost, allocatedPort);\n     ContainerId containerId =\n         ConverterUtils.toContainerId(entity.getEntityId());\n-    String logUrl = WebAppUtils.getAggregatedLogURL(\n-        serverHttpAddress,\n-        allocatedNode.toString(),\n-        containerId.toString(),\n-        containerId.toString(),\n-        user);\n+    String logUrl = null;\n+    NodeId allocatedNode = null;\n+    if (allocatedHost != null) {\n+      allocatedNode = NodeId.newInstance(allocatedHost, allocatedPort);\n+      logUrl = WebAppUtils.getAggregatedLogURL(\n+          serverHttpAddress,\n+          allocatedNode.toString(),\n+          containerId.toString(),\n+          containerId.toString(),\n+          user);\n+    }\n     return ContainerReport.newInstance(\n         ConverterUtils.toContainerId(entity.getEntityId()),\n-        Resource.newInstance(allocatedMem, allocatedVcore),\n-        NodeId.newInstance(allocatedHost, allocatedPort),\n+        Resource.newInstance(allocatedMem, allocatedVcore), allocatedNode,\n         Priority.newInstance(allocatedPriority),\n         createdTime, finishedTime, diagnosticsInfo, logUrl, exitStatus, state,\n         nodeHttpAddress);",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "sha": "19afb253e0f48fd26bed27f5be2e01996d2e0582",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java",
                "patch": "@@ -20,24 +20,28 @@\n \n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n \n public class ContainerFinishedEvent extends SystemMetricsEvent {\n \n   private ContainerId containerId;\n   private String diagnosticsInfo;\n   private int containerExitStatus;\n   private ContainerState state;\n+  private NodeId allocatedNode;\n \n   public ContainerFinishedEvent(\n       ContainerId containerId,\n       String diagnosticsInfo,\n       int containerExitStatus,\n       ContainerState state,\n-      long finishedTime) {\n+      long finishedTime,\n+      NodeId allocatedNode) {\n     super(SystemMetricsEventType.CONTAINER_FINISHED, finishedTime);\n     this.containerId = containerId;\n     this.diagnosticsInfo = diagnosticsInfo;\n     this.containerExitStatus = containerExitStatus;\n+    this.allocatedNode = allocatedNode;\n     this.state = state;\n   }\n \n@@ -62,4 +66,7 @@ public ContainerState getContainerState() {\n     return state;\n   }\n \n+  public NodeId getAllocatedNode() {\n+    return allocatedNode;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java",
                "sha": "ca4d3117aa52f4973bd5c53dde74bf07d526d42e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -218,7 +218,7 @@ public void containerFinished(RMContainer container, long finishedTime) {\n               container.getDiagnosticsInfo(),\n               container.getContainerExitStatus(),\n               container.getContainerState(),\n-              finishedTime));\n+              finishedTime, container.getAllocatedNode()));\n     }\n   }\n \n@@ -479,6 +479,12 @@ private void publishContainerFinishedEvent(ContainerFinishedEvent event) {\n         event.getContainerExitStatus());\n     eventInfo.put(ContainerMetricsConstants.STATE_EVENT_INFO,\n         event.getContainerState().toString());\n+    Map<String, Object> entityInfo = new HashMap<String, Object>();\n+    entityInfo.put(ContainerMetricsConstants.ALLOCATED_HOST_ENTITY_INFO,\n+        event.getAllocatedNode().getHost());\n+    entityInfo.put(ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO,\n+        event.getAllocatedNode().getPort());\n+    entity.setOtherInfo(entityInfo);\n     tEvent.setEventInfo(eventInfo);\n     entity.addEvent(tEvent);\n     putEntity(entity);",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "cba87903fb3332c93121e40a46ada5cf5e2ff1c6",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -344,6 +344,36 @@ public void testPublishAppAttemptMetrics() throws Exception {\n     Assert.assertTrue(hasRegisteredEvent && hasFinishedEvent);\n   }\n \n+  @Test(timeout = 10000)\n+  public void testPublishHostPortInfoOnContainerFinished() throws Exception {\n+    ContainerId containerId =\n+        ContainerId.newContainerId(ApplicationAttemptId.newInstance(\n+            ApplicationId.newInstance(0, 1), 1), 1);\n+    RMContainer container = createRMContainer(containerId);\n+    metricsPublisher.containerFinished(container, container.getFinishTime());\n+    TimelineEntity entity = null;\n+    do {\n+      entity =\n+          store.getEntity(containerId.toString(),\n+              ContainerMetricsConstants.ENTITY_TYPE,\n+              EnumSet.allOf(Field.class));\n+    } while (entity == null || entity.getEvents().size() < 1);\n+    Assert.assertNotNull(entity.getOtherInfo());\n+    Assert.assertEquals(2, entity.getOtherInfo().size());\n+    Assert.assertNotNull(entity.getOtherInfo().get(\n+        ContainerMetricsConstants.ALLOCATED_HOST_ENTITY_INFO));\n+    Assert.assertNotNull(entity.getOtherInfo().get(\n+        ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO));\n+    Assert.assertEquals(\n+        container.getAllocatedNode().getHost(),\n+        entity.getOtherInfo().get(\n+            ContainerMetricsConstants.ALLOCATED_HOST_ENTITY_INFO));\n+    Assert.assertEquals(\n+        container.getAllocatedNode().getPort(),\n+        entity.getOtherInfo().get(\n+            ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO));\n+  }\n+\n   @Test(timeout = 10000)\n   public void testPublishContainerMetrics() throws Exception {\n     ContainerId containerId =",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "0738a2b77ced683a9cc82264a28767072cb74294",
                "status": "modified"
            }
        ],
        "message": "YARN-4747. AHS error 500 due to NPE when container start event is missing. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/23248f63aab74a19dba38d348f2b231c8360770a",
        "patched_files": [
            "ApplicationHistoryManagerOnTimelineStore.java",
            "SystemMetricsPublisher.java",
            "ContainerFinishedEvent.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationHistoryManagerOnTimelineStore.java",
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_b2f1ec3": {
        "bug_id": "hadoop_b2f1ec3",
        "commit": "https://github.com/apache/hadoop/commit/b2f1ec312ee431aef762cfb49cb29cd6f4661e86",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/b2f1ec312ee431aef762cfb49cb29cd6f4661e86/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java?ref=b2f1ec312ee431aef762cfb49cb29cd6f4661e86",
                "deletions": 16,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "patch": "@@ -571,12 +571,12 @@ public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n         rmNode.nodeUpdateQueue.clear();\n         rmNode.context.getDispatcher().getEventHandler().handle(\n             new NodeRemovedSchedulerEvent(rmNode));\n-        \n+\n         if (rmNode.getHttpPort() == newNode.getHttpPort()) {\n           // Reset heartbeat ID since node just restarted.\n           rmNode.getLastNodeHeartBeatResponse().setResponseId(0);\n-          if (rmNode.getState() != NodeState.UNHEALTHY) {\n-            // Only add new node if old state is not UNHEALTHY\n+          if (rmNode.getState().equals(NodeState.RUNNING)) {\n+            // Only add new node if old state is RUNNING\n             rmNode.context.getDispatcher().getEventHandler().handle(\n                 new NodeAddedSchedulerEvent(newNode));\n           }\n@@ -599,30 +599,32 @@ public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n       } else {\n         rmNode.httpPort = newNode.getHttpPort();\n         rmNode.httpAddress = newNode.getHttpAddress();\n-        rmNode.totalCapability = newNode.getTotalCapability();\n+        boolean isCapabilityChanged = false;\n+        if (rmNode.getTotalCapability() != newNode.getTotalCapability()) {\n+          rmNode.totalCapability = newNode.getTotalCapability();\n+          isCapabilityChanged = true;\n+        }\n       \n         handleNMContainerStatus(reconnectEvent.getNMContainerStatuses(), rmNode);\n \n         // Reset heartbeat ID since node just restarted.\n         rmNode.getLastNodeHeartBeatResponse().setResponseId(0);\n-      }\n \n-      if (null != reconnectEvent.getRunningApplications()) {\n         for (ApplicationId appId : reconnectEvent.getRunningApplications()) {\n           handleRunningAppOnNode(rmNode, rmNode.context, appId, rmNode.nodeId);\n         }\n-      }\n \n-      rmNode.context.getDispatcher().getEventHandler().handle(\n-          new NodesListManagerEvent(\n-              NodesListManagerEventType.NODE_USABLE, rmNode));\n-      if (rmNode.getState().equals(NodeState.RUNNING)) {\n-        // Update scheduler node's capacity for reconnect node.\n-        rmNode.context.getDispatcher().getEventHandler().handle(\n-            new NodeResourceUpdateSchedulerEvent(rmNode, \n-                ResourceOption.newInstance(newNode.getTotalCapability(), -1)));\n+        if (isCapabilityChanged\n+            && rmNode.getState().equals(NodeState.RUNNING)) {\n+          // Update scheduler node's capacity for reconnect node.\n+          rmNode.context\n+              .getDispatcher()\n+              .getEventHandler()\n+              .handle(\n+                  new NodeResourceUpdateSchedulerEvent(rmNode, ResourceOption\n+                      .newInstance(newNode.getTotalCapability(), -1)));\n+        }\n       }\n-      \n     }\n \n     private void handleNMContainerStatus(",
                "raw_url": "https://github.com/apache/hadoop/raw/b2f1ec312ee431aef762cfb49cb29cd6f4661e86/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "sha": "c556b80469a0ead0fc67acbd82317f0a100aae17",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/b2f1ec312ee431aef762cfb49cb29cd6f4661e86/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java?ref=b2f1ec312ee431aef762cfb49cb29cd6f4661e86",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "patch": "@@ -51,7 +51,7 @@\n   private final int memory;\n   private final int vCores;\n   private ResourceTrackerService resourceTracker;\n-  private final int httpPort = 2;\n+  private int httpPort = 2;\n   private MasterKey currentContainerTokenMasterKey;\n   private MasterKey currentNMTokenMasterKey;\n   private String version;\n@@ -87,6 +87,10 @@ public int getHttpPort() {\n     return httpPort;\n   }\n   \n+  public void setHttpPort(int port) {\n+    httpPort = port;\n+  }\n+\n   public void setResourceTrackerService(ResourceTrackerService resourceTracker) {\n     this.resourceTracker = resourceTracker;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/b2f1ec312ee431aef762cfb49cb29cd6f4661e86/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "sha": "c917f7976b006ef31faadb4f51824996ec4e0066",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/b2f1ec312ee431aef762cfb49cb29cd6f4661e86/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=b2f1ec312ee431aef762cfb49cb29cd6f4661e86",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -56,6 +56,7 @@\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n@@ -623,7 +624,7 @@ protected Dispatcher createDispatcher() {\n     dispatcher.await();\n     Assert.assertTrue(NodeAction.NORMAL.equals(response.getNodeAction()));\n     Assert.assertEquals(5120 + 10240, metrics.getAvailableMB());\n-    \n+\n     // reconnect of node with changed capability and running applications\n     List<ApplicationId> runningApps = new ArrayList<ApplicationId>();\n     runningApps.add(ApplicationId.newInstance(1, 0));\n@@ -633,6 +634,20 @@ protected Dispatcher createDispatcher() {\n     dispatcher.await();\n     Assert.assertTrue(NodeAction.NORMAL.equals(response.getNodeAction()));\n     Assert.assertEquals(5120 + 15360, metrics.getAvailableMB());\n+    \n+    // reconnect healthy node changing http port\n+    nm1 = new MockNM(\"host1:1234\", 5120, rm.getResourceTrackerService());\n+    nm1.setHttpPort(3);\n+    nm1.registerNode();\n+    dispatcher.await();\n+    response = nm1.nodeHeartbeat(true);\n+    response = nm1.nodeHeartbeat(true);\n+    dispatcher.await();\n+    RMNode rmNode = rm.getRMContext().getRMNodes().get(nm1.getNodeId());\n+    Assert.assertEquals(3, rmNode.getHttpPort());\n+    Assert.assertEquals(5120, rmNode.getTotalCapability().getMemory());\n+    Assert.assertEquals(5120 + 15360, metrics.getAvailableMB());\n+\n   }\n \n   private void writeToHostsFile(String... hosts) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/b2f1ec312ee431aef762cfb49cb29cd6f4661e86/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "a904dc0af4c64883ce871df4a154c375c6005ee3",
                "status": "modified"
            }
        ],
        "message": "YARN-3222. Fixed NPE on RMNodeImpl#ReconnectNodeTransition when a node is reconnected with a different port. Contributed by Rohith Sharmaks",
        "parent": "https://github.com/apache/hadoop/commit/15b7076ad5f2ae92d231140b2f8cebc392a92c87",
        "patched_files": [
            "ResourceTrackerService.java",
            "MockNM.java",
            "RMNodeImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_b406d8e": {
        "bug_id": "hadoop_b406d8e",
        "commit": "https://github.com/apache/hadoop/commit/b406d8e3755d24ce72c443fd893a5672fd56babc",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java?ref=b406d8e3755d24ce72c443fd893a5672fd56babc",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
                "patch": "@@ -275,8 +275,16 @@ private INodeAttributes getINodeAttrs(byte[][] pathByNameArr, int pathIdx,\n     INodeAttributes inodeAttrs = inode.getSnapshotINode(snapshotId);\n     if (getAttributesProvider() != null) {\n       String[] elements = new String[pathIdx + 1];\n-      for (int i = 0; i < elements.length; i++) {\n-        elements[i] = DFSUtil.bytes2String(pathByNameArr[i]);\n+      /**\n+       * {@link INode#getPathComponents(String)} returns a null component\n+       * for the root only path \"/\". Assign an empty string if so.\n+       */\n+      if (pathByNameArr.length == 1 && pathByNameArr[0] == null) {\n+        elements[0] = \"\";\n+      } else {\n+        for (int i = 0; i < elements.length; i++) {\n+          elements[i] = DFSUtil.bytes2String(pathByNameArr[i]);\n+        }\n       }\n       inodeAttrs = getAttributesProvider().getAttributes(elements, inodeAttrs);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
                "sha": "c854b49fb4f0687cb6c7ee86af0efe16b73e1543",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java?ref=b406d8e3755d24ce72c443fd893a5672fd56babc",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "patch": "@@ -313,31 +313,59 @@ public void testAuthzBypassingProvider() throws Exception {\n     testBypassProviderHelper(users, HDFS_PERMISSION, true);\n   }\n \n-  @Test\n-  public void testCustomProvider() throws Exception {\n+  private void verifyFileStatus(UserGroupInformation ugi) throws IOException {\n     FileSystem fs = FileSystem.get(miniDFS.getConfiguration(0));\n-    fs.mkdirs(new Path(\"/user/xxx\"));\n-    FileStatus status = fs.getFileStatus(new Path(\"/user/xxx\"));\n-    Assert.assertEquals(System.getProperty(\"user.name\"), status.getOwner());\n+\n+    FileStatus status = fs.getFileStatus(new Path(\"/\"));\n+    LOG.info(\"Path '/' is owned by: \"\n+        + status.getOwner() + \":\" + status.getGroup());\n+\n+    Path userDir = new Path(\"/user/\" + ugi.getShortUserName());\n+    fs.mkdirs(userDir);\n+    status = fs.getFileStatus(userDir);\n+    Assert.assertEquals(ugi.getShortUserName(), status.getOwner());\n     Assert.assertEquals(\"supergroup\", status.getGroup());\n     Assert.assertEquals(new FsPermission((short) 0755), status.getPermission());\n-    fs.mkdirs(new Path(\"/user/authz\"));\n-    Path p = new Path(\"/user/authz\");\n-    status = fs.getFileStatus(p);\n+\n+    Path authzDir = new Path(\"/user/authz\");\n+    fs.mkdirs(authzDir);\n+    status = fs.getFileStatus(authzDir);\n     Assert.assertEquals(\"foo\", status.getOwner());\n     Assert.assertEquals(\"bar\", status.getGroup());\n     Assert.assertEquals(new FsPermission((short) 0770), status.getPermission());\n-    AclStatus aclStatus = fs.getAclStatus(p);\n+\n+    AclStatus aclStatus = fs.getAclStatus(authzDir);\n     Assert.assertEquals(1, aclStatus.getEntries().size());\n-    Assert.assertEquals(AclEntryType.GROUP, aclStatus.getEntries().get(0)\n-            .getType());\n-    Assert.assertEquals(\"xxx\", aclStatus.getEntries().get(0)\n-            .getName());\n-    Assert.assertEquals(FsAction.ALL, aclStatus.getEntries().get(0)\n-            .getPermission());\n-    Map<String, byte[]> xAttrs = fs.getXAttrs(p);\n+    Assert.assertEquals(AclEntryType.GROUP,\n+        aclStatus.getEntries().get(0).getType());\n+    Assert.assertEquals(\"xxx\",\n+        aclStatus.getEntries().get(0).getName());\n+    Assert.assertEquals(FsAction.ALL,\n+        aclStatus.getEntries().get(0).getPermission());\n+    Map<String, byte[]> xAttrs = fs.getXAttrs(authzDir);\n     Assert.assertTrue(xAttrs.containsKey(\"user.test\"));\n     Assert.assertEquals(2, xAttrs.get(\"user.test\").length);\n   }\n \n+  /**\n+   * With the custom provider configured, verify file status attributes.\n+   * A superuser can bypass permission check while resolving paths. So,\n+   * verify file status for both superuser and non-superuser.\n+   */\n+  @Test\n+  public void testCustomProvider() throws Exception {\n+    final UserGroupInformation[] users = new UserGroupInformation[]{\n+        UserGroupInformation.createUserForTesting(\n+            System.getProperty(\"user.name\"), new String[]{\"supergroup\"}),\n+        UserGroupInformation.createUserForTesting(\n+            \"normaluser\", new String[]{\"normalusergroup\"}),\n+    };\n+\n+    for (final UserGroupInformation user : users) {\n+      user.doAs((PrivilegedExceptionAction<Object>) () -> {\n+        verifyFileStatus(user);\n+        return null;\n+      });\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "sha": "9c7dcd335232e18859841e3bcb523e8f118e496a",
                "status": "modified"
            }
        ],
        "message": "HDFS-12614. FSPermissionChecker#getINodeAttrs() throws NPE when INodeAttributesProvider configured.",
        "parent": "https://github.com/apache/hadoop/commit/e906108fc98a011630d12a43e557b81d7ef7ea5d",
        "patched_files": [
            "INodeAttributeProvider.java",
            "FSPermissionChecker.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSPermissionChecker.java",
            "TestINodeAttributeProvider.java"
        ]
    },
    "hadoop_b4097b9": {
        "bug_id": "hadoop_b4097b9",
        "commit": "https://github.com/apache/hadoop/commit/b4097b96a39bad6214b01989e7f2fb37dad70793",
        "file": [
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/b4097b96a39bad6214b01989e7f2fb37dad70793/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java?ref=b4097b96a39bad6214b01989e7f2fb37dad70793",
                "deletions": 26,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "patch": "@@ -793,39 +793,42 @@ private TimelineEntities getEntityByTime(byte[] base, String entityType,\n             entity = getEntity(entityId, entityType, startTime, queryFields,\n                 iterator, key, kp.getOffset());\n           }\n-          // determine if the retrieved entity matches the provided secondary\n-          // filters, and if so add it to the list of entities to return\n-          boolean filterPassed = true;\n-          if (secondaryFilters != null) {\n-            for (NameValuePair filter : secondaryFilters) {\n-              Object v = entity.getOtherInfo().get(filter.getName());\n-              if (v == null) {\n-                Set<Object> vs = entity.getPrimaryFilters()\n-                    .get(filter.getName());\n-                if (vs == null || !vs.contains(filter.getValue())) {\n+\n+          if (entity != null) {\n+            // determine if the retrieved entity matches the provided secondary\n+            // filters, and if so add it to the list of entities to return\n+            boolean filterPassed = true;\n+            if (secondaryFilters != null) {\n+              for (NameValuePair filter : secondaryFilters) {\n+                Object v = entity.getOtherInfo().get(filter.getName());\n+                if (v == null) {\n+                  Set<Object> vs = entity.getPrimaryFilters()\n+                          .get(filter.getName());\n+                  if (vs == null || !vs.contains(filter.getValue())) {\n+                    filterPassed = false;\n+                    break;\n+                  }\n+                } else if (!v.equals(filter.getValue())) {\n                   filterPassed = false;\n                   break;\n                 }\n-              } else if (!v.equals(filter.getValue())) {\n-                filterPassed = false;\n-                break;\n               }\n             }\n-          }\n-          if (filterPassed) {\n-            if (entity.getDomainId() == null) {\n-              entity.setDomainId(DEFAULT_DOMAIN_ID);\n-            }\n-            if (checkAcl == null || checkAcl.check(entity)) {\n-              // Remove primary filter and other info if they are added for\n-              // matching secondary filters\n-              if (addPrimaryFilters) {\n-                entity.setPrimaryFilters(null);\n+            if (filterPassed) {\n+              if (entity.getDomainId() == null) {\n+                entity.setDomainId(DEFAULT_DOMAIN_ID);\n               }\n-              if (addOtherInfo) {\n-                entity.setOtherInfo(null);\n+              if (checkAcl == null || checkAcl.check(entity)) {\n+                // Remove primary filter and other info if they are added for\n+                // matching secondary filters\n+                if (addPrimaryFilters) {\n+                  entity.setPrimaryFilters(null);\n+                }\n+                if (addOtherInfo) {\n+                  entity.setOtherInfo(null);\n+                }\n+                entities.addEntity(entity);\n               }\n-              entities.addEntity(entity);\n             }\n           }\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/b4097b96a39bad6214b01989e7f2fb37dad70793/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "sha": "e85505f73e877128a52bd24b0668469b55a5e097",
                "status": "modified"
            }
        ],
        "message": "YARN-9744. RollingLevelDBTimelineStore.getEntityByTime fails with NPE. Contributed by Prabhu Joseph.",
        "parent": "https://github.com/apache/hadoop/commit/0b507d2ddf132985b43b4e2d3ad11d7fd2d90cd3",
        "patched_files": [
            "RollingLevelDBTimelineStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRollingLevelDBTimelineStore.java"
        ]
    },
    "hadoop_b489080": {
        "bug_id": "hadoop_b489080",
        "commit": "https://github.com/apache/hadoop/commit/b48908033fcac7a4bd4313c1fd1457999fba08e1",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b48908033fcac7a4bd4313c1fd1457999fba08e1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=b48908033fcac7a4bd4313c1fd1457999fba08e1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1029,6 +1029,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-8749. Fix findbugs warnings in BlockManager.java.\n     (Brahma Reddy Battula via aajisaka)\n \n+    HDFS-2956. calling fetchdt without a --renewer argument throws NPE\n+    (vinayakumarb)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b48908033fcac7a4bd4313c1fd1457999fba08e1/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "13b2621362d7aa7f85033c5d8547a5f2ed7ab417",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b48908033fcac7a4bd4313c1fd1457999fba08e1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java?ref=b48908033fcac7a4bd4313c1fd1457999fba08e1",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
                "patch": "@@ -929,7 +929,7 @@ public void updatePipeline(String clientName, ExtendedBlock oldBlock,\n       throws IOException {\n     GetDelegationTokenRequestProto req = GetDelegationTokenRequestProto\n         .newBuilder()\n-        .setRenewer(renewer.toString())\n+        .setRenewer(renewer == null ? \"\" : renewer.toString())\n         .build();\n     try {\n       GetDelegationTokenResponseProto resp = rpcProxy.getDelegationToken(null, req);",
                "raw_url": "https://github.com/apache/hadoop/raw/b48908033fcac7a4bd4313c1fd1457999fba08e1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/ClientNamenodeProtocolTranslatorPB.java",
                "sha": "566d54f01da642b5d3c557233a4125aa7eca9d21",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/b48908033fcac7a4bd4313c1fd1457999fba08e1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java?ref=b48908033fcac7a4bd4313c1fd1457999fba08e1",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "patch": "@@ -18,7 +18,10 @@\n \n package org.apache.hadoop.hdfs.tools;\n \n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n import static org.mockito.Matchers.anyString;\n import static org.mockito.Mockito.doReturn;\n import static org.mockito.Mockito.doThrow;\n@@ -28,12 +31,18 @@\n import java.util.Iterator;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.web.WebHdfsFileSystem;\n import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.tools.FakeRenewer;\n import org.junit.Assert;\n import org.junit.Rule;\n@@ -105,4 +114,34 @@ public void testReturnedTokenIsNull() throws Exception {\n     Assert.assertFalse(p.getFileSystem(conf).exists(p));\n \n   }\n+\n+  @Test\n+  public void testDelegationTokenWithoutRenewerViaRPC() throws Exception {\n+    conf.setBoolean(DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY, true);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0)\n+        .build();\n+    try {\n+      cluster.waitActive();\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      // Should be able to fetch token without renewer.\n+      LocalFileSystem localFileSystem = FileSystem.getLocal(conf);\n+      Path p = new Path(f.getRoot().getAbsolutePath(), tokenFile);\n+      p = localFileSystem.makeQualified(p);\n+      DelegationTokenFetcher.saveDelegationToken(conf, fs, null, p);\n+      Credentials creds = Credentials.readTokenStorageFile(p, conf);\n+      Iterator<Token<?>> itr = creds.getAllTokens().iterator();\n+      assertTrue(\"token not exist error\", itr.hasNext());\n+      assertNotNull(\"Token should be there without renewer\", itr.next());\n+      try {\n+        // Without renewer renewal of token should fail.\n+        DelegationTokenFetcher.renewTokens(conf, p);\n+        fail(\"Should have failed to renew\");\n+      } catch (AccessControlException e) {\n+        GenericTestUtils.assertExceptionContains(\n+            \"tried to renew a token without a renewer\", e);\n+      }\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b48908033fcac7a4bd4313c1fd1457999fba08e1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "sha": "80a1a6c93a502d1678953c9ba8d86e45bbe2fb69",
                "status": "modified"
            }
        ],
        "message": "HDFS-2956. calling fetchdt without a --renewer argument throws NPE (Contributed by Vinayakumar B)HDFS-2956. calling fetchdt without a --renewer argument throws NPE (Contributed by Vinayakumar B)",
        "parent": "https://github.com/apache/hadoop/commit/d66302ed9b2c25b560d8319d6d755aee7cfa4d67",
        "patched_files": [
            "DelegationTokenFetcher.java",
            "ClientNamenodeProtocolTranslatorPB.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationTokenFetcher.java"
        ]
    },
    "hadoop_b57f08c": {
        "bug_id": "hadoop_b57f08c",
        "commit": "https://github.com/apache/hadoop/commit/b57f08c0d2dae57b545a3baa213e18464060ae3b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b57f08c0d2dae57b545a3baa213e18464060ae3b/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=b57f08c0d2dae57b545a3baa213e18464060ae3b",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -997,6 +997,8 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-3724. Use POSIX nftw(3) instead of fts(3) (Alan Burlison via aw)\n \n+    YARN-4246. NPE while listing app attempt. (Nijel S F via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b57f08c0d2dae57b545a3baa213e18464060ae3b/hadoop-yarn-project/CHANGES.txt",
                "sha": "192843e25b8f8795197756f886b230f211fb94f8",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b57f08c0d2dae57b545a3baa213e18464060ae3b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java?ref=b57f08c0d2dae57b545a3baa213e18464060ae3b",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java",
                "patch": "@@ -349,8 +349,9 @@ private int printApplicationAttemptReport(String applicationAttemptId)\n       appAttemptReportStr.println(appAttemptReport\n           .getYarnApplicationAttemptState());\n       appAttemptReportStr.print(\"\\tAMContainer : \");\n-      appAttemptReportStr.println(appAttemptReport.getAMContainerId()\n-          .toString());\n+      appAttemptReportStr\n+          .println(appAttemptReport.getAMContainerId() == null ? \"N/A\"\n+              : appAttemptReport.getAMContainerId().toString());\n       appAttemptReportStr.print(\"\\tTracking-URL : \");\n       appAttemptReportStr.println(appAttemptReport.getTrackingUrl());\n       appAttemptReportStr.print(\"\\tRPC Port : \");\n@@ -667,6 +668,7 @@ private void listApplicationAttempts(String applicationId) throws YarnException,\n       writer.printf(APPLICATION_ATTEMPTS_PATTERN, appAttemptReport\n           .getApplicationAttemptId(), appAttemptReport\n           .getYarnApplicationAttemptState(), appAttemptReport\n+          .getAMContainerId() == null ? \"N/A\" : appAttemptReport\n           .getAMContainerId().toString(), appAttemptReport.getTrackingUrl());\n     }\n     writer.flush();",
                "raw_url": "https://github.com/apache/hadoop/raw/b57f08c0d2dae57b545a3baa213e18464060ae3b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java",
                "sha": "b4860748a2d7fabcf54af6467ad1099f327afdd1",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/b57f08c0d2dae57b545a3baa213e18464060ae3b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java?ref=b57f08c0d2dae57b545a3baa213e18464060ae3b",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java",
                "patch": "@@ -1590,4 +1590,27 @@ private String createNodeCLIHelpMessage() throws IOException {\n   private static String normalize(String s) {\n     return SPACES_PATTERN.matcher(s).replaceAll(\" \"); // single space\n   }\n+\n+  @Test\n+  public void testAppAttemptReportWhileContainerIsNotAssigned()\n+      throws Exception {\n+    ApplicationCLI cli = createAndGetAppCLI();\n+    ApplicationId applicationId = ApplicationId.newInstance(1234, 5);\n+    ApplicationAttemptId attemptId =\n+        ApplicationAttemptId.newInstance(applicationId, 1);\n+    ApplicationAttemptReport attemptReport =\n+        ApplicationAttemptReport.newInstance(attemptId, \"host\", 124, \"url\",\n+            \"oUrl\", \"diagnostics\", YarnApplicationAttemptState.SCHEDULED, null,\n+            1000l, 2000l);\n+    when(client.getApplicationAttemptReport(any(ApplicationAttemptId.class)))\n+        .thenReturn(attemptReport);\n+    int result =\n+        cli.run(new String[] { \"applicationattempt\", \"-status\",\n+            attemptId.toString() });\n+    assertEquals(0, result);\n+    result =\n+        cli.run(new String[] { \"applicationattempt\", \"-list\",\n+            applicationId.toString() });\n+    assertEquals(0, result);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b57f08c0d2dae57b545a3baa213e18464060ae3b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/cli/TestYarnCLI.java",
                "sha": "3dab504e05ab962f451dab6b04185505e355fb12",
                "status": "modified"
            }
        ],
        "message": "YARN-4246. NPE while listing app attempt. (Nijel S F via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/1aa735c188a308ca608694546c595e3c51f38612",
        "patched_files": [
            "ApplicationCLI.java",
            "CHANGES.java",
            "YarnCLI.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestYarnCLI.java"
        ]
    },
    "hadoop_b5cdf78": {
        "bug_id": "hadoop_b5cdf78",
        "commit": "https://github.com/apache/hadoop/commit/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -598,6 +598,9 @@ Release 2.7.2 - UNRELEASED\n \n   BUG FIXES\n \n+    YARN-3793. Several NPEs when deleting local files on NM recovery (Varun\n+    Saxena via jlowe)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/CHANGES.txt",
                "sha": "3620a718a74ea02eaea1e12686321100483320ab",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java?ref=b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
                "patch": "@@ -276,10 +276,10 @@ private void uploadLogsForContainers(boolean appFinished) {\n             aggregator.doContainerLogAggregation(writer, appFinished);\n         if (uploadedFilePathsInThisCycle.size() > 0) {\n           uploadedLogsInThisCycle = true;\n+          this.delService.delete(this.userUgi.getShortUserName(), null,\n+              uploadedFilePathsInThisCycle\n+                  .toArray(new Path[uploadedFilePathsInThisCycle.size()]));\n         }\n-        this.delService.delete(this.userUgi.getShortUserName(), null,\n-          uploadedFilePathsInThisCycle\n-            .toArray(new Path[uploadedFilePathsInThisCycle.size()]));\n \n         // This container is finished, and all its logs have been uploaded,\n         // remove it from containerLogAggregators.",
                "raw_url": "https://github.com/apache/hadoop/raw/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
                "sha": "654eb0b1a52b6db6c7a4123f407e79f2460793c5",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java?ref=b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "patch": "@@ -31,6 +31,7 @@\n import static org.mockito.Mockito.never;\n import static org.mockito.Mockito.reset;\n import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n@@ -280,6 +281,41 @@ public void testLocalFileDeletionOnDiskFull() throws Exception {\n     verifyLocalFileDeletion(logAggregationService);\n   }\n \n+  /* Test to verify fix for YARN-3793 */\n+  @Test\n+  public void testNoLogsUploadedOnAppFinish() throws Exception {\n+    this.delSrvc = new DeletionService(createContainerExecutor());\n+    delSrvc = spy(delSrvc);\n+    this.delSrvc.init(conf);\n+    this.conf.set(YarnConfiguration.NM_LOG_DIRS, localLogDir.getAbsolutePath());\n+    this.conf.set(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,\n+        this.remoteRootLogDir.getAbsolutePath());\n+\n+    LogAggregationService logAggregationService = new LogAggregationService(\n+        dispatcher, this.context, this.delSrvc, super.dirsHandler);\n+    logAggregationService.init(this.conf);\n+    logAggregationService.start();\n+\n+    ApplicationId app = BuilderUtils.newApplicationId(1234, 1);\n+    File appLogDir = new File(localLogDir, ConverterUtils.toString(app));\n+    appLogDir.mkdir();\n+    LogAggregationContext context =\n+        LogAggregationContext.newInstance(\"HOST*\", \"sys*\");\n+    logAggregationService.handle(new LogHandlerAppStartedEvent(app, this.user,\n+        null, ContainerLogsRetentionPolicy.ALL_CONTAINERS, this.acls, context));\n+\n+    ApplicationAttemptId appAttemptId =\n+        BuilderUtils.newApplicationAttemptId(app, 1);\n+    ContainerId cont = BuilderUtils.newContainerId(appAttemptId, 1);\n+    writeContainerLogs(appLogDir, cont, new String[] { \"stdout\",\n+        \"stderr\", \"syslog\" });\n+    logAggregationService.handle(new LogHandlerContainerFinishedEvent(cont, 0));\n+    logAggregationService.handle(new LogHandlerAppFinishedEvent(app));\n+    logAggregationService.stop();\n+    delSrvc.stop();\n+    // Aggregated logs should not be deleted if not uploaded.\n+    verify(delSrvc, times(0)).delete(user, null);\n+  }\n \n   @Test\n   public void testNoContainerOnNode() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "sha": "6a3d270eac01f910ce0980be26e3e3397951e638",
                "status": "modified"
            }
        ],
        "message": "YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/eac1d1894354e90d314087af8e7fb168ddef9a3d",
        "patched_files": [
            "LogAggregationService.java",
            "CHANGES.java",
            "AppLogAggregatorImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLogAggregationService.java"
        ]
    },
    "hadoop_b7070f3": {
        "bug_id": "hadoop_b7070f3",
        "commit": "https://github.com/apache/hadoop/commit/b7070f3308fc4c6a8a9a25021562169cae87d223",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -1442,8 +1442,10 @@ public static InetSocketAddress getBindAddress(Configuration conf) {\n    * @param conf\n    * @throws Exception\n    */\n-  private static void deleteRMStateStore(Configuration conf) throws Exception {\n+  @VisibleForTesting\n+  static void deleteRMStateStore(Configuration conf) throws Exception {\n     RMStateStore rmStore = RMStateStoreFactory.getStore(conf);\n+    rmStore.setResourceManager(new ResourceManager());\n     rmStore.init(conf);\n     rmStore.start();\n     try {\n@@ -1455,9 +1457,11 @@ private static void deleteRMStateStore(Configuration conf) throws Exception {\n     }\n   }\n \n-  private static void removeApplication(Configuration conf, String applicationId)\n+  @VisibleForTesting\n+  static void removeApplication(Configuration conf, String applicationId)\n       throws Exception {\n     RMStateStore rmStore = RMStateStoreFactory.getStore(conf);\n+    rmStore.setResourceManager(new ResourceManager());\n     rmStore.init(conf);\n     rmStore.start();\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "8ddbc20569e881c42641ccc22a2f301f6af37abf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java",
                "patch": "@@ -86,7 +86,8 @@\n public abstract class RMStateStore extends AbstractService {\n \n   // constants for RM App state and RMDTSecretManagerState.\n-  protected static final String RM_APP_ROOT = \"RMAppRoot\";\n+  @VisibleForTesting\n+  public static final String RM_APP_ROOT = \"RMAppRoot\";\n   protected static final String RM_DT_SECRET_MANAGER_ROOT = \"RMDTSecretManagerRoot\";\n   protected static final String DELEGATION_KEY_PREFIX = \"DelegationKey_\";\n   protected static final String DELEGATION_TOKEN_PREFIX = \"RMDelegationToken_\";",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java",
                "sha": "a6527d8a4023b11ffaff99ec5b719ebc59c4ef28",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -127,7 +127,8 @@\n       \"RMDTSequentialNumber\";\n   private static final String RM_DT_MASTER_KEYS_ROOT_ZNODE_NAME =\n       \"RMDTMasterKeysRoot\";\n-  protected static final String ROOT_ZNODE_NAME = \"ZKRMStateRoot\";\n+  @VisibleForTesting\n+  public static final String ROOT_ZNODE_NAME = \"ZKRMStateRoot\";\n   protected static final Version CURRENT_VERSION_INFO =\n       Version.newInstance(1, 3);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "cf6380f55ed09d730272bba04519f44743d48d7b",
                "status": "modified"
            },
            {
                "additions": 103,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java",
                "changes": 103,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java",
                "patch": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.test.TestingServer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.ha.HAServiceProtocol;\n+import org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore;\n+import org.junit.Test;\n+\n+public class TestRMStoreCommands {\n+\n+  @Test\n+  public void testFormatStateStoreCmdForZK() throws Exception {\n+    StateChangeRequestInfo req = new StateChangeRequestInfo(\n+        HAServiceProtocol.RequestSource.REQUEST_BY_USER);\n+    try (TestingServer curatorTestingServer =\n+        TestZKRMStateStore.setupCuratorServer();\n+        CuratorFramework curatorFramework = TestZKRMStateStore.\n+            setupCuratorFramework(curatorTestingServer)) {\n+      Configuration conf = TestZKRMStateStore.createHARMConf(\"rm1,rm2\", \"rm1\",\n+          1234, false, curatorTestingServer);\n+      ResourceManager rm = new MockRM(conf);\n+      rm.start();\n+      rm.getRMContext().getRMAdminService().transitionToActive(req);\n+      String zkStateRoot = ZKRMStateStore.ROOT_ZNODE_NAME;\n+      assertEquals(\"RM State store parent path should have a child node \" +\n+          zkStateRoot, zkStateRoot, curatorFramework.getChildren().forPath(\n+              YarnConfiguration.DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH).get(0));\n+      rm.close();\n+      try {\n+        ResourceManager.deleteRMStateStore(conf);\n+      } catch (Exception e) {\n+        fail(\"Exception should not be thrown during format rm state store\" +\n+            \" operation.\");\n+      }\n+      assertTrue(\"After store format parent path should have no child nodes\",\n+          curatorFramework.getChildren().forPath(\n+          YarnConfiguration.DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH).isEmpty());\n+    }\n+  }\n+\n+  @Test\n+  public void testRemoveApplicationFromStateStoreCmdForZK() throws Exception {\n+    StateChangeRequestInfo req = new StateChangeRequestInfo(\n+        HAServiceProtocol.RequestSource.REQUEST_BY_USER);\n+    try (TestingServer curatorTestingServer =\n+        TestZKRMStateStore.setupCuratorServer();\n+        CuratorFramework curatorFramework = TestZKRMStateStore.\n+            setupCuratorFramework(curatorTestingServer)) {\n+      Configuration conf = TestZKRMStateStore.createHARMConf(\"rm1,rm2\", \"rm1\",\n+          1234, false, curatorTestingServer);\n+      ResourceManager rm = new MockRM(conf);\n+      rm.start();\n+      rm.getRMContext().getRMAdminService().transitionToActive(req);\n+      rm.close();\n+      String appId = ApplicationId.newInstance(\n+          System.currentTimeMillis(), 1).toString();\n+      String appRootPath = YarnConfiguration.\n+          DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH + \"/\"+\n+          ZKRMStateStore.ROOT_ZNODE_NAME + \"/\" + RMStateStore.RM_APP_ROOT;\n+      String appIdPath = appRootPath + \"/\" + appId;\n+      curatorFramework.create().forPath(appIdPath);\n+      assertEquals(\"Application node for \" + appId + \"should exist\",\n+          appId, curatorFramework.getChildren().forPath(appRootPath).get(0));\n+      try {\n+        ResourceManager.removeApplication(conf, appId);\n+      } catch (Exception e) {\n+        fail(\"Exception should not be thrown while removing app from \" +\n+            \"rm state store.\");\n+      }\n+      assertTrue(\"After remove app from store there should be no child nodes\" +\n+          \" in app root path\",\n+          curatorFramework.getChildren().forPath(appRootPath).isEmpty());\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java",
                "sha": "6c74616379e735a08cc29adf2fa66eb1fd114ff3",
                "status": "added"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 15,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java",
                "patch": "@@ -53,7 +53,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n import org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM;\n-import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.ZooDefs.Perms;\n import org.apache.zookeeper.data.ACL;\n@@ -79,15 +78,26 @@\n   private TestingServer curatorTestingServer;\n   private CuratorFramework curatorFramework;\n \n-  @Before\n-  public void setupCuratorServer() throws Exception {\n-    curatorTestingServer = new TestingServer();\n+  public static TestingServer setupCuratorServer() throws Exception {\n+    TestingServer curatorTestingServer = new TestingServer();\n     curatorTestingServer.start();\n-    curatorFramework = CuratorFrameworkFactory.builder()\n+    return curatorTestingServer;\n+  }\n+\n+  public static CuratorFramework setupCuratorFramework(\n+      TestingServer curatorTestingServer) throws Exception {\n+    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n         .connectString(curatorTestingServer.getConnectString())\n         .retryPolicy(new RetryNTimes(100, 100))\n         .build();\n     curatorFramework.start();\n+    return curatorFramework;\n+  }\n+\n+  @Before\n+  public void setupCurator() throws Exception {\n+    curatorTestingServer = setupCuratorServer();\n+    curatorFramework = setupCuratorFramework(curatorTestingServer);\n   }\n \n   @After\n@@ -243,19 +253,21 @@ protected synchronized void storeVersion() throws Exception {\n     Assert.assertEquals(defaultVersion, store.loadVersion());\n   }\n \n-  private Configuration createHARMConf(\n-      String rmIds, String rmId, int adminPort) {\n+  public static Configuration createHARMConf(String rmIds, String rmId,\n+      int adminPort, boolean autoFailoverEnabled,\n+      TestingServer curatorTestServer) {\n     Configuration conf = new YarnConfiguration();\n     conf.setBoolean(YarnConfiguration.RM_HA_ENABLED, true);\n     conf.set(YarnConfiguration.RM_HA_IDS, rmIds);\n     conf.setBoolean(YarnConfiguration.RECOVERY_ENABLED, true);\n     conf.set(YarnConfiguration.RM_STORE, ZKRMStateStore.class.getName());\n     conf.set(YarnConfiguration.RM_ZK_ADDRESS,\n-        curatorTestingServer.getConnectString());\n+        curatorTestServer.getConnectString());\n     conf.setInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, ZK_TIMEOUT_MS);\n     conf.set(YarnConfiguration.RM_HA_ID, rmId);\n     conf.set(YarnConfiguration.RM_WEBAPP_ADDRESS, \"localhost:0\");\n-\n+    conf.setBoolean(\n+        YarnConfiguration.AUTO_FAILOVER_ENABLED, autoFailoverEnabled);\n     for (String rpcAddress : YarnConfiguration.getServiceAddressConfKeys(conf)) {\n       for (String id : HAUtil.getRMHAIds(conf)) {\n         conf.set(HAUtil.addSuffix(rpcAddress, id), \"localhost:0\");\n@@ -293,8 +305,8 @@ public void testZKRootPathAcls() throws Exception {\n             ZKRMStateStore.ROOT_ZNODE_NAME;\n \n     // Start RM with HA enabled\n-    Configuration conf = createHARMConf(\"rm1,rm2\", \"rm1\", 1234);\n-    conf.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);\n+    Configuration conf =\n+        createHARMConf(\"rm1,rm2\", \"rm1\", 1234, false, curatorTestingServer);\n     ResourceManager rm = new MockRM(conf);\n     rm.start();\n     rm.getRMContext().getRMAdminService().transitionToActive(req);\n@@ -336,8 +348,8 @@ public void testFencing() throws Exception {\n     StateChangeRequestInfo req = new StateChangeRequestInfo(\n         HAServiceProtocol.RequestSource.REQUEST_BY_USER);\n \n-    Configuration conf1 = createHARMConf(\"rm1,rm2\", \"rm1\", 1234);\n-    conf1.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);\n+    Configuration conf1 =\n+        createHARMConf(\"rm1,rm2\", \"rm1\", 1234, false, curatorTestingServer);\n     ResourceManager rm1 = new MockRM(conf1);\n     rm1.start();\n     rm1.getRMContext().getRMAdminService().transitionToActive(req);\n@@ -347,8 +359,8 @@ public void testFencing() throws Exception {\n         HAServiceProtocol.HAServiceState.ACTIVE,\n         rm1.getRMContext().getRMAdminService().getServiceStatus().getState());\n \n-    Configuration conf2 = createHARMConf(\"rm1,rm2\", \"rm2\", 5678);\n-    conf2.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);\n+    Configuration conf2 =\n+        createHARMConf(\"rm1,rm2\", \"rm2\", 5678, false, curatorTestingServer);\n     ResourceManager rm2 = new MockRM(conf2);\n     rm2.start();\n     rm2.getRMContext().getRMAdminService().transitionToActive(req);",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java",
                "sha": "f71cf25568d9c5bcc9672fd12e3ea18bc3ed2b1a",
                "status": "modified"
            }
        ],
        "message": "YARN-5874. RM -format-state-store and -remove-application-from-state-store commands fail with NPE. Contributed by Varun Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/296c5de0cfee88389cf9f90263280b2034e54cd5",
        "patched_files": [
            "ResourceManager.java",
            "RMStateStore.java",
            "ZKRMStateStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStore.java",
            "TestRMStoreCommands.java",
            "TestResourceManager.java"
        ]
    },
    "hadoop_b778887": {
        "bug_id": "hadoop_b778887",
        "commit": "https://github.com/apache/hadoop/commit/b778887af59d96f1fac30cae14be1cabbdb74c8b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b778887af59d96f1fac30cae14be1cabbdb74c8b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java?ref=b778887af59d96f1fac30cae14be1cabbdb74c8b",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "patch": "@@ -1219,7 +1219,7 @@ public void run() {\n     }\n   }\n \n-  @Test\n+  @Test(timeout = 60000)\n   public void testBlockManagerMachinesArray() throws Exception {\n     final Configuration conf = new HdfsConfiguration();\n     final MiniDFSCluster cluster =\n@@ -1230,6 +1230,8 @@ public void testBlockManagerMachinesArray() throws Exception {\n     final Path filePath = new Path(\"/tmp.txt\");\n     final long fileLen = 1L;\n     DFSTestUtil.createFile(fs, filePath, fileLen, (short) 3, 1L);\n+    DFSTestUtil.waitForReplication((DistributedFileSystem)fs,\n+        filePath, (short) 3, 60000);\n     ArrayList<DataNode> datanodes = cluster.getDataNodes();\n     assertEquals(datanodes.size(), 4);\n     FSNamesystem ns = cluster.getNamesystem();",
                "raw_url": "https://github.com/apache/hadoop/raw/b778887af59d96f1fac30cae14be1cabbdb74c8b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "sha": "6b1a9795de364e0aa821519fa0f5ccc787e9c8f6",
                "status": "modified"
            }
        ],
        "message": "HDFS-12112. TestBlockManager#testBlockManagerMachinesArray sometimes fails with NPE. Contributed by Wei-Chiu Chuang.",
        "parent": "https://github.com/apache/hadoop/commit/06ece483222b82404ee198159c6866db89043459",
        "patched_files": [
            "BlockManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_b897d6c": {
        "bug_id": "hadoop_b897d6c",
        "commit": "https://github.com/apache/hadoop/commit/b897d6c35a0036ab8b6a73f8dc76064f351b612d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=b897d6c35a0036ab8b6a73f8dc76064f351b612d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -558,6 +558,9 @@ Release 0.23.4 - UNRELEASED\n     MAPREDUCE-4691. Historyserver can report \"Unknown job\" after RM says job\n     has completed (Robert Joseph Evans via jlowe)\n \n+    MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE\n+    (jlowe via bobby)\n+\n Release 0.23.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "9e98ac96e1d250ab9b6265c43322bbd37c6f80f3",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java?ref=b897d6c35a0036ab8b6a73f8dc76064f351b612d",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "patch": "@@ -42,6 +42,8 @@\n \n public class CompletedTask implements Task {\n \n+  private static final Counters EMPTY_COUNTERS = new Counters();\n+\n   private final TaskId taskId;\n   private final TaskInfo taskInfo;\n   private TaskReport report;\n@@ -124,7 +126,11 @@ private void constructTaskReport() {\n     report.setFinishTime(taskInfo.getFinishTime());\n     report.setTaskState(getState());\n     report.setProgress(getProgress());\n-    report.setCounters(TypeConverter.toYarn(getCounters()));\n+    Counters counters = getCounters();\n+    if (counters == null) {\n+      counters = EMPTY_COUNTERS;\n+    }\n+    report.setCounters(TypeConverter.toYarn(counters));\n     if (successfulAttempt != null) {\n       report.setSuccessfulAttempt(successfulAttempt);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "sha": "830b64f1ad364a1d9982e3dad1b0bd37f2175732",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/hadoop/blob/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java?ref=b897d6c35a0036ab8b6a73f8dc76064f351b612d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "patch": "@@ -49,6 +49,7 @@\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId;\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskId;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskState;\n import org.apache.hadoop.mapreduce.v2.app.MRApp;\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n@@ -402,6 +403,63 @@ public void testHistoryParsingForFailedAttempts() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testCountersForFailedTask() throws Exception {\n+    LOG.info(\"STARTING testCountersForFailedTask\");\n+    try {\n+    Configuration conf = new Configuration();\n+    conf\n+        .setClass(\n+            CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n+            MyResolver.class, DNSToSwitchMapping.class);\n+    RackResolver.init(conf);\n+    MRApp app = new MRAppWithHistoryWithFailedTask(2, 1, true,\n+        this.getClass().getName(), true);\n+    app.submit(conf);\n+    Job job = app.getContext().getAllJobs().values().iterator().next();\n+    JobId jobId = job.getID();\n+    app.waitForState(job, JobState.FAILED);\n+\n+    // make sure all events are flushed\n+    app.waitForState(Service.STATE.STOPPED);\n+\n+    String jobhistoryDir = JobHistoryUtils\n+        .getHistoryIntermediateDoneDirForUser(conf);\n+    JobHistory jobHistory = new JobHistory();\n+    jobHistory.init(conf);\n+\n+    JobIndexInfo jobIndexInfo = jobHistory.getJobFileInfo(jobId)\n+        .getJobIndexInfo();\n+    String jobhistoryFileName = FileNameIndexUtils\n+        .getDoneFileName(jobIndexInfo);\n+\n+    Path historyFilePath = new Path(jobhistoryDir, jobhistoryFileName);\n+    FSDataInputStream in = null;\n+    FileContext fc = null;\n+    try {\n+      fc = FileContext.getFileContext(conf);\n+      in = fc.open(fc.makeQualified(historyFilePath));\n+    } catch (IOException ioe) {\n+      LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\n+      throw (new Exception(\"Can not open History File\"));\n+    }\n+\n+    JobHistoryParser parser = new JobHistoryParser(in);\n+    JobInfo jobInfo = parser.parse();\n+    Exception parseException = parser.getParseException();\n+    Assert.assertNull(\"Caught an expected exception \" + parseException,\n+        parseException);\n+    for (Map.Entry<TaskID,TaskInfo> entry : jobInfo.getAllTasks().entrySet()) {\n+      TaskId yarnTaskID = TypeConverter.toYarn(entry.getKey());\n+      CompletedTask ct = new CompletedTask(yarnTaskID, entry.getValue());\n+      Assert.assertNotNull(\"completed task report has null counters\",\n+          ct.getReport().getCounters());\n+    }\n+    } finally {\n+      LOG.info(\"FINISHED testCountersForFailedTask\");\n+    }\n+  }\n+\n   static class MRAppWithHistoryWithFailedAttempt extends MRAppWithHistory {\n \n     public MRAppWithHistoryWithFailedAttempt(int maps, int reduces, boolean autoComplete,\n@@ -422,6 +480,26 @@ protected void attemptLaunched(TaskAttemptId attemptID) {\n     }\n   }\n \n+  static class MRAppWithHistoryWithFailedTask extends MRAppWithHistory {\n+\n+    public MRAppWithHistoryWithFailedTask(int maps, int reduces, boolean autoComplete,\n+        String testName, boolean cleanOnStart) {\n+      super(maps, reduces, autoComplete, testName, cleanOnStart);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Override\n+    protected void attemptLaunched(TaskAttemptId attemptID) {\n+      if (attemptID.getTaskId().getId() == 0) {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_FAILMSG));\n+      } else {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE));\n+      }\n+    }\n+  }\n+\n   public static void main(String[] args) throws Exception {\n     TestJobHistoryParsing t = new TestJobHistoryParsing();\n     t.testHistoryParsing();",
                "raw_url": "https://github.com/apache/hadoop/raw/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "sha": "f9acb1a38212081cd4aa43beb959dc58d3cd310d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE (jlowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1391679 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/03b7ad04fadeb1a98271ac1775f900999989eafb",
        "patched_files": [
            "CompletedTask.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJobHistoryParsing.java"
        ]
    },
    "hadoop_b908c9e": {
        "bug_id": "hadoop_b908c9e",
        "commit": "https://github.com/apache/hadoop/commit/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/CHANGES.txt?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 0,
                "filename": "common/CHANGES.txt",
                "patch": "@@ -346,6 +346,9 @@ Trunk (unreleased changes)\n     HADOOP-7090. Fix resource leaks in s3.INode, BloomMapFile, WritableUtils\n     and CBZip2OutputStream.  (Uma Maheswara Rao G via szetszwo)\n \n+    HADOOP-7440. HttpServer.getParameterValues throws NPE for missing\n+    parameters. (Uma Maheswara Rao G and todd via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/CHANGES.txt",
                "sha": "c84dc559c8557a4434498e93f6e7e12e1e42daef",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/java/org/apache/hadoop/http/HttpServer.java?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/http/HttpServer.java",
                "patch": "@@ -800,6 +800,9 @@ public String getParameter(String name) {\n       public String[] getParameterValues(String name) {\n         String unquoteName = HtmlQuoting.unquoteHtmlChars(name);\n         String[] unquoteValue = rawRequest.getParameterValues(unquoteName);\n+        if (unquoteValue == null) {\n+          return null;\n+        }\n         String[] result = new String[unquoteValue.length];\n         for(int i=0; i < result.length; ++i) {\n           result[i] = HtmlQuoting.quoteHtmlChars(unquoteValue[i]);",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "sha": "6d6864c63dcb62429fd757215868e1cae03c94fd",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 3,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "patch": "@@ -17,11 +17,12 @@\n  */\n package org.apache.hadoop.http;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n+\n+import javax.servlet.http.HttpServletRequest;\n \n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHtmlQuoting {\n \n@@ -62,4 +63,28 @@ private void runRoundTrip(String str) throws Exception {\n     }\n     runRoundTrip(buffer.toString());\n   }\n+  \n+\n+  @Test\n+  public void testRequestQuoting() throws Exception {\n+    HttpServletRequest mockReq = Mockito.mock(HttpServletRequest.class);\n+    HttpServer.QuotingInputFilter.RequestQuoter quoter =\n+      new HttpServer.QuotingInputFilter.RequestQuoter(mockReq);\n+    \n+    Mockito.doReturn(\"a<b\").when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test simple param quoting\",\n+        \"a&lt;b\", quoter.getParameter(\"x\"));\n+    \n+    Mockito.doReturn(null).when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test that missing parameters dont cause NPE\",\n+        null, quoter.getParameter(\"x\"));\n+\n+    Mockito.doReturn(new String[]{\"a<b\", \"b\"}).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test escaping of an array\",\n+        new String[]{\"a&lt;b\", \"b\"}, quoter.getParameterValues(\"x\"));\n+\n+    Mockito.doReturn(null).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test that missing parameters dont cause NPE for array\",\n+        null, quoter.getParameterValues(\"x\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "sha": "9fc53a3b6fb9d50beafba1b66721310239b18b8b",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 0,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "patch": "@@ -45,16 +45,20 @@\n import javax.servlet.http.HttpServletRequestWrapper;\n import javax.servlet.http.HttpServletResponse;\n \n+import junit.framework.Assert;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.http.HttpServer.QuotingInputFilter.RequestQuoter;\n import org.apache.hadoop.security.Groups;\n import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;\n import org.apache.hadoop.security.authorize.AccessControlList;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHttpServer extends HttpServerFunctionalTest {\n   private static HttpServer server;\n@@ -379,4 +383,26 @@ public void testAuthorizationOfDefaultServlets() throws Exception {\n     }\n     myServer.stop();\n   }\n+  \n+  @Test\n+  public void testRequestQuoterWithNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    Mockito.doReturn(null).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertEquals(\"It should return null \"\n+        + \"when there are no values for the parameter\", null, parameterValues);\n+  }\n+\n+  @Test\n+  public void testRequestQuoterWithNotNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    String[] values = new String[] { \"abc\", \"def\" };\n+    Mockito.doReturn(values).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertTrue(\"It should return Parameter Values\", Arrays.equals(\n+        values, parameterValues));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "sha": "0a25236fc8381e3ece6424ab16a94c890afc2f0d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7440. HttpServer.getParameterValues throws NPE for missing parameters. Contributed by Uma Maheswara Rao G and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1143212 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/d7f712cd4262f51ea2972596ce0a48cde623ecf9",
        "patched_files": [
            "HtmlQuoting.java",
            "CHANGES.java",
            "HttpServer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestHtmlQuoting.java",
            "TestHttpServer.java"
        ]
    },
    "hadoop_b9d561c": {
        "bug_id": "hadoop_b9d561c",
        "commit": "https://github.com/apache/hadoop/commit/b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -49,3 +49,5 @@ IMPROVEMENTS:\n     HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)\n \n     HDFS-5417. Fix storage IDs in PBHelper and UpgradeUtilities.  (szetszwo)\n+\n+    HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "d878b66b3032b4dcaa4bfc1dbe688bbd82fcf1d9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1833,7 +1833,10 @@ private void reportDiff(DatanodeDescriptor dn, DatanodeStorage storage,\n       ReplicaState iState = itBR.getCurrentReplicaState();\n       BlockInfo storedBlock = processReportedBlock(dn, storage.getStorageID(),\n           iblk, iState, toAdd, toInvalidate, toCorrupt, toUC);\n-      toRemove.remove(storedBlock);\n+\n+      if (storedBlock != null) {\n+        toRemove.remove(storedBlock);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "6d5c604ba7dbc631855003ffb675575f3d842d5d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "patch": "@@ -187,7 +187,7 @@ public LinkedElement getNext() {\n         + hours + \" hours for block pool \" + bpid);\n \n     // get the list of blocks and arrange them in random order\n-    List<Block> arr = dataset.getFinalizedBlocks(blockPoolId);\n+    List<FinalizedReplica> arr = dataset.getFinalizedBlocks(blockPoolId);\n     Collections.shuffle(arr);\n     \n     long scanTime = -1;",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "sha": "13a83bce5fdc7a701bba62f192f37f588df07ead",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "patch": "@@ -230,10 +230,6 @@ private static String getSuffix(File f, String prefix) {\n       throw new RuntimeException(prefix + \" is not a prefix of \" + fullPath);\n     }\n \n-    ScanInfo(long blockId) {\n-      this(blockId, null, null, null);\n-    }\n-\n     ScanInfo(long blockId, File blockFile, File metaFile, FsVolumeSpi vol) {\n       this.blockId = blockId;\n       String condensedVolPath = vol == null ? null :\n@@ -439,8 +435,8 @@ void scan() {\n         diffs.put(bpid, diffRecord);\n         \n         statsRecord.totalBlocks = blockpoolReport.length;\n-        List<Block> bl = dataset.getFinalizedBlocks(bpid);\n-        Block[] memReport = bl.toArray(new Block[bl.size()]);\n+        List<FinalizedReplica> bl = dataset.getFinalizedBlocks(bpid);\n+        FinalizedReplica[] memReport = bl.toArray(new FinalizedReplica[bl.size()]);\n         Arrays.sort(memReport); // Sort based on blockId\n   \n         int d = 0; // index for blockpoolReport\n@@ -458,7 +454,8 @@ void scan() {\n           }\n           if (info.getBlockId() > memBlock.getBlockId()) {\n             // Block is missing on the disk\n-            addDifference(diffRecord, statsRecord, memBlock.getBlockId());\n+            addDifference(diffRecord, statsRecord,\n+                          memBlock.getBlockId(), info.getVolume());\n             m++;\n             continue;\n           }\n@@ -478,7 +475,9 @@ void scan() {\n           m++;\n         }\n         while (m < memReport.length) {\n-          addDifference(diffRecord, statsRecord, memReport[m++].getBlockId());\n+          FinalizedReplica current = memReport[m++];\n+          addDifference(diffRecord, statsRecord,\n+                        current.getBlockId(), current.getVolume());\n         }\n         while (d < blockpoolReport.length) {\n           statsRecord.missingMemoryBlocks++;\n@@ -502,10 +501,11 @@ private void addDifference(LinkedList<ScanInfo> diffRecord,\n \n   /** Block is not found on the disk */\n   private void addDifference(LinkedList<ScanInfo> diffRecord,\n-                             Stats statsRecord, long blockId) {\n+                             Stats statsRecord, long blockId,\n+                             FsVolumeSpi vol) {\n     statsRecord.missingBlockFile++;\n     statsRecord.missingMetaFile++;\n-    diffRecord.add(new ScanInfo(blockId));\n+    diffRecord.add(new ScanInfo(blockId, null, null, vol));\n   }\n \n   /** Is the given volume still valid in the dataset? */",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "sha": "17ec35d6fb670e13c585e456bbdfafc917a8d991",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "patch": "@@ -61,6 +61,10 @@ public FinalizedReplica(FinalizedReplica from) {\n     this.unlinked = from.isUnlinked();\n   }\n \n+  public FinalizedReplica(ReplicaInfo replicaInfo) {\n+    super(replicaInfo);\n+  }\n+\n   @Override  // ReplicaInfo\n   public ReplicaState getState() {\n     return ReplicaState.FINALIZED;",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "sha": "1a852c346689ca36638e027e966dcc8ae1b080c2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hdfs.protocol.HdfsBlocksMetadata;\n import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.DataStorage;\n+import org.apache.hadoop.hdfs.server.datanode.FinalizedReplica;\n import org.apache.hadoop.hdfs.server.datanode.Replica;\n import org.apache.hadoop.hdfs.server.datanode.ReplicaInPipelineInterface;\n import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory;\n@@ -98,7 +99,7 @@ public RollingLogs createRollingLogs(String bpid, String prefix\n   public Map<String, Object> getVolumeInfoMap();\n \n   /** @return a list of finalized blocks for the given block pool. */\n-  public List<Block> getFinalizedBlocks(String bpid);\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid);\n \n   /**\n    * Check whether the in-memory block record matches the block on the disk,",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "sha": "90edd5104ffbf33263006a7c4a17a07433bb74f4",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -1079,11 +1079,12 @@ public BlockListAsLongs getBlockReport(String bpid) {\n    * Get the list of finalized blocks from in-memory blockmap for a block pool.\n    */\n   @Override\n-  public synchronized List<Block> getFinalizedBlocks(String bpid) {\n-    ArrayList<Block> finalized = new ArrayList<Block>(volumeMap.size(bpid));\n+  public synchronized List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n+    ArrayList<FinalizedReplica> finalized =\n+        new ArrayList<FinalizedReplica>(volumeMap.size(bpid));\n     for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n       if(b.getState() == ReplicaState.FINALIZED) {\n-        finalized.add(new Block(b));\n+        finalized.add(new FinalizedReplica(b));\n       }\n     }\n     return finalized;",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "8677131d4abbe7ffcae4aa85738d8991ce486dcc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -1006,7 +1006,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n   }\n \n   @Override\n-  public List<Block> getFinalizedBlocks(String bpid) {\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n     throw new UnsupportedOperationException();\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "6f3bed9fda04b728c1665a972ff34b09696110fd",
                "status": "modified"
            },
            {
                "additions": 115,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "changes": 163,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 48,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "patch": "@@ -17,14 +17,17 @@\n  */\n package org.apache.hadoop.hdfs.server.datanode;\n \n+import static org.hamcrest.core.Is.is;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n import java.io.File;\n import java.io.FilenameFilter;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Map;\n import java.util.Random;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.TimeoutException;\n@@ -89,7 +92,7 @@\n   private MiniDFSCluster cluster;\n   private DistributedFileSystem fs;\n \n-  Random rand = new Random(RAND_LIMIT);\n+  private static Random rand = new Random(RAND_LIMIT);\n \n   private static Configuration conf;\n \n@@ -113,6 +116,57 @@ public void shutDownCluster() throws IOException {\n     cluster.shutdown();\n   }\n \n+  private static StorageBlockReport[] getBlockReports(DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n+  // Get block reports but modify the GS of one of the blocks.\n+  private static StorageBlockReport[] getBlockReportsCorruptSingleBlockGS(\n+      DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    boolean corruptedBlock = false;\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      if (!corruptedBlock) {\n+        blockList[4] = rand.nextInt();      // Bad GS.\n+        corruptedBlock = true;\n+      }\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n   /**\n    * Test write a file, verifies and closes it. Then the length of the blocks\n    * are messed up and BlockReport is forced.\n@@ -153,10 +207,8 @@ public void blockReport_01() throws IOException {\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     List<LocatedBlock> blocksAfterReport =\n       DFSTestUtil.getAllBlocks(fs.open(filePath));\n@@ -211,7 +263,6 @@ public void blockReport_02() throws IOException {\n     for (Integer aRemovedIndex : removedIndex) {\n       blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());\n     }\n-    ArrayList<Block> blocks = locatedToBlocks(lBlocks, removedIndex);\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Number of blocks allocated \" + lBlocks.size());\n@@ -225,8 +276,11 @@ public void blockReport_02() throws IOException {\n       for (File f : findAllFiles(dataDir,\n         new MyFileFilter(b.getBlockName(), true))) {\n         DataNodeTestUtils.getFSDataset(dn0).unfinalizeBlock(b);\n-        if (!f.delete())\n+        if (!f.delete()) {\n           LOG.warn(\"Couldn't delete \" + b.getBlockName());\n+        } else {\n+          LOG.debug(\"Deleted file \" + f.toString());\n+        }\n       }\n     }\n \n@@ -235,10 +289,8 @@ public void blockReport_02() throws IOException {\n     // all blocks belong to the same file, hence same BP\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn0.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn0, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     BlockManagerTestUtil.getComputedDatanodeWork(cluster.getNamesystem()\n         .getBlockManager());\n@@ -253,9 +305,8 @@ public void blockReport_02() throws IOException {\n \n \n   /**\n-   * Test writes a file and closes it. Then test finds a block\n-   * and changes its GS to be < of original one.\n-   * New empty block is added to the list of blocks.\n+   * Test writes a file and closes it.\n+   * Block reported is generated with a bad GS for a single block.\n    * Block report is forced and the check for # of corrupted blocks is performed.\n    *\n    * @throws IOException in case of an error\n@@ -264,41 +315,65 @@ public void blockReport_02() throws IOException {\n   public void blockReport_03() throws IOException {\n     final String METHOD_NAME = GenericTestUtils.getMethodName();\n     Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n-\n-    ArrayList<Block> blocks =\n-      prepareForRide(filePath, METHOD_NAME, FILE_SIZE);\n-\n-    // The block with modified GS won't be found. Has to be deleted\n-    blocks.get(0).setGenerationStamp(rand.nextLong());\n-    // This new block is unknown to NN and will be mark for deletion.\n-    blocks.add(new Block());\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n     \n     // all blocks belong to the same file, hence same BP\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] reports = getBlockReportsCorruptSingleBlockGS(dn, poolId);\n     DatanodeCommand dnCmd =\n-      cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+      cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Got the command: \" + dnCmd);\n     }\n     printStats();\n \n-    assertEquals(\"Wrong number of CorruptedReplica+PendingDeletion \" +\n-      \"blocks is found\", 2,\n-        cluster.getNamesystem().getCorruptReplicaBlocks() +\n-        cluster.getNamesystem().getPendingDeletionBlocks());\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(1L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(0L));\n   }\n \n   /**\n-   * This test isn't a representative case for BlockReport\n-   * The empty method is going to be left here to keep the naming\n-   * of the test plan in synch with the actual implementation\n+   * Test writes a file and closes it.\n+   * Block reported is generated with an extra block.\n+   * Block report is forced and the check for # of pendingdeletion\n+   * blocks is performed.\n+   *\n+   * @throws IOException in case of an error\n    */\n-  public void blockReport_04() {\n+  @Test\n+  public void blockReport_04() throws IOException {\n+    final String METHOD_NAME = GenericTestUtils.getMethodName();\n+    Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n+\n+\n+    DataNode dn = cluster.getDataNodes().get(DN_N0);\n+    // all blocks belong to the same file, hence same BP\n+    String poolId = cluster.getNamesystem().getBlockPoolId();\n+\n+    // Create a bogus new block which will not be present on the namenode.\n+    ExtendedBlock b = new ExtendedBlock(\n+        poolId, rand.nextLong(), 1024L, rand.nextLong());\n+    dn.getFSDataset().createRbw(b);\n+\n+    DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    DatanodeCommand dnCmd =\n+        cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Got the command: \" + dnCmd);\n+    }\n+    printStats();\n+\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(0L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(1L));\n   }\n \n   // Client requests new block from NN. The test corrupts this very block\n@@ -331,10 +406,8 @@ public void blockReport_06() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     printStats();\n     assertEquals(\"Wrong number of PendingReplication Blocks\",\n       0, cluster.getNamesystem().getUnderReplicatedBlocks());\n@@ -382,9 +455,7 @@ public void blockReport_07() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] report = getBlockReports(dn, poolId);\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n     assertEquals(\"Wrong number of Corrupted blocks\",\n@@ -407,7 +478,7 @@ public void blockReport_07() throws Exception {\n     }\n     \n     report[0] = new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n+        report[0].getStorage(),\n         new BlockListAsLongs(blocks, null).getBlockListAsLongs());\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n@@ -458,9 +529,7 @@ public void blockReport_08() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",\n@@ -506,9 +575,7 @@ public void blockReport_09() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "sha": "21d0339888a6729d0c75a8febe0d7c9413df252c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "patch": "@@ -447,7 +447,7 @@ void testScanInfoObject(long blockId, File blockFile, File metaFile)\n   \n   void testScanInfoObject(long blockId) throws Exception {\n     DirectoryScanner.ScanInfo scanInfo =\n-        new DirectoryScanner.ScanInfo(blockId);\n+        new DirectoryScanner.ScanInfo(blockId, null, null, null);\n     assertEquals(blockId, scanInfo.getBlockId());\n     assertNull(scanInfo.getBlockFile());\n     assertNull(scanInfo.getMetaFile());",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "sha": "f5b535d394363b27012b10a4b62bf1d7de7481d1",
                "status": "modified"
            }
        ],
        "message": "HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1536179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/9043a92219149390d81f7443180ff62efc9b4c29",
        "patched_files": [
            "SimulatedFSDataset.java",
            "DirectoryScanner.java",
            "FsDatasetImpl.java",
            "CHANGES_HDFS-2832.java",
            "BlockPoolSliceScanner.java",
            "BlockManager.java",
            "FinalizedReplica.java",
            "FsDatasetSpi.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestSimulatedFSDataset.java",
            "TestBlockReport.java",
            "TestDirectoryScanner.java"
        ]
    },
    "hadoop_b9e74da": {
        "bug_id": "hadoop_b9e74da",
        "commit": "https://github.com/apache/hadoop/commit/b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -180,3 +180,5 @@ HDFS-2733. Document HA configuration and CLI. (atm)\n HDFS-2794. Active NN may purge edit log files before standby NN has a chance to read them (todd)\n \n HDFS-2901. Improvements for SBN web UI - not show under-replicated/missing blocks. (Brandon Li via jitendra)\n+\n+HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. (Bikas Saha via jitendra)",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "36c162482b0eb0bf27c12881b58dd11c50b67e73",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "patch": "@@ -135,8 +135,7 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n    */\n   List<RemoteEditLog> getRemoteEditLogs(long firstTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(\n-        FileUtil.listFiles(currentDir));\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<RemoteEditLog> ret = Lists.newArrayListWithCapacity(\n         allLogFiles.size());\n \n@@ -155,6 +154,20 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n     return ret;\n   }\n \n+  /**\n+   * returns matching edit logs via the log directory. Simple helper function\n+   * that lists the files in the logDir and calls matchEditLogs(File[])\n+   * \n+   * @param logDir\n+   *          directory to match edit logs in\n+   * @return matched edit logs\n+   * @throws IOException\n+   *           IOException thrown for invalid logDir\n+   */\n+  static List<EditLogFile> matchEditLogs(File logDir) throws IOException {\n+    return matchEditLogs(FileUtil.listFiles(logDir));\n+  }\n+  \n   static List<EditLogFile> matchEditLogs(File[] filesInStorage) {\n     List<EditLogFile> ret = Lists.newArrayList();\n     for (File f : filesInStorage) {\n@@ -278,7 +291,7 @@ public long getNumberOfTransactions(long fromTxId, boolean inProgressOk)\n   synchronized public void recoverUnfinalizedSegments() throws IOException {\n     File currentDir = sd.getCurrentDir();\n     LOG.info(\"Recovering unfinalized segments in \" + currentDir);\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n \n     for (EditLogFile elf : allLogFiles) {\n       if (elf.getFile().equals(currentInProgress)) {\n@@ -318,7 +331,7 @@ synchronized public void recoverUnfinalizedSegments() throws IOException {\n \n   private List<EditLogFile> getLogFiles(long fromTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<EditLogFile> logFiles = Lists.newArrayList();\n     \n     for (EditLogFile elf : allLogFiles) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "sha": "1eca2797b44c09be1830b59be2c639d7f061435b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "patch": "@@ -440,7 +440,7 @@ public static EditLogFile findLatestEditsLog(StorageDirectory sd)\n   throws IOException {\n     File currentDir = sd.getCurrentDir();\n     List<EditLogFile> foundEditLogs \n-      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir.listFiles()));\n+      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir));\n     return Collections.max(foundEditLogs, EditLogFile.COMPARE_BY_START_TXID);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "sha": "665e088cb800bd95af4a5a4771d5f5b280e188b6",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "patch": "@@ -315,6 +315,15 @@ public void testGetRemoteEditLog() throws IOException {\n         \"\", getLogsAsString(fjm, 9999));\n   }\n \n+  /**\n+   * tests that passing an invalid dir to matchEditLogs throws IOException \n+   */\n+  @Test(expected = IOException.class)\n+  public void testMatchEditLogInvalidDirThrowsIOException() throws IOException {\n+    File badDir = new File(\"does not exist\");\n+    FileJournalManager.matchEditLogs(badDir);\n+  }\n+  \n   /**\n    * Make sure that we starting reading the correct op when we request a stream\n    * with a txid in the middle of an edit log file.",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "sha": "def293657768e434d587492e65dea58631153e45",
                "status": "modified"
            }
        ],
        "message": "HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. Contributed by Bikas Saha.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1241757 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/acacde55e6a4488cd749eba630ff2e68c4dc5c63",
        "patched_files": [
            "FileJournalManager.java",
            "CHANGES.java",
            "FSImageTestUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFileJournalManager.java"
        ]
    },
    "hadoop_ba1f9d6": {
        "bug_id": "hadoop_ba1f9d6",
        "commit": "https://github.com/apache/hadoop/commit/ba1f9d66d94ed0b85084d7c40c09a87478b3a05a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ba1f9d66d94ed0b85084d7c40c09a87478b3a05a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java?ref=ba1f9d66d94ed0b85084d7c40c09a87478b3a05a",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java",
                "patch": "@@ -910,6 +910,8 @@ public void setPort(int port) {\n     @Override\n     public void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent evt) \n         throws Exception {\n+      super.channelOpen(ctx, evt);\n+\n       if ((maxShuffleConnections > 0) && (accepted.size() >= maxShuffleConnections)) {\n         LOG.info(String.format(\"Current number of shuffle connections (%d) is \" + \n             \"greater than or equal to the max allowed shuffle connections (%d)\", \n@@ -925,8 +927,6 @@ public void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent evt)\n         return;\n       }\n       accepted.add(evt.getChannel());\n-      super.channelOpen(ctx, evt);\n-     \n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/ba1f9d66d94ed0b85084d7c40c09a87478b3a05a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java",
                "sha": "c2226855fd555f635efdcf2210db03774e1466ac",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-7156. NullPointerException when reaching max shuffle connections. Contributed by Peter Bacsko",
        "parent": "https://github.com/apache/hadoop/commit/08d69d91f2f09a3ee7711f9d56a535787b30ffd2",
        "patched_files": [
            "ShuffleHandler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestShuffleHandler.java"
        ]
    },
    "hadoop_ba38db4": {
        "bug_id": "hadoop_ba38db4",
        "commit": "https://github.com/apache/hadoop/commit/ba38db4f5b7d8a1432a9a1b4adaa5c1545218799",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/ba38db4f5b7d8a1432a9a1b4adaa5c1545218799/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java?ref=ba38db4f5b7d8a1432a9a1b4adaa5c1545218799",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java",
                "patch": "@@ -676,6 +676,12 @@ public boolean run() throws IOException, YarnException {\n     }\n \n     QueueInfo queueInfo = yarnClient.getQueueInfo(this.amQueue);\n+    if (queueInfo == null) {\n+      throw new IllegalArgumentException(String\n+          .format(\"Queue %s not present in scheduler configuration.\",\n+              this.amQueue));\n+    }\n+\n     LOG.info(\"Queue info\"\n         + \", queueName=\" + queueInfo.getQueueName()\n         + \", queueCurrentCapacity=\" + queueInfo.getCurrentCapacity()",
                "raw_url": "https://github.com/apache/hadoop/raw/ba38db4f5b7d8a1432a9a1b4adaa5c1545218799/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java",
                "sha": "257f590c122fbe258e9f30df49a4282c1011366e",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/ba38db4f5b7d8a1432a9a1b4adaa5c1545218799/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java?ref=ba38db4f5b7d8a1432a9a1b4adaa5c1545218799",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java",
                "patch": "@@ -1728,6 +1728,24 @@ public void testDistributedShellAMResourcesWithUnknownResource()\n     client.run();\n   }\n \n+  @Test(expected=IllegalArgumentException.class)\n+  public void testDistributedShellNonExistentQueue()\n+      throws Exception {\n+    String[] args = {\n+        \"--jar\",\n+        APPMASTER_JAR,\n+        \"--num_containers\",\n+        \"1\",\n+        \"--shell_command\",\n+        Shell.WINDOWS ? \"dir\" : \"ls\",\n+        \"--queue\",\n+        \"non-existent-queue\"\n+    };\n+    Client client = new Client(new Configuration(yarnCluster.getConfig()));\n+    client.init(args);\n+    client.run();\n+  }\n+\n   @Test\n   public void testDistributedShellWithSingleFileLocalization()\n       throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/ba38db4f5b7d8a1432a9a1b4adaa5c1545218799/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java",
                "sha": "1c606abe826cff89eaaf28b39b658c84e8052170",
                "status": "modified"
            }
        ],
        "message": "YARN-9257. Distributed Shell client throws a NPE for a non-existent queue. Contributed by Charan Hebri.",
        "parent": "https://github.com/apache/hadoop/commit/aa7ce50e557b0257ca7980d1d736a62b1f6ccb5d",
        "patched_files": [
            "Client.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDistributedShell.java"
        ]
    },
    "hadoop_ba90c9c": {
        "bug_id": "hadoop_ba90c9c",
        "commit": "https://github.com/apache/hadoop/commit/ba90c9c867151ef484a091dfb1b66e147ad1f100",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ba90c9c867151ef484a091dfb1b66e147ad1f100/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=ba90c9c867151ef484a091dfb1b66e147ad1f100",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -155,6 +155,9 @@ Release 2.0.3-alpha - Unreleased\n     YARN-283. Fair scheduler fails to get queue info without root prefix. \n     (sandyr via tucu)\n \n+    YARN-192. Node update causes NPE in the fair scheduler.\n+    (Sandy Ryza via tomwhite)\n+\n Release 2.0.2-alpha - 2012-09-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ba90c9c867151ef484a091dfb1b66e147ad1f100/hadoop-yarn-project/CHANGES.txt",
                "sha": "baa0d4ef5e43a20fb4c53bda061dc604968b072a",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/ba90c9c867151ef484a091dfb1b66e147ad1f100/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java?ref=ba90c9c867151ef484a091dfb1b66e147ad1f100",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java",
                "patch": "@@ -18,6 +18,9 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;\n \n+import java.util.Arrays;\n+import java.util.Collection;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience.Private;\n@@ -293,12 +296,16 @@ public Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n     } else {\n       // If this app is over quota, don't schedule anything\n       if (!(getRunnable())) { return Resources.none(); }\n-\n     }\n+\n+    Collection<Priority> prioritiesToTry = (reserved) ? \n+        Arrays.asList(node.getReservedContainer().getReservedPriority()) : \n+        app.getPriorities();\n+    \n     // For each priority, see if we can schedule a node local, rack local\n     // or off-switch request. Rack of off-switch requests may be delayed\n     // (not scheduled) in order to promote better locality.\n-    for (Priority priority : app.getPriorities()) {\n+    for (Priority priority : prioritiesToTry) {\n       app.addSchedulingOpportunity(priority);\n       NodeType allowedLocality = app.getAllowedLocalityLevel(priority,\n           scheduler.getNumClusterNodes(), scheduler.getNodeLocalityThreshold(),",
                "raw_url": "https://github.com/apache/hadoop/raw/ba90c9c867151ef484a091dfb1b66e147ad1f100/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/AppSchedulable.java",
                "sha": "d26e7cad6fb6ff7bd48fe6a77e795ec2e12941b3",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/hadoop/blob/ba90c9c867151ef484a091dfb1b66e147ad1f100/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=ba90c9c867151ef484a091dfb1b66e147ad1f100",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.io.IOException;\n import java.io.PrintWriter;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Collection;\n import java.util.LinkedList;\n import java.util.List;\n@@ -53,6 +54,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n import org.apache.hadoop.yarn.server.resourcemanager.resource.Resources;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent;\n@@ -1187,4 +1189,58 @@ public void testUserMaxRunningApps() throws Exception {\n     // Request should be fulfilled\n     assertEquals(2, scheduler.applications.get(attId1).getLiveContainers().size());\n   }\n+  \n+  @Test\n+  public void testReservationWhileMultiplePriorities() {\n+    // Add a node\n+    RMNode node1 = MockNodes.newNodeInfo(1, Resources.createResource(1024));\n+    NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);\n+    scheduler.handle(nodeEvent1);\n+\n+    ApplicationAttemptId attId = createSchedulingRequest(1024, \"queue1\",\n+        \"user1\", 1, 2);\n+    scheduler.update();\n+    NodeUpdateSchedulerEvent updateEvent = new NodeUpdateSchedulerEvent(node1,\n+      new ArrayList<ContainerStatus>(), new ArrayList<ContainerStatus>());\n+    scheduler.handle(updateEvent);\n+    \n+    FSSchedulerApp app = scheduler.applications.get(attId);\n+    assertEquals(1, app.getLiveContainers().size());\n+    \n+    ContainerId containerId = scheduler.applications.get(attId)\n+        .getLiveContainers().iterator().next().getContainerId();\n+\n+    // Cause reservation to be created\n+    createSchedulingRequestExistingApplication(1024, 2, attId);\n+    scheduler.update();\n+    scheduler.handle(updateEvent);\n+\n+    assertEquals(1, app.getLiveContainers().size());\n+    \n+    // Create request at higher priority\n+    createSchedulingRequestExistingApplication(1024, 1, attId);\n+    scheduler.update();\n+    scheduler.handle(updateEvent);\n+    \n+    assertEquals(1, app.getLiveContainers().size());\n+    // Reserved container should still be at lower priority\n+    for (RMContainer container : app.getReservedContainers()) {\n+      assertEquals(2, container.getReservedPriority().getPriority());\n+    }\n+    \n+    // Complete container\n+    scheduler.allocate(attId, new ArrayList<ResourceRequest>(),\n+        Arrays.asList(containerId));\n+    \n+    // Schedule at opening\n+    scheduler.update();\n+    scheduler.handle(updateEvent);\n+    \n+    // Reserved container (at lower priority) should be run\n+    Collection<RMContainer> liveContainers = app.getLiveContainers();\n+    assertEquals(1, liveContainers.size());\n+    for (RMContainer liveContainer : liveContainers) {\n+      Assert.assertEquals(2, liveContainer.getContainer().getPriority().getPriority());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/ba90c9c867151ef484a091dfb1b66e147ad1f100/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "47a35378e595bb714b6d572c7728ff08f0551ab6",
                "status": "modified"
            }
        ],
        "message": "YARN-192. Node update causes NPE in the fair scheduler. Contributed by Sandy Ryza\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1428314 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/36c5fe9961a905385282d1a05ced08c83684dd02",
        "patched_files": [
            "FairScheduler.java",
            "AppSchedulable.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop_bbe8591": {
        "bug_id": "hadoop_bbe8591",
        "commit": "https://github.com/apache/hadoop/commit/bbe859177d67fcdfd5377b1abff4a637fbbd4587",
        "file": [
            {
                "additions": 129,
                "blob_url": "https://github.com/apache/hadoop/blob/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java",
                "changes": 149,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java?ref=bbe859177d67fcdfd5377b1abff4a637fbbd4587",
                "deletions": 20,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java",
                "patch": "@@ -168,8 +168,12 @@ public void close() {\n     }\n   }\n \n-  private FederationMetrics getFederationMetrics() {\n-    return this.router.getMetrics();\n+  private FederationMetrics getFederationMetrics() throws IOException {\n+    FederationMetrics metrics = getRouter().getMetrics();\n+    if (metrics == null) {\n+      throw new IOException(\"Federated metrics is not initialized\");\n+    }\n+    return metrics;\n   }\n \n   /////////////////////////////////////////////////////////\n@@ -188,22 +192,42 @@ public String getSoftwareVersion() {\n \n   @Override\n   public long getUsed() {\n-    return getFederationMetrics().getUsedCapacity();\n+    try {\n+      return getFederationMetrics().getUsedCapacity();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the used capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getFree() {\n-    return getFederationMetrics().getRemainingCapacity();\n+    try {\n+      return getFederationMetrics().getRemainingCapacity();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get remaining capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getTotal() {\n-    return getFederationMetrics().getTotalCapacity();\n+    try {\n+      return getFederationMetrics().getTotalCapacity();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to Get total capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getProvidedCapacity() {\n-    return getFederationMetrics().getProvidedSpace();\n+    try {\n+      return getFederationMetrics().getProvidedSpace();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get provided capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -261,39 +285,79 @@ public float getPercentBlockPoolUsed() {\n \n   @Override\n   public long getTotalBlocks() {\n-    return getFederationMetrics().getNumBlocks();\n+    try {\n+      return getFederationMetrics().getNumBlocks();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getNumberOfMissingBlocks() {\n-    return getFederationMetrics().getNumOfMissingBlocks();\n+    try {\n+      return getFederationMetrics().getNumOfMissingBlocks();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of missing blocks\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   @Deprecated\n   public long getPendingReplicationBlocks() {\n-    return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks pending replica\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getPendingReconstructionBlocks() {\n-    return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks pending replica\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   @Deprecated\n   public long getUnderReplicatedBlocks() {\n-    return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks under replicated\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getLowRedundancyBlocks() {\n-    return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks under replicated\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getPendingDeletionBlocks() {\n-    return getFederationMetrics().getNumOfBlocksPendingDeletion();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksPendingDeletion();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks pending deletion\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -471,7 +535,12 @@ public String getJournalTransactionInfo() {\n \n   @Override\n   public long getNNStartedTimeInMillis() {\n-    return this.router.getStartTime();\n+    try {\n+      return getRouter().getStartTime();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the router startup time\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -527,7 +596,12 @@ public long getProvidedCapacityTotal() {\n \n   @Override\n   public long getFilesTotal() {\n-    return getFederationMetrics().getNumFiles();\n+    try {\n+      return getFederationMetrics().getNumFiles();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of files\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -537,12 +611,22 @@ public int getTotalLoad() {\n \n   @Override\n   public int getNumLiveDataNodes() {\n-    return this.router.getMetrics().getNumLiveNodes();\n+    try {\n+      return getFederationMetrics().getNumLiveNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of live nodes\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public int getNumDeadDataNodes() {\n-    return this.router.getMetrics().getNumDeadNodes();\n+    try {\n+      return getFederationMetrics().getNumDeadNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of dead nodes\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -552,17 +636,35 @@ public int getNumStaleDataNodes() {\n \n   @Override\n   public int getNumDecomLiveDataNodes() {\n-    return this.router.getMetrics().getNumDecomLiveNodes();\n+    try {\n+      return getFederationMetrics().getNumDecomLiveNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the number of live decommissioned datanodes\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public int getNumDecomDeadDataNodes() {\n-    return this.router.getMetrics().getNumDecomDeadNodes();\n+    try {\n+      return getFederationMetrics().getNumDecomDeadNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the number of dead decommissioned datanodes\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public int getNumDecommissioningDataNodes() {\n-    return this.router.getMetrics().getNumDecommissioningNodes();\n+    try {\n+      return getFederationMetrics().getNumDecommissioningNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of decommissioning nodes\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -702,4 +804,11 @@ public int getNumEncryptionZones() {\n   public String getVerifyECWithTopologyResult() {\n     return null;\n   }\n+\n+  private Router getRouter() throws IOException {\n+    if (this.router == null) {\n+      throw new IOException(\"Router is not initialized\");\n+    }\n+    return this.router;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java",
                "sha": "a05fdc144989dec0f4e71e5cee9747f797f6da72",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java?ref=bbe859177d67fcdfd5377b1abff4a637fbbd4587",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java",
                "patch": "@@ -586,11 +586,11 @@ public FederationMetrics getMetrics() {\n    *\n    * @return Namenode metrics.\n    */\n-  public NamenodeBeanMetrics getNamenodeMetrics() {\n-    if (this.metrics != null) {\n-      return this.metrics.getNamenodeMetrics();\n+  public NamenodeBeanMetrics getNamenodeMetrics() throws IOException {\n+    if (this.metrics == null) {\n+      throw new IOException(\"Namenode metrics is not initialized\");\n     }\n-    return null;\n+    return this.metrics.getNamenodeMetrics();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java",
                "sha": "3182e27bcc93d46f1c29995a89914715c8ffa503",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java?ref=bbe859177d67fcdfd5377b1abff4a637fbbd4587",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java",
                "patch": "@@ -203,4 +203,18 @@ public void testRouterIDInRouterRpcClient() throws Exception {\n     router.stop();\n     router.close();\n   }\n+\n+  @Test\n+  public void testRouterMetricsWhenDisabled() throws Exception {\n+\n+    Router router = new Router();\n+    router.init(new RouterConfigBuilder(conf).rpc().build());\n+    router.start();\n+\n+    intercept(IOException.class, \"Namenode metrics is not initialized\",\n+        () -> router.getNamenodeMetrics().getCacheCapacity());\n+\n+    router.stop();\n+    router.close();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java",
                "sha": "f83cfda6015eaacafc36087dbcbfc3160db99e2b",
                "status": "modified"
            }
        ],
        "message": "HDFS-13869. RBF: Handle NPE for NamenodeBeanMetrics#getFederationMetrics. Contributed by Ranith Sardar.",
        "parent": "https://github.com/apache/hadoop/commit/01b4126b4e8124edfde20ba4733c6300bb994251",
        "patched_files": [
            "NamenodeBeanMetrics.java",
            "Router.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRouter.java"
        ]
    },
    "hadoop_bbff96b": {
        "bug_id": "hadoop_bbff96b",
        "commit": "https://github.com/apache/hadoop/commit/bbff96be48119774688981d04baf444639135977",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -491,6 +491,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2594. Potential deadlock in RM when querying \n     ApplicationResourceUsageReport. (Wangda Tan via kasha)\n \n+    YARN-2602. Fixed possible NPE in ApplicationHistoryManagerOnTimelineStore.\n+    (Zhijie Shen via jianhe)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/CHANGES.txt",
                "sha": "bfaaa90d0841ebb2fc41544fbe2443e232def9ee",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -227,7 +227,9 @@ private static ApplicationReportExt convertToApplicationReport(\n       if (entityInfo.containsKey(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO)) {\n         String appViewACLsStr = entityInfo.get(\n             ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO).toString();\n-        appViewACLs.put(ApplicationAccessType.VIEW_APP, appViewACLsStr);\n+        if (appViewACLsStr.length() > 0) {\n+          appViewACLs.put(ApplicationAccessType.VIEW_APP, appViewACLsStr);\n+        }\n       }\n       if (field == ApplicationReportField.USER_AND_ACLS) {\n         return new ApplicationReportExt(ApplicationReport.newInstance(",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "sha": "5381bd6cb216566887102013e7237de3a5f044ce",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 47,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -122,7 +122,11 @@ private static void prepareTimelineStore(TimelineStore store)\n     for (int i = 1; i <= SCALE; ++i) {\n       TimelineEntities entities = new TimelineEntities();\n       ApplicationId appId = ApplicationId.newInstance(0, i);\n-      entities.addEntity(createApplicationTimelineEntity(appId));\n+      if (i == 2) {\n+        entities.addEntity(createApplicationTimelineEntity(appId, true));\n+      } else {\n+        entities.addEntity(createApplicationTimelineEntity(appId, false));\n+      }\n       store.put(entities);\n       for (int j = 1; j <= SCALE; ++j) {\n         entities = new TimelineEntities();\n@@ -142,50 +146,58 @@ private static void prepareTimelineStore(TimelineStore store)\n \n   @Test\n   public void testGetApplicationReport() throws Exception {\n-    final ApplicationId appId = ApplicationId.newInstance(0, 1);\n-    ApplicationReport app;\n-    if (callerUGI == null) {\n-      app = historyManager.getApplication(appId);\n-    } else {\n-      app =\n-          callerUGI.doAs(new PrivilegedExceptionAction<ApplicationReport> () {\n-        @Override\n-        public ApplicationReport run() throws Exception {\n-          return historyManager.getApplication(appId);\n-        }\n-      });\n-    }\n-    Assert.assertNotNull(app);\n-    Assert.assertEquals(appId, app.getApplicationId());\n-    Assert.assertEquals(\"test app\", app.getName());\n-    Assert.assertEquals(\"test app type\", app.getApplicationType());\n-    Assert.assertEquals(\"user1\", app.getUser());\n-    Assert.assertEquals(\"test queue\", app.getQueue());\n-    Assert.assertEquals(Integer.MAX_VALUE + 2L, app.getStartTime());\n-    Assert.assertEquals(Integer.MAX_VALUE + 3L, app.getFinishTime());\n-    Assert.assertTrue(Math.abs(app.getProgress() - 1.0F) < 0.0001);\n-    if (callerUGI != null && callerUGI.getShortUserName().equals(\"user3\")) {\n-      Assert.assertEquals(ApplicationAttemptId.newInstance(appId, -1),\n-          app.getCurrentApplicationAttemptId());\n-      Assert.assertEquals(null, app.getHost());\n-      Assert.assertEquals(-1, app.getRpcPort());\n-      Assert.assertEquals(null, app.getTrackingUrl());\n-      Assert.assertEquals(null, app.getOriginalTrackingUrl());\n-      Assert.assertEquals(null, app.getDiagnostics());\n-    } else {\n-      Assert.assertEquals(ApplicationAttemptId.newInstance(appId, 1),\n-          app.getCurrentApplicationAttemptId());\n-      Assert.assertEquals(\"test host\", app.getHost());\n-      Assert.assertEquals(-100, app.getRpcPort());\n-      Assert.assertEquals(\"test tracking url\", app.getTrackingUrl());\n-      Assert.assertEquals(\"test original tracking url\",\n-          app.getOriginalTrackingUrl());\n-      Assert.assertEquals(\"test diagnostics info\", app.getDiagnostics());\n+    for (int i = 1; i <= 2; ++i) {\n+      final ApplicationId appId = ApplicationId.newInstance(0, i);\n+      ApplicationReport app;\n+      if (callerUGI == null) {\n+        app = historyManager.getApplication(appId);\n+      } else {\n+        app =\n+            callerUGI.doAs(new PrivilegedExceptionAction<ApplicationReport> () {\n+          @Override\n+          public ApplicationReport run() throws Exception {\n+            return historyManager.getApplication(appId);\n+          }\n+        });\n+      }\n+      Assert.assertNotNull(app);\n+      Assert.assertEquals(appId, app.getApplicationId());\n+      Assert.assertEquals(\"test app\", app.getName());\n+      Assert.assertEquals(\"test app type\", app.getApplicationType());\n+      Assert.assertEquals(\"user1\", app.getUser());\n+      Assert.assertEquals(\"test queue\", app.getQueue());\n+      Assert.assertEquals(Integer.MAX_VALUE + 2L, app.getStartTime());\n+      Assert.assertEquals(Integer.MAX_VALUE + 3L, app.getFinishTime());\n+      Assert.assertTrue(Math.abs(app.getProgress() - 1.0F) < 0.0001);\n+      // App 2 doesn't have the ACLs, such that the default ACLs \" \" will be used.\n+      // Nobody except admin and owner has access to the details of the app.\n+      if ((i ==  1 && callerUGI != null &&\n+          callerUGI.getShortUserName().equals(\"user3\")) ||\n+          (i ==  2 && callerUGI != null &&\n+          (callerUGI.getShortUserName().equals(\"user2\") ||\n+              callerUGI.getShortUserName().equals(\"user3\")))) {\n+        Assert.assertEquals(ApplicationAttemptId.newInstance(appId, -1),\n+            app.getCurrentApplicationAttemptId());\n+        Assert.assertEquals(null, app.getHost());\n+        Assert.assertEquals(-1, app.getRpcPort());\n+        Assert.assertEquals(null, app.getTrackingUrl());\n+        Assert.assertEquals(null, app.getOriginalTrackingUrl());\n+        Assert.assertEquals(null, app.getDiagnostics());\n+      } else {\n+        Assert.assertEquals(ApplicationAttemptId.newInstance(appId, 1),\n+            app.getCurrentApplicationAttemptId());\n+        Assert.assertEquals(\"test host\", app.getHost());\n+        Assert.assertEquals(-100, app.getRpcPort());\n+        Assert.assertEquals(\"test tracking url\", app.getTrackingUrl());\n+        Assert.assertEquals(\"test original tracking url\",\n+            app.getOriginalTrackingUrl());\n+        Assert.assertEquals(\"test diagnostics info\", app.getDiagnostics());\n+      }\n+      Assert.assertEquals(FinalApplicationStatus.UNDEFINED,\n+          app.getFinalApplicationStatus());\n+      Assert.assertEquals(YarnApplicationState.FINISHED,\n+          app.getYarnApplicationState());\n     }\n-    Assert.assertEquals(FinalApplicationStatus.UNDEFINED,\n-        app.getFinalApplicationStatus());\n-    Assert.assertEquals(YarnApplicationState.FINISHED,\n-        app.getYarnApplicationState());\n   }\n \n   @Test\n@@ -396,7 +408,7 @@ public ContainerReport run() throws Exception {\n   }\n \n   private static TimelineEntity createApplicationTimelineEntity(\n-      ApplicationId appId) {\n+      ApplicationId appId, boolean emptyACLs) {\n     TimelineEntity entity = new TimelineEntity();\n     entity.setEntityType(ApplicationMetricsConstants.ENTITY_TYPE);\n     entity.setEntityId(appId.toString());\n@@ -410,8 +422,12 @@ private static TimelineEntity createApplicationTimelineEntity(\n     entityInfo.put(ApplicationMetricsConstants.QUEUE_ENTITY_INFO, \"test queue\");\n     entityInfo.put(ApplicationMetricsConstants.SUBMITTED_TIME_ENTITY_INFO,\n         Integer.MAX_VALUE + 1L);\n-    entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO,\n-        \"user2\");\n+    if (emptyACLs) {\n+      entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO, \"\");\n+    } else {\n+      entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO,\n+          \"user2\");\n+    }\n     entity.setOtherInfo(entityInfo);\n     TimelineEvent tEvent = new TimelineEvent();\n     tEvent.setEventType(ApplicationMetricsConstants.CREATED_EVENT_TYPE);",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "sha": "49386c5b3cfac1caf36aef63f1c517354664709d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -137,7 +137,7 @@ public void appACLsUpdated(RMApp app, String appViewACLs,\n       dispatcher.getEventHandler().handle(\n           new ApplicationACLsUpdatedEvent(\n               app.getApplicationId(),\n-              appViewACLs,\n+              appViewACLs == null ? \"\" : appViewACLs,\n               updatedTime));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "e2ecf9a83df9c2f2807b6c47b0476bcf1bc1b486",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 146,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 67,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -95,77 +95,89 @@ public static void tearDown() throws Exception {\n \n   @Test(timeout = 10000)\n   public void testPublishApplicationMetrics() throws Exception {\n-    ApplicationId appId = ApplicationId.newInstance(0, 1);\n-    RMApp app = createRMApp(appId);\n-    metricsPublisher.appCreated(app, app.getStartTime());\n-    metricsPublisher.appFinished(app, RMAppState.FINISHED, app.getFinishTime());\n-    metricsPublisher.appACLsUpdated(app, \"uers1,user2\", 4L);\n-    TimelineEntity entity = null;\n-    do {\n-      entity =\n-          store.getEntity(appId.toString(),\n-              ApplicationMetricsConstants.ENTITY_TYPE,\n-              EnumSet.allOf(Field.class));\n-      // ensure three events are both published before leaving the loop\n-    } while (entity == null || entity.getEvents().size() < 3);\n-    // verify all the fields\n-    Assert.assertEquals(ApplicationMetricsConstants.ENTITY_TYPE,\n-        entity.getEntityType());\n-    Assert\n-        .assertEquals(app.getApplicationId().toString(), entity.getEntityId());\n-    Assert\n-        .assertEquals(\n-            app.getName(),\n-            entity.getOtherInfo().get(\n-                ApplicationMetricsConstants.NAME_ENTITY_INFO));\n-    Assert.assertEquals(app.getQueue(),\n-        entity.getOtherInfo()\n-            .get(ApplicationMetricsConstants.QUEUE_ENTITY_INFO));\n-    Assert\n-        .assertEquals(\n-            app.getUser(),\n-            entity.getOtherInfo().get(\n-                ApplicationMetricsConstants.USER_ENTITY_INFO));\n-    Assert\n-        .assertEquals(\n-            app.getApplicationType(),\n+    for (int i = 1; i <= 2; ++i) {\n+      ApplicationId appId = ApplicationId.newInstance(0, i);\n+      RMApp app = createRMApp(appId);\n+      metricsPublisher.appCreated(app, app.getStartTime());\n+      metricsPublisher.appFinished(app, RMAppState.FINISHED, app.getFinishTime());\n+      if (i == 1) {\n+        metricsPublisher.appACLsUpdated(app, \"uers1,user2\", 4L);\n+      } else {\n+        // in case user doesn't specify the ACLs\n+        metricsPublisher.appACLsUpdated(app, null, 4L);\n+      }\n+      TimelineEntity entity = null;\n+      do {\n+        entity =\n+            store.getEntity(appId.toString(),\n+                ApplicationMetricsConstants.ENTITY_TYPE,\n+                EnumSet.allOf(Field.class));\n+        // ensure three events are both published before leaving the loop\n+      } while (entity == null || entity.getEvents().size() < 3);\n+      // verify all the fields\n+      Assert.assertEquals(ApplicationMetricsConstants.ENTITY_TYPE,\n+          entity.getEntityType());\n+      Assert\n+          .assertEquals(app.getApplicationId().toString(), entity.getEntityId());\n+      Assert\n+          .assertEquals(\n+              app.getName(),\n+              entity.getOtherInfo().get(\n+                  ApplicationMetricsConstants.NAME_ENTITY_INFO));\n+      Assert.assertEquals(app.getQueue(),\n+          entity.getOtherInfo()\n+              .get(ApplicationMetricsConstants.QUEUE_ENTITY_INFO));\n+      Assert\n+          .assertEquals(\n+              app.getUser(),\n+              entity.getOtherInfo().get(\n+                  ApplicationMetricsConstants.USER_ENTITY_INFO));\n+      Assert\n+          .assertEquals(\n+              app.getApplicationType(),\n+              entity.getOtherInfo().get(\n+                  ApplicationMetricsConstants.TYPE_ENTITY_INFO));\n+      Assert.assertEquals(app.getSubmitTime(),\n+          entity.getOtherInfo().get(\n+              ApplicationMetricsConstants.SUBMITTED_TIME_ENTITY_INFO));\n+      if (i == 1) {\n+        Assert.assertEquals(\"uers1,user2\",\n             entity.getOtherInfo().get(\n-                ApplicationMetricsConstants.TYPE_ENTITY_INFO));\n-    Assert.assertEquals(app.getSubmitTime(),\n-        entity.getOtherInfo().get(\n-            ApplicationMetricsConstants.SUBMITTED_TIME_ENTITY_INFO));\n-    Assert.assertEquals(\"uers1,user2\",\n-        entity.getOtherInfo().get(\n+                ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO));\n+      } else {\n+        Assert.assertEquals(\"\", entity.getOtherInfo().get(\n             ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO));\n-    boolean hasCreatedEvent = false;\n-    boolean hasFinishedEvent = false;\n-    boolean hasACLsUpdatedEvent = false;\n-    for (TimelineEvent event : entity.getEvents()) {\n-      if (event.getEventType().equals(\n-          ApplicationMetricsConstants.CREATED_EVENT_TYPE)) {\n-        hasCreatedEvent = true;\n-        Assert.assertEquals(app.getStartTime(), event.getTimestamp());\n-      } else if (event.getEventType().equals(\n-          ApplicationMetricsConstants.FINISHED_EVENT_TYPE)) {\n-        hasFinishedEvent = true;\n-        Assert.assertEquals(app.getFinishTime(), event.getTimestamp());\n-        Assert.assertEquals(\n-            app.getDiagnostics().toString(),\n-            event.getEventInfo().get(\n-                ApplicationMetricsConstants.DIAGNOSTICS_INFO_EVENT_INFO));\n-        Assert.assertEquals(\n-            app.getFinalApplicationStatus().toString(),\n-            event.getEventInfo().get(\n-                ApplicationMetricsConstants.FINAL_STATUS_EVENT_INFO));\n-        Assert.assertEquals(YarnApplicationState.FINISHED.toString(), event\n-            .getEventInfo().get(ApplicationMetricsConstants.STATE_EVENT_INFO));\n-      } else if (event.getEventType().equals(\n-          ApplicationMetricsConstants.ACLS_UPDATED_EVENT_TYPE)) {\n-        hasACLsUpdatedEvent = true;\n-        Assert.assertEquals(4L, event.getTimestamp());\n       }\n+      boolean hasCreatedEvent = false;\n+      boolean hasFinishedEvent = false;\n+      boolean hasACLsUpdatedEvent = false;\n+      for (TimelineEvent event : entity.getEvents()) {\n+        if (event.getEventType().equals(\n+            ApplicationMetricsConstants.CREATED_EVENT_TYPE)) {\n+          hasCreatedEvent = true;\n+          Assert.assertEquals(app.getStartTime(), event.getTimestamp());\n+        } else if (event.getEventType().equals(\n+            ApplicationMetricsConstants.FINISHED_EVENT_TYPE)) {\n+          hasFinishedEvent = true;\n+          Assert.assertEquals(app.getFinishTime(), event.getTimestamp());\n+          Assert.assertEquals(\n+              app.getDiagnostics().toString(),\n+              event.getEventInfo().get(\n+                  ApplicationMetricsConstants.DIAGNOSTICS_INFO_EVENT_INFO));\n+          Assert.assertEquals(\n+              app.getFinalApplicationStatus().toString(),\n+              event.getEventInfo().get(\n+                  ApplicationMetricsConstants.FINAL_STATUS_EVENT_INFO));\n+          Assert.assertEquals(YarnApplicationState.FINISHED.toString(), event\n+              .getEventInfo().get(ApplicationMetricsConstants.STATE_EVENT_INFO));\n+        } else if (event.getEventType().equals(\n+            ApplicationMetricsConstants.ACLS_UPDATED_EVENT_TYPE)) {\n+          hasACLsUpdatedEvent = true;\n+          Assert.assertEquals(4L, event.getTimestamp());\n+        }\n+      }\n+      Assert.assertTrue(hasCreatedEvent && hasFinishedEvent && hasACLsUpdatedEvent);\n     }\n-    Assert.assertTrue(hasCreatedEvent && hasFinishedEvent && hasACLsUpdatedEvent);\n   }\n \n   @Test(timeout = 10000)",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "52faf12f506e46ec0199a7913386421736d04118",
                "status": "modified"
            }
        ],
        "message": "YARN-2602. Fixed possible NPE in ApplicationHistoryManagerOnTimelineStore. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/d7075ada5d3019a8c520d34bfddb0cd73a449343",
        "patched_files": [
            "ApplicationHistoryManagerOnTimelineStore.java",
            "CHANGES.java",
            "SystemMetricsPublisher.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationHistoryManagerOnTimelineStore.java",
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_beb65c9": {
        "bug_id": "hadoop_beb65c9",
        "commit": "https://github.com/apache/hadoop/commit/beb65c9465806114237aa271b07b31ff3c1f4404",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/CHANGES.txt",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=beb65c9465806114237aa271b07b31ff3c1f4404",
                "deletions": 2,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -413,12 +413,15 @@ Release 2.8.0 - UNRELEASED\n     YARN-4026. Refactored ContainerAllocator to accept a list of priorites\n     rather than a single priority. (Wangda Tan via jianhe)\n \n-   YARN-4031. Add JvmPauseMonitor to ApplicationHistoryServer and\n-   WebAppProxyServer (djp via rkanter)\n+    YARN-4031. Add JvmPauseMonitor to ApplicationHistoryServer and\n+    WebAppProxyServer (djp via rkanter)\n \n     YARN-4057. If ContainersMonitor is not enabled, only print\n     related log info one time. (Jun Gong via zxu)\n \n+    YARN-1556. NPE getting application report with a null appId. (Weiwei Yang via \n+    junping_du)\n+\n   OPTIMIZATIONS\n \n     YARN-3339. TestDockerContainerExecutor should pull a single image and not",
                "raw_url": "https://github.com/apache/hadoop/raw/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/CHANGES.txt",
                "sha": "0b733a4a40c3a19aad03dead146859f97b7bc815",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=beb65c9465806114237aa271b07b31ff3c1f4404",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -315,6 +315,9 @@ public GetNewApplicationResponse getNewApplication(\n   public GetApplicationReportResponse getApplicationReport(\n       GetApplicationReportRequest request) throws YarnException {\n     ApplicationId applicationId = request.getApplicationId();\n+    if (applicationId == null) {\n+      throw new ApplicationNotFoundException(\"Invalid application id: null\");\n+    }\n \n     UserGroupInformation callerUGI;\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "cce0fe57aa75b24cc02920f6c9a1e0859e3a1290",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java?ref=beb65c9465806114237aa271b07b31ff3c1f4404",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "patch": "@@ -333,6 +333,18 @@ public void testGetApplicationReport() throws Exception {\n           report.getApplicationResourceUsageReport();\n       Assert.assertEquals(10, usageReport.getMemorySeconds());\n       Assert.assertEquals(3, usageReport.getVcoreSeconds());\n+\n+      // if application id is null\n+      GetApplicationReportRequest invalidRequest = recordFactory\n+          .newRecordInstance(GetApplicationReportRequest.class);\n+      invalidRequest.setApplicationId(null);\n+      try {\n+        rmService.getApplicationReport(invalidRequest);\n+      } catch (YarnException e) {\n+        // rmService should return a ApplicationNotFoundException\n+        // when a null application id is provided\n+        Assert.assertTrue(e instanceof ApplicationNotFoundException);\n+      }\n     } finally {\n       rmService.close();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "sha": "6a0b99c7461ca0f490ed90feb85b5efb16c0250f",
                "status": "modified"
            }
        ],
        "message": "YARN-1556. NPE getting application report with a null appId. Contributed by Weiwei Yang.",
        "parent": "https://github.com/apache/hadoop/commit/e166c038c0aaa57b245f985a1c0fadd5fe33c384",
        "patched_files": [
            "CHANGES.java",
            "ClientRMService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestClientRMService.java"
        ]
    },
    "hadoop_bf44d16": {
        "bug_id": "hadoop_bf44d16",
        "commit": "https://github.com/apache/hadoop/commit/bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -324,6 +324,9 @@ Release 2.0.4-beta - UNRELEASED\n     but not in dfs.namenode.edits.dir are silently ignored.  (Arpit Agarwal\n     via szetszwo)\n \n+    HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race \n+    between delete and replication of same file. (umamahesh)\n+\n Release 2.0.3-alpha - 2013-02-06\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3b1a1e36bceb5c82791b0db68a2ddae8471b5c0a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1343,6 +1343,11 @@ static String getFullPathName(INode inode) {\n \n     // fill up the inodes in the path from this inode to root\n     for (int i = 0; i < depth; i++) {\n+      if (inode == null) {\n+        NameNode.stateChangeLog.warn(\"Could not get full path.\"\n+            + \" Corresponding file might have deleted already.\");\n+        return null;\n+      }\n       inodes[depth-i-1] = inode;\n       inode = inode.parent;\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "b11059a4bb47c865d14e439b1bbabfa418b90e2b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java?ref=bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "patch": "@@ -282,7 +282,11 @@ String getLocalName() {\n \n   String getLocalParentDir() {\n     INode inode = isRoot() ? this : getParent();\n-    return (inode != null) ? inode.getFullPathName() : \"\";\n+    String parentDir = \"\";\n+    if (inode != null) {\n+      parentDir = inode.getFullPathName();\n+    }\n+    return (parentDir != null) ? parentDir : \"\";\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "sha": "b407a62da97b1808bbbc8d7169ecd48a9b05292a",
                "status": "modified"
            }
        ],
        "message": "HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race between delete and replication of same file. Contributed by Uma Maheswara Rao G.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448708 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0b73dde6ce865ff94b483558ff0701de9932e211",
        "patched_files": [
            "FSDirectory.java",
            "INode.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFSDirectory.java",
            "TestINode.java"
        ]
    },
    "hadoop_c020b62": {
        "bug_id": "hadoop_c020b62",
        "commit": "https://github.com/apache/hadoop/commit/c020b62cf8de1f3baadc9d2f3410640ef7880543",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -707,6 +707,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-3982. container-executor parsing of container-executor.cfg broken in\n     trunk and branch-2. (Varun Vasudev via xgong)\n \n+    YARN-3919. NPEs' while stopping service after exception during\n+    CommonNodeLabelsManager#start. (varun saxane via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/CHANGES.txt",
                "sha": "8e8a76b7c87b8eb9a2d2bd9eb6d27dddacc621c6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "patch": "@@ -139,7 +139,8 @@ protected void serviceStop() throws Exception {\n       blockNewEvents = true;\n       LOG.info(\"AsyncDispatcher is draining to stop, igonring any new events.\");\n       synchronized (waitForDrained) {\n-        while (!drained && eventHandlingThread.isAlive()) {\n+        while (!drained && eventHandlingThread != null\n+            && eventHandlingThread.isAlive()) {\n           waitForDrained.wait(1000);\n           LOG.info(\"Waiting for AsyncDispatcher to drain. Thread state is :\" +\n               eventHandlingThread.getState());",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "sha": "f6701128ac625fed1dd8ddbcccb6a7ea206a40f3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.LocalFileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeLabel;\n@@ -92,12 +93,7 @@ public void init(Configuration conf) throws Exception {\n \n   @Override\n   public void close() throws IOException {\n-    try {\n-      fs.close();\n-      editlogOs.close();\n-    } catch (IOException e) {\n-      LOG.warn(\"Exception happened whiling shutting down,\", e);\n-    }\n+    IOUtils.cleanup(LOG, fs, editlogOs);\n   }\n \n   private void setFileSystem(Configuration conf) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "sha": "20dc67c10e4a4068eb122076a32a441a98f3eda1",
                "status": "modified"
            }
        ],
        "message": "YARN-3919. NPEs' while stopping service after exception during CommonNodeLabelsManager#start. (varun saxena via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/5205a330b387d2e133ee790b9fe7d5af3cd8bccc",
        "patched_files": [
            "FileSystemNodeLabelsStore.java",
            "AsyncDispatcher.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFileSystemNodeLabelsStore.java",
            "TestAsyncDispatcher.java"
        ]
    },
    "hadoop_c0a903d": {
        "bug_id": "hadoop_c0a903d",
        "commit": "https://github.com/apache/hadoop/commit/c0a903da22c65294b232c7530a6a684ee93daba4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c0a903da22c65294b232c7530a6a684ee93daba4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -525,6 +525,9 @@ Release 2.4.0 - UNRELEASED\n     HDFS-6040. fix DFSClient issue without libhadoop.so and some other\n     ShortCircuitShm cleanups (cmccabe)\n \n+    HDFS-6047 TestPread NPE inside in DFSInputStream hedgedFetchBlockByteRange\n+    (stack)\n+\n   BREAKDOWN OF HDFS-5698 SUBTASKS AND RELATED JIRAS\n \n     HDFS-5717. Save FSImage header in protobuf. (Haohui Mai via jing9)",
                "raw_url": "https://github.com/apache/hadoop/raw/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "acd5f15ae89902dd1eb04f86547b0e58e1cb793b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=c0a903da22c65294b232c7530a6a684ee93daba4",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -1177,8 +1177,11 @@ private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n           // exception already handled in the call method. getFirstToComplete\n           // will remove the failing future from the list. nothing more to do.\n         }\n-        // We got here if exception.  Ignore this node on next go around.\n-        ignored.add(chosenNode.info);\n+        // We got here if exception.  Ignore this node on next go around IFF\n+        // we found a chosenNode to hedge read against.\n+        if (chosenNode != null && chosenNode.info != null) {\n+          ignored.add(chosenNode.info);\n+        }\n       }\n       // executed if we get an error from a data node\n       block = getBlockAt(block.getStartOffset(), false);",
                "raw_url": "https://github.com/apache/hadoop/raw/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "3705a2fd4f353b3074cc01d750440d26ce8da993",
                "status": "modified"
            }
        ],
        "message": "HDFS-6047 TestPread NPE inside in DFSInputStream hedgedFetchBlockByteRange (stack)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1574205 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/88245b6a41171f939b22186c533ea2bc7994f9b3",
        "patched_files": [
            "DFSInputStream.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java"
        ]
    },
    "hadoop_c2460da": {
        "bug_id": "hadoop_c2460da",
        "commit": "https://github.com/apache/hadoop/commit/c2460dad642feee1086442d33c30c24ec77236b9",
        "file": [
            {
                "additions": 96,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java",
                "changes": 143,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 47,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java",
                "patch": "@@ -132,6 +132,9 @@\n   private static final FastDateFormat DATE_FORMAT =\n       FastDateFormat.getInstance(\"yyyyMMddHH\", TimeZone.getTimeZone(\"GMT\"));\n   private final Object lock = new Object();\n+  private boolean initialized = false;\n+  private SubsetConfiguration properties;\n+  private Configuration conf;\n   private String source;\n   private boolean ignoreError;\n   private boolean allowAppend;\n@@ -163,63 +166,102 @@\n   protected static FileSystem suppliedFilesystem = null;\n \n   @Override\n-  public void init(SubsetConfiguration conf) {\n-    basePath = new Path(conf.getString(BASEPATH_KEY, BASEPATH_DEFAULT));\n-    source = conf.getString(SOURCE_KEY, SOURCE_DEFAULT);\n-    ignoreError = conf.getBoolean(IGNORE_ERROR_KEY, false);\n-    allowAppend = conf.getBoolean(ALLOW_APPEND_KEY, false);\n+  public void init(SubsetConfiguration metrics2Properties) {\n+    properties = metrics2Properties;\n+    basePath = new Path(properties.getString(BASEPATH_KEY, BASEPATH_DEFAULT));\n+    source = properties.getString(SOURCE_KEY, SOURCE_DEFAULT);\n+    ignoreError = properties.getBoolean(IGNORE_ERROR_KEY, false);\n+    allowAppend = properties.getBoolean(ALLOW_APPEND_KEY, false);\n \n-    Configuration configuration = loadConf();\n-\n-    UserGroupInformation.setConfiguration(configuration);\n+    conf = loadConf();\n+    UserGroupInformation.setConfiguration(conf);\n \n     // Don't do secure setup if it's not needed.\n     if (UserGroupInformation.isSecurityEnabled()) {\n       // Validate config so that we don't get an NPE\n-      checkForProperty(conf, KEYTAB_PROPERTY_KEY);\n-      checkForProperty(conf, USERNAME_PROPERTY_KEY);\n+      checkForProperty(properties, KEYTAB_PROPERTY_KEY);\n+      checkForProperty(properties, USERNAME_PROPERTY_KEY);\n \n \n       try {\n         // Login as whoever we're supposed to be and let the hostname be pulled\n         // from localhost. If security isn't enabled, this does nothing.\n-        SecurityUtil.login(configuration, conf.getString(KEYTAB_PROPERTY_KEY),\n-            conf.getString(USERNAME_PROPERTY_KEY));\n+        SecurityUtil.login(conf, properties.getString(KEYTAB_PROPERTY_KEY),\n+            properties.getString(USERNAME_PROPERTY_KEY));\n       } catch (IOException ex) {\n         throw new MetricsException(\"Error logging in securely: [\"\n             + ex.toString() + \"]\", ex);\n       }\n     }\n+  }\n+\n+  /**\n+   * Initialize the connection to HDFS and create the base directory. Also\n+   * launch the flush thread.\n+   */\n+  private boolean initFs() {\n+    boolean success = false;\n \n-    fileSystem = getFileSystem(configuration);\n+    fileSystem = getFileSystem();\n \n     // This step isn't strictly necessary, but it makes debugging issues much\n     // easier. We try to create the base directory eagerly and fail with\n     // copious debug info if it fails.\n     try {\n       fileSystem.mkdirs(basePath);\n+      success = true;\n     } catch (Exception ex) {\n-      throw new MetricsException(\"Failed to create \" + basePath + \"[\"\n-          + SOURCE_KEY + \"=\" + source + \", \"\n-          + IGNORE_ERROR_KEY + \"=\" + ignoreError + \", \"\n-          + ALLOW_APPEND_KEY + \"=\" + allowAppend + \", \"\n-          + KEYTAB_PROPERTY_KEY + \"=\"\n-          + conf.getString(KEYTAB_PROPERTY_KEY) + \", \"\n-          + conf.getString(KEYTAB_PROPERTY_KEY) + \"=\"\n-          + configuration.get(conf.getString(KEYTAB_PROPERTY_KEY)) + \", \"\n-          + USERNAME_PROPERTY_KEY + \"=\"\n-          + conf.getString(USERNAME_PROPERTY_KEY) + \", \"\n-          + conf.getString(USERNAME_PROPERTY_KEY) + \"=\"\n-          + configuration.get(conf.getString(USERNAME_PROPERTY_KEY))\n-          + \"] -- \" + ex.toString(), ex);\n+      if (!ignoreError) {\n+        throw new MetricsException(\"Failed to create \" + basePath + \"[\"\n+            + SOURCE_KEY + \"=\" + source + \", \"\n+            + ALLOW_APPEND_KEY + \"=\" + allowAppend + \", \"\n+            + stringifySecurityProperty(KEYTAB_PROPERTY_KEY) + \", \"\n+            + stringifySecurityProperty(USERNAME_PROPERTY_KEY)\n+            + \"] -- \" + ex.toString(), ex);\n+      }\n     }\n \n-    // If we're permitted to append, check if we actually can\n-    if (allowAppend) {\n-      allowAppend = checkAppend(fileSystem);\n+    if (success) {\n+      // If we're permitted to append, check if we actually can\n+      if (allowAppend) {\n+        allowAppend = checkAppend(fileSystem);\n+      }\n+\n+      flushTimer = new Timer(\"RollingFileSystemSink Flusher\", true);\n     }\n \n-    flushTimer = new Timer(\"RollingFileSystemSink Flusher\", true);\n+    return success;\n+  }\n+\n+  /**\n+   * Turn a security property into a nicely formatted set of <i>name=value</i>\n+   * strings, allowing for either the property or the configuration not to be\n+   * set.\n+   *\n+   * @param properties the sink properties\n+   * @param conf the conf\n+   * @param property the property to stringify\n+   * @return the stringified property\n+   */\n+  private String stringifySecurityProperty(String property) {\n+    String securityProperty;\n+\n+    if (properties.containsKey(property)) {\n+      String propertyValue = properties.getString(property);\n+      String confValue = conf.get(properties.getString(property));\n+\n+      if (confValue != null) {\n+        securityProperty = property + \"=\" + propertyValue\n+            + \", \" + properties.getString(property) + \"=\" + confValue;\n+      } else {\n+        securityProperty = property + \"=\" + propertyValue\n+            + \", \" + properties.getString(property) + \"=<NOT SET>\";\n+      }\n+    } else {\n+      securityProperty = property + \"=<NOT SET>\";\n+    }\n+\n+    return securityProperty;\n   }\n \n   /**\n@@ -242,17 +284,17 @@ private static void checkForProperty(SubsetConfiguration conf, String key) {\n    * @return the configuration to use\n    */\n   private Configuration loadConf() {\n-    Configuration conf;\n+    Configuration c;\n \n     if (suppliedConf != null) {\n-      conf = suppliedConf;\n+      c = suppliedConf;\n     } else {\n       // The config we're handed in init() isn't the one we want here, so we\n       // create a new one to pick up the full settings.\n-      conf = new Configuration();\n+      c = new Configuration();\n     }\n \n-    return conf;\n+    return c;\n   }\n \n   /**\n@@ -263,7 +305,7 @@ private Configuration loadConf() {\n    * @return the file system to use\n    * @throws MetricsException thrown if the file system could not be retrieved\n    */\n-  private FileSystem getFileSystem(Configuration conf) throws MetricsException {\n+  private FileSystem getFileSystem() throws MetricsException {\n     FileSystem fs = null;\n \n     if (suppliedFilesystem != null) {\n@@ -317,22 +359,29 @@ private void rollLogDirIfNeeded() throws MetricsException {\n     // because if currentDirPath is null, then currentOutStream is null, but\n     // currentOutStream can be null for other reasons.\n     if ((currentOutStream == null) || !path.equals(currentDirPath)) {\n-      // Close the stream. This step could have been handled already by the\n-      // flusher thread, but if it has, the PrintStream will just swallow the\n-      // exception, which is fine.\n-      if (currentOutStream != null) {\n-        currentOutStream.close();\n+      // If we're not yet connected to HDFS, create the connection\n+      if (!initialized) {\n+        initialized = initFs();\n       }\n \n-      currentDirPath = path;\n+      if (initialized) {\n+        // Close the stream. This step could have been handled already by the\n+        // flusher thread, but if it has, the PrintStream will just swallow the\n+        // exception, which is fine.\n+        if (currentOutStream != null) {\n+          currentOutStream.close();\n+        }\n \n-      try {\n-        rollLogDir();\n-      } catch (IOException ex) {\n-        throwMetricsException(\"Failed to create new log file\", ex);\n-      }\n+        currentDirPath = path;\n \n-      scheduleFlush(now);\n+        try {\n+          rollLogDir();\n+        } catch (IOException ex) {\n+          throwMetricsException(\"Failed to create new log file\", ex);\n+        }\n+\n+        scheduleFlush(now);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java",
                "sha": "9a439013e6e19306eb168e414d9d43883ab61c6d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java",
                "patch": "@@ -175,7 +175,7 @@ protected MetricsSystem initMetricsSystem(String path, boolean ignoreErrors,\n     String prefix = methodName.getMethodName().toLowerCase();\n \n     ConfigBuilder builder = new ConfigBuilder().add(\"*.period\", 10000)\n-        .add(prefix + \".sink.mysink0.class\", ErrorSink.class.getName())\n+        .add(prefix + \".sink.mysink0.class\", MockSink.class.getName())\n         .add(prefix + \".sink.mysink0.basepath\", path)\n         .add(prefix + \".sink.mysink0.source\", \"testsrc\")\n         .add(prefix + \".sink.mysink0.context\", \"test1\")\n@@ -503,8 +503,9 @@ public void assertFileCount(FileSystem fs, Path dir, int expected)\n    * This class is a {@link RollingFileSystemSink} wrapper that tracks whether\n    * an exception has been thrown during operations.\n    */\n-  public static class ErrorSink extends RollingFileSystemSink {\n+  public static class MockSink extends RollingFileSystemSink {\n     public static volatile boolean errored = false;\n+    public static volatile boolean initialized = false;\n \n     @Override\n     public void init(SubsetConfiguration conf) {\n@@ -515,6 +516,8 @@ public void init(SubsetConfiguration conf) {\n \n         throw new MetricsException(ex);\n       }\n+\n+      initialized = true;\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java",
                "sha": "9914c5e30f3565334390e97ce2fbe5dbda1aaca5",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java",
                "patch": "@@ -23,6 +23,8 @@\n import org.junit.Test;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n \n /**\n  * Test the {@link RollingFileSystemSink} class in the context of the local file\n@@ -106,15 +108,15 @@ public void testFailedWrite() {\n     new MyMetrics1().registerWith(ms);\n \n     methodDir.setWritable(false);\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     try {\n       // publish the metrics\n       ms.publishMetricsNow();\n \n       assertTrue(\"No exception was generated while writing metrics \"\n           + \"even though the target directory was not writable\",\n-          ErrorSink.errored);\n+          MockSink.errored);\n \n       ms.stop();\n       ms.shutdown();\n@@ -135,7 +137,7 @@ public void testSilentFailedWrite() {\n     new MyMetrics1().registerWith(ms);\n \n     methodDir.setWritable(false);\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     try {\n       // publish the metrics\n@@ -144,7 +146,7 @@ public void testSilentFailedWrite() {\n       assertFalse(\"An exception was generated while writing metrics \"\n           + \"when the target directory was not writable, even though the \"\n           + \"sink is set to ignore errors\",\n-          ErrorSink.errored);\n+          MockSink.errored);\n \n       ms.stop();\n       ms.shutdown();",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java",
                "sha": "3c6cd27b59b396afdeec123e78d519da6e534bdf",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1047,6 +1047,9 @@ Release 2.9.0 - UNRELEASED\n     HDFS-9608. Disk IO imbalance in HDFS with heterogeneous storages.\n     (Wei Zhou via wang)\n \n+    HDFS-9858. RollingFileSystemSink can throw an NPE on non-secure clusters. \n+    (Daniel Templeton via kasha)\n+\n Release 2.8.0 - UNRELEASED\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "104b46ddc3add9e8485717093c3ad9be462e9753",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java",
                "patch": "@@ -151,12 +151,12 @@ public void testFailedWrite() throws IOException {\n     new MyMetrics1().registerWith(ms);\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.publishMetricsNow(); // publish the metrics\n \n     assertTrue(\"No exception was generated while writing metrics \"\n-        + \"even though HDFS was unavailable\", ErrorSink.errored);\n+        + \"even though HDFS was unavailable\", MockSink.errored);\n \n     ms.stop();\n     ms.shutdown();\n@@ -178,12 +178,12 @@ public void testFailedClose() throws IOException {\n     ms.publishMetricsNow(); // publish the metrics\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.stop();\n \n     assertTrue(\"No exception was generated while stopping sink \"\n-        + \"even though HDFS was unavailable\", ErrorSink.errored);\n+        + \"even though HDFS was unavailable\", MockSink.errored);\n \n     ms.shutdown();\n   }\n@@ -203,13 +203,13 @@ public void testSilentFailedWrite() throws IOException, InterruptedException {\n     new MyMetrics1().registerWith(ms);\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.publishMetricsNow(); // publish the metrics\n \n     assertFalse(\"An exception was generated writing metrics \"\n         + \"while HDFS was unavailable, even though the sink is set to \"\n-        + \"ignore errors\", ErrorSink.errored);\n+        + \"ignore errors\", MockSink.errored);\n \n     ms.stop();\n     ms.shutdown();\n@@ -231,13 +231,13 @@ public void testSilentFailedClose() throws IOException {\n     ms.publishMetricsNow(); // publish the metrics\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.stop();\n \n     assertFalse(\"An exception was generated stopping sink \"\n         + \"while HDFS was unavailable, even though the sink is set to \"\n-        + \"ignore errors\", ErrorSink.errored);\n+        + \"ignore errors\", MockSink.errored);\n \n     ms.shutdown();\n   }\n@@ -283,4 +283,22 @@ public void testFlushThread() throws Exception {\n \n     ms.stop();\n   }\n+\n+  /**\n+   * Test that a failure to connect to HDFS does not cause the init() method\n+   * to fail.\n+   */\n+  @Test\n+  public void testInitWithNoHDFS() {\n+    String path = \"hdfs://\" + cluster.getNameNode().getHostAndPort() + \"/tmp\";\n+\n+    shutdownHdfs();\n+    MockSink.errored = false;\n+    initMetricsSystem(path, true, false);\n+\n+    assertTrue(\"The sink was not initialized as expected\",\n+        MockSink.initialized);\n+    assertFalse(\"The sink threw an unexpected error on initialization\",\n+        MockSink.errored);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java",
                "sha": "9984b34fbff33518d81137c2a29bc0e158316900",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.security.ssl.KeyStoreTestUtil;\n import org.junit.Test;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertTrue;\n \n /**\n  * Test the {@link RollingFileSystemSink} class in the context of HDFS with\n@@ -147,7 +148,7 @@ public void testMissingPropertiesWithSecureHDFS() throws Exception {\n \n       assertTrue(\"No exception was generated initializing the sink against a \"\n           + \"secure cluster even though the principal and keytab properties \"\n-          + \"were missing\", ErrorSink.errored);\n+          + \"were missing\", MockSink.errored);\n     } finally {\n       if (cluster != null) {\n         cluster.shutdown();",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java",
                "sha": "dce4fdcf2294bec9cd9b83f21950ecf2af92cdda",
                "status": "modified"
            }
        ],
        "message": "HDFS-9858. RollingFileSystemSink can throw an NPE on non-secure clusters. (Daniel Templeton via kasha)",
        "parent": "https://github.com/apache/hadoop/commit/b2951f9fbccee8aeab04c1f5ee3fc6db1ef6b2da",
        "patched_files": [
            "RollingFileSystemSinkTestBase.java",
            "CHANGES.java",
            "RollingFileSystemSink.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRollingFileSystemSinkWithSecureHdfs.java",
            "TestRollingFileSystemSinkWithHdfs.java",
            "TestRollingFileSystemSink.java"
        ]
    },
    "hadoop_c30e495": {
        "bug_id": "hadoop_c30e495",
        "commit": "https://github.com/apache/hadoop/commit/c30e495557359b23681a61edbc90cfafafdb7dfe",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java?ref=c30e495557359b23681a61edbc90cfafafdb7dfe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
                "patch": "@@ -226,6 +226,9 @@ Node chooseRandomWithStorageType(final String scope,\n           String nodeLocation = excludedNode.getNetworkLocation()\n               + \"/\" + excludedNode.getName();\n           DatanodeDescriptor dn = (DatanodeDescriptor)getNode(nodeLocation);\n+          if (dn == null) {\n+            continue;\n+          }\n           availableCount -= dn.hasStorageType(type)? 1 : 0;\n         } else {\n           LOG.error(\"Unexpected node type: {}.\", excludedNode.getClass());",
                "raw_url": "https://github.com/apache/hadoop/raw/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
                "sha": "0884fc002c3784e951604e0647fa0066c033ac60",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java?ref=c30e495557359b23681a61edbc90cfafafdb7dfe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java",
                "patch": "@@ -23,6 +23,8 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.StorageType;\n import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.protocol.DatanodeID;\n+import org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfoBuilder;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo;\n import org.apache.hadoop.net.Node;\n@@ -37,9 +39,11 @@\n import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n+\n /**\n  * This class tests the correctness of storage type info stored in\n  * DFSNetworkTopology.\n@@ -368,6 +372,18 @@ public void testChooseRandomWithStorageTypeWithExcluded() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testChooseRandomWithStorageTypeWithExcludedforNullCheck()\n+      throws Exception {\n+    HashSet<Node> excluded = new HashSet<>();\n+\n+    excluded.add(new DatanodeInfoBuilder()\n+        .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build());\n+    Node node = CLUSTER.chooseRandomWithStorageType(\"/\", \"/l1/d1/r1\", excluded,\n+        StorageType.ARCHIVE);\n+\n+    assertNotNull(node);\n+  }\n \n   /**\n    * This test tests the wrapper method. The wrapper method only takes one scope",
                "raw_url": "https://github.com/apache/hadoop/raw/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java",
                "sha": "3360d68f2bd4131416d5cd10ed8165b27ca20c6c",
                "status": "modified"
            }
        ],
        "message": "HDFS-14853. NPE in DFSNetworkTopology#chooseRandomWithStorageType() when the excludedNode is not present. Contributed by Ranith Sardar.",
        "parent": "https://github.com/apache/hadoop/commit/2b5fc95851552599e33674d9a23e7e9af74a304e",
        "patched_files": [
            "DFSNetworkTopology.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSNetworkTopology.java"
        ]
    },
    "hadoop_c32ca00": {
        "bug_id": "hadoop_c32ca00",
        "commit": "https://github.com/apache/hadoop/commit/c32ca00a752dfc885af487bdd9158a67d5359779",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c32ca00a752dfc885af487bdd9158a67d5359779/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=c32ca00a752dfc885af487bdd9158a67d5359779",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -299,6 +299,9 @@ Release 0.23.1 - Unreleased\n     MAPREDUCE-3398. Fixed log aggregation to work correctly in secure mode.\n     (Siddharth Seth via vinodkv)\n \n+    MAPREDUCE-3530. Fixed an NPE occuring during scheduling in the\n+    ResourceManager. (Arun C Murthy via vinodkv)\n+\n Release 0.23.0 - 2011-11-01 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c32ca00a752dfc885af487bdd9158a67d5359779/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "6748d60a7f8edd48b2701df15ac4b4f5dbc0638f",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/c32ca00a752dfc885af487bdd9158a67d5359779/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java?ref=c32ca00a752dfc885af487bdd9158a67d5359779",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "patch": "@@ -262,6 +262,16 @@ public RMNodeState getState() {\n \n   }\n \n+  @Private\n+  public List<ContainerId> getContainersToCleanUp() {\n+    this.readLock.lock();\n+    try {\n+      return new ArrayList<ContainerId>(containersToClean);\n+    } finally {\n+      this.readLock.unlock();\n+    }\n+  }\n+  \n   @Override\n   public List<ContainerId> pullContainersToCleanUp() {\n \n@@ -342,7 +352,6 @@ public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n     @Override\n     public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n-\n       rmNode.containersToClean.add(((\n           RMNodeCleanContainerEvent) event).getContainerId());\n     }\n@@ -396,8 +405,17 @@ public RMNodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n       List<ContainerStatus> completedContainers = \n           new ArrayList<ContainerStatus>();\n       for (ContainerStatus remoteContainer : statusEvent.getContainers()) {\n-        // Process running containers\n         ContainerId containerId = remoteContainer.getContainerId();\n+        \n+        // Don't bother with containers already scheduled for cleanup,\n+        // the scheduler doens't need to know any more about this container\n+        if (rmNode.containersToClean.contains(containerId)) {\n+          LOG.info(\"Container \" + containerId + \" already scheduled for \" +\n+          \t\t\"cleanup, no further processing\");\n+          continue;\n+        }\n+        \n+        // Process running containers\n         if (remoteContainer.getState() == ContainerState.RUNNING) {\n           if (!rmNode.justLaunchedContainers.containsKey(containerId)) {\n             // Just launched container. RM knows about it the first time.",
                "raw_url": "https://github.com/apache/hadoop/raw/c32ca00a752dfc885af487bdd9158a67d5359779/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "sha": "2cadd89071243d8e5afd12beb4683f8c4f84bc94",
                "status": "modified"
            },
            {
                "additions": 148,
                "blob_url": "https://github.com/apache/hadoop/blob/c32ca00a752dfc885af487bdd9158a67d5359779/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "changes": 148,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java?ref=c32ca00a752dfc885af487bdd9158a67d5359779",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "patch": "@@ -0,0 +1,148 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.yarn.server.resourcemanager;\n+\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerStatus;\n+import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n+import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.MemStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.InlineDispatcher;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeCleanContainerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeAddedSchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType;\n+import org.apache.hadoop.yarn.util.BuilderUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+public class TestRMNodeTransitions {\n+\n+  RMNodeImpl node;\n+  \n+  private RMContext rmContext;\n+  private YarnScheduler scheduler;\n+\n+  private SchedulerEventType eventType;\n+  private List<ContainerStatus> completedContainers;\n+  \n+  private final class TestSchedulerEventDispatcher implements\n+  EventHandler<SchedulerEvent> {\n+    @Override\n+    public void handle(SchedulerEvent event) {\n+      scheduler.handle(event);\n+    }\n+  }\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    InlineDispatcher rmDispatcher = new InlineDispatcher();\n+    \n+    rmContext = \n+        new RMContextImpl(new MemStore(), rmDispatcher, null, null, null);\n+    scheduler = mock(YarnScheduler.class);\n+    doAnswer(\n+        new Answer<Void>() {\n+\n+          @Override\n+          public Void answer(InvocationOnMock invocation) throws Throwable {\n+            final SchedulerEvent event = (SchedulerEvent)(invocation.getArguments()[0]);\n+            eventType = event.getType();\n+            if (eventType == SchedulerEventType.NODE_UPDATE) {\n+              completedContainers = \n+                  ((NodeUpdateSchedulerEvent)event).getCompletedContainers();\n+            } else {\n+              completedContainers = null;\n+            }\n+            return null;\n+          }\n+        }\n+        ).when(scheduler).handle(any(SchedulerEvent.class));\n+    \n+    rmDispatcher.register(SchedulerEventType.class, \n+        new TestSchedulerEventDispatcher());\n+    \n+    \n+    node = new RMNodeImpl(null, rmContext, null, 0, 0, null, null);\n+\n+  }\n+  \n+  @After\n+  public void tearDown() throws Exception {\n+  }\n+  \n+  private RMNodeStatusEvent getMockRMNodeStatusEvent() {\n+    HeartbeatResponse response = mock(HeartbeatResponse.class);\n+\n+    NodeHealthStatus healthStatus = mock(NodeHealthStatus.class);\n+    Boolean yes = new Boolean(true);\n+    doReturn(yes).when(healthStatus).getIsNodeHealthy();\n+    \n+    RMNodeStatusEvent event = mock(RMNodeStatusEvent.class);\n+    doReturn(healthStatus).when(event).getNodeHealthStatus();\n+    doReturn(response).when(event).getLatestResponse();\n+    doReturn(RMNodeEventType.STATUS_UPDATE).when(event).getType();\n+    return event;\n+  }\n+  \n+  @Test\n+  public void testExpiredContainer() {\n+    // Start the node\n+    node.handle(new RMNodeEvent(null, RMNodeEventType.STARTED));\n+    verify(scheduler).handle(any(NodeAddedSchedulerEvent.class));\n+    \n+    // Expire a container\n+\t\tContainerId completedContainerId = BuilderUtils.newContainerId(\n+\t\t\t\tBuilderUtils.newApplicationAttemptId(\n+\t\t\t\t\t\tBuilderUtils.newApplicationId(0, 0), 0), 0);\n+    node.handle(new RMNodeCleanContainerEvent(null, completedContainerId));\n+    Assert.assertEquals(1, node.getContainersToCleanUp().size());\n+    \n+    // Now verify that scheduler isn't notified of an expired container\n+    // by checking number of 'completedContainers' it got in the previous event\n+    RMNodeStatusEvent statusEvent = getMockRMNodeStatusEvent();\n+    ContainerStatus containerStatus = mock(ContainerStatus.class);\n+    doReturn(completedContainerId).when(containerStatus).getContainerId();\n+    doReturn(Collections.singletonList(containerStatus)).\n+        when(statusEvent).getContainers();\n+    node.handle(statusEvent);\n+    Assert.assertEquals(0, completedContainers.size());\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/c32ca00a752dfc885af487bdd9158a67d5359779/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "sha": "6a717a4daf9c134bd9ff3fde38987e95c5c295d5",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-3530. Fixed an NPE occuring during scheduling in the ResourceManager. Contributed by Arun C Murthy.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1214476 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/50fa9b89f42bd3fe6aad5086b0df14a00dadb24b",
        "patched_files": [
            "CHANGES.java",
            "RMNodeImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMNodeTransitions.java"
        ]
    },
    "hadoop_c354815": {
        "bug_id": "hadoop_c354815",
        "commit": "https://github.com/apache/hadoop/commit/c35481594ffc372e3f846b0c8ebc2ff9e36ffdb0",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/c35481594ffc372e3f846b0c8ebc2ff9e36ffdb0/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/appmaster/TestAMSimulator.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/appmaster/TestAMSimulator.java?ref=c35481594ffc372e3f846b0c8ebc2ff9e36ffdb0",
                "deletions": 5,
                "filename": "hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/appmaster/TestAMSimulator.java",
                "patch": "@@ -49,8 +49,8 @@\n   private YarnConfiguration conf;\n   private Path metricOutputDir;\n \n-  private Class slsScheduler;\n-  private Class scheduler;\n+  private Class<?> slsScheduler;\n+  private Class<?> scheduler;\n \n   @Parameterized.Parameters\n   public static Collection<Object[]> params() {\n@@ -60,7 +60,7 @@\n     });\n   }\n \n-  public TestAMSimulator(Class slsScheduler, Class scheduler) {\n+  public TestAMSimulator(Class<?> slsScheduler, Class<?> scheduler) {\n     this.slsScheduler = slsScheduler;\n     this.scheduler = scheduler;\n   }\n@@ -115,7 +115,8 @@ private void verifySchedulerMetrics(String appId) {\n   }\n \n   private void createMetricOutputDir() {\n-    Path testDir = Paths.get(System.getProperty(\"test.build.data\"));\n+    Path testDir =\n+        Paths.get(System.getProperty(\"test.build.data\", \"target/test-dir\"));\n     try {\n       metricOutputDir = Files.createTempDirectory(testDir, \"output\");\n     } catch (IOException e) {\n@@ -153,7 +154,9 @@ public void testAMSimulator() throws Exception {\n \n   @After\n   public void tearDown() {\n-    rm.stop();\n+    if (rm != null) {\n+      rm.stop();\n+    }\n \n     deleteMetricOutputDir();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/c35481594ffc372e3f846b0c8ebc2ff9e36ffdb0/hadoop-tools/hadoop-sls/src/test/java/org/apache/hadoop/yarn/sls/appmaster/TestAMSimulator.java",
                "sha": "bc8ea70e46b70b8fd90ccad81a73bd4fcf61e23c",
                "status": "modified"
            }
        ],
        "message": "YARN-8422. TestAMSimulator failing with NPE. Contributed by Giovanni Matteo Fumarola.",
        "parent": "https://github.com/apache/hadoop/commit/6e756e8a620e4d6dc3192986679060c52063489b",
        "patched_files": [
            "AMSimulator.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAMSimulator.java"
        ]
    },
    "hadoop_c36d69a": {
        "bug_id": "hadoop_c36d69a",
        "commit": "https://github.com/apache/hadoop/commit/c36d69a7b30927eaea16335e06cfcc247accde35",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java",
                "patch": "@@ -52,7 +52,7 @@\n   /**\n    * Block collection ID.\n    */\n-  private long bcId;\n+  private volatile long bcId;\n \n   /** For implementing {@link LightWeightGSet.LinkedElement} interface. */\n   private LightWeightGSet.LinkedElement nextLinkedElement;",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java",
                "sha": "d160f61fc8f54c13fb12ff4a30778cd681c843c7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -4171,6 +4171,10 @@ void processExtraRedundancyBlocksOnInService(\n     int numExtraRedundancy = 0;\n     while(it.hasNext()) {\n       final BlockInfo block = it.next();\n+      if (block.isDeleted()) {\n+        //Orphan block, will be handled eventually, skip\n+        continue;\n+      }\n       int expectedReplication = this.getExpectedRedundancyNum(block);\n       NumberReplicas num = countNodes(block);\n       if (shouldProcessExtraRedundancy(num, expectedReplication)) {",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "675221a1ec52d4ec859306ae56d581bf94483e48",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -4128,7 +4128,7 @@ private void clearCorruptLazyPersistFiles()\n         while (it.hasNext()) {\n           Block b = it.next();\n           BlockInfo blockInfo = blockManager.getStoredBlock(b);\n-          if (blockInfo == null) {\n+          if (blockInfo == null || blockInfo.isDeleted()) {\n             LOG.info(\"Cannot find block info for block \" + b);\n           } else {\n             BlockCollection bc = getBlockCollection(blockInfo);",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "74c9f10482609ad6709292084e47309769576151",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
                "patch": "@@ -264,12 +264,13 @@ public void blockIdCK(String blockId) {\n       return;\n     }\n \n+    namenode.getNamesystem().readLock();\n     try {\n       //get blockInfo\n       Block block = new Block(Block.getBlockId(blockId));\n       //find which file this block belongs to\n       BlockInfo blockInfo = blockManager.getStoredBlock(block);\n-      if(blockInfo == null) {\n+      if (blockInfo == null || blockInfo.isDeleted()) {\n         out.println(\"Block \"+ blockId +\" \" + NONEXISTENT_STATUS);\n         LOG.warn(\"Block \"+ blockId + \" \" + NONEXISTENT_STATUS);\n         return;\n@@ -329,6 +330,8 @@ public void blockIdCK(String blockId) {\n       out.println(e.getMessage());\n       out.print(\"\\n\\n\" + errMsg);\n       LOG.warn(\"Error in looking up block\", e);\n+    } finally {\n+      namenode.getNamesystem().readUnlock(\"fsck\");\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
                "sha": "0201ca116105de5577eb027d0ab8d0f22c5dd2ff",
                "status": "modified"
            }
        ],
        "message": "HDFS-13027. Handle possible NPEs due to deleted blocks in race condition. Contributed by Vinayakumar B.\n\n(cherry picked from commit 65977e5d8124be2bc208af25beed934933f170b3)",
        "parent": "https://github.com/apache/hadoop/commit/f2c2a68ec208f640e778fc41f95f0284fcc44729",
        "patched_files": [
            "BlockManager.java",
            "BlockInfo.java",
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestBlockInfo.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_c416284": {
        "bug_id": "hadoop_c416284",
        "commit": "https://github.com/apache/hadoop/commit/c416284bb7581747beef36d7899d8680fe33abbd",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java?ref=c416284bb7581747beef36d7899d8680fe33abbd",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java",
                "patch": "@@ -18,6 +18,7 @@\n \n package org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu;\n \n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n@@ -33,8 +34,14 @@\n import org.apache.hadoop.yarn.server.nodemanager.webapp.dao.gpu.NMGpuResourceInfo;\n \n import java.util.List;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class GpuResourcePlugin implements ResourcePlugin {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(GpuResourcePlugin.class);\n+\n   private final GpuNodeResourceUpdateHandler resourceDiscoverHandler;\n   private final GpuDiscoverer gpuDiscoverer;\n   private GpuResourceHandlerImpl gpuResourceHandler = null;\n@@ -84,6 +91,10 @@ public DockerCommandPlugin getDockerCommandPluginInstance() {\n   public synchronized NMResourceInfo getNMResourceInfo() throws YarnException {\n     GpuDeviceInformation gpuDeviceInformation =\n         gpuDiscoverer.getGpuDeviceInformation();\n+\n+    //At this point the gpu plugin is already enabled\n+    checkGpuResourceHandler();\n+\n     GpuResourceAllocator gpuResourceAllocator =\n         gpuResourceHandler.getGpuAllocator();\n     List<GpuDevice> totalGpus = gpuResourceAllocator.getAllowedGpusCopy();\n@@ -94,6 +105,17 @@ public synchronized NMResourceInfo getNMResourceInfo() throws YarnException {\n         assignedGpuDevices);\n   }\n \n+  private void checkGpuResourceHandler() throws YarnException {\n+    if(gpuResourceHandler == null) {\n+      String errorMsg =\n+          \"Linux Container Executor is not configured for the NodeManager. \"\n+              + \"To fully enable GPU feature on the node also set \"\n+              + YarnConfiguration.NM_CONTAINER_EXECUTOR + \" properly.\";\n+      LOG.warn(errorMsg);\n+      throw new YarnException(errorMsg);\n+    }\n+  }\n+\n   @Override\n   public String toString() {\n     return GpuResourcePlugin.class.getName();",
                "raw_url": "https://github.com/apache/hadoop/raw/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java",
                "sha": "1ac6f83846633574a248bd1f2bb35d227570672f",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop/blob/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java?ref=c416284bb7581747beef36d7899d8680fe33abbd",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java",
                "patch": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu;\n+\n+import static org.mockito.Mockito.mock;\n+\n+import org.apache.hadoop.yarn.exceptions.YarnException;\n+import org.junit.Test;\n+\n+public class TestGpuResourcePlugin {\n+\n+  @Test(expected = YarnException.class)\n+  public void testResourceHandlerNotInitialized() throws YarnException {\n+    GpuDiscoverer gpuDiscoverer = mock(GpuDiscoverer.class);\n+    GpuNodeResourceUpdateHandler gpuNodeResourceUpdateHandler =\n+        mock(GpuNodeResourceUpdateHandler.class);\n+\n+    GpuResourcePlugin target =\n+        new GpuResourcePlugin(gpuNodeResourceUpdateHandler, gpuDiscoverer);\n+\n+    target.getNMResourceInfo();\n+  }\n+\n+  @Test\n+  public void testResourceHandlerIsInitialized() throws YarnException {\n+    GpuDiscoverer gpuDiscoverer = mock(GpuDiscoverer.class);\n+    GpuNodeResourceUpdateHandler gpuNodeResourceUpdateHandler =\n+        mock(GpuNodeResourceUpdateHandler.class);\n+\n+    GpuResourcePlugin target =\n+        new GpuResourcePlugin(gpuNodeResourceUpdateHandler, gpuDiscoverer);\n+\n+    target.createResourceHandler(null, null, null);\n+\n+    //Not throwing any exception\n+    target.getNMResourceInfo();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java",
                "sha": "888f8999d5d755976256a2a499775029f645979f",
                "status": "added"
            }
        ],
        "message": "YARN-9235. If linux container executor is not set for a GPU cluster GpuResourceHandlerImpl is not initialized and NPE is thrown. Contributed by Antal Balint Steinbach, Adam Antal",
        "parent": "https://github.com/apache/hadoop/commit/190e4349d77e7ae0601ff81a70c7569c72833ee3",
        "patched_files": [
            "GpuResourcePlugin.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestGpuResourcePlugin.java"
        ]
    },
    "hadoop_c4382e7": {
        "bug_id": "hadoop_c4382e7",
        "commit": "https://github.com/apache/hadoop/commit/c4382e7565447277e716c22dd20053113e0732cb",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c4382e7565447277e716c22dd20053113e0732cb",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -561,6 +561,8 @@ Release 2.1.0-beta - UNRELEASED\n \n     HDFS-4382. Fix typo MAX_NOT_CHANGED_INTERATIONS. (Ted Yu via suresh)\n \n+    HDFS-4840. ReplicationMonitor gets NPE during shutdown. (kihwal)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop/raw/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "a85341ec5480ca225f9dfb62af0e0a0ffabd4601",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=c4382e7565447277e716c22dd20053113e0732cb",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3094,10 +3094,15 @@ public void run() {\n           computeDatanodeWork();\n           processPendingReplications();\n           Thread.sleep(replicationRecheckInterval);\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"ReplicationMonitor thread received InterruptedException.\", ie);\n-          break;\n         } catch (Throwable t) {\n+          if (!namesystem.isRunning()) {\n+            LOG.info(\"Stopping ReplicationMonitor.\");\n+            if (!(t instanceof InterruptedException)) {\n+              LOG.info(\"ReplicationMonitor received an exception\"\n+                  + \" while shutting down.\", t);\n+            }\n+            break;\n+          }\n           LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n           terminate(1, t);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "46809da5cb1dca4eaeaef0696b662f86e6bdd197",
                "status": "modified"
            }
        ],
        "message": "HDFS-4840. ReplicationMonitor gets NPE during shutdown. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489634 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/af65d6f80ee095f8a7652244511d02ce5584a160",
        "patched_files": [
            "BlockManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_c457876": {
        "bug_id": "hadoop_c457876",
        "commit": "https://github.com/apache/hadoop/commit/c4578760b67d5b5169949a1b059f4472a268ff1b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c4578760b67d5b5169949a1b059f4472a268ff1b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c4578760b67d5b5169949a1b059f4472a268ff1b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -610,6 +610,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-8309. Skip unit test using DataNodeTestUtils#injectDataDirFailure() on Windows.\n     (xyao)\n \n+    HDFS-8290. WebHDFS calls before namesystem initialization can cause\n+    NullPointerException. (cnauroth)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c4578760b67d5b5169949a1b059f4472a268ff1b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "a365b865b6a8ed4b9e7ccb483cd1729b4efaf7cd",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/c4578760b67d5b5169949a1b059f4472a268ff1b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java?ref=c4578760b67d5b5169949a1b059f4472a268ff1b",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "patch": "@@ -69,6 +69,7 @@\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo;\n import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n import org.apache.hadoop.hdfs.web.JsonUtil;\n@@ -164,7 +165,11 @@ private static NamenodeProtocols getRPCServer(NameNode namenode)\n   static DatanodeInfo chooseDatanode(final NameNode namenode,\n       final String path, final HttpOpParam.Op op, final long openOffset,\n       final long blocksize, final String excludeDatanodes) throws IOException {\n-    final BlockManager bm = namenode.getNamesystem().getBlockManager();\n+    FSNamesystem fsn = namenode.getNamesystem();\n+    if (fsn == null) {\n+      throw new IOException(\"Namesystem has not been intialized yet.\");\n+    }\n+    final BlockManager bm = fsn.getBlockManager();\n     \n     HashSet<Node> excludes = new HashSet<Node>();\n     if (excludeDatanodes != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/c4578760b67d5b5169949a1b059f4472a268ff1b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java",
                "sha": "d33721c0fd55ea07c7427f45d906864e6d281723",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/c4578760b67d5b5169949a1b059f4472a268ff1b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java?ref=c4578760b67d5b5169949a1b059f4472a268ff1b",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java",
                "patch": "@@ -17,6 +17,9 @@\n  */\n package org.apache.hadoop.hdfs.server.namenode.web.resources;\n \n+import static org.mockito.Mockito.*;\n+\n+import java.io.IOException;\n import java.util.Arrays;\n import java.util.List;\n \n@@ -42,7 +45,9 @@\n import org.apache.hadoop.hdfs.web.resources.PutOpParam;\n import org.apache.log4j.Level;\n import org.junit.Assert;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n \n /**\n  * Test WebHDFS which provides data locality using HTTP redirection.\n@@ -57,6 +62,9 @@\n   private static final String RACK1 = \"/rack1\";\n   private static final String RACK2 = \"/rack2\";\n \n+  @Rule\n+  public final ExpectedException exception = ExpectedException.none();\n+\n   @Test\n   public void testDataLocality() throws Exception {\n     final Configuration conf = WebHdfsTestUtil.createConf();\n@@ -213,4 +221,14 @@ public void testExcludeDataNodes() throws Exception {\n       cluster.shutdown();\n     }\n   }\n-}\n\\ No newline at end of file\n+\n+  @Test\n+  public void testChooseDatanodeBeforeNamesystemInit() throws Exception {\n+    NameNode nn = mock(NameNode.class);\n+    when(nn.getNamesystem()).thenReturn(null);\n+    exception.expect(IOException.class);\n+    exception.expectMessage(\"Namesystem has not been intialized yet.\");\n+    NamenodeWebHdfsMethods.chooseDatanode(nn, \"/path\", PutOpParam.Op.CREATE, 0,\n+        DFSConfigKeys.DFS_BLOCK_SIZE_DEFAULT, null);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/c4578760b67d5b5169949a1b059f4472a268ff1b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/web/resources/TestWebHdfsDataLocality.java",
                "sha": "15e1c04b817ad64a265f7f1165303fefcd13ea08",
                "status": "modified"
            }
        ],
        "message": "HDFS-8290. WebHDFS calls before namesystem initialization can cause NullPointerException. Contributed by Chris Nauroth.",
        "parent": "https://github.com/apache/hadoop/commit/8f65c793f2930bfd16885a2ab188a9970b754974",
        "patched_files": [
            "NamenodeWebHdfsMethods.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestWebHdfsDataLocality.java"
        ]
    },
    "hadoop_c48f297": {
        "bug_id": "hadoop_c48f297",
        "commit": "https://github.com/apache/hadoop/commit/c48f2976a3de60b95c4a5ada4f0131c4cdde177a",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java?ref=c48f2976a3de60b95c4a5ada4f0131c4cdde177a",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -330,20 +330,19 @@ private static ApplicationReportExt convertToApplicationReport(\n       }\n \n       if (entityInfo.containsKey(ApplicationMetricsConstants.APP_CPU_METRICS)) {\n-        long vcoreSeconds=Long.parseLong(entityInfo.get(\n-                ApplicationMetricsConstants.APP_CPU_METRICS).toString());\n-        long memorySeconds=Long.parseLong(entityInfo.get(\n-                ApplicationMetricsConstants.APP_MEM_METRICS).toString());\n-        long preemptedMemorySeconds = Long.parseLong(entityInfo.get(\n-            ApplicationMetricsConstants\n-                .APP_MEM_PREEMPT_METRICS).toString());\n-        long preemptedVcoreSeconds = Long.parseLong(entityInfo.get(\n-            ApplicationMetricsConstants\n-                .APP_CPU_PREEMPT_METRICS).toString());\n-        appResources = ApplicationResourceUsageReport\n-            .newInstance(0, 0, null, null, null, memorySeconds, vcoreSeconds, 0,\n-                0, preemptedMemorySeconds, preemptedVcoreSeconds);\n+        long vcoreSeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_CPU_METRICS);\n+        long memorySeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_MEM_METRICS);\n+        long preemptedMemorySeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_MEM_PREEMPT_METRICS);\n+        long preemptedVcoreSeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_CPU_PREEMPT_METRICS);\n+        appResources = ApplicationResourceUsageReport.newInstance(0, 0, null,\n+            null, null, memorySeconds, vcoreSeconds, 0, 0,\n+            preemptedMemorySeconds, preemptedVcoreSeconds);\n       }\n+\n       if (entityInfo.containsKey(ApplicationMetricsConstants.APP_TAGS_INFO)) {\n         appTags = new HashSet<String>();\n         Object obj = entityInfo.get(ApplicationMetricsConstants.APP_TAGS_INFO);\n@@ -445,6 +444,16 @@ private static ApplicationReportExt convertToApplicationReport(\n         amNodeLabelExpression), appViewACLs);\n   }\n \n+  private static long parseLong(Map<String, Object> entityInfo,\n+      String infoKey) {\n+    long result = 0;\n+    Object infoValue = entityInfo.get(infoKey);\n+    if (infoValue != null) {\n+      result = Long.parseLong(infoValue.toString());\n+    }\n+    return result;\n+  }\n+\n   private static boolean isFinalState(YarnApplicationState state) {\n     return state == YarnApplicationState.FINISHED\n         || state == YarnApplicationState.FAILED",
                "raw_url": "https://github.com/apache/hadoop/raw/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "sha": "d18f3dc2257feaad6c842afca273a9e79900ba0d",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java?ref=c48f2976a3de60b95c4a5ada4f0131c4cdde177a",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -143,6 +143,10 @@ private static void prepareTimelineStore(TimelineStore store, int scale)\n       if (i == 2) {\n         entities.addEntity(createApplicationTimelineEntity(\n             appId, true, false, false, true, YarnApplicationState.FINISHED));\n+      } else if (i == 3) {\n+        entities.addEntity(createApplicationTimelineEntity(\n+            appId, false, false, false, false, YarnApplicationState.FINISHED,\n+            true));\n       } else {\n         entities.addEntity(createApplicationTimelineEntity(\n             appId, false, false, false, false, YarnApplicationState.FINISHED));\n@@ -176,7 +180,7 @@ private static void prepareTimelineStore(TimelineStore store, int scale)\n \n   @Test\n   public void testGetApplicationReport() throws Exception {\n-    for (int i = 1; i <= 2; ++i) {\n+    for (int i = 1; i <= 3; ++i) {\n       final ApplicationId appId = ApplicationId.newInstance(0, i);\n       ApplicationReport app;\n       if (callerUGI == null) {\n@@ -214,7 +218,7 @@ public ApplicationReport run() throws Exception {\n       Assert.assertTrue(app.getApplicationTags().contains(\"Test_APP_TAGS_2\"));\n       // App 2 doesn't have the ACLs, such that the default ACLs \" \" will be used.\n       // Nobody except admin and owner has access to the details of the app.\n-      if ((i ==  1 && callerUGI != null &&\n+      if ((i != 2 && callerUGI != null &&\n           callerUGI.getShortUserName().equals(\"user3\")) ||\n           (i ==  2 && callerUGI != null &&\n           (callerUGI.getShortUserName().equals(\"user2\") ||\n@@ -245,10 +249,16 @@ public ApplicationReport run() throws Exception {\n           applicationResourceUsageReport.getMemorySeconds());\n       Assert\n           .assertEquals(345, applicationResourceUsageReport.getVcoreSeconds());\n-      Assert.assertEquals(456,\n+      long expectedPreemptMemSecs = 456;\n+      long expectedPreemptVcoreSecs = 789;\n+      if (i == 3) {\n+        expectedPreemptMemSecs = 0;\n+        expectedPreemptVcoreSecs = 0;\n+      }\n+      Assert.assertEquals(expectedPreemptMemSecs,\n           applicationResourceUsageReport.getPreemptedMemorySeconds());\n       Assert\n-          .assertEquals(789, applicationResourceUsageReport\n+          .assertEquals(expectedPreemptVcoreSecs, applicationResourceUsageReport\n               .getPreemptedVcoreSeconds());\n       Assert.assertEquals(FinalApplicationStatus.UNDEFINED,\n           app.getFinalApplicationStatus());\n@@ -486,6 +496,14 @@ private static TimelineEntity createApplicationTimelineEntity(\n       ApplicationId appId, boolean emptyACLs, boolean noAttemptId,\n       boolean wrongAppId, boolean enableUpdateEvent,\n       YarnApplicationState state) {\n+    return createApplicationTimelineEntity(appId, emptyACLs, noAttemptId,\n+        wrongAppId, enableUpdateEvent, state, false);\n+  }\n+\n+  private static TimelineEntity createApplicationTimelineEntity(\n+      ApplicationId appId, boolean emptyACLs, boolean noAttemptId,\n+      boolean wrongAppId, boolean enableUpdateEvent,\n+      YarnApplicationState state, boolean missingPreemptMetrics) {\n     TimelineEntity entity = new TimelineEntity();\n     entity.setEntityType(ApplicationMetricsConstants.ENTITY_TYPE);\n     if (wrongAppId) {\n@@ -510,8 +528,10 @@ private static TimelineEntity createApplicationTimelineEntity(\n         Integer.MAX_VALUE + 1L);\n     entityInfo.put(ApplicationMetricsConstants.APP_MEM_METRICS, 123);\n     entityInfo.put(ApplicationMetricsConstants.APP_CPU_METRICS, 345);\n-    entityInfo.put(ApplicationMetricsConstants.APP_MEM_PREEMPT_METRICS,456);\n-    entityInfo.put(ApplicationMetricsConstants.APP_CPU_PREEMPT_METRICS,789);\n+    if (!missingPreemptMetrics) {\n+      entityInfo.put(ApplicationMetricsConstants.APP_MEM_PREEMPT_METRICS, 456);\n+      entityInfo.put(ApplicationMetricsConstants.APP_CPU_PREEMPT_METRICS, 789);\n+    }\n     if (emptyACLs) {\n       entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO, \"\");\n     } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "sha": "96002516fbafd587b7b667f3d7a3262041430f5e",
                "status": "modified"
            }
        ],
        "message": "YARN-6598. History server getApplicationReport NPE when fetching report for pre-2.8 job (Jason Lowe via jeagles)",
        "parent": "https://github.com/apache/hadoop/commit/6600abbb5c23a83e3a9ef48a945bc8fe19c8178a",
        "patched_files": [
            "ApplicationHistoryManagerOnTimelineStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationHistoryManagerOnTimelineStore.java"
        ]
    },
    "hadoop_c5017c7": {
        "bug_id": "hadoop_c5017c7",
        "commit": "https://github.com/apache/hadoop/commit/c5017c71e42d23005fdbb3224e55e048d47b2971",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c5017c71e42d23005fdbb3224e55e048d47b2971/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c5017c71e42d23005fdbb3224e55e048d47b2971",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -391,6 +391,8 @@ Release 2.0.5-beta - UNRELEASED\n     HDFS-4571. WebHDFS should not set the service hostname on the server side. \n     (tucu)\n \n+    HDFS-4013. TestHftpURLTimeouts throws NPE. (Chao Shi via suresh)\n+\n Release 2.0.4-alpha - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c5017c71e42d23005fdbb3224e55e048d47b2971/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "f306aae88ac38abb9dadc6d3a2fdf0d498e55414",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c5017c71e42d23005fdbb3224e55e048d47b2971/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java?ref=c5017c71e42d23005fdbb3224e55e048d47b2971",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.hdfs;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n \n import java.io.IOException;\n@@ -39,7 +40,7 @@\n public class TestHftpURLTimeouts {\n   @BeforeClass\n   public static void setup() {\n-    URLUtils.SOCKET_TIMEOUT = 1;\n+    URLUtils.SOCKET_TIMEOUT = 5;\n   }\n   \n   @Test\n@@ -116,6 +117,7 @@ private boolean checkConnectTimeout(HftpFileSystem fs, boolean ignoreReadTimeout\n           conns.add(fs.openConnection(\"/\", \"\"));\n         } catch (SocketTimeoutException ste) {\n           String message = ste.getMessage();\n+          assertNotNull(message);\n           // https will get a read timeout due to SSL negotiation, but\n           // a normal http will not, so need to ignore SSL read timeouts\n           // until a connect timeout occurs",
                "raw_url": "https://github.com/apache/hadoop/raw/c5017c71e42d23005fdbb3224e55e048d47b2971/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHftpURLTimeouts.java",
                "sha": "d9a22c10111e557bcd8bc5200256b93c2899fdea",
                "status": "modified"
            }
        ],
        "message": "HDFS-4013. TestHftpURLTimeouts throws NPE. Contributed by Chao Shi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1455755 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/57803245ecc806249fcd0cd5fb3ca593098ac877",
        "patched_files": [
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestHftpURLTimeouts.java"
        ]
    },
    "hadoop_c50aad0": {
        "bug_id": "hadoop_c50aad0",
        "commit": "https://github.com/apache/hadoop/commit/c50aad0f854b74ede9668e35db314b0a93be81b2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c50aad0f854b74ede9668e35db314b0a93be81b2/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java?ref=c50aad0f854b74ede9668e35db314b0a93be81b2",
                "deletions": 1,
                "filename": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
                "patch": "@@ -450,7 +450,9 @@ public void initialize(URI uri, Configuration conf, AzureFileSystemInstrumentati\n       // Add to this the hbase root directory, or /hbase is that is not set.\n       hbaseRoot = verifyAndConvertToStandardFormat(\n           sessionConfiguration.get(\"hbase.rootdir\", \"hbase\"));\n-      atomicRenameDirs.add(hbaseRoot);\n+      if (hbaseRoot != null) {\n+        atomicRenameDirs.add(hbaseRoot);\n+      }\n     } catch (URISyntaxException e) {\n       LOG.warn(\"Unable to initialize HBase root as an atomic rename directory.\");\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/c50aad0f854b74ede9668e35db314b0a93be81b2/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java",
                "sha": "e261c4de0115f303ff621ba2060af592cc3863fd",
                "status": "modified"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/hadoop/blob/c50aad0f854b74ede9668e35db314b0a93be81b2/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemAtomicRenameDirList.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemAtomicRenameDirList.java?ref=c50aad0f854b74ede9668e35db314b0a93be81b2",
                "deletions": 0,
                "filename": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemAtomicRenameDirList.java",
                "patch": "@@ -0,0 +1,49 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azure;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.Test;\n+\n+public class TestNativeAzureFileSystemAtomicRenameDirList extends NativeAzureFileSystemBaseTest {\n+  private AzureBlobStorageTestAccount testAccount;\n+\n+  // HBase-site config controlling HBase root dir\n+  private static final String HBASE_ROOT_DIR_CONF_STRING = \"hbase.rootdir\";\n+  private static final String HBASE_ROOT_DIR_VALUE_ON_DIFFERENT_FS = \"wasb://somedifferentfilesystem.blob.core.windows.net/hbase\";\n+  @Override\n+  protected AzureBlobStorageTestAccount createTestAccount() throws Exception {\n+    testAccount = AzureBlobStorageTestAccount.create();\n+    return testAccount;\n+  }\n+\n+  @Test\n+  public void testAzureNativeStoreIsAtomicRenameKeyDoesNotThrowNPEOnInitializingWithNonDefaultURI () throws IOException {\n+    NativeAzureFileSystem azureFs = (NativeAzureFileSystem)fs;\n+    AzureNativeFileSystemStore azureStore = azureFs.getStore();\n+    Configuration conf = fs.getConf();\n+    conf.set(HBASE_ROOT_DIR_CONF_STRING, HBASE_ROOT_DIR_VALUE_ON_DIFFERENT_FS);\n+    URI uri = fs.getUri();\n+    fs.initialize(uri, conf);\n+    azureStore.isAtomicRenameKey(\"anyrandomkey\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/c50aad0f854b74ede9668e35db314b0a93be81b2/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/TestNativeAzureFileSystemAtomicRenameDirList.java",
                "sha": "b9cca25c6f6983970d33ed1b9a4d483750bce5f7",
                "status": "added"
            }
        ],
        "message": "HADOOP-12717. NPE when trying to rename a directory in Windows Azure Storage FileSystem. Contributed by Robert Yokota and Gaurav Kanade.",
        "parent": "https://github.com/apache/hadoop/commit/2759689d7d23001f007cb0dbe2521de90734dd5c",
        "patched_files": [
            "AzureNativeFileSystemStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNativeAzureFileSystemAtomicRenameDirList.java"
        ]
    },
    "hadoop_c54a4bb": {
        "bug_id": "hadoop_c54a4bb",
        "commit": "https://github.com/apache/hadoop/commit/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -326,6 +326,8 @@ Trunk (Unreleased)\n \n     HADOOP-10431. Change visibility of KeyStore.Options getter methods to public. (tucu)\n \n+    HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "36fe52b7b5d307bfa4020b5af7d58dc7c841997c",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 33,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "patch": "@@ -27,9 +27,7 @@\n import java.security.NoSuchAlgorithmException;\n import java.text.MessageFormat;\n import java.util.Date;\n-import java.util.LinkedHashMap;\n import java.util.List;\n-import java.util.Map;\n \n import com.google.gson.stream.JsonReader;\n import com.google.gson.stream.JsonWriter;\n@@ -176,22 +174,26 @@ protected int addVersion() {\n     protected byte[] serialize() throws IOException {\n       ByteArrayOutputStream buffer = new ByteArrayOutputStream();\n       JsonWriter writer = new JsonWriter(new OutputStreamWriter(buffer));\n-      writer.beginObject();\n-      if (cipher != null) {\n-        writer.name(CIPHER_FIELD).value(cipher);\n-      }\n-      if (bitLength != 0) {\n-        writer.name(BIT_LENGTH_FIELD).value(bitLength);\n-      }\n-      if (created != null) {\n-        writer.name(CREATED_FIELD).value(created.getTime());\n-      }\n-      if (description != null) {\n-        writer.name(DESCRIPTION_FIELD).value(description);\n+      try {\n+        writer.beginObject();\n+        if (cipher != null) {\n+          writer.name(CIPHER_FIELD).value(cipher);\n+        }\n+        if (bitLength != 0) {\n+          writer.name(BIT_LENGTH_FIELD).value(bitLength);\n+        }\n+        if (created != null) {\n+          writer.name(CREATED_FIELD).value(created.getTime());\n+        }\n+        if (description != null) {\n+          writer.name(DESCRIPTION_FIELD).value(description);\n+        }\n+        writer.name(VERSIONS_FIELD).value(versions);\n+        writer.endObject();\n+        writer.flush();\n+      } finally {\n+        writer.close();\n       }\n-      writer.name(VERSIONS_FIELD).value(versions);\n-      writer.endObject();\n-      writer.flush();\n       return buffer.toByteArray();\n     }\n \n@@ -207,23 +209,27 @@ protected Metadata(byte[] bytes) throws IOException {\n       int versions = 0;\n       String description = null;\n       JsonReader reader = new JsonReader(new InputStreamReader\n-          (new ByteArrayInputStream(bytes)));\n-      reader.beginObject();\n-      while (reader.hasNext()) {\n-        String field = reader.nextName();\n-        if (CIPHER_FIELD.equals(field)) {\n-          cipher = reader.nextString();\n-        } else if (BIT_LENGTH_FIELD.equals(field)) {\n-          bitLength = reader.nextInt();\n-        } else if (CREATED_FIELD.equals(field)) {\n-          created = new Date(reader.nextLong());\n-        } else if (VERSIONS_FIELD.equals(field)) {\n-          versions = reader.nextInt();\n-        } else if (DESCRIPTION_FIELD.equals(field)) {\n-          description = reader.nextString();\n+        (new ByteArrayInputStream(bytes)));\n+      try {\n+        reader.beginObject();\n+        while (reader.hasNext()) {\n+          String field = reader.nextName();\n+          if (CIPHER_FIELD.equals(field)) {\n+            cipher = reader.nextString();\n+          } else if (BIT_LENGTH_FIELD.equals(field)) {\n+            bitLength = reader.nextInt();\n+          } else if (CREATED_FIELD.equals(field)) {\n+            created = new Date(reader.nextLong());\n+          } else if (VERSIONS_FIELD.equals(field)) {\n+            versions = reader.nextInt();\n+          } else if (DESCRIPTION_FIELD.equals(field)) {\n+            description = reader.nextString();\n+          }\n         }\n+        reader.endObject();\n+      } finally {\n+        reader.close();\n       }\n-      reader.endObject();\n       this.cipher = cipher;\n       this.bitLength = bitLength;\n       this.created = created;\n@@ -310,7 +316,6 @@ public abstract KeyVersion getKeyVersion(String versionName\n    */\n   public abstract List<String> getKeys() throws IOException;\n \n-\n   /**\n    * Get key metadata in bulk.\n    * @param names the names of the keys to get",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "sha": "0b031c0493b8f6cc2961f2ecd54b3d74ddc54ef8",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "changes": 168,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 80,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "patch": "@@ -23,9 +23,6 @@\n import java.security.InvalidParameterException;\n import java.security.NoSuchAlgorithmException;\n import java.util.List;\n-import java.util.Map;\n-\n-import javax.crypto.KeyGenerator;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n@@ -93,41 +90,54 @@ public int run(String[] args) throws Exception {\n    */\n   private int init(String[] args) throws IOException {\n     for (int i = 0; i < args.length; i++) { // parse command line\n+      boolean moreTokens = (i < args.length - 1);\n       if (args[i].equals(\"create\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new CreateCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new DeleteCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new RollCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n-      } else if (args[i].equals(\"list\")) {\n+      } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n-      } else if (args[i].equals(\"--size\")) {\n+      } else if (\"--size\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_BITLENGTH_NAME, args[++i]);\n-      } else if (args[i].equals(\"--cipher\")) {\n+      } else if (\"--cipher\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_CIPHER_NAME, args[++i]);\n-      } else if (args[i].equals(\"--provider\")) {\n+      } else if (\"--provider\".equals(args[i]) && moreTokens) {\n         userSuppliedProvider = true;\n         getConf().set(KeyProviderFactory.KEY_PROVIDER_PATH, args[++i]);\n-      } else if (args[i].equals(\"--metadata\")) {\n+      } else if (\"--metadata\".equals(args[i])) {\n         getConf().setBoolean(LIST_METADATA, true);\n-      } else if (args[i].equals(\"-i\") || (args[i].equals(\"--interactive\"))) {\n+      } else if (\"-i\".equals(args[i]) || (\"--interactive\".equals(args[i]))) {\n         interactive = true;\n-      } else if (args[i].equals(\"--help\")) {\n+      } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n         return -1;\n       } else {\n@@ -136,15 +146,20 @@ private int init(String[] args) throws IOException {\n         return -1;\n       }\n     }\n+\n+    if (command == null) {\n+      printKeyShellUsage();\n+      return -1;\n+    }\n+\n     return 0;\n   }\n \n   private void printKeyShellUsage() {\n     out.println(USAGE_PREFIX + COMMANDS);\n     if (command != null) {\n       out.println(command.getUsage());\n-    }\n-    else {\n+    } else {\n       out.println(\"=========================================================\" +\n       \t\t\"======\");\n       out.println(CreateCommand.USAGE + \":\\n\\n\" + CreateCommand.DESC);\n@@ -174,8 +189,7 @@ protected KeyProvider getKeyProvider() {\n         providers = KeyProviderFactory.getProviders(getConf());\n         if (userSuppliedProvider) {\n           provider = providers.get(0);\n-        }\n-        else {\n+        } else {\n           for (KeyProvider p : providers) {\n             if (!p.isTransient()) {\n               provider = p;\n@@ -190,7 +204,7 @@ protected KeyProvider getKeyProvider() {\n     }\n \n     protected void printProviderWritten() {\n-        out.println(provider.getClass().getName() + \" has been updated.\");\n+        out.println(provider + \" has been updated.\");\n     }\n \n     protected void warnIfTransientProvider() {\n@@ -206,12 +220,12 @@ protected void warnIfTransientProvider() {\n \n   private class ListCommand extends Command {\n     public static final String USAGE =\n-        \"list [--provider] [--metadata] [--help]\";\n+        \"list [--provider <provider>] [--metadata] [--help]\";\n     public static final String DESC =\n-        \"The list subcommand displays the keynames contained within \\n\" +\n-        \"a particular provider - as configured in core-site.xml or \" +\n-        \"indicated\\nthrough the --provider argument.\\n\" +\n-        \"If the --metadata option is used, the keys metadata will be printed\";\n+        \"The list subcommand displays the keynames contained within\\n\" +\n+        \"a particular provider as configured in core-site.xml or\\n\" +\n+        \"specified with the --provider argument. --metadata displays\\n\" +\n+        \"the metadata.\";\n \n     private boolean metadata = false;\n \n@@ -220,9 +234,9 @@ public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n         out.println(\"There are no non-transient KeyProviders configured.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\\n\"\n-            + \"to use. If you want to list a transient provider then you\\n\"\n-            + \"you MUST use the --provider argument.\");\n+          + \"Use the --provider option to specify a provider. If you\\n\"\n+          + \"want to list a transient provider then you must use the\\n\"\n+          + \"--provider argument.\");\n         rc = false;\n       }\n       metadata = getConf().getBoolean(LIST_METADATA, false);\n@@ -231,12 +245,12 @@ public boolean validate() {\n \n     public void execute() throws IOException {\n       try {\n-        List<String> keys = provider.getKeys();\n-        out.println(\"Listing keys for KeyProvider: \" + provider.toString());\n+        final List<String> keys = provider.getKeys();\n+        out.println(\"Listing keys for KeyProvider: \" + provider);\n         if (metadata) {\n-          Metadata[] meta =\n+          final Metadata[] meta =\n             provider.getKeysMetadata(keys.toArray(new String[keys.size()]));\n-          for(int i=0; i < meta.length; ++i) {\n+          for (int i = 0; i < meta.length; ++i) {\n             out.println(keys.get(i) + \" : \" + meta[i]);\n           }\n         } else {\n@@ -245,7 +259,7 @@ public void execute() throws IOException {\n           }\n         }\n       } catch (IOException e) {\n-        out.println(\"Cannot list keys for KeyProvider: \" + provider.toString()\n+        out.println(\"Cannot list keys for KeyProvider: \" + provider\n             + \": \" + e.getMessage());\n         throw e;\n       }\n@@ -258,11 +272,10 @@ public String getUsage() {\n   }\n \n   private class RollCommand extends Command {\n-    public static final String USAGE = \"roll <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"roll <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The roll subcommand creates a new version of the key specified\\n\" +\n-        \"through the <keyname> argument within the provider indicated using\\n\" +\n-        \"the --provider argument\";\n+      \"The roll subcommand creates a new version for the specified key\\n\" +\n+      \"within the provider indicated using the --provider argument\\n\";\n \n     String keyName = null;\n \n@@ -274,39 +287,37 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Key will not be rolled.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. The key\\n\" +\n+          \"has not been rolled. Use the --provider option to specify\\n\" +\n+          \"a provider.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>.\\n\" +\n+          \"See the usage description by using --help.\");\n         rc = false;\n       }\n       return rc;\n     }\n \n     public void execute() throws NoSuchAlgorithmException, IOException {\n       try {\n-        Metadata md = provider.getMetadata(keyName);\n         warnIfTransientProvider();\n         out.println(\"Rolling key version from KeyProvider: \"\n-            + provider.toString() + \" for key name: \" + keyName);\n+            + provider + \"\\n  for key name: \" + keyName);\n         try {\n           provider.rollNewVersion(keyName);\n           out.println(keyName + \" has been successfully rolled.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider.toString());\n+              + provider);\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider.toString());\n+            + provider);\n         throw e1;\n       }\n     }\n@@ -318,11 +329,11 @@ public String getUsage() {\n   }\n \n   private class DeleteCommand extends Command {\n-    public static final String USAGE = \"delete <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"delete <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The delete subcommand deletes all of the versions of the key\\n\" +\n-        \"specified as the <keyname> argument from within the provider\\n\" +\n-        \"indicated through the --provider argument\";\n+        \"The delete subcommand deletes all versions of the key\\n\" +\n+        \"specified by the <keyname> argument from within the\\n\" +\n+        \"provider specified --provider.\";\n \n     String keyName = null;\n     boolean cont = true;\n@@ -335,23 +346,21 @@ public DeleteCommand(String keyName) {\n     public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Nothing will be deleted.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. Nothing\\n\"\n+          + \"was deleted. Use the --provider option to specify a provider.\");\n         return false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"There is no keyName specified. Please specify a \" +\n+            \"<keyname>. See the usage description with --help.\");\n         return false;\n       }\n       if (interactive) {\n         try {\n           cont = ToolRunner\n               .confirmPrompt(\"You are about to DELETE all versions of \"\n-                  + \"the key: \" + keyName + \" from KeyProvider \"\n-                  + provider.toString() + \". Continue?:\");\n+                  + \" key: \" + keyName + \" from KeyProvider \"\n+                  + provider + \". Continue?:\");\n           if (!cont) {\n             out.println(\"Nothing has been be deleted.\");\n           }\n@@ -367,15 +376,15 @@ public boolean validate() {\n     public void execute() throws IOException {\n       warnIfTransientProvider();\n       out.println(\"Deleting key: \" + keyName + \" from KeyProvider: \"\n-          + provider.toString());\n+          + provider);\n       if (cont) {\n         try {\n           provider.deleteKey(keyName);\n           out.println(keyName + \" has been successfully deleted.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \"has NOT been deleted.\");\n+          out.println(keyName + \" has not been deleted.\");\n           throw e;\n         }\n       }\n@@ -388,16 +397,16 @@ public String getUsage() {\n   }\n \n   private class CreateCommand extends Command {\n-    public static final String USAGE = \"create <keyname> [--cipher] \" +\n-    \t\t\"[--size] [--provider] [--help]\";\n+    public static final String USAGE =\n+      \"create <keyname> [--cipher <cipher>] [--size <size>]\\n\" +\n+      \"                     [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The create subcommand creates a new key for the name specified\\n\" +\n-        \"as the <keyname> argument within the provider indicated through\\n\" +\n-        \"the --provider argument. You may also indicate the specific\\n\" +\n-        \"cipher through the --cipher argument. The default for cipher is\\n\" +\n-        \"currently \\\"AES/CTR/NoPadding\\\". The default keysize is \\\"256\\\".\\n\" +\n-        \"You may also indicate the requested key length through the --size\\n\" +\n-        \"argument.\";\n+      \"The create subcommand creates a new key for the name specified\\n\" +\n+      \"by the <keyname> argument within the provider specified by the\\n\" +\n+      \"--provider argument. You may specify a cipher with the --cipher\\n\" +\n+      \"argument. The default cipher is currently \\\"AES/CTR/NoPadding\\\".\\n\" +\n+      \"The default keysize is 256. You may specify the requested key\\n\" +\n+      \"length using the --size argument.\\n\";\n \n     String keyName = null;\n \n@@ -409,15 +418,14 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\nKey\" +\n-        \t\t\" will not be created.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\" +\n-            \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. No key\\n\" +\n+          \" was created. You can use the --provider option to specify\\n\" +\n+          \" a provider to use.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-        \t\t\"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>. See the usage description\" +\n+          \" with --help.\");\n         rc = false;\n       }\n       return rc;\n@@ -432,13 +440,13 @@ public void execute() throws IOException, NoSuchAlgorithmException {\n         provider.flush();\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "sha": "3d56640e11e4f674a7c09c65b47b7208373fd599",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "patch": "@@ -126,7 +126,6 @@ public KeyProvider createProvider(URI providerName, Configuration conf)\n     return o;\n   }\n \n-\n   public static String checkNotEmpty(String s, String name)\n       throws IllegalArgumentException {\n     checkNotNull(s, name);\n@@ -140,6 +139,13 @@ public static String checkNotEmpty(String s, String name)\n   private String kmsUrl;\n   private SSLFactory sslFactory;\n \n+  @Override\n+  public String toString() {\n+    final StringBuilder sb = new StringBuilder(\"KMSClientProvider[\");\n+    sb.append(kmsUrl).append(\"]\");\n+    return sb.toString();\n+  }\n+\n   public KMSClientProvider(URI uri, Configuration conf) throws IOException {\n     Path path = unnestUri(uri);\n     URL url = path.toUri().toURL();\n@@ -515,5 +521,4 @@ public void flush() throws IOException {\n   public static String buildVersionName(String name, int version) {\n     return KeyProvider.buildVersionName(name, version);\n   }\n-\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "sha": "ff30f86de377f4a763aec5ccd62d532101d99311",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "patch": "@@ -121,7 +121,7 @@ public void testInvalidKeySize() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test\n@@ -134,7 +134,7 @@ public void testInvalidCipher() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "sha": "ae6938eb68ce2fe2f963d70438e8c0959a4edd1d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594320 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/4bc3371824d6f0495a5a6a2892c0683b4d66a5a8",
        "patched_files": [
            "KeyProvider.java",
            "KMSClientProvider.java",
            "KeyShell.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestKeyShell.java",
            "TestKeyProvider.java"
        ]
    },
    "hadoop_c5665b2": {
        "bug_id": "hadoop_c5665b2",
        "commit": "https://github.com/apache/hadoop/commit/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java?ref=c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe",
                "deletions": 1,
                "filename": "hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java",
                "patch": "@@ -84,13 +84,16 @@\n    * milliseconds\n    * @param dtRemoverScanInterval how often the tokens are scanned for expired\n    * tokens in milliseconds\n+   * @param certClient certificate client to SCM CA\n    */\n   public OzoneDelegationTokenSecretManager(OzoneConfiguration conf,\n       long tokenMaxLifetime, long tokenRenewInterval,\n       long dtRemoverScanInterval, Text service,\n-      S3SecretManager s3SecretManager) throws IOException {\n+      S3SecretManager s3SecretManager, CertificateClient certClient)\n+      throws IOException {\n     super(new SecurityConfig(conf), tokenMaxLifetime, tokenRenewInterval,\n         service, LOG);\n+    setCertClient(certClient);\n     currentTokens = new ConcurrentHashMap();\n     this.tokenRemoverScanInterval = dtRemoverScanInterval;\n     this.s3SecretManager = (S3SecretManagerImpl) s3SecretManager;",
                "raw_url": "https://github.com/apache/hadoop/raw/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneDelegationTokenSecretManager.java",
                "sha": "7e03095cdc45cd7bfa7df4fd2a1e047b38f70adb",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java?ref=c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe",
                "deletions": 1,
                "filename": "hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java",
                "patch": "@@ -70,6 +70,7 @@\n    * @param tokenRenewInterval how often the tokens must be renewed in\n    * milliseconds\n    * @param service name of service\n+   * @param logger logger for the secret manager\n    */\n   public OzoneSecretManager(SecurityConfig secureConf, long tokenMaxLifetime,\n       long tokenRenewInterval, Text service, Logger logger) {\n@@ -188,7 +189,7 @@ public String formatTokenId(T id) {\n   public synchronized void start(CertificateClient client)\n       throws IOException {\n     Preconditions.checkState(!isRunning());\n-    this.certClient = client;\n+    setCertClient(client);\n     updateCurrentKey(new KeyPair(certClient.getPublicKey(),\n         certClient.getPrivateKey()));\n     setIsRunning(true);\n@@ -247,5 +248,9 @@ public AtomicInteger getTokenSequenceNumber() {\n   public CertificateClient getCertClient() {\n     return certClient;\n   }\n+\n+  public void setCertClient(CertificateClient client) {\n+    this.certClient = client;\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/security/OzoneSecretManager.java",
                "sha": "78f0565b81dc7678b74152d7933a671bf16c8d06",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java?ref=c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe",
                "deletions": 1,
                "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "patch": "@@ -627,7 +627,7 @@ private OzoneDelegationTokenSecretManager createDelegationTokenSecretManager(\n \n     return new OzoneDelegationTokenSecretManager(conf, tokenMaxLifetime,\n         tokenRenewInterval, tokenRemoverScanInterval, omRpcAddressTxt,\n-        s3SecretManager);\n+        s3SecretManager, certClient);\n   }\n \n   private OzoneBlockTokenSecretManager createBlockTokenSecretManager(",
                "raw_url": "https://github.com/apache/hadoop/raw/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
                "sha": "a6503d73140a301adc9383d2a4e818c532cf9b8f",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java?ref=c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe",
                "deletions": 3,
                "filename": "hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java",
                "patch": "@@ -169,19 +169,41 @@ public void testCreateToken() throws Exception {\n     validateHash(token.getPassword(), token.getIdentifier());\n   }\n \n-  @Test\n-  public void testRenewTokenSuccess() throws Exception {\n+  private void restartSecretManager() throws IOException {\n+    secretManager.stop();\n+    secretManager = null;\n+    secretManager = createSecretManager(conf, tokenMaxLifetime,\n+        expiryTime, tokenRemoverScanInterval);\n+  }\n+\n+  private void testRenewTokenSuccessHelper(boolean restartSecretManager)\n+      throws Exception {\n     secretManager = createSecretManager(conf, tokenMaxLifetime,\n         expiryTime, tokenRemoverScanInterval);\n     secretManager.start(certificateClient);\n     Token<OzoneTokenIdentifier> token = secretManager.createToken(TEST_USER,\n         TEST_USER,\n         TEST_USER);\n     Thread.sleep(10 * 5);\n+\n+    if (restartSecretManager) {\n+      restartSecretManager();\n+    }\n+\n     long renewalTime = secretManager.renewToken(token, TEST_USER.toString());\n     Assert.assertTrue(renewalTime > 0);\n   }\n \n+  @Test\n+  public void testReloadAndRenewToken() throws Exception {\n+    testRenewTokenSuccessHelper(true);\n+  }\n+\n+  @Test\n+  public void testRenewTokenSuccess() throws Exception {\n+    testRenewTokenSuccessHelper(false);\n+  }\n+\n   /**\n    * Tests failure for mismatch in renewer.\n    */\n@@ -375,6 +397,7 @@ private void validateHash(byte[] hash, byte[] identifier) throws Exception {\n       createSecretManager(OzoneConfiguration config, long tokenMaxLife,\n       long expiry, long tokenRemoverScanTime) throws IOException {\n     return new OzoneDelegationTokenSecretManager(config, tokenMaxLife,\n-        expiry, tokenRemoverScanTime, serviceRpcAdd, s3SecretManager);\n+        expiry, tokenRemoverScanTime, serviceRpcAdd, s3SecretManager,\n+        certificateClient);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/c5665b23ca92a8e18c4e9d24413c13f7cb7fd5fe/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/security/TestOzoneDelegationTokenSecretManager.java",
                "sha": "874252d171faf53308c1a746a158e186142bad95",
                "status": "modified"
            }
        ],
        "message": "HDDS-2228. Fix NPE in OzoneDelegationTokenManager#addPersistedDelegat\u2026 (#1571)",
        "parent": "https://github.com/apache/hadoop/commit/4c24f2434dd8c09bb104ee660975855eca287fe6",
        "patched_files": [
            "OzoneDelegationTokenSecretManager.java",
            "OzoneSecretManager.java",
            "OzoneManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOzoneDelegationTokenSecretManager.java"
        ]
    },
    "hadoop_c5a241f": {
        "bug_id": "hadoop_c5a241f",
        "commit": "https://github.com/apache/hadoop/commit/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -280,6 +280,8 @@ Release 2.4.0 - UNRELEASED\n     MAPREDUCE-5724. JobHistoryServer does not start if HDFS is not running. \n     (tucu)\n \n+    MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "88b0ff8ec1ae84287873f6f5ba86abadc8e0a6ed",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java?ref=c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.hadoop.mapreduce.v2.util.MRApps;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueACL;\n@@ -445,11 +446,18 @@ public static JobStatus fromYarn(ApplicationReport application,\n     jobStatus.setStartTime(application.getStartTime());\n     jobStatus.setFinishTime(application.getFinishTime());\n     jobStatus.setFailureInfo(application.getDiagnostics());\n-    jobStatus.setNeededMem(application.getApplicationResourceUsageReport().getNeededResources().getMemory());\n-    jobStatus.setNumReservedSlots(application.getApplicationResourceUsageReport().getNumReservedContainers());\n-    jobStatus.setNumUsedSlots(application.getApplicationResourceUsageReport().getNumUsedContainers());\n-    jobStatus.setReservedMem(application.getApplicationResourceUsageReport().getReservedResources().getMemory());\n-    jobStatus.setUsedMem(application.getApplicationResourceUsageReport().getUsedResources().getMemory());\n+    ApplicationResourceUsageReport resourceUsageReport =\n+        application.getApplicationResourceUsageReport();\n+    if (resourceUsageReport != null) {\n+      jobStatus.setNeededMem(\n+          resourceUsageReport.getNeededResources().getMemory());\n+      jobStatus.setNumReservedSlots(\n+          resourceUsageReport.getNumReservedContainers());\n+      jobStatus.setNumUsedSlots(resourceUsageReport.getNumUsedContainers());\n+      jobStatus.setReservedMem(\n+          resourceUsageReport.getReservedResources().getMemory());\n+      jobStatus.setUsedMem(resourceUsageReport.getUsedResources().getMemory());\n+    }\n     return jobStatus;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "sha": "6b4aa4ed1e40a57ce9558d146e02b40b60bc35ea",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java?ref=c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "patch": "@@ -23,8 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import junit.framework.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.JobStatus.State;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n@@ -40,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.util.Records;\n+import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n@@ -112,6 +111,14 @@ public void testFromYarnApplicationReport() {\n     when(mockReport.getUser()).thenReturn(\"dummy-user\");\n     when(mockReport.getQueue()).thenReturn(\"dummy-queue\");\n     String jobFile = \"dummy-path/job.xml\";\n+\n+    try {\n+      JobStatus status = TypeConverter.fromYarn(mockReport, jobFile);\n+    } catch (NullPointerException npe) {\n+      Assert.fail(\"Type converstion from YARN fails for jobs without \" +\n+          \"ApplicationUsageReport\");\n+    }\n+\n     ApplicationResourceUsageReport appUsageRpt = Records\n         .newRecord(ApplicationResourceUsageReport.class);\n     Resource r = Records.newRecord(Resource.class);",
                "raw_url": "https://github.com/apache/hadoop/raw/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "sha": "cc42b9c220f4b7704379054d7d90025117aed5d5",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1559811 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/602f71a8daa0dc98d0183028935e3b10460e28a5",
        "patched_files": [
            "TypeConverter.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestTypeConverter.java"
        ]
    },
    "hadoop_c684f2b": {
        "bug_id": "hadoop_c684f2b",
        "commit": "https://github.com/apache/hadoop/commit/c684f2b007a4808dafbe1c1d3ce01758e281d329",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c684f2b007a4808dafbe1c1d3ce01758e281d329",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -227,6 +227,9 @@ Release 2.9.0 - UNRELEASED\n     YARN-4651. Document movetoqueue option in 'YARN Commands'\n     (Takashi Ohnishi via rohithsharmaks)\n \n+    YARN-4729. SchedulerApplicationAttempt#getTotalRequiredResources can throw \n+    an NPE. (kasha)\n+\n Release 2.8.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/CHANGES.txt",
                "sha": "7f26f8e63cda9d19ac4d07eec0a910e412a5d630",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java?ref=c684f2b007a4808dafbe1c1d3ce01758e281d329",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "patch": "@@ -244,7 +244,8 @@ public synchronized ResourceRequest getResourceRequest(Priority priority,\n   }\n \n   public synchronized int getTotalRequiredResources(Priority priority) {\n-    return getResourceRequest(priority, ResourceRequest.ANY).getNumContainers();\n+    ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n+    return request == null ? 0 : request.getNumContainers();\n   }\n \n   public synchronized Resource getResource(Priority priority) {",
                "raw_url": "https://github.com/apache/hadoop/raw/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "sha": "254200972cbb71aff59569c4d2c50df28390de0d",
                "status": "modified"
            }
        ],
        "message": "YARN-4729. SchedulerApplicationAttempt#getTotalRequiredResources can throw an NPE. (kasha)",
        "parent": "https://github.com/apache/hadoop/commit/dbbfc58c33fd1d2f7abae1784c2d78b7438825e2",
        "patched_files": [
            "SchedulerApplicationAttempt.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSchedulerApplicationAttempt.java"
        ]
    },
    "hadoop_c797103": {
        "bug_id": "hadoop_c797103",
        "commit": "https://github.com/apache/hadoop/commit/c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1107,6 +1107,9 @@ Release 2.7.0 - UNRELEASED\n     HDFS-7885. Datanode should not trust the generation stamp provided by\n     client. (Tsz Wo Nicholas Sze via jing9)\n \n+    HDFS-7818. OffsetParam should return the default value instead of throwing\n+    NPE when the value is unspecified. (Eric Payne via wheat9)\n+\n     BREAKDOWN OF HDFS-7584 SUBTASKS AND RELATED JIRAS\n \n       HDFS-7720. Quota by Storage Type API, tools and ClientNameNode",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "b443902f3c7b4fd825f4e4bc599d0ffdb6e61dc4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java",
                "patch": "@@ -62,7 +62,7 @@ String op() {\n   }\n \n   long offset() {\n-    return new OffsetParam(param(OffsetParam.NAME)).getValue();\n+    return new OffsetParam(param(OffsetParam.NAME)).getOffset();\n   }\n \n   String namenodeId() {",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java",
                "sha": "2baafe8f29c4c195c31ac5dfb10681034e86cefb",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java",
                "patch": "@@ -46,4 +46,9 @@ public OffsetParam(final String str) {\n   public String getName() {\n     return NAME;\n   }\n+\n+  public Long getOffset() {\n+    Long offset = getValue();\n+    return (offset == null) ? Long.valueOf(0) : offset;\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java",
                "sha": "6d88703da0abc27dce2baeccbadf2b7b2c26c8b5",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.web.resources.DelegationParam;\n import org.apache.hadoop.hdfs.web.resources.NamenodeAddressParam;\n+import org.apache.hadoop.hdfs.web.resources.OffsetParam;\n import org.apache.hadoop.security.token.Token;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -65,4 +66,22 @@ public void testDecodePath() {\n     ParameterParser testParser = new ParameterParser(decoder, conf);\n     Assert.assertEquals(EXPECTED_PATH, testParser.path());\n   }\n+\n+  @Test\n+  public void testOffset() throws IOException {\n+    final long X = 42;\n+\n+    long offset = new OffsetParam(Long.toString(X)).getOffset();\n+    Assert.assertEquals(\"OffsetParam: \", X, offset);\n+\n+    offset = new OffsetParam((String) null).getOffset();\n+    Assert.assertEquals(\"OffsetParam with null should have defaulted to 0\", 0, offset);\n+\n+    try {\n+      offset = new OffsetParam(\"abc\").getValue();\n+      Assert.fail(\"OffsetParam with nondigit value should have thrown IllegalArgumentException\");\n+    } catch (IllegalArgumentException iae) {\n+      // Ignore\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java",
                "sha": "8aee1d8565c1caa83b51cd9d3a77357319432f67",
                "status": "modified"
            }
        ],
        "message": "HDFS-7818. OffsetParam should return the default value instead of throwing NPE when the value is unspecified. Contributed by Eric Payne.",
        "parent": "https://github.com/apache/hadoop/commit/21101c01f242439ec8ec40fb3a9ab1991ae0adc7",
        "patched_files": [
            "OffsetParam.java",
            "CHANGES.java",
            "ParameterParser.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestParameterParser.java"
        ]
    },
    "hadoop_c9cb6a5": {
        "bug_id": "hadoop_c9cb6a5",
        "commit": "https://github.com/apache/hadoop/commit/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -863,6 +863,8 @@ Release 2.8.0 - UNRELEASED\n     YARN-4135. Improve the assertion message in MockRM while failing after waiting for the state.\n     (Nijel S F via rohithsharmaks)\n \n+    YARN-4167. NPE on RMActiveServices#serviceStop when store is null. (Bibin A Chundatt via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/CHANGES.txt",
                "sha": "a3dfb85445ab887596e48830f8ba686ce32cc482",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -605,7 +605,9 @@ protected void serviceStop() throws Exception {\n       if (rmContext != null) {\n         RMStateStore store = rmContext.getStateStore();\n         try {\n-          store.close();\n+          if (null != store) {\n+            store.close();\n+          }\n         } catch (Exception e) {\n           LOG.error(\"Error closing store.\", e);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "d1f339a1a848082dd2dcef8b13c34a5e2da2620a",
                "status": "modified"
            }
        ],
        "message": "YARN-4167. NPE on RMActiveServices#serviceStop when store is null. (Bibin A Chundatt via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
        "patched_files": [
            "ResourceManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java"
        ]
    },
    "hadoop_c9dd2ca": {
        "bug_id": "hadoop_c9dd2ca",
        "commit": "https://github.com/apache/hadoop/commit/c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -598,6 +598,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-2194. Fix bug causing CGroups functionality to fail on RHEL7.\n     (Wei Yan via vvasudev)\n \n+    YARN-3892. Fixed NPE on RMStateStore#serviceStop when\n+    CapacityScheduler#serviceInit fails. (Bibin A Chundatt via jianhe)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/CHANGES.txt",
                "sha": "8b5cc0cd9ba6f7ace5feb0c8dfd27eb6cfc2bff1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.classification.InterfaceStability.Unstable;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.token.delegation.DelegationKey;\n import org.apache.hadoop.util.ZKUtil;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n@@ -312,7 +313,7 @@ protected synchronized void closeInternal() throws Exception {\n       verifyActiveStatusThread.interrupt();\n       verifyActiveStatusThread.join(1000);\n     }\n-    curatorFramework.close();\n+    IOUtils.closeStream(curatorFramework);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "8f096d882aaf14adf29a5235f481904bbade6376",
                "status": "modified"
            }
        ],
        "message": "YARN-3892. Fixed NPE on RMStateStore#serviceStop when CapacityScheduler#serviceInit fails. Contributed by Bibin A Chundatt",
        "parent": "https://github.com/apache/hadoop/commit/c0b8e4e5b5083631ed22d8d36c8992df7d34303c",
        "patched_files": [
            "ZKRMStateStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop_ca2265b": {
        "bug_id": "hadoop_ca2265b",
        "commit": "https://github.com/apache/hadoop/commit/ca2265b581a0eccbd366a9a1b504a61eb67d23df",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/ca2265b581a0eccbd366a9a1b504a61eb67d23df/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=ca2265b581a0eccbd366a9a1b504a61eb67d23df",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -426,6 +426,9 @@ Release 2.0.5-beta - UNRELEASED\n     MAPREDUCE-5244. Two functions changed their visibility in JobStatus. \n     (zjshen via tucu)\n \n+    MAPREDUCE-4927. Historyserver 500 error due to NPE when accessing specific\n+    counters page for failed job. (Ashwin Shankar via jlowe)\n+\n Release 2.0.4-alpha - 2013-04-25\n \n   INCOMPATIBLE CHANGES\n@@ -991,6 +994,9 @@ Release 0.23.8 - UNRELEASED\n     MAPREDUCE-5147. Maven build should create \n     hadoop-mapreduce-client-app-VERSION.jar directly (Robert Parker via tgraves)\n \n+    MAPREDUCE-4927. Historyserver 500 error due to NPE when accessing specific\n+    counters page for failed job. (Ashwin Shankar via jlowe)\n+\n Release 0.23.7 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ca2265b581a0eccbd366a9a1b504a61eb67d23df/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "337144af92f814a0e9ddc57a8c225977b2b0f817",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ca2265b581a0eccbd366a9a1b504a61eb67d23df/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java?ref=ca2265b581a0eccbd366a9a1b504a61eb67d23df",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java",
                "patch": "@@ -143,8 +143,9 @@ private void populateMembers(AppContext ctx) {\n     Map<TaskId, Task> tasks = job.getTasks();\n     for(Map.Entry<TaskId, Task> entry : tasks.entrySet()) {\n       long value = 0;\n-      CounterGroup group = entry.getValue().getCounters()\n-        .getGroup($(COUNTER_GROUP));\n+      Counters counters = entry.getValue().getCounters();\n+      CounterGroup group = (counters != null) ? counters\n+        .getGroup($(COUNTER_GROUP)) : null;\n       if(group != null)  {\n         Counter c = group.findCounter($(COUNTER_NAME));\n         if(c != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/ca2265b581a0eccbd366a9a1b504a61eb67d23df/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/SingleCounterBlock.java",
                "sha": "974b3ff8f839584844a32fce7ce79671b4e32a58",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ca2265b581a0eccbd366a9a1b504a61eb67d23df/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java?ref=ca2265b581a0eccbd366a9a1b504a61eb67d23df",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java",
                "patch": "@@ -182,6 +182,11 @@ public ClusterInfo getClusterInfo() {\n   \n   @Test public void testSingleCounterView() {\n     AppContext appContext = new TestAppContext();\n+    Job job = appContext.getAllJobs().values().iterator().next();\n+    // add a failed task to the job without any counters\n+    Task failedTask = MockJobs.newTask(job.getID(), 2, 1, true);\n+    Map<TaskId,Task> tasks = job.getTasks();\n+    tasks.put(failedTask.getID(), failedTask);\n     Map<String, String> params = getJobParams(appContext);\n     params.put(AMParams.COUNTER_GROUP, \n         \"org.apache.hadoop.mapreduce.FileSystemCounter\");",
                "raw_url": "https://github.com/apache/hadoop/raw/ca2265b581a0eccbd366a9a1b504a61eb67d23df/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebApp.java",
                "sha": "7622a9ab955a8db7bada383d4061d917a1e574fa",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4927. Historyserver 500 error due to NPE when accessing specific counters page for failed job. Contributed by Ashwin Shankar\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1483974 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0b668c21b22b0024b878ff75739fe3463572048e",
        "patched_files": [
            "SingleCounterBlock.java",
            "CHANGES.java",
            "AMWebApp.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAMWebApp.java"
        ]
    },
    "hadoop_cb5a515": {
        "bug_id": "hadoop_cb5a515",
        "commit": "https://github.com/apache/hadoop/commit/cb5a51565a70d89e83486b8eadfd7b2b44257c4c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/cb5a51565a70d89e83486b8eadfd7b2b44257c4c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=cb5a51565a70d89e83486b8eadfd7b2b44257c4c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -435,6 +435,9 @@ Release 2.1.2 - UNRELEASED\n     HADOOP-10003. HarFileSystem.listLocatedStatus() fails.\n     (Jason Dere and suresh via suresh)\n \n+    HADOOP-10017. Fix NPE in DFSClient#getDelegationToken when doing Distcp \n+    from a secured cluster to an insecured cluster. (Haohui Mai via jing9)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/cb5a51565a70d89e83486b8eadfd7b2b44257c4c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "12911a3cdbc528e0535921ded9b761ad2c277e33",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/cb5a51565a70d89e83486b8eadfd7b2b44257c4c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java?ref=cb5a51565a70d89e83486b8eadfd7b2b44257c4c",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "patch": "@@ -900,10 +900,15 @@ public String getCanonicalServiceName() {\n     assert dtService != null;\n     Token<DelegationTokenIdentifier> token =\n       namenode.getDelegationToken(renewer);\n-    token.setService(this.dtService);\n \n-    LOG.info(\"Created \" + DelegationTokenIdentifier.stringifyToken(token));\n+    if (token != null) {\n+      token.setService(this.dtService);\n+      LOG.info(\"Created \" + DelegationTokenIdentifier.stringifyToken(token));\n+    } else {\n+      LOG.info(\"Cannot get delegation token from \" + renewer);\n+    }\n     return token;\n+\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/cb5a51565a70d89e83486b8eadfd7b2b44257c4c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "sha": "1e1b9861dbeef0a68a7465450f7065f99d41df77",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/cb5a51565a70d89e83486b8eadfd7b2b44257c4c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=cb5a51565a70d89e83486b8eadfd7b2b44257c4c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -94,6 +94,19 @@ private HdfsConfiguration getTestConfiguration() {\n     return conf;\n   }\n \n+  @Test\n+  public void testEmptyDelegationToken() throws IOException {\n+    Configuration conf = getTestConfiguration();\n+    MiniDFSCluster cluster = null;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n+      FileSystem fileSys = cluster.getFileSystem();\n+      fileSys.getDelegationToken(\"\");\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n+\n   @Test\n   public void testFileSystemCloseAll() throws Exception {\n     Configuration conf = getTestConfiguration();",
                "raw_url": "https://github.com/apache/hadoop/raw/cb5a51565a70d89e83486b8eadfd7b2b44257c4c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "0a97a62001458290acdbe4c8786f9c5dabfceb92",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10017. Fix NPE in DFSClient#getDelegationToken when doing Distcp from a secured cluster to an insecured cluster. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529571 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/8e0804666189ce9a66b7b41b744776bad29770dd",
        "patched_files": [
            "DFSClient.java",
            "DistributedFileSystem.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDistributedFileSystem.java"
        ]
    },
    "hadoop_cbb2495": {
        "bug_id": "hadoop_cbb2495",
        "commit": "https://github.com/apache/hadoop/commit/cbb249534aa72ff6c290c4f99766415aeea9d6f5",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/cbb249534aa72ff6c290c4f99766415aeea9d6f5/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=cbb249534aa72ff6c290c4f99766415aeea9d6f5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -559,6 +559,13 @@ Release 2.8.0 - UNRELEASED\n     committing is not utilized when input path is absolute.\n     (Dustin Cote via aajisaka)\n \n+    MAPREDUCE-6357. MultipleOutputs.write() API should document that output\n+    committing is not utilized when input path is absolute.\n+    (Dustin Cote via aajisaka)\n+\n+    MAPREDUCE-6452. NPE when intermediate encrypt enabled for LocalRunner.\n+    (Zhihai Xu)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/cbb249534aa72ff6c290c4f99766415aeea9d6f5/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "27af9f91356ef1f7603d41c05ad7266598901e23",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/cbb249534aa72ff6c290c4f99766415aeea9d6f5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java?ref=cbb249534aa72ff6c290c4f99766415aeea9d6f5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
                "patch": "@@ -24,6 +24,7 @@\n import java.io.DataOutputStream;\n import java.io.IOException;\n import java.io.OutputStream;\n+import java.security.NoSuchAlgorithmException;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.HashMap;\n@@ -36,6 +37,8 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n \n+import javax.crypto.KeyGenerator;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n@@ -47,14 +50,17 @@\n import org.apache.hadoop.ipc.ProtocolSignature;\n import org.apache.hadoop.mapreduce.Cluster.JobTrackerStatus;\n import org.apache.hadoop.mapreduce.ClusterMetrics;\n+import org.apache.hadoop.mapreduce.CryptoUtils;\n import org.apache.hadoop.mapreduce.MRConfig;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.OutputFormat;\n import org.apache.hadoop.mapreduce.QueueInfo;\n import org.apache.hadoop.mapreduce.TaskCompletionEvent;\n import org.apache.hadoop.mapreduce.TaskTrackerInfo;\n import org.apache.hadoop.mapreduce.TaskType;\n import org.apache.hadoop.mapreduce.checkpoint.TaskCheckpointID;\n import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\n+import org.apache.hadoop.mapreduce.security.TokenCache;\n import org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\n import org.apache.hadoop.mapreduce.split.JobSplit.TaskSplitMetaInfo;\n@@ -84,6 +90,8 @@\n   public static final String LOCAL_MAX_REDUCES =\n     \"mapreduce.local.reduce.tasks.maximum\";\n \n+  public static final String INTERMEDIATE_DATA_ENCRYPTION_ALGO = \"HmacSHA1\";\n+\n   private FileSystem fs;\n   private HashMap<JobID, Job> jobs = new HashMap<JobID, Job>();\n   private JobConf conf;\n@@ -188,6 +196,25 @@ public Job(JobID jobid, String jobSubmitDir) throws IOException {\n \n       jobs.put(id, this);\n \n+      if (CryptoUtils.isEncryptedSpillEnabled(job)) {\n+        try {\n+          int keyLen = conf.getInt(\n+              MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS,\n+              MRJobConfig\n+                  .DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS);\n+          KeyGenerator keyGen =\n+              KeyGenerator.getInstance(INTERMEDIATE_DATA_ENCRYPTION_ALGO);\n+          keyGen.init(keyLen);\n+          Credentials creds =\n+              UserGroupInformation.getCurrentUser().getCredentials();\n+          TokenCache.setEncryptedSpillKey(keyGen.generateKey().getEncoded(),\n+              creds);\n+          UserGroupInformation.getCurrentUser().addCredentials(creds);\n+        } catch (NoSuchAlgorithmException e) {\n+          throw new IOException(\"Error generating encrypted spill key\", e);\n+        }\n+      }\n+\n       this.start();\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cbb249534aa72ff6c290c4f99766415aeea9d6f5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java",
                "sha": "45d3cc5b29a4569d3f2b7d27d7230fc1493e8df4",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/cbb249534aa72ff6c290c4f99766415aeea9d6f5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLocalJobSubmission.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLocalJobSubmission.java?ref=cbb249534aa72ff6c290c4f99766415aeea9d6f5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLocalJobSubmission.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.fs.Path;\n \n import org.apache.hadoop.mapreduce.MRConfig;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.SleepJob;\n import org.apache.hadoop.util.ToolRunner;\n import org.junit.After;\n@@ -81,6 +82,30 @@ public void testLocalJobLibjarsOption() throws IOException {\n     assertEquals(\"dist job res is not 0:\", 0, res);\n   }\n \n+  /**\n+   * test the local job submission with\n+   * intermediate data encryption enabled.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testLocalJobEncryptedIntermediateData() throws IOException {\n+    Configuration conf = new Configuration();\n+    conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\n+    conf.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, true);\n+    final String[] args = {\n+        \"-m\", \"1\", \"-r\", \"1\", \"-mt\", \"1\", \"-rt\", \"1\"\n+    };\n+    int res = -1;\n+    try {\n+      res = ToolRunner.run(conf, new SleepJob(), args);\n+    } catch (Exception e) {\n+      System.out.println(\"Job failed with \" + e.getLocalizedMessage());\n+      e.printStackTrace(System.out);\n+      fail(\"Job failed\");\n+    }\n+    assertEquals(\"dist job res is not 0:\", 0, res);\n+  }\n+\n   private Path makeJar(Path p) throws IOException {\n     FileOutputStream fos = new FileOutputStream(new File(p.toString()));\n     JarOutputStream jos = new JarOutputStream(fos);",
                "raw_url": "https://github.com/apache/hadoop/raw/cbb249534aa72ff6c290c4f99766415aeea9d6f5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestLocalJobSubmission.java",
                "sha": "8b02857b23a45e2c988ff8e8f97f7569b4c93978",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6452. NPE when intermediate encrypt enabled for LocalRunner. Contributed by Zhihai Xu",
        "parent": "https://github.com/apache/hadoop/commit/b6ceee9bf42eec15891f60a014bbfa47e03f563c",
        "patched_files": [
            "CHANGES.java",
            "LocalJobRunner.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalJobSubmission.java"
        ]
    },
    "hadoop_cde3a00": {
        "bug_id": "hadoop_cde3a00",
        "commit": "https://github.com/apache/hadoop/commit/cde3a00526c562a500308232e2b93498d22c90d7",
        "file": [
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 15,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java",
                "patch": "@@ -67,8 +67,8 @@\n  * underlying OS.  All executor implementations must extend ContainerExecutor.\n  */\n public abstract class ContainerExecutor implements Configurable {\n-  private static final String WILDCARD = \"*\";\n   private static final Log LOG = LogFactory.getLog(ContainerExecutor.class);\n+  protected static final String WILDCARD = \"*\";\n \n   /**\n    * The permissions to use when creating the launch script.\n@@ -274,15 +274,16 @@ public int reacquireContainer(ContainerReacquisitionContext ctx)\n    * @param environment the environment variables and their values\n    * @param resources the resources which have been localized for this\n    * container. Symlinks will be created to these localized resources\n-   * @param command the command that will be run.\n-   * @param logDir the log dir to copy debugging information to\n+   * @param command the command that will be run\n+   * @param logDir the log dir to which to copy debugging information\n+   * @param user the username of the job owner\n    * @throws IOException if any errors happened writing to the OutputStream,\n    * while creating symlinks\n    */\n   public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n-      Map<Path, List<String>> resources, List<String> command, Path logDir)\n-      throws IOException {\n-    this.writeLaunchEnv(out, environment, resources, command, logDir,\n+      Map<Path, List<String>> resources, List<String> command, Path logDir,\n+      String user) throws IOException {\n+    this.writeLaunchEnv(out, environment, resources, command, logDir, user,\n         ContainerLaunch.CONTAINER_SCRIPT);\n   }\n \n@@ -295,17 +296,17 @@ public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n    * @param environment the environment variables and their values\n    * @param resources the resources which have been localized for this\n    * container. Symlinks will be created to these localized resources\n-   * @param command the command that will be run.\n-   * @param logDir the log dir to copy debugging information to\n+   * @param command the command that will be run\n+   * @param logDir the log dir to which to copy debugging information\n+   * @param user the username of the job owner\n    * @param outFilename the path to which to write the launch environment\n    * @throws IOException if any errors happened writing to the OutputStream,\n    * while creating symlinks\n    */\n   @VisibleForTesting\n-  public void writeLaunchEnv(OutputStream out,\n-      Map<String, String> environment, Map<Path, List<String>> resources,\n-      List<String> command, Path logDir, String outFilename)\n-      throws IOException {\n+  public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n+      Map<Path, List<String>> resources, List<String> command, Path logDir,\n+      String user, String outFilename) throws IOException {\n     ContainerLaunch.ShellScriptBuilder sb =\n         ContainerLaunch.ShellScriptBuilder.create();\n     Set<String> whitelist = new HashSet<>();\n@@ -334,9 +335,7 @@ public void writeLaunchEnv(OutputStream out,\n           if (new Path(linkName).getName().equals(WILDCARD)) {\n             // If this is a wildcarded path, link to everything in the\n             // directory from the working directory\n-            File directory = new File(resourceEntry.getKey().toString());\n-\n-            for (File wildLink : directory.listFiles()) {\n+            for (File wildLink : readDirAsUser(user, resourceEntry.getKey())) {\n               sb.symlink(new Path(wildLink.toString()),\n                   new Path(wildLink.getName()));\n             }\n@@ -370,6 +369,19 @@ public void writeLaunchEnv(OutputStream out,\n     }\n   }\n \n+  /**\n+   * Return the files in the target directory. If retrieving the list of files\n+   * requires specific access rights, that access will happen as the\n+   * specified user. The list will not include entries for \".\" or \"..\".\n+   *\n+   * @param user the user as whom to access the target directory\n+   * @param dir the target directory\n+   * @return a list of files in the target directory\n+   */\n+  protected File[] readDirAsUser(String user, Path dir) {\n+    return new File(dir.toString()).listFiles();\n+  }\n+\n   /**\n    * The container exit code.\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java",
                "sha": "818b0ea4557e4e897a90930aad58a360a0c273e8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java",
                "patch": "@@ -330,8 +330,8 @@ public int launchContainer(ContainerStartContext ctx) throws IOException {\n    * the docker image and write them out to an OutputStream.\n    */\n   public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n-    Map<Path, List<String>> resources, List<String> command, Path logDir)\n-    throws IOException {\n+      Map<Path, List<String>> resources, List<String> command, Path logDir,\n+      String user) throws IOException {\n     ContainerLaunch.ShellScriptBuilder sb =\n       ContainerLaunch.ShellScriptBuilder.create();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java",
                "sha": "ebf9566fb3ddcafefa075826ce1e6a5b7c2782f3",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -54,7 +54,6 @@\n import org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler;\n import org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler;\n import org.apache.hadoop.yarn.server.nodemanager.util.LCEResourcesHandler;\n-\n import java.io.File;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n@@ -644,6 +643,45 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     }\n   }\n \n+  @Override\n+  protected File[] readDirAsUser(String user, Path dir) {\n+    List<File> files = new ArrayList<>();\n+    PrivilegedOperation listAsUserOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.LIST_AS_USER, (String)null);\n+    String runAsUser = getRunAsUser(user);\n+    String dirString = \"\";\n+\n+    if (dir != null) {\n+      dirString = dir.toUri().getPath();\n+    }\n+\n+    listAsUserOp.appendArgs(runAsUser, user,\n+        Integer.toString(\n+            PrivilegedOperation.RunAsUserCommand.LIST_AS_USER.getValue()),\n+        dirString);\n+\n+    try {\n+      PrivilegedOperationExecutor privOpExecutor =\n+          PrivilegedOperationExecutor.getInstance(super.getConf());\n+\n+      String results =\n+          privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n+\n+      for (String file: results.split(\"\\n\")) {\n+        // The container-executor always dumps its log output to stdout, which\n+        // includes 3 lines that start with \"main : \"\n+        if (!file.startsWith(\"main :\")) {\n+          files.add(new File(new File(dirString), file));\n+        }\n+      }\n+    } catch (PrivilegedOperationException e) {\n+      LOG.error(\"ListAsUser for \" + dir + \" returned with exit code: \"\n+          + e.getExitCode(), e);\n+    }\n+\n+    return files.toArray(new File[files.size()]);\n+  }\n+\n   @Override\n   public boolean isContainerAlive(ContainerLivenessContext ctx)\n       throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "6890b256e52b262a87737de319a2866a7be3c43d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java",
                "patch": "@@ -267,7 +267,7 @@ public Integer call() {\n         // Write out the environment\n         exec.writeLaunchEnv(containerScriptOutStream, environment,\n           localResources, launchContext.getCommands(),\n-            new Path(containerLogDirs.get(0)));\n+            new Path(containerLogDirs.get(0)), user);\n \n         // /////////// End of writing out container-script\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java",
                "sha": "14190fc98dc2b02cb0e3a36b64d269e2de775eeb",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "patch": "@@ -50,7 +50,8 @@\n     TC_READ_STATE(\"--tc-read-state\"),\n     TC_READ_STATS(\"--tc-read-stats\"),\n     ADD_PID_TO_CGROUP(\"\"), //no CLI switch supported yet.\n-    RUN_DOCKER_CMD(\"--run-docker\");\n+    RUN_DOCKER_CMD(\"--run-docker\"),\n+    LIST_AS_USER(\"\"); //no CLI switch supported yet.\n \n     private final String option;\n \n@@ -146,7 +147,8 @@ public int hashCode() {\n     LAUNCH_CONTAINER(1),\n     SIGNAL_CONTAINER(2),\n     DELETE_AS_USER(3),\n-    LAUNCH_DOCKER_CONTAINER(4);\n+    LAUNCH_DOCKER_CONTAINER(4),\n+    LIST_AS_USER(5);\n \n     private int value;\n     RunAsUserCommand(int value) {",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "sha": "8402a16339dbb0f992fd56228bd096c7e8048591",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "patch": "@@ -439,7 +439,7 @@ char *concatenate(char *concat_pattern, char *return_path_name,\n   for (j = 0; j < numArgs; j++) {\n     arg = va_arg(ap, char*);\n     if (arg == NULL) {\n-      fprintf(LOGFILE, \"One of the arguments passed for %s in null.\\n\",\n+      fprintf(LOGFILE, \"One of the arguments passed for %s is null.\\n\",\n           return_path_name);\n       return NULL;\n     }\n@@ -1929,6 +1929,58 @@ int delete_as_user(const char *user,\n   return ret;\n }\n \n+/**\n+ * List the files in the given directory as the user.\n+ * user: the user listing the files\n+ * target_dir: the directory from which to list files\n+ */\n+int list_as_user(const char *target_dir) {\n+  int ret = 0;\n+  struct stat sb;\n+\n+  if (stat(target_dir, &sb) != 0) {\n+    // If directory doesn't exist or can't be accessed, error out\n+    fprintf(LOGFILE, \"Could not stat %s - %s\\n\", target_dir,\n+        strerror(errno));\n+    ret = -1;\n+  } else if (!S_ISDIR(sb.st_mode)) {\n+    // If it's not a directory, list it as the only file\n+    printf(\"%s\\n\", target_dir);\n+  } else {\n+    DIR *dir = opendir(target_dir);\n+\n+    if (dir != NULL) {\n+      struct dirent *file;\n+\n+      errno = 0;\n+\n+      do {\n+        file = readdir(dir);\n+\n+        // Ignore the . and .. entries\n+        if ((file != NULL) &&\n+            (strcmp(\".\", file->d_name) != 0) &&\n+            (strcmp(\"..\", file->d_name) != 0)) {\n+          printf(\"%s\\n\", file->d_name);\n+        }\n+      } while (file != NULL);\n+\n+      // If we ended the directory read early on an error, then error out\n+      if (errno != 0) {\n+        fprintf(LOGFILE, \"Could not read directory %s - %s\\n\", target_dir,\n+            strerror(errno));\n+        ret = -1;\n+      }\n+    } else {\n+      fprintf(LOGFILE, \"Could not open directory %s - %s\\n\", target_dir,\n+          strerror(errno));\n+      ret = -1;\n+    }\n+  }\n+\n+  return ret;\n+}\n+\n void chown_dir_contents(const char *dir_path, uid_t uid, gid_t gid) {\n   DIR *dp;\n   struct dirent *ep;",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "sha": "ca3847edde59c3cb6177be55582fa7d68b0cb94c",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "patch": "@@ -31,7 +31,8 @@ enum command {\n   LAUNCH_CONTAINER = 1,\n   SIGNAL_CONTAINER = 2,\n   DELETE_AS_USER = 3,\n-  LAUNCH_DOCKER_CONTAINER = 4\n+  LAUNCH_DOCKER_CONTAINER = 4,\n+  LIST_AS_USER = 5\n };\n \n enum errorcodes {\n@@ -79,7 +80,8 @@ enum operations {\n   RUN_AS_USER_SIGNAL_CONTAINER = 8,\n   RUN_AS_USER_DELETE = 9,\n   RUN_AS_USER_LAUNCH_DOCKER_CONTAINER = 10,\n-  RUN_DOCKER = 11\n+  RUN_DOCKER = 11,\n+  RUN_AS_USER_LIST = 12\n };\n \n #define NM_GROUP_KEY \"yarn.nodemanager.linux-container-executor.group\"\n@@ -189,6 +191,10 @@ int delete_as_user(const char *user,\n                    const char *dir_to_be_deleted,\n                    char* const* baseDirs);\n \n+// List the files in the given directory on stdout. The target_dir is always\n+// assumed to be an absolute path.\n+int list_as_user(const char *target_dir);\n+\n // set the uid and gid of the node manager.  This is used when doing some\n // priviledged operations for setting the effective uid and gid.\n void set_nm_uid(uid_t user, gid_t group);",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "sha": "18585550ed2a88e59e959b4a1da33aa582209fa6",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "patch": "@@ -58,11 +58,12 @@ static void display_usage(FILE *stream) {\n       \"            launch docker container:      %2d appid containerid workdir container-script \" \\\n                               \"tokens pidfile nm-local-dirs nm-log-dirs docker-command-file resources optional-tc-command-file\\n\" \\\n       \"            signal container:      %2d container-pid signal\\n\" \\\n-      \"            delete as user:        %2d relative-path\\n\" ;\n+      \"            delete as user:        %2d relative-path\\n\" \\\n+      \"            list as user:          %2d relative-path\\n\" ;\n \n \n   fprintf(stream, usage_template, INITIALIZE_CONTAINER, LAUNCH_CONTAINER, LAUNCH_DOCKER_CONTAINER,\n-          SIGNAL_CONTAINER, DELETE_AS_USER);\n+          SIGNAL_CONTAINER, DELETE_AS_USER, LIST_AS_USER);\n }\n \n /* Sets up log files for normal/error logging */\n@@ -169,20 +170,20 @@ static void assert_valid_setup(char *argv0) {\n static struct {\n   char *cgroups_hierarchy;\n   char *traffic_control_command_file;\n-  const char * run_as_user_name;\n-  const char * yarn_user_name;\n+  const char *run_as_user_name;\n+  const char *yarn_user_name;\n   char *local_dirs;\n   char *log_dirs;\n   char *resources_key;\n   char *resources_value;\n   char **resources_values;\n-  const char * app_id;\n-  const char * container_id;\n-  const char * cred_file;\n-  const char * script_file;\n-  const char * current_dir;\n-  const char * pid_file;\n-  const char *dir_to_be_deleted;\n+  const char *app_id;\n+  const char *container_id;\n+  const char *cred_file;\n+  const char *script_file;\n+  const char *current_dir;\n+  const char *pid_file;\n+  const char *target_dir;\n   int container_pid;\n   int signal;\n   const char *docker_command_file;\n@@ -417,9 +418,13 @@ static int validate_run_as_user_commands(int argc, char **argv, int *operation)\n     return 0;\n \n   case DELETE_AS_USER:\n-    cmd_input.dir_to_be_deleted = argv[optind++];\n+    cmd_input.target_dir = argv[optind++];\n     *operation = RUN_AS_USER_DELETE;\n     return 0;\n+  case LIST_AS_USER:\n+    cmd_input.target_dir = argv[optind++];\n+    *operation = RUN_AS_USER_LIST;\n+    return 0;\n   default:\n     fprintf(ERRORFILE, \"Invalid command %d not supported.\",command);\n     fflush(ERRORFILE);\n@@ -554,9 +559,18 @@ int main(int argc, char **argv) {\n     }\n \n     exit_code = delete_as_user(cmd_input.yarn_user_name,\n-                        cmd_input.dir_to_be_deleted,\n+                        cmd_input.target_dir,\n                         argv + optind);\n     break;\n+  case RUN_AS_USER_LIST:\n+    exit_code = set_user(cmd_input.run_as_user_name);\n+\n+    if (exit_code != 0) {\n+      break;\n+    }\n+\n+    exit_code = list_as_user(cmd_input.target_dir);\n+    break;\n   }\n \n   flush_and_close_log_files();",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "sha": "56215ca4c79531acf18016c0a0d9b282c2d368ed",
                "status": "modified"
            },
            {
                "additions": 157,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c",
                "changes": 157,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c",
                "patch": "@@ -454,6 +454,163 @@ void test_delete_user() {\n   free(app_dir);\n }\n \n+/**\n+ * Read a file and tokenize it on newlines.  Place up to max lines into lines.\n+ * The max+1st element of lines will be set to NULL.\n+ *\n+ * @param file the name of the file to open\n+ * @param lines the pointer array into which to place the lines\n+ * @param max the max number of lines to add to lines\n+ */\n+void read_lines(const char* file, char **lines, size_t max) {\n+  char buf[4096];\n+  size_t nread;\n+\n+  int fd = open(file, O_RDONLY);\n+\n+  if (fd < 0) {\n+    printf(\"FAIL: failed to open directory listing file: %s\\n\", file);\n+    exit(1);\n+  } else {\n+    char *cur = buf;\n+    size_t count = sizeof buf;\n+\n+    while ((nread = read(fd, cur, count)) > 0) {\n+      cur += nread;\n+      count -= nread;\n+    }\n+\n+    if (nread < 0) {\n+      printf(\"FAIL: failed to read directory listing file: %s\\n\", file);\n+      exit(1);\n+    }\n+\n+    close(fd);\n+  }\n+\n+  char* entity = strtok(buf, \"\\n\");\n+  int i;\n+\n+  for (i = 0; i < max; i++) {\n+    if (entity == NULL) {\n+      break;\n+    }\n+\n+    lines[i] = (char *)malloc(sizeof(char) * (strlen(entity) + 1));\n+    strcpy(lines[i], entity);\n+    entity = strtok(NULL, \"\\n\");\n+  }\n+\n+  lines[i] = NULL;\n+}\n+\n+void test_list_as_user() {\n+  printf(\"\\nTesting list_as_user\\n\");\n+  char buffer[4096];\n+\n+  char *app_dir =\n+      get_app_directory(TEST_ROOT \"/local-1\", \"yarn\", \"app_4\");\n+\n+  if (mkdirs(app_dir, 0700) != 0) {\n+    printf(\"FAIL: unble to create application directory: %s\\n\", app_dir);\n+    exit(1);\n+  }\n+\n+  // Test with empty dir string\n+  sprintf(buffer, \"\");\n+  int ret = list_as_user(buffer);\n+\n+  if (ret == 0) {\n+    printf(\"FAIL: did not fail on empty directory string\\n\");\n+    exit(1);\n+  }\n+\n+  // Test with a non-existent directory\n+  sprintf(buffer, \"%s/output\", app_dir);\n+\n+  ret = list_as_user(buffer);\n+\n+  if (ret == 0) {\n+    printf(\"FAIL: did not fail on non-existent directory\\n\");\n+    exit(1);\n+  }\n+\n+  // Write a couple files to list\n+  sprintf(buffer, \"%s/file1\", app_dir);\n+\n+  if (write_config_file(buffer, 1) != 0) {\n+    exit(1);\n+  }\n+\n+  sprintf(buffer, \"%s/.file2\", app_dir);\n+\n+  if (write_config_file(buffer, 1) != 0) {\n+    exit(1);\n+  }\n+\n+  // Also create a directory\n+  sprintf(buffer, \"%s/output\", app_dir);\n+\n+  if (mkdirs(buffer, 0700) != 0) {\n+    exit(1);\n+  }\n+\n+  // Test the regular case\n+  // Store a copy of stdout, then redirect it to a file\n+  sprintf(buffer, \"%s/output/files\", app_dir);\n+\n+  int oldout = dup(STDOUT_FILENO);\n+  int fd = open(buffer, O_WRONLY | O_CREAT, S_IRUSR | S_IWUSR);\n+\n+  dup2(fd, STDOUT_FILENO);\n+\n+  // Now list the files\n+  ret = list_as_user(app_dir);\n+\n+  if (ret != 0) {\n+    printf(\"FAIL: unable to list files in regular case\\n\");\n+    exit(1);\n+  }\n+\n+  // Restore stdout\n+  close(fd);\n+  dup2(oldout, STDOUT_FILENO);\n+\n+  // Check the output -- shouldn't be more than a couple lines\n+  char *lines[16];\n+\n+  read_lines(buffer, lines, 15);\n+\n+  int got_file1 = 0;\n+  int got_file2 = 0;\n+  int got_output = 0;\n+  int i;\n+\n+  for (i = 0; i < sizeof lines; i++) {\n+    if (lines[i] == NULL) {\n+      break;\n+    } else if (strcmp(\"file1\", lines[i]) == 0) {\n+      got_file1 = 1;\n+    } else if (strcmp(\".file2\", lines[i]) == 0) {\n+      got_file2 = 1;\n+    } else if (strcmp(\"output\", lines[i]) == 0) {\n+      got_output = 1;\n+    } else {\n+      printf(\"FAIL: listed extraneous file: %s\\n\", lines[i]);\n+      exit(1);\n+    }\n+\n+    free(lines[i]);\n+  }\n+\n+  if (!got_file1 || !got_file2 || !got_output) {\n+    printf(\"FAIL: missing files in listing\\n\");\n+    exit(1);\n+  }\n+\n+  free(app_dir);\n+}\n+\n void run_test_in_child(const char* test_name, void (*func)()) {\n   printf(\"\\nRunning test %s in child process\\n\", test_name);\n   fflush(stdout);",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c",
                "sha": "f174a9ff1c7479f2e18fe637077d8f37b7b94471",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java",
                "patch": "@@ -174,7 +174,8 @@ public void testSpecialCharSymlinks() throws IOException  {\n \n       new DefaultContainerExecutor()\n           .writeLaunchEnv(fos, env, resources, commands,\n-              new Path(localLogDir.getAbsolutePath()), tempFile.getName());\n+              new Path(localLogDir.getAbsolutePath()), \"user\",\n+              tempFile.getName());\n       fos.flush();\n       fos.close();\n       FileUtil.setExecutable(tempFile, true);\n@@ -243,7 +244,7 @@ public void testInvalidSymlinkDiagnostics() throws IOException  {\n       }\n       new DefaultContainerExecutor()\n           .writeLaunchEnv(fos, env, resources, commands,\n-              new Path(localLogDir.getAbsolutePath()));\n+              new Path(localLogDir.getAbsolutePath()), \"user\");\n       fos.flush();\n       fos.close();\n       FileUtil.setExecutable(tempFile, true);\n@@ -298,7 +299,7 @@ public void testInvalidEnvSyntaxDiagnostics() throws IOException  {\n       List<String> commands = new ArrayList<String>();\n       new DefaultContainerExecutor()\n           .writeLaunchEnv(fos, env, resources, commands,\n-              new Path(localLogDir.getAbsolutePath()));\n+              new Path(localLogDir.getAbsolutePath()), \"user\");\n       fos.flush();\n       fos.close();\n \n@@ -377,7 +378,7 @@ public void testContainerLaunchStdoutAndStderrDiagnostics() throws IOException {\n       commands.add(command);\n       ContainerExecutor exec = new DefaultContainerExecutor();\n       exec.writeLaunchEnv(fos, env, resources, commands,\n-          new Path(localLogDir.getAbsolutePath()));\n+          new Path(localLogDir.getAbsolutePath()), \"user\");\n       fos.flush();\n       fos.close();\n \n@@ -1400,7 +1401,8 @@ public void testDebuggingInformation() throws IOException {\n         ContainerExecutor exec = new DefaultContainerExecutor();\n         exec.setConf(conf);\n         exec.writeLaunchEnv(fos, env, resources, commands,\n-          new Path(localLogDir.getAbsolutePath()), tempFile.getName());\n+            new Path(localLogDir.getAbsolutePath()), \"user\",\n+            tempFile.getName());\n         fos.flush();\n         fos.close();\n         FileUtil.setExecutable(tempFile, true);",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java",
                "sha": "be6eadba0a34e51b4d3265ffef23292c7cd9f75f",
                "status": "modified"
            }
        ],
        "message": "YARN-5373. NPE listing wildcard directory in containerLaunch. (Daniel Templeton via kasha)",
        "parent": "https://github.com/apache/hadoop/commit/9ef632f3b0b0e0808116cd1c7482a205b7ebef7d",
        "patched_files": [
            "ContainerExecutor.java",
            "DockerContainerExecutor.java",
            "container-executor.java",
            "LinuxContainerExecutor.java",
            "main.java",
            "ContainerLaunch.java",
            "PrivilegedOperation.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerLaunch.java",
            "TestDockerContainerExecutor.java",
            "TestContainerExecutor.java",
            "test-container-executor.java",
            "TestLinuxContainerExecutor.java"
        ]
    },
    "hadoop_cdf1af0": {
        "bug_id": "hadoop_cdf1af0",
        "commit": "https://github.com/apache/hadoop/commit/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -772,6 +772,11 @@ Release 2.6.0 - UNRELEASED\n     HDFS-7157. Using Time.now() for recording start/end time of reconfiguration\n     tasks (Lei Xu via cmccabe)\n \n+    HDFS-6664. HDFS permissions guide documentation states incorrect default \n+    group mapping class. (Ray Chiang via aw)\n+\n+    HDFS-4227. Document dfs.namenode.resource.*  (Daisuke Kobayashi via aw)\n+\n     BREAKDOWN OF HDFS-6134 AND HADOOP-10150 SUBTASKS AND RELATED JIRAS\n   \n       HDFS-6387. HDFS CLI admin tool for creating & deleting an\n@@ -1003,10 +1008,7 @@ Release 2.6.0 - UNRELEASED\n     HDFS-7140. Add a tool to list all the existing block storage policies.\n     (jing9)\n \n-    HDFS-6664. HDFS permissions guide documentation states incorrect default \n-    group mapping class. (Ray Chiang via aw)\n-\n-    HDFS-4227. Document dfs.namenode.resource.*  (Daisuke Kobayashi via aw)\n+    HDFS-7167. NPE while running Mover if the given path is for a file. (jing9)\n \n Release 2.5.1 - 2014-09-05\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "40891bfc5193b609999b8ebcceb4955ae8aecac5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java?ref=cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
                "patch": "@@ -252,14 +252,9 @@ private boolean isSnapshotPathInCurrent(String path) throws IOException {\n      */\n     private boolean processNamespace() {\n       getSnapshottableDirs();\n-      boolean hasRemaining = true;\n-      try {\n-        for (Path target : targetPaths) {\n-          hasRemaining = processDirRecursively(\"\", dfs.getFileInfo(target\n-              .toUri().getPath()));\n-        }\n-      } catch (IOException e) {\n-        LOG.warn(\"Failed to get root directory status. Ignore and continue.\", e);\n+      boolean hasRemaining = false;\n+      for (Path target : targetPaths) {\n+        hasRemaining |= processPath(target.toUri().getPath());\n       }\n       // wait for pending move to finish and retry the failed migration\n       hasRemaining |= Dispatcher.waitForMoveCompletion(storages.targets.values());\n@@ -270,7 +265,7 @@ private boolean processNamespace() {\n      * @return whether there is still remaing migration work for the next\n      *         round\n      */\n-    private boolean processChildrenList(String fullPath) {\n+    private boolean processPath(String fullPath) {\n       boolean hasRemaining = false;\n       for (byte[] lastReturnedName = HdfsFileStatus.EMPTY_NAME;;) {\n         final DirectoryListing children;\n@@ -285,7 +280,7 @@ private boolean processChildrenList(String fullPath) {\n           return hasRemaining;\n         }\n         for (HdfsFileStatus child : children.getPartialListing()) {\n-          hasRemaining |= processDirRecursively(fullPath, child);\n+          hasRemaining |= processRecursively(fullPath, child);\n         }\n         if (children.hasMore()) {\n           lastReturnedName = children.getLastName();\n@@ -296,20 +291,19 @@ private boolean processChildrenList(String fullPath) {\n     }\n \n     /** @return whether the migration requires next round */\n-    private boolean processDirRecursively(String parent,\n-                                          HdfsFileStatus status) {\n+    private boolean processRecursively(String parent, HdfsFileStatus status) {\n       String fullPath = status.getFullName(parent);\n       boolean hasRemaining = false;\n       if (status.isDir()) {\n         if (!fullPath.endsWith(Path.SEPARATOR)) {\n           fullPath = fullPath + Path.SEPARATOR;\n         }\n \n-        hasRemaining = processChildrenList(fullPath);\n+        hasRemaining = processPath(fullPath);\n         // process snapshots if this is a snapshottable directory\n         if (snapshottableDirs.contains(fullPath)) {\n           final String dirSnapshot = fullPath + HdfsConstants.DOT_SNAPSHOT_DIR;\n-          hasRemaining |= processChildrenList(dirSnapshot);\n+          hasRemaining |= processPath(dirSnapshot);\n         }\n       } else if (!status.isSymlink()) { // file\n         try {",
                "raw_url": "https://github.com/apache/hadoop/raw/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
                "sha": "04133bd23cc5694fb6a2ee47cf1c80415eeedbd5",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java?ref=cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java",
                "patch": "@@ -17,7 +17,6 @@\n  */\n package org.apache.hadoop.hdfs.server.mover;\n \n-import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n import java.util.ArrayList;\n@@ -27,12 +26,10 @@\n import java.util.List;\n import java.util.Map;\n \n-import com.google.common.base.Joiner;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.conf.ReconfigurationException;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n@@ -46,7 +43,6 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.DirectoryListing;\n-import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;\n import org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n@@ -514,6 +510,42 @@ static void banner(String string) {\n         \"==================================================\\n\\n\");\n   }\n \n+  /**\n+   * Run Mover with arguments specifying files and directories\n+   */\n+  @Test\n+  public void testMoveSpecificPaths() throws Exception {\n+    LOG.info(\"testMoveSpecificPaths\");\n+    final Path foo = new Path(\"/foo\");\n+    final Path barFile = new Path(foo, \"bar\");\n+    final Path foo2 = new Path(\"/foo2\");\n+    final Path bar2File = new Path(foo2, \"bar2\");\n+    Map<Path, BlockStoragePolicy> policyMap = Maps.newHashMap();\n+    policyMap.put(foo, COLD);\n+    policyMap.put(foo2, WARM);\n+    NamespaceScheme nsScheme = new NamespaceScheme(Arrays.asList(foo, foo2),\n+        Arrays.asList(barFile, bar2File), BLOCK_SIZE, null, policyMap);\n+    ClusterScheme clusterScheme = new ClusterScheme(DEFAULT_CONF,\n+        NUM_DATANODES, REPL, genStorageTypes(NUM_DATANODES), null);\n+    MigrationTest test = new MigrationTest(clusterScheme, nsScheme);\n+    test.setupCluster();\n+\n+    try {\n+      test.prepareNamespace();\n+      test.setStoragePolicy();\n+\n+      Map<URI, List<Path>> map = Mover.Cli.getNameNodePathsToMove(test.conf,\n+          \"-p\", \"/foo/bar\", \"/foo2\");\n+      int result = Mover.run(map, test.conf);\n+      Assert.assertEquals(ExitStatus.SUCCESS.getExitCode(), result);\n+\n+      Thread.sleep(5000);\n+      test.verify(true);\n+    } finally {\n+      test.shutdownCluster();\n+    }\n+  }\n+\n   /**\n    * Move an open file into archival storage\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java",
                "sha": "a6edd8090e8e9cb1beb636da6667e370826e98a4",
                "status": "modified"
            }
        ],
        "message": "HDFS-7167. NPE while running Mover if the given path is for a file. Contributed by Jing Zhao.",
        "parent": "https://github.com/apache/hadoop/commit/ea32a66f7d37d4d6a121e34c95d93ae8992be571",
        "patched_files": [
            "CHANGES.java",
            "Mover.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestStorageMover.java",
            "TestMover.java"
        ]
    },
    "hadoop_ce74e64": {
        "bug_id": "hadoop_ce74e64",
        "commit": "https://github.com/apache/hadoop/commit/ce74e64363abc64561263be70a923ab3e67f043f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "patch": "@@ -344,11 +344,7 @@ private void registerServiceInstance(ApplicationAttemptId attemptId,\n         attemptId.getApplicationId().toString());\n     serviceRecord.set(YarnRegistryAttributes.YARN_PERSISTENCE,\n         PersistencePolicies.APPLICATION);\n-    serviceRecord.description = \"Yarn Service Master\";\n-\n-    serviceRecord.addExternalEndpoint(RegistryTypeUtils\n-        .ipcEndpoint(\"classpath:org.apache.hadoop.yarn.service.appmaster.ipc\",\n-            context.clientAMService.getBindAddress()));\n+    serviceRecord.description = \"YarnServiceMaster\";\n \n     // set any provided attributes\n     setUserProvidedServiceRecordAttributes(service.getConfiguration(),",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "sha": "ec5f3ed1519d5e0fd639dfc25023c4359e8f1de9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "patch": "@@ -170,6 +170,9 @@ private Service loadAppJsonFromLocalFS(\n     if (!StringUtils.isEmpty(args.getServiceName())) {\n       service.setName(args.getServiceName());\n     }\n+    if (!StringUtils.isEmpty(args.queue)) {\n+      service.setQueue(args.queue);\n+    }\n     return service;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "sha": "a3a9fd0ae5fc10ff78724a0305c613148515599d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java",
                "patch": "@@ -37,7 +37,7 @@ public File getFile() {\n   }\n \n   @Parameter(names = {\n-      ARG_QUEUE }, description = \"Queue to submit the service\")\n+      ARG_QUEUE, ARG_SHORT_QUEUE}, description = \"Queue to submit the service\")\n   public String queue;\n \n   @Parameter(names = {",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java",
                "sha": "4ecbe9c64230afa14836aa91e4febc144de1c19f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java",
                "patch": "@@ -77,6 +77,7 @@\n   String ARG_PATH = \"--path\";\n   String ARG_PRINCIPAL = \"--principal\";\n   String ARG_QUEUE = \"--queue\";\n+  String ARG_SHORT_QUEUE = \"-q\";\n   String ARG_LIFETIME = \"--lifetime\";\n   String ARG_RESOURCE = \"--resource\";\n   String ARG_RESOURCE_MANAGER = \"--rm\";",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java",
                "sha": "67571e2b45ad81231e15b01d30f24daa9c186cba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "patch": "@@ -59,6 +59,7 @@\n import java.util.concurrent.locks.ReentrantReadWriteLock.ReadLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock.WriteLock;\n \n+import static org.apache.hadoop.registry.client.types.yarn.YarnRegistryAttributes.*;\n import static org.apache.hadoop.yarn.api.records.ContainerExitStatus.KILLED_BY_APPMASTER;\n import static org.apache.hadoop.yarn.api.records.ContainerState.COMPLETE;\n import static org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEventType.*;\n@@ -356,12 +357,11 @@ private  void updateServiceRecord(\n       YarnRegistryViewForProviders yarnRegistry, ContainerStatus status) {\n     ServiceRecord record = new ServiceRecord();\n     String containerId = status.getContainerId().toString();\n-    record.set(YarnRegistryAttributes.YARN_ID, containerId);\n+    record.set(YARN_ID, containerId);\n     record.description = getCompInstanceName();\n-    record.set(YarnRegistryAttributes.YARN_PERSISTENCE,\n-        PersistencePolicies.CONTAINER);\n-    record.set(\"yarn:ip\", status.getIPs());\n-    record.set(\"yarn:hostname\", status.getHost());\n+    record.set(YARN_PERSISTENCE, PersistencePolicies.CONTAINER);\n+    record.set(YARN_IP, status.getIPs().get(0));\n+    record.set(YARN_HOSTNAME, status.getHost());\n     try {\n       yarnRegistry\n           .putComponent(RegistryPathUtils.encodeYarnID(containerId), record);",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "sha": "3c1e48ff1ab63bd580879b45ba4170f8bddc0b0a",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.yarn.service.provider;\n \n+import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.api.ApplicationConstants;\n import org.apache.hadoop.yarn.service.api.records.Service;\n@@ -91,13 +92,16 @@ public void buildContainerLaunchContext(AbstractLauncher launcher,\n         component, tokensForSubstitution, instance, context);\n \n     // substitute launch command\n-    String launchCommand = ProviderUtils\n-        .substituteStrWithTokens(component.getLaunchCommand(),\n-            tokensForSubstitution);\n-    CommandLineBuilder operation = new CommandLineBuilder();\n-    operation.add(launchCommand);\n-    operation.addOutAndErrFiles(OUT_FILE, ERR_FILE);\n-    launcher.addCommand(operation.build());\n+    String launchCommand = component.getLaunchCommand();\n+    // docker container may have empty commands\n+    if (!StringUtils.isEmpty(launchCommand)) {\n+      launchCommand = ProviderUtils\n+          .substituteStrWithTokens(launchCommand, tokensForSubstitution);\n+      CommandLineBuilder operation = new CommandLineBuilder();\n+      operation.add(launchCommand);\n+      operation.addOutAndErrFiles(OUT_FILE, ERR_FILE);\n+      launcher.addCommand(operation.build());\n+    }\n \n     // By default retry forever every 30 seconds\n     launcher.setRetryContext(YarnServiceConf",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "sha": "6ffb84defb83a5e4768b827025048b403526a1ae",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java",
                "patch": "@@ -22,7 +22,6 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.registry.client.exceptions.InvalidRecordException;\n-import static org.apache.hadoop.registry.client.types.AddressTypes.*;\n import org.apache.hadoop.registry.client.types.Endpoint;\n import org.apache.hadoop.registry.client.types.ProtocolTypes;\n import org.apache.hadoop.registry.client.types.ServiceRecord;\n@@ -36,6 +35,8 @@\n import java.util.List;\n import java.util.Map;\n \n+import static org.apache.hadoop.registry.client.types.AddressTypes.*;\n+\n /**\n  * Static methods to work with registry types \u2014primarily endpoints and the\n  * list representation of addresses.",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java",
                "sha": "05df3255e3a76d2c78230e0660ff5d9b07ba0f8b",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java",
                "patch": "@@ -18,6 +18,8 @@\n \n import org.apache.hadoop.registry.client.types.Endpoint;\n import org.apache.hadoop.registry.client.types.ServiceRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n import org.xbill.DNS.Name;\n import org.xbill.DNS.Type;\n \n@@ -32,7 +34,8 @@\n  */\n public class ApplicationServiceRecordProcessor extends\n     BaseServiceRecordProcessor {\n-\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ApplicationServiceRecordProcessor.class);\n   /**\n    * Create an application service record processor.\n    *\n@@ -57,6 +60,10 @@ public ApplicationServiceRecordProcessor(\n    */\n   @Override public void initTypeToInfoMapping(ServiceRecord serviceRecord)\n       throws Exception {\n+    if (serviceRecord.external.isEmpty()) {\n+      LOG.info(serviceRecord.description + \": No external endpoints defined.\");\n+      return;\n+    }\n     for (int type : getRecordTypes()) {\n       switch (type) {\n       case Type.A:\n@@ -309,6 +316,9 @@ public AApplicationRecordDescriptor(String path,\n         throws Exception {\n       this.setNames(new Name[] {getServiceName()});\n       List<Endpoint> endpoints = serviceRecord.external;\n+      if (endpoints.isEmpty()) {\n+        return;\n+      }\n       // TODO:  do we need a \"hostname\" attribute for an application record or\n       // can we rely on the first endpoint record.\n       this.setTarget(InetAddress.getByName(\n@@ -342,6 +352,9 @@ public AAAAApplicationRecordDescriptor(String path,\n     @Override protected void init(ServiceRecord serviceRecord)\n         throws Exception {\n       super.init(serviceRecord);\n+      if (getTarget() == null) {\n+        return;\n+      }\n       try {\n         this.setTarget(getIpv6Address(getTarget()));\n       } catch (UnknownHostException e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java",
                "sha": "0b5f724f8691cebefbce0090ec8678567b3108dd",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java",
                "patch": "@@ -52,8 +52,8 @@\n   private String domain;\n \n   private static final Pattern USER_NAME = Pattern.compile(\"/users/(\\\\w*)/?\");\n-  private static final String SLIDER_API_PREFIX =\n-      \"classpath:org.apache.slider.\";\n+  private static final String YARN_SERVICE_API_PREFIX =\n+      \"classpath:org.apache.hadoop.yarn.service.\";\n   private static final String HTTP_API_TYPE = \"http://\";\n \n   /**\n@@ -425,8 +425,8 @@ protected int getPort(Endpoint endpoint) {\n      */\n     protected String getDNSApiFragment(String api) {\n       String dnsApi = null;\n-      if (api.startsWith(SLIDER_API_PREFIX)) {\n-        dnsApi = api.substring(SLIDER_API_PREFIX.length());\n+      if (api.startsWith(YARN_SERVICE_API_PREFIX)) {\n+        dnsApi = api.substring(YARN_SERVICE_API_PREFIX.length());\n       } else if (api.startsWith(HTTP_API_TYPE)) {\n         dnsApi = \"http\";\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java",
                "sha": "fd5c74f64900971d8648f28471a23b9890f35b65",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java",
                "patch": "@@ -69,7 +69,8 @@\n       + \"  \\\"type\\\" : \\\"JSONServiceRecord\\\",\\n\"\n       + \"  \\\"description\\\" : \\\"Slider Application Master\\\",\\n\"\n       + \"  \\\"external\\\" : [ {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.appmaster.ipc\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.appmaster.ipc\"\n+      + \"\\\",\\n\"\n       + \"    \\\"addressType\\\" : \\\"host/port\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"hadoop/IPC\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"\n@@ -84,22 +85,25 @@\n       + \"      \\\"uri\\\" : \\\"http://192.168.1.5:1027\\\"\\n\"\n       + \"    } ]\\n\"\n       + \"  }, {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.management\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.management\\\"\"\n+      + \",\\n\"\n       + \"    \\\"addressType\\\" : \\\"uri\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"REST\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"\n       + \"      \\\"uri\\\" : \\\"http://192.168.1.5:1027/ws/v1/slider/mgmt\\\"\\n\"\n       + \"    } ]\\n\"\n       + \"  } ],\\n\"\n       + \"  \\\"internal\\\" : [ {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.agents.secure\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.agents.secure\"\n+      + \"\\\",\\n\"\n       + \"    \\\"addressType\\\" : \\\"uri\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"REST\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"\n       + \"      \\\"uri\\\" : \\\"https://192.168.1.5:47700/ws/v1/slider/agents\\\"\\n\"\n       + \"    } ]\\n\"\n       + \"  }, {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.agents.oneway\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.agents.oneway\"\n+      + \"\\\",\\n\"\n       + \"    \\\"addressType\\\" : \\\"uri\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"REST\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java",
                "sha": "ac8d9392d5b7323a550a76f94a585410d2c40463",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "patch": "@@ -812,13 +812,13 @@ public void reapContainer(ContainerRuntimeContext ctx)\n           .executePrivilegedOperation(null, privOp, null,\n               null, true, false);\n       LOG.info(\"Docker inspect output for \" + containerId + \": \" + output);\n+      // strip off quotes if any\n+      output = output.replaceAll(\"['\\\"]\", \"\");\n       int index = output.lastIndexOf(',');\n       if (index == -1) {\n         LOG.error(\"Incorrect format for ip and host\");\n         return null;\n       }\n-      // strip off quotes if any\n-      output = output.replaceAll(\"['\\\"]\", \"\");\n       String ips = output.substring(0, index).trim();\n       String host = output.substring(index+1).trim();\n       String[] ipAndHost = new String[2];",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "sha": "75a28e648b36fbf5d77bf28951553f0db15caf2c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md",
                "patch": "@@ -92,6 +92,7 @@ Usage `yarn service [sub-command] [service-name] [options]`\n \n    Options:\n     --file,-f       The local path to the service definition file.\n+    --queue,-q      The queue to which the service is submitted.\n     --example,-e    The name of the example service such as:\n                     Sleeper      A simple service that launches a few non-docker sleep containers on YARN.\n    ```",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md",
                "sha": "51e27cc1cf07d8cf18eaf81a1e1f949f25a2eabc",
                "status": "modified"
            }
        ],
        "message": "YARN-7210. Some NPE fixes in Registry DNS. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/37c9b7327d188ccad7fd36b7466a65f68ad0c899",
        "patched_files": [
            "RegistryDNS.java",
            "ComponentInstance.java",
            "AbstractProviderService.java",
            "RegistryTypeUtils.java",
            "YarnCommands.java",
            "AbstractClusterBuildingActionArgs.java",
            "ServiceClient.java",
            "ServiceScheduler.java",
            "BaseServiceRecordProcessor.java",
            "ApplicationServiceRecordProcessor.java",
            "Arguments.java",
            "DockerLinuxContainerRuntime.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRegistryDNS.java"
        ]
    },
    "hadoop_cf23f2c": {
        "bug_id": "hadoop_cf23f2c",
        "commit": "https://github.com/apache/hadoop/commit/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -945,6 +945,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-4250. NPE in AppSchedulingInfo#isRequestLabelChanged. (Brahma Reddy Battula via rohithsharmaks)\n \n+    YARN-4000. RM crashes with NPE if leaf queue becomes parent queue during restart. \n+    (Varun Saxena via jianhe)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/CHANGES.txt",
                "sha": "824a4b8863aa6b043a8f3143aff67a52a6bbd9d5",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -141,7 +141,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMoveEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptFailedEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeSignalContainerEvent;\n@@ -676,11 +677,8 @@ public FailApplicationAttemptResponse failApplicationAttempt(\n       }\n     }\n \n-    this.rmContext\n-        .getDispatcher()\n-        .getEventHandler()\n-        .handle(\n-            new RMAppAttemptFailedEvent(attemptId,\n+    this.rmContext.getDispatcher().getEventHandler().handle(\n+        new RMAppAttemptEvent(attemptId, RMAppAttemptEventType.FAIL,\n         \"Attempt failed by user.\"));\n \n     RMAuditLogger.logSuccess(callerUGI.getShortUserName(),\n@@ -735,8 +733,9 @@ public KillApplicationResponse forceKillApplication(\n       return KillApplicationResponse.newInstance(true);\n     }\n \n-    this.rmContext.getDispatcher().getEventHandler()\n-        .handle(new RMAppEvent(applicationId, RMAppEventType.KILL));\n+    this.rmContext.getDispatcher().getEventHandler().handle(\n+        new RMAppEvent(applicationId, RMAppEventType.KILL,\n+        \"Application killed by user.\"));\n \n     // For UnmanagedAMs, return true so they don't retry\n     return KillApplicationResponse.newInstance(",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "4a02580b51017579d7a412400d9d06094e27a8fc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                "patch": "@@ -50,7 +50,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRecoverEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n@@ -304,7 +303,8 @@ protected void submitApplication(\n       // scheduler about the existence of the application\n       assert application.getState() == RMAppState.NEW;\n       this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, e.getMessage()));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, e.getMessage()));\n       throw RPCUtil.getRemoteException(e);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                "sha": "0b7083c45f9068db95280681b6287d193b9efb62",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "patch": "@@ -60,7 +60,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.util.ConverterUtils;\n \n import com.google.common.annotations.VisibleForTesting;\n@@ -257,8 +256,8 @@ public void run() {\n         String message = \"Error launching \" + application.getAppAttemptId()\n             + \". Got exception: \" + StringUtils.stringifyException(ie);\n         LOG.info(message);\n-        handler.handle(new RMAppAttemptLaunchFailedEvent(application\n-            .getAppAttemptId(), message));\n+        handler.handle(new RMAppAttemptEvent(application\n+            .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED, message));\n       }\n       break;\n     case CLEANUP:",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "sha": "b927bb425726b47875ba9c94e03ba7feb5c252d9",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java",
                "patch": "@@ -24,13 +24,24 @@\n public class RMAppEvent extends AbstractEvent<RMAppEventType>{\n \n   private final ApplicationId appId;\n+  private final String diagnosticMsg;\n \n   public RMAppEvent(ApplicationId appId, RMAppEventType type) {\n+    this(appId, type, \"\");\n+  }\n+\n+  public RMAppEvent(ApplicationId appId, RMAppEventType type,\n+      String diagnostic) {\n     super(type);\n     this.appId = appId;\n+    this.diagnosticMsg = diagnostic;\n   }\n \n   public ApplicationId getApplicationId() {\n     return this.appId;\n   }\n+\n+  public String getDiagnosticMsg() {\n+    return this.diagnosticMsg;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java",
                "sha": "649640207211522532ab2e934f826ff07262b1c3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java",
                "patch": "@@ -22,20 +22,14 @@\n \n public class RMAppFailedAttemptEvent extends RMAppEvent {\n \n-  private final String diagnostics;\n   private final boolean transferStateFromPreviousAttempt;\n \n   public RMAppFailedAttemptEvent(ApplicationId appId, RMAppEventType event, \n       String diagnostics, boolean transferStateFromPreviousAttempt) {\n-    super(appId, event);\n-    this.diagnostics = diagnostics;\n+    super(appId, event, diagnostics);\n     this.transferStateFromPreviousAttempt = transferStateFromPreviousAttempt;\n   }\n \n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-\n   public boolean getTransferStateFromPreviousAttempt() {\n     return transferStateFromPreviousAttempt;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java",
                "sha": "835322a37e61022d9eeb6181ea77aa0f77f5c667",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 35,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java",
                "patch": "@@ -1,35 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n-\n-public class RMAppFinishedAttemptEvent extends RMAppEvent {\n-\n-  private final String diagnostics;\n-\n-  public RMAppFinishedAttemptEvent(ApplicationId appId, String diagnostics) {\n-    super(appId, RMAppEventType.ATTEMPT_FINISHED);\n-    this.diagnostics = diagnostics;\n-  }\n-\n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java",
                "sha": "f1a6340ba8ef06784949b9a573937b97da25acec",
                "status": "removed"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 23,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "patch": "@@ -1046,7 +1046,7 @@ private String getAppAttemptFailedDiagnostics(RMAppEvent event) {\n     if (this.submissionContext.getUnmanagedAM()) {\n       // RM does not manage the AM. Do not retry\n       msg = \"Unmanaged application \" + this.getApplicationId()\n-              + \" failed due to \" + failedEvent.getDiagnostics()\n+              + \" failed due to \" + failedEvent.getDiagnosticMsg()\n               + \". Failing the application.\";\n     } else if (this.isNumAttemptsBeyondThreshold) {\n       int globalLimit = conf.getInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS,\n@@ -1061,7 +1061,7 @@ private String getAppAttemptFailedDiagnostics(RMAppEvent event) {\n           (globalLimit == maxAppAttempts) ? \"\"\n               : (\" (global limit =\" + globalLimit\n                  + \"; local limit is =\" + maxAppAttempts + \")\"),\n-          failedEvent.getDiagnostics());\n+          failedEvent.getDiagnosticMsg());\n     }\n     return msg;\n   }\n@@ -1102,21 +1102,14 @@ private void rememberTargetTransitionsAndStoreState(RMAppEvent event,\n     String diags = null;\n     switch (event.getType()) {\n     case APP_REJECTED:\n-      RMAppRejectedEvent rejectedEvent = (RMAppRejectedEvent) event;\n-      diags = rejectedEvent.getMessage();\n-      break;\n     case ATTEMPT_FINISHED:\n-      RMAppFinishedAttemptEvent finishedEvent =\n-          (RMAppFinishedAttemptEvent) event;\n-      diags = finishedEvent.getDiagnostics();\n+    case ATTEMPT_KILLED:\n+      diags = event.getDiagnosticMsg();\n       break;\n     case ATTEMPT_FAILED:\n       RMAppFailedAttemptEvent failedEvent = (RMAppFailedAttemptEvent) event;\n       diags = getAppAttemptFailedDiagnostics(failedEvent);\n       break;\n-    case ATTEMPT_KILLED:\n-      diags = getAppKilledDiagnostics();\n-      break;\n     default:\n       break;\n     }\n@@ -1164,9 +1157,7 @@ public AppFinishedTransition() {\n     }\n \n     public void transition(RMAppImpl app, RMAppEvent event) {\n-      RMAppFinishedAttemptEvent finishedEvent =\n-          (RMAppFinishedAttemptEvent)event;\n-      app.diagnostics.append(finishedEvent.getDiagnostics());\n+      app.diagnostics.append(event.getDiagnosticMsg());\n       super.transition(app, event);\n     };\n   }\n@@ -1212,21 +1203,21 @@ public AppKilledTransition() {\n \n     @Override\n     public void transition(RMAppImpl app, RMAppEvent event) {\n-      app.diagnostics.append(getAppKilledDiagnostics());\n+      app.diagnostics.append(event.getDiagnosticMsg());\n       super.transition(app, event);\n     };\n   }\n \n-  private static String getAppKilledDiagnostics() {\n-    return \"Application killed by user.\";\n-  }\n-\n   private static class KillAttemptTransition extends RMAppTransition {\n     @Override\n     public void transition(RMAppImpl app, RMAppEvent event) {\n       app.stateBeforeKilling = app.getState();\n-      app.handler.handle(new RMAppAttemptEvent(app.currentAttempt\n-        .getAppAttemptId(), RMAppAttemptEventType.KILL));\n+      // Forward app kill diagnostics in the event to kill app attempt.\n+      // These diagnostics will be returned back in ATTEMPT_KILLED event sent by\n+      // RMAppAttemptImpl.\n+      app.handler.handle(\n+          new RMAppAttemptEvent(app.currentAttempt.getAppAttemptId(),\n+              RMAppAttemptEventType.KILL, event.getDiagnosticMsg()));\n     }\n   }\n \n@@ -1237,8 +1228,7 @@ public AppRejectedTransition() {\n     }\n \n     public void transition(RMAppImpl app, RMAppEvent event) {\n-      RMAppRejectedEvent rejectedEvent = (RMAppRejectedEvent)event;\n-      app.diagnostics.append(rejectedEvent.getMessage());\n+      app.diagnostics.append(event.getDiagnosticMsg());\n       super.transition(app, event);\n     };\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "sha": "43a3a51a467940f91fc41b66b8c164d44fd8f52f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 35,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java",
                "patch": "@@ -1,35 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n-\n-public class RMAppRejectedEvent extends RMAppEvent {\n-\n-  private final String message;\n-\n-  public RMAppRejectedEvent(ApplicationId appId, String message) {\n-    super(appId, RMAppEventType.APP_REJECTED);\n-    this.message = message;\n-  }\n-\n-  public String getMessage() {\n-    return this.message;\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java",
                "sha": "baaef238ca5070d887be04844e7a0703cbec3d50",
                "status": "removed"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java",
                "patch": "@@ -24,14 +24,25 @@\n public class RMAppAttemptEvent extends AbstractEvent<RMAppAttemptEventType> {\n \n   private final ApplicationAttemptId appAttemptId;\n+  private final String diagnosticMsg;\n \n   public RMAppAttemptEvent(ApplicationAttemptId appAttemptId,\n       RMAppAttemptEventType type) {\n+    this(appAttemptId, type, \"\");\n+  }\n+\n+  public RMAppAttemptEvent(ApplicationAttemptId appAttemptId,\n+      RMAppAttemptEventType type, String diagnostics) {\n     super(type);\n     this.appAttemptId = appAttemptId;\n+    this.diagnosticMsg = diagnostics;\n   }\n \n   public ApplicationAttemptId getApplicationAttemptId() {\n     return this.appAttemptId;\n   }\n+\n+  public String getDiagnosticMsg() {\n+    return diagnosticMsg;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java",
                "sha": "6df6b19f97843691a4ad5194c36ef24572d9953a",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 22,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -82,12 +82,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFinishedAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerAllocatedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptFailedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptRegistrationEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptStatusupdateEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptUnregistrationEvent;\n@@ -1085,8 +1081,9 @@ public void run() {\n           LOG.warn(\"Interrupted while waiting to resend the\"\n               + \" ContainerAllocated Event.\");\n         }\n-        appAttempt.eventHandler.handle(new RMAppAttemptContainerAllocatedEvent(\n-          appAttempt.applicationAttemptId));\n+        appAttempt.eventHandler.handle(\n+            new RMAppAttemptEvent(appAttempt.applicationAttemptId,\n+                RMAppAttemptEventType.CONTAINER_ALLOCATED));\n       }\n     }.start();\n   }\n@@ -1195,17 +1192,15 @@ private void rememberTargetTransitionsAndStoreState(RMAppAttemptEvent event,\n     int exitStatus = ContainerExitStatus.INVALID;\n     switch (event.getType()) {\n     case LAUNCH_FAILED:\n-      RMAppAttemptLaunchFailedEvent launchFaileEvent =\n-          (RMAppAttemptLaunchFailedEvent) event;\n-      diags = launchFaileEvent.getMessage();\n+      diags = event.getDiagnosticMsg();\n       break;\n     case REGISTERED:\n       diags = getUnexpectedAMRegisteredDiagnostics();\n       break;\n     case UNREGISTERED:\n       RMAppAttemptUnregistrationEvent unregisterEvent =\n           (RMAppAttemptUnregistrationEvent) event;\n-      diags = unregisterEvent.getDiagnostics();\n+      diags = unregisterEvent.getDiagnosticMsg();\n       // reset finalTrackingUrl to url sent by am\n       finalTrackingUrl = sanitizeTrackingUrl(unregisterEvent.getFinalTrackingUrl());\n       finalStatus = unregisterEvent.getFinalApplicationStatus();\n@@ -1219,9 +1214,7 @@ private void rememberTargetTransitionsAndStoreState(RMAppAttemptEvent event,\n     case KILL:\n       break;\n     case FAIL:\n-      RMAppAttemptFailedEvent failEvent =\n-          (RMAppAttemptFailedEvent) event;\n-      diags = failEvent.getDiagnostics();\n+      diags = event.getDiagnosticMsg();\n       break;\n     case EXPIRE:\n       diags = getAMExpiredDiagnostics(event);\n@@ -1309,17 +1302,19 @@ public void transition(RMAppAttemptImpl appAttempt,\n       switch (finalAttemptState) {\n         case FINISHED:\n         {\n-          appEvent = new RMAppFinishedAttemptEvent(applicationId,\n+          appEvent =\n+              new RMAppEvent(applicationId, RMAppEventType.ATTEMPT_FINISHED,\n               appAttempt.getDiagnostics());\n         }\n         break;\n         case KILLED:\n         {\n           appAttempt.invalidateAMHostAndPort();\n+          // Forward diagnostics received in attempt kill event.\n           appEvent =\n               new RMAppFailedAttemptEvent(applicationId,\n                   RMAppEventType.ATTEMPT_KILLED,\n-                  \"Application killed by user.\", false);\n+                  event.getDiagnosticMsg(), false);\n         }\n         break;\n         case FAILED:\n@@ -1377,9 +1372,8 @@ public AttemptFailedTransition() {\n \n     @Override\n     public void transition(RMAppAttemptImpl appAttempt, RMAppAttemptEvent event) {\n-      RMAppAttemptFailedEvent failedEvent = (RMAppAttemptFailedEvent) event;\n-      if (failedEvent.getDiagnostics() != null) {\n-        appAttempt.diagnostics.append(failedEvent.getDiagnostics());\n+      if (event.getDiagnosticMsg() != null) {\n+        appAttempt.diagnostics.append(event.getDiagnosticMsg());\n       }\n       super.transition(appAttempt, event);\n     }\n@@ -1451,9 +1445,7 @@ public void transition(RMAppAttemptImpl appAttempt,\n         RMAppAttemptEvent event) {\n \n       // Use diagnostic from launcher\n-      RMAppAttemptLaunchFailedEvent launchFaileEvent\n-        = (RMAppAttemptLaunchFailedEvent) event;\n-      appAttempt.diagnostics.append(launchFaileEvent.getMessage());\n+      appAttempt.diagnostics.append(event.getDiagnosticMsg());\n \n       // Tell the app, scheduler\n       super.transition(appAttempt, event);\n@@ -1708,7 +1700,7 @@ private void updateInfoOnAMUnregister(RMAppAttemptEvent event) {\n     progress = 1.0f;\n     RMAppAttemptUnregistrationEvent unregisterEvent =\n         (RMAppAttemptUnregistrationEvent) event;\n-    diagnostics.append(unregisterEvent.getDiagnostics());\n+    diagnostics.append(unregisterEvent.getDiagnosticMsg());\n     originalTrackingUrl = sanitizeTrackingUrl(unregisterEvent.getFinalTrackingUrl());\n     finalStatus = unregisterEvent.getFinalApplicationStatus();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "36fb9fc73879fce8d89c64cda5c54397ca66a97e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 31,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java",
                "patch": "@@ -1,31 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.Container;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n-\n-public class RMAppAttemptContainerAllocatedEvent extends RMAppAttemptEvent {\n-\n-  public RMAppAttemptContainerAllocatedEvent(ApplicationAttemptId appAttemptId) {\n-    super(appAttemptId, RMAppAttemptEventType.CONTAINER_ALLOCATED);\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java",
                "sha": "681f38c2c2dbe00906e1123bc5bad855a2dcce69",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 39,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java",
                "patch": "@@ -1,39 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n-\n-public class RMAppAttemptFailedEvent extends RMAppAttemptEvent {\n-\n-  private final String diagnostics;\n-\n-  public RMAppAttemptFailedEvent(ApplicationAttemptId appAttemptId,\n-      String diagnostics) {\n-    super(appAttemptId, RMAppAttemptEventType.FAIL);\n-    this.diagnostics = diagnostics;\n-  }\n-\n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java",
                "sha": "c698e7d872417a3e199623f6110a7c9595385c4d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 38,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java",
                "patch": "@@ -1,38 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n-\n-public class RMAppAttemptLaunchFailedEvent extends RMAppAttemptEvent {\n-\n-  private final String message;\n-\n-  public RMAppAttemptLaunchFailedEvent(ApplicationAttemptId appAttemptId,\n-      String message) {\n-    super(appAttemptId, RMAppAttemptEventType.LAUNCH_FAILED);\n-    this.message = message;\n-  }\n-\n-  public String getMessage() {\n-    return this.message;\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java",
                "sha": "d0b49b2d160adcf6fd3fb90a42c2123441a8b2d1",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java",
                "patch": "@@ -27,14 +27,13 @@\n \n   private final String finalTrackingUrl;\n   private final FinalApplicationStatus finalStatus;\n-  private final String diagnostics;\n \n   public RMAppAttemptUnregistrationEvent(ApplicationAttemptId appAttemptId,\n-      String trackingUrl, FinalApplicationStatus finalStatus, String diagnostics) {\n-    super(appAttemptId, RMAppAttemptEventType.UNREGISTERED);\n+      String trackingUrl, FinalApplicationStatus finalStatus,\n+      String diagnostics) {\n+    super(appAttemptId, RMAppAttemptEventType.UNREGISTERED, diagnostics);\n     this.finalTrackingUrl = trackingUrl;\n     this.finalStatus = finalStatus;\n-    this.diagnostics = diagnostics;\n   }\n \n   public String getFinalTrackingUrl() {\n@@ -45,8 +44,4 @@ public FinalApplicationStatus getFinalApplicationStatus() {\n     return this.finalStatus;\n   }\n \n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java",
                "sha": "1ce51507ee515adbe677289fc912c87c6875900e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -45,7 +45,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerAllocatedEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeCleanContainerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.ContainerRescheduledEvent;\n@@ -511,8 +512,8 @@ public void transition(RMContainerImpl container, RMContainerEvent event) {\n \n     @Override\n     public void transition(RMContainerImpl container, RMContainerEvent event) {\n-      container.eventHandler.handle(new RMAppAttemptContainerAllocatedEvent(\n-          container.appAttemptId));\n+      container.eventHandler.handle(new RMAppAttemptEvent(\n+          container.appAttemptId, RMAppAttemptEventType.CONTAINER_ALLOCATED));\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "96c4f2772d6d504c28e051f4bc34e0f61f787281",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "patch": "@@ -651,7 +651,9 @@ public synchronized void killAllAppsInQueue(String queueName)\n       this.rmContext\n           .getDispatcher()\n           .getEventHandler()\n-          .handle(new RMAppEvent(app.getApplicationId(), RMAppEventType.KILL));\n+          .handle(new RMAppEvent(app.getApplicationId(), RMAppEventType.KILL,\n+          \"Application killed due to expiry of reservation queue \" +\n+          queueName + \".\"));\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "sha": "abd72bfa8baca48883b2055e7416bcce9298d356",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java",
                "patch": "@@ -22,11 +22,11 @@\n import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;\n \n @Private\n-public class QueueNotFoundException extends YarnRuntimeException {\n+public class QueueInvalidException extends YarnRuntimeException {\n \n   private static final long serialVersionUID = 187239430L;\n \n-  public QueueNotFoundException(String message) {\n+  public QueueInvalidException(String message) {\n     super(message);\n   }\n }",
                "previous_filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueNotFoundException.java",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java",
                "sha": "7c6be4f0ab5ff73f356895db6b53f4f10364ab12",
                "status": "renamed"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 124,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 36,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -80,7 +80,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n@@ -97,8 +96,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueInvalidException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueNotFoundException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication;\n@@ -666,47 +665,97 @@ public CSQueue getQueue(String queueName) {\n     return queues.get(queueName);\n   }\n \n-  private synchronized void addApplication(ApplicationId applicationId,\n-      String queueName, String user, boolean isAppRecovering, Priority priority) {\n-    // sanity checks.\n+  private synchronized void addApplicationOnRecovery(\n+      ApplicationId applicationId, String queueName, String user,\n+      Priority priority) {\n     CSQueue queue = getQueue(queueName);\n     if (queue == null) {\n       //During a restart, this indicates a queue was removed, which is\n       //not presently supported\n-      if (isAppRecovering) {\n+      if (!YarnConfiguration.shouldRMFailFast(getConfig())) {\n+        this.rmContext.getDispatcher().getEventHandler().handle(\n+            new RMAppEvent(applicationId, RMAppEventType.KILL,\n+            \"Application killed on recovery as it was submitted to queue \" +\n+            queueName + \" which no longer exists after restart.\"));\n+        return;\n+      } else {\n         String queueErrorMsg = \"Queue named \" + queueName\n-           + \" missing during application recovery.\"\n-           + \" Queue removal during recovery is not presently supported by the\"\n-           + \" capacity scheduler, please restart with all queues configured\"\n-           + \" which were present before shutdown/restart.\";\n+            + \" missing during application recovery.\"\n+            + \" Queue removal during recovery is not presently supported by the\"\n+            + \" capacity scheduler, please restart with all queues configured\"\n+            + \" which were present before shutdown/restart.\";\n         LOG.fatal(queueErrorMsg);\n-        throw new QueueNotFoundException(queueErrorMsg);\n+        throw new QueueInvalidException(queueErrorMsg);\n       }\n-      String message = \"Application \" + applicationId + \n+    }\n+    if (!(queue instanceof LeafQueue)) {\n+      // During RM restart, this means leaf queue was converted to a parent\n+      // queue, which is not supported for running apps.\n+      if (!YarnConfiguration.shouldRMFailFast(getConfig())) {\n+        this.rmContext.getDispatcher().getEventHandler().handle(\n+            new RMAppEvent(applicationId, RMAppEventType.KILL,\n+            \"Application killed on recovery as it was submitted to queue \" +\n+            queueName + \" which is no longer a leaf queue after restart.\"));\n+        return;\n+      } else {\n+        String queueErrorMsg = \"Queue named \" + queueName\n+            + \" is no longer a leaf queue during application recovery.\"\n+            + \" Changing a leaf queue to a parent queue during recovery is\"\n+            + \" not presently supported by the capacity scheduler. Please\"\n+            + \" restart with leaf queues before shutdown/restart continuing\"\n+            + \" as leaf queues.\";\n+        LOG.fatal(queueErrorMsg);\n+        throw new QueueInvalidException(queueErrorMsg);\n+      }\n+    }\n+    // Submit to the queue\n+    try {\n+      queue.submitApplication(applicationId, user, queueName);\n+    } catch (AccessControlException ace) {\n+      // Ignore the exception for recovered app as the app was previously\n+      // accepted.\n+    }\n+    queue.getMetrics().submitApp(user);\n+    SchedulerApplication<FiCaSchedulerApp> application =\n+        new SchedulerApplication<FiCaSchedulerApp>(queue, user, priority);\n+    applications.put(applicationId, application);\n+    LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n+        + \", in queue: \" + queueName);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n+    }\n+  }\n+\n+  private synchronized void addApplication(ApplicationId applicationId,\n+      String queueName, String user, Priority priority) {\n+    // Sanity checks.\n+    CSQueue queue = getQueue(queueName);\n+    if (queue == null) {\n+      String message = \"Application \" + applicationId +\n       \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n       this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n     if (!(queue instanceof LeafQueue)) {\n       String message = \"Application \" + applicationId + \n           \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n       this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n     // Submit to the queue\n     try {\n       queue.submitApplication(applicationId, user, queueName);\n     } catch (AccessControlException ace) {\n-      // Ignore the exception for recovered app as the app was previously accepted\n-      if (!isAppRecovering) {\n-        LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n-            + queueName + \" from user \" + user, ace);\n-        this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n-        return;\n-      }\n+      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n+          + queueName + \" from user \" + user, ace);\n+      this.rmContext.getDispatcher().getEventHandler()\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, ace.toString()));\n+      return;\n     }\n     // update the metrics\n     queue.getMetrics().submitApp(user);\n@@ -715,14 +764,8 @@ private synchronized void addApplication(ApplicationId applicationId,\n     applications.put(applicationId, application);\n     LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n         + \", in queue: \" + queueName);\n-    if (isAppRecovering) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n-      }\n-    } else {\n-      rmContext.getDispatcher().getEventHandler()\n+    rmContext.getDispatcher().getEventHandler()\n         .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n-    }\n   }\n \n   private synchronized void addApplicationAttempt(\n@@ -731,6 +774,11 @@ private synchronized void addApplicationAttempt(\n       boolean isAttemptRecovering) {\n     SchedulerApplication<FiCaSchedulerApp> application =\n         applications.get(applicationAttemptId.getApplicationId());\n+    if (application == null) {\n+      LOG.warn(\"Application \" + applicationAttemptId.getApplicationId() +\n+          \" cannot be found in scheduler.\");\n+      return;\n+    }\n     CSQueue queue = (CSQueue) application.getQueue();\n \n     FiCaSchedulerApp attempt = new FiCaSchedulerApp(applicationAttemptId,\n@@ -1277,11 +1325,13 @@ public void handle(SchedulerEvent event) {\n               appAddedEvent.getApplicationId(),\n               appAddedEvent.getReservationID());\n       if (queueName != null) {\n-        addApplication(appAddedEvent.getApplicationId(),\n-            queueName,\n-            appAddedEvent.getUser(),\n-            appAddedEvent.getIsAppRecovering(),\n-            appAddedEvent.getApplicatonPriority());\n+        if (!appAddedEvent.getIsAppRecovering()) {\n+          addApplication(appAddedEvent.getApplicationId(), queueName,\n+              appAddedEvent.getUser(), appAddedEvent.getApplicatonPriority());\n+        } else {\n+          addApplicationOnRecovery(appAddedEvent.getApplicationId(), queueName,\n+              appAddedEvent.getUser(), appAddedEvent.getApplicatonPriority());\n+        }\n       }\n     }\n     break;\n@@ -1631,7 +1681,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + \" submitted to a reservation which is not yet currently active: \"\n                 + resQName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       if (!queue.getParent().getQueueName().equals(queueName)) {\n@@ -1640,7 +1691,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + resQName + \" which does not belong to the specified queue: \"\n                 + queueName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       // use the reservation queue to run the app",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "6e356b5f16ec93f7c095ca533bedfc0656abd337",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -61,7 +61,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n@@ -614,7 +613,8 @@ protected synchronized void addApplication(ApplicationId applicationId,\n               \" submitted by user \" + user + \" with an empty queue name.\";\n       LOG.info(message);\n       rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n \n@@ -625,7 +625,8 @@ protected synchronized void addApplication(ApplicationId applicationId,\n           + \"The queue name cannot start/end with period.\";\n       LOG.info(message);\n       rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n \n@@ -644,7 +645,8 @@ protected synchronized void addApplication(ApplicationId applicationId,\n               \" cannot submit applications to queue \" + queue.getName();\n       LOG.info(msg);\n       rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, msg));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, msg));\n       return;\n     }\n   \n@@ -742,7 +744,8 @@ FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n     if (appRejectMsg != null && rmApp != null) {\n       LOG.error(appRejectMsg);\n       rmContext.getDispatcher().getEventHandler().handle(\n-          new RMAppRejectedEvent(rmApp.getApplicationId(), appRejectMsg));\n+          new RMAppEvent(rmApp.getApplicationId(),\n+              RMAppEventType.APP_REJECTED, appRejectMsg));\n       return null;\n     }\n \n@@ -1302,7 +1305,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + \" submitted to a reservation which is not yet currently active: \"\n                 + resQName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       if (!queue.getParent().getQueueName().equals(queueName)) {\n@@ -1311,7 +1315,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + resQName + \" which does not belong to the specified queue: \"\n                 + queueName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       // use the reservation queue to run the app",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "33d01fc1d9ba4d4ae7810013cdd57808786536cf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "patch": "@@ -66,7 +66,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n@@ -872,7 +871,8 @@ private void handleDTRenewerAppSubmitEvent(\n         // RMApp is in NEW state and thus we havne't yet informed the\n         // Scheduler about the existence of the application\n         rmContext.getDispatcher().getEventHandler().handle(\n-            new RMAppRejectedEvent(event.getApplicationId(), t.getMessage()));\n+            new RMAppEvent(event.getApplicationId(),\n+                RMAppEventType.APP_REJECTED, t.getMessage()));\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "sha": "426e460ec6b68e0851a519dc3b0a69c5ceac1b5d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "patch": "@@ -76,7 +76,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n@@ -622,7 +621,8 @@ public void sendAMLaunchFailed(ApplicationAttemptId appAttemptId)\n     MockAM am = new MockAM(getRMContext(), masterService, appAttemptId);\n     am.waitForState(RMAppAttemptState.ALLOCATED);\n     getRMContext().getDispatcher().getEventHandler()\n-        .handle(new RMAppAttemptLaunchFailedEvent(appAttemptId, \"Failed\"));\n+        .handle(new RMAppAttemptEvent(appAttemptId,\n+            RMAppAttemptEventType.LAUNCH_FAILED, \"Failed\"));\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "sha": "a0619cf06006d63f0801e1b798d6e8d0c151e0d6",
                "status": "modified"
            },
            {
                "additions": 137,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "changes": 151,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 14,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "patch": "@@ -22,6 +22,8 @@\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.net.UnknownHostException;\n@@ -39,25 +41,31 @@\n import org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceRequest;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus;\n import org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.TestSecurityMockRM;\n import org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.RMState;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueInvalidException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueNotFoundException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;\n@@ -86,6 +94,7 @@\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n+import org.mortbay.log.Log;\n \n import com.google.common.base.Supplier;\n \n@@ -361,6 +370,8 @@ private void checkFSQueue(ResourceManager rm,\n   private static final String R = \"Default\";\n   private static final String A = \"QueueA\";\n   private static final String B = \"QueueB\";\n+  private static final String B1 = \"QueueB1\";\n+  private static final String B2 = \"QueueB2\";\n   //don't ever create the below queue ;-)\n   private static final String QUEUE_DOESNT_EXIST = \"NoSuchQueue\";\n   private static final String USER_1 = \"user1\";\n@@ -391,6 +402,24 @@ private void setupQueueConfigurationOnlyA(\n       .MAXIMUM_APPLICATION_MASTERS_RESOURCE_PERCENT, 1.0f);\n   }\n \n+  private void setupQueueConfigurationChildOfB(CapacitySchedulerConfiguration conf) {\n+    conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { R });\n+    final String Q_R = CapacitySchedulerConfiguration.ROOT + \".\" + R;\n+    conf.setCapacity(Q_R, 100);\n+    final String Q_A = Q_R + \".\" + A;\n+    final String Q_B = Q_R + \".\" + B;\n+    final String Q_B1 = Q_B + \".\" + B1;\n+    final String Q_B2 = Q_B + \".\" + B2;\n+    conf.setQueues(Q_R, new String[] {A, B});\n+    conf.setCapacity(Q_A, 50);\n+    conf.setCapacity(Q_B, 50);\n+    conf.setQueues(Q_B, new String[] {B1, B2});\n+    conf.setCapacity(Q_B1, 50);\n+    conf.setCapacity(Q_B2, 50);\n+    conf.setDouble(CapacitySchedulerConfiguration\n+        .MAXIMUM_APPLICATION_MASTERS_RESOURCE_PERCENT, 0.5f);\n+  }\n+\n   // Test CS recovery with multi-level queues and multi-users:\n   // 1. setup 2 NMs each with 8GB memory;\n   // 2. setup 2 level queues: Default -> (QueueA, QueueB)\n@@ -513,18 +542,106 @@ public void testCapacitySchedulerRecovery() throws Exception {\n         totalAvailableResource.getVirtualCores(), totalUsedResource.getMemory(),\n         totalUsedResource.getVirtualCores());\n   }\n-  \n-  //Test that we receive a meaningful exit-causing exception if a queue\n-  //is removed during recovery\n+\n+  private void verifyAppRecoveryWithWrongQueueConfig(\n+      CapacitySchedulerConfiguration csConf, RMApp app, String diagnostics,\n+      MemoryRMStateStore memStore, RMState state) throws Exception {\n+    // Restart RM with fail-fast as false. App should be killed.\n+    csConf.setBoolean(YarnConfiguration.RM_FAIL_FAST, false);\n+    rm2 = new MockRM(csConf, memStore);\n+    rm2.start();\n+    // Wait for app to be killed.\n+    rm2.waitForState(app.getApplicationId(), RMAppState.KILLED);\n+    ApplicationReport report = rm2.getApplicationReport(app.getApplicationId());\n+    assertEquals(report.getFinalApplicationStatus(),\n+        FinalApplicationStatus.KILLED);\n+    assertEquals(report.getYarnApplicationState(), YarnApplicationState.KILLED);\n+    assertEquals(report.getDiagnostics(), diagnostics);\n+\n+    // Remove updated app info(app being KILLED) from state store and reinstate\n+    // state store to previous state i.e. which indicates app is RUNNING.\n+    // This is to simulate app recovery with fail fast config as true.\n+    for(Map.Entry<ApplicationId, ApplicationStateData> entry :\n+        state.getApplicationState().entrySet()) {\n+      ApplicationStateData appState = mock(ApplicationStateData.class);\n+      ApplicationSubmissionContext ctxt =\n+          mock(ApplicationSubmissionContext.class);\n+      when(appState.getApplicationSubmissionContext()).thenReturn(ctxt);\n+      when(ctxt.getApplicationId()).thenReturn(entry.getKey());\n+      memStore.removeApplicationStateInternal(appState);\n+      memStore.storeApplicationStateInternal(\n+          entry.getKey(), entry.getValue());\n+    }\n+\n+    // Now restart RM with fail-fast as true. QueueException should be thrown.\n+    csConf.setBoolean(YarnConfiguration.RM_FAIL_FAST, true);\n+    MockRM rm = new MockRM(csConf, memStore);\n+    try {\n+      rm.start();\n+      Assert.fail(\"QueueException must have been thrown\");\n+    } catch (QueueInvalidException e) {\n+    } finally {\n+      rm.close();\n+    }\n+  }\n+\n+  //Test behavior of an app if queue is changed from leaf to parent during\n+  //recovery. Test case does following:\n+  //1. Add an app to QueueB and start the attempt.\n+  //2. Add 2 subqueues(QueueB1 and QueueB2) to QueueB, restart the RM, once with\n+  //   fail fast config as false and once with fail fast as true.\n+  //3. Verify that app was killed if fail fast is false.\n+  //4. Verify that QueueException was thrown if fail fast is true.\n+  @Test (timeout = 30000)\n+  public void testCapacityLeafQueueBecomesParentOnRecovery() throws Exception {\n+    if (getSchedulerType() != SchedulerType.CAPACITY) {\n+      return;\n+    }\n+    conf.setBoolean(CapacitySchedulerConfiguration.ENABLE_USER_METRICS, true);\n+    conf.set(CapacitySchedulerConfiguration.RESOURCE_CALCULATOR_CLASS,\n+        DominantResourceCalculator.class.getName());\n+    CapacitySchedulerConfiguration csConf =\n+        new CapacitySchedulerConfiguration(conf);\n+    setupQueueConfiguration(csConf);\n+    MemoryRMStateStore memStore = new MemoryRMStateStore();\n+    memStore.init(csConf);\n+    rm1 = new MockRM(csConf, memStore);\n+    rm1.start();\n+    MockNM nm =\n+        new MockNM(\"127.1.1.1:4321\", 8192, rm1.getResourceTrackerService());\n+    nm.registerNode();\n+\n+    // Submit an app to QueueB.\n+    RMApp app = rm1.submitApp(1024, \"app\", USER_2, null, B);\n+    MockRM.launchAndRegisterAM(app, rm1, nm);\n+    assertEquals(rm1.getApplicationReport(app.getApplicationId()).\n+        getYarnApplicationState(), YarnApplicationState.RUNNING);\n+\n+    // Take a copy of state store so that it can be reset to this state.\n+    RMState state = memStore.loadState();\n+\n+    // Change scheduler config with child queues added to QueueB.\n+    csConf = new CapacitySchedulerConfiguration(conf);\n+    setupQueueConfigurationChildOfB(csConf);\n+\n+    String diags = \"Application killed on recovery as it was submitted to \" +\n+        \"queue QueueB which is no longer a leaf queue after restart.\";\n+    verifyAppRecoveryWithWrongQueueConfig(csConf, app, diags, memStore, state);\n+  }\n+\n+  //Test behavior of an app if queue is removed during recovery. Test case does\n+  //following:\n   //1. Add some apps to two queues, attempt to add an app to a non-existant\n   //   queue to verify that the new logic is not in effect during normal app\n   //   submission\n-  //2. Remove one of the queues, restart the RM\n-  //3. Verify that the expected exception was thrown\n-  @Test (timeout = 30000, expected = QueueNotFoundException.class)\n+  //2. Remove one of the queues, restart the RM, once with fail fast config as\n+  //   false and once with fail fast as true.\n+  //3. Verify that app was killed if fail fast is false.\n+  //4. Verify that QueueException was thrown if fail fast is true.\n+  @Test (timeout = 30000)\n   public void testCapacitySchedulerQueueRemovedRecovery() throws Exception {\n     if (getSchedulerType() != SchedulerType.CAPACITY) {\n-      throw new QueueNotFoundException(\"Dummy\");\n+      return;\n     }\n     conf.setBoolean(CapacitySchedulerConfiguration.ENABLE_USER_METRICS, true);\n     conf.set(CapacitySchedulerConfiguration.RESOURCE_CALCULATOR_CLASS,\n@@ -549,7 +666,9 @@ public void testCapacitySchedulerQueueRemovedRecovery() throws Exception {\n \n     RMApp app2 = rm1.submitApp(1024, \"app2\", USER_2, null, B);\n     MockAM am2 = MockRM.launchAndRegisterAM(app2, rm1, nm2);\n-    \n+    assertEquals(rm1.getApplicationReport(app2.getApplicationId()).\n+        getYarnApplicationState(), YarnApplicationState.RUNNING);\n+\n     //Submit an app with a non existant queue to make sure it does not\n     //cause a fatal failure in the non-recovery case\n     RMApp appNA = rm1.submitApp(1024, \"app1_2\", USER_1, null,\n@@ -560,12 +679,16 @@ public void testCapacitySchedulerQueueRemovedRecovery() throws Exception {\n     rm1.clearQueueMetrics(app1_2);\n     rm1.clearQueueMetrics(app2);\n \n-    // Re-start RM\n-    csConf =\n-        new CapacitySchedulerConfiguration(conf);\n+    // Take a copy of state store so that it can be reset to this state.\n+    RMState state = memStore.loadState();\n+\n+    // Set new configuration with QueueB removed.\n+    csConf = new CapacitySchedulerConfiguration(conf);\n     setupQueueConfigurationOnlyA(csConf);\n-    rm2 = new MockRM(csConf, memStore);\n-    rm2.start();\n+\n+    String diags = \"Application killed on recovery as it was submitted to \" +\n+        \"queue QueueB which no longer exists after restart.\";\n+    verifyAppRecoveryWithWrongQueueConfig(csConf, app2, diags, memStore, state);\n   }\n \n   private void checkParentQueue(ParentQueue parentQueue, int numContainers,",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "sha": "479ee9344c5b6e62f84bdf8f68d9f4b3ec712f4f",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 35,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "patch": "@@ -473,8 +473,8 @@ protected RMApp testCreateAppFinished(\n       application = testCreateAppFinishing(submissionContext);\n     }\n     // RUNNING/FINISHING => FINISHED event RMAppEventType.ATTEMPT_FINISHED\n-    RMAppEvent finishedEvent = new RMAppFinishedAttemptEvent(\n-        application.getApplicationId(), diagnostics);\n+    RMAppEvent finishedEvent = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.ATTEMPT_FINISHED, diagnostics);\n     application.handle(finishedEvent);\n     assertAppState(RMAppState.FINISHED, application);\n     assertTimesAtFinish(application);\n@@ -548,8 +548,9 @@ public void testAppNewKill() throws IOException {\n \n     RMApp application = createNewTestApp(null);\n     // NEW => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -566,8 +567,8 @@ public void testAppNewReject() throws IOException {\n     RMApp application = createNewTestApp(null);\n     // NEW => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"Test Application Rejected\";\n-    RMAppEvent event = \n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -583,8 +584,8 @@ public void testAppNewRejectAddToStore() throws IOException {\n     RMApp application = createNewTestApp(null);\n     // NEW => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"Test Application Rejected\";\n-    RMAppEvent event =\n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -601,7 +602,8 @@ public void testAppNewSavingKill() throws IOException {\n     RMApp application = testCreateAppNewSaving(null);\n     // NEW_SAVING => KILLED event RMAppEventType.KILL\n     RMAppEvent event =\n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -617,8 +619,8 @@ public void testAppNewSavingReject() throws IOException {\n     RMApp application = testCreateAppNewSaving(null);\n     // NEW_SAVING => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"Test Application Rejected\";\n-    RMAppEvent event =\n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -634,8 +636,8 @@ public void testAppSubmittedRejected() throws IOException {\n     RMApp application = testCreateAppSubmittedNoRecovery(null);\n     // SUBMITTED => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"app rejected\";\n-    RMAppEvent event = \n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -649,8 +651,9 @@ public void testAppSubmittedKill() throws IOException, InterruptedException {\n     LOG.info(\"--- START: testAppSubmittedKill---\");\n     RMApp application = testCreateAppSubmittedNoRecovery(null);\n     // SUBMITTED => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n-        RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -700,15 +703,16 @@ public void testAppAcceptedKill() throws IOException, InterruptedException {\n     LOG.info(\"--- START: testAppAcceptedKill ---\");\n     RMApp application = testCreateAppAccepted(null);\n     // ACCEPTED => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n-        RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n \n     assertAppState(RMAppState.KILLING, application);\n     RMAppEvent appAttemptKilled =\n         new RMAppEvent(application.getApplicationId(),\n-          RMAppEventType.ATTEMPT_KILLED);\n+          RMAppEventType.ATTEMPT_KILLED, \"Application killed by user.\");\n     application.handle(appAttemptKilled);\n     assertAppState(RMAppState.FINAL_SAVING, application);\n     sendAppUpdateSavedEvent(application);\n@@ -729,7 +733,7 @@ public void testAppAcceptedAttemptKilled() throws IOException,\n     // RUNNING.\n     RMAppEvent event =\n         new RMAppEvent(application.getApplicationId(),\n-            RMAppEventType.ATTEMPT_KILLED);\n+            RMAppEventType.ATTEMPT_KILLED, \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n \n@@ -747,8 +751,9 @@ public void testAppRunningKill() throws IOException {\n \n     RMApp application = testCreateAppRunning(null);\n     // RUNNING => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n \n@@ -806,7 +811,9 @@ public void testAppRunningFailed() throws IOException {\n     assertAppFinalStateSaved(application);\n \n     // FAILED => FAILED event RMAppEventType.KILL\n-    event = new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertFailed(application, \".*Failing the application.*\");\n@@ -821,7 +828,8 @@ public void testAppAtFinishingIgnoreKill() throws IOException {\n     RMApp application = testCreateAppFinishing(null);\n     // FINISHING => FINISHED event RMAppEventType.KILL\n     RMAppEvent event =\n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertAppState(RMAppState.FINISHING, application);\n@@ -838,8 +846,8 @@ public void testAppFinalSavingToFinished() throws IOException {\n     RMApp application = testCreateAppFinalSaving(null);\n     final String diagMsg = \"some diagnostics\";\n     // attempt_finished event comes before attempt_saved event\n-    RMAppEvent event =\n-        new RMAppFinishedAttemptEvent(application.getApplicationId(), diagMsg);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.ATTEMPT_FINISHED, diagMsg);\n     application.handle(event);\n     assertAppState(RMAppState.FINAL_SAVING, application);\n     RMAppEvent appUpdated =\n@@ -860,8 +868,9 @@ public void testAppFinishedFinished() throws IOException {\n \n     RMApp application = testCreateAppFinished(null, \"\");\n     // FINISHED => FINISHED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);\n@@ -879,8 +888,8 @@ public void testAppFailedFailed() throws IOException {\n     RMApp application = testCreateAppNewSaving(null);\n \n     // NEW_SAVING => FAILED event RMAppEventType.APP_REJECTED\n-    RMAppEvent event =\n-        new RMAppRejectedEvent(application.getApplicationId(), \"\");\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, \"\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -889,7 +898,8 @@ public void testAppFailedFailed() throws IOException {\n \n     // FAILED => FAILED event RMAppEventType.KILL\n     event =\n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);\n@@ -907,8 +917,9 @@ public void testAppKilledKilled() throws IOException {\n     RMApp application = testCreateAppRunning(null);\n \n     // RUNNING => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAttemptUpdateSavedEvent(application);\n@@ -917,8 +928,8 @@ public void testAppKilledKilled() throws IOException {\n     assertAppState(RMAppState.KILLED, application);\n \n     // KILLED => KILLED event RMAppEventType.ATTEMPT_FINISHED\n-    event = new RMAppFinishedAttemptEvent(\n-        application.getApplicationId(), \"\");\n+    event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.ATTEMPT_FINISHED, \"\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);\n@@ -935,7 +946,9 @@ public void testAppKilledKilled() throws IOException {\n \n \n     // KILLED => KILLED event RMAppEventType.KILL\n-    event = new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "sha": "91388dbc8096c407e7bad0e60afbaece5c7b3e76",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 27,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "patch": "@@ -26,6 +26,7 @@\n import static org.junit.Assume.assumeTrue;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Matchers.argThat;\n import static org.mockito.Matchers.eq;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.never;\n@@ -86,12 +87,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerAllocatedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptFailedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptRegistrationEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptUnregistrationEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer;\n@@ -124,6 +121,7 @@\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n import org.mockito.ArgumentCaptor;\n+import org.mockito.ArgumentMatcher;\n import org.mockito.Matchers;\n import org.mockito.Mockito;\n import org.mockito.invocation.InvocationOnMock;\n@@ -416,10 +414,16 @@ private void testAppAttemptSubmittedToFailedState(String diagnostics) {\n     // Check events\n     verify(masterService).\n         unregisterAttempt(applicationAttempt.getAppAttemptId());\n-    \n-    // this works for unmanaged and managed AM's because this is actually doing\n-    // verify(application).handle(anyObject());\n-    verify(application).handle(any(RMAppRejectedEvent.class));\n+    // ATTEMPT_FAILED should be notified to app if app attempt is submitted to\n+    // failed state.\n+    ArgumentMatcher<RMAppEvent> matcher = new ArgumentMatcher<RMAppEvent>() {\n+      @Override\n+      public boolean matches(Object o) {\n+        RMAppEvent event = (RMAppEvent) o;\n+        return event.getType() == RMAppEventType.ATTEMPT_FAILED;\n+      }\n+    };\n+    verify(application).handle(argThat(matcher));\n     verifyTokenCount(applicationAttempt.getAppAttemptId(), 1);\n     verifyApplicationAttemptFinished(RMAppAttemptState.FAILED);\n   }\n@@ -649,8 +653,8 @@ private Container allocateApplicationAttempt() {\n         thenReturn(rmContainer);\n     \n     applicationAttempt.handle(\n-        new RMAppAttemptContainerAllocatedEvent(\n-            applicationAttempt.getAppAttemptId()));\n+        new RMAppAttemptEvent(applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.CONTAINER_ALLOCATED));\n     \n     assertEquals(RMAppAttemptState.ALLOCATED_SAVING, \n         applicationAttempt.getAppAttemptState());\n@@ -906,9 +910,8 @@ public void testAllocatedToFailed() {\n     Container amContainer = allocateApplicationAttempt();\n     String diagnostics = \"Launch Failed\";\n     applicationAttempt.handle(\n-        new RMAppAttemptLaunchFailedEvent(\n-            applicationAttempt.getAppAttemptId(), \n-            diagnostics));\n+        new RMAppAttemptEvent(applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.LAUNCH_FAILED, diagnostics));\n     assertEquals(YarnApplicationAttemptState.ALLOCATED,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(amContainer, diagnostics);\n@@ -927,8 +930,9 @@ public void testLaunchedAtFinalSaving() {\n     // verify for both launched and launch_failed transitions in final_saving\n     applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n         .getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));\n-    applicationAttempt.handle(new RMAppAttemptLaunchFailedEvent(\n-        applicationAttempt.getAppAttemptId(), \"Launch Failed\"));\n+    applicationAttempt.handle(\n+        new RMAppAttemptEvent(applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.LAUNCH_FAILED, \"Launch Failed\"));\n \n     assertEquals(RMAppAttemptState.FINAL_SAVING,\n         applicationAttempt.getAppAttemptState());\n@@ -938,8 +942,9 @@ public void testLaunchedAtFinalSaving() {\n     // verify for both launched and launch_failed transitions in killed\n     applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n         .getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));\n-    applicationAttempt.handle(new RMAppAttemptLaunchFailedEvent(\n-        applicationAttempt.getAppAttemptId(), \"Launch Failed\"));\n+    applicationAttempt.handle(new RMAppAttemptEvent(\n+        applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.LAUNCH_FAILED, \"Launch Failed\"));\n     assertEquals(RMAppAttemptState.KILLED,\n         applicationAttempt.getAppAttemptState());\n   }\n@@ -1546,8 +1551,8 @@ public Allocation answer(InvocationOnMock invocation)\n \n   @Test(timeout = 30000)\n   public void testNewToFailed() {\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(YarnApplicationAttemptState.NEW,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(null, FAILED_DIAGNOSTICS);\n@@ -1557,8 +1562,8 @@ public void testNewToFailed() {\n   @Test(timeout = 30000)\n   public void testSubmittedToFailed() {\n     submitApplicationAttempt();\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(YarnApplicationAttemptState.SUBMITTED,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(null, FAILED_DIAGNOSTICS);\n@@ -1567,8 +1572,8 @@ public void testSubmittedToFailed() {\n   @Test(timeout = 30000)\n   public void testScheduledToFailed() {\n     scheduleApplicationAttempt();\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(YarnApplicationAttemptState.SCHEDULED,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(null, FAILED_DIAGNOSTICS);\n@@ -1579,8 +1584,8 @@ public void testAllocatedToFailedUserTriggeredFailEvent() {\n     Container amContainer = allocateApplicationAttempt();\n     assertEquals(YarnApplicationAttemptState.ALLOCATED,\n         applicationAttempt.createApplicationAttemptState());\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     testAppAttemptFailedState(amContainer, FAILED_DIAGNOSTICS);\n   }\n \n@@ -1589,8 +1594,8 @@ public void testRunningToFailedUserTriggeredFailEvent() {\n     Container amContainer = allocateApplicationAttempt();\n     launchApplicationAttempt(amContainer);\n     runApplicationAttempt(amContainer, \"host\", 8042, \"oldtrackingurl\", false);\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(RMAppAttemptState.FINAL_SAVING,\n         applicationAttempt.getAppAttemptState());\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "sha": "7f9610faa5313a9f8dc24aed49562f7231874e9f",
                "status": "modified"
            }
        ],
        "message": "YARN-4000. RM crashes with NPE if leaf queue becomes parent queue during restart. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
        "patched_files": [
            "AbstractYarnScheduler.java",
            "RMAppFailedAttemptEvent.java",
            "RMAppAttemptUnregistrationEvent.java",
            "RMAppManager.java",
            "ClientRMService.java",
            "DelegationTokenRenewer.java",
            "RMAppEvent.java",
            "RMAppAttemptContainerAllocatedEvent.java",
            "FairScheduler.java",
            "QueueInvalidException.java",
            "RMAppRejectedEvent.java",
            "RMAppFinishedAttemptEvent.java",
            "MockRM.java",
            "RMAppAttemptLaunchFailedEvent.java",
            "CHANGES.java",
            "RMAppAttemptEvent.java",
            "AMLauncher.java",
            "RMAppAttemptImpl.java",
            "RMContainerImpl.java",
            "CapacityScheduler.java",
            "RMAppImpl.java",
            "RMAppAttemptFailedEvent.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAbstractYarnScheduler.java",
            "TestRMContainerImpl.java",
            "TestRMAppAttemptTransitions.java",
            "TestClientRMService.java",
            "TestDelegationTokenRenewer.java",
            "TestRMAppTransitions.java",
            "TestCapacityScheduler.java",
            "TestWorkPreservingRMRestart.java",
            "TestFairScheduler.java"
        ]
    },
    "hadoop_cfb0186": {
        "bug_id": "hadoop_cfb0186",
        "commit": "https://github.com/apache/hadoop/commit/cfb01869038065defe50ab53d4d1eda4e6cdee33",
        "file": [
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/cfb01869038065defe50ab53d4d1eda4e6cdee33/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java?ref=cfb01869038065defe50ab53d4d1eda4e6cdee33",
                "deletions": 11,
                "filename": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
                "patch": "@@ -638,22 +638,42 @@ public DirListingMetadata listChildren(final Path path) throws IOException {\n             metas.add(meta);\n           }\n \n+          // Minor race condition here - if the path is deleted between\n+          // getting the list of items and the directory metadata we might\n+          // get a null in DDBPathMetadata.\n           DDBPathMetadata dirPathMeta = get(path);\n-          boolean isAuthoritative = false;\n-          if(dirPathMeta != null) {\n-            isAuthoritative = dirPathMeta.isAuthoritativeDir();\n-          }\n-\n-          LOG.trace(\"Listing table {} in region {} for {} returning {}\",\n-              tableName, region, path, metas);\n \n-          return (metas.isEmpty() && dirPathMeta == null)\n-              ? null\n-              : new DirListingMetadata(path, metas, isAuthoritative,\n-              dirPathMeta.getLastUpdated());\n+          return getDirListingMetadataFromDirMetaAndList(path, metas,\n+              dirPathMeta);\n         });\n   }\n \n+  DirListingMetadata getDirListingMetadataFromDirMetaAndList(Path path,\n+      List<PathMetadata> metas, DDBPathMetadata dirPathMeta) {\n+    boolean isAuthoritative = false;\n+    if (dirPathMeta != null) {\n+      isAuthoritative = dirPathMeta.isAuthoritativeDir();\n+    }\n+\n+    LOG.trace(\"Listing table {} in region {} for {} returning {}\",\n+        tableName, region, path, metas);\n+\n+    if (!metas.isEmpty() && dirPathMeta == null) {\n+      // We handle this case as the directory is deleted.\n+      LOG.warn(\"Directory marker is deleted, but the list of the directory \"\n+          + \"elements is not empty: {}. This case is handled as if the \"\n+          + \"directory was deleted.\", metas);\n+      return null;\n+    }\n+\n+    if(metas.isEmpty() && dirPathMeta == null) {\n+      return null;\n+    }\n+\n+    return new DirListingMetadata(path, metas, isAuthoritative,\n+        dirPathMeta.getLastUpdated());\n+  }\n+\n   /**\n    * build the list of all parent entries.\n    * @param pathsToCreate paths to create",
                "raw_url": "https://github.com/apache/hadoop/raw/cfb01869038065defe50ab53d4d1eda4e6cdee33/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/s3guard/DynamoDBMetadataStore.java",
                "sha": "590b9c49021ff58661347e59a87a96abce16622f",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/cfb01869038065defe50ab53d4d1eda4e6cdee33/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDynamoDBMiscOperations.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDynamoDBMiscOperations.java?ref=cfb01869038065defe50ab53d4d1eda4e6cdee33",
                "deletions": 0,
                "filename": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDynamoDBMiscOperations.java",
                "patch": "@@ -20,13 +20,18 @@\n \n import java.io.FileNotFoundException;\n import java.io.IOException;\n+import java.util.List;\n \n import com.amazonaws.services.dynamodbv2.model.ResourceNotFoundException;\n import com.amazonaws.waiters.WaiterTimedOutException;\n import org.junit.Test;\n \n import org.apache.hadoop.fs.s3a.AWSClientIOException;\n import org.apache.hadoop.test.HadoopTestBase;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n import static org.apache.hadoop.fs.s3a.s3guard.DynamoDBMetadataStore.translateTableWaitFailure;\n import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n@@ -91,4 +96,17 @@ public void testTranslateWrappedOtherException() throws Throwable {\n     assertEquals(e, ex.getCause());\n   }\n \n+  @Test\n+  public void testInnerListChildrenDirectoryNpe() throws Exception {\n+    DynamoDBMetadataStore ddbms = new DynamoDBMetadataStore();\n+    Path p = mock(Path.class);\n+    List<PathMetadata> metas = mock(List.class);\n+\n+    when(metas.isEmpty()).thenReturn(false);\n+    DDBPathMetadata dirPathMeta = null;\n+\n+    assertNull(\"The return value should be null.\",\n+        ddbms.getDirListingMetadataFromDirMetaAndList(p, metas, dirPathMeta));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cfb01869038065defe50ab53d4d1eda4e6cdee33/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/s3guard/TestDynamoDBMiscOperations.java",
                "sha": "fc0f8940de0f6ad2c377b4d60af851a879f78900",
                "status": "modified"
            }
        ],
        "message": "HADOOP-16186. S3Guard: NPE in DynamoDBMetadataStore.lambda$listChildren.\n\nAuthor:    Gabor Bota",
        "parent": "https://github.com/apache/hadoop/commit/15d38b1bf9fbd41658f6980c1a484dd28f746654",
        "patched_files": [
            "DynamoDBMetadataStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDynamoDBMiscOperations.java"
        ]
    },
    "hadoop_cfe89e6": {
        "bug_id": "hadoop_cfe89e6",
        "commit": "https://github.com/apache/hadoop/commit/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java",
                "patch": "@@ -348,9 +348,11 @@ private void handleNewContainers(List<Container> allocContainers,\n       RMContainer rmContainer =\n           SchedulerUtils.createOpportunisticRmContainer(\n               rmContext, container, isRemotelyAllocated);\n-      rmContainer.handle(\n-          new RMContainerEvent(container.getId(),\n-              RMContainerEventType.ACQUIRED));\n+      if (rmContainer!=null) {\n+        rmContainer.handle(\n+            new RMContainerEvent(container.getId(),\n+                RMContainerEventType.ACQUIRED));\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java",
                "sha": "69a704ee96ddb9f0b7be526e172bb18687d4411b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "patch": "@@ -693,8 +693,10 @@ public void completedContainer(RMContainer rmContainer,\n         LOG.debug(\"Completed container: \" + rmContainer.getContainerId() +\n             \" in state: \" + rmContainer.getState() + \" event:\" + event);\n       }\n-      getSchedulerNode(rmContainer.getNodeId()).releaseContainer(\n-          rmContainer.getContainerId(), false);\n+      SchedulerNode node = getSchedulerNode(rmContainer.getNodeId());\n+      if (node != null) {\n+        node.releaseContainer(rmContainer.getContainerId(), false);\n+      }\n     }\n \n     // If the container is getting killed in ACQUIRED state, the requester (AM\n@@ -1300,8 +1302,10 @@ private void handleDecreaseRequests(SchedulerApplicationAttempt appAttempt,\n               uReq.getContainerUpdateType()) {\n             RMContainer demotedRMContainer =\n                 createDemotedRMContainer(appAttempt, oppCntxt, rmContainer);\n-            appAttempt.addToNewlyDemotedContainers(\n-                uReq.getContainerId(), demotedRMContainer);\n+            if (demotedRMContainer != null) {\n+              appAttempt.addToNewlyDemotedContainers(\n+                      uReq.getContainerId(), demotedRMContainer);\n+            }\n           } else {\n             RMContainer demotedRMContainer = createDecreasedRMContainer(\n                 appAttempt, uReq, rmContainer);",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "sha": "0ad47bb8f909f9e39ef783a7ba6a6460d56c3946",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java",
                "patch": "@@ -564,6 +564,11 @@ public static boolean hasPendingResourceRequest(ResourceCalculator rc,\n \n   public static RMContainer createOpportunisticRmContainer(RMContext rmContext,\n       Container container, boolean isRemotelyAllocated) {\n+    SchedulerNode node = ((AbstractYarnScheduler) rmContext.getScheduler())\n+        .getNode(container.getNodeId());\n+    if (node == null) {\n+      return null;\n+    }\n     SchedulerApplicationAttempt appAttempt =\n         ((AbstractYarnScheduler) rmContext.getScheduler())\n             .getCurrentAttemptForContainer(container.getId());\n@@ -572,8 +577,7 @@ public static RMContainer createOpportunisticRmContainer(RMContext rmContext,\n         appAttempt.getApplicationAttemptId(), container.getNodeId(),\n         appAttempt.getUser(), rmContext, isRemotelyAllocated);\n     appAttempt.addRMContainer(container.getId(), rmContainer);\n-    ((AbstractYarnScheduler) rmContext.getScheduler()).getNode(\n-        container.getNodeId()).allocateContainer(rmContainer);\n+    node.allocateContainer(rmContainer);\n     return rmContainer;\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java",
                "sha": "a048dacb688afa39d96a937ff139eaea414f3ee5",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.ipc.RPC;\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB;\n import org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl;\n import org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl;\n@@ -72,14 +73,19 @@\n import org.apache.hadoop.yarn.server.api.records.OpportunisticContainersStatus;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeAddedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeRemovedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent;\n@@ -88,19 +94,25 @@\n     .FifoScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n import org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext;\n+import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;\n import org.apache.hadoop.yarn.util.resource.Resources;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n+import com.google.common.base.Supplier;\n+\n+import static org.junit.Assert.fail;\n+\n import java.io.IOException;\n import java.net.InetSocketAddress;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n+import java.util.concurrent.TimeoutException;\n \n /**\n  * Test cases for {@link OpportunisticContainerAllocatorAMService}.\n@@ -798,6 +810,72 @@ public void testNodeRemovalDuringAllocate() throws Exception {\n     Assert.assertEquals(1, ctxt.getNodeMap().size());\n   }\n \n+  @Test(timeout = 60000)\n+  public void testAppAttemptRemovalAfterNodeRemoval() throws Exception {\n+    MockNM nm = new MockNM(\"h:1234\", 4096, rm.getResourceTrackerService());\n+    nm.registerNode();\n+    OpportunisticContainerAllocatorAMService amservice =\n+        (OpportunisticContainerAllocatorAMService) rm\n+            .getApplicationMasterService();\n+    RMApp app = rm.submitApp(1 * GB, \"app\", \"user\", null, \"default\");\n+    ApplicationAttemptId attemptId =\n+        app.getCurrentAppAttempt().getAppAttemptId();\n+    MockAM am = MockRM.launchAndRegisterAM(app, rm, nm);\n+    ResourceScheduler scheduler = rm.getResourceScheduler();\n+    SchedulerApplicationAttempt schedulerAttempt =\n+        ((CapacityScheduler)scheduler).getApplicationAttempt(attemptId);\n+    RMNode rmNode1 = rm.getRMContext().getRMNodes().get(nm.getNodeId());\n+    nm.nodeHeartbeat(true);\n+    ((RMNodeImpl) rmNode1)\n+        .setOpportunisticContainersStatus(getOppurtunisticStatus(-1, 100));\n+    // Send add and update node events to AM Service.\n+    amservice.handle(new NodeAddedSchedulerEvent(rmNode1));\n+    amservice.handle(new NodeUpdateSchedulerEvent(rmNode1));\n+    try {\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override public Boolean get() {\n+          return scheduler.getNumClusterNodes() == 1;\n+        }\n+      }, 10, 200 * 100);\n+    }catch (TimeoutException e) {\n+      fail(\"timed out while waiting for NM to add.\");\n+    }\n+    AllocateResponse allocateResponse = am.allocate(\n+            Arrays.asList(ResourceRequest.newInstance(Priority.newInstance(1),\n+                \"*\", Resources.createResource(1 * GB), 2, true, null,\n+                ExecutionTypeRequest.newInstance(\n+                    ExecutionType.OPPORTUNISTIC, true))),\n+                null);\n+    List<Container> allocatedContainers = allocateResponse\n+        .getAllocatedContainers();\n+    Container container = allocatedContainers.get(0);\n+    scheduler.handle(new NodeRemovedSchedulerEvent(rmNode1));\n+    try {\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override public Boolean get() {\n+          return scheduler.getNumClusterNodes() == 0;\n+        }\n+      }, 10, 200 * 100);\n+    }catch (TimeoutException e) {\n+      fail(\"timed out while waiting for NM to remove.\");\n+    }\n+    //test YARN-9165\n+    RMContainer rmContainer = null;\n+    rmContainer = SchedulerUtils.createOpportunisticRmContainer(\n+                    rm.getRMContext(), container, true);\n+    if (rmContainer == null) {\n+      rmContainer = new RMContainerImpl(container,\n+        SchedulerRequestKey.extractFrom(container),\n+        schedulerAttempt.getApplicationAttemptId(), container.getNodeId(),\n+        schedulerAttempt.getUser(), rm.getRMContext(), true);\n+    }\n+    assert(rmContainer!=null);\n+    //test YARN-9164\n+    schedulerAttempt.addRMContainer(container.getId(), rmContainer);\n+    scheduler.handle(new AppAttemptRemovedSchedulerEvent(attemptId,\n+        RMAppAttemptState.FAILED, false));\n+  }\n+\n   private OpportunisticContainersStatus getOppurtunisticStatus(int waitTime,\n       int queueLength) {\n     OpportunisticContainersStatus status1 =",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java",
                "sha": "b2be1e732ba8081583febd5f70c1ee3457caec60",
                "status": "modified"
            }
        ],
        "message": "YARN-9164. Shutdown NM may cause NPE when opportunistic container scheduling is enabled. Contributed by lujie.",
        "parent": "https://github.com/apache/hadoop/commit/040a202b202a37f3b922cd321eb0a8ded457d88b",
        "patched_files": [
            "AbstractYarnScheduler.java",
            "OpportunisticContainerAllocatorAMService.java",
            "SchedulerUtils.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAbstractYarnScheduler.java",
            "TestOpportunisticContainerAllocatorAMService.java",
            "TestSchedulerUtils.java"
        ]
    },
    "hadoop_d19d187": {
        "bug_id": "hadoop_d19d187",
        "commit": "https://github.com/apache/hadoop/commit/d19d18775368f5aaa254881165acc1299837072b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=d19d18775368f5aaa254881165acc1299837072b",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -672,6 +672,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-3845. Scheduler page does not render RGBA color combinations in IE11. \n     (Contributed by Mohammad Shahid Khan)\n \n+    YARN-3957. FairScheduler NPE In FairSchedulerQueueInfo causing scheduler page to \n+    return 500. (Anubhav Dhoot via kasha)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/CHANGES.txt",
                "sha": "44e55100eb90c135d07ec95872f796cab8f56a69",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java?ref=d19d18775368f5aaa254881165acc1299837072b",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.server.resourcemanager.webapp.dao;\n \n \n+import java.util.ArrayList;\n import java.util.Collection;\n \n import javax.xml.bind.annotation.XmlAccessType;\n@@ -204,6 +205,7 @@ public String getSchedulingPolicy() {\n   }\n \n   public Collection<FairSchedulerQueueInfo> getChildQueues() {\n-    return childQueues.getQueueInfoList();\n+    return childQueues != null ? childQueues.getQueueInfoList() :\n+        new ArrayList<FairSchedulerQueueInfo>();\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java",
                "sha": "7ba0988be80e91e5ac9205e474d5eb7cbbaaeb07",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop/blob/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java?ref=d19d18775368f5aaa254881165acc1299837072b",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java",
                "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager.webapp.dao;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager;\n+import org.apache.hadoop.yarn.util.SystemClock;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Collection;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class TestFairSchedulerQueueInfo {\n+\n+  @Test\n+  public void testEmptyChildQueues() throws Exception {\n+    FairSchedulerConfiguration conf = new FairSchedulerConfiguration();\n+    FairScheduler scheduler = mock(FairScheduler.class);\n+    AllocationConfiguration allocConf = new AllocationConfiguration(conf);\n+    when(scheduler.getAllocationConfiguration()).thenReturn(allocConf);\n+    when(scheduler.getConf()).thenReturn(conf);\n+    when(scheduler.getClusterResource()).thenReturn(Resource.newInstance(1, 1));\n+    SystemClock clock = new SystemClock();\n+    when(scheduler.getClock()).thenReturn(clock);\n+    QueueManager queueManager = new QueueManager(scheduler);\n+    queueManager.initialize(conf);\n+\n+    FSQueue testQueue = queueManager.getLeafQueue(\"test\", true);\n+    FairSchedulerQueueInfo queueInfo =\n+        new FairSchedulerQueueInfo(testQueue, scheduler);\n+    Collection<FairSchedulerQueueInfo> childQueues =\n+        queueInfo.getChildQueues();\n+    Assert.assertNotNull(childQueues);\n+    Assert.assertEquals(\"Child QueueInfo was not empty\", 0, childQueues.size());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java",
                "sha": "973afcfb99f7a37e0cc41bf9f601b28eb2015f4f",
                "status": "added"
            }
        ],
        "message": "YARN-3957. FairScheduler NPE In FairSchedulerQueueInfo causing scheduler page to return 500. (Anubhav Dhoot via kasha)",
        "parent": "https://github.com/apache/hadoop/commit/f8f60918230dd466ae8dda1fbc28878e19273232",
        "patched_files": [
            "CHANGES.java",
            "FairSchedulerQueueInfo.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFairSchedulerQueueInfo.java"
        ]
    },
    "hadoop_d215357": {
        "bug_id": "hadoop_d215357",
        "commit": "https://github.com/apache/hadoop/commit/d2153577181f900ee6d8bf67d254e408bbaad243",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java?ref=d2153577181f900ee6d8bf67d254e408bbaad243",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.primitives.Bytes;\n \n import org.apache.commons.codec.binary.Base64;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n@@ -358,6 +359,10 @@ private static String encodeWritable(Writable obj) throws IOException {\n    */\n   private static void decodeWritable(Writable obj,\n                                      String newValue) throws IOException {\n+    if (newValue == null) {\n+      throw new HadoopIllegalArgumentException(\n+              \"Invalid argument, newValue is null\");\n+    }\n     Base64 decoder = new Base64(0, null, true);\n     DataInputBuffer buf = new DataInputBuffer();\n     byte[] decoded = decoder.decode(newValue);",
                "raw_url": "https://github.com/apache/hadoop/raw/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "sha": "25aac8853ef8f1231f73703e457160c2accff2bb",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java?ref=d2153577181f900ee6d8bf67d254e408bbaad243",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java",
                "patch": "@@ -21,6 +21,7 @@\n import java.io.*;\n import java.util.Arrays;\n \n+import org.apache.hadoop.HadoopIllegalArgumentException;\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier;\n import org.apache.hadoop.security.token.delegation.TestDelegationToken.TestDelegationTokenIdentifier;\n@@ -100,6 +101,23 @@ public void testEncodeWritable() throws Exception {\n     }\n   }\n \n+  /*\n+   * Test decodeWritable() with null newValue string argument,\n+   * should throw HadoopIllegalArgumentException.\n+   */\n+  @Test\n+  public void testDecodeWritableArgSanityCheck() throws Exception {\n+    Token<AbstractDelegationTokenIdentifier> token =\n+            new Token<AbstractDelegationTokenIdentifier>();\n+    try {\n+      token.decodeFromUrlString(null);\n+      fail(\"Should have thrown HadoopIllegalArgumentException\");\n+    }\n+    catch (HadoopIllegalArgumentException e) {\n+      Token.LOG.info(\"Test decodeWritable() sanity check success.\");\n+    }\n+  }\n+\n   @Test\n   public void testDecodeIdentifier() throws IOException {\n     TestDelegationTokenSecretManager secretManager =",
                "raw_url": "https://github.com/apache/hadoop/raw/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java",
                "sha": "3a3567ce2a4314efb774fdbf7c0a56e71f583463",
                "status": "modified"
            }
        ],
        "message": "HDFS-13485. DataNode WebHDFS endpoint throws NPE. Contributed by Siyao Meng.",
        "parent": "https://github.com/apache/hadoop/commit/121865c3f96166e2190ed54b433ebcf8d053b91c",
        "patched_files": [
            "Token.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestToken.java"
        ]
    },
    "hadoop_d310c48": {
        "bug_id": "hadoop_d310c48",
        "commit": "https://github.com/apache/hadoop/commit/d310c48ce4dc3fc24506455ed5addf1d24f441ee",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=d310c48ce4dc3fc24506455ed5addf1d24f441ee",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -52,6 +52,8 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-4083. [Gridmix] NPE in cpu emulation. (amarrk)\n+\n     MAPREDUCE-4087. [Gridmix] GenerateDistCacheData job of Gridmix can\n                     become slow in some cases (ravigummadi).\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "b8921ae2d5d3ac7c334122c89d4551582c946bd7",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java?ref=d310c48ce4dc3fc24506455ed5addf1d24f441ee",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java",
                "patch": "@@ -235,7 +235,9 @@ private synchronized long getCurrentCPUUsage() {\n   \n   @Override\n   public float getProgress() {\n-    return Math.min(1f, ((float)getCurrentCPUUsage())/targetCpuUsage);\n+    return enabled \n+           ? Math.min(1f, ((float)getCurrentCPUUsage())/targetCpuUsage)\n+           : 1.0f;\n   }\n   \n   @Override\n@@ -297,6 +299,9 @@ public void emulate() throws IOException, InterruptedException {\n   public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n                          ResourceCalculatorPlugin monitor,\n                          Progressive progress) {\n+    this.monitor = monitor;\n+    this.progress = progress;\n+    \n     // get the target CPU usage\n     targetCpuUsage = metrics.getCumulativeCpuUsage();\n     if (targetCpuUsage <= 0 ) {\n@@ -306,8 +311,6 @@ public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n       enabled = true;\n     }\n     \n-    this.monitor = monitor;\n-    this.progress = progress;\n     emulationInterval =  conf.getFloat(CPU_EMULATION_PROGRESS_INTERVAL, \n                                        DEFAULT_EMULATION_FREQUENCY);\n     ",
                "raw_url": "https://github.com/apache/hadoop/raw/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java",
                "sha": "c2b2a018ff365aee5c323f033640508b8be8b26d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java?ref=d310c48ce4dc3fc24506455ed5addf1d24f441ee",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java",
                "patch": "@@ -188,7 +188,9 @@ protected long getMaxHeapUsageInMB() {\n   \n   @Override\n   public float getProgress() {\n-    return Math.min(1f, ((float)getTotalHeapUsageInMB())/targetHeapUsageInMB);\n+    return enabled \n+           ? Math.min(1f, ((float)getTotalHeapUsageInMB())/targetHeapUsageInMB)\n+           : 1.0f;\n   }\n   \n   @Override\n@@ -237,6 +239,8 @@ public void emulate() throws IOException, InterruptedException {\n   public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n                          ResourceCalculatorPlugin monitor,\n                          Progressive progress) {\n+    this.progress = progress;\n+    \n     // get the target heap usage\n     targetHeapUsageInMB = metrics.getHeapUsage() / ONE_MB;\n     if (targetHeapUsageInMB <= 0 ) {\n@@ -248,7 +252,6 @@ public void initialize(Configuration conf, ResourceUsageMetrics metrics,\n       enabled = true;\n     }\n     \n-    this.progress = progress;\n     emulationInterval = \n       conf.getFloat(HEAP_EMULATION_PROGRESS_INTERVAL, \n                     DEFAULT_EMULATION_PROGRESS_INTERVAL);",
                "raw_url": "https://github.com/apache/hadoop/raw/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java",
                "sha": "47941ccfffb908da85dd655799ac1752addc161b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java?ref=d310c48ce4dc3fc24506455ed5addf1d24f441ee",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java",
                "patch": "@@ -171,6 +171,11 @@ public void testTotalHeapUsageEmulatorPlugin() throws Exception {\n     assertEquals(\"Disabled heap usage emulation plugin works!\", \n                  heapUsagePre, heapUsagePost);\n     \n+    // test with get progress\n+    float progress = heapPlugin.getProgress();\n+    assertEquals(\"Invalid progress of disabled cumulative heap usage emulation \"\n+                 + \"plugin!\", 1.0f, progress, 0f);\n+    \n     // test with wrong/invalid configuration\n     Boolean failed = null;\n     invalidUsage = ",
                "raw_url": "https://github.com/apache/hadoop/raw/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java",
                "sha": "486165d9efa2d2c808e31569243dd7d773323991",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java?ref=d310c48ce4dc3fc24506455ed5addf1d24f441ee",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;\n import org.apache.hadoop.mapreduce.task.MapContextImpl;\n import org.apache.hadoop.mapreduce.util.ResourceCalculatorPlugin;\n-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.ProcResourceValues;\n import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;\n import org.apache.hadoop.mapred.DummyResourceCalculatorPlugin;\n import org.apache.hadoop.mapred.gridmix.LoadJob.ResourceUsageMatcherRunner;\n@@ -484,6 +483,11 @@ public void testCumulativeCpuUsageEmulatorPlugin() throws Exception {\n     assertEquals(\"Disabled cumulative CPU usage emulation plugin works!\", \n                  cpuUsagePre, cpuUsagePost);\n     \n+    // test with get progress\n+    float progress = cpuPlugin.getProgress();\n+    assertEquals(\"Invalid progress of disabled cumulative CPU usage emulation \" \n+                 + \"plugin!\", 1.0f, progress, 0f);\n+    \n     // test with valid resource usage value\n     ResourceUsageMetrics metrics = createMetrics(targetCpuUsage);\n     ",
                "raw_url": "https://github.com/apache/hadoop/raw/d310c48ce4dc3fc24506455ed5addf1d24f441ee/hadoop-mapreduce-project/src/contrib/gridmix/src/test/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java",
                "sha": "9874be3229e34651a3f5914335b8d82c9f46313f",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4083. [Gridmix] NPE in cpu emulation. (amarrk)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1325145 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/4ea042666c4d7997bd5fac893a27152dddfbd957",
        "patched_files": [
            "CumulativeCpuUsageEmulatorPlugin.java",
            "CHANGES.java",
            "TotalHeapUsageEmulatorPlugin.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceUsageEmulators.java",
            "TestGridmixMemoryEmulation.java"
        ]
    },
    "hadoop_d31c9d8": {
        "bug_id": "hadoop_d31c9d8",
        "commit": "https://github.com/apache/hadoop/commit/d31c9d8c495794a803fb20729b5ed6b374e23eb4",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java?ref=d31c9d8c495794a803fb20729b5ed6b374e23eb4",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -1253,7 +1253,10 @@ void fixKerberosTicketOrder() {\n         Object cred = iter.next();\n         if (cred instanceof KerberosTicket) {\n           KerberosTicket ticket = (KerberosTicket) cred;\n-          if (!ticket.getServer().getName().startsWith(\"krbtgt\")) {\n+          if (ticket.isDestroyed() || ticket.getServer() == null) {\n+            LOG.warn(\"Ticket is already destroyed, remove it.\");\n+            iter.remove();\n+          } else if (!ticket.getServer().getName().startsWith(\"krbtgt\")) {\n             LOG.warn(\n                 \"The first kerberos ticket is not TGT\"\n                     + \"(the server principal is {}), remove and destroy it.\",",
                "raw_url": "https://github.com/apache/hadoop/raw/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "726e81111d1d9ef2d44a0aa2848f711f43012a19",
                "status": "modified"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/hadoop/blob/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java?ref=d31c9d8c495794a803fb20729b5ed6b374e23eb4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java",
                "patch": "@@ -155,4 +155,81 @@ public Void run() throws Exception {\n             .filter(t -> t.getServer().getName().startsWith(server2Protocol))\n             .findAny().isPresent());\n   }\n+\n+  @Test\n+  public void testWithDestroyedTGT() throws Exception {\n+    UserGroupInformation ugi =\n+        UserGroupInformation.loginUserFromKeytabAndReturnUGI(clientPrincipal,\n+            keytabFile.getCanonicalPath());\n+    ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+\n+      @Override\n+      public Void run() throws Exception {\n+        SaslClient client = Sasl.createSaslClient(\n+            new String[] {AuthMethod.KERBEROS.getMechanismName()},\n+            clientPrincipal, server1Protocol, host, props, null);\n+        client.evaluateChallenge(new byte[0]);\n+        client.dispose();\n+        return null;\n+      }\n+    });\n+\n+    Subject subject = ugi.getSubject();\n+\n+    // mark the ticket as destroyed\n+    for (KerberosTicket ticket : subject\n+        .getPrivateCredentials(KerberosTicket.class)) {\n+      if (ticket.getServer().getName().startsWith(\"krbtgt\")) {\n+        ticket.destroy();\n+        break;\n+      }\n+    }\n+\n+    ugi.fixKerberosTicketOrder();\n+\n+    // verify that after fixing, the tgt ticket should be removed\n+    assertFalse(\"The first ticket is not tgt\",\n+        subject.getPrivateCredentials().stream()\n+            .filter(c -> c instanceof KerberosTicket)\n+            .map(c -> ((KerberosTicket) c).getServer().getName()).findFirst()\n+            .isPresent());\n+\n+\n+    // should fail as we send a service ticket instead of tgt to KDC.\n+    intercept(SaslException.class,\n+        () -> ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+\n+          @Override\n+          public Void run() throws Exception {\n+            SaslClient client = Sasl.createSaslClient(\n+                new String[] {AuthMethod.KERBEROS.getMechanismName()},\n+                clientPrincipal, server2Protocol, host, props, null);\n+            client.evaluateChallenge(new byte[0]);\n+            client.dispose();\n+            return null;\n+          }\n+        }));\n+\n+    // relogin to get a new ticket\n+    ugi.reloginFromKeytab();\n+\n+    // make sure we can get new service ticket after the relogin.\n+    ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+\n+      @Override\n+      public Void run() throws Exception {\n+        SaslClient client = Sasl.createSaslClient(\n+            new String[] {AuthMethod.KERBEROS.getMechanismName()},\n+            clientPrincipal, server2Protocol, host, props, null);\n+        client.evaluateChallenge(new byte[0]);\n+        client.dispose();\n+        return null;\n+      }\n+    });\n+\n+    assertTrue(\"No service ticket for \" + server2Protocol + \" found\",\n+        subject.getPrivateCredentials(KerberosTicket.class).stream()\n+            .filter(t -> t.getServer().getName().startsWith(server2Protocol))\n+            .findAny().isPresent());\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java",
                "sha": "cbea393d931644f906519dc0013a1b5e3a01d6df",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15143. NPE due to Invalid KerberosTicket in UGI. Contributed by Mukul Kumar Singh.",
        "parent": "https://github.com/apache/hadoop/commit/52babbb4a0e3c89f2025bf6e9a1b51a96e8f8fb0",
        "patched_files": [
            "UserGroupInformation.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFixKerberosTicketOrder.java",
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_d37b45d": {
        "bug_id": "hadoop_d37b45d",
        "commit": "https://github.com/apache/hadoop/commit/d37b45d613b768950d1cbe342961cd71776816ae",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java?ref=d37b45d613b768950d1cbe342961cd71776816ae",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "patch": "@@ -422,7 +422,7 @@ public JobPriority getPriority() throws IOException, InterruptedException {\n    * The user-specified job name.\n    */\n   public String getJobName() {\n-    if (state == JobState.DEFINE) {\n+    if (state == JobState.DEFINE || status == null) {\n       return super.getJobName();\n     }\n     ensureState(JobState.RUNNING);",
                "raw_url": "https://github.com/apache/hadoop/raw/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "sha": "45c065da5e48461b847901b607b16215f606b60d",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java?ref=d37b45d613b768950d1cbe342961cd71776816ae",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.JobStatus.State;\n+import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;\n import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -54,6 +55,41 @@ public void testJobToString() throws IOException, InterruptedException {\n     Assert.assertNotNull(job.toString());\n   }\n \n+  @Test\n+  public void testUnexpectedJobStatus() throws Exception {\n+    Cluster cluster = mock(Cluster.class);\n+    JobID jobid = new JobID(\"1014873536921\", 6);\n+    ClientProtocol clientProtocol = mock(ClientProtocol.class);\n+    when(cluster.getClient()).thenReturn(clientProtocol);\n+    JobStatus status = new JobStatus(jobid, 0f, 0f, 0f, 0f,\n+        State.RUNNING, JobPriority.DEFAULT, \"root\",\n+        \"testUnexpectedJobStatus\", \"job file\", \"tracking URL\");\n+    when(clientProtocol.getJobStatus(jobid)).thenReturn(status);\n+    Job job = Job.getInstance(cluster, status, new JobConf());\n+\n+    // ensurer job status is RUNNING\n+    Assert.assertNotNull(job.getStatus());\n+    Assert.assertTrue(job.getStatus().getState() == State.RUNNING);\n+\n+    // when updating job status, job client could not retrieve\n+    // job status, and status reset to null\n+    when(clientProtocol.getJobStatus(jobid)).thenReturn(null);\n+\n+    try {\n+      job.updateStatus();\n+    } catch (IOException e) {\n+      Assert.assertTrue(e != null\n+          && e.getMessage().contains(\"Job status not available\"));\n+    }\n+\n+    try {\n+      ControlledJob cj = new ControlledJob(job, null);\n+      Assert.assertNotNull(cj.toString());\n+    } catch (NullPointerException e) {\n+      Assert.fail(\"job API fails with NPE\");\n+    }\n+  }\n+\n   @Test\n   public void testUGICredentialsPropogation() throws Exception {\n     Credentials creds = new Credentials();",
                "raw_url": "https://github.com/apache/hadoop/raw/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "sha": "60f390f44650d7559715b9f2f50a8185800642b2",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6762. ControlledJob#toString failed with NPE when job status is not successfully updated (Weiwei Yang via Varun Saxena)",
        "parent": "https://github.com/apache/hadoop/commit/0faee62a0c8c1b8fd83227babfd00fbc2b26bddf",
        "patched_files": [
            "Job.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJob.java"
        ]
    },
    "hadoop_d3ac516": {
        "bug_id": "hadoop_d3ac516",
        "commit": "https://github.com/apache/hadoop/commit/d3ac516665b551ff0f9b55b668e2c9fca9a3fde1",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/d3ac516665b551ff0f9b55b668e2c9fca9a3fde1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java?ref=d3ac516665b551ff0f9b55b668e2c9fca9a3fde1",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "patch": "@@ -296,7 +296,6 @@ public void testGetBlockType() {\n     preferredBlockSize = 128*1024*1024;\n     INodeFile inf = createINodeFile(replication, preferredBlockSize);\n     assertEquals(inf.getBlockType(), CONTIGUOUS);\n-    ErasureCodingPolicyManager.getInstance().init(new Configuration());\n     INodeFile striped = createStripedINodeFile(preferredBlockSize);\n     assertEquals(striped.getBlockType(), STRIPED);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/d3ac516665b551ff0f9b55b668e2c9fca9a3fde1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "sha": "1392f9d9eb239b60f915db0a8073bb1b0f5d2122",
                "status": "modified"
            }
        ],
        "message": "Revert \"HDFS-13287. TestINodeFile#testGetBlockType results in NPE when run alone. Contributed by Virajith Jalaparti.\"\n\nThis reverts commit a1c3868c4f027adcb814b30d842e60d1f94326ea.",
        "parent": "https://github.com/apache/hadoop/commit/48e564f7e2f9223ba8521c36431358adb47d8bf6",
        "patched_files": [
            "INodeFile.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestINodeFile.java"
        ]
    },
    "hadoop_d4bde13": {
        "bug_id": "hadoop_d4bde13",
        "commit": "https://github.com/apache/hadoop/commit/d4bde134e37a5dfc72f15aa626cc49a461c1b254",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/d4bde134e37a5dfc72f15aa626cc49a461c1b254/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestContinuousScheduling.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestContinuousScheduling.java?ref=d4bde134e37a5dfc72f15aa626cc49a461c1b254",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestContinuousScheduling.java",
                "patch": "@@ -18,7 +18,9 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair;\n \n+import com.google.common.base.Supplier;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeId;\n@@ -383,8 +385,8 @@ public void testFairSchedulerContinuousSchedulingInitTime() throws Exception {\n         true);\n     ask1.add(request1);\n     ask1.add(request2);\n-    scheduler.allocate(id11, ask1, null, new ArrayList<ContainerId>(), null, null,\n-        NULL_UPDATE_REQUESTS);\n+    scheduler.allocate(id11, ask1, null, new ArrayList<ContainerId>(), null,\n+        null, NULL_UPDATE_REQUESTS);\n \n     NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);\n     scheduler.handle(nodeEvent1);\n@@ -393,6 +395,11 @@ public void testFairSchedulerContinuousSchedulingInitTime() throws Exception {\n     // time\n     mockClock.tickSec(delayThresholdTimeMs / 1000);\n     scheduler.attemptScheduling(node);\n+    GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+      public Boolean get() {\n+        return fsAppAttempt.getLastScheduledContainer().size() != 0;\n+      }\n+    }, 10, 4000);\n     Map<SchedulerRequestKey, Long> lastScheduledContainer =\n         fsAppAttempt.getLastScheduledContainer();\n     long initSchedulerTime =",
                "raw_url": "https://github.com/apache/hadoop/raw/d4bde134e37a5dfc72f15aa626cc49a461c1b254/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestContinuousScheduling.java",
                "sha": "443c7963cc9326d6ba20d9c05e9f7b13cd389406",
                "status": "modified"
            }
        ],
        "message": "YARN-7721. Fix TestContinuousScheduling fails sporadically with NPE.\n\nContributed by Wilfred Spiegelenburg.",
        "parent": "https://github.com/apache/hadoop/commit/c4733377d0fa375a8d585f5cb1db79bf20ec6710",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "TestContinuousScheduling.java"
        ]
    },
    "hadoop_d4c8858": {
        "bug_id": "hadoop_d4c8858",
        "commit": "https://github.com/apache/hadoop/commit/d4c8858586eeed2820f3ab21da79603b52c64594",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/d4c8858586eeed2820f3ab21da79603b52c64594/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsUrlConnection.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsUrlConnection.java?ref=d4c8858586eeed2820f3ab21da79603b52c64594",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsUrlConnection.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import java.io.IOException;\n import java.io.InputStream;\n+import java.net.URI;\n import java.net.URISyntaxException;\n import java.net.URL;\n import java.net.URLConnection;\n@@ -56,8 +57,18 @@ public void connect() throws IOException {\n     Preconditions.checkState(is == null, \"Already connected\");\n     try {\n       LOG.debug(\"Connecting to {}\", url);\n-      FileSystem fs = FileSystem.get(url.toURI(), conf);\n-      is = fs.open(new Path(url.toURI()));\n+      URI uri = url.toURI();\n+      FileSystem fs = FileSystem.get(uri, conf);\n+      // URI#getPath returns null value if path contains relative path\n+      // i.e file:root/dir1/file1\n+      // So path can not be constructed from URI.\n+      // We can only use schema specific part in URI.\n+      // Uri#isOpaque return true if path is relative.\n+      if(uri.isOpaque() && uri.getScheme().equals(\"file\")) {\n+        is = fs.open(new Path(uri.getSchemeSpecificPart()));\n+      } else {\n+        is = fs.open(new Path(uri));\n+      }\n     } catch (URISyntaxException e) {\n       throw new IOException(e.toString());\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/d4c8858586eeed2820f3ab21da79603b52c64594/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FsUrlConnection.java",
                "sha": "c5429d2370250d59540b9b0452fc722688cdd04a",
                "status": "modified"
            },
            {
                "additions": 107,
                "blob_url": "https://github.com/apache/hadoop/blob/d4c8858586eeed2820f3ab21da79603b52c64594/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsUrlConnectionPath.java",
                "changes": 107,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsUrlConnectionPath.java?ref=d4c8858586eeed2820f3ab21da79603b52c64594",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsUrlConnectionPath.java",
                "patch": "@@ -0,0 +1,107 @@\n+/**\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License. See accompanying LICENSE file.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.io.*;\n+import java.net.URL;\n+import java.nio.file.Paths;\n+\n+/**\n+ * Test case for FsUrlConnection with relativePath and SPACE.\n+ */\n+public class TestFsUrlConnectionPath {\n+\n+  private static final String CURRENT = Paths.get(\"\").toAbsolutePath()\n+      .toString();\n+  private static final String ABSOLUTE_PATH = \"file:\" + CURRENT + \"/abs.txt\";\n+  private static final String RELATIVE_PATH = \"file:relative.txt\";\n+  private static final String ABSOLUTE_PATH_W_SPACE = \"file:\"\n+      + CURRENT + \"/abs 1.txt\";\n+  private static final String RELATIVE_PATH_W_SPACE = \"file:relative 1.txt\";\n+  private static final String ABSOLUTE_PATH_W_ENCODED_SPACE =\n+      \"file:\" + CURRENT + \"/abs%201.txt\";\n+  private static final String RELATIVE_PATH_W_ENCODED_SPACE =\n+      \"file:relative%201.txt\";\n+  private static final String DATA = \"data\";\n+  private static final Configuration CONFIGURATION = new Configuration();\n+\n+\n+  @BeforeClass\n+  public static void initialize() throws IOException{\n+    write(ABSOLUTE_PATH.substring(5), DATA);\n+    write(RELATIVE_PATH.substring(5), DATA);\n+    write(ABSOLUTE_PATH_W_SPACE.substring(5), DATA);\n+    write(RELATIVE_PATH_W_SPACE.substring(5), DATA);\n+    URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());\n+  }\n+\n+  @AfterClass\n+  public static void cleanup(){\n+    delete(ABSOLUTE_PATH.substring(5));\n+    delete(RELATIVE_PATH.substring(5));\n+    delete(ABSOLUTE_PATH_W_SPACE.substring(5));\n+    delete(RELATIVE_PATH_W_SPACE.substring(5));\n+  }\n+\n+  public static void delete(String path){\n+    File file = new File(path);\n+    file.delete();\n+  }\n+\n+  public static void write(String path, String data) throws IOException{\n+    File file = new File(path);\n+    file.createNewFile();\n+    FileWriter fw = new FileWriter(file);\n+    fw.write(data);\n+    fw.close();\n+  }\n+\n+  public static int readStream(String path) throws Exception{\n+    URL url = new URL(path);\n+    InputStream is = url.openStream();\n+    return is.available();\n+  }\n+\n+\n+  @Test\n+  public void testAbsolutePath() throws Exception{\n+    int length = readStream(ABSOLUTE_PATH);\n+    Assert.assertTrue(length > 1);\n+  }\n+\n+  @Test\n+  public void testRelativePath() throws Exception{\n+    int length = readStream(RELATIVE_PATH);\n+    Assert.assertTrue(length > 1);\n+  }\n+\n+  @Test\n+  public void testAbsolutePathWithSpace() throws Exception{\n+    int length = readStream(ABSOLUTE_PATH_W_ENCODED_SPACE);\n+    Assert.assertTrue(length > 1);\n+  }\n+\n+  @Test\n+  public void testRelativePathWithSpace() throws Exception{\n+    int length = readStream(RELATIVE_PATH_W_ENCODED_SPACE);\n+    Assert.assertTrue(length > 1);\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/d4c8858586eeed2820f3ab21da79603b52c64594/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFsUrlConnectionPath.java",
                "sha": "d15c1ac5158568df5a61eab5720b89126a3ad8bc",
                "status": "added"
            }
        ],
        "message": "HADOOP-16247. NPE in FsUrlConnection. Contributed by Karthik Palanisamy.",
        "parent": "https://github.com/apache/hadoop/commit/77170e70d16e309121ca7730974617c05e66d063",
        "patched_files": [
            "FsUrlConnection.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFsUrlConnectionPath.java"
        ]
    },
    "hadoop_d55f378": {
        "bug_id": "hadoop_d55f378",
        "commit": "https://github.com/apache/hadoop/commit/d55f3780fbf9308554ef3362c2be89651db43f46",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=d55f3780fbf9308554ef3362c2be89651db43f46",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -179,6 +179,8 @@ Release 2.1.2 - UNRELEASED\n     YARN-1273. Fixed Distributed-shell to account for containers that failed\n     to start. (Hitesh Shah via vinodkv)\n \n+    YARN-1032. Fixed NPE in RackResolver. (Lohit Vijayarenu via acmurthy)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/CHANGES.txt",
                "sha": "0c3a0307fbf19277a23271d1b9cc3981adc9398f",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java?ref=d55f3780fbf9308554ef3362c2be89651db43f46",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.CachedDNSToSwitchMapping;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.net.ScriptBasedMapping;\n@@ -98,8 +99,15 @@ private static Node coreResolve(String hostName) {\n     List <String> tmpList = new ArrayList<String>(1);\n     tmpList.add(hostName);\n     List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n-    String rName = rNameList.get(0);\n-    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    String rName = null;\n+    if (rNameList == null || rNameList.get(0) == null) {\n+      rName = NetworkTopology.DEFAULT_RACK;\n+      LOG.info(\"Couldn't resolve \" + hostName + \". Falling back to \"\n+          + NetworkTopology.DEFAULT_RACK);\n+    } else {\n+      rName = rNameList.get(0);\n+      LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    }\n     return new NodeBase(hostName, rName);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "sha": "cc2a56c3be6819cfaa5a05b6ccc38a0b54079607",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java?ref=d55f3780fbf9308554ef3362c2be89651db43f46",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "patch": "@@ -28,13 +28,16 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.junit.Assert;\n import org.junit.Test;\n \n public class TestRackResolver {\n \n   private static Log LOG = LogFactory.getLog(TestRackResolver.class);\n+  private static final String invalidHost = \"invalidHost\";\n+\n \n   public static final class MyResolver implements DNSToSwitchMapping {\n \n@@ -50,6 +53,11 @@\n       if (hostList.isEmpty()) {\n         return returnList;\n       }\n+      if (hostList.get(0).equals(invalidHost)) {\n+        // Simulate condition where resolving host returns null\n+        return null; \n+      }\n+        \n       LOG.info(\"Received resolve request for \"\n           + hostList.get(0));\n       if (hostList.get(0).equals(\"host1\")\n@@ -90,6 +98,8 @@ public void testCaching() {\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n     node = RackResolver.resolve(\"host1\");\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n+    node = RackResolver.resolve(invalidHost);\n+    Assert.assertEquals(NetworkTopology.DEFAULT_RACK, node.getNetworkLocation());\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "sha": "70ca23c3a2e0b97fd7100ce0519947a806b43a79",
                "status": "modified"
            }
        ],
        "message": "YARN-1032. Fixed NPE in RackResolver. Contributed by Lohit Vijayarenu.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529534 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/25361d56cf824ae2e68f45a6962146ba7bd54e01",
        "patched_files": [
            "RackResolver.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRackResolver.java"
        ]
    },
    "hadoop_d565480": {
        "bug_id": "hadoop_d565480",
        "commit": "https://github.com/apache/hadoop/commit/d565480da2f646b40c3180e1ccb2935c9863dfef",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=d565480da2f646b40c3180e1ccb2935c9863dfef",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2216,6 +2216,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9329. TestBootstrapStandby#testRateThrottling is flaky because fsimage\n     size is smaller than IO buffer size. (zhz)\n \n+    HDFS-9313. Possible NullPointerException in BlockManager if no excess\n+    replica can be chosen. (mingma)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "879c015e88b57571ddb268b79a28db21d5cfafa5",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy.java?ref=d565480da2f646b40c3180e1ccb2935c9863dfef",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy.java",
                "patch": "@@ -23,8 +23,6 @@\n import java.util.Map;\n import java.util.Set;\n \n-import org.apache.commons.logging.Log;\n-import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.StorageType;\n@@ -33,13 +31,17 @@\n import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n \n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n /** \n  * This interface is used for choosing the desired number of targets\n  * for placing block replicas.\n  */\n @InterfaceAudience.Private\n public abstract class BlockPlacementPolicy {\n-  static final Log LOG = LogFactory.getLog(BlockPlacementPolicy.class);\n+  static final Logger LOG = LoggerFactory.getLogger(\n+      BlockPlacementPolicy.class);\n \n   @InterfaceAudience.Private\n   public static class NotEnoughReplicasException extends Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicy.java",
                "sha": "526a5d7655c82677eef277741a6faf9fe570aecb",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java?ref=d565480da2f646b40c3180e1ccb2935c9863dfef",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
                "patch": "@@ -981,6 +981,12 @@ public DatanodeStorageInfo chooseReplicaToDelete(short replicationFactor,\n                 excessTypes);\n       }\n       firstOne = false;\n+      if (cur == null) {\n+        LOG.warn(\"No excess replica can be found. excessTypes: {}.\" +\n+            \" moreThanOne: {}. exactlyOne: {}.\", excessTypes, moreThanOne,\n+            exactlyOne);\n+        break;\n+      }\n \n       // adjust rackmap, moreThanOne, and exactlyOne\n       adjustSetsWithChosenReplica(rackMap, moreThanOne, exactlyOne, cur);",
                "raw_url": "https://github.com/apache/hadoop/raw/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java",
                "sha": "2723ed95f9162e54a2c8b1153aa4e51f20b35d23",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicy.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicy.java?ref=d565480da2f646b40c3180e1ccb2935c9863dfef",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicy.java",
                "patch": "@@ -1000,6 +1000,14 @@ public void testChooseReplicasToDelete() throws Exception {\n     BlockStoragePolicySuite POLICY_SUITE = BlockStoragePolicySuite\n         .createDefaultSuite();\n     BlockStoragePolicy storagePolicy = POLICY_SUITE.getDefaultPolicy();\n+    DatanodeStorageInfo excessSSD = DFSTestUtil.createDatanodeStorageInfo(\n+        \"Storage-excess-SSD-ID\", \"localhost\",\n+        storages[0].getDatanodeDescriptor().getNetworkLocation(),\n+        \"foo.com\", StorageType.SSD);\n+    updateHeartbeatWithUsage(excessSSD.getDatanodeDescriptor(),\n+        2* HdfsServerConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L,\n+        2* HdfsServerConstants.MIN_BLOCKS_FOR_WRITE*BLOCK_SIZE, 0L, 0L, 0L, 0,\n+        0);\n \n     // use delete hint case.\n \n@@ -1022,6 +1030,29 @@ public void testChooseReplicasToDelete() throws Exception {\n     excessReplicas = replicator.chooseReplicasToDelete(nonExcess, 3,\n         excessTypes, storages[3].getDatanodeDescriptor(), null);\n     assertTrue(excessReplicas.contains(excessStorage));\n+\n+\n+    // The block was initially created on excessSSD(rack r1),\n+    // storages[4](rack r3) and storages[5](rack r3) with\n+    // ONESSD_STORAGE_POLICY_NAME storage policy.\n+    // Right after balancer moves the block from storages[5] to\n+    // storages[3](rack r2), the application changes the storage policy from\n+    // ONESSD_STORAGE_POLICY_NAME to HOT_STORAGE_POLICY_ID. In this case,\n+    // no replica can be chosen as the excessive replica as\n+    // chooseReplicasToDelete only considers storages[4] and storages[5] that\n+    // are the same rack. But neither's storage type is SSD.\n+    // TODO BlockPlacementPolicyDefault should be able to delete excessSSD.\n+    nonExcess.clear();\n+    nonExcess.add(excessSSD);\n+    nonExcess.add(storages[3]);\n+    nonExcess.add(storages[4]);\n+    nonExcess.add(storages[5]);\n+    excessTypes = storagePolicy.chooseExcess((short) 3,\n+        DatanodeStorageInfo.toStorageTypes(nonExcess));\n+    excessReplicas = replicator.chooseReplicasToDelete(nonExcess, 3,\n+        excessTypes, storages[3].getDatanodeDescriptor(),\n+        storages[5].getDatanodeDescriptor());\n+    assertTrue(excessReplicas.size() == 0);\n   }\n \n  @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/d565480da2f646b40c3180e1ccb2935c9863dfef/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicy.java",
                "sha": "37fcf346de0b7666d736ed7638d355a65102b95b",
                "status": "modified"
            }
        ],
        "message": "HDFS-9313. Possible NullPointerException in BlockManager if no excess replica can be chosen. (mingma)",
        "parent": "https://github.com/apache/hadoop/commit/8e05dbf2bddce95d5f5a5bae5df61acabf0ba7c5",
        "patched_files": [
            "BlockPlacementPolicy.java",
            "BlockPlacementPolicyDefault.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestReplicationPolicy.java"
        ]
    },
    "hadoop_d5ef38b": {
        "bug_id": "hadoop_d5ef38b",
        "commit": "https://github.com/apache/hadoop/commit/d5ef38b0935dd6e17a73337f03a096d384fe25d2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d5ef38b0935dd6e17a73337f03a096d384fe25d2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java?ref=d5ef38b0935dd6e17a73337f03a096d384fe25d2",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java",
                "patch": "@@ -109,7 +109,9 @@ public Configuration getConf() {\n       int nodesRequired, final long sizeRequired) throws SCMException {\n     List<DatanodeDetails> healthyNodes =\n         nodeManager.getNodes(HddsProtos.NodeState.HEALTHY);\n-    healthyNodes.removeAll(excludedNodes);\n+    if (excludedNodes != null) {\n+      healthyNodes.removeAll(excludedNodes);\n+    }\n     String msg;\n     if (healthyNodes.size() == 0) {\n       msg = \"No healthy node found to allocate container.\";",
                "raw_url": "https://github.com/apache/hadoop/raw/d5ef38b0935dd6e17a73337f03a096d384fe25d2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/SCMCommonPolicy.java",
                "sha": "77cdd83f7938e36c958dc368ee4ebdd627e29602",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/d5ef38b0935dd6e17a73337f03a096d384fe25d2/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java?ref=d5ef38b0935dd6e17a73337f03a096d384fe25d2",
                "deletions": 8,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java",
                "patch": "@@ -63,6 +63,13 @@\n   public void setup() {\n     //initialize network topology instance\n     conf = new OzoneConfiguration();\n+  }\n+\n+  @Test\n+  public void testRackAwarePolicy() throws IOException {\n+    conf.set(ScmConfigKeys.OZONE_SCM_CONTAINER_PLACEMENT_IMPL_KEY,\n+        SCMContainerPlacementRackAware.class.getName());\n+\n     NodeSchema[] schemas = new NodeSchema[]\n         {ROOT_SCHEMA, RACK_SCHEMA, LEAF_SCHEMA};\n     NodeSchemaManager.getInstance().init(schemas, true);\n@@ -91,11 +98,7 @@ public void setup() {\n         .thenReturn(new SCMNodeMetric(storageCapacity, 80L, 20L));\n     when(nodeManager.getNodeStat(datanodes.get(4)))\n         .thenReturn(new SCMNodeMetric(storageCapacity, 70L, 30L));\n-  }\n-\n \n-  @Test\n-  public void testDefaultPolicy() throws IOException {\n     ContainerPlacementPolicy policy = ContainerPlacementPolicyFactory\n         .getPolicy(conf, nodeManager, cluster, true);\n \n@@ -111,14 +114,21 @@ public void testDefaultPolicy() throws IOException {\n         datanodeDetails.get(2)));\n   }\n \n+  @Test\n+  public void testDefaultPolicy() throws IOException {\n+    ContainerPlacementPolicy policy = ContainerPlacementPolicyFactory\n+        .getPolicy(conf, null, null, true);\n+    Assert.assertSame(SCMContainerPlacementRandom.class, policy.getClass());\n+  }\n+\n   /**\n    * A dummy container placement implementation for test.\n    */\n-  public class DummyImpl implements ContainerPlacementPolicy {\n+  public static class DummyImpl implements ContainerPlacementPolicy {\n     @Override\n     public List<DatanodeDetails> chooseDatanodes(\n         List<DatanodeDetails> excludedNodes, List<DatanodeDetails> favoredNodes,\n-        int nodesRequired, long sizeRequired) throws IOException {\n+        int nodesRequired, long sizeRequired) {\n       return null;\n     }\n   }\n@@ -127,8 +137,7 @@ public void testDefaultPolicy() throws IOException {\n   public void testConstuctorNotFound() throws SCMException {\n     // set a placement class which does't have the right constructor implemented\n     conf.set(ScmConfigKeys.OZONE_SCM_CONTAINER_PLACEMENT_IMPL_KEY,\n-        \"org.apache.hadoop.hdds.scm.container.placement.algorithms.\" +\n-            \"TestContainerPlacementFactory$DummyImpl\");\n+        DummyImpl.class.getName());\n     ContainerPlacementPolicyFactory.getPolicy(conf, null, null, true);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/d5ef38b0935dd6e17a73337f03a096d384fe25d2/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/container/placement/algorithms/TestContainerPlacementFactory.java",
                "sha": "456b44bbc549502fefcbd5d20ca3faa1cdae367b",
                "status": "modified"
            }
        ],
        "message": "HDDS-1822. NPE in SCMCommonPolicy.chooseDatanodes (#1120)",
        "parent": "https://github.com/apache/hadoop/commit/9838a47d44c31ac8557b4e8f67c1676c356ec9f7",
        "patched_files": [
            "SCMCommonPolicy.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerPlacementFactory.java"
        ]
    },
    "hadoop_d62e121": {
        "bug_id": "hadoop_d62e121",
        "commit": "https://github.com/apache/hadoop/commit/d62e121ffc0239e7feccc1e23ece92c5fac685f6",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=d62e121ffc0239e7feccc1e23ece92c5fac685f6",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -1209,11 +1209,18 @@ private void updateSchedulerHealth(long now, FiCaSchedulerNode node,\n  }\n \n   @VisibleForTesting\n-  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n+  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         && !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n+\n+    if (!nodeTracker.exists(node.getNodeID())) {\n+      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n+          \" has been removed\");\n+      return;\n+    }\n+\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));",
                "raw_url": "https://github.com/apache/hadoop/raw/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "bedf45570c4dc5e8d468460a74211f13a3bd4028",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hadoop/blob/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java?ref=d62e121ffc0239e7feccc1e23ece92c5fac685f6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "patch": "@@ -3375,4 +3375,44 @@ public void handle(Event event) {\n     Assert.assertEquals(availableResource.getMemorySize(), 0);\n     Assert.assertEquals(availableResource.getVirtualCores(), 0);\n   }\n+\n+  @Test\n+  public void testSchedulingOnRemovedNode() throws Exception {\n+    Configuration conf = new YarnConfiguration();\n+    conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,\n+        ResourceScheduler.class);\n+    conf.setBoolean(\n+        CapacitySchedulerConfiguration.SCHEDULE_ASYNCHRONOUSLY_ENABLE,\n+            false);\n+\n+    MockRM rm = new MockRM(conf);\n+    rm.start();\n+    RMApp app = rm.submitApp(100);\n+    rm.drainEvents();\n+\n+    MockNM nm1 = rm.registerNode(\"127.0.0.1:1234\", 10240, 10);\n+    MockAM am = MockRM.launchAndRegisterAM(app, rm, nm1);\n+\n+    //remove nm2 to keep am alive\n+    MockNM nm2 = rm.registerNode(\"127.0.0.1:1235\", 10240, 10);\n+\n+    am.allocate(ResourceRequest.ANY, 2048, 1, null);\n+\n+    CapacityScheduler scheduler =\n+        (CapacityScheduler) rm.getRMContext().getScheduler();\n+    FiCaSchedulerNode node =\n+        (FiCaSchedulerNode)\n+            scheduler.getNodeTracker().getNode(nm2.getNodeId());\n+    scheduler.handle(new NodeRemovedSchedulerEvent(\n+        rm.getRMContext().getRMNodes().get(nm2.getNodeId())));\n+    // schedulerNode is removed, try allocate a container\n+    scheduler.allocateContainersToNode(node);\n+\n+    AppAttemptRemovedSchedulerEvent appRemovedEvent1 =\n+        new AppAttemptRemovedSchedulerEvent(\n+            am.getApplicationAttemptId(),\n+            RMAppAttemptState.FINISHED, false);\n+    scheduler.handle(appRemovedEvent1);\n+    rm.stop();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "sha": "d3567f5110c07b1d4ca14164e7e7e8a761e17871",
                "status": "modified"
            }
        ],
        "message": "YARN-5195. RM intermittently crashed with NPE while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler. (sandflee via wangda)",
        "parent": "https://github.com/apache/hadoop/commit/2d8d183b1992b82c4d8dd3d6b41a1964685d909e",
        "patched_files": [
            "CapacityScheduler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_d726156": {
        "bug_id": "hadoop_d726156",
        "commit": "https://github.com/apache/hadoop/commit/d72615611cfa6bd82756270d4b10136ec1e56741",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageApps.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageApps.java?ref=d72615611cfa6bd82756270d4b10136ec1e56741",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageApps.java",
                "patch": "@@ -1936,6 +1936,8 @@ public void testReadAppsInfoFilters() throws Exception {\n \n   @AfterClass\n   public static void tearDownAfterClass() throws Exception {\n-    util.shutdownMiniCluster();\n+    if (util != null) {\n+      util.shutdownMiniCluster();\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageApps.java",
                "sha": "0dee442264efdf147d41d7dd1664fbf2c261fbbd",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageDomain.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageDomain.java?ref=d72615611cfa6bd82756270d4b10136ec1e56741",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageDomain.java",
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn;\n import org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainRowKey;\n import org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainTableRW;\n+import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n@@ -123,4 +124,11 @@ public void testDomainIdTable() throws Exception {\n     assertEquals(\"user1,user2 group1,group2\", readers);\n     assertEquals(\"writer1,writer2\", writers);\n   }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    if (util != null) {\n+      util.shutdownMiniCluster();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageDomain.java",
                "sha": "1f59088435e5b1500271f998a247504153a5663d",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageEntities.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageEntities.java?ref=d72615611cfa6bd82756270d4b10136ec1e56741",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageEntities.java",
                "patch": "@@ -1879,7 +1879,9 @@ public void testListTypesInApp() throws Exception {\n \n   @AfterClass\n   public static void tearDownAfterClass() throws Exception {\n-    util.shutdownMiniCluster();\n+    if (util != null) {\n+      util.shutdownMiniCluster();\n+    }\n   }\n \n   private boolean verifyRowKeyForSubApplication(byte[] rowKey, String suAppUser,",
                "raw_url": "https://github.com/apache/hadoop/raw/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageEntities.java",
                "sha": "116285c23fa5b8c902fa70287c3fe16c918a0ea6",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageSchema.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageSchema.java?ref=d72615611cfa6bd82756270d4b10136ec1e56741",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageSchema.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.hadoop.yarn.server.timelineservice.storage.common.BaseTableRW;\n import org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityTableRW;\n import org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunTableRW;\n+import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n@@ -137,4 +138,11 @@ public void createWithSetPrefix() throws IOException {\n     hbaseConf\n     .unset(YarnConfiguration.TIMELINE_SERVICE_HBASE_SCHEMA_PREFIX_NAME);\n   }\n+\n+  @AfterClass\n+  public static void tearDownAfterClass() throws Exception {\n+    if (util != null) {\n+      util.shutdownMiniCluster();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/TestHBaseTimelineStorageSchema.java",
                "sha": "950ce62be59c0ee066f33d71d346d880d34e9960",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowActivity.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowActivity.java?ref=d72615611cfa6bd82756270d4b10136ec1e56741",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowActivity.java",
                "patch": "@@ -492,6 +492,8 @@ private void checkFlowActivityRunId(long runid, String flowVersion,\n \n   @AfterClass\n   public static void tearDownAfterClass() throws Exception {\n-    util.shutdownMiniCluster();\n+    if (util != null) {\n+      util.shutdownMiniCluster();\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowActivity.java",
                "sha": "31434633b4a57af39b7bad8bcd1454e164dcec42",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRun.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRun.java?ref=d72615611cfa6bd82756270d4b10136ec1e56741",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRun.java",
                "patch": "@@ -1075,6 +1075,8 @@ public void testMetricFilters() throws Exception {\n \n   @AfterClass\n   public static void tearDownAfterClass() throws Exception {\n-    util.shutdownMiniCluster();\n+    if (util != null) {\n+      util.shutdownMiniCluster();\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRun.java",
                "sha": "c3ee758294f74dabbe7060a59177cc1fde71a25a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRunCompaction.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRunCompaction.java?ref=d72615611cfa6bd82756270d4b10136ec1e56741",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRunCompaction.java",
                "patch": "@@ -850,6 +850,8 @@ public void testProcessSummationEmpty() throws IOException {\n \n   @AfterClass\n   public static void tearDownAfterClass() throws Exception {\n-    util.shutdownMiniCluster();\n+    if (util != null) {\n+      util.shutdownMiniCluster();\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d72615611cfa6bd82756270d4b10136ec1e56741/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase-tests/src/test/java/org/apache/hadoop/yarn/server/timelineservice/storage/flow/TestHBaseStorageFlowRunCompaction.java",
                "sha": "e0a58da98057ac5756b86f588cb34fce0123f7d7",
                "status": "modified"
            }
        ],
        "message": "YARN-8348. Incorrect and missing AfterClass in HBase-tests to fix NPE failures. Contributed by Giovanni Matteo Fumarola.",
        "parent": "https://github.com/apache/hadoop/commit/e99e5bf104e9664bc1b43a2639d87355d47a77e2",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "TestHBaseTimelineStorageApps.java",
            "TestHBaseStorageFlowRun.java",
            "TestHBaseStorageFlowRunCompaction.java",
            "TestHBaseStorageFlowActivity.java",
            "TestHBaseTimelineStorageDomain.java",
            "TestHBaseTimelineStorageSchema.java",
            "TestHBaseTimelineStorageEntities.java"
        ]
    },
    "hadoop_d86db3f": {
        "bug_id": "hadoop_d86db3f",
        "commit": "https://github.com/apache/hadoop/commit/d86db3f76f03a63c56e6dd9f5531d3b8a78980f6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d86db3f76f03a63c56e6dd9f5531d3b8a78980f6/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt?ref=d86db3f76f03a63c56e6dd9f5531d3b8a78980f6",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "patch": "@@ -24,6 +24,9 @@ fs-encryption (Unreleased)\n     HADOOP-10653. Add a new constructor for CryptoInputStream that \n     receives current position of wrapped stream. (Yi Liu)\n \n+    HADOOP-10662. NullPointerException in CryptoInputStream while wrapped\n+    stream is not ByteBufferReadable. Add tests using normal stream. (Yi Liu)\n+\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop/raw/d86db3f76f03a63c56e6dd9f5531d3b8a78980f6/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "sha": "66b55b0f50888a76c2a5cfc0fb17d04f9d18983f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/d86db3f76f03a63c56e6dd9f5531d3b8a78980f6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java?ref=d86db3f76f03a63c56e6dd9f5531d3b8a78980f6",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java",
                "patch": "@@ -172,6 +172,8 @@ public int read(byte[] b, int off, int len) throws IOException {\n           } catch (UnsupportedOperationException e) {\n             usingByteBufferRead = Boolean.FALSE;\n           }\n+        } else {\n+          usingByteBufferRead = Boolean.FALSE;\n         }\n         if (!usingByteBufferRead) {\n           n = readFromUnderlyingStream(inBuffer);",
                "raw_url": "https://github.com/apache/hadoop/raw/d86db3f76f03a63c56e6dd9f5531d3b8a78980f6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoInputStream.java",
                "sha": "55c891a0acea65abdb14f882d82eedd7e3a5ded1",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop/blob/d86db3f76f03a63c56e6dd9f5531d3b8a78980f6/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java",
                "changes": 123,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java?ref=d86db3f76f03a63c56e6dd9f5531d3b8a78980f6",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java",
                "patch": "@@ -0,0 +1,123 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.crypto;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+/**\n+ * Test crypto streams using normal stream which does not support the \n+ * additional interfaces that the Hadoop FileSystem streams implement \n+ * (Seekable, PositionedReadable, ByteBufferReadable, HasFileDescriptor, \n+ * CanSetDropBehind, CanSetReadahead, HasEnhancedByteBufferAccess, Syncable, \n+ * CanSetDropBehind)\n+ */\n+public class TestCryptoStreamsNormal extends CryptoStreamsTestBase {\n+  /**\n+   * Data storage.\n+   * {@link #getOutputStream(int, byte[], byte[])} will write to this buffer.\n+   * {@link #getInputStream(int, byte[], byte[])} will read from this buffer.\n+   */\n+  private byte[] buffer;\n+  private int bufferLen;\n+  \n+  @BeforeClass\n+  public static void init() throws Exception {\n+    Configuration conf = new Configuration();\n+    codec = CryptoCodec.getInstance(conf);\n+  }\n+  \n+  @AfterClass\n+  public static void shutdown() throws Exception {\n+  }\n+\n+  @Override\n+  protected OutputStream getOutputStream(int bufferSize, byte[] key, byte[] iv)\n+      throws IOException {\n+    OutputStream out = new ByteArrayOutputStream() {\n+      @Override\n+      public void flush() throws IOException {\n+        buffer = buf;\n+        bufferLen = count;\n+      }\n+      @Override\n+      public void close() throws IOException {\n+        buffer = buf;\n+        bufferLen = count;\n+      }\n+    };\n+    return new CryptoOutputStream(out, codec, bufferSize, key, iv);\n+  }\n+\n+  @Override\n+  protected InputStream getInputStream(int bufferSize, byte[] key, byte[] iv)\n+      throws IOException {\n+    ByteArrayInputStream in = new ByteArrayInputStream(buffer, 0, bufferLen);\n+    return new CryptoInputStream(in, codec, bufferSize, \n+        key, iv);\n+  }\n+  \n+  @Ignore(\"Wrapped stream doesn't support Syncable\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testSyncable() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support PositionedRead\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testPositionedRead() throws IOException {}\n+\n+  @Ignore(\"Wrapped stream doesn't support ReadFully\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testReadFully() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support Seek\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testSeek() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support ByteBufferRead\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testByteBufferRead() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support ByteBufferRead, Seek\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testCombinedOp() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support SeekToNewSource\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testSeekToNewSource() throws IOException {}\n+  \n+  @Ignore(\"Wrapped stream doesn't support HasEnhancedByteBufferAccess\")\n+  @Override\n+  @Test(timeout=1000)\n+  public void testHasEnhancedByteBufferAccess() throws IOException {}\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/d86db3f76f03a63c56e6dd9f5531d3b8a78980f6/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsNormal.java",
                "sha": "e9c313fde3695b344305cec39a0d7d03ac886f9f",
                "status": "added"
            }
        ],
        "message": "HADOOP-10662. NullPointerException in CryptoInputStream while wrapped stream is not ByteBufferReadable. Add tests using normal stream. Contributed by Yi Liu\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1600553 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/75ec5792dfc5472b2aa9da7030d4e2ae7421d87c",
        "patched_files": [
            "CryptoInputStream.java",
            "CHANGES-fs-encryption.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCryptoStreamsNormal.java"
        ]
    },
    "hadoop_d9852eb": {
        "bug_id": "hadoop_d9852eb",
        "commit": "https://github.com/apache/hadoop/commit/d9852eb5897a25323ab0302c2c0decb61d310e5e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/d9852eb5897a25323ab0302c2c0decb61d310e5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java?ref=d9852eb5897a25323ab0302c2c0decb61d310e5e",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "patch": "@@ -1198,6 +1198,7 @@ public Service getStatus(String serviceName)\n     ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n     Service appSpec = new Service();\n     appSpec.setName(serviceName);\n+    appSpec.setState(ServiceState.STOPPED);\n     ApplicationId currentAppId = getAppId(serviceName);\n     if (currentAppId == null) {\n       LOG.info(\"Service {} does not have an application ID\", serviceName);",
                "raw_url": "https://github.com/apache/hadoop/raw/d9852eb5897a25323ab0302c2c0decb61d310e5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "sha": "0ab332280f2447a16347dfd1b81b2ba2fa399140",
                "status": "modified"
            }
        ],
        "message": "YARN-8357.  Fixed NPE when YARN service is saved and not deployed.\n            Contributed by Chandni Singh",
        "parent": "https://github.com/apache/hadoop/commit/7ff5a40218241ad2380595175a493794129a7402",
        "patched_files": [
            "ServiceClient.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestServiceClient.java",
            "ServiceClientTest.java"
        ]
    },
    "hadoop_da2fb2b": {
        "bug_id": "hadoop_da2fb2b",
        "commit": "https://github.com/apache/hadoop/commit/da2fb2bc46bddf42d79c6d7664cbf0311973709e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=da2fb2bc46bddf42d79c6d7664cbf0311973709e",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -510,6 +510,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-3089. LinuxContainerExecutor does not handle file arguments to\n     deleteAsUser (Eric Payne via jlowe)\n \n+    YARN-3143. RM Apps REST API can return NPE or entries missing id and other\n+    fields (jlowe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/CHANGES.txt",
                "sha": "7951eef27ad91b2871baf875595884b1d041fff8",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java?ref=da2fb2bc46bddf42d79c6d7664cbf0311973709e",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "patch": "@@ -24,7 +24,6 @@\n import java.nio.ByteBuffer;\n import java.security.Principal;\n import java.security.PrivilegedExceptionAction;\n-import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.EnumSet;\n@@ -169,6 +168,12 @@ public RMWebServices(final ResourceManager rm, Configuration conf) {\n     this.conf = conf;\n   }\n \n+  RMWebServices(ResourceManager rm, Configuration conf,\n+      HttpServletResponse response) {\n+    this(rm, conf);\n+    this.response = response;\n+  }\n+\n   protected Boolean hasAccess(RMApp app, HttpServletRequest hsr) {\n     // Check for the authorization.\n     UserGroupInformation callerUGI = getCallerUserGroupInformation(hsr, true);\n@@ -459,6 +464,9 @@ public AppsInfo getApps(@Context HttpServletRequest hsr,\n     AppsInfo allApps = new AppsInfo();\n     for (ApplicationReport report : appReports) {\n       RMApp rmapp = apps.get(report.getApplicationId());\n+      if (rmapp == null) {\n+        continue;\n+      }\n \n       if (finalStatusQuery != null && !finalStatusQuery.isEmpty()) {\n         FinalApplicationStatus.valueOf(finalStatusQuery);",
                "raw_url": "https://github.com/apache/hadoop/raw/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "sha": "1834b6a1a7dcaf975c40eb47546bbe0e8ef6cc4d",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hadoop/blob/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java?ref=da2fb2bc46bddf42d79c6d7664cbf0311973709e",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java",
                "patch": "@@ -21,24 +21,40 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.anyBoolean;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n import java.io.StringReader;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Set;\n \n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n import javax.ws.rs.core.MediaType;\n import javax.xml.parsers.DocumentBuilder;\n import javax.xml.parsers.DocumentBuilderFactory;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.service.Service.STATE;\n import org.apache.hadoop.util.VersionInfo;\n+import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest;\n+import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsResponse;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n import org.apache.hadoop.yarn.api.records.QueueState;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.ClientRMService;\n import org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.MockRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppsInfo;\n import org.apache.hadoop.yarn.util.YarnVersionInfo;\n import org.apache.hadoop.yarn.webapp.GenericExceptionHandler;\n import org.apache.hadoop.yarn.webapp.JerseyTestBase;\n@@ -586,4 +602,43 @@ public void verifyClusterSchedulerFifoGeneric(String type, String state,\n \n   }\n \n+  // Test the scenario where the RM removes an app just as we try to\n+  // look at it in the apps list\n+  @Test\n+  public void testAppsRace() throws Exception {\n+    // mock up an RM that returns app reports for apps that don't exist\n+    // in the RMApps list\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationReport mockReport = mock(ApplicationReport.class);\n+    when(mockReport.getApplicationId()).thenReturn(appId);\n+    GetApplicationsResponse mockAppsResponse =\n+        mock(GetApplicationsResponse.class);\n+    when(mockAppsResponse.getApplicationList())\n+      .thenReturn(Arrays.asList(new ApplicationReport[] { mockReport }));\n+    ClientRMService mockClientSvc = mock(ClientRMService.class);\n+    when(mockClientSvc.getApplications(isA(GetApplicationsRequest.class),\n+        anyBoolean())).thenReturn(mockAppsResponse);\n+    ResourceManager mockRM = mock(ResourceManager.class);\n+    RMContextImpl rmContext = new RMContextImpl(null, null, null, null, null,\n+        null, null, null, null, null);\n+    when(mockRM.getRMContext()).thenReturn(rmContext);\n+    when(mockRM.getClientRMService()).thenReturn(mockClientSvc);\n+\n+    RMWebServices webSvc = new RMWebServices(mockRM, new Configuration(),\n+        mock(HttpServletResponse.class));\n+\n+    final Set<String> emptySet =\n+        Collections.unmodifiableSet(Collections.<String>emptySet());\n+\n+    // verify we don't get any apps when querying\n+    HttpServletRequest mockHsr = mock(HttpServletRequest.class);\n+    AppsInfo appsInfo = webSvc.getApps(mockHsr, null, emptySet, null,\n+        null, null, null, null, null, null, null, emptySet, emptySet);\n+    assertTrue(appsInfo.getApps().isEmpty());\n+\n+    // verify we don't get an NPE when specifying a final status query\n+    appsInfo = webSvc.getApps(mockHsr, null, emptySet, \"FAILED\",\n+        null, null, null, null, null, null, null, emptySet, emptySet);\n+    assertTrue(appsInfo.getApps().isEmpty());\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java",
                "sha": "298246ca301e2da4a545ce5ab36136bddef6e8c8",
                "status": "modified"
            }
        ],
        "message": "YARN-3143. RM Apps REST API can return NPE or entries missing id and other fields. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/5c79439568ff0c73062cf09d87f1e739703c7dc0",
        "patched_files": [
            "CHANGES.java",
            "RMWebServices.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMWebServices.java"
        ]
    },
    "hadoop_da6f1b8": {
        "bug_id": "hadoop_da6f1b8",
        "commit": "https://github.com/apache/hadoop/commit/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -1169,8 +1169,25 @@ public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid)\n    * Report a bad block which is hosted on the local DN.\n    */\n   public void reportBadBlocks(ExtendedBlock block) throws IOException{\n-    BPOfferService bpos = getBPOSForBlock(block);\n     FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    if (volume == null) {\n+      LOG.warn(\"Cannot find FsVolumeSpi to report bad block: \" + block);\n+      return;\n+    }\n+    reportBadBlocks(block, volume);\n+  }\n+\n+  /**\n+   * Report a bad block which is hosted on the local DN.\n+   *\n+   * @param block the bad block which is hosted on the local DN\n+   * @param volume the volume that block is stored in and the volume\n+   *        must not be null\n+   * @throws IOException\n+   */\n+  public void reportBadBlocks(ExtendedBlock block, FsVolumeSpi volume)\n+      throws IOException {\n+    BPOfferService bpos = getBPOSForBlock(block);\n     bpos.reportBadBlocks(\n         block, volume.getStorageID(), volume.getStorageType());\n   }\n@@ -2101,6 +2118,10 @@ public void decrementXmitsInProgress() {\n   private void reportBadBlock(final BPOfferService bpos,\n       final ExtendedBlock block, final String msg) {\n     FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    if (volume == null) {\n+      LOG.warn(\"Cannot find FsVolumeSpi to report bad block: \" + block);\n+      return;\n+    }\n     bpos.reportBadBlocks(\n         block, volume.getStorageID(), volume.getStorageType());\n     LOG.warn(msg);",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "1cd2dee669a13f62969de803d646a99ebbc14ea1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java",
                "patch": "@@ -283,7 +283,7 @@ public void handle(ExtendedBlock block, IOException e) {\n       }\n       LOG.warn(\"Reporting bad {} on {}\", block, volume.getBasePath());\n       try {\n-        scanner.datanode.reportBadBlocks(block);\n+        scanner.datanode.reportBadBlocks(block, volume);\n       } catch (IOException ie) {\n         // This is bad, but not bad enough to shut down the scanner.\n         LOG.warn(\"Cannot report bad \" + block.getBlockId(), e);",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java",
                "sha": "7a9ecf2aaa7a40542da26089064dc22953391532",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -2417,7 +2417,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n       LOG.warn(\"Reporting the block \" + corruptBlock\n           + \" as corrupt due to length mismatch\");\n       try {\n-        datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock));  \n+        datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock),\n+            memBlockInfo.getVolume());\n       } catch (IOException e) {\n         LOG.warn(\"Failed to repot bad block \" + corruptBlock, e);\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "c0f2fbdb7b14f50d5cbf7e417c65532f7f681f1d",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;\n import org.apache.hadoop.hdfs.server.common.Storage;\n import org.apache.hadoop.hdfs.server.common.StorageInfo;\n@@ -98,6 +99,7 @@\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n+\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -691,4 +693,45 @@ public void testCleanShutdownOfVolume() throws Exception {\n     cluster.shutdown();\n     }\n   }\n+\n+  @Test(timeout = 30000)\n+  public void testReportBadBlocks() throws Exception {\n+    boolean threwException = false;\n+    MiniDFSCluster cluster = null;\n+    try {\n+      Configuration config = new HdfsConfiguration();\n+      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\n+      cluster.waitActive();\n+\n+      Assert.assertEquals(0, cluster.getNamesystem().getCorruptReplicaBlocks());\n+      DataNode dataNode = cluster.getDataNodes().get(0);\n+      ExtendedBlock block =\n+          new ExtendedBlock(cluster.getNamesystem().getBlockPoolId(), 0);\n+      try {\n+        // Test the reportBadBlocks when the volume is null\n+        dataNode.reportBadBlocks(block);\n+      } catch (NullPointerException npe) {\n+        threwException = true;\n+      }\n+      Thread.sleep(3000);\n+      Assert.assertFalse(threwException);\n+      Assert.assertEquals(0, cluster.getNamesystem().getCorruptReplicaBlocks());\n+\n+      FileSystem fs = cluster.getFileSystem();\n+      Path filePath = new Path(\"testData\");\n+      DFSTestUtil.createFile(fs, filePath, 1, (short) 1, 0);\n+\n+      block = DFSTestUtil.getFirstBlock(fs, filePath);\n+      // Test for the overloaded method reportBadBlocks\n+      dataNode.reportBadBlocks(block, dataNode.getFSDataset()\n+          .getFsVolumeReferences().get(0));\n+      Thread.sleep(3000);\n+      BlockManagerTestUtil.updateState(cluster.getNamesystem()\n+          .getBlockManager());\n+      // Verify the bad block has been reported to namenode\n+      Assert.assertEquals(1, cluster.getNamesystem().getCorruptReplicaBlocks());\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "sha": "e73a6127df4bcb5ede7162dbd897957d20e25a6a",
                "status": "modified"
            }
        ],
        "message": "HDFS-10512. VolumeScanner may terminate due to NPE in DataNode.reportBadBlocks. Contributed by Wei-Chiu Chuang and Yiqun Lin.",
        "parent": "https://github.com/apache/hadoop/commit/932aed64d77edcc8483a95c1ce31a4c9ae679446",
        "patched_files": [
            "VolumeScanner.java",
            "DataNode.java",
            "FsDatasetImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_db334bb": {
        "bug_id": "hadoop_db334bb",
        "commit": "https://github.com/apache/hadoop/commit/db334bb8625da97c7e518cbcf477530c7ba7001e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -783,6 +783,9 @@ Release 2.6.1 - UNRELEASED\n     HDFS-7443. Datanode upgrade to BLOCKID_BASED_LAYOUT fails if duplicate\n     block files are present in the same volume (cmccabe)\n \n+    HDFS-3443. Fix NPE when namenode transition to active during startup by\n+    adding checkNNStartup() in NameNodeRpcServer.  (Vinayakumar B via szetszwo)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "0a301f8373328bff0268b07ae38e79dfb3eaeb92",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -79,6 +79,7 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.List;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY;\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_DEFAULT;\n@@ -265,6 +266,7 @@ public long getProtocolVersion(String protocol,\n   private final boolean haEnabled;\n   private final HAContext haContext;\n   protected final boolean allowStaleStandbyReads;\n+  private AtomicBoolean started = new AtomicBoolean(false); \n \n   \n   /** httpServer */\n@@ -775,6 +777,7 @@ protected NameNode(Configuration conf, NamenodeRole role)\n       this.stop();\n       throw e;\n     }\n+    this.started.set(true);\n   }\n \n   protected HAState createHAState(StartupOption startOpt) {\n@@ -1743,7 +1746,14 @@ public boolean isStandbyState() {\n   public boolean isActiveState() {\n     return (state.equals(ACTIVE_STATE));\n   }\n-  \n+\n+  /**\n+   * Returns whether the NameNode is completely started\n+   */\n+  boolean isStarted() {\n+    return this.started.get();\n+  }\n+\n   /**\n    * Check that a request to change this node's HA state is valid.\n    * In particular, verifies that, if auto failover is enabled, non-forced",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "fea7c62be4c2d4888ea3b63b53056c9814944557",
                "status": "modified"
            },
            {
                "additions": 151,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 36,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -69,7 +69,6 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.HDFSPolicyProvider;\n-import org.apache.hadoop.hdfs.inotify.Event;\n import org.apache.hadoop.hdfs.inotify.EventBatch;\n import org.apache.hadoop.hdfs.inotify.EventBatchList;\n import org.apache.hadoop.hdfs.protocol.AclException;\n@@ -479,12 +478,14 @@ public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size)\n       throw new IllegalArgumentException(\n         \"Unexpected not positive size: \"+size);\n     }\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getBlockManager().getBlocks(datanode, size); \n   }\n \n   @Override // NamenodeProtocol\n   public ExportedBlockKeys getBlockKeys() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getBlockManager().getBlockKeys();\n   }\n@@ -493,6 +494,7 @@ public ExportedBlockKeys getBlockKeys() throws IOException {\n   public void errorReport(NamenodeRegistration registration,\n                           int errorCode, \n                           String msg) throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     verifyRequest(registration);\n@@ -505,6 +507,7 @@ public void errorReport(NamenodeRegistration registration,\n   @Override // NamenodeProtocol\n   public NamenodeRegistration registerSubordinateNamenode(\n       NamenodeRegistration registration) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     verifyLayoutVersion(registration.getVersion());\n     NamenodeRegistration myRegistration = nn.setRegistration();\n@@ -514,7 +517,8 @@ public NamenodeRegistration registerSubordinateNamenode(\n \n   @Override // NamenodeProtocol\n   public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n-  throws IOException {\n+      throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     verifyRequest(registration);\n     if(!nn.isRole(NamenodeRole.NAMENODE))\n@@ -537,6 +541,7 @@ public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n   @Override // NamenodeProtocol\n   public void endCheckpoint(NamenodeRegistration registration,\n                             CheckpointSignature sig) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -554,18 +559,21 @@ public void endCheckpoint(NamenodeRegistration registration,\n   @Override // ClientProtocol\n   public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getDelegationToken(renewer);\n   }\n \n   @Override // ClientProtocol\n   public long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws InvalidToken, IOException {\n+    checkNNStartup();\n     return namesystem.renewDelegationToken(token);\n   }\n \n   @Override // ClientProtocol\n   public void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.cancelDelegationToken(token);\n   }\n   \n@@ -574,13 +582,15 @@ public LocatedBlocks getBlockLocations(String src,\n                                           long offset, \n                                           long length) \n       throws IOException {\n+    checkNNStartup();\n     metrics.incrGetBlockLocations();\n     return namesystem.getBlockLocations(getClientMachine(), \n                                         src, offset, length);\n   }\n   \n   @Override // ClientProtocol\n   public FsServerDefaults getServerDefaults() throws IOException {\n+    checkNNStartup();\n     return namesystem.getServerDefaults();\n   }\n \n@@ -590,6 +600,7 @@ public HdfsFileStatus create(String src, FsPermission masked,\n       boolean createParent, short replication, long blockSize, \n       CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n@@ -624,6 +635,7 @@ public HdfsFileStatus create(String src, FsPermission masked,\n   @Override // ClientProtocol\n   public LastBlockWithStatus append(String src, String clientName) \n       throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.append: file \"\n@@ -649,36 +661,42 @@ public LastBlockWithStatus append(String src, String clientName)\n \n   @Override // ClientProtocol\n   public boolean recoverLease(String src, String clientName) throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     return namesystem.recoverLease(src, clientName, clientMachine);\n   }\n \n   @Override // ClientProtocol\n   public boolean setReplication(String src, short replication) \n-    throws IOException {  \n+    throws IOException {\n+    checkNNStartup();\n     return namesystem.setReplication(src, replication);\n   }\n \n   @Override\n   public void setStoragePolicy(String src, String policyName)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setStoragePolicy(src, policyName);\n   }\n \n   @Override\n   public BlockStoragePolicy[] getStoragePolicies() throws IOException {\n+    checkNNStartup();\n     return namesystem.getStoragePolicies();\n   }\n \n   @Override // ClientProtocol\n   public void setPermission(String src, FsPermission permissions)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setPermission(src, permissions);\n   }\n \n   @Override // ClientProtocol\n   public void setOwner(String src, String username, String groupname)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setOwner(src, username, groupname);\n   }\n   \n@@ -687,6 +705,7 @@ public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes)\n       throws IOException {\n+    checkNNStartup();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId=\" + fileId + \" for \" + clientName);\n@@ -714,6 +733,7 @@ public LocatedBlock getAdditionalDatanode(final String src,\n       final DatanodeInfo[] excludes,\n       final int numAdditionalNodes, final String clientName\n       ) throws IOException {\n+    checkNNStartup();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getAdditionalDatanode: src=\" + src\n           + \", fileId=\" + fileId\n@@ -742,6 +762,7 @@ public LocatedBlock getAdditionalDatanode(final String src,\n   @Override // ClientProtocol\n   public void abandonBlock(ExtendedBlock b, long fileId, String src,\n         String holder) throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.abandonBlock: \"\n           +b+\" of file \"+src);\n@@ -755,6 +776,7 @@ public void abandonBlock(ExtendedBlock b, long fileId, String src,\n   public boolean complete(String src, String clientName,\n                           ExtendedBlock last,  long fileId)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.complete: \"\n           + src + \" fileId=\" + fileId +\" for \" + clientName);\n@@ -770,12 +792,14 @@ public boolean complete(String src, String clientName,\n    */\n   @Override // ClientProtocol, DatanodeProtocol\n   public void reportBadBlocks(LocatedBlock[] blocks) throws IOException {\n+    checkNNStartup();\n     namesystem.reportBadBlocks(blocks);\n   }\n \n   @Override // ClientProtocol\n   public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.updateBlockForPipeline(block, clientName);\n   }\n \n@@ -784,6 +808,7 @@ public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientNam\n   public void updatePipeline(String clientName, ExtendedBlock oldBlock,\n       ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs)\n       throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -805,19 +830,22 @@ public void commitBlockSynchronization(ExtendedBlock block,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.commitBlockSynchronization(block, newgenerationstamp,\n         newlength, closeFile, deleteblock, newtargets, newtargetstorages);\n   }\n   \n   @Override // ClientProtocol\n   public long getPreferredBlockSize(String filename) \n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getPreferredBlockSize(filename);\n   }\n     \n   @Deprecated\n   @Override // ClientProtocol\n   public boolean rename(String src, String dst) throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n     }\n@@ -845,6 +873,7 @@ public boolean rename(String src, String dst) throws IOException {\n   \n   @Override // ClientProtocol\n   public void concat(String trg, String[] src) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -862,6 +891,7 @@ public void concat(String trg, String[] src) throws IOException {\n   @Override // ClientProtocol\n   public void rename2(String src, String dst, Options.Rename... options)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n     }\n@@ -886,6 +916,7 @@ public void rename2(String src, String dst, Options.Rename... options)\n   @Override // ClientProtocol\n   public boolean truncate(String src, long newLength, String clientName)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.truncate: \" + src + \" to \" +\n           newLength);\n@@ -901,6 +932,7 @@ public boolean truncate(String src, long newLength, String clientName)\n \n   @Override // ClientProtocol\n   public boolean delete(String src, boolean recursive) throws IOException {\n+    checkNNStartup();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* Namenode.delete: src=\" + src\n           + \", recursive=\" + recursive);\n@@ -935,6 +967,7 @@ private boolean checkPathLength(String src) {\n   @Override // ClientProtocol\n   public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.mkdirs: \" + src);\n     }\n@@ -949,12 +982,14 @@ public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n \n   @Override // ClientProtocol\n   public void renewLease(String clientName) throws IOException {\n+    checkNNStartup();\n     namesystem.renewLease(clientName);        \n   }\n \n   @Override // ClientProtocol\n   public DirectoryListing getListing(String src, byte[] startAfter,\n       boolean needLocation) throws IOException {\n+    checkNNStartup();\n     DirectoryListing files = namesystem.getListing(\n         src, startAfter, needLocation);\n     if (files != null) {\n@@ -966,44 +1001,51 @@ public DirectoryListing getListing(String src, byte[] startAfter,\n \n   @Override // ClientProtocol\n   public HdfsFileStatus getFileInfo(String src)  throws IOException {\n+    checkNNStartup();\n     metrics.incrFileInfoOps();\n     return namesystem.getFileInfo(src, true);\n   }\n   \n   @Override // ClientProtocol\n   public boolean isFileClosed(String src) throws IOException{\n+    checkNNStartup();\n     return namesystem.isFileClosed(src);\n   }\n   \n   @Override // ClientProtocol\n-  public HdfsFileStatus getFileLinkInfo(String src) throws IOException { \n+  public HdfsFileStatus getFileLinkInfo(String src) throws IOException {\n+    checkNNStartup();\n     metrics.incrFileInfoOps();\n     return namesystem.getFileInfo(src, false);\n   }\n   \n   @Override // ClientProtocol\n   public long[] getStats() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ);\n     return namesystem.getStats();\n   }\n \n   @Override // ClientProtocol\n   public DatanodeInfo[] getDatanodeReport(DatanodeReportType type)\n   throws IOException {\n+    checkNNStartup();\n     DatanodeInfo results[] = namesystem.datanodeReport(type);\n     return results;\n   }\n     \n   @Override // ClientProtocol\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n+    checkNNStartup();\n     final DatanodeStorageReport[] reports = namesystem.getDatanodeStorageReport(type);\n     return reports;\n   }\n \n   @Override // ClientProtocol\n   public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n       throws IOException {\n+    checkNNStartup();\n     OperationCategory opCategory = OperationCategory.UNCHECKED;\n     if (isChecked) {\n       if (action == SafeModeAction.SAFEMODE_GET) {\n@@ -1018,11 +1060,13 @@ public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n \n   @Override // ClientProtocol\n   public boolean restoreFailedStorage(String arg) throws IOException { \n+    checkNNStartup();\n     return namesystem.restoreFailedStorage(arg);\n   }\n \n   @Override // ClientProtocol\n   public void saveNamespace() throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1038,50 +1082,58 @@ public void saveNamespace() throws IOException {\n   \n   @Override // ClientProtocol\n   public long rollEdits() throws AccessControlException, IOException {\n+    checkNNStartup();\n     CheckpointSignature sig = namesystem.rollEditLog();\n     return sig.getCurSegmentTxId();\n   }\n \n   @Override // ClientProtocol\n   public void refreshNodes() throws IOException {\n+    checkNNStartup();\n     namesystem.refreshNodes();\n   }\n \n   @Override // NamenodeProtocol\n   public long getTransactionID() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getFSImage().getLastAppliedOrWrittenTxId();\n   }\n   \n   @Override // NamenodeProtocol\n   public long getMostRecentCheckpointTxId() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getFSImage().getMostRecentCheckpointTxId();\n   }\n   \n   @Override // NamenodeProtocol\n   public CheckpointSignature rollEditLog() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.rollEditLog();\n   }\n   \n   @Override // NamenodeProtocol\n   public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n-  throws IOException {\n+      throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getEditLog().getEditLogManifest(sinceTxId);\n   }\n     \n   @Override // ClientProtocol\n   public void finalizeUpgrade() throws IOException {\n+    checkNNStartup();\n     namesystem.finalizeUpgrade();\n   }\n \n   @Override // ClientProtocol\n   public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException {\n+    checkNNStartup();\n     LOG.info(\"rollingUpgrade \" + action);\n     switch(action) {\n     case QUERY:\n@@ -1098,12 +1150,14 @@ public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOE\n \n   @Override // ClientProtocol\n   public void metaSave(String filename) throws IOException {\n+    checkNNStartup();\n     namesystem.metaSave(filename);\n   }\n \n   @Override // ClientProtocol\n   public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n       throws IOException {\n+    checkNNStartup();\n     String[] cookieTab = new String[] { cookie };\n     Collection<FSNamesystem.CorruptFileBlockInfo> fbs =\n       namesystem.listCorruptFileBlocks(path, cookieTab);\n@@ -1124,36 +1178,42 @@ public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n    */\n   @Override // ClientProtocol\n   public void setBalancerBandwidth(long bandwidth) throws IOException {\n+    checkNNStartup();\n     namesystem.setBalancerBandwidth(bandwidth);\n   }\n   \n   @Override // ClientProtocol\n   public ContentSummary getContentSummary(String path) throws IOException {\n+    checkNNStartup();\n     return namesystem.getContentSummary(path);\n   }\n \n   @Override // ClientProtocol\n   public void setQuota(String path, long namespaceQuota, long diskspaceQuota) \n       throws IOException {\n+    checkNNStartup();\n     namesystem.setQuota(path, namespaceQuota, diskspaceQuota);\n   }\n   \n   @Override // ClientProtocol\n   public void fsync(String src, long fileId, String clientName,\n                     long lastBlockLength)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.fsync(src, fileId, clientName, lastBlockLength);\n   }\n \n   @Override // ClientProtocol\n   public void setTimes(String src, long mtime, long atime) \n       throws IOException {\n+    checkNNStartup();\n     namesystem.setTimes(src, mtime, atime);\n   }\n \n   @Override // ClientProtocol\n   public void createSymlink(String target, String link, FsPermission dirPerms,\n       boolean createParent) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1184,6 +1244,7 @@ public void createSymlink(String target, String link, FsPermission dirPerms,\n \n   @Override // ClientProtocol\n   public String getLinkTarget(String path) throws IOException {\n+    checkNNStartup();\n     metrics.incrGetLinkTargetOps();\n     HdfsFileStatus stat = null;\n     try {\n@@ -1206,6 +1267,7 @@ public String getLinkTarget(String path) throws IOException {\n   @Override // DatanodeProtocol\n   public DatanodeRegistration registerDatanode(DatanodeRegistration nodeReg)\n       throws IOException {\n+    checkNNStartup();\n     verifySoftwareVersion(nodeReg);\n     namesystem.registerDatanode(nodeReg);\n     return nodeReg;\n@@ -1216,6 +1278,7 @@ public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n       StorageReport[] report, long dnCacheCapacity, long dnCacheUsed,\n       int xmitsInProgress, int xceiverCount,\n       int failedVolumes) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     return namesystem.handleHeartbeat(nodeReg, report,\n         dnCacheCapacity, dnCacheUsed, xceiverCount, xmitsInProgress,\n@@ -1225,6 +1288,7 @@ public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n   @Override // DatanodeProtocol\n   public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n       String poolId, StorageBlockReport[] reports) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     if(blockStateChangeLog.isDebugEnabled()) {\n       blockStateChangeLog.debug(\"*BLOCK* NameNode.blockReport: \"\n@@ -1256,6 +1320,7 @@ public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n   @Override\n   public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,\n       String poolId, List<Long> blockIds) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     if (blockStateChangeLog.isDebugEnabled()) {\n       blockStateChangeLog.debug(\"*BLOCK* NameNode.cacheReport: \"\n@@ -1268,6 +1333,7 @@ public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,\n   @Override // DatanodeProtocol\n   public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n       StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     metrics.incrBlockReceivedAndDeletedOps();\n     if(blockStateChangeLog.isDebugEnabled()) {\n@@ -1283,6 +1349,7 @@ public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n   @Override // DatanodeProtocol\n   public void errorReport(DatanodeRegistration nodeReg,\n                           int errorCode, String msg) throws IOException { \n+    checkNNStartup();\n     String dnName = \n        (nodeReg == null) ? \"Unknown DataNode\" : nodeReg.toString();\n \n@@ -1304,6 +1371,7 @@ public void errorReport(DatanodeRegistration nodeReg,\n     \n   @Override // DatanodeProtocol, NamenodeProtocol\n   public NamespaceInfo versionRequest() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getNamespaceInfo();\n   }\n@@ -1328,6 +1396,7 @@ private void verifyRequest(NodeRegistration nodeReg) throws IOException {\n \n   @Override // RefreshAuthorizationPolicyProtocol\n   public void refreshServiceAcl() throws IOException {\n+    checkNNStartup();\n     if (!serviceAuthEnabled) {\n       throw new AuthorizationException(\"Service Level Authorization not enabled!\");\n     }\n@@ -1378,28 +1447,32 @@ public void refreshCallQueue() {\n   }\n \n   @Override // HAServiceProtocol\n-  public synchronized void monitorHealth() \n-      throws HealthCheckFailedException, AccessControlException {\n+  public synchronized void monitorHealth() throws HealthCheckFailedException,\n+      AccessControlException, IOException {\n+    checkNNStartup();\n     nn.monitorHealth();\n   }\n   \n   @Override // HAServiceProtocol\n   public synchronized void transitionToActive(StateChangeRequestInfo req) \n-      throws ServiceFailedException, AccessControlException {\n+      throws ServiceFailedException, AccessControlException, IOException {\n+    checkNNStartup();\n     nn.checkHaStateChange(req);\n     nn.transitionToActive();\n   }\n   \n   @Override // HAServiceProtocol\n   public synchronized void transitionToStandby(StateChangeRequestInfo req) \n-      throws ServiceFailedException, AccessControlException {\n+      throws ServiceFailedException, AccessControlException, IOException {\n+    checkNNStartup();\n     nn.checkHaStateChange(req);\n     nn.transitionToStandby();\n   }\n \n   @Override // HAServiceProtocol\n   public synchronized HAServiceStatus getServiceStatus() \n-      throws AccessControlException, ServiceFailedException {\n+      throws AccessControlException, ServiceFailedException, IOException {\n+    checkNNStartup();\n     return nn.getServiceStatus();\n   }\n \n@@ -1456,12 +1529,14 @@ private static String getClientMachine() {\n \n   @Override\n   public DataEncryptionKey getDataEncryptionKey() throws IOException {\n+    checkNNStartup();\n     return namesystem.getBlockManager().generateDataEncryptionKey();\n   }\n \n   @Override\n   public String createSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n+    checkNNStartup();\n     if (!checkPathLength(snapshotRoot)) {\n       throw new IOException(\"createSnapshot: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n@@ -1486,6 +1561,7 @@ public String createSnapshot(String snapshotRoot, String snapshotName)\n   @Override\n   public void deleteSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n+    checkNNStartup();\n     metrics.incrDeleteSnapshotOps();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -1503,20 +1579,24 @@ public void deleteSnapshot(String snapshotRoot, String snapshotName)\n   @Override\n   // Client Protocol\n   public void allowSnapshot(String snapshotRoot) throws IOException {\n+    checkNNStartup();\n     metrics.incrAllowSnapshotOps();\n     namesystem.allowSnapshot(snapshotRoot);\n   }\n \n   @Override\n   // Client Protocol\n   public void disallowSnapshot(String snapshot) throws IOException {\n+    checkNNStartup();\n     metrics.incrDisAllowSnapshotOps();\n     namesystem.disallowSnapshot(snapshot);\n   }\n \n   @Override\n+  // ClientProtocol\n   public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n       String snapshotNewName) throws IOException {\n+    checkNNStartup();\n     if (snapshotNewName == null || snapshotNewName.isEmpty()) {\n       throw new IOException(\"The new snapshot name is null or empty.\");\n     }\n@@ -1538,24 +1618,27 @@ public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n   @Override // Client Protocol\n   public SnapshottableDirectoryStatus[] getSnapshottableDirListing()\n       throws IOException {\n+    checkNNStartup();\n     SnapshottableDirectoryStatus[] status = namesystem\n         .getSnapshottableDirListing();\n     metrics.incrListSnapshottableDirOps();\n     return status;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot,\n       String earlierSnapshotName, String laterSnapshotName) throws IOException {\n+    checkNNStartup();\n     SnapshotDiffReport report = namesystem.getSnapshotDiffReport(snapshotRoot,\n         earlierSnapshotName, laterSnapshotName);\n     metrics.incrSnapshotDiffReportOps();\n     return report;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public long addCacheDirective(\n       CacheDirectiveInfo path, EnumSet<CacheFlag> flags) throws IOException {\n+    checkNNStartup();\n     CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion\n       (retryCache, null);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -1573,9 +1656,10 @@ public long addCacheDirective(\n     return ret;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyCacheDirective(\n       CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1590,8 +1674,9 @@ public void modifyCacheDirective(\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeCacheDirective(long id) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1605,17 +1690,19 @@ public void removeCacheDirective(long id) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<CacheDirectiveEntry> listCacheDirectives(long prevId,\n       CacheDirectiveInfo filter) throws IOException {\n+    checkNNStartup();\n     if (filter == null) {\n       filter = new CacheDirectiveInfo.Builder().build();\n     }\n     return namesystem.listCacheDirectives(prevId, filter);\n   }\n \n-  @Override\n+  @Override //ClientProtocol\n   public void addCachePool(CachePoolInfo info) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1629,8 +1716,9 @@ public void addCachePool(CachePoolInfo info) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyCachePool(CachePoolInfo info) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1644,8 +1732,9 @@ public void modifyCachePool(CachePoolInfo info) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeCachePool(String cachePoolName) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1659,47 +1748,55 @@ public void removeCachePool(String cachePoolName) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<CachePoolEntry> listCachePools(String prevKey)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.listCachePools(prevKey != null ? prevKey : \"\");\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.modifyAclEntries(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClienProtocol\n   public void removeAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.removeAclEntries(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeDefaultAcl(String src) throws IOException {\n+    checkNNStartup();\n     namesystem.removeDefaultAcl(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeAcl(String src) throws IOException {\n+    checkNNStartup();\n     namesystem.removeAcl(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void setAcl(String src, List<AclEntry> aclSpec) throws IOException {\n+    checkNNStartup();\n     namesystem.setAcl(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public AclStatus getAclStatus(String src) throws IOException {\n+    checkNNStartup();\n     return namesystem.getAclStatus(src);\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public void createEncryptionZone(String src, String keyName)\n     throws IOException {\n+    checkNNStartup();\n     final CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1713,21 +1810,24 @@ public void createEncryptionZone(String src, String keyName)\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public EncryptionZone getEZForPath(String src)\n     throws IOException {\n+    checkNNStartup();\n     return namesystem.getEZForPath(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<EncryptionZone> listEncryptionZones(\n       long prevId) throws IOException {\n+    checkNNStartup();\n     return namesystem.listEncryptionZones(prevId);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n       throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1741,19 +1841,22 @@ public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n     }\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public List<XAttr> getXAttrs(String src, List<XAttr> xAttrs) \n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getXAttrs(src, xAttrs);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public List<XAttr> listXAttrs(String src) throws IOException {\n+    checkNNStartup();\n     return namesystem.listXAttrs(src);\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public void removeXAttr(String src, XAttr xAttr) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1767,13 +1870,21 @@ public void removeXAttr(String src, XAttr xAttr) throws IOException {\n     }\n   }\n \n-  @Override\n+  private void checkNNStartup() throws IOException {\n+    if (!this.nn.isStarted()) {\n+      throw new IOException(this.nn.getRole() + \" still not started\");\n+    }\n+  }\n+\n+  @Override // ClientProtocol\n   public void checkAccess(String path, FsAction mode) throws IOException {\n+    checkNNStartup();\n     namesystem.checkAccess(path, mode);\n   }\n \n   @Override // ClientProtocol\n   public long getCurrentEditLogTxid() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n     namesystem.checkSuperuserPrivilege();\n     // if it's not yet open for write, we may be in the process of transitioning\n@@ -1802,6 +1913,7 @@ private static FSEditLogOp readOp(EditLogInputStream elis)\n \n   @Override // ClientProtocol\n   public EventBatchList getEditsFromTxid(long txid) throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n     namesystem.checkSuperuserPrivilege();\n     int maxEventsPerRPC = nn.conf.getInt(\n@@ -1885,20 +1997,23 @@ public EventBatchList getEditsFromTxid(long txid) throws IOException {\n     return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public SpanReceiverInfo[] listSpanReceivers() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return nn.spanReceiverHost.listSpanReceivers();\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public long addSpanReceiver(SpanReceiverInfo info) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return nn.spanReceiverHost.addSpanReceiver(info);\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public void removeSpanReceiver(long id) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     nn.spanReceiverHost.removeSpanReceiver(id);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "a3ac455ee3ad5349298f4798020baeb3885df38f",
                "status": "modified"
            }
        ],
        "message": "HDFS-3443. Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer.  Contributed by Vinayakumar B",
        "parent": "https://github.com/apache/hadoop/commit/39c1bcf7d9c9331d25ca0aded85a293df04e0b52",
        "patched_files": [
            "NameNodeRpcServer.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop_dd57c20": {
        "bug_id": "hadoop_dd57c20",
        "commit": "https://github.com/apache/hadoop/commit/dd57c2047bfd21910acc38c98153eedf1db75169",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -319,6 +319,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-2958. Made RMStateStore not update the last sequence number when updating the\n     delegation token. (Varun Saxena via zjshen)\n \n+    YARN-2978. Fixed potential NPE while getting queue info. (Varun Saxena via\n+    jianhe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/CHANGES.txt",
                "sha": "2f7b07cfacb52efe641f2194699b1db83dd9b352",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "patch": "@@ -65,7 +65,6 @@\n   RMNodeLabelsManager labelManager;\n   String defaultLabelExpression;\n   Resource usedResources = Resources.createResource(0, 0);\n-  QueueInfo queueInfo;\n   Map<String, Float> absoluteCapacityByNodeLabels;\n   Map<String, Float> capacitiyByNodeLabels;\n   Map<String, Resource> usedResourcesByNodeLabels = new HashMap<String, Resource>();\n@@ -87,7 +86,6 @@ public AbstractCSQueue(CapacitySchedulerContext cs,\n     this.parent = parent;\n     this.queueName = queueName;\n     this.resourceCalculator = cs.getResourceCalculator();\n-    this.queueInfo = recordFactory.newRecordInstance(QueueInfo.class);\n     \n     // must be called after parent and queueName is set\n     this.metrics = old != null ? old.getMetrics() :\n@@ -99,9 +97,7 @@ public AbstractCSQueue(CapacitySchedulerContext cs,\n     this.accessibleLabels = cs.getConfiguration().getAccessibleNodeLabels(getQueuePath());\n     this.defaultLabelExpression = cs.getConfiguration()\n         .getDefaultNodeLabelExpression(getQueuePath());\n-    \n-    this.queueInfo.setQueueName(queueName);\n-    \n+\n     // inherit from parent if labels not set\n     if (this.accessibleLabels == null && parent != null) {\n       this.accessibleLabels = parent.getAccessibleNodeLabels();\n@@ -280,12 +276,6 @@ synchronized void setupQueueConfigs(Resource clusterResource, float capacity,\n     this.capacitiyByNodeLabels = new HashMap<String, Float>(nodeLabelCapacities);\n     this.maxCapacityByNodeLabels =\n         new HashMap<String, Float>(maximumNodeLabelCapacities);\n-    \n-    this.queueInfo.setAccessibleNodeLabels(this.accessibleLabels);\n-    this.queueInfo.setCapacity(this.capacity);\n-    this.queueInfo.setMaximumCapacity(this.maximumCapacity);\n-    this.queueInfo.setQueueState(this.state);\n-    this.queueInfo.setDefaultNodeLabelExpression(this.defaultLabelExpression);\n \n     // Update metrics\n     CSQueueUtils.updateQueueStatistics(\n@@ -330,6 +320,18 @@ synchronized void setupQueueConfigs(Resource clusterResource, float capacity,\n     this.reservationsContinueLooking = reservationContinueLooking;\n   }\n   \n+  protected QueueInfo getQueueInfo() {\n+    QueueInfo queueInfo = recordFactory.newRecordInstance(QueueInfo.class);\n+    queueInfo.setQueueName(queueName);\n+    queueInfo.setAccessibleNodeLabels(accessibleLabels);\n+    queueInfo.setCapacity(capacity);\n+    queueInfo.setMaximumCapacity(maximumCapacity);\n+    queueInfo.setQueueState(state);\n+    queueInfo.setDefaultNodeLabelExpression(defaultLabelExpression);\n+    queueInfo.setCurrentCapacity(getUsedCapacity());\n+    return queueInfo;\n+  }\n+  \n   @Private\n   public Resource getMaximumAllocation() {\n     return maximumAllocation;",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "sha": "fec3a567744612905cb323d27e1ac9184b60ac07",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -163,8 +163,6 @@ public LeafQueue(CapacitySchedulerContext cs,\n         CSQueueUtils.computeMaxActiveApplicationsPerUser(\n             maxActiveAppsUsingAbsCap, userLimit, userLimitFactor);\n \n-    this.queueInfo.setChildQueues(new ArrayList<QueueInfo>());\n-\n     QueueState state = cs.getConfiguration().getState(getQueuePath());\n \n     Map<QueueACL, AccessControlList> acls = \n@@ -235,14 +233,14 @@ protected synchronized void setupQueueConfigs(\n         this.defaultLabelExpression)) {\n       throw new IOException(\"Invalid default label expression of \"\n           + \" queue=\"\n-          + queueInfo.getQueueName()\n+          + getQueueName()\n           + \" doesn't have permission to access all labels \"\n           + \"in default label expression. labelExpression of resource request=\"\n           + (this.defaultLabelExpression == null ? \"\"\n               : this.defaultLabelExpression)\n           + \". Queue labels=\"\n-          + (queueInfo.getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(queueInfo\n-              .getAccessibleNodeLabels().iterator(), ',')));\n+          + (getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(\n+              getAccessibleNodeLabels().iterator(), ',')));\n     }\n     \n     this.nodeLocalityDelay = nodeLocalityDelay;\n@@ -433,7 +431,7 @@ public synchronized float getUserLimitFactor() {\n   @Override\n   public synchronized QueueInfo getQueueInfo(\n       boolean includeChildQueues, boolean recursive) {\n-    queueInfo.setCurrentCapacity(usedCapacity);\n+    QueueInfo queueInfo = getQueueInfo();\n     return queueInfo;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "dd710695f186a387c37e3f7e237f9add53372966",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "patch": "@@ -110,8 +110,6 @@ public ParentQueue(CapacitySchedulerContext cs,\n     Map<QueueACL, AccessControlList> acls = \n       cs.getConfiguration().getAcls(getQueuePath());\n \n-    this.queueInfo.setChildQueues(new ArrayList<QueueInfo>());\n-\n     setupQueueConfigs(cs.getClusterResource(), capacity, absoluteCapacity,\n         maximumCapacity, absoluteMaxCapacity, state, acls, accessibleLabels,\n         defaultLabelExpression, capacitiyByNodeLabels, maxCapacityByNodeLabels, \n@@ -206,7 +204,7 @@ public String getQueuePath() {\n   @Override\n   public synchronized QueueInfo getQueueInfo( \n       boolean includeChildQueues, boolean recursive) {\n-    queueInfo.setCurrentCapacity(usedCapacity);\n+    QueueInfo queueInfo = getQueueInfo();\n \n     List<QueueInfo> childQueuesInfo = new ArrayList<QueueInfo>();\n     if (includeChildQueues) {",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "sha": "f820ccab929a905c78ccd1cf8422fb78f58d82e4",
                "status": "modified"
            }
        ],
        "message": "YARN-2978. Fixed potential NPE while getting queue info. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/d02fb53750bc592c23ba470ae82eb6f47d9a00ec",
        "patched_files": [
            "LeafQueue.java",
            "ParentQueue.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLeafQueue.java",
            "TestParentQueue.java"
        ]
    },
    "hadoop_dd852f5": {
        "bug_id": "hadoop_dd852f5",
        "commit": "https://github.com/apache/hadoop/commit/dd852f5b8c8fe9e52d15987605f36b5b60f02701",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=dd852f5b8c8fe9e52d15987605f36b5b60f02701",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -153,6 +153,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-3110. Few issues in ApplicationHistory web ui. (Naganarasimha G R via xgong)\n \n+    YARN-3457. NPE when NodeManager.serviceInit fails and stopRecoveryStore called.\n+    (Bibin A Chundatt via ozawa)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/CHANGES.txt",
                "sha": "d5f6ce0c4e4a12079e5ca90e6d8a1ca6e42f7885",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=dd852f5b8c8fe9e52d15987605f36b5b60f02701",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -177,16 +177,18 @@ private void initAndStartRecoveryStore(Configuration conf)\n \n   private void stopRecoveryStore() throws IOException {\n     nmStore.stop();\n-    if (context.getDecommissioned() && nmStore.canRecover()) {\n-      LOG.info(\"Removing state store due to decommission\");\n-      Configuration conf = getConfig();\n-      Path recoveryRoot = new Path(\n-          conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n-      LOG.info(\"Removing state store at \" + recoveryRoot\n-          + \" due to decommission\");\n-      FileSystem recoveryFs = FileSystem.getLocal(conf);\n-      if (!recoveryFs.delete(recoveryRoot, true)) {\n-        LOG.warn(\"Unable to delete \" + recoveryRoot);\n+    if (null != context) {\n+      if (context.getDecommissioned() && nmStore.canRecover()) {\n+        LOG.info(\"Removing state store due to decommission\");\n+        Configuration conf = getConfig();\n+        Path recoveryRoot =\n+            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n+        LOG.info(\"Removing state store at \" + recoveryRoot\n+            + \" due to decommission\");\n+        FileSystem recoveryFs = FileSystem.getLocal(conf);\n+        if (!recoveryFs.delete(recoveryRoot, true)) {\n+          LOG.warn(\"Unable to delete \" + recoveryRoot);\n+        }\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "9831fc42c46a9ece18a7987e91c69f0fbfc12aa4",
                "status": "modified"
            }
        ],
        "message": "YARN-3457. NPE when NodeManager.serviceInit fails and stopRecoveryStore called. Contributed by Bibin A Chundatt.",
        "parent": "https://github.com/apache/hadoop/commit/ab04ff9efe632b4eca6faca7407ac35e00e6a379",
        "patched_files": [
            "NodeManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_df76cdc": {
        "bug_id": "hadoop_df76cdc",
        "commit": "https://github.com/apache/hadoop/commit/df76cdc8959c51b71704ab5c38335f745a6f35d8",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/df76cdc8959c51b71704ab5c38335f745a6f35d8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TimelineServiceV2Publisher.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TimelineServiceV2Publisher.java?ref=df76cdc8959c51b71704ab5c38335f745a6f35d8",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TimelineServiceV2Publisher.java",
                "patch": "@@ -473,10 +473,15 @@ private void putEntity(TimelineEntity entity, ApplicationId appId) {\n       }\n       TimelineCollector timelineCollector =\n           rmTimelineCollectorManager.get(appId);\n-      TimelineEntities entities = new TimelineEntities();\n-      entities.addEntity(entity);\n-      timelineCollector.putEntities(entities,\n-          UserGroupInformation.getCurrentUser());\n+      if (timelineCollector != null) {\n+        TimelineEntities entities = new TimelineEntities();\n+        entities.addEntity(entity);\n+        timelineCollector.putEntities(entities,\n+                UserGroupInformation.getCurrentUser());\n+      } else {\n+        LOG.debug(\"Cannot find active collector while publishing entity \"\n+            + entity);\n+      }\n     } catch (IOException e) {\n       LOG.error(\"Error when publishing entity \" + entity);\n       LOG.debug(\"Error when publishing entity {}\", entity, e);",
                "raw_url": "https://github.com/apache/hadoop/raw/df76cdc8959c51b71704ab5c38335f745a6f35d8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TimelineServiceV2Publisher.java",
                "sha": "eeeed48bbac5e9ed3a9bb196be9b26572e2811f8",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/hadoop/blob/df76cdc8959c51b71704ab5c38335f745a6f35d8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisherForV2.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisherForV2.java?ref=df76cdc8959c51b71704ab5c38335f745a6f35d8",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisherForV2.java",
                "patch": "@@ -28,12 +28,17 @@\n import java.io.File;\n import java.io.FileReader;\n import java.io.IOException;\n+import java.util.ArrayList;\n import java.util.Collections;\n import java.util.HashMap;\n+import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n \n+import org.apache.log4j.AppenderSkeleton;\n+import org.apache.log4j.Logger;\n+import org.apache.log4j.spi.LoggingEvent;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileContext;\n import org.apache.hadoop.fs.Path;\n@@ -293,6 +298,48 @@ public void testPublishContainerMetrics() throws Exception {\n         TimelineServiceHelper.invertLong(containerId.getContainerId()));\n   }\n \n+  @Test(timeout = 10000)\n+  public void testPutEntityWhenNoCollector() throws Exception {\n+    // Validating the logs as DrainDispatcher won't throw exception\n+    class TestAppender extends AppenderSkeleton {\n+      private final List<LoggingEvent> log = new ArrayList<>();\n+\n+      @Override\n+      public boolean requiresLayout() {\n+        return false;\n+      }\n+\n+      @Override\n+      protected void append(final LoggingEvent loggingEvent) {\n+        log.add(loggingEvent);\n+      }\n+\n+      @Override\n+      public void close() {\n+      }\n+\n+      public List<LoggingEvent> getLog() {\n+        return new ArrayList<>(log);\n+      }\n+    }\n+\n+    TestAppender appender = new TestAppender();\n+    final Logger logger = Logger.getRootLogger();\n+    logger.addAppender(appender);\n+\n+    try {\n+      RMApp app = createRMApp(ApplicationId.newInstance(0, 1));\n+      metricsPublisher.appCreated(app, app.getStartTime());\n+      dispatcher.await();\n+      for (LoggingEvent event : appender.getLog()) {\n+        assertFalse(\"Dispatcher Crashed\",\n+            event.getRenderedMessage().contains(\"Error in dispatcher thread\"));\n+      }\n+    } finally {\n+      logger.removeAppender(appender);\n+    }\n+  }\n+\n   private RMApp createAppAndRegister(ApplicationId appId) {\n     RMApp app = createRMApp(appId);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/df76cdc8959c51b71704ab5c38335f745a6f35d8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisherForV2.java",
                "sha": "76e8f0ee8cf93cdeeb3b691c471d1f393e3ef630",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/df76cdc8959c51b71704ab5c38335f745a6f35d8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/TimelineServiceV2.md",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/TimelineServiceV2.md?ref=df76cdc8959c51b71704ab5c38335f745a6f35d8",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/TimelineServiceV2.md",
                "patch": "@@ -330,14 +330,6 @@ Following are the basic configurations to start Timeline service v.2:\n   <name>yarn.system-metrics-publisher.enabled</name>\n   <value>true</value>\n </property>\n-\n-<property>\n-  <description>The setting that controls whether yarn container events are\n-  published to the timeline service or not by RM. This configuration setting\n-  is for ATS V2.</description>\n-  <name>yarn.rm.system-metrics-publisher.emit-container-events</name>\n-  <value>true</value>\n-</property>\n ```\n \n If using an aux services manifest instead of setting aux services through the Configuration, ensure that the manifest services array includes the timeline\\_collector service as follows:",
                "raw_url": "https://github.com/apache/hadoop/raw/df76cdc8959c51b71704ab5c38335f745a6f35d8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/TimelineServiceV2.md",
                "sha": "b75a9d4b3d6deed54a086b8bb87b7ffdcb264a60",
                "status": "modified"
            }
        ],
        "message": "YARN-6695. Fixed NPE in publishing appFinished events to ATSv2.\n           Contributed by Prabhu Joseph",
        "parent": "https://github.com/apache/hadoop/commit/b979fdde9907c36fd7da000b9fd64144d61b4306",
        "patched_files": [
            "TimelineServiceV2Publisher.java",
            "TimelineServiceV2.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestSystemMetricsPublisherForV2.java"
        ]
    },
    "hadoop_e111789": {
        "bug_id": "hadoop_e111789",
        "commit": "https://github.com/apache/hadoop/commit/e111789aeb005c76e443c96418cd9fddf9bdb8a2",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/e111789aeb005c76e443c96418cd9fddf9bdb8a2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java?ref=e111789aeb005c76e443c96418cd9fddf9bdb8a2",
                "deletions": 2,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "patch": "@@ -486,8 +486,11 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n         final List<DatanodeDetails> excludeList = replicas.stream()\n             .map(ContainerReplica::getDatanodeDetails)\n             .collect(Collectors.toList());\n-        inflightReplication.get(id).stream().map(r -> r.datanode)\n-            .forEach(excludeList::add);\n+        List<InflightAction> actionList = inflightReplication.get(id);\n+        if (actionList != null) {\n+          actionList.stream().map(r -> r.datanode)\n+              .forEach(excludeList::add);\n+        }\n         final List<DatanodeDetails> selectedDatanodes = containerPlacement\n             .chooseDatanodes(excludeList, null, delta,\n                 container.getUsedBytes());",
                "raw_url": "https://github.com/apache/hadoop/raw/e111789aeb005c76e443c96418cd9fddf9bdb8a2/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "sha": "a8dff405e4386a30cbcaf1d3b94dd1b3977b8fb2",
                "status": "modified"
            }
        ],
        "message": "HDDS-1882. TestReplicationManager failed with NPE. (#1197)",
        "parent": "https://github.com/apache/hadoop/commit/a7371a779c591893700df1df279330589474960c",
        "patched_files": [
            "ReplicationManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestReplicationManager.java"
        ]
    },
    "hadoop_e188bb1": {
        "bug_id": "hadoop_e188bb1",
        "commit": "https://github.com/apache/hadoop/commit/e188bb12b0e715ab623e4e803aa5e69f381a99ce",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java?ref=e188bb12b0e715ab623e4e803aa5e69f381a99ce",
                "deletions": 13,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "patch": "@@ -30,7 +30,7 @@\n import org.apache.hadoop.hdds.scm.pipeline.PipelineID;\n import org.apache.hadoop.hdds.scm.node.states.NodeAlreadyExistsException;\n import org.apache.hadoop.hdds.scm.node.states.NodeNotFoundException;\n-import org.apache.hadoop.hdds.scm.server.StorageContainerManager;\n+import org.apache.hadoop.hdds.scm.server.SCMStorageConfig;\n import org.apache.hadoop.hdds.scm.VersionInfo;\n import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMNodeMetric;\n import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMNodeStat;\n@@ -94,32 +94,30 @@\n       LoggerFactory.getLogger(SCMNodeManager.class);\n \n   private final NodeStateManager nodeStateManager;\n-  private final String clusterID;\n   private final VersionInfo version;\n   private final CommandQueue commandQueue;\n   private final SCMNodeMetrics metrics;\n   // Node manager MXBean\n   private ObjectName nmInfoBean;\n-  private final StorageContainerManager scmManager;\n+  private final SCMStorageConfig scmStorageConfig;\n   private final NetworkTopology clusterMap;\n   private final DNSToSwitchMapping dnsToSwitchMapping;\n   private final boolean useHostname;\n \n   /**\n    * Constructs SCM machine Manager.\n    */\n-  public SCMNodeManager(OzoneConfiguration conf, String clusterID,\n-      StorageContainerManager scmManager, EventPublisher eventPublisher)\n-      throws IOException {\n+  public SCMNodeManager(OzoneConfiguration conf,\n+      SCMStorageConfig scmStorageConfig, EventPublisher eventPublisher,\n+      NetworkTopology networkTopology) {\n     this.nodeStateManager = new NodeStateManager(conf, eventPublisher);\n-    this.clusterID = clusterID;\n     this.version = VersionInfo.getLatestVersion();\n     this.commandQueue = new CommandQueue();\n-    this.scmManager = scmManager;\n+    this.scmStorageConfig = scmStorageConfig;\n     LOG.info(\"Entering startup safe mode.\");\n     registerMXBean();\n     this.metrics = SCMNodeMetrics.create(this);\n-    this.clusterMap = scmManager.getClusterMap();\n+    this.clusterMap = networkTopology;\n     Class<? extends DNSToSwitchMapping> dnsToSwitchMappingClass =\n         conf.getClass(DFSConfigKeys.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n             TableMapping.class, DNSToSwitchMapping.class);\n@@ -221,9 +219,8 @@ public VersionResponse getVersion(SCMVersionRequestProto versionRequest) {\n     return VersionResponse.newBuilder()\n         .setVersion(this.version.getVersion())\n         .addValue(OzoneConsts.SCM_ID,\n-            this.scmManager.getScmStorageConfig().getScmId())\n-        .addValue(OzoneConsts.CLUSTER_ID, this.scmManager.getScmStorageConfig()\n-            .getClusterID())\n+            this.scmStorageConfig.getScmId())\n+        .addValue(OzoneConsts.CLUSTER_ID, this.scmStorageConfig.getClusterID())\n         .build();\n   }\n \n@@ -274,7 +271,7 @@ public RegisteredCommand register(\n \n     return RegisteredCommand.newBuilder().setErrorCode(ErrorCode.success)\n         .setDatanodeUUID(datanodeDetails.getUuidString())\n-        .setClusterID(this.clusterID)\n+        .setClusterID(this.scmStorageConfig.getClusterID())\n         .setHostname(datanodeDetails.getHostName())\n         .setIpAddress(datanodeDetails.getIpAddress())\n         .build();",
                "raw_url": "https://github.com/apache/hadoop/raw/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/SCMNodeManager.java",
                "sha": "eaa2255cb0db360b4ef57b1aa0d1a8576b3a4ebc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java?ref=e188bb12b0e715ab623e4e803aa5e69f381a99ce",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java",
                "patch": "@@ -378,7 +378,7 @@ private void initializeSystemManagers(OzoneConfiguration conf,\n       scmNodeManager = configurator.getScmNodeManager();\n     } else {\n       scmNodeManager = new SCMNodeManager(\n-          conf, scmStorageConfig.getClusterID(), this, eventQueue);\n+          conf, scmStorageConfig, eventQueue, clusterMap);\n     }\n \n     ContainerPlacementPolicy containerPlacementPolicy =",
                "raw_url": "https://github.com/apache/hadoop/raw/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java",
                "sha": "08712ccdbc5b1f13437910865ee591a6df119a20",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java?ref=e188bb12b0e715ab623e4e803aa5e69f381a99ce",
                "deletions": 2,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java",
                "patch": "@@ -36,6 +36,7 @@\n import org.apache.hadoop.hdds.scm.events.SCMEvents;\n import org.apache.hadoop.hdds.scm.pipeline.PipelineManager;\n import org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager;\n+import org.apache.hadoop.hdds.scm.server.SCMStorageConfig;\n import org.apache.hadoop.hdds.server.events.EventQueue;\n import org.apache.hadoop.ozone.OzoneConsts;\n import org.apache.hadoop.test.PathUtils;\n@@ -48,7 +49,6 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.List;\n-import java.util.UUID;\n import java.util.concurrent.TimeoutException;\n \n import static org.apache.hadoop.hdds.scm.ScmConfigKeys\n@@ -94,8 +94,12 @@ SCMNodeManager createNodeManager(OzoneConfiguration config)\n         Mockito.mock(StaleNodeHandler.class));\n     eventQueue.addHandler(SCMEvents.DEAD_NODE,\n         Mockito.mock(DeadNodeHandler.class));\n+\n+    SCMStorageConfig storageConfig = Mockito.mock(SCMStorageConfig.class);\n+    Mockito.when(storageConfig.getClusterID()).thenReturn(\"cluster1\");\n+\n     SCMNodeManager nodeManager = new SCMNodeManager(config,\n-        UUID.randomUUID().toString(), null, eventQueue);\n+        storageConfig, eventQueue, null);\n     return nodeManager;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestContainerPlacement.java",
                "sha": "ec0c4c3447042e9211be2ed1d28d21c4ff01f623",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java?ref=e188bb12b0e715ab623e4e803aa5e69f381a99ce",
                "deletions": 1,
                "filename": "hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java",
                "patch": "@@ -24,14 +24,17 @@\n import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.StorageReportProto;\n import org.apache.hadoop.hdds.scm.TestUtils;\n import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMNodeMetric;\n+import org.apache.hadoop.hdds.scm.net.NetworkTopology;\n import org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher.NodeReportFromDatanode;\n+import org.apache.hadoop.hdds.scm.server.SCMStorageConfig;\n import org.apache.hadoop.hdds.server.events.Event;\n import org.apache.hadoop.hdds.server.events.EventPublisher;\n import org.apache.hadoop.hdds.server.events.EventQueue;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n+import org.mockito.Mockito;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -50,7 +53,11 @@\n   @Before\n   public void resetEventCollector() throws IOException {\n     OzoneConfiguration conf = new OzoneConfiguration();\n-    nodeManager = new SCMNodeManager(conf, \"cluster1\", null, new EventQueue());\n+    SCMStorageConfig storageConfig = Mockito.mock(SCMStorageConfig.class);\n+    Mockito.when(storageConfig.getClusterID()).thenReturn(\"cluster1\");\n+    nodeManager =\n+        new SCMNodeManager(conf, storageConfig, new EventQueue(), Mockito.mock(\n+            NetworkTopology.class));\n     nodeReportHandler = new NodeReportHandler(nodeManager);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/e188bb12b0e715ab623e4e803aa5e69f381a99ce/hadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/node/TestNodeReportHandler.java",
                "sha": "88de27d996507e937a0d9d511d4bb889b57601eb",
                "status": "modified"
            }
        ],
        "message": "HDDS-1694. TestNodeReportHandler is failing with NPE\n\nCloses #978",
        "parent": "https://github.com/apache/hadoop/commit/dd4a7633ece11c528a58146f92522b55e7be4dc6",
        "patched_files": [
            "SCMNodeManager.java",
            "StorageContainerManager.java",
            "NodeReportHandler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerPlacement.java",
            "TestStorageContainerManager.java",
            "TestNodeReportHandler.java",
            "TestSCMNodeManager.java"
        ]
    },
    "hadoop_e276c75": {
        "bug_id": "hadoop_e276c75",
        "commit": "https://github.com/apache/hadoop/commit/e276c75ec17634fc3b521fdb15b6ac141b001274",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/CollectorInfoPBImpl.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/CollectorInfoPBImpl.java?ref=e276c75ec17634fc3b521fdb15b6ac141b001274",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/CollectorInfoPBImpl.java",
                "patch": "@@ -114,9 +114,13 @@ public void setCollectorAddr(String addr) {\n   @Override\n   public Token getCollectorToken() {\n     CollectorInfoProtoOrBuilder p = viaProto ? proto : builder;\n-    if (this.collectorToken == null && p.hasCollectorToken()) {\n-      this.collectorToken = convertFromProtoFormat(p.getCollectorToken());\n+    if (this.collectorToken != null) {\n+      return this.collectorToken;\n+    }\n+    if (!p.hasCollectorToken()) {\n+      return null;\n     }\n+    this.collectorToken = convertFromProtoFormat(p.getCollectorToken());\n     return this.collectorToken;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/CollectorInfoPBImpl.java",
                "sha": "5835d1a2b006898dc8ba6c5657cecdfc579484c7",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java?ref=e276c75ec17634fc3b521fdb15b6ac141b001274",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "patch": "@@ -168,12 +168,17 @@ private void addRegisteringCollectorsToProto() {\n     for (Map.Entry<ApplicationId, AppCollectorData> entry :\n         registeringCollectors.entrySet()) {\n       AppCollectorData data = entry.getValue();\n-      builder.addRegisteringCollectors(AppCollectorDataProto.newBuilder()\n-          .setAppId(convertToProtoFormat(entry.getKey()))\n-          .setAppCollectorAddr(data.getCollectorAddr())\n-          .setAppCollectorToken(convertToProtoFormat(data.getCollectorToken()))\n-          .setRmIdentifier(data.getRMIdentifier())\n-          .setVersion(data.getVersion()));\n+      AppCollectorDataProto.Builder appCollectorDataBuilder =\n+          AppCollectorDataProto.newBuilder()\n+              .setAppId(convertToProtoFormat(entry.getKey()))\n+              .setAppCollectorAddr(data.getCollectorAddr())\n+              .setRmIdentifier(data.getRMIdentifier())\n+              .setVersion(data.getVersion());\n+      if (data.getCollectorToken() != null) {\n+        appCollectorDataBuilder.setAppCollectorToken(\n+            convertToProtoFormat(data.getCollectorToken()));\n+      }\n+      builder.addRegisteringCollectors(appCollectorDataBuilder);\n     }\n   }\n \n@@ -274,7 +279,10 @@ private void initRegisteredCollectors() {\n       this.registeringCollectors = new HashMap<>();\n       for (AppCollectorDataProto c : list) {\n         ApplicationId appId = convertFromProtoFormat(c.getAppId());\n-        Token collectorToken = convertFromProtoFormat(c.getAppCollectorToken());\n+        Token collectorToken = null;\n+        if (c.hasAppCollectorToken()){\n+          collectorToken = convertFromProtoFormat(c.getAppCollectorToken());\n+        }\n         AppCollectorData data = AppCollectorData.newInstance(appId,\n             c.getAppCollectorAddr(), c.getRmIdentifier(), c.getVersion(),\n             collectorToken);",
                "raw_url": "https://github.com/apache/hadoop/raw/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "sha": "1ffd223f8a67414b1c19329dc7e6a99151bc3d2a",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatResponsePBImpl.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatResponsePBImpl.java?ref=e276c75ec17634fc3b521fdb15b6ac141b001274",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatResponsePBImpl.java",
                "patch": "@@ -154,13 +154,17 @@ private void addAppCollectorsMapToProto() {\n     for (Map.Entry<ApplicationId, AppCollectorData> entry\n         : appCollectorsMap.entrySet()) {\n       AppCollectorData data = entry.getValue();\n-      builder.addAppCollectors(AppCollectorDataProto.newBuilder()\n-          .setAppId(convertToProtoFormat(entry.getKey()))\n-          .setAppCollectorAddr(data.getCollectorAddr())\n-          .setAppCollectorToken(\n-              convertToProtoFormat(entry.getValue().getCollectorToken()))\n-          .setRmIdentifier(data.getRMIdentifier())\n-          .setVersion(data.getVersion()));\n+      AppCollectorDataProto.Builder appCollectorDataBuilder =\n+          AppCollectorDataProto.newBuilder()\n+              .setAppId(convertToProtoFormat(entry.getKey()))\n+              .setAppCollectorAddr(data.getCollectorAddr())\n+              .setRmIdentifier(data.getRMIdentifier())\n+              .setVersion(data.getVersion());\n+      if (data.getCollectorToken() != null) {\n+        appCollectorDataBuilder.setAppCollectorToken(\n+            convertToProtoFormat(data.getCollectorToken()));\n+      }\n+      builder.addAppCollectors(appCollectorDataBuilder);\n     }\n   }\n \n@@ -604,7 +608,10 @@ private void initAppCollectorsMap() {\n       this.appCollectorsMap = new HashMap<>();\n       for (AppCollectorDataProto c : list) {\n         ApplicationId appId = convertFromProtoFormat(c.getAppId());\n-        Token collectorToken = convertFromProtoFormat(c.getAppCollectorToken());\n+        Token collectorToken = null;\n+        if (c.hasAppCollectorToken()){\n+          collectorToken = convertFromProtoFormat(c.getAppCollectorToken());\n+        }\n         AppCollectorData data = AppCollectorData.newInstance(appId,\n             c.getAppCollectorAddr(), c.getRmIdentifier(), c.getVersion(),\n             collectorToken);",
                "raw_url": "https://github.com/apache/hadoop/raw/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatResponsePBImpl.java",
                "sha": "11f5f61416f9aa19ccfb507483eafdeaee839013",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/impl/pb/AppCollectorDataPBImpl.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/impl/pb/AppCollectorDataPBImpl.java?ref=e276c75ec17634fc3b521fdb15b6ac141b001274",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/impl/pb/AppCollectorDataPBImpl.java",
                "patch": "@@ -163,9 +163,13 @@ public void setVersion(long version) {\n   @Override\n   public Token getCollectorToken() {\n     AppCollectorDataProtoOrBuilder p = viaProto ? proto : builder;\n-    if (this.collectorToken == null && p.hasAppCollectorToken()) {\n-      this.collectorToken = new TokenPBImpl(p.getAppCollectorToken());\n+    if (this.collectorToken != null) {\n+      return this.collectorToken;\n+    }\n+    if (!p.hasAppCollectorToken()) {\n+      return null;\n     }\n+    this.collectorToken = new TokenPBImpl(p.getAppCollectorToken());\n     return this.collectorToken;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/records/impl/pb/AppCollectorDataPBImpl.java",
                "sha": "c08e9ca06064bdfffffd3665c503ef1435e2e1ff",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/TestYarnServerApiClasses.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/TestYarnServerApiClasses.java?ref=e276c75ec17634fc3b521fdb15b6ac141b001274",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/TestYarnServerApiClasses.java",
                "patch": "@@ -111,7 +111,7 @@ public void testNodeHeartbeatRequestPBImpl() {\n     original.setLastKnownNMTokenMasterKey(getMasterKey());\n     original.setNodeStatus(getNodeStatus());\n     original.setNodeLabels(getValidNodeLabels());\n-    Map<ApplicationId, AppCollectorData> collectors = getCollectors();\n+    Map<ApplicationId, AppCollectorData> collectors = getCollectors(false);\n     original.setRegisteringCollectors(collectors);\n     NodeHeartbeatRequestPBImpl copy = new NodeHeartbeatRequestPBImpl(\n         original.getProto());\n@@ -130,6 +130,16 @@ public void testNodeHeartbeatRequestPBImpl() {\n     Assert.assertEquals(0, copy.getNodeLabels().size());\n   }\n \n+  @Test\n+  public void testNodeHBRequestPBImplWithNullCollectorToken() {\n+    NodeHeartbeatRequestPBImpl original = new NodeHeartbeatRequestPBImpl();\n+    Map<ApplicationId, AppCollectorData> collectors = getCollectors(true);\n+    original.setRegisteringCollectors(collectors);\n+    NodeHeartbeatRequestPBImpl copy = new NodeHeartbeatRequestPBImpl(\n+        original.getProto());\n+    assertEquals(collectors, copy.getRegisteringCollectors());\n+  }\n+\n   /**\n    * Test NodeHeartbeatRequestPBImpl.\n    */\n@@ -155,7 +165,7 @@ public void testNodeHeartbeatResponsePBImpl() {\n     original.setNextHeartBeatInterval(1000);\n     original.setNodeAction(NodeAction.NORMAL);\n     original.setResponseId(100);\n-    Map<ApplicationId, AppCollectorData> collectors = getCollectors();\n+    Map<ApplicationId, AppCollectorData> collectors = getCollectors(false);\n     original.setAppCollectors(collectors);\n \n     NodeHeartbeatResponsePBImpl copy = new NodeHeartbeatResponsePBImpl(\n@@ -179,6 +189,16 @@ public void testNodeHeartbeatResponsePBImplWithRMAcceptLbls() {\n     assertTrue(copy.getAreNodeLabelsAcceptedByRM());\n   }\n \n+  @Test\n+  public void testNodeHBResponsePBImplWithNullCollectorToken() {\n+    NodeHeartbeatResponsePBImpl original = new NodeHeartbeatResponsePBImpl();\n+    Map<ApplicationId, AppCollectorData> collectors = getCollectors(true);\n+    original.setAppCollectors(collectors);\n+    NodeHeartbeatResponsePBImpl copy = new NodeHeartbeatResponsePBImpl(\n+        original.getProto());\n+    assertEquals(collectors, copy.getAppCollectors());\n+  }\n+\n   @Test\n   public void testNodeHeartbeatResponsePBImplWithDecreasedContainers() {\n     NodeHeartbeatResponsePBImpl original = new NodeHeartbeatResponsePBImpl();\n@@ -349,11 +369,15 @@ public void testUnRegisterNodeManagerRequestPBImpl() throws Exception {\n     return nodeLabels;\n   }\n \n-  private Map<ApplicationId, AppCollectorData> getCollectors() {\n+  private Map<ApplicationId, AppCollectorData> getCollectors(\n+      boolean hasNullCollectorToken) {\n     ApplicationId appID = ApplicationId.newInstance(1L, 1);\n     String collectorAddr = \"localhost:0\";\n-    AppCollectorData data = AppCollectorData.newInstance(appID, collectorAddr,\n-        Token.newInstance(new byte[0], \"kind\", new byte[0], \"s\"));\n+    AppCollectorData data = AppCollectorData.newInstance(appID, collectorAddr);\n+    if (!hasNullCollectorToken) {\n+      data.setCollectorToken(\n+          Token.newInstance(new byte[0], \"kind\", new byte[0], \"s\"));\n+    }\n     Map<ApplicationId, AppCollectorData> collectorMap =\n         new HashMap<>();\n     collectorMap.put(appID, data);",
                "raw_url": "https://github.com/apache/hadoop/raw/e276c75ec17634fc3b521fdb15b6ac141b001274/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/TestYarnServerApiClasses.java",
                "sha": "8b1d0bb49e51608f7c7765807539bc5836b40ec6",
                "status": "modified"
            }
        ],
        "message": "YARN-7041. Nodemanager NPE running jobs with security off. Contributed by Varun Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/32188d32954d94ec2efeec2b2fcc5b2abff4c1ea",
        "patched_files": [
            "NodeHeartbeatResponsePBImpl.java",
            "CollectorInfoPBImpl.java",
            "NodeHeartbeatRequestPBImpl.java",
            "AppCollectorDataPBImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestYarnServerApiClasses.java"
        ]
    },
    "hadoop_e4b4901": {
        "bug_id": "hadoop_e4b4901",
        "commit": "https://github.com/apache/hadoop/commit/e4b4901d36875faa98ec8628e22e75499e0741ab",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=e4b4901d36875faa98ec8628e22e75499e0741ab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -872,6 +872,8 @@ Release 2.6.0 - UNRELEASED\n     FatalEventDispatcher try to transition RM to StandBy at the same time.\n     (Rohith Sharmaks via jianhe)\n \n+    YARN-2813. Fixed NPE from MemoryTimelineStore.getDomains. (Zhijie Shen via xgong)\n+\n Release 2.5.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/CHANGES.txt",
                "sha": "d65860cdb25f2de7163fe95161d55a6abe0d59e2",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java?ref=e4b4901d36875faa98ec8628e22e75499e0741ab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java",
                "patch": "@@ -241,6 +241,10 @@ public TimelineDomain getDomain(String domainId)\n   public TimelineDomains getDomains(String owner)\n       throws IOException {\n     List<TimelineDomain> domains = new ArrayList<TimelineDomain>();\n+    Set<TimelineDomain> domainsOfOneOwner = domainsByOwner.get(owner);\n+    if (domainsOfOneOwner == null) {\n+      return new TimelineDomains();\n+    }\n     for (TimelineDomain domain : domainsByOwner.get(owner)) {\n       TimelineDomain domainToReturn = createTimelineDomain(\n           domain.getId(),",
                "raw_url": "https://github.com/apache/hadoop/raw/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java",
                "sha": "af714b17b800f55c8c2c1cf07af7287df747fcaa",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java?ref=e4b4901d36875faa98ec8628e22e75499e0741ab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "patch": "@@ -946,6 +946,10 @@ public void testGetDomains() throws IOException {\n     assertEquals(2, actualDomains.getDomains().size());\n     verifyDomainInfo(domain3, actualDomains.getDomains().get(0));\n     verifyDomainInfo(domain1, actualDomains.getDomains().get(1));\n+\n+    // owner without any domain\n+    actualDomains = store.getDomains(\"owner_4\");\n+    assertEquals(0, actualDomains.getDomains().size());\n   }\n \n   private static void verifyDomainInfo(",
                "raw_url": "https://github.com/apache/hadoop/raw/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "sha": "242478cafa98322e0479119af52b519a5fcfeaf2",
                "status": "modified"
            }
        ],
        "message": "YARN-2813. Fixed NPE from MemoryTimelineStore.getDomains. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/ef5af4f8de91fbe7891ae3471eb03397e74e1811",
        "patched_files": [
            "MemoryTimelineStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestMemoryTimelineStore.java"
        ]
    },
    "hadoop_e71f61e": {
        "bug_id": "hadoop_e71f61e",
        "commit": "https://github.com/apache/hadoop/commit/e71f61ecb87e04727a5a76e578a75714c9db6706",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java?ref=e71f61ecb87e04727a5a76e578a75714c9db6706",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "patch": "@@ -743,7 +743,7 @@ public static String createStartupShutdownMessage(String classname,\n     return toStartupShutdownString(\"STARTUP_MSG: \", new String[] {\n         \"Starting \" + classname,\n         \"  host = \" + hostname,\n-        \"  args = \" + Arrays.asList(args),\n+        \"  args = \" + (args != null ? Arrays.asList(args) : new ArrayList<>()),\n         \"  version = \" + VersionInfo.getVersion(),\n         \"  classpath = \" + System.getProperty(\"java.class.path\"),\n         \"  build = \" + VersionInfo.getUrl() + \" -r \"",
                "raw_url": "https://github.com/apache/hadoop/raw/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "sha": "f49698ca5ac1b09b37f584998b248ac2eac83dd1",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java?ref=e71f61ecb87e04727a5a76e578a75714c9db6706",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "patch": "@@ -503,6 +503,15 @@ public void testEscapeHTML() {\n         escapedStr, StringUtils.escapeHTML(htmlStr));\n   }\n \n+  @Test\n+  public void testCreateStartupShutdownMessage() {\n+    //pass null args and method must still return a string beginning with\n+    // \"STARTUP_MSG\"\n+    String msg = StringUtils.createStartupShutdownMessage(\n+        this.getClass().getName(), \"test.host\", null);\n+    assertTrue(msg.startsWith(\"STARTUP_MSG:\"));\n+  }\n+\n   // Benchmark for StringUtils split\n   public static void main(String []args) {\n     final String TO_SPLIT = \"foo,bar,baz,blah,blah\";",
                "raw_url": "https://github.com/apache/hadoop/raw/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "sha": "f05b5895676060d84087b13f89751fc3893b29db",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15755. StringUtils#createStartupShutdownMessage throws NPE when args is null. Contributed by Lokesh Jain and Dinesh Chitlangia",
        "parent": "https://github.com/apache/hadoop/commit/589637276105b0f9b9d5b7f6207f6ad0892f0b28",
        "patched_files": [
            "StringUtils.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestStringUtils.java"
        ]
    },
    "hadoop_e7438a1": {
        "bug_id": "hadoop_e7438a1",
        "commit": "https://github.com/apache/hadoop/commit/e7438a1b38ff1d2bb25aa9d849a227c6f354143b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e7438a1b38ff1d2bb25aa9d849a227c6f354143b/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java?ref=e7438a1b38ff1d2bb25aa9d849a227c6f354143b",
                "deletions": 0,
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "patch": "@@ -90,6 +90,10 @@ public static void setup() throws StorageContainerException {\n     Mockito.when(handler.handle(any(), any())).thenCallRealMethod();\n     doCallRealMethod().when(dispatcher).setMetricsForTesting(any());\n     dispatcher.setMetricsForTesting(Mockito.mock(ContainerMetrics.class));\n+    Mockito.when(dispatcher.buildAuditMessageForFailure(any(), any(), any()))\n+        .thenCallRealMethod();\n+    Mockito.when(dispatcher.buildAuditMessageForSuccess(any(), any()))\n+        .thenCallRealMethod();\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/e7438a1b38ff1d2bb25aa9d849a227c6f354143b/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "sha": "29d74c2ab10e832dba7d6e5065873c546e775d1e",
                "status": "modified"
            }
        ],
        "message": "HDDS-849. Fix NPE in TestKeyValueHandler because of audit log write.\nContributed by Dinesh Chitlangia.",
        "parent": "https://github.com/apache/hadoop/commit/8b2381441558cd49b4c940b0760c8accbb2a5567",
        "patched_files": [
            "KeyValueHandler.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestKeyValueHandler.java"
        ]
    },
    "hadoop_e8071aa": {
        "bug_id": "hadoop_e8071aa",
        "commit": "https://github.com/apache/hadoop/commit/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ContainerLaunchContextPBImpl.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ContainerLaunchContextPBImpl.java?ref=e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ContainerLaunchContextPBImpl.java",
                "patch": "@@ -208,11 +208,24 @@ public void setLocalResources(\n       final Map<String, LocalResource> localResources) {\n     if (localResources == null)\n       return;\n+    checkLocalResources(localResources);\n     initLocalResources();\n     this.localResources.clear();\n     this.localResources.putAll(localResources);\n   }\n   \n+  private void checkLocalResources(Map<String, LocalResource> localResources) {\n+    for (Map.Entry<String, LocalResource> rsrcEntry : localResources\n+        .entrySet()) {\n+      if (rsrcEntry.getValue() == null\n+          || rsrcEntry.getValue().getResource() == null) {\n+        throw new NullPointerException(\n+            \"Null resource URL for local resource \" + rsrcEntry.getKey() + \" : \"\n+                + rsrcEntry.getValue());\n+      }\n+    }\n+  }\n+\n   private void addLocalResourcesToProto() {\n     maybeInitBuilder();\n     builder.clearLocalResources();",
                "raw_url": "https://github.com/apache/hadoop/raw/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/records/impl/pb/ContainerLaunchContextPBImpl.java",
                "sha": "f07a9d6f05514a9ccfa06af60c83b610b5193fac",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/records/impl/pb/TestApplicationClientProtocolRecords.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/records/impl/pb/TestApplicationClientProtocolRecords.java?ref=e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/records/impl/pb/TestApplicationClientProtocolRecords.java",
                "patch": "@@ -30,6 +30,10 @@\n import org.apache.hadoop.yarn.api.records.ApplicationAccessType;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.api.records.LocalResource;\n+import org.apache.hadoop.yarn.api.records.LocalResourceType;\n+import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;\n+import org.apache.hadoop.yarn.factories.RecordFactory;\n+import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -66,4 +70,29 @@ public void testCLCPBImplNullEnv() throws IOException {\n         clcProto.getEnvironment().get(\"testCLCPBImplNullEnv\"));\n \n   }\n+\n+  /*\n+   * This test validates the scenario in which the client sets a null value for\n+   * local resource URL.\n+   */\n+  @Test\n+  public void testCLCPBImplNullResourceURL() throws IOException {\n+    RecordFactory recordFactory = RecordFactoryProvider.getRecordFactory(null);\n+    try {\n+      LocalResource rsrc_alpha = recordFactory.newRecordInstance(LocalResource.class);\n+      rsrc_alpha.setResource(null);\n+      rsrc_alpha.setSize(-1);\n+      rsrc_alpha.setVisibility(LocalResourceVisibility.APPLICATION);\n+      rsrc_alpha.setType(LocalResourceType.FILE);\n+      rsrc_alpha.setTimestamp(System.currentTimeMillis());\n+      Map<String, LocalResource> localResources =\n+          new HashMap<String, LocalResource>();\n+      localResources.put(\"null_url_resource\", rsrc_alpha);\n+      ContainerLaunchContext containerLaunchContext = recordFactory.newRecordInstance(ContainerLaunchContext.class);\n+      containerLaunchContext.setLocalResources(localResources);\n+      Assert.fail(\"Setting an invalid local resource should be an error!\");\n+    } catch (NullPointerException e) {\n+      Assert.assertTrue(e.getMessage().contains(\"Null resource URL for local resource\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/api/records/impl/pb/TestApplicationClientProtocolRecords.java",
                "sha": "8773d11e2c06405517905a57764fefe8c2be313b",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java?ref=e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java",
                "patch": "@@ -64,6 +64,7 @@\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n+import org.apache.hadoop.yarn.api.records.LocalResource;\n import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;\n import org.apache.hadoop.yarn.api.records.LogAggregationContext;\n import org.apache.hadoop.yarn.api.records.NodeId;\n@@ -996,6 +997,15 @@ protected void startContainerInternal(\n \n     ContainerLaunchContext launchContext = request.getContainerLaunchContext();\n \n+    // Sanity check for local resources\n+    for (Map.Entry<String, LocalResource> rsrc : launchContext\n+        .getLocalResources().entrySet()) {\n+      if (rsrc.getValue() == null || rsrc.getValue().getResource() == null) {\n+        throw new YarnException(\n+            \"Null resource URL for local resource \" + rsrc.getKey() + \" : \" + rsrc.getValue());\n+      }\n+    }\n+\n     Credentials credentials =\n         YarnServerSecurityUtils.parseCredentials(launchContext);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/ContainerManagerImpl.java",
                "sha": "85dc5fc83578cff8c9c9fc29e0a8963d0f654af1",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestContainerManagerWithLCE.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestContainerManagerWithLCE.java?ref=e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestContainerManagerWithLCE.java",
                "patch": "@@ -364,6 +364,17 @@ public void testContainerRestart() throws IOException, InterruptedException,\n     super.testContainerRestart();\n   }\n \n+  @Override\n+  public void testStartContainerFailureWithInvalidLocalResource() throws Exception {\n+    // Don't run the test if the binary is not available.\n+    if (!shouldRunTest()) {\n+      LOG.info(\"LCE binary path is not passed. Not running the test\");\n+      return;\n+    }\n+    LOG.info(\"Running testStartContainerFailureWithInvalidLocalResource\");\n+    super.testStartContainerFailureWithInvalidLocalResource();\n+  }\n+\n   private boolean shouldRunTest() {\n     return System\n         .getProperty(YarnConfiguration.NM_LINUX_CONTAINER_EXECUTOR_PATH) != null;",
                "raw_url": "https://github.com/apache/hadoop/raw/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestContainerManagerWithLCE.java",
                "sha": "5dc47173ffa4660a7ab9be4e243b4e17c90e87c4",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/hadoop/blob/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestContainerManager.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestContainerManager.java?ref=e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestContainerManager.java",
                "patch": "@@ -1859,4 +1859,49 @@ private void testContainerLaunchAndSignal(SignalContainerCommand command)\n       Assert.assertEquals(signal, signalContext.getSignal());\n     }\n   }\n+\n+  @Test\n+  public void testStartContainerFailureWithInvalidLocalResource()\n+      throws Exception {\n+    containerManager.start();\n+    LocalResource rsrc_alpha =\n+        recordFactory.newRecordInstance(LocalResource.class);\n+    rsrc_alpha.setResource(null);\n+    rsrc_alpha.setSize(-1);\n+    rsrc_alpha.setVisibility(LocalResourceVisibility.APPLICATION);\n+    rsrc_alpha.setType(LocalResourceType.FILE);\n+    rsrc_alpha.setTimestamp(System.currentTimeMillis());\n+    Map<String, LocalResource> localResources =\n+        new HashMap<String, LocalResource>();\n+    localResources.put(\"invalid_resource\", rsrc_alpha);\n+    ContainerLaunchContext containerLaunchContext =\n+        recordFactory.newRecordInstance(ContainerLaunchContext.class);\n+    ContainerLaunchContext spyContainerLaunchContext =\n+        Mockito.spy(containerLaunchContext);\n+    Mockito.when(spyContainerLaunchContext.getLocalResources())\n+        .thenReturn(localResources);\n+\n+    ContainerId cId = createContainerId(0);\n+    String user = \"start_container_fail\";\n+    Token containerToken =\n+        createContainerToken(cId, DUMMY_RM_IDENTIFIER, context.getNodeId(),\n+            user, context.getContainerTokenSecretManager());\n+    StartContainerRequest request = StartContainerRequest\n+        .newInstance(spyContainerLaunchContext, containerToken);\n+\n+    // start containers\n+    List<StartContainerRequest> startRequest =\n+        new ArrayList<StartContainerRequest>();\n+    startRequest.add(request);\n+    StartContainersRequest requestList =\n+        StartContainersRequest.newInstance(startRequest);\n+\n+    StartContainersResponse response =\n+        containerManager.startContainers(requestList);\n+    Assert.assertTrue(response.getFailedRequests().size() == 1);\n+    Assert.assertTrue(response.getSuccessfullyStartedContainers().size() == 0);\n+    Assert.assertTrue(response.getFailedRequests().containsKey(cId));\n+    Assert.assertTrue(response.getFailedRequests().get(cId).getMessage()\n+        .contains(\"Null resource URL for local resource\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/e8071aa249c7b21b1de084ee5a9ca2a44efd3bf0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/TestContainerManager.java",
                "sha": "6fead7ef4c174b3a3537d668b5c638c77319bf10",
                "status": "modified"
            }
        ],
        "message": "YARN-6403. Invalid local resource request can raise NPE and make NM exit. Contributed by Tao Yang",
        "parent": "https://github.com/apache/hadoop/commit/34ab8e73d48ea3c21c9e9571419b43b605de15c3",
        "patched_files": [
            "ContainerLaunchContextPBImpl.java",
            "ContainerManager.java",
            "ContainerManagerImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerManager.java",
            "TestApplicationClientProtocolRecords.java",
            "TestContainerManagerWithLCE.java"
        ]
    },
    "hadoop_e817ced": {
        "bug_id": "hadoop_e817ced",
        "commit": "https://github.com/apache/hadoop/commit/e817cedcdc262630206630d4a58d1051ceab8794",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e817cedcdc262630206630d4a58d1051ceab8794/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=e817cedcdc262630206630d4a58d1051ceab8794",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1181,6 +1181,10 @@ Release 2.7.0 - UNRELEASED\n     HDFS-7950. Fix TestFsDatasetImpl#testAddVolumes failure on Windows.\n     (Xiaoyu Yao via Arpit Agarwal)\n \n+    HDFS-7951. Fix NPE for\n+    TestFsDatasetImpl#testAddVolumeFailureReleasesInUseLock on Linux\n+    (Xiaoyu Yao via Arpit Agarwal)\n+\n     BREAKDOWN OF HDFS-7584 SUBTASKS AND RELATED JIRAS\n \n       HDFS-7720. Quota by Storage Type API, tools and ClientNameNode",
                "raw_url": "https://github.com/apache/hadoop/raw/e817cedcdc262630206630d4a58d1051ceab8794/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "eb84213bd4b755a5a46cfb617e58c2ec5bde19c0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/e817cedcdc262630206630d4a58d1051ceab8794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java?ref=e817cedcdc262630206630d4a58d1051ceab8794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "patch": "@@ -330,7 +330,7 @@ public void testAddVolumeFailureReleasesInUseLock() throws IOException {\n     Storage.StorageDirectory sd = createStorageDirectory(badDir);\n     sd.lock();\n     DataStorage.VolumeBuilder builder = new DataStorage.VolumeBuilder(storage, sd);\n-    when(storage.prepareVolume(eq(datanode), eq(badDir),\n+    when(storage.prepareVolume(eq(datanode), eq(badDir.getAbsoluteFile()),\n         Matchers.<List<NamespaceInfo>>any()))\n         .thenReturn(builder);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/e817cedcdc262630206630d4a58d1051ceab8794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "sha": "56a4287d631b0cc6f6a050ff3395b7fc9f6cf850",
                "status": "modified"
            }
        ],
        "message": "HDFS-7951. Fix NPE for TestFsDatasetImpl#testAddVolumeFailureReleasesInUseLock on Linux. (Contributed by Xiaoyu Yao)",
        "parent": "https://github.com/apache/hadoop/commit/d462c62755171a2e980a4946d7c8b2d689a715b5",
        "patched_files": [
            "CHANGES.java",
            "FsDatasetImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_e83be44": {
        "bug_id": "hadoop_e83be44",
        "commit": "https://github.com/apache/hadoop/commit/e83be44af530d57d9c49cd989d030052548a068b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java?ref=e83be44af530d57d9c49cd989d030052548a068b",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "patch": "@@ -557,6 +557,10 @@ public void close() throws IOException {\n   public KeyVersion rollNewVersion(String name) throws NoSuchAlgorithmException,\n                                                        IOException {\n     Metadata meta = getMetadata(name);\n+    if (meta == null) {\n+      throw new IOException(\"Can't find Metadata for key \" + name);\n+    }\n+\n     byte[] material = generateKey(meta.getBitLength(), meta.getCipher());\n     return rollNewVersion(name, material);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "sha": "c99a7bf08ceee613156f085aab9432e5c462a8e4",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java?ref=e83be44af530d57d9c49cd989d030052548a068b",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.security.ProviderUtils;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.junit.Test;\n \n import java.io.IOException;\n@@ -38,6 +39,7 @@\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n \n public class TestKeyProvider {\n \n@@ -182,7 +184,10 @@ public KeyVersion getKeyVersion(String versionName)\n \n     @Override\n     public Metadata getMetadata(String name) throws IOException {\n-      return new Metadata(CIPHER, 128, \"description\", null, new Date(), 0);\n+      if (!\"unknown\".equals(name)) {\n+        return new Metadata(CIPHER, 128, \"description\", null, new Date(), 0);\n+      }\n+      return null;\n     }\n \n     @Override\n@@ -236,6 +241,27 @@ public void testMaterialGeneration() throws Exception {\n     Assert.assertNotNull(kp.material);\n   }\n \n+  @Test\n+  public void testRolloverUnknownKey() throws Exception {\n+    MyKeyProvider kp = new MyKeyProvider(new Configuration());\n+    KeyProvider.Options options = new KeyProvider.Options(new Configuration());\n+    options.setCipher(CIPHER);\n+    options.setBitLength(128);\n+    kp.createKey(\"hello\", options);\n+    Assert.assertEquals(128, kp.size);\n+    Assert.assertEquals(CIPHER, kp.algorithm);\n+    Assert.assertNotNull(kp.material);\n+\n+    kp = new MyKeyProvider(new Configuration());\n+    try {\n+      kp.rollNewVersion(\"unknown\");\n+      fail(\"should have thrown\");\n+    } catch (IOException e) {\n+      String expectedError = \"Can't find Metadata for key\";\n+      GenericTestUtils.assertExceptionContains(expectedError, e);\n+    }\n+  }\n+\n   @Test\n   public void testConfiguration() throws Exception {\n     Configuration conf = new Configuration(false);",
                "raw_url": "https://github.com/apache/hadoop/raw/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java",
                "sha": "9c01175c43dd9559cac11cd2b77d7a12a2b43490",
                "status": "modified"
            }
        ],
        "message": "HADOOP-13461. NPE in KeyProvider.rollNewVersion. Contributed by Colm O hEigeartaigh.",
        "parent": "https://github.com/apache/hadoop/commit/ec289bbeceff064ad24e189db20a3e0a296822c1",
        "patched_files": [
            "KeyProvider.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestKeyProvider.java"
        ]
    },
    "hadoop_eb484bb": {
        "bug_id": "hadoop_eb484bb",
        "commit": "https://github.com/apache/hadoop/commit/eb484bb5629e57c97192b6794f30c1fbb290b6ee",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=eb484bb5629e57c97192b6794f30c1fbb290b6ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -401,6 +401,9 @@ Release 2.1.1-beta - UNRELEASED\n     HDFS-5132. Deadlock in NameNode between SafeModeMonitor#run and \n     DatanodeManager#handleHeartbeat. (kihwal)\n \n+    HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization().\n+    (Plamen Jeliazkov via shv)\n+\n Release 2.1.0-beta - 2013-08-22\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "84100e18c4dd693927880cc330ce7cad9758642e",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=eb484bb5629e57c97192b6794f30c1fbb290b6ee",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -174,7 +174,6 @@\n import org.apache.hadoop.hdfs.server.namenode.INode.BlocksMapUpdateInfo;\n import org.apache.hadoop.hdfs.server.namenode.JournalSet.JournalAndStream;\n import org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNode.OperationCategory;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;\n@@ -3772,24 +3771,32 @@ void commitBlockSynchronization(ExtendedBlock lastblock,\n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n-        DatanodeDescriptor[] descriptors = null;\n+        List<DatanodeDescriptor> targetList =\n+            new ArrayList<DatanodeDescriptor>(newtargets.length);\n         if (newtargets.length > 0) {\n-          descriptors = new DatanodeDescriptor[newtargets.length];\n-          for(int i = 0; i < newtargets.length; i++) {\n-            descriptors[i] = blockManager.getDatanodeManager().getDatanode(\n-                newtargets[i]);\n+          for (DatanodeID newtarget : newtargets) {\n+            // try to get targetNode\n+            DatanodeDescriptor targetNode =\n+                blockManager.getDatanodeManager().getDatanode(newtarget);\n+            if (targetNode != null)\n+              targetList.add(targetNode);\n+            else if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"DatanodeDescriptor (=\" + newtarget + \") not found\");\n+            }\n           }\n         }\n-        if ((closeFile) && (descriptors != null)) {\n+        if ((closeFile) && !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n-          for (int i = 0; i < descriptors.length; i++) {\n-            descriptors[i].addBlock(storedBlock);\n+          for (DatanodeDescriptor targetNode : targetList) {\n+            targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n-        pendingFile.setLastBlock(storedBlock, descriptors);\n+        DatanodeDescriptor[] targetArray =\n+            new DatanodeDescriptor[targetList.size()];\n+        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {",
                "raw_url": "https://github.com/apache/hadoop/raw/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "a397ce94fd1e4727903d2b56e5d84a487b8c0502",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java?ref=eb484bb5629e57c97192b6794f30c1fbb290b6ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "patch": "@@ -169,4 +169,23 @@ public void testCommitBlockSynchronizationWithClose() throws IOException {\n     namesystemSpy.commitBlockSynchronization(\n         lastBlock, genStamp, length, true, false, newTargets, null);\n   }\n+\n+  @Test\n+  public void testCommitBlockSynchronizationWithCloseAndNonExistantTarget()\n+      throws IOException {\n+    INodeFileUnderConstruction file = mock(INodeFileUnderConstruction.class);\n+    Block block = new Block(blockId, length, genStamp);\n+    FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);\n+    DatanodeID[] newTargets = new DatanodeID[]{\n+        new DatanodeID(\"0.0.0.0\", \"nonexistantHost\", \"1\", 0, 0, 0)};\n+\n+    ExtendedBlock lastBlock = new ExtendedBlock();\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true,\n+        false, newTargets, null);\n+\n+    // Repeat the call to make sure it returns true\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true, false, newTargets, null);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "sha": "f40b799d1a824c27dfc948f51d85052a8fb04392",
                "status": "modified"
            }
        ],
        "message": "HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization(). Contributed by Plamen Jeliazkov.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1518851 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/eef32121d1d81076fd7e49ae65af03d1a6837dca",
        "patched_files": [
            "CHANGES.java",
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestCommitBlockSynchronization.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_ebc048c": {
        "bug_id": "hadoop_ebc048c",
        "commit": "https://github.com/apache/hadoop/commit/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -275,6 +275,10 @@ protected void addSchedPriorityCommand(List<String> command) {\n     }\n   }\n \n+  protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+    return PrivilegedOperationExecutor.getInstance(getConf());\n+  }\n+\n   @Override\n   public void init() throws IOException {\n     Configuration conf = super.getConf();\n@@ -285,7 +289,7 @@ public void init() throws IOException {\n       PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.CHECK_SETUP);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n           false);\n@@ -382,7 +386,7 @@ public void startLocalizer(LocalizerStartContext ctx)\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n           initializeContainerOp, null, null, false, true);\n@@ -530,8 +534,9 @@ public int launchContainer(ContainerStartContext ctx)\n         }\n         builder.append(\"Stack trace: \"\n             + StringUtils.stringifyException(e) + \"\\n\");\n-        if (!e.getOutput().isEmpty()) {\n-          builder.append(\"Shell output: \" + e.getOutput() + \"\\n\");\n+        String output = e.getOutput();\n+        if (output != null && !e.getOutput().isEmpty()) {\n+          builder.append(\"Shell output: \" + output + \"\\n\");\n         }\n         String diagnostics = builder.toString();\n         logOutput(diagnostics);\n@@ -729,7 +734,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp,\n           false);\n@@ -759,7 +764,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n \n     try {\n       PrivilegedOperationExecutor privOpExecutor =\n-          PrivilegedOperationExecutor.getInstance(super.getConf());\n+          getPrivilegedOperationExecutor();\n \n       String results =\n           privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n@@ -818,7 +823,7 @@ public void mountCgroups(List<String> cgroupKVs, String hierarchy)\n \n       mountCGroupsOp.appendArgs(cgroupKVs);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(mountCGroupsOp,\n           false);",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "2aaa8359e7b9abc6ed29c39b11dfede9e3ef376f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "patch": "@@ -24,7 +24,7 @@\n \n public class PrivilegedOperationException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private Integer exitCode;\n+  private int exitCode = -1;\n   private String output;\n   private String errorOutput;\n \n@@ -36,7 +36,7 @@ public PrivilegedOperationException(String message) {\n     super(message);\n   }\n \n-  public PrivilegedOperationException(String message, Integer exitCode,\n+  public PrivilegedOperationException(String message, int exitCode,\n       String output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n@@ -48,8 +48,8 @@ public PrivilegedOperationException(Throwable cause) {\n     super(cause);\n   }\n \n-  public PrivilegedOperationException(Throwable cause, Integer exitCode, String\n-      output, String errorOutput) {\n+  public PrivilegedOperationException(Throwable cause, int exitCode,\n+      String output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n@@ -59,7 +59,7 @@ public PrivilegedOperationException(String message, Throwable cause) {\n     super(message, cause);\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "sha": "9a11194f143e0e832d9371bddfd1229aeaeb0bd4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "patch": "@@ -32,10 +32,10 @@\n @InterfaceStability.Unstable\n public class ContainerExecutionException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private static final Integer EXIT_CODE_UNSET = -1;\n+  private static final int EXIT_CODE_UNSET = -1;\n   private static final String OUTPUT_UNSET = \"<unknown>\";\n \n-  private Integer exitCode;\n+  private int exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -54,23 +54,23 @@ public ContainerExecutionException(Throwable throwable) {\n   }\n \n \n-  public ContainerExecutionException(String message, Integer exitCode, String\n+  public ContainerExecutionException(String message, int exitCode, String\n       output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public ContainerExecutionException(Throwable cause, Integer exitCode, String\n+  public ContainerExecutionException(Throwable cause, int exitCode, String\n       output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "sha": "3147277704265b1cb5bc2aa2f21d8a417e31c381",
                "status": "modified"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -23,7 +23,9 @@\n import static org.junit.Assert.assertNotEquals;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n@@ -40,20 +42,24 @@\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.ConfigurationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerDiagnosticsUpdateEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime;\n@@ -516,4 +522,87 @@ public void testDeleteAsUser() throws IOException {\n         appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n         readMockParams());\n   }\n+\n+  @Test\n+  public void testNoExitCodeFromPrivilegedOperation() throws Exception {\n+    Configuration conf = new Configuration();\n+    final PrivilegedOperationExecutor spyPrivilegedExecutor =\n+        spy(PrivilegedOperationExecutor.getInstance(conf));\n+    doThrow(new PrivilegedOperationException(\"interrupted\"))\n+        .when(spyPrivilegedExecutor).executePrivilegedOperation(\n+            any(List.class), any(PrivilegedOperation.class),\n+            any(File.class), any(Map.class), anyBoolean(), anyBoolean());\n+    LinuxContainerRuntime runtime = new DefaultLinuxContainerRuntime(\n+        spyPrivilegedExecutor);\n+    runtime.initialize(conf);\n+    mockExec = new LinuxContainerExecutor(runtime);\n+    mockExec.setConf(conf);\n+    LinuxContainerExecutor lce = new LinuxContainerExecutor(runtime) {\n+      @Override\n+      protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+        return spyPrivilegedExecutor;\n+      }\n+    };\n+    lce.setConf(conf);\n+    InetSocketAddress address = InetSocketAddress.createUnresolved(\n+        \"localhost\", 8040);\n+    Path nmPrivateCTokensPath= new Path(\"file:///bin/nmPrivateCTokensPath\");\n+    LocalDirsHandlerService dirService = new LocalDirsHandlerService();\n+    dirService.init(conf);\n+\n+    String appSubmitter = \"nobody\";\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\n+    ContainerId cid = ContainerId.newContainerId(attemptId, 1);\n+    HashMap<String, String> env = new HashMap<>();\n+    Container container = mock(Container.class);\n+    ContainerLaunchContext context = mock(ContainerLaunchContext.class);\n+    when(container.getContainerId()).thenReturn(cid);\n+    when(container.getLaunchContext()).thenReturn(context);\n+    when(context.getEnvironment()).thenReturn(env);\n+    Path workDir = new Path(\"/tmp\");\n+\n+    try {\n+      lce.startLocalizer(new LocalizerStartContext.Builder()\n+          .setNmPrivateContainerTokens(nmPrivateCTokensPath)\n+          .setNmAddr(address)\n+          .setUser(appSubmitter)\n+          .setAppId(appId.toString())\n+          .setLocId(\"12345\")\n+          .setDirsHandler(dirService)\n+          .build());\n+      Assert.fail(\"startLocalizer should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exitCode\"));\n+    }\n+\n+    lce.activateContainer(cid, new Path(workDir, \"pid.txt\"));\n+    lce.launchContainer(new ContainerStartContext.Builder()\n+        .setContainer(container)\n+        .setNmPrivateContainerScriptPath(new Path(\"file:///bin/echo\"))\n+        .setNmPrivateTokensPath(new Path(\"file:///dev/null\"))\n+        .setUser(appSubmitter)\n+        .setAppId(appId.toString())\n+        .setContainerWorkDir(workDir)\n+        .setLocalDirs(dirsHandler.getLocalDirs())\n+        .setLogDirs(dirsHandler.getLogDirs())\n+        .setFilecacheDirs(new ArrayList<>())\n+        .setUserLocalDirs(new ArrayList<>())\n+        .setContainerLocalDirs(new ArrayList<>())\n+        .setContainerLogDirs(new ArrayList<>())\n+        .build());\n+    lce.deleteAsUser(new DeletionAsUserContext.Builder()\n+        .setUser(appSubmitter)\n+        .setSubDir(new Path(\"/tmp/testdir\"))\n+        .build());\n+\n+    try {\n+      lce.mountCgroups(new ArrayList<String>(), \"hierarchy\");\n+      Assert.fail(\"mountCgroups should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exit code\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "cfd0e364a2d4ad5735590d69d90cdce9f81b0012",
                "status": "modified"
            }
        ],
        "message": "YARN-6805. NPE in LinuxContainerExecutor due to null PrivilegedOperationException exit code. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/0ffca5d347df0acb1979dff7a07ae88ea834adc7",
        "patched_files": [
            "LinuxContainerExecutor.java",
            "PrivilegedOperationException.java",
            "ContainerExecutionException.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java",
            "TestLinuxContainerExecutorWithMocks.java"
        ]
    },
    "hadoop_ee21b13": {
        "bug_id": "hadoop_ee21b13",
        "commit": "https://github.com/apache/hadoop/commit/ee21b13cbd4654d7181306404174329f12193613",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -380,6 +380,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2558. Updated ContainerTokenIdentifier#read/write to use\n     ContainerId#getContainerId. (Tsuyoshi OZAWA via jianhe)\n \n+    YARN-2559. Fixed NPE in SystemMetricsPublisher when retrieving\n+    FinalApplicationStatus. (Zhijie Shen via jianhe)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/CHANGES.txt",
                "sha": "5a238140c89771e308fd744dd64e43fbfe14519e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -160,16 +160,18 @@ public void appAttemptRegistered(RMAppAttempt appAttempt,\n \n   @SuppressWarnings(\"unchecked\")\n   public void appAttemptFinished(RMAppAttempt appAttempt,\n-      RMAppAttemptState state, long finishedTime) {\n+      RMAppAttemptState appAttemtpState, RMApp app, long finishedTime) {\n     if (publishSystemMetrics) {\n       dispatcher.getEventHandler().handle(\n           new AppAttemptFinishedEvent(\n               appAttempt.getAppAttemptId(),\n               appAttempt.getTrackingUrl(),\n               appAttempt.getOriginalTrackingUrl(),\n               appAttempt.getDiagnostics(),\n-              appAttempt.getFinalApplicationStatus(),\n-              RMServerUtils.createApplicationAttemptState(state),\n+              // app will get the final status from app attempt, or create one\n+              // based on app state if it doesn't exist\n+              app.getFinalApplicationStatus(),\n+              RMServerUtils.createApplicationAttemptState(appAttemtpState),\n               finishedTime));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "5da006c0095561e299c719ce07017868a679a60d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -1159,8 +1159,10 @@ public void transition(RMAppAttemptImpl appAttempt,\n       appAttempt.rmContext.getRMApplicationHistoryWriter()\n           .applicationAttemptFinished(appAttempt, finalAttemptState);\n       appAttempt.rmContext.getSystemMetricsPublisher()\n-          .appAttemptFinished(\n-              appAttempt, finalAttemptState, System.currentTimeMillis());\n+          .appAttemptFinished(appAttempt, finalAttemptState,\n+              appAttempt.rmContext.getRMApps().get(\n+                  appAttempt.applicationAttemptId.getApplicationId()),\n+              System.currentTimeMillis());\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "7ca57ee018a188ad14c12feff5992930773cecfe",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -174,7 +174,9 @@ public void testPublishAppAttemptMetrics() throws Exception {\n         ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1);\n     RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId);\n     metricsPublisher.appAttemptRegistered(appAttempt, Integer.MAX_VALUE + 1L);\n-    metricsPublisher.appAttemptFinished(appAttempt, RMAppAttemptState.FINISHED,\n+    RMApp app = mock(RMApp.class);\n+    when(app.getFinalApplicationStatus()).thenReturn(FinalApplicationStatus.UNDEFINED);\n+    metricsPublisher.appAttemptFinished(appAttempt, RMAppAttemptState.FINISHED, app,\n         Integer.MAX_VALUE + 2L);\n     TimelineEntity entity = null;\n     do {\n@@ -222,7 +224,7 @@ public void testPublishAppAttemptMetrics() throws Exception {\n             event.getEventInfo().get(\n                 AppAttemptMetricsConstants.ORIGINAL_TRACKING_URL_EVENT_INFO));\n         Assert.assertEquals(\n-            appAttempt.getFinalApplicationStatus().toString(),\n+            FinalApplicationStatus.UNDEFINED.toString(),\n             event.getEventInfo().get(\n                 AppAttemptMetricsConstants.FINAL_STATUS_EVENT_INFO));\n         Assert.assertEquals(\n@@ -340,8 +342,6 @@ private static RMAppAttempt createRMAppAttempt(\n     when(appAttempt.getTrackingUrl()).thenReturn(\"test tracking url\");\n     when(appAttempt.getOriginalTrackingUrl()).thenReturn(\n         \"test original tracking url\");\n-    when(appAttempt.getFinalApplicationStatus()).thenReturn(\n-        FinalApplicationStatus.UNDEFINED);\n     return appAttempt;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "63343e9521d49df756693f9307ba9731b90d430b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "patch": "@@ -76,6 +76,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher;\n import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore;\n import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.ApplicationAttemptState;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent;\n@@ -92,7 +93,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent;\n@@ -289,7 +289,6 @@ public void setUp() throws Exception {\n     Mockito.doReturn(resourceScheduler).when(spyRMContext).getScheduler();\n \n \n-    final String user = MockApps.newUserName();\n     final String queue = MockApps.newQueue();\n     submissionContext = mock(ApplicationSubmissionContext.class);\n     when(submissionContext.getQueue()).thenReturn(queue);\n@@ -1385,7 +1384,7 @@ private void verifyApplicationAttemptFinished(RMAppAttemptState state) {\n     finalState =\n         ArgumentCaptor.forClass(RMAppAttemptState.class);\n     verify(publisher).appAttemptFinished(any(RMAppAttempt.class), finalState.capture(),\n-        anyLong());\n+        any(RMApp.class), anyLong());\n     Assert.assertEquals(state, finalState.getValue());\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "sha": "b8e6f434e7c94b8b267109436fd2e8e3bc617327",
                "status": "modified"
            }
        ],
        "message": "YARN-2559. Fixed NPE in SystemMetricsPublisher when retrieving FinalApplicationStatus. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/0ecefe60179968984b1892a14411566b7a0c8df3",
        "patched_files": [
            "CHANGES.java",
            "SystemMetricsPublisher.java",
            "RMAppAttemptImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMAppAttemptTransitions.java",
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_ef43257": {
        "bug_id": "hadoop_ef43257",
        "commit": "https://github.com/apache/hadoop/commit/ef432579a7763cc0e482fe049027c6e5325eb034",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java?ref=ef432579a7763cc0e482fe049027c6e5325eb034",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                "patch": "@@ -76,13 +76,17 @@ protected DatanodeDescriptor chooseDataNode(final String scope,\n         (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n     DatanodeDescriptor b =\n         (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n-    int ret = compareDataNode(a, b);\n-    if (ret == 0) {\n-      return a;\n-    } else if (ret < 0) {\n-      return (RAND.nextInt(100) < balancedPreference) ? a : b;\n+    if (a != null && b != null){\n+      int ret = compareDataNode(a, b);\n+      if (ret == 0) {\n+        return a;\n+      } else if (ret < 0) {\n+        return (RAND.nextInt(100) < balancedPreference) ? a : b;\n+      } else {\n+        return (RAND.nextInt(100) < balancedPreference) ? b : a;\n+      }\n     } else {\n-      return (RAND.nextInt(100) < balancedPreference) ? b : a;\n+      return a == null ? b : a;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                "sha": "706768c8ade547c147f8902774afe3cc11a97adb",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java?ref=ef432579a7763cc0e482fe049027c6e5325eb034",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java",
                "patch": "@@ -20,6 +20,8 @@\n \n import java.io.File;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -30,6 +32,7 @@\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;\n import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.net.NetworkTopology;\n+import org.apache.hadoop.net.Node;\n import org.apache.hadoop.test.PathUtils;\n import org.junit.AfterClass;\n import org.junit.Assert;\n@@ -158,6 +161,21 @@ public void testChooseTarget() {\n     Assert.assertTrue(possibility < 0.55);\n   }\n \n+  @Test\n+  public void testChooseDataNode() {\n+    try {\n+      Collection<Node> allNodes = new ArrayList<>(dataNodes.length);\n+      Collections.addAll(allNodes, dataNodes);\n+      if (placementPolicy instanceof AvailableSpaceBlockPlacementPolicy){\n+        // exclude all datanodes when chooseDataNode, no NPE should be thrown\n+        ((AvailableSpaceBlockPlacementPolicy)placementPolicy)\n+                .chooseDataNode(\"~\", allNodes);\n+      }\n+    }catch (NullPointerException npe){\n+      Assert.fail(\"NPE should not be thrown\");\n+    }\n+  }\n+\n   @AfterClass\n   public static void teardownCluster() {\n     if (namenode != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java",
                "sha": "5b8ad1c4fd85bb4d2268d6e195ef0c287e69afd3",
                "status": "modified"
            }
        ],
        "message": "HDFS-10715. NPE when applying AvailableSpaceBlockPlacementPolicy. Contributed by Guangbin Zhu.",
        "parent": "https://github.com/apache/hadoop/commit/18d9e6ec0bdb4bce316f8af5d3f13902dd899325",
        "patched_files": [
            "AvailableSpaceBlockPlacementPolicy.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestAvailableSpaceBlockPlacementPolicy.java"
        ]
    },
    "hadoop_efaaf58": {
        "bug_id": "hadoop_efaaf58",
        "commit": "https://github.com/apache/hadoop/commit/efaaf586053b3275209e3d992df68d6a04d6181f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/efaaf586053b3275209e3d992df68d6a04d6181f/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=efaaf586053b3275209e3d992df68d6a04d6181f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -331,6 +331,9 @@ Release 0.23.7 - UNRELEASED\n     YARN-362. Unexpected extra results when using webUI table search (Ravi\n     Prakash via jlowe)\n \n+    YARN-400. RM can return null application resource usage report leading to \n+    NPE in client (Jason Lowe via tgraves)\n+\n Release 0.23.6 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/efaaf586053b3275209e3d992df68d6a04d6181f/hadoop-yarn-project/CHANGES.txt",
                "sha": "ca9fff4bf2c0afac733c34b81395bdaea94c80f2",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/efaaf586053b3275209e3d992df68d6a04d6181f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java?ref=efaaf586053b3275209e3d992df68d6a04d6181f",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "patch": "@@ -406,7 +406,8 @@ public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {\n       String host = UNAVAILABLE;\n       String origTrackingUrl = UNAVAILABLE;\n       int rpcPort = -1;\n-      ApplicationResourceUsageReport appUsageReport = null;\n+      ApplicationResourceUsageReport appUsageReport =\n+          DUMMY_APPLICATION_RESOURCE_USAGE_REPORT;\n       FinalApplicationStatus finishState = getFinalApplicationStatus();\n       String diags = UNAVAILABLE;\n       if (allowAccess) {\n@@ -418,18 +419,17 @@ public ApplicationReport createAndGetApplicationReport(boolean allowAccess) {\n           host = this.currentAttempt.getHost();\n           rpcPort = this.currentAttempt.getRpcPort();\n           appUsageReport = currentAttempt.getApplicationResourceUsageReport();\n-        } else {\n-          currentApplicationAttemptId = \n-              BuilderUtils.newApplicationAttemptId(this.applicationId, \n-                  DUMMY_APPLICATION_ATTEMPT_NUMBER);\n         }\n+\n         diags = this.diagnostics.toString();\n-      } else {\n-        appUsageReport = DUMMY_APPLICATION_RESOURCE_USAGE_REPORT;\n+      }\n+\n+      if (currentApplicationAttemptId == null) {\n         currentApplicationAttemptId = \n             BuilderUtils.newApplicationAttemptId(this.applicationId, \n                 DUMMY_APPLICATION_ATTEMPT_NUMBER);\n       }\n+\n       return BuilderUtils.newApplicationReport(this.applicationId,\n           currentApplicationAttemptId, this.user, this.queue,\n           this.name, host, rpcPort, clientToken,",
                "raw_url": "https://github.com/apache/hadoop/raw/efaaf586053b3275209e3d992df68d6a04d6181f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "sha": "fa8070929cc3a7a2fabad57143ffead013cc010d",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/efaaf586053b3275209e3d992df68d6a04d6181f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java?ref=efaaf586053b3275209e3d992df68d6a04d6181f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.MockApps;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl;\n@@ -616,4 +617,12 @@ public void testAppKilledKilled() throws IOException {\n     assertTimesAtFinish(application);\n     assertAppState(RMAppState.KILLED, application);\n   }\n+\n+  @Test\n+  public void testGetAppReport() {\n+    RMApp app = createNewTestApp(null);\n+    assertAppState(RMAppState.NEW, app);\n+    ApplicationReport report = app.createAndGetApplicationReport(true);\n+    Assert.assertNotNull(report.getApplicationResourceUsageReport());\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/efaaf586053b3275209e3d992df68d6a04d6181f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "sha": "6d407dd44599e17379acf015c85821be21d9bc1d",
                "status": "modified"
            }
        ],
        "message": "YARN-400. RM can return null application resource usage report leading to NPE in client (Jason Lowe via tgraves)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448241 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/c8f35bc3d285816a91e37be0d953a9a788c491a5",
        "patched_files": [
            "RMAppImpl.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMAppTransitions.java"
        ]
    },
    "hadoop_eff5d9b": {
        "bug_id": "hadoop_eff5d9b",
        "commit": "https://github.com/apache/hadoop/commit/eff5d9b17e0853e82968a695b498b4be37148a05",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=eff5d9b17e0853e82968a695b498b4be37148a05",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -663,6 +663,8 @@ Release 2.1.0-beta - 2013-07-02\n     mechanisms are enabled and thus fix YARN/MR test failures after HADOOP-9421.\n     (Daryn Sharp and Vinod Kumar Vavilapalli via vinodkv)\n \n+    YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n+\n   BREAKDOWN OF HADOOP-8562 SUBTASKS AND RELATED JIRAS\n \n     YARN-158. Yarn creating package-info.java must not depend on sh.",
                "raw_url": "https://github.com/apache/hadoop/raw/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/CHANGES.txt",
                "sha": "a0e3a9b75e4a6f22677ddb2bea8c2e4a0fc80283",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=eff5d9b17e0853e82968a695b498b4be37148a05",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -801,9 +801,10 @@ private synchronized FiCaSchedulerApp getApplication(\n     if (reservedContainer != null) {\n       FiCaSchedulerApp application = \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return \n-          assignReservedContainer(application, node, reservedContainer, \n-              clusterResource); \n+      synchronized (application) {\n+        return assignReservedContainer(application, node, reservedContainer,\n+          clusterResource);\n+      }\n     }\n     \n     // Try to assign containers to applications in order",
                "raw_url": "https://github.com/apache/hadoop/raw/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "dbfa7444183198dd512254b8ff5daaa6b7208180",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=eff5d9b17e0853e82968a695b498b4be37148a05",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceRequest;\n+import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger;\n@@ -426,6 +427,16 @@ public synchronized void unreserve(FiCaSchedulerNode node, Priority priority) {\n       this.reservedContainers.remove(priority);\n     }\n     \n+    // reservedContainer should not be null here\n+    if (reservedContainer == null) {\n+      String errorMesssage =\n+          \"Application \" + getApplicationId() + \" is trying to unreserve \"\n+              + \" on node \" + node + \", currently has \"\n+              + reservedContainers.size() + \" at priority \" + priority\n+              + \"; currentReservation \" + currentReservation;\n+      LOG.warn(errorMesssage);\n+      throw new YarnRuntimeException(errorMesssage);\n+    }\n     // Reset the re-reservation count\n     resetReReservations(priority);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "8e2020abc79259fa89228678556aead46ddddbc6",
                "status": "modified"
            }
        ],
        "message": "YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499886 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/5428bfbf53d264cc3c67b39b94c30a93cc4578c3",
        "patched_files": [
            "LeafQueue.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop_f1552f6": {
        "bug_id": "hadoop_f1552f6",
        "commit": "https://github.com/apache/hadoop/commit/f1552f6edb8fe152003fd71944851b2b46a6677d",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java?ref=f1552f6edb8fe152003fd71944851b2b46a6677d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java",
                "patch": "@@ -1115,6 +1115,11 @@ public TimelineEvents getEntityTimelines(String entityType,\n     LOG.debug(\"getEntityTimelines type={} ids={}\", entityType, entityIds);\n     TimelineEvents returnEvents = new TimelineEvents();\n     List<EntityCacheItem> relatedCacheItems = new ArrayList<>();\n+\n+    if (entityIds == null || entityIds.isEmpty()) {\n+      return returnEvents;\n+    }\n+\n     for (String entityId : entityIds) {\n       LOG.debug(\"getEntityTimeline type={} id={}\", entityType, entityId);\n       List<TimelineStore> stores",
                "raw_url": "https://github.com/apache/hadoop/raw/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java",
                "sha": "a5e5b419d50d6a357df1a3085f0aff363c0b2561",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java?ref=f1552f6edb8fe152003fd71944851b2b46a6677d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.yarn.server.timeline.TimelineReader.Field;\n import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.junit.After;\n+import org.junit.Assert;\n import org.junit.AfterClass;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -379,6 +380,16 @@ public void testCleanBuckets() throws Exception {\n     assertFalse(fs.exists(clusterTimeStampDir1));\n   }\n \n+  @Test\n+  public void testNullCheckGetEntityTimelines() throws Exception {\n+    try {\n+      store.getEntityTimelines(\"YARN_APPLICATION\", null, null, null, null,\n+          null);\n+    } catch (NullPointerException e) {\n+      Assert.fail(\"NPE when getEntityTimelines called with Null EntityIds\");\n+    }\n+  }\n+\n   @Test\n   public void testPluginRead() throws Exception {\n     // Verify precondition",
                "raw_url": "https://github.com/apache/hadoop/raw/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java",
                "sha": "8fcc696aad42b580e93406ac998928544f3dd953",
                "status": "modified"
            }
        ],
        "message": "YARN-9553. Fix NPE in EntityGroupFSTimelineStore#getEntityTimelines. Contributed by Prabhu Joseph.",
        "parent": "https://github.com/apache/hadoop/commit/30c6dd92e1d4075d143adc891dc8ec536dddc0d9",
        "patched_files": [
            "EntityGroupFSTimelineStore.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestEntityGroupFSTimelineStore.java"
        ]
    },
    "hadoop_f214a99": {
        "bug_id": "hadoop_f214a99",
        "commit": "https://github.com/apache/hadoop/commit/f214a9961fdeeebc6157992ed54a777983e218e9",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f214a9961fdeeebc6157992ed54a777983e218e9/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3native/NativeS3FileSystemContractBaseTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3native/NativeS3FileSystemContractBaseTest.java?ref=f214a9961fdeeebc6157992ed54a777983e218e9",
                "deletions": 3,
                "filename": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3native/NativeS3FileSystemContractBaseTest.java",
                "patch": "@@ -44,19 +44,21 @@\n   @Before\n   public void setUp() throws Exception {\n     Configuration conf = new Configuration();\n-    store = getNativeFileSystemStore();\n-    fs = new NativeS3FileSystem(store);\n     String fsname = conf.get(KEY_TEST_FS);\n     if (StringUtils.isEmpty(fsname)) {\n       throw new AssumptionViolatedException(\n           \"No test FS defined in :\" + KEY_TEST_FS);\n     }\n+    store = getNativeFileSystemStore();\n+    fs = new NativeS3FileSystem(store);\n     fs.initialize(URI.create(fsname), conf);\n   }\n   \n   @After\n   public void tearDown() throws Exception {\n-    store.purge(\"test\");\n+    if (store != null) {\n+      store.purge(\"test\");\n+    }\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/f214a9961fdeeebc6157992ed54a777983e218e9/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3native/NativeS3FileSystemContractBaseTest.java",
                "sha": "bfbca71421497cc70f351e7d83501ce6554a044f",
                "status": "modified"
            }
        ],
        "message": "HADOOP-14494. ITestJets3tNativeS3FileSystemContract tests NPEs in teardown if store undefined. Contributed by Steve Loughran",
        "parent": "https://github.com/apache/hadoop/commit/3f5108723c6272d2fded8d3563c4b793e1d88f8b",
        "patched_files": [],
        "repo": "hadoop",
        "unit_tests": [
            "NativeS3FileSystemContractBaseTest.java"
        ]
    },
    "hadoop_f267917": {
        "bug_id": "hadoop_f267917",
        "commit": "https://github.com/apache/hadoop/commit/f267917ce3cf282b32166e39af871a8d1231d090",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java?ref=f267917ce3cf282b32166e39af871a8d1231d090",
                "deletions": 1,
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "patch": "@@ -78,7 +78,9 @@\n   private boolean isSecurityEnabled;\n   private final boolean topologyAwareRead;\n   /**\n-   * Creates a new XceiverClientManager.\n+   * Creates a new XceiverClientManager for non secured ozone cluster.\n+   * For security enabled ozone cluster, client should use the other constructor\n+   * with a valid ca certificate in pem string format.\n    *\n    * @param conf configuration\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "sha": "b15828a153098650236023fb3f307ea35c111283",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java?ref=f267917ce3cf282b32166e39af871a8d1231d090",
                "deletions": 2,
                "filename": "hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.hdds.cli.GenericCli;\n import org.apache.hadoop.hdds.cli.HddsVersionProvider;\n import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.SCMSecurityProtocol;\n import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n import org.apache.hadoop.hdds.scm.XceiverClientManager;\n import org.apache.hadoop.hdds.scm.cli.container.ContainerCommands;\n@@ -36,17 +37,20 @@\n import org.apache.hadoop.hdds.scm.protocolPB\n     .StorageContainerLocationProtocolClientSideTranslatorPB;\n import org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolPB;\n+import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n import org.apache.hadoop.hdds.tracing.TracingUtil;\n import org.apache.hadoop.ipc.Client;\n import org.apache.hadoop.ipc.ProtobufRpcEngine;\n import org.apache.hadoop.ipc.RPC;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.OzoneSecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.util.NativeCodeLoader;\n \n import org.apache.commons.lang3.StringUtils;\n import static org.apache.hadoop.hdds.HddsUtils.getScmAddressForClients;\n+import static org.apache.hadoop.hdds.HddsUtils.getScmSecurityClient;\n import static org.apache.hadoop.hdds.scm.ScmConfigKeys\n     .OZONE_SCM_CLIENT_ADDRESS_KEY;\n import static org.apache.hadoop.hdds.scm.ScmConfigKeys.OZONE_SCM_CONTAINER_SIZE;\n@@ -136,8 +140,21 @@ public ScmClient createScmClient()\n                 NetUtils.getDefaultSocketFactory(ozoneConf),\n                 Client.getRpcTimeout(ozoneConf))),\n             StorageContainerLocationProtocol.class, ozoneConf);\n-    return new ContainerOperationClient(\n-        client, new XceiverClientManager(ozoneConf));\n+\n+    XceiverClientManager xceiverClientManager = null;\n+    if (OzoneSecurityUtil.isSecurityEnabled(ozoneConf)) {\n+      SecurityConfig securityConfig = new SecurityConfig(ozoneConf);\n+      SCMSecurityProtocol scmSecurityProtocolClient = getScmSecurityClient(\n+          (OzoneConfiguration) securityConfig.getConfiguration());\n+      String caCertificate =\n+          scmSecurityProtocolClient.getCACertificate();\n+      xceiverClientManager = new XceiverClientManager(ozoneConf,\n+          OzoneConfiguration.of(ozoneConf).getObject(XceiverClientManager\n+              .ScmClientConfig.class), caCertificate);\n+    } else {\n+      xceiverClientManager = new XceiverClientManager(ozoneConf);\n+    }\n+    return new ContainerOperationClient(client, xceiverClientManager);\n   }\n \n   public void checkContainerExists(ScmClient scmClient, long containerId)",
                "raw_url": "https://github.com/apache/hadoop/raw/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SCMCLI.java",
                "sha": "0b5c18e8205cb5d11d0c5ba7435d4e441d4e3d8c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh?ref=f267917ce3cf282b32166e39af871a8d1231d090",
                "deletions": 0,
                "filename": "hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh",
                "patch": "@@ -35,6 +35,8 @@ execute_robot_test scm ozonefs/ozonefs.robot\n \n execute_robot_test s3g s3\n \n+execute_robot_test scm scmcli\n+\n stop_docker_env\n \n generate_report",
                "raw_url": "https://github.com/apache/hadoop/raw/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-ozone/dist/src/main/compose/ozonesecure/test.sh",
                "sha": "f32846386a9f772bbd14615d378bd8bde304e06a",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot?ref=f267917ce3cf282b32166e39af871a8d1231d090",
                "deletions": 0,
                "filename": "hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot",
                "patch": "@@ -0,0 +1,28 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+*** Settings ***\n+Documentation       Smoketest ozone cluster startup\n+Library             OperatingSystem\n+Library             BuiltIn\n+Resource            ../commonlib.robot\n+\n+*** Variables ***\n+\n+\n+*** Test Cases ***\n+Run list pipeline\n+    ${output} =         Execute          ozone scmcli pipeline list\n+                        Should contain   ${output}   Type:RATIS, Factor:ONE, State:OPEN\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/f267917ce3cf282b32166e39af871a8d1231d090/hadoop-ozone/dist/src/main/smoketest/scmcli/pipeline.robot",
                "sha": "6a6f0b0eb782aa1c7f2ba9744616b9f1a4bfafe9",
                "status": "added"
            }
        ],
        "message": "HDDS-2282. scmcli pipeline list command throws NullPointerException. Contributed by Xiaoyu Yao. (#1642)",
        "parent": "https://github.com/apache/hadoop/commit/9c72bf462196e1d71a243903b74e3c4673f29efb",
        "patched_files": [
            "XceiverClientManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestXceiverClientManager.java"
        ]
    },
    "hadoop_f39f8c5": {
        "bug_id": "hadoop_f39f8c5",
        "commit": "https://github.com/apache/hadoop/commit/f39f8c57344ede533ca4363c98230f3a0c401a76",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -45,3 +45,5 @@ IMPROVEMENTS:\n \n     HDFS-5390. Send one incremental block report per storage directory.\n     (Arpit Agarwal)\n+\n+    HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "cd139d4845e203e0125d72371d510352118fa6bc",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -180,10 +181,11 @@ public String toString() {\n     }\n   }\n   \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+                       String storageUuid, StorageType storageType) {\n     checkBlock(block);\n     for (BPServiceActor actor : bpServices) {\n-      actor.reportBadBlocks(block);\n+      actor.reportBadBlocks(block, storageUuid, storageType);\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "5d584616df335258b426878141001405130adb92",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hdfs.DFSUtil;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -237,12 +238,18 @@ void scheduleBlockReport(long delay) {\n     resetBlockReportTime = true; // reset future BRs for randomness\n   }\n \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+      String storageUuid, StorageType storageType) {\n     if (bpRegistration == null) {\n       return;\n     }\n     DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n-    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n+    String[] uuids = { storageUuid };\n+    StorageType[] types = { storageType };\n+    // TODO: Corrupt flag is set to false for compatibility. We can probably\n+    // set it to true here.\n+    LocatedBlock[] blocks = {\n+        new LocatedBlock(block, dnArr, uuids, types, -1, false) };\n     \n     try {\n       bpNamenode.reportBadBlocks(blocks);  ",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "sha": "172fb0fc30ec6cd3d5fdbe7dc2459e8c7d25cf48",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -559,7 +559,9 @@ public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid)\n    */\n   public void reportBadBlocks(ExtendedBlock block) throws IOException{\n     BPOfferService bpos = getBPOSForBlock(block);\n-    bpos.reportBadBlocks(block);\n+    FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    bpos.reportBadBlocks(\n+        block, volume.getStorageID(), volume.getStorageType());\n   }\n \n   /**\n@@ -1265,8 +1267,10 @@ private void transferBlock(ExtendedBlock block, DatanodeInfo xferTargets[])\n     // Check if NN recorded length matches on-disk length \n     long onDiskLength = data.getLength(block);\n     if (block.getNumBytes() > onDiskLength) {\n+      FsVolumeSpi volume = getFSDataset().getVolume(block);\n       // Shorter on-disk len indicates corruption so report NN the corrupt block\n-      bpos.reportBadBlocks(block);\n+      bpos.reportBadBlocks(\n+          block, volume.getStorageID(), volume.getStorageType());\n       LOG.warn(\"Can't replicate block \" + block\n           + \" because on-disk length \" + onDiskLength \n           + \" is shorter than NameNode recorded length \" + block.getNumBytes());",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "318d2f3705a05cad964d1b9b68ed52b7f4f3237a",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -198,7 +198,9 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n   //                 two maps. This might require some refactoring\n   //                 rewrite of FsDatasetImpl.\n   final ReplicaMap volumeMap;\n-  final Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap;\n+\n+  // Map from StorageID to ReplicaMap.\n+  final Map<String, ReplicaMap> perVolumeReplicaMap;\n \n \n   // Used for synchronizing access to usage stats\n@@ -249,7 +251,7 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n       LOG.info(\"Added volume - \" + dir + \", StorageType: \" + storageType);\n     }\n     volumeMap = new ReplicaMap(this);\n-    perVolumeReplicaMap = new HashMap<FsVolumeImpl, ReplicaMap>();\n+    perVolumeReplicaMap = new HashMap<String, ReplicaMap>();\n \n     @SuppressWarnings(\"unchecked\")\n     final VolumeChoosingPolicy<FsVolumeImpl> blockChooserImpl =\n@@ -628,7 +630,7 @@ private synchronized ReplicaBeingWritten append(String bpid,\n     \n     // Replace finalized replica by a RBW replica in replicas map\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(bpid, newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -759,7 +761,7 @@ public synchronized ReplicaInPipeline createRbw(ExtendedBlock b)\n     ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     return newReplicaInfo;\n   }\n   \n@@ -878,7 +880,7 @@ public synchronized ReplicaInPipeline convertTemporaryToRbw(\n     rbw.setBytesAcked(visible);\n     // overwrite the RBW in the volume map\n     volumeMap.add(b.getBlockPoolId(), rbw);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), rbw);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), rbw);\n     return rbw;\n   }\n \n@@ -898,7 +900,7 @@ public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n     ReplicaInPipeline newReplicaInfo = new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -967,7 +969,8 @@ private synchronized FinalizedReplica finalizeReplica(String bpid,\n       newReplicaInfo = new FinalizedReplica(replicaInfo, v, dest.getParentFile());\n     }\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(newReplicaInfo.getVolume()).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(newReplicaInfo.getVolume().getStorageID())\n+        .add(bpid, newReplicaInfo);\n     return newReplicaInfo;\n   }\n \n@@ -981,7 +984,7 @@ public synchronized void unfinalizeBlock(ExtendedBlock b) throws IOException {\n     if (replicaInfo != null && replicaInfo.getState() == ReplicaState.TEMPORARY) {\n       // remove from volumeMap\n       volumeMap.remove(b.getBlockPoolId(), b.getLocalBlock());\n-      perVolumeReplicaMap.get((FsVolumeImpl) replicaInfo.getVolume())\n+      perVolumeReplicaMap.get(replicaInfo.getVolume().getStorageID())\n           .remove(b.getBlockPoolId(), b.getLocalBlock());\n       \n       // delete the on-disk temp file\n@@ -1064,7 +1067,7 @@ public BlockListAsLongs getBlockReport(String bpid) {\n         new HashMap<String, BlockListAsLongs>();\n \n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       BlockListAsLongs blockList = getBlockReportWithReplicaMap(bpid, rMap);\n       blockReportMap.put(v.getStorageID(), blockList);\n     }\n@@ -1212,7 +1215,7 @@ public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n-        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n+        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n       }\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n@@ -1274,7 +1277,8 @@ public void checkDataDir() throws DiskErrorException {\n               LOG.warn(\"Removing replica \" + bpid + \":\" + b.getBlockId()\n                   + \" on failed volume \" + fv.getCurrentDir().getAbsolutePath());\n               ib.remove();\n-              perVolumeReplicaMap.get(fv).remove(bpid, b.getBlockId());\n+              perVolumeReplicaMap.get(fv.getStorageID())\n+                  .remove(bpid, b.getBlockId());\n               removedBlocks++;\n             }\n           }\n@@ -1391,8 +1395,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n           // Block is in memory and not on the disk\n           // Remove the block from volumeMap\n           volumeMap.remove(bpid, blockId);\n-          perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume())\n-              .remove(bpid, blockId);\n+          perVolumeReplicaMap.get(vol.getStorageID()).remove(bpid, blockId);\n           final DataBlockScanner blockScanner = datanode.getBlockScanner();\n           if (blockScanner != null) {\n             blockScanner.deleteBlock(bpid, new Block(blockId));\n@@ -1416,8 +1419,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n         ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n             diskFile.length(), diskGS, vol, diskFile.getParentFile());\n         volumeMap.add(bpid, diskBlockInfo);\n-        perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume()).\n-            remove(bpid, diskBlockInfo);\n+        perVolumeReplicaMap.get(vol.getStorageID())\n+            .remove(bpid, diskBlockInfo);\n         final DataBlockScanner blockScanner = datanode.getBlockScanner();\n         if (blockScanner != null) {\n           blockScanner.addBlock(new ExtendedBlock(bpid, diskBlockInfo));\n@@ -1695,7 +1698,7 @@ public synchronized void addBlockPool(String bpid, Configuration conf)\n \n     // TODO: Avoid the double scan.\n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       rMap.initBlockPool(bpid);\n       volumes.getVolumeMap(bpid, v, rMap);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "9077c40a8367a9f4bdddc10e439a1e95920ef481",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "patch": "@@ -90,13 +90,13 @@ long getRemaining() throws IOException {\n     return remaining;\n   }\n     \n-  void initializeReplicaMaps(Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap,\n+  void initializeReplicaMaps(Map<String, ReplicaMap> perVolumeReplicaMap,\n                              ReplicaMap globalReplicaMap,\n                              Object mutex) throws IOException {\n     for (FsVolumeImpl v : volumes) {\n       ReplicaMap rMap = new ReplicaMap(mutex);\n       v.getVolumeMap(rMap);\n-      perVolumeReplicaMap.put(v, rMap);\n+      perVolumeReplicaMap.put(v.getStorageID(), rMap);\n       globalReplicaMap.addAll(rMap);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "sha": "671996718be2eea6db9b076cf8a5aed4de36705e",
                "status": "modified"
            }
        ],
        "message": "HDFS-5401. Fix NPE in Directory Scanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1535158 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b442fe92fbaeb6bd891b94c50d6086a46d4af4ac",
        "patched_files": [
            "BPOfferService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBPOfferService.java"
        ]
    },
    "hadoop_f3f5e7a": {
        "bug_id": "hadoop_f3f5e7a",
        "commit": "https://github.com/apache/hadoop/commit/f3f5e7ad005a88afad6fa09602073eaa450e21ed",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -2447,7 +2447,7 @@ public long getProvidedCapacity() {\n     return providedStorageMap.getCapacity();\n   }\n \n-  public void updateHeartbeat(DatanodeDescriptor node, StorageReport[] reports,\n+  void updateHeartbeat(DatanodeDescriptor node, StorageReport[] reports,\n       long cacheCapacity, long cacheUsed, int xceiverCount, int failedVolumes,\n       VolumeFailureSummary volumeFailureSummary) {\n \n@@ -2458,6 +2458,17 @@ public void updateHeartbeat(DatanodeDescriptor node, StorageReport[] reports,\n         failedVolumes, volumeFailureSummary);\n   }\n \n+  void updateHeartbeatState(DatanodeDescriptor node,\n+      StorageReport[] reports, long cacheCapacity, long cacheUsed,\n+      int xceiverCount, int failedVolumes,\n+      VolumeFailureSummary volumeFailureSummary) {\n+    for (StorageReport report: reports) {\n+      providedStorageMap.updateStorage(node, report.getStorage());\n+    }\n+    node.updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n+        failedVolumes, volumeFailureSummary);\n+  }\n+\n   /**\n    * StatefulBlockInfo is used to build the \"toUC\" list, which is a list of\n    * updates to the information about under-construction blocks.",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "a5fb0b17303cef37f436f4148e499a5fc3831aec",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
                "patch": "@@ -373,7 +373,7 @@ public int numBlocks() {\n   /**\n    * Updates stats from datanode heartbeat.\n    */\n-  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n+  void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures,\n       VolumeFailureSummary volumeFailureSummary) {\n     updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n@@ -384,7 +384,7 @@ public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n   /**\n    * process datanode heartbeat or stats initialization.\n    */\n-  public void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,\n+  void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures,\n       VolumeFailureSummary volumeFailureSummary) {\n     updateStorageStats(reports, cacheCapacity, cacheUsed, xceiverCount,",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
                "sha": "6aa23765c766f260c18fbb70deafd00141d2a8ff",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
                "patch": "@@ -251,7 +251,7 @@ synchronized void updateLifeline(final DatanodeDescriptor node,\n     // updateHeartbeat, because we don't want to modify the\n     // heartbeatedSinceRegistration flag.  Arrival of a lifeline message does\n     // not count as arrival of the first heartbeat.\n-    node.updateHeartbeatState(reports, cacheCapacity, cacheUsed,\n+    blockManager.updateHeartbeatState(node, reports, cacheCapacity, cacheUsed,\n         xceiverCount, failedVolumes, volumeFailureSummary);\n     stats.add(node);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
                "sha": "d2c279f8b4a3b484039b2a7cd77b06fbe68c772f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import static java.util.concurrent.TimeUnit.SECONDS;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_LIFELINE_INTERVAL_SECONDS_KEY;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY;\n@@ -196,6 +197,10 @@ public void testSendLifelineIfHeartbeatBlocked() throws Exception {\n           namesystem.getNumDeadDataNodes());\n       assertEquals(\"Expect DataNode not marked stale due to lifeline.\", 0,\n           namesystem.getNumStaleDataNodes());\n+      // add a new volume on the next heartbeat\n+      cluster.getDataNodes().get(0).reconfigurePropertyImpl(\n+          DFS_DATANODE_DATA_DIR_KEY,\n+          cluster.getDataDirectory().concat(\"/data-new\"));\n     }\n \n     // Verify that we did in fact call the lifeline RPC.",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java",
                "sha": "14134e697dbd35729448467eee1520de938e6cfc",
                "status": "modified"
            }
        ],
        "message": "HDFS-14042. Fix NPE when PROVIDED storage is missing. Contributed by Virajith Jalaparti.",
        "parent": "https://github.com/apache/hadoop/commit/50f40e0536f38517aa33e8859f299bcf19f2f319",
        "patched_files": [
            "DatanodeDescriptor.java",
            "HeartbeatManager.java",
            "BlockManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDataNodeLifeline.java",
            "TestBlockManager.java",
            "TestDatanodeDescriptor.java"
        ]
    },
    "hadoop_f4d5d20": {
        "bug_id": "hadoop_f4d5d20",
        "commit": "https://github.com/apache/hadoop/commit/f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 27,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "patch": "@@ -132,7 +132,6 @@\n   private AMRMClientAsync<AMRMClient.ContainerRequest> amRMClient;\n   private NMClientAsync nmClient;\n   private AsyncDispatcher dispatcher;\n-  AsyncDispatcher compInstanceDispatcher;\n   private YarnRegistryViewForProviders yarnRegistryOperations;\n   private ServiceContext context;\n   private ContainerLaunchService containerLaunchService;\n@@ -152,7 +151,7 @@ public void buildInstance(ServiceContext context, Configuration configuration)\n     yarnRegistryOperations =\n         createYarnRegistryOperations(context, registryClient);\n \n-    // register metrics\n+    // register metrics,\n     serviceMetrics = ServiceMetrics\n         .register(app.getName(), \"Metrics for service\");\n     serviceMetrics.tag(\"type\", \"Metrics type [component or service]\", \"service\");\n@@ -167,14 +166,11 @@ public void buildInstance(ServiceContext context, Configuration configuration)\n     dispatcher = new AsyncDispatcher(\"Component  dispatcher\");\n     dispatcher.register(ComponentEventType.class,\n         new ComponentEventHandler());\n+    dispatcher.register(ComponentInstanceEventType.class,\n+        new ComponentInstanceEventHandler());\n     dispatcher.setDrainEventsOnStop();\n     addIfService(dispatcher);\n \n-    compInstanceDispatcher =\n-        new AsyncDispatcher(\"CompInstance dispatcher\");\n-    compInstanceDispatcher.register(ComponentInstanceEventType.class,\n-        new ComponentInstanceEventHandler());\n-    addIfService(compInstanceDispatcher);\n     containerLaunchService = new ContainerLaunchService(context.fs);\n     addService(containerLaunchService);\n \n@@ -277,10 +273,10 @@ public void serviceStart() throws Exception {\n   }\n \n   private void recoverComponents(RegisterApplicationMasterResponse response) {\n-    List<Container> recoveredContainers = response\n+    List<Container> containersFromPrevAttempt = response\n         .getContainersFromPreviousAttempts();\n     LOG.info(\"Received {} containers from previous attempt.\",\n-        recoveredContainers.size());\n+        containersFromPrevAttempt.size());\n     Map<String, ServiceRecord> existingRecords = new HashMap<>();\n     List<String> existingComps = null;\n     try {\n@@ -302,9 +298,8 @@ private void recoverComponents(RegisterApplicationMasterResponse response) {\n         }\n       }\n     }\n-    for (Container container : recoveredContainers) {\n-      LOG.info(\"Handling container {} from previous attempt\",\n-          container.getId());\n+    for (Container container : containersFromPrevAttempt) {\n+      LOG.info(\"Handling {} from previous attempt\", container.getId());\n       ServiceRecord record = existingRecords.get(RegistryPathUtils\n           .encodeYarnID(container.getId().toString()));\n       if (record != null) {\n@@ -487,16 +482,21 @@ public void onContainersAllocated(List<Container> containers) {\n             new ComponentEvent(comp.getName(), CONTAINER_ALLOCATED)\n                 .setContainer(container);\n         dispatcher.getEventHandler().handle(event);\n-        Collection<AMRMClient.ContainerRequest> requests = amRMClient\n-            .getMatchingRequests(container.getAllocationRequestId());\n-        LOG.info(\"[COMPONENT {}]: {} outstanding container requests.\",\n-            comp.getName(), requests.size());\n-        // remove the corresponding request\n-        if (requests.iterator().hasNext()) {\n-          LOG.info(\"[COMPONENT {}]: removing one container request.\", comp\n-              .getName());\n-          AMRMClient.ContainerRequest request = requests.iterator().next();\n-          amRMClient.removeContainerRequest(request);\n+        try {\n+          Collection<AMRMClient.ContainerRequest> requests = amRMClient\n+              .getMatchingRequests(container.getAllocationRequestId());\n+          LOG.info(\"[COMPONENT {}]: remove {} outstanding container requests \" +\n+                  \"for allocateId \" + container.getAllocationRequestId(),\n+              comp.getName(), requests.size());\n+          // remove the corresponding request\n+          if (requests.iterator().hasNext()) {\n+            AMRMClient.ContainerRequest request = requests.iterator().next();\n+            amRMClient.removeContainerRequest(request);\n+          }\n+        } catch(Exception e) {\n+          //TODO Due to YARN-7490, exception may be thrown, catch and ignore for\n+          //now.\n+          LOG.error(\"Exception when removing the matching requests. \", e);\n         }\n       }\n     }\n@@ -569,7 +569,7 @@ public void onContainersUpdated(List<UpdatedContainer> containers) {\n       }\n       ComponentEvent event =\n           new ComponentEvent(instance.getCompName(), CONTAINER_STARTED)\n-              .setInstance(instance);\n+              .setInstance(instance).setContainerId(containerId);\n       dispatcher.getEventHandler().handle(event);\n     }\n \n@@ -649,10 +649,6 @@ public void removeLiveCompInstance(ContainerId containerId) {\n     liveInstances.remove(containerId);\n   }\n \n-  public AsyncDispatcher getCompInstanceDispatcher() {\n-    return compInstanceDispatcher;\n-  }\n-\n   public YarnRegistryViewForProviders getYarnRegistryOperations() {\n     return yarnRegistryOperations;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "sha": "6bc567328fe0f91693e239efbdcc1783160a428d",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 36,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "patch": "@@ -82,7 +82,8 @@\n   private Map<String, ComponentInstance> compInstances =\n       new ConcurrentHashMap<>();\n   // component instances to be assigned with a container\n-  private List<ComponentInstance> pendingInstances = new LinkedList<>();\n+  private List<ComponentInstance> pendingInstances =\n+      Collections.synchronizedList(new LinkedList<>());\n   private ContainerFailureTracker failureTracker;\n   private Probe probe;\n   private final ReentrantReadWriteLock.ReadLock readLock;\n@@ -94,7 +95,7 @@\n \n   private StateMachine<ComponentState, ComponentEventType, ComponentEvent>\n       stateMachine;\n-  private AsyncDispatcher compInstanceDispatcher;\n+  private AsyncDispatcher dispatcher;\n   private static final StateMachineFactory<Component, ComponentState, ComponentEventType, ComponentEvent>\n       stateMachineFactory =\n       new StateMachineFactory<Component, ComponentState, ComponentEventType, ComponentEvent>(\n@@ -149,7 +150,7 @@ public Component(\n     this.readLock = lock.readLock();\n     this.writeLock = lock.writeLock();\n     this.stateMachine = stateMachineFactory.make(this);\n-    compInstanceDispatcher = scheduler.getCompInstanceDispatcher();\n+    dispatcher = scheduler.getDispatcher();\n     failureTracker =\n         new ContainerFailureTracker(context, this);\n     probe = MonitorUtils.getProbe(componentSpec.getReadinessCheck());\n@@ -256,30 +257,18 @@ public void transition(Component component, ComponentEvent event) {\n         component.releaseContainer(container);\n         return;\n       }\n-      if (instance.hasContainer()) {\n-        LOG.info(\n-            \"[COMPONENT {}]: Instance {} already has container, release \" +\n-                \"surplus container {}\",\n-            instance.getCompName(), instance.getCompInstanceId(), container\n-                .getId());\n-        component.releaseContainer(container);\n-        return;\n-      }\n+\n       component.pendingInstances.remove(instance);\n-      LOG.info(\"[COMPONENT {}]: Recovered {} for component instance {} on \" +\n-              \"host {}, num pending component instances reduced to {} \",\n-          component.getName(), container.getId(), instance\n-              .getCompInstanceName(), container.getNodeId(), component\n-              .pendingInstances.size());\n       instance.setContainer(container);\n       ProviderUtils.initCompInstanceDir(component.getContext().fs, instance);\n       component.getScheduler().addLiveCompInstance(container.getId(), instance);\n-      LOG.info(\"[COMPONENT {}]: Marking {} as started for component \" +\n-          \"instance {}\", component.getName(), event.getContainer().getId(),\n-          instance.getCompInstanceId());\n-      component.compInstanceDispatcher.getEventHandler().handle(\n-          new ComponentInstanceEvent(instance.getContainerId(),\n-              START));\n+      LOG.info(\"[COMPONENT {}]: Recovered {} for component instance {} on \" +\n+              \"host {}, num pending component instances reduced to {} \",\n+          component.getName(), container.getId(),\n+          instance.getCompInstanceName(), container.getNodeId(),\n+          component.pendingInstances.size());\n+      component.dispatcher.getEventHandler().handle(\n+          new ComponentInstanceEvent(container.getId(), START));\n     }\n   }\n \n@@ -288,9 +277,8 @@ public void transition(Component component, ComponentEvent event) {\n \n     @Override public ComponentState transition(Component component,\n         ComponentEvent event) {\n-      component.compInstanceDispatcher.getEventHandler().handle(\n-          new ComponentInstanceEvent(event.getInstance().getContainerId(),\n-              START));\n+      component.dispatcher.getEventHandler().handle(\n+          new ComponentInstanceEvent(event.getContainerId(), START));\n       return checkIfStable(component);\n     }\n   }\n@@ -313,23 +301,16 @@ private static ComponentState checkIfStable(Component component) {\n     @Override\n     public void transition(Component component, ComponentEvent event) {\n       component.updateMetrics(event.getStatus());\n-\n-      // add back to pending list\n-      component.pendingInstances.add(event.getInstance());\n-      LOG.info(\n-          \"[COMPONENT {}]: {} completed, num pending comp instances increased to {}.\",\n-          component.getName(), event.getStatus().getContainerId(),\n-          component.pendingInstances.size());\n-      component.compInstanceDispatcher.getEventHandler().handle(\n+      component.dispatcher.getEventHandler().handle(\n           new ComponentInstanceEvent(event.getStatus().getContainerId(),\n               STOP).setStatus(event.getStatus()));\n       component.componentSpec.setState(\n           org.apache.hadoop.yarn.service.api.records.ComponentState.FLEXING);\n     }\n   }\n \n-  public ServiceMetrics getCompMetrics () {\n-    return componentMetrics;\n+  public void reInsertPendingInstance(ComponentInstance instance) {\n+    pendingInstances.add(instance);\n   }\n \n   private void releaseContainer(Container container) {\n@@ -581,4 +562,9 @@ public void handle(ComponentEvent event) {\n   public ServiceContext getContext() {\n     return context;\n   }\n+\n+  // Only for testing\n+  public List<ComponentInstance> getPendingInstances() {\n+    return pendingInstances;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "sha": "88f47635e275a3d4e85b2954fab4db0dc4ad75bf",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.service.component;\n \n import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.event.AbstractEvent;\n import org.apache.hadoop.yarn.service.component.instance.ComponentInstance;\n@@ -30,6 +31,16 @@\n   private Container container;\n   private ComponentInstance instance;\n   private ContainerStatus status;\n+  private ContainerId containerId;\n+\n+  public ContainerId getContainerId() {\n+    return containerId;\n+  }\n+\n+  public ComponentEvent setContainerId(ContainerId containerId) {\n+    this.containerId = containerId;\n+    return this;\n+  }\n \n   public ComponentEvent(String name, ComponentEventType type) {\n     super(type);",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java",
                "sha": "447b436fc9d9853a06901c7548ba584c5f2ae357",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 39,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "patch": "@@ -146,7 +146,7 @@ public ComponentInstance(Component component,\n       compInstance.containerStatusFuture =\n           compInstance.scheduler.executorService.scheduleAtFixedRate(\n               new ContainerStatusRetriever(compInstance.scheduler,\n-                  compInstance.getContainerId(), compInstance), 0, 1,\n+                  event.getContainerId(), compInstance), 0, 1,\n               TimeUnit.SECONDS);\n       compInstance.component.incRunningContainers();\n       long containerStartTime = System.currentTimeMillis();\n@@ -160,10 +160,10 @@ public ComponentInstance(Component component,\n       }\n       org.apache.hadoop.yarn.service.api.records.Container container =\n           new org.apache.hadoop.yarn.service.api.records.Container();\n-      container.setId(compInstance.getContainerId().toString());\n+      container.setId(event.getContainerId().toString());\n       container.setLaunchTime(new Date(containerStartTime));\n       container.setState(ContainerState.RUNNING_BUT_UNREADY);\n-      container.setBareHost(compInstance.container.getNodeId().getHost());\n+      container.setBareHost(compInstance.getNodeId().getHost());\n       container.setComponentInstanceName(compInstance.getCompInstanceName());\n       if (compInstance.containerSpec != null) {\n         // remove the previous container.\n@@ -219,15 +219,11 @@ public void transition(ComponentInstance compInstance,\n       // re-ask the failed container.\n       Component comp = compInstance.component;\n       comp.requestContainers(1);\n-      LOG.info(compInstance.getCompInstanceId()\n-              + \": Container completed. Requested a new container.\" + System\n-              .lineSeparator() + \" exitStatus={}, diagnostics={}.\",\n-          event.getStatus().getExitStatus(),\n-          event.getStatus().getDiagnostics());\n       String containerDiag =\n           compInstance.getCompInstanceId() + \": \" + event.getStatus()\n               .getDiagnostics();\n       compInstance.diagnostics.append(containerDiag + System.lineSeparator());\n+      compInstance.cancelContainerStatusRetriever();\n \n       if (compInstance.getState().equals(READY)) {\n         compInstance.component.decContainersReady();\n@@ -255,18 +251,28 @@ public void transition(ComponentInstance compInstance,\n         // hdfs dir content will be overwritten when a new container gets started,\n         // so no need remove.\n         compInstance.scheduler.executorService\n-            .submit(compInstance::cleanupRegistry);\n+            .submit(() -> compInstance.cleanupRegistry(event.getContainerId()));\n+\n         if (compInstance.timelineServiceEnabled) {\n           // record in ATS\n-          compInstance.serviceTimelinePublisher.componentInstanceFinished\n-              (compInstance, event.getStatus().getExitStatus(), containerDiag);\n+          compInstance.serviceTimelinePublisher\n+              .componentInstanceFinished(event.getContainerId(),\n+                  event.getStatus().getExitStatus(), containerDiag);\n         }\n         compInstance.containerSpec.setState(ContainerState.STOPPED);\n       }\n \n       // remove the failed ContainerId -> CompInstance mapping\n       comp.getScheduler().removeLiveCompInstance(event.getContainerId());\n \n+      comp.reInsertPendingInstance(compInstance);\n+\n+      LOG.info(compInstance.getCompInstanceId()\n+              + \": {} completed. Reinsert back to pending list and requested \" +\n+              \"a new container.\" + System.lineSeparator() +\n+              \" exitStatus={}, diagnostics={}.\",\n+          event.getContainerId(), event.getStatus().getExitStatus(),\n+          event.getStatus().getDiagnostics());\n       if (shouldExit) {\n         // Sleep for 5 seconds in hope that the state can be recorded in ATS.\n         // in case there's a client polling the comp state, it can be notified.\n@@ -277,8 +283,6 @@ public void transition(ComponentInstance compInstance,\n         }\n         ExitUtil.terminate(-1);\n       }\n-\n-      compInstance.removeContainer();\n     }\n   }\n \n@@ -312,15 +316,6 @@ public void handle(ComponentInstanceEvent event) {\n     }\n   }\n \n-  public boolean hasContainer() {\n-    return this.container != null;\n-  }\n-\n-  public void removeContainer() {\n-    this.container = null;\n-    this.compInstanceId.setContainerId(null);\n-  }\n-\n   public void setContainer(Container container) {\n     this.container = container;\n     this.compInstanceId.setContainerId(container.getId());\n@@ -337,7 +332,7 @@ public ContainerStatus getContainerStatus() {\n   public void updateContainerStatus(ContainerStatus status) {\n     this.status = status;\n     org.apache.hadoop.yarn.service.api.records.Container container =\n-        getCompSpec().getContainer(getContainerId().toString());\n+        getCompSpec().getContainer(status.getContainerId().toString());\n     if (container != null) {\n       container.setIp(StringUtils.join(\",\", status.getIPs()));\n       container.setHostname(status.getHost());\n@@ -348,10 +343,6 @@ public void updateContainerStatus(ContainerStatus status) {\n     updateServiceRecord(yarnRegistryOperations, status);\n   }\n \n-  public ContainerId getContainerId() {\n-    return container.getId();\n-  }\n-\n   public String getCompName() {\n     return compInstanceId.getCompName();\n   }\n@@ -423,12 +414,7 @@ private  void updateServiceRecord(\n   public void destroy() {\n     LOG.info(getCompInstanceId() + \": Flexed down by user, destroying.\");\n     diagnostics.append(getCompInstanceId() + \": Flexed down by user\");\n-    if (container != null) {\n-      scheduler.removeLiveCompInstance(container.getId());\n-      component.getScheduler().getAmRMClient()\n-          .releaseAssignedContainer(container.getId());\n-      getCompSpec().removeContainer(containerSpec);\n-    }\n+\n     // update metrics\n     if (getState() == STARTED) {\n       component.decRunningContainers();\n@@ -437,16 +423,29 @@ public void destroy() {\n       component.decContainersReady();\n       component.decRunningContainers();\n     }\n+    getCompSpec().removeContainer(containerSpec);\n+\n+    if (container == null) {\n+      LOG.info(getCompInstanceId() + \" no container is assigned when \" +\n+          \"destroying\");\n+      return;\n+    }\n+\n+    ContainerId containerId = container.getId();\n+    scheduler.removeLiveCompInstance(containerId);\n+    component.getScheduler().getAmRMClient()\n+        .releaseAssignedContainer(containerId);\n \n     if (timelineServiceEnabled) {\n-      serviceTimelinePublisher.componentInstanceFinished(this,\n+      serviceTimelinePublisher.componentInstanceFinished(containerId,\n           KILLED_BY_APPMASTER, diagnostics.toString());\n     }\n-    scheduler.executorService.submit(this::cleanupRegistryAndCompHdfsDir);\n+    cancelContainerStatusRetriever();\n+    scheduler.executorService.submit(() ->\n+        cleanupRegistryAndCompHdfsDir(containerId));\n   }\n \n-  private void cleanupRegistry() {\n-    ContainerId containerId = getContainerId();\n+  private void cleanupRegistry(ContainerId containerId) {\n     String cid = RegistryPathUtils.encodeYarnID(containerId.toString());\n     try {\n        yarnRegistryOperations.deleteComponent(getCompInstanceId(), cid);\n@@ -456,8 +455,8 @@ private void cleanupRegistry() {\n   }\n \n   //TODO Maybe have a dedicated cleanup service.\n-  public void cleanupRegistryAndCompHdfsDir() {\n-    cleanupRegistry();\n+  public void cleanupRegistryAndCompHdfsDir(ContainerId containerId) {\n+    cleanupRegistry(containerId);\n     try {\n       if (compInstanceDir != null && fs.exists(compInstanceDir)) {\n         boolean deleted = fs.delete(compInstanceDir, true);\n@@ -515,6 +514,12 @@ public void cleanupRegistryAndCompHdfsDir() {\n     }\n   }\n \n+  private void cancelContainerStatusRetriever() {\n+    if (containerStatusFuture != null && !containerStatusFuture.isDone()) {\n+      containerStatusFuture.cancel(true);\n+    }\n+  }\n+\n   @Override\n   public int compareTo(ComponentInstance to) {\n     long delta = containerStartedTime - to.containerStartedTime;",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "sha": "509f6675006d4a3ebfa9da02234bbb821e739c8a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java",
                "patch": "@@ -87,7 +87,7 @@ public ContainerLauncher(\n       AbstractLauncher launcher = new AbstractLauncher(fs, null);\n       try {\n         provider.buildContainerLaunchContext(launcher, service,\n-            instance, fs, getConfig());\n+            instance, fs, getConfig(), container);\n         instance.getComponent().getScheduler().getNmClient()\n             .startContainerAsync(container,\n                 launcher.completeContainerLaunch());",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java",
                "sha": "b9f3a245c69b27859cc393ef4b0105bf29659259",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.api.ApplicationConstants;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.conf.YarnServiceConf;\n import org.apache.hadoop.yarn.service.api.records.Component;\n@@ -55,7 +56,7 @@ public abstract void processArtifact(AbstractLauncher launcher,\n \n   public void buildContainerLaunchContext(AbstractLauncher launcher,\n       Service service, ComponentInstance instance,\n-      SliderFileSystem fileSystem, Configuration yarnConf)\n+      SliderFileSystem fileSystem, Configuration yarnConf, Container container)\n       throws IOException, SliderException {\n     Component component = instance.getComponent().getComponentSpec();;\n     processArtifact(launcher, instance, fileSystem, service);\n@@ -67,7 +68,7 @@ public void buildContainerLaunchContext(AbstractLauncher launcher,\n     Map<String, String> globalTokens =\n         instance.getComponent().getScheduler().globalTokens;\n     Map<String, String> tokensForSubstitution = ProviderUtils\n-        .initCompTokensForSubstitute(instance);\n+        .initCompTokensForSubstitute(instance, container);\n     tokensForSubstitution.putAll(globalTokens);\n     // Set the environment variables in launcher\n     launcher.putEnv(ServiceUtils",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "sha": "70155915ea627ab730acfc4a0aa36ad1c24c83c6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.service.provider;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.utils.SliderFileSystem;\n import org.apache.hadoop.yarn.service.exceptions.SliderException;\n@@ -34,6 +35,6 @@\n    */\n   void buildContainerLaunchContext(AbstractLauncher containerLauncher,\n       Service service, ComponentInstance instance,\n-      SliderFileSystem sliderFileSystem, Configuration yarnConf)\n-      throws IOException, SliderException;\n+      SliderFileSystem sliderFileSystem, Configuration yarnConf, Container\n+      container) throws IOException, SliderException;\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java",
                "sha": "11015ea17504455bfdc21a02b17316b1a346352a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.LocalResource;\n import org.apache.hadoop.yarn.api.records.LocalResourceType;\n import org.apache.hadoop.yarn.service.ServiceContext;\n@@ -393,13 +394,13 @@ private static void resolvePlainTemplateAndSaveOnHdfs(FileSystem fs,\n    * @return tokens to replace\n    */\n   public static Map<String, String> initCompTokensForSubstitute(\n-      ComponentInstance instance) {\n+      ComponentInstance instance, Container container) {\n     Map<String, String> tokens = new HashMap<>();\n     tokens.put(COMPONENT_NAME, instance.getCompSpec().getName());\n     tokens\n         .put(COMPONENT_NAME_LC, instance.getCompSpec().getName().toLowerCase());\n     tokens.put(COMPONENT_INSTANCE_NAME, instance.getCompInstanceName());\n-    tokens.put(CONTAINER_ID, instance.getContainer().getId().toString());\n+    tokens.put(CONTAINER_ID, container.getId().toString());\n     tokens.put(COMPONENT_ID,\n         String.valueOf(instance.getCompInstanceId().getId()));\n     tokens.putAll(instance.getComponent().getDependencyHostIpTokens());",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java",
                "sha": "c0c44c3db23b3247b905c9d61304f5ff8a82a875",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.hadoop.metrics2.AbstractMetric;\n import org.apache.hadoop.service.CompositeService;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent;\n@@ -178,10 +179,10 @@ public void componentInstanceStarted(Container container,\n     putEntity(entity);\n   }\n \n-  public void componentInstanceFinished(ComponentInstance instance,\n+  public void componentInstanceFinished(ContainerId containerId,\n       int exitCode, String diagnostics) {\n     TimelineEntity entity = createComponentInstanceEntity(\n-        instance.getContainer().getId().toString());\n+        containerId.toString());\n \n     // create info keys\n     Map<String, Object> entityInfos = new HashMap<String, Object>();",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java",
                "sha": "949ce19c8dc1880107dc1e7fdcaff7a4ddc6fc60",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 14,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java",
                "patch": "@@ -24,14 +24,8 @@\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse;\n import org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse;\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n-import org.apache.hadoop.yarn.api.records.Container;\n-import org.apache.hadoop.yarn.api.records.ContainerId;\n-import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n-import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Priority;\n-import org.apache.hadoop.yarn.api.records.Resource;\n+\n+import org.apache.hadoop.yarn.api.records.*;\n import org.apache.hadoop.yarn.client.api.AMRMClient;\n import org.apache.hadoop.yarn.client.api.NMClient;\n import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n@@ -42,15 +36,15 @@\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.component.Component;\n import org.apache.hadoop.yarn.service.component.ComponentState;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstance;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstanceState;\n import org.apache.hadoop.yarn.service.exceptions.BadClusterStateException;\n import org.apache.hadoop.yarn.service.registry.YarnRegistryViewForProviders;\n import org.apache.hadoop.yarn.service.utils.SliderFileSystem;\n+import org.apache.hadoop.yarn.util.Records;\n \n import java.io.IOException;\n-import java.util.Collections;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n-import java.util.List;\n+import java.util.*;\n import java.util.concurrent.TimeoutException;\n \n import static org.mockito.Mockito.mock;\n@@ -63,6 +57,8 @@\n   final List<Container> feedContainers =\n       Collections.synchronizedList(new LinkedList<>());\n \n+  final List<ContainerStatus> failedContainers =\n+      Collections.synchronizedList(new LinkedList<>());\n   public MockServiceAM(Service service) {\n     super(service.getName());\n     this.service = service;\n@@ -102,10 +98,10 @@ protected YarnRegistryViewForProviders createYarnRegistryOperations(\n \n             AllocateResponse.AllocateResponseBuilder builder =\n                 AllocateResponse.newBuilder();\n+            // add new containers if any\n             synchronized (feedContainers) {\n               if (feedContainers.isEmpty()) {\n                 System.out.println(\"Allocating........ no containers\");\n-                return builder.build();\n               } else {\n                 // The AMRMClient will return containers for compoenent that are\n                 // at FLEXING state\n@@ -121,9 +117,20 @@ protected YarnRegistryViewForProviders createYarnRegistryOperations(\n                     itor.remove();\n                   }\n                 }\n-                return builder.allocatedContainers(allocatedContainers).build();\n+                builder.allocatedContainers(allocatedContainers);\n+              }\n+            }\n+\n+            // add failed containers if any\n+            synchronized (failedContainers) {\n+              if (!failedContainers.isEmpty()) {\n+                List<ContainerStatus> failed =\n+                    new LinkedList<>(failedContainers);\n+                failedContainers.clear();\n+                builder.completedContainersStatuses(failed);\n               }\n             }\n+            return builder.build();\n           }\n \n           @Override\n@@ -184,6 +191,19 @@ public Container feedContainerToComp(Service service, int id,\n     return container;\n   }\n \n+  public void feedFailedContainerToComp(Service service, int id, String\n+      compName) {\n+    ApplicationId applicationId = ApplicationId.fromString(service.getId());\n+    ContainerId containerId = ContainerId\n+        .newContainerId(ApplicationAttemptId.newInstance(applicationId, 1), id);\n+    ContainerStatus containerStatus = Records.newRecord(ContainerStatus.class);\n+    containerStatus.setContainerId(containerId);\n+    synchronized (failedContainers) {\n+      failedContainers.add(containerStatus);\n+    }\n+  }\n+\n+\n   public void flexComponent(String compName, long numberOfContainers)\n       throws IOException {\n     ClientAMProtocol.ComponentCountProto componentCountProto =\n@@ -218,4 +238,22 @@ public void waitForNumDesiredContainers(String compName,\n       }\n     }, 1000, 20000);\n   }\n+\n+\n+  public ComponentInstance getCompInstance(String compName, String\n+      instanceName) {\n+    return context.scheduler.getAllComponents().get(compName)\n+        .getComponentInstance(instanceName);\n+  }\n+\n+  public void waitForCompInstanceState(ComponentInstance instance,\n+      ComponentInstanceState state)\n+      throws TimeoutException, InterruptedException {\n+    GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+      @Override\n+      public Boolean get() {\n+        return instance.getState().equals(state);\n+      }\n+    }, 1000, 20000);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java",
                "sha": "429816137c5f74eb220ea0c15cab54196acbd01f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java",
                "patch": "@@ -65,6 +65,7 @@\n \n   private MiniYARNCluster yarnCluster = null;\n   private MiniDFSCluster hdfsCluster = null;\n+  TestingCluster zkCluster;\n   private FileSystem fs = null;\n   private Configuration conf = null;\n   public static final int NUM_NMS = 1;\n@@ -165,7 +166,6 @@ protected void setupInternal(int numNodeManager)\n     conf.setBoolean(NM_VMEM_CHECK_ENABLED, false);\n     conf.setBoolean(NM_PMEM_CHECK_ENABLED, false);\n     // setup zk cluster\n-    TestingCluster zkCluster;\n     zkCluster = new TestingCluster(1);\n     zkCluster.start();\n     conf.set(YarnConfiguration.RM_ZK_ADDRESS, zkCluster.getConnectString());\n@@ -239,6 +239,9 @@ public void shutdown() throws IOException {\n         hdfsCluster = null;\n       }\n     }\n+    if (zkCluster != null) {\n+      zkCluster.stop();\n+    }\n     if (basedir != null) {\n       FileUtils.deleteDirectory(basedir);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java",
                "sha": "a70a0c280d377232fdc5a4bb301e7bb0b8e090b4",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java",
                "patch": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.service;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.curator.test.TestingCluster;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.service.api.records.Service;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstance;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstanceState;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.concurrent.TimeoutException;\n+\n+import static org.apache.hadoop.registry.client.api.RegistryConstants\n+    .KEY_REGISTRY_ZK_QUORUM;\n+\n+public class TestServiceAM extends ServiceTestUtils{\n+\n+  private File basedir;\n+  YarnConfiguration conf = new YarnConfiguration();\n+  TestingCluster zkCluster;\n+\n+  @Before\n+  public void setup() throws Exception {\n+    basedir = new File(\"target\", \"apps\");\n+    if (basedir.exists()) {\n+      FileUtils.deleteDirectory(basedir);\n+    } else {\n+      basedir.mkdirs();\n+    }\n+    zkCluster = new TestingCluster(1);\n+    zkCluster.start();\n+    conf.set(KEY_REGISTRY_ZK_QUORUM, zkCluster.getConnectString());\n+    System.out.println(\"ZK cluster: \" +  zkCluster.getConnectString());\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (basedir != null) {\n+      FileUtils.deleteDirectory(basedir);\n+    }\n+    if (zkCluster != null) {\n+      zkCluster.stop();\n+    }\n+  }\n+\n+  // Race condition YARN-7486\n+  // 1. Allocate 1 container to compa and wait it to be started\n+  // 2. Fail this container, and in the meanwhile allocate the 2nd container.\n+  // 3. The 2nd container should not be assigned to compa-0 instance, because\n+  //   the compa-0 instance is not stopped yet.\n+  // 4. check compa still has the instance in the pending list.\n+  @Test\n+  public void testContainerCompleted() throws TimeoutException,\n+      InterruptedException {\n+    ApplicationId applicationId = ApplicationId.newInstance(123456, 1);\n+    Service exampleApp = new Service();\n+    exampleApp.setId(applicationId.toString());\n+    exampleApp.setName(\"testContainerCompleted\");\n+    exampleApp.addComponent(createComponent(\"compa\", 1, \"pwd\"));\n+\n+    MockServiceAM am = new MockServiceAM(exampleApp);\n+    am.init(conf);\n+    am.start();\n+\n+    ComponentInstance compa0 = am.getCompInstance(\"compa\", \"compa-0\");\n+    // allocate a container\n+    am.feedContainerToComp(exampleApp, 1, \"compa\");\n+    am.waitForCompInstanceState(compa0, ComponentInstanceState.STARTED);\n+\n+    System.out.println(\"Fail the container 1\");\n+    // fail the container\n+    am.feedFailedContainerToComp(exampleApp, 1, \"compa\");\n+\n+    // allocate the second container immediately, this container will not be\n+    // assigned to comp instance\n+    // because the instance is not yet added to the pending list.\n+    am.feedContainerToComp(exampleApp, 2, \"compa\");\n+\n+    am.waitForCompInstanceState(compa0, ComponentInstanceState.INIT);\n+    // still 1 pending instance\n+    Assert.assertEquals(1,\n+        am.getComponent(\"compa\").getPendingInstances().size());\n+    am.stop();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java",
                "sha": "fb4de0d57147e7e087c17670ebbb615d3e63e52e",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.hadoop.yarn.service.monitor;\n \n import org.apache.commons.io.FileUtils;\n+import org.apache.curator.test.TestingCluster;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.service.MockServiceAM;\n@@ -37,10 +38,14 @@\n import java.io.IOException;\n import java.util.Collections;\n \n+import static org.apache.hadoop.registry.client.api.RegistryConstants\n+    .KEY_REGISTRY_ZK_QUORUM;\n+\n public class TestServiceMonitor extends ServiceTestUtils {\n \n   private File basedir;\n   YarnConfiguration conf = new YarnConfiguration();\n+  TestingCluster zkCluster;\n \n   @Before\n   public void setup() throws Exception {\n@@ -51,13 +56,20 @@ public void setup() throws Exception {\n       basedir.mkdirs();\n     }\n     conf.setLong(YarnServiceConf.READINESS_CHECK_INTERVAL, 2);\n+    zkCluster = new TestingCluster(1);\n+    zkCluster.start();\n+    conf.set(KEY_REGISTRY_ZK_QUORUM, zkCluster.getConnectString());\n+    System.out.println(\"ZK cluster: \" +  zkCluster.getConnectString());\n   }\n \n   @After\n   public void tearDown() throws IOException {\n     if (basedir != null) {\n       FileUtils.deleteDirectory(basedir);\n     }\n+    if (zkCluster != null) {\n+      zkCluster.stop();\n+    }\n   }\n \n   // Create compa with 1 container",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java",
                "sha": "e25d38dd491e85acb07bf163ffd5682581c2ee4f",
                "status": "modified"
            }
        ],
        "message": "YARN-7486. Race condition in service AM that can cause NPE. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/462e25a3b264e1148d0cbca00db7f10d43a0555f",
        "patched_files": [
            "ComponentEvent.java",
            "ComponentInstance.java",
            "AbstractProviderService.java",
            "ProviderUtils.java",
            "MockServiceAM.java",
            "ServiceMonitor.java",
            "ServiceTimelinePublisher.java",
            "ContainerLaunchService.java",
            "ServiceTestUtils.java",
            "ServiceScheduler.java",
            "Component.java",
            "ProviderService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestServiceAM.java",
            "TestServiceMonitor.java",
            "TestServiceTimelinePublisher.java"
        ]
    },
    "hadoop_f4e2b3c": {
        "bug_id": "hadoop_f4e2b3c",
        "commit": "https://github.com/apache/hadoop/commit/f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -414,6 +414,10 @@ Release 2.8.0 - UNRELEASED\n     MAPREDUCE-6360. TestMapreduceConfigFields is placed in wrong dir, \n     introducing compile error (Arshad Mohammad via vinayakumarb)\n \n+    MAPREDUCE-6361. NPE issue in shuffle caused by concurrent issue between\n+    copySucceeded() in one thread and copyFailed() in another thread on the\n+    same host. (Junping Du via ozawa)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "15cdf90a32490caa4acda46f8cc16432916500aa",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleSchedulerImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleSchedulerImpl.java?ref=f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleSchedulerImpl.java",
                "patch": "@@ -239,7 +239,7 @@ private synchronized void updateStatus(String individualProgress) {\n   }\n   \n   private void updateStatus() {\n-    updateStatus(null);\t\n+    updateStatus(null);\n   }\n \n   public synchronized void hostFailed(String hostname) {\n@@ -263,9 +263,17 @@ public synchronized void copyFailed(TaskAttemptID mapId, MapHost host,\n       failureCounts.put(mapId, new IntWritable(1));\n     }\n     String hostname = host.getHostName();\n+    IntWritable hostFailedNum = hostFailures.get(hostname);\n+    // MAPREDUCE-6361: hostname could get cleanup from hostFailures in another\n+    // thread with copySucceeded.\n+    // In this case, add back hostname to hostFailures to get rid of NPE issue.\n+    if (hostFailedNum == null) {\n+      hostFailures.put(hostname, new IntWritable(1));\n+    }\n     //report failure if already retried maxHostFailures times\n-    boolean hostFail = hostFailures.get(hostname).get() > getMaxHostFailures() ? true : false;\n-    \n+    boolean hostFail = hostFailures.get(hostname).get() >\n+        getMaxHostFailures() ? true : false;\n+\n     if (failures >= abortFailureLimit) {\n       try {\n         throw new IOException(failures + \" failures downloading \" + mapId);",
                "raw_url": "https://github.com/apache/hadoop/raw/f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/ShuffleSchedulerImpl.java",
                "sha": "ff0bb4fab2423b192365f6b62ac2a3a997d2862c",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/hadoop/blob/f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestShuffleScheduler.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestShuffleScheduler.java?ref=f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestShuffleScheduler.java",
                "patch": "@@ -213,6 +213,76 @@ public void addFetchFailedMap(TaskAttemptID mapTaskId) {\n     Assert.assertEquals(copyMessage(10, 1, 2), progress.toString());\n   }\n \n+  @SuppressWarnings(\"rawtypes\")\n+  @Test\n+  public <K, V> void TestSucceedAndFailedCopyMap() throws Exception {\n+    JobConf job = new JobConf();\n+    job.setNumMapTasks(2);\n+    //mock creation\n+    TaskUmbilicalProtocol mockUmbilical = mock(TaskUmbilicalProtocol.class);\n+    Reporter mockReporter = mock(Reporter.class);\n+    FileSystem mockFileSystem = mock(FileSystem.class);\n+    Class<? extends org.apache.hadoop.mapred.Reducer>  combinerClass = job.getCombinerClass();\n+    @SuppressWarnings(\"unchecked\")  // needed for mock with generic\n+    CombineOutputCollector<K, V>  mockCombineOutputCollector =\n+        (CombineOutputCollector<K, V>) mock(CombineOutputCollector.class);\n+    org.apache.hadoop.mapreduce.TaskAttemptID mockTaskAttemptID =\n+        mock(org.apache.hadoop.mapreduce.TaskAttemptID.class);\n+    LocalDirAllocator mockLocalDirAllocator = mock(LocalDirAllocator.class);\n+    CompressionCodec mockCompressionCodec = mock(CompressionCodec.class);\n+    Counter mockCounter = mock(Counter.class);\n+    TaskStatus mockTaskStatus = mock(TaskStatus.class);\n+    Progress mockProgress = mock(Progress.class);\n+    MapOutputFile mockMapOutputFile = mock(MapOutputFile.class);\n+    Task mockTask = mock(Task.class);\n+    @SuppressWarnings(\"unchecked\")\n+    MapOutput<K, V> output = mock(MapOutput.class);\n+\n+    ShuffleConsumerPlugin.Context<K, V> context =\n+        new ShuffleConsumerPlugin.Context<K, V>(\n+            mockTaskAttemptID, job, mockFileSystem,\n+            mockUmbilical, mockLocalDirAllocator,\n+            mockReporter, mockCompressionCodec,\n+            combinerClass, mockCombineOutputCollector,\n+            mockCounter, mockCounter, mockCounter,\n+            mockCounter, mockCounter, mockCounter,\n+            mockTaskStatus, mockProgress, mockProgress,\n+            mockTask, mockMapOutputFile, null);\n+    TaskStatus status = new TaskStatus() {\n+      @Override\n+      public boolean getIsMap() {\n+        return false;\n+      }\n+      @Override\n+      public void addFetchFailedMap(TaskAttemptID mapTaskId) {\n+      }\n+    };\n+    Progress progress = new Progress();\n+    ShuffleSchedulerImpl<K, V> scheduler = new ShuffleSchedulerImpl<K, V>(job,\n+        status, null, null, progress, context.getShuffledMapsCounter(),\n+        context.getReduceShuffleBytes(), context.getFailedShuffleCounter());\n+\n+    MapHost host1 = new MapHost(\"host1\", null);\n+    TaskAttemptID failedAttemptID = new TaskAttemptID(\n+        new org.apache.hadoop.mapred.TaskID(\n+        new JobID(\"test\",0), TaskType.MAP, 0), 0);\n+\n+    TaskAttemptID succeedAttemptID = new TaskAttemptID(\n+        new org.apache.hadoop.mapred.TaskID(\n+        new JobID(\"test\",0), TaskType.MAP, 1), 1);\n+\n+    // handle output fetch failure for failedAttemptID, part I\n+    scheduler.hostFailed(host1.getHostName());\n+\n+    // handle output fetch succeed for succeedAttemptID\n+    long bytes = (long)500 * 1024 * 1024;\n+    scheduler.copySucceeded(succeedAttemptID, host1, bytes, 0, 500000, output);\n+\n+    // handle output fetch failure for failedAttemptID, part II\n+    // for MAPREDUCE-6361: verify no NPE exception get thrown out\n+    scheduler.copyFailed(failedAttemptID, host1, true, false);\n+  }\n+\n   private static String copyMessage(int attemptNo, double rate1, double rate2) {\n     int attemptZero = attemptNo - 1;\n     return String.format(\"copy task(attempt_test_0000_m_%06d_%d succeeded at %1.2f MB/s)\"",
                "raw_url": "https://github.com/apache/hadoop/raw/f4e2b3cc0b1f4e49c306bc09a9dddd0495225bb2/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestShuffleScheduler.java",
                "sha": "654b7488b98c82cf8326000ce37d2742011490f7",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6361. NPE issue in shuffle caused by concurrent issue between copySucceeded() in one thread and copyFailed() in another thread on the same host. Contributed by Junping Du.",
        "parent": "https://github.com/apache/hadoop/commit/6d5da9484185ca9f585195d6da069b9cd5be4044",
        "patched_files": [
            "ShuffleSchedulerImpl.java",
            "ShuffleScheduler.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestShuffleScheduler.java"
        ]
    },
    "hadoop_f5756a2": {
        "bug_id": "hadoop_f5756a2",
        "commit": "https://github.com/apache/hadoop/commit/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -543,6 +543,8 @@ Trunk (Unreleased)\n     HADOOP-12638. UnsatisfiedLinkError while checking ISA-L in checknative\n     command. (Kai Sasaki via Colin P. McCabe)\n \n+    HADOOP-12615. Fix NPE in MiniKMS.start(). (Wei-Chiu Chuang via zhz)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "3c98eadad054bd20bdfcbd7d9636e9e9f4c98113",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "patch": "@@ -22,6 +22,9 @@\n \n import org.apache.hadoop.classification.InterfaceStability;\n \n+import java.io.IOException;\n+import java.io.InputStream;\n+\n @InterfaceStability.Evolving\n public class ThreadUtil {\n   \n@@ -46,4 +49,31 @@ public static void sleepAtLeastIgnoreInterrupts(long millis) {\n       }\n     }\n   }\n+\n+  /**\n+   * Convenience method that returns a resource as inputstream from the\n+   * classpath.\n+   * <p>\n+   * It first attempts to use the Thread's context classloader and if not\n+   * set it uses the class' classloader.\n+   *\n+   * @param resourceName resource to retrieve.\n+   *\n+   * @throws IOException thrown if resource cannot be loaded\n+   * @return inputstream with the resource.\n+   */\n+  public static InputStream getResourceAsStream(String resourceName)\n+      throws IOException {\n+    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n+    if (cl == null) {\n+      throw new IOException(\"Can not read resource file '\" + resourceName +\n+          \"' because class loader of the current thread is null\");\n+    }\n+    InputStream is = cl.getResourceAsStream(resourceName);\n+    if (is == null) {\n+      throw new IOException(\"Can not read resource file '\" +\n+          resourceName + \"'\");\n+    }\n+    return is;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "sha": "ab7b5fdeddbd56e94998c0effb1ab86a085ea70e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 5,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "patch": "@@ -43,11 +43,7 @@ protected VersionInfo(String component) {\n     String versionInfoFile = component + \"-version-info.properties\";\n     InputStream is = null;\n     try {\n-      is = Thread.currentThread().getContextClassLoader()\n-        .getResourceAsStream(versionInfoFile);\n-      if (is == null) {\n-        throw new IOException(\"Resource not found\");\n-      }\n+      is = ThreadUtil.getResourceAsStream(versionInfoFile);\n       info.load(is);\n     } catch (IOException ex) {\n       LogFactory.getLog(getClass()).warn(\"Could not read '\" +",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "sha": "dc8d36959a0867cc2cbcab46688d749c3466fdb1",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 10,
                "filename": "hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.crypto.key.kms.KMSRESTConstants;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.security.ssl.SslSocketConnectorSecure;\n+import org.apache.hadoop.util.ThreadUtil;\n import org.mortbay.jetty.Connector;\n import org.mortbay.jetty.Server;\n import org.mortbay.jetty.security.SslSocketConnector;\n@@ -34,6 +35,7 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.io.Writer;\n+import java.io.IOException;\n import java.net.InetAddress;\n import java.net.MalformedURLException;\n import java.net.ServerSocket;\n@@ -149,16 +151,26 @@ public MiniKMS(String kmsConfDir, String log4ConfFile, String keyStore,\n     this.inPort = inPort;\n   }\n \n+  private void copyResource(String inputResourceName, File outputFile) throws\n+      IOException {\n+    InputStream is = null;\n+    OutputStream os = null;\n+    try {\n+      is = ThreadUtil.getResourceAsStream(inputResourceName);\n+      os = new FileOutputStream(outputFile);\n+      IOUtils.copy(is, os);\n+    } finally {\n+      IOUtils.closeQuietly(is);\n+      IOUtils.closeQuietly(os);\n+    }\n+  }\n+\n   public void start() throws Exception {\n     ClassLoader cl = Thread.currentThread().getContextClassLoader();\n     System.setProperty(KMSConfiguration.KMS_CONFIG_DIR, kmsConfDir);\n     File aclsFile = new File(kmsConfDir, \"kms-acls.xml\");\n     if (!aclsFile.exists()) {\n-      InputStream is = cl.getResourceAsStream(\"mini-kms-acls-default.xml\");\n-      OutputStream os = new FileOutputStream(aclsFile);\n-      IOUtils.copy(is, os);\n-      is.close();\n-      os.close();\n+      copyResource(\"mini-kms-acls-default.xml\", aclsFile);\n     }\n     File coreFile = new File(kmsConfDir, \"core-site.xml\");\n     if (!coreFile.exists()) {\n@@ -195,11 +207,7 @@ public void start() throws Exception {\n           \"/kms-webapp/WEB-INF\");\n       webInf.mkdirs();\n       new File(webInf, \"web.xml\").delete();\n-      InputStream is = cl.getResourceAsStream(\"kms-webapp/WEB-INF/web.xml\");\n-      OutputStream os = new FileOutputStream(new File(webInf, \"web.xml\"));\n-      IOUtils.copy(is, os);\n-      is.close();\n-      os.close();\n+      copyResource(\"kms-webapp/WEB-INF/web.xml\", new File(webInf, \"web.xml\"));\n       webappPath = webInf.getParentFile().getAbsolutePath();\n     } else {\n       webappPath = cl.getResource(\"kms-webapp\").getPath();",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "sha": "f520edfa51d390cae77068a50d0e7d2be4746af5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "patch": "@@ -19,11 +19,15 @@\n \n import java.io.ByteArrayOutputStream;\n import java.io.FilterOutputStream;\n+import java.io.InputStream;\n+import java.io.IOException;\n import java.io.OutputStream;\n import java.io.PrintStream;\n \n import org.apache.hadoop.crypto.key.kms.server.KMS.KMSOp;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.ThreadUtil;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.PropertyConfigurator;\n import org.junit.After;\n@@ -52,15 +56,17 @@ public void setOutputStream(OutputStream out) {\n   }\n \n   @Before\n-  public void setUp() {\n+  public void setUp() throws IOException {\n     originalOut = System.err;\n     memOut = new ByteArrayOutputStream();\n     filterOut = new FilterOut(memOut);\n     capturedOut = new PrintStream(filterOut);\n     System.setErr(capturedOut);\n-    PropertyConfigurator.configure(Thread.currentThread().\n-        getContextClassLoader()\n-        .getResourceAsStream(\"log4j-kmsaudit.properties\"));\n+    InputStream is =\n+        ThreadUtil.getResourceAsStream(\"log4j-kmsaudit.properties\");\n+    PropertyConfigurator.configure(is);\n+    IOUtils.closeStream(is);\n+\n     this.kmsAudit = new KMSAudit(1000);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "sha": "7e1c0d7b08ec209fded079985ca929c95f797ed7",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "patch": "@@ -61,6 +61,7 @@\n import java.io.FileInputStream;\n import java.io.InputStream;\n import java.io.InputStreamReader;\n+import java.io.IOException;\n import java.io.StringReader;\n import java.lang.reflect.Method;\n import java.net.InetAddress;\n@@ -389,6 +390,32 @@ private void initDirectoryService() throws Exception {\n     ds.getAdminSession().add(entry);\n   }\n \n+  /**\n+   * Convenience method that returns a resource as inputstream from the\n+   * classpath.\n+   * <p>\n+   * It first attempts to use the Thread's context classloader and if not\n+   * set it uses the class' classloader.\n+   *\n+   * @param resourceName resource to retrieve.\n+   *\n+   * @throws IOException thrown if resource cannot be loaded\n+   * @return inputstream with the resource.\n+   */\n+  public static InputStream getResourceAsStream(String resourceName)\n+      throws IOException {\n+    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n+    if (cl == null) {\n+      cl = MiniKdc.class.getClassLoader();\n+    }\n+    InputStream is = cl.getResourceAsStream(resourceName);\n+    if (is == null) {\n+      throw new IOException(\"Can not read resource file '\" +\n+          resourceName + \"'\");\n+    }\n+    return is;\n+  }\n+\n   private void initKDCServer() throws Exception {\n     String orgName= conf.getProperty(ORG_NAME);\n     String orgDomain = conf.getProperty(ORG_DOMAIN);\n@@ -400,8 +427,7 @@ private void initKDCServer() throws Exception {\n     map.put(\"3\", orgDomain.toUpperCase(Locale.ENGLISH));\n     map.put(\"4\", bindAddress);\n \n-    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n-    InputStream is1 = cl.getResourceAsStream(\"minikdc.ldiff\");\n+    InputStream is1 = getResourceAsStream(\"minikdc.ldiff\");\n \n     SchemaManager schemaManager = ds.getSchemaManager();\n     LdifReader reader = null;\n@@ -443,7 +469,7 @@ private void initKDCServer() throws Exception {\n     kdc.start();\n \n     StringBuilder sb = new StringBuilder();\n-    InputStream is2 = cl.getResourceAsStream(\"minikdc-krb5.conf\");\n+    InputStream is2 = getResourceAsStream(\"minikdc-krb5.conf\");\n \n     BufferedReader r = null;\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "sha": "a5253c4521715cc911a29003685f196bce456ad6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12615. Fix NPE in MiniKMS.start(). Contributed by Wei-Chiu Chuang.\n\nChange-Id: Ie3e148bd1401618b1737a577957298bf622891f4",
        "parent": "https://github.com/apache/hadoop/commit/5104077e1f431ad3675d0b1c5c3cf53936902d8e",
        "patched_files": [
            "VersionInfo.java",
            "MiniKMS.java",
            "CHANGES.java",
            "KMSAudit.java",
            "MiniKdc.java",
            "ThreadUtil.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestMiniKdc.java",
            "TestKMSAudit.java"
        ]
    },
    "hadoop_f672188": {
        "bug_id": "hadoop_f672188",
        "commit": "https://github.com/apache/hadoop/commit/f67218809c50b194e463af6e6196db298353c8c1",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -539,6 +539,9 @@ Release 2.4.0 - UNRELEASED\n     YARN-1670. Fixed a bug in log-aggregation that can cause the writer to write\n     more log-data than the log-length that it records. (Mit Desai via vinodk)\n \n+    YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM\n+    (Karthik Kambatla via jianhe )\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/CHANGES.txt",
                "sha": "cfb0052a201002073a5a8de6ed7ca0adea4dea93",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 24,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.service.AbstractService;\n import org.apache.hadoop.util.VersionUtil;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n@@ -187,12 +188,51 @@ protected void serviceStop() throws Exception {\n     super.serviceStop();\n   }\n \n+  /**\n+   * Helper method to handle received ContainerStatus. If this corresponds to\n+   * the completion of a master-container of a managed AM,\n+   * we call the handler for RMAppAttemptContainerFinishedEvent.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  @VisibleForTesting\n+  void handleContainerStatus(ContainerStatus containerStatus) {\n+    ApplicationAttemptId appAttemptId =\n+        containerStatus.getContainerId().getApplicationAttemptId();\n+    RMApp rmApp =\n+        rmContext.getRMApps().get(appAttemptId.getApplicationId());\n+    if (rmApp == null) {\n+      LOG.error(\"Received finished container : \"\n+          + containerStatus.getContainerId()\n+          + \"for unknown application \" + appAttemptId.getApplicationId()\n+          + \" Skipping.\");\n+      return;\n+    }\n+\n+    if (rmApp.getApplicationSubmissionContext().getUnmanagedAM()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Ignoring container completion status for unmanaged AM\"\n+            + rmApp.getApplicationId());\n+      }\n+      return;\n+    }\n+\n+    RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n+    Container masterContainer = rmAppAttempt.getMasterContainer();\n+    if (masterContainer.getId().equals(containerStatus.getContainerId())\n+        && containerStatus.getState() == ContainerState.COMPLETE) {\n+      // sending master container finished event.\n+      RMAppAttemptContainerFinishedEvent evt =\n+          new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+              containerStatus);\n+      rmContext.getDispatcher().getEventHandler().handle(evt);\n+    }\n+  }\n+\n   @SuppressWarnings(\"unchecked\")\n   @Override\n   public RegisterNodeManagerResponse registerNodeManager(\n       RegisterNodeManagerRequest request) throws YarnException,\n       IOException {\n-\n     NodeId nodeId = request.getNodeId();\n     String host = nodeId.getHost();\n     int cmPort = nodeId.getPort();\n@@ -204,29 +244,7 @@ public RegisterNodeManagerResponse registerNodeManager(\n       LOG.info(\"received container statuses on node manager register :\"\n           + request.getContainerStatuses());\n       for (ContainerStatus containerStatus : request.getContainerStatuses()) {\n-        ApplicationAttemptId appAttemptId =\n-            containerStatus.getContainerId().getApplicationAttemptId();\n-        RMApp rmApp =\n-            rmContext.getRMApps().get(appAttemptId.getApplicationId());\n-        if (rmApp != null) {\n-          RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt != null) {\n-            if (rmAppAttempt.getMasterContainer().getId()\n-                .equals(containerStatus.getContainerId())\n-                && containerStatus.getState() == ContainerState.COMPLETE) {\n-              // sending master container finished event.\n-              RMAppAttemptContainerFinishedEvent evt =\n-                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                      containerStatus);\n-              rmContext.getDispatcher().getEventHandler().handle(evt);\n-            }\n-          }\n-        } else {\n-          LOG.error(\"Received finished container :\"\n-              + containerStatus.getContainerId()\n-              + \" for non existing application :\"\n-              + appAttemptId.getApplicationId());\n-        }\n+        handleContainerStatus(containerStatus);\n       }\n     }\n     RegisterNodeManagerResponse response = recordFactory",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "1d4032048e468a9652093acc18cf0c0b82007a70",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -35,9 +35,11 @@\n \n import javax.crypto.SecretKey;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -629,7 +631,9 @@ public Container getMasterContainer() {\n     }\n   }\n \n-  private void setMasterContainer(Container container) {\n+  @InterfaceAudience.Private\n+  @VisibleForTesting\n+  public void setMasterContainer(Container container) {\n     masterContainer = container;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "3e90ec8ec1d5aaec6619f357ceaadecbb7bca194",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -26,8 +26,6 @@\n import java.util.HashMap;\n import java.util.List;\n \n-import org.junit.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.metrics2.MetricsSystem;\n@@ -45,21 +43,29 @@\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.event.Event;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n import org.apache.hadoop.yarn.util.YarnVersionInfo;\n+\n import org.junit.After;\n+import org.junit.Assert;\n import org.junit.Test;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.verify;\n \n public class TestResourceTrackerService {\n \n@@ -468,26 +474,64 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   @Test\n-  public void testNodeRegistrationWithContainers() throws Exception {\n-    rm = new MockRM();\n-    rm.init(new YarnConfiguration());\n+  public void testHandleContainerStatusInvalidCompletions() throws Exception {\n+    rm = new MockRM(new YarnConfiguration());\n     rm.start();\n-    RMApp app = rm.submitApp(1024);\n \n-    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n-    nm.nodeHeartbeat(true);\n+    EventHandler handler =\n+        spy(rm.getRMContext().getDispatcher().getEventHandler());\n \n-    // Register node with some container statuses\n+    // Case 1: Unmanaged AM\n+    RMApp app = rm.submitApp(1024, true);\n+\n+    // Case 1.1: AppAttemptId is null\n     ContainerStatus status = ContainerStatus.newInstance(\n         ContainerId.newInstance(ApplicationAttemptId.newInstance(\n             app.getApplicationId(), 2), 1),\n         ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event) any());\n+\n+    // Case 1.2: Master container is null\n+    RMAppAttemptImpl currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event)any());\n \n-    // The following shouldn't throw NPE\n-    nm.registerNode(Collections.singletonList(status));\n-    assertEquals(\"Incorrect number of nodes\", 1,\n-        rm.getRMContext().getRMNodes().size());\n+    // Case 2: Managed AM\n+    app = rm.submitApp(1024);\n+\n+    // Case 2.1: AppAttemptId is null\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n+\n+    // Case 2.2: Master container is null\n+    currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "2f16b85699d3705be3743e4d54b069b42fab2e3d",
                "status": "modified"
            }
        ],
        "message": "YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM. Contributed by Karthik Kambatla\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1580077 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/a5c08eed16e797d2ba9f98f7bc6a8e1bf09aaddd",
        "patched_files": [
            "ResourceTrackerService.java",
            "CHANGES.java",
            "RMAppAttemptImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_f74e446": {
        "bug_id": "hadoop_f74e446",
        "commit": "https://github.com/apache/hadoop/commit/f74e44635596276f35b7127f99bc5ab96ab534ed",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=f74e44635596276f35b7127f99bc5ab96ab534ed",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -139,6 +139,9 @@ Trunk (Unreleased)\n \n     MAPREDUCE-5717. Task pings are interpreted as task progress (jlowe)\n \n+    MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to \n+    ProportionalCapacityPreemptionPolicy (Sunil G via devaraj)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c2a4f14514d0bd184f92e05ba595fadc2966b1b3",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java?ref=f74e44635596276f35b7127f99bc5ab96ab534ed",
                "deletions": 6,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "patch": "@@ -29,7 +29,9 @@\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.PreemptionContainer;\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\n import org.apache.hadoop.yarn.api.records.PreemptionMessage;\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\n import org.apache.hadoop.yarn.event.EventHandler;\n \n /**\n@@ -52,13 +54,18 @@ public void init(AppContext context) {\n   public void preempt(Context ctxt, PreemptionMessage preemptionRequests) {\n     // for both strict and negotiable preemption requests kill the\n     // container\n-    for (PreemptionContainer c :\n-        preemptionRequests.getStrictContract().getContainers()) {\n-      killContainer(ctxt, c);\n+    StrictPreemptionContract strictContract = preemptionRequests\n+        .getStrictContract();\n+    if (strictContract != null) {\n+      for (PreemptionContainer c : strictContract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n-    for (PreemptionContainer c :\n-         preemptionRequests.getContract().getContainers()) {\n-       killContainer(ctxt, c);\n+    PreemptionContract contract = preemptionRequests.getContract();\n+    if (contract != null) {\n+      for (PreemptionContainer c : contract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "sha": "09237aaa297f82b2a60ecb2f5ba9f0d620130094",
                "status": "modified"
            },
            {
                "additions": 144,
                "blob_url": "https://github.com/apache/hadoop/blob/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java?ref=f74e44635596276f35b7127f99bc5ab96ab534ed",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "patch": "@@ -0,0 +1,144 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+package org.apache.hadoop.mapreduce.v2.app;\r\n+\r\n+import static org.mockito.Matchers.any;\r\n+import static org.mockito.Mockito.mock;\r\n+import static org.mockito.Mockito.times;\r\n+import static org.mockito.Mockito.verify;\r\n+import static org.mockito.Mockito.when;\r\n+\r\n+import java.util.ArrayList;\r\n+import java.util.HashSet;\r\n+import java.util.List;\r\n+import java.util.Set;\r\n+\r\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskType;\r\n+import org.apache.hadoop.mapreduce.v2.app.MRAppMaster.RunningAppContext;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.JobCounterUpdateEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\r\n+import org.apache.hadoop.yarn.api.records.Container;\r\n+import org.apache.hadoop.yarn.api.records.ContainerId;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContainer;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionMessage;\r\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\r\n+import org.apache.hadoop.yarn.event.EventHandler;\r\n+import org.apache.hadoop.yarn.factories.RecordFactory;\r\n+import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\r\n+import org.junit.Test;\r\n+\r\n+public class TestKillAMPreemptionPolicy {\r\n+  private final RecordFactory recordFactory = RecordFactoryProvider\r\n+      .getRecordFactory(null);\r\n+\r\n+  @SuppressWarnings(\"unchecked\")\r\n+  @Test\r\n+  public void testKillAMPreemptPolicy() {\r\n+\r\n+    ApplicationId appId = ApplicationId.newInstance(123456789, 1);\r\n+    ContainerId container = ContainerId.newInstance(\r\n+        ApplicationAttemptId.newInstance(appId, 1), 1);\r\n+    AMPreemptionPolicy.Context mPctxt = mock(AMPreemptionPolicy.Context.class);\r\n+    when(mPctxt.getTaskAttempt(any(ContainerId.class))).thenReturn(\r\n+        MRBuilderUtils.newTaskAttemptId(MRBuilderUtils.newTaskId(\r\n+            MRBuilderUtils.newJobId(appId, 1), 1, TaskType.MAP), 0));\r\n+    List<Container> p = new ArrayList<Container>();\r\n+    p.add(Container.newInstance(container, null, null, null, null, null));\r\n+    when(mPctxt.getContainers(any(TaskType.class))).thenReturn(p);\r\n+\r\n+    KillAMPreemptionPolicy policy = new KillAMPreemptionPolicy();\r\n+\r\n+    // strictContract is null & contract is null\r\n+    RunningAppContext mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    PreemptionMessage pM = getPreemptionMessage(false, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(false, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+  }\r\n+\r\n+  private RunningAppContext getRunningAppContext() {\r\n+    RunningAppContext mActxt = mock(RunningAppContext.class);\r\n+    EventHandler<?> eventHandler = mock(EventHandler.class);\r\n+    when(mActxt.getEventHandler()).thenReturn(eventHandler);\r\n+    return mActxt;\r\n+  }\r\n+\r\n+  private PreemptionMessage getPreemptionMessage(boolean strictContract,\r\n+      boolean contract, final ContainerId container) {\r\n+    PreemptionMessage preemptionMessage = recordFactory\r\n+        .newRecordInstance(PreemptionMessage.class);\r\n+    Set<PreemptionContainer> cntrs = new HashSet<PreemptionContainer>();\r\n+    PreemptionContainer preemptContainer = recordFactory\r\n+        .newRecordInstance(PreemptionContainer.class);\r\n+    preemptContainer.setId(container);\r\n+    cntrs.add(preemptContainer);\r\n+    if (strictContract) {\r\n+      StrictPreemptionContract set = recordFactory\r\n+          .newRecordInstance(StrictPreemptionContract.class);\r\n+      set.setContainers(cntrs);\r\n+      preemptionMessage.setStrictContract(set);\r\n+    }\r\n+    if (contract) {\r\n+      PreemptionContract preemptContract = recordFactory\r\n+          .newRecordInstance(PreemptionContract.class);\r\n+      preemptContract.setContainers(cntrs);\r\n+      preemptionMessage.setContract(preemptContract);\r\n+    }\r\n+    return preemptionMessage;\r\n+  }\r\n+\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop/raw/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "sha": "fa930ae1262be2ed4687cc09456a5d80496c7b0d",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to ProportionalCapacityPreemptionPolicy. Contributed by Sunil G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595754 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/9a0ed1c4afd95827c6ff27490f33d0b86851e551",
        "patched_files": [
            "CHANGES.java",
            "KillAMPreemptionPolicy.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestKillAMPreemptionPolicy.java"
        ]
    },
    "hadoop_f76f5c0": {
        "bug_id": "hadoop_f76f5c0",
        "commit": "https://github.com/apache/hadoop/commit/f76f5c0919cdb0b032edb309d137093952e77268",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -275,6 +275,10 @@ protected void addSchedPriorityCommand(List<String> command) {\n     }\n   }\n \n+  protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+    return PrivilegedOperationExecutor.getInstance(getConf());\n+  }\n+\n   @Override\n   public void init() throws IOException {\n     Configuration conf = super.getConf();\n@@ -285,7 +289,7 @@ public void init() throws IOException {\n       PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.CHECK_SETUP);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n           false);\n@@ -382,7 +386,7 @@ public void startLocalizer(LocalizerStartContext ctx)\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n           initializeContainerOp, null, null, false, true);\n@@ -530,8 +534,9 @@ public int launchContainer(ContainerStartContext ctx)\n         }\n         builder.append(\"Stack trace: \"\n             + StringUtils.stringifyException(e) + \"\\n\");\n-        if (!e.getOutput().isEmpty()) {\n-          builder.append(\"Shell output: \" + e.getOutput() + \"\\n\");\n+        String output = e.getOutput();\n+        if (output!= null && !e.getOutput().isEmpty()) {\n+          builder.append(\"Shell output: \" + output + \"\\n\");\n         }\n         String diagnostics = builder.toString();\n         logOutput(diagnostics);\n@@ -729,7 +734,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp,\n           false);\n@@ -759,7 +764,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n \n     try {\n       PrivilegedOperationExecutor privOpExecutor =\n-          PrivilegedOperationExecutor.getInstance(super.getConf());\n+          getPrivilegedOperationExecutor();\n \n       String results =\n           privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n@@ -818,7 +823,7 @@ public void mountCgroups(List<String> cgroupKVs, String hierarchy)\n \n       mountCGroupsOp.appendArgs(cgroupKVs);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(mountCGroupsOp,\n           false);",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "47b99c25bb6d9571fe3d027101791344468c05c8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "patch": "@@ -24,7 +24,7 @@\n \n public class PrivilegedOperationException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private Integer exitCode;\n+  private int exitCode = -1;\n   private String output;\n   private String errorOutput;\n \n@@ -36,7 +36,7 @@ public PrivilegedOperationException(String message) {\n     super(message);\n   }\n \n-  public PrivilegedOperationException(String message, Integer exitCode,\n+  public PrivilegedOperationException(String message, int exitCode,\n       String output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n@@ -48,8 +48,8 @@ public PrivilegedOperationException(Throwable cause) {\n     super(cause);\n   }\n \n-  public PrivilegedOperationException(Throwable cause, Integer exitCode, String\n-      output, String errorOutput) {\n+  public PrivilegedOperationException(Throwable cause, int exitCode,\n+      String output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n@@ -59,7 +59,7 @@ public PrivilegedOperationException(String message, Throwable cause) {\n     super(message, cause);\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "sha": "9a11194f143e0e832d9371bddfd1229aeaeb0bd4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "patch": "@@ -32,10 +32,10 @@\n @InterfaceStability.Unstable\n public class ContainerExecutionException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private static final Integer EXIT_CODE_UNSET = -1;\n+  private static final int EXIT_CODE_UNSET = -1;\n   private static final String OUTPUT_UNSET = \"<unknown>\";\n \n-  private Integer exitCode;\n+  private int exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -54,23 +54,23 @@ public ContainerExecutionException(Throwable throwable) {\n   }\n \n \n-  public ContainerExecutionException(String message, Integer exitCode, String\n+  public ContainerExecutionException(String message, int exitCode, String\n       output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public ContainerExecutionException(Throwable cause, Integer exitCode, String\n+  public ContainerExecutionException(Throwable cause, int exitCode, String\n       output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "sha": "3147277704265b1cb5bc2aa2f21d8a417e31c381",
                "status": "modified"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -23,7 +23,9 @@\n import static org.junit.Assert.assertNotEquals;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n@@ -40,20 +42,24 @@\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.ConfigurationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerDiagnosticsUpdateEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime;\n@@ -516,4 +522,87 @@ public void testDeleteAsUser() throws IOException {\n         appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n         readMockParams());\n   }\n+\n+  @Test\n+  public void testNoExitCodeFromPrivilegedOperation() throws Exception {\n+    Configuration conf = new Configuration();\n+    final PrivilegedOperationExecutor spyPrivilegedExecutor =\n+        spy(PrivilegedOperationExecutor.getInstance(conf));\n+    doThrow(new PrivilegedOperationException(\"interrupted\"))\n+        .when(spyPrivilegedExecutor).executePrivilegedOperation(\n+            any(List.class), any(PrivilegedOperation.class),\n+            any(File.class), any(Map.class), anyBoolean(), anyBoolean());\n+    LinuxContainerRuntime runtime = new DefaultLinuxContainerRuntime(\n+        spyPrivilegedExecutor);\n+    runtime.initialize(conf);\n+    mockExec = new LinuxContainerExecutor(runtime);\n+    mockExec.setConf(conf);\n+    LinuxContainerExecutor lce = new LinuxContainerExecutor(runtime) {\n+      @Override\n+      protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+        return spyPrivilegedExecutor;\n+      }\n+    };\n+    lce.setConf(conf);\n+    InetSocketAddress address = InetSocketAddress.createUnresolved(\n+        \"localhost\", 8040);\n+    Path nmPrivateCTokensPath= new Path(\"file:///bin/nmPrivateCTokensPath\");\n+    LocalDirsHandlerService dirService = new LocalDirsHandlerService();\n+    dirService.init(conf);\n+\n+    String appSubmitter = \"nobody\";\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\n+    ContainerId cid = ContainerId.newContainerId(attemptId, 1);\n+    HashMap<String, String> env = new HashMap<>();\n+    Container container = mock(Container.class);\n+    ContainerLaunchContext context = mock(ContainerLaunchContext.class);\n+    when(container.getContainerId()).thenReturn(cid);\n+    when(container.getLaunchContext()).thenReturn(context);\n+    when(context.getEnvironment()).thenReturn(env);\n+    Path workDir = new Path(\"/tmp\");\n+\n+    try {\n+      lce.startLocalizer(new LocalizerStartContext.Builder()\n+          .setNmPrivateContainerTokens(nmPrivateCTokensPath)\n+          .setNmAddr(address)\n+          .setUser(appSubmitter)\n+          .setAppId(appId.toString())\n+          .setLocId(\"12345\")\n+          .setDirsHandler(dirService)\n+          .build());\n+      Assert.fail(\"startLocalizer should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exitCode\"));\n+    }\n+\n+    lce.activateContainer(cid, new Path(workDir, \"pid.txt\"));\n+    lce.launchContainer(new ContainerStartContext.Builder()\n+        .setContainer(container)\n+        .setNmPrivateContainerScriptPath(new Path(\"file:///bin/echo\"))\n+        .setNmPrivateTokensPath(new Path(\"file:///dev/null\"))\n+        .setUser(appSubmitter)\n+        .setAppId(appId.toString())\n+        .setContainerWorkDir(workDir)\n+        .setLocalDirs(dirsHandler.getLocalDirs())\n+        .setLogDirs(dirsHandler.getLogDirs())\n+        .setFilecacheDirs(new ArrayList<>())\n+        .setUserLocalDirs(new ArrayList<>())\n+        .setContainerLocalDirs(new ArrayList<>())\n+        .setContainerLogDirs(new ArrayList<>())\n+        .build());\n+    lce.deleteAsUser(new DeletionAsUserContext.Builder()\n+        .setUser(appSubmitter)\n+        .setSubDir(new Path(\"/tmp/testdir\"))\n+        .build());\n+\n+    try {\n+      lce.mountCgroups(new ArrayList<String>(), \"hierarchy\");\n+      Assert.fail(\"mountCgroups should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exit code\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "cfd0e364a2d4ad5735590d69d90cdce9f81b0012",
                "status": "modified"
            }
        ],
        "message": "YARN-6805. NPE in LinuxContainerExecutor due to null PrivilegedOperationException exit code. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/5f1ee72b0ebf0330417b7c0115083bc851923be4",
        "patched_files": [
            "LinuxContainerExecutor.java",
            "PrivilegedOperationException.java",
            "ContainerExecutionException.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java",
            "TestLinuxContainerExecutorWithMocks.java"
        ]
    },
    "hadoop_f858f18": {
        "bug_id": "hadoop_f858f18",
        "commit": "https://github.com/apache/hadoop/commit/f858f1855455348f5ac517ebd96c2bbc3bc97489",
        "file": [
            {
                "additions": 114,
                "blob_url": "https://github.com/apache/hadoop/blob/f858f1855455348f5ac517ebd96c2bbc3bc97489/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java",
                "changes": 209,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java?ref=f858f1855455348f5ac517ebd96c2bbc3bc97489",
                "deletions": 95,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java",
                "patch": "@@ -58,6 +58,8 @@\n   class RequestHedgingInvocationHandler implements InvocationHandler {\n \n     final Map<String, ProxyInfo<T>> targetProxies;\n+    // Proxy of the active nn\n+    private volatile ProxyInfo<T> currentUsedProxy = null;\n \n     public RequestHedgingInvocationHandler(\n             Map<String, ProxyInfo<T>> targetProxies) {\n@@ -79,104 +81,118 @@ public RequestHedgingInvocationHandler(\n     public Object\n     invoke(Object proxy, final Method method, final Object[] args)\n             throws Throwable {\n-      if (currentUsedProxy != null) {\n-        try {\n-          Object retVal = method.invoke(currentUsedProxy.proxy, args);\n-          LOG.debug(\"Invocation successful on [{}]\",\n-              currentUsedProxy.proxyInfo);\n-          return retVal;\n-        } catch (InvocationTargetException ex) {\n-          Exception unwrappedException = unwrapInvocationTargetException(ex);\n-          logProxyException(unwrappedException, currentUsedProxy.proxyInfo);\n-          LOG.trace(\"Unsuccessful invocation on [{}]\",\n-              currentUsedProxy.proxyInfo);\n-          throw unwrappedException;\n-        }\n-      }\n-      Map<Future<Object>, ProxyInfo<T>> proxyMap = new HashMap<>();\n-      int numAttempts = 0;\n+      // Need double check locking to guarantee thread-safe since\n+      // currentUsedProxy is lazily initialized.\n+      if (currentUsedProxy == null) {\n+        synchronized (this) {\n+          if (currentUsedProxy == null) {\n+            Map<Future<Object>, ProxyInfo<T>> proxyMap = new HashMap<>();\n+            int numAttempts = 0;\n \n-      ExecutorService executor = null;\n-      CompletionService<Object> completionService;\n-      try {\n-        // Optimization : if only 2 proxies are configured and one had failed\n-        // over, then we dont need to create a threadpool etc.\n-        targetProxies.remove(toIgnore);\n-        if (targetProxies.size() == 0) {\n-          LOG.trace(\"No valid proxies left\");\n-          throw new RemoteException(IOException.class.getName(),\n-              \"No valid proxies left. All NameNode proxies have failed over.\");\n-        }\n-        if (targetProxies.size() == 1) {\n-          ProxyInfo<T> proxyInfo = targetProxies.values().iterator().next();\n-          try {\n-            currentUsedProxy = proxyInfo;\n-            Object retVal = method.invoke(proxyInfo.proxy, args);\n-            LOG.debug(\"Invocation successful on [{}]\",\n-                currentUsedProxy.proxyInfo);\n-            return retVal;\n-          } catch (InvocationTargetException ex) {\n-            Exception unwrappedException = unwrapInvocationTargetException(ex);\n-            logProxyException(unwrappedException, currentUsedProxy.proxyInfo);\n-            LOG.trace(\"Unsuccessful invocation on [{}]\",\n-                currentUsedProxy.proxyInfo);\n-            throw unwrappedException;\n-          }\n-        }\n-        executor = Executors.newFixedThreadPool(proxies.size());\n-        completionService = new ExecutorCompletionService<>(executor);\n-        for (final Map.Entry<String, ProxyInfo<T>> pEntry :\n-                targetProxies.entrySet()) {\n-          Callable<Object> c = new Callable<Object>() {\n-            @Override\n-            public Object call() throws Exception {\n-              LOG.trace(\"Invoking method {} on proxy {}\", method,\n-                  pEntry.getValue().proxyInfo);\n-              return method.invoke(pEntry.getValue().proxy, args);\n-            }\n-          };\n-          proxyMap.put(completionService.submit(c), pEntry.getValue());\n-          numAttempts++;\n-        }\n+            ExecutorService executor = null;\n+            CompletionService<Object> completionService;\n+            try {\n+              // Optimization : if only 2 proxies are configured and one had\n+              // failed\n+              // over, then we dont need to create a threadpool etc.\n+              targetProxies.remove(toIgnore);\n+              if (targetProxies.size() == 0) {\n+                LOG.trace(\"No valid proxies left\");\n+                throw new RemoteException(IOException.class.getName(),\n+                    \"No valid proxies left. \"\n+                        + \"All NameNode proxies have failed over.\");\n+              }\n+              if (targetProxies.size() == 1) {\n+                ProxyInfo<T> proxyInfo =\n+                    targetProxies.values().iterator().next();\n+                try {\n+                  currentUsedProxy = proxyInfo;\n+                  Object retVal = method.invoke(proxyInfo.proxy, args);\n+                  LOG.debug(\"Invocation successful on [{}]\",\n+                      currentUsedProxy.proxyInfo);\n+                  return retVal;\n+                } catch (InvocationTargetException ex) {\n+                  Exception unwrappedException =\n+                      unwrapInvocationTargetException(ex);\n+                  logProxyException(unwrappedException,\n+                      currentUsedProxy.proxyInfo);\n+                  LOG.trace(\"Unsuccessful invocation on [{}]\",\n+                      currentUsedProxy.proxyInfo);\n+                  throw unwrappedException;\n+                }\n+              }\n+              executor = Executors.newFixedThreadPool(proxies.size());\n+              completionService = new ExecutorCompletionService<>(executor);\n+              for (final Map.Entry<String, ProxyInfo<T>> pEntry : targetProxies\n+                  .entrySet()) {\n+                Callable<Object> c = new Callable<Object>() {\n+                  @Override\n+                  public Object call() throws Exception {\n+                    LOG.trace(\"Invoking method {} on proxy {}\", method,\n+                        pEntry.getValue().proxyInfo);\n+                    return method.invoke(pEntry.getValue().proxy, args);\n+                  }\n+                };\n+                proxyMap.put(completionService.submit(c), pEntry.getValue());\n+                numAttempts++;\n+              }\n \n-        Map<String, Exception> badResults = new HashMap<>();\n-        while (numAttempts > 0) {\n-          Future<Object> callResultFuture = completionService.take();\n-          Object retVal;\n-          try {\n-            currentUsedProxy = proxyMap.get(callResultFuture);\n-            retVal = callResultFuture.get();\n-            LOG.debug(\"Invocation successful on [{}]\",\n-                currentUsedProxy.proxyInfo);\n-            return retVal;\n-          } catch (ExecutionException ex) {\n-            Exception unwrappedException = unwrapExecutionException(ex);\n-            ProxyInfo<T> tProxyInfo = proxyMap.get(callResultFuture);\n-            logProxyException(unwrappedException, tProxyInfo.proxyInfo);\n-            badResults.put(tProxyInfo.proxyInfo, unwrappedException);\n-            LOG.trace(\"Unsuccessful invocation on [{}]\", tProxyInfo.proxyInfo);\n-            numAttempts--;\n-          }\n-        }\n+              Map<String, Exception> badResults = new HashMap<>();\n+              while (numAttempts > 0) {\n+                Future<Object> callResultFuture = completionService.take();\n+                Object retVal;\n+                try {\n+                  currentUsedProxy = proxyMap.get(callResultFuture);\n+                  retVal = callResultFuture.get();\n+                  LOG.debug(\"Invocation successful on [{}]\",\n+                      currentUsedProxy.proxyInfo);\n+                  return retVal;\n+                } catch (ExecutionException ex) {\n+                  Exception unwrappedException = unwrapExecutionException(ex);\n+                  ProxyInfo<T> tProxyInfo = proxyMap.get(callResultFuture);\n+                  logProxyException(unwrappedException, tProxyInfo.proxyInfo);\n+                  badResults.put(tProxyInfo.proxyInfo, unwrappedException);\n+                  LOG.trace(\"Unsuccessful invocation on [{}]\",\n+                      tProxyInfo.proxyInfo);\n+                  numAttempts--;\n+                }\n+              }\n \n-        // At this point we should have All bad results (Exceptions)\n-        // Or should have returned with successful result.\n-        if (badResults.size() == 1) {\n-          throw badResults.values().iterator().next();\n-        } else {\n-          throw new MultiException(badResults);\n-        }\n-      } finally {\n-        if (executor != null) {\n-          LOG.trace(\"Shutting down threadpool executor\");\n-          executor.shutdownNow();\n+              // At this point we should have All bad results (Exceptions)\n+              // Or should have returned with successful result.\n+              if (badResults.size() == 1) {\n+                throw badResults.values().iterator().next();\n+              } else {\n+                throw new MultiException(badResults);\n+              }\n+            } finally {\n+              if (executor != null) {\n+                LOG.trace(\"Shutting down threadpool executor\");\n+                executor.shutdownNow();\n+              }\n+            }\n+          }\n         }\n       }\n+      // Because the above synchronized block will return or throw an exception,\n+      // so we don't need to do any check to prevent the first initialized\n+      // thread from stepping to following codes.\n+      try {\n+        Object retVal = method.invoke(currentUsedProxy.proxy, args);\n+        LOG.debug(\"Invocation successful on [{}]\", currentUsedProxy.proxyInfo);\n+        return retVal;\n+      } catch (InvocationTargetException ex) {\n+        Exception unwrappedException = unwrapInvocationTargetException(ex);\n+        logProxyException(unwrappedException, currentUsedProxy.proxyInfo);\n+        LOG.trace(\"Unsuccessful invocation on [{}]\",\n+            currentUsedProxy.proxyInfo);\n+        throw unwrappedException;\n+      }\n     }\n   }\n \n-\n-  private volatile ProxyInfo<T> currentUsedProxy = null;\n+  /** A proxy wrapping {@link RequestHedgingInvocationHandler}. */\n+  private ProxyInfo<T> currentUsedHandler = null;\n   private volatile String toIgnore = null;\n \n   public RequestHedgingProxyProvider(Configuration conf, URI uri,\n@@ -187,8 +203,8 @@ public RequestHedgingProxyProvider(Configuration conf, URI uri,\n   @SuppressWarnings(\"unchecked\")\n   @Override\n   public synchronized ProxyInfo<T> getProxy() {\n-    if (currentUsedProxy != null) {\n-      return currentUsedProxy;\n+    if (currentUsedHandler != null) {\n+      return currentUsedHandler;\n     }\n     Map<String, ProxyInfo<T>> targetProxyInfos = new HashMap<>();\n     StringBuilder combinedInfo = new StringBuilder(\"[\");\n@@ -203,13 +219,16 @@ public RequestHedgingProxyProvider(Configuration conf, URI uri,\n             RequestHedgingInvocationHandler.class.getClassLoader(),\n             new Class<?>[]{xface},\n             new RequestHedgingInvocationHandler(targetProxyInfos));\n-    return new ProxyInfo<T>(wrappedProxy, combinedInfo.toString());\n+    currentUsedHandler =\n+        new ProxyInfo<T>(wrappedProxy, combinedInfo.toString());\n+    return currentUsedHandler;\n   }\n \n   @Override\n   public synchronized void performFailover(T currentProxy) {\n-    toIgnore = this.currentUsedProxy.proxyInfo;\n-    this.currentUsedProxy = null;\n+    toIgnore = ((RequestHedgingInvocationHandler) Proxy.getInvocationHandler(\n+        currentUsedHandler.proxy)).currentUsedProxy.proxyInfo;\n+    this.currentUsedHandler = null;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/f858f1855455348f5ac517ebd96c2bbc3bc97489/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/RequestHedgingProxyProvider.java",
                "sha": "9011b25eda043dadd86cd44a0962f2cb483351ef",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hadoop/blob/f858f1855455348f5ac517ebd96c2bbc3bc97489/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRequestHedgingProxyProvider.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRequestHedgingProxyProvider.java?ref=f858f1855455348f5ac517ebd96c2bbc3bc97489",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRequestHedgingProxyProvider.java",
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.hadoop.ipc.StandbyException;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.test.GenericTestUtils;\n+import org.apache.hadoop.test.LambdaTestUtils;\n import org.apache.hadoop.util.Time;\n import org.apache.log4j.Level;\n import org.junit.Assert;\n@@ -634,6 +635,65 @@ public void testHedgingWhenConnectAndEOFException() throws Exception {\n     Mockito.verify(standby).getStats();\n   }\n \n+  /**\n+   * HDFS-14088, we first make a successful RPC call, so\n+   * RequestHedgingInvocationHandler#currentUsedProxy will be assigned to the\n+   * delayMock. Then: <br/>\n+   * 1. We start a thread which sleep for 1 sec and call\n+   * RequestHedgingProxyProvider#performFailover() <br/>\n+   * 2. We make an RPC call again, the call will sleep for 2 sec and throw an\n+   * exception for test.<br/>\n+   * 3. RequestHedgingInvocationHandler#invoke() will catch the exception and\n+   * log RequestHedgingInvocationHandler#currentUsedProxy. Before patch, there\n+   * will throw NullPointException.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testHedgingMultiThreads() throws Exception {\n+    final AtomicInteger counter = new AtomicInteger(0);\n+    final ClientProtocol delayMock = Mockito.mock(ClientProtocol.class);\n+    Mockito.when(delayMock.getStats()).thenAnswer(new Answer<long[]>() {\n+      @Override\n+      public long[] answer(InvocationOnMock invocation) throws Throwable {\n+        int flag = counter.incrementAndGet();\n+        Thread.sleep(2000);\n+        if (flag == 1) {\n+          return new long[]{1};\n+        } else {\n+          throw new IOException(\"Exception for test.\");\n+        }\n+      }\n+    });\n+    final ClientProtocol badMock = Mockito.mock(ClientProtocol.class);\n+    Mockito.when(badMock.getStats()).thenThrow(new IOException(\"Bad mock !!\"));\n+    final RequestHedgingProxyProvider<ClientProtocol> provider =\n+        new RequestHedgingProxyProvider<>(conf, nnUri, ClientProtocol.class,\n+            createFactory(delayMock, badMock));\n+    final ClientProtocol delayProxy = provider.getProxy().proxy;\n+    long[] stats = delayProxy.getStats();\n+    Assert.assertTrue(stats.length == 1);\n+    Assert.assertEquals(1, stats[0]);\n+    Assert.assertEquals(1, counter.get());\n+\n+    Thread t = new Thread() {\n+      @Override\n+      public void run() {\n+        try {\n+          // Fail over between calling delayProxy.getStats() and throw\n+          // exception.\n+          Thread.sleep(1000);\n+          provider.performFailover(delayProxy);\n+        } catch (Exception e) {\n+          e.printStackTrace();\n+        }\n+      }\n+    };\n+    t.start();\n+    LambdaTestUtils.intercept(IOException.class, \"Exception for test.\",\n+        delayProxy::getStats);\n+    t.join();\n+  }\n+\n   private HAProxyFactory<ClientProtocol> createFactory(\n       ClientProtocol... protos) {\n     final Iterator<ClientProtocol> iterator =",
                "raw_url": "https://github.com/apache/hadoop/raw/f858f1855455348f5ac517ebd96c2bbc3bc97489/hadoop-hdfs-project/hadoop-hdfs-client/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRequestHedgingProxyProvider.java",
                "sha": "fc81a09f424f0ebdeea1c0c61744ca7949777a8c",
                "status": "modified"
            }
        ],
        "message": "HDFS-14088. RequestHedgingProxyProvider can throw NullPointerException when failover due to no lock on currentUsedProxy. Contributed by Yuxuan Wang.",
        "parent": "https://github.com/apache/hadoop/commit/cb3382f667a7f9c1f10f92e5c2fb315d72a4cec9",
        "patched_files": [
            "RequestHedgingProxyProvider.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRequestHedgingProxyProvider.java"
        ]
    },
    "hadoop_f9e36de": {
        "bug_id": "hadoop_f9e36de",
        "commit": "https://github.com/apache/hadoop/commit/f9e36dea96f592d09f159e521379e426e7f07ec9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=f9e36dea96f592d09f159e521379e426e7f07ec9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -99,6 +99,9 @@ Release 2.9.0 - UNRELEASED\n     YARN-2934. Improve handling of container's stderr.\n     (Naganarasimha G R via gera)\n \n+    YARN-4530. LocalizedResource trigger a NPE Cause the NodeManager exit\n+    (tangshangwen via rohithsharmaks)\n+\n Release 2.8.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/CHANGES.txt",
                "sha": "80a3ed1dd270719218e99ba5966206fdc132ba33",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=f9e36dea96f592d09f159e521379e426e7f07ec9",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -878,12 +878,12 @@ public void run() {\n             Future<Path> completed = queue.take();\n             LocalizerResourceRequestEvent assoc = pending.remove(completed);\n             try {\n-              Path local = completed.get();\n               if (null == assoc) {\n                 LOG.error(\"Localized unknown resource to \" + completed);\n                 // TODO delete\n                 return;\n               }\n+              Path local = completed.get();\n               LocalResourceRequest key = assoc.getResource().getRequest();\n               publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil\n                 .getDU(new File(local.toUri()))));",
                "raw_url": "https://github.com/apache/hadoop/raw/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "c0c2e8e4fbcc69cc1b20aa6ba95f2bbff9dfe939",
                "status": "modified"
            }
        ],
        "message": "YARN-4530. LocalizedResource trigger a NPE Cause the NodeManager exit. (tangshangwen via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/4e4b3a8465a8433e78e015cb1ce7e0dc1ebeb523",
        "patched_files": [
            "ResourceLocalizationService.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_fa80ca4": {
        "bug_id": "hadoop_fa80ca4",
        "commit": "https://github.com/apache/hadoop/commit/fa80ca49bdd741823ff012ddbd7a0f1aecf26195",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=fa80ca49bdd741823ff012ddbd7a0f1aecf26195",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -249,6 +249,8 @@ Release 2.6.0 - UNRELEASED\n     YARN-2035. FileSystemApplicationHistoryStore should not make working dir\n     when it already exists. (Jonathan Eagles via zjshen)\n \n+    YARN-2405. NPE in FairSchedulerAppsBlock. (Tsuyoshi Ozawa via kasha)\n+\n Release 2.5.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/CHANGES.txt",
                "sha": "fa47c8ee2d85bc719c53cee1292491c85f79fd95",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java?ref=fa80ca49bdd741823ff012ddbd7a0f1aecf26195",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "patch": "@@ -110,6 +110,10 @@\n       String percent = String.format(\"%.1f\", appInfo.getProgress());\n       ApplicationAttemptId attemptId = app.getCurrentAppAttempt().getAppAttemptId();\n       int fairShare = fsinfo.getAppFairShare(attemptId);\n+      if (fairShare == FairSchedulerInfo.INVALID_FAIR_SHARE) {\n+        // FairScheduler#applications don't have the entry. Skip it.\n+        continue;\n+      }\n       //AppID numerical value parsed by parseHadoopID in yarn.dt.plugins.js\n       appsTableData.append(\"[\\\"<a href='\")\n       .append(url(\"app\", appInfo.getAppId())).append(\"'>\")",
                "raw_url": "https://github.com/apache/hadoop/raw/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/FairSchedulerAppsBlock.java",
                "sha": "2a1442ea09d4d7760606e5b067f618fe0fd3137d",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerInfo.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerInfo.java?ref=fa80ca49bdd741823ff012ddbd7a0f1aecf26195",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerInfo.java",
                "patch": "@@ -25,12 +25,14 @@\n import javax.xml.bind.annotation.XmlType;\n \n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n \n @XmlRootElement(name = \"fairScheduler\")\n @XmlType(name = \"fairScheduler\")\n @XmlAccessorType(XmlAccessType.FIELD)\n public class FairSchedulerInfo extends SchedulerInfo {\n+  public static final int INVALID_FAIR_SHARE = -1;\n   private FairSchedulerQueueInfo rootQueue;\n   \n   @XmlTransient\n@@ -44,9 +46,18 @@ public FairSchedulerInfo(FairScheduler fs) {\n     rootQueue = new FairSchedulerQueueInfo(scheduler.getQueueManager().\n         getRootQueue(), scheduler);\n   }\n-  \n+\n+  /**\n+   * Get the fair share assigned to the appAttemptId.\n+   * @param appAttemptId\n+   * @return The fair share assigned to the appAttemptId,\n+   * <code>FairSchedulerInfo#INVALID_FAIR_SHARE</code> if the scheduler does\n+   * not know about this application attempt.\n+   */\n   public int getAppFairShare(ApplicationAttemptId appAttemptId) {\n-    return scheduler.getSchedulerApp(appAttemptId).getFairShare().getMemory();\n+    FSAppAttempt fsAppAttempt = scheduler.getSchedulerApp(appAttemptId);\n+    return fsAppAttempt == null ?\n+        INVALID_FAIR_SHARE :  fsAppAttempt.getFairShare().getMemory();\n   }\n   \n   public FairSchedulerQueueInfo getRootQueueInfo() {",
                "raw_url": "https://github.com/apache/hadoop/raw/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerInfo.java",
                "sha": "f97ff8ae64bb6b0d29b19669db9473786fa2344c",
                "status": "modified"
            },
            {
                "additions": 95,
                "blob_url": "https://github.com/apache/hadoop/blob/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java?ref=fa80ca49bdd741823ff012ddbd7a0f1aecf26195",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "patch": "@@ -22,20 +22,29 @@\n import com.google.inject.Binder;\n import com.google.inject.Injector;\n import com.google.inject.Module;\n+import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n+\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration;\n import org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM;\n import org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM;\n import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n import org.apache.hadoop.yarn.webapp.test.WebAppTests;\n+import org.junit.Assert;\n import org.junit.Test;\n \n import java.io.IOException;\n@@ -75,12 +84,67 @@ public void configure(Binder binder) {\n     WebAppTests.flushOutput(injector);\n   }\n \n+\n+  /**\n+   *  Testing inconsistent state between AbstractYarnScheduler#applications and\n+   *  RMContext#applications\n+   */\n+  @Test\n+  public void testFairSchedulerWebAppPageInInconsistentState() {\n+    List<RMAppState> appStates = Arrays.asList(\n+        RMAppState.NEW,\n+        RMAppState.NEW_SAVING,\n+        RMAppState.SUBMITTED,\n+        RMAppState.RUNNING,\n+        RMAppState.FINAL_SAVING,\n+        RMAppState.ACCEPTED,\n+        RMAppState.FINISHED\n+    );\n+    final RMContext rmContext = mockRMContext(appStates);\n+    Injector injector = WebAppTests.createMockInjector(RMContext.class,\n+        rmContext,\n+        new Module() {\n+          @Override\n+          public void configure(Binder binder) {\n+            try {\n+              ResourceManager mockRmWithFairScheduler =\n+                  mockRmWithApps(rmContext);\n+              binder.bind(ResourceManager.class).toInstance\n+                  (mockRmWithFairScheduler);\n+\n+            } catch (IOException e) {\n+              throw new IllegalStateException(e);\n+            }\n+          }\n+        });\n+    FairSchedulerPage fsViewInstance =\n+        injector.getInstance(FairSchedulerPage.class);\n+    try {\n+      fsViewInstance.render();\n+    } catch (Exception e) {\n+      Assert.fail(\"Failed to render FairSchedulerPage: \" +\n+          StringUtils.stringifyException(e));\n+    }\n+    WebAppTests.flushOutput(injector);\n+  }\n+\n   private static RMContext mockRMContext(List<RMAppState> states) {\n     final ConcurrentMap<ApplicationId, RMApp> applicationsMaps = Maps\n         .newConcurrentMap();\n     int i = 0;\n     for (RMAppState state : states) {\n-      MockRMApp app = new MockRMApp(i, i, state);\n+      MockRMApp app = new MockRMApp(i, i, state) {\n+        @Override\n+        public RMAppMetrics getRMAppMetrics() {\n+          return new RMAppMetrics(Resource.newInstance(0, 0), 0, 0);\n+        }\n+        @Override\n+        public YarnApplicationState createApplicationState() {\n+          return YarnApplicationState.ACCEPTED;\n+        }\n+      };\n+      RMAppAttempt attempt = mock(RMAppAttempt.class);\n+      app.setCurrentAppAttempt(attempt);\n       applicationsMaps.put(app.getApplicationId(), app);\n       i++;\n     }\n@@ -113,4 +177,34 @@ private static FairScheduler mockFairScheduler() throws IOException {\n     fs.init(conf);\n     return fs;\n   }\n+\n+  private static ResourceManager mockRmWithApps(RMContext rmContext) throws\n+      IOException {\n+    ResourceManager rm = mock(ResourceManager.class);\n+    ResourceScheduler rs =  mockFairSchedulerWithoutApps(rmContext);\n+    when(rm.getResourceScheduler()).thenReturn(rs);\n+    when(rm.getRMContext()).thenReturn(rmContext);\n+    return rm;\n+  }\n+\n+  private static FairScheduler mockFairSchedulerWithoutApps(RMContext rmContext)\n+      throws IOException {\n+    FairScheduler fs = new FairScheduler() {\n+      @Override\n+      public FSAppAttempt getSchedulerApp(ApplicationAttemptId\n+          applicationAttemptId) {\n+        return null ;\n+      }\n+      @Override\n+      public FSAppAttempt getApplicationAttempt(ApplicationAttemptId\n+          applicationAttemptId) {\n+        return null;\n+      }\n+    };\n+    FairSchedulerConfiguration conf = new FairSchedulerConfiguration();\n+    fs.setRMContext(rmContext);\n+    fs.init(conf);\n+    return fs;\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/fa80ca49bdd741823ff012ddbd7a0f1aecf26195/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebAppFairScheduler.java",
                "sha": "111bf47d2b15ee18b07f04ecf3d83b898bb5e207",
                "status": "modified"
            }
        ],
        "message": "YARN-2405. NPE in FairSchedulerAppsBlock. (Tsuyoshi Ozawa via kasha)",
        "parent": "https://github.com/apache/hadoop/commit/9d68445710feff9fda9ee69847beeaf3e99b85ef",
        "patched_files": [
            "FairSchedulerAppsBlock.java",
            "FairSchedulerInfo.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestRMWebAppFairScheduler.java"
        ]
    },
    "hadoop_fb69519": {
        "bug_id": "hadoop_fb69519",
        "commit": "https://github.com/apache/hadoop/commit/fb69519e68c53aa2830ac50915011867ab0b1971",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/fb69519e68c53aa2830ac50915011867ab0b1971/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java?ref=fb69519e68c53aa2830ac50915011867ab0b1971",
                "deletions": 1,
                "filename": "hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "patch": "@@ -219,7 +219,9 @@ public Builder addMetadata(String key, String value) {\n     }\n \n     public Builder addAllMetadata(Map<String, String> additionalMetadata) {\n-      metadata.putAll(additionalMetadata);\n+      if (additionalMetadata != null) {\n+        metadata.putAll(additionalMetadata);\n+      }\n       return this;\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/fb69519e68c53aa2830ac50915011867ab0b1971/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "sha": "14ed5f665b267ae704bd0826effdb3f28fe6c38d",
                "status": "modified"
            }
        ],
        "message": "HDDS-1011. Fix NPE BucketManagerImpl.setBucketProperty. Contributed by Xiaoyu Yao.",
        "parent": "https://github.com/apache/hadoop/commit/6cace58e212d3ee0aec988926a5a17c9cc58e645",
        "patched_files": [
            "OmBucketInfo.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestOmBucketInfo.java"
        ]
    },
    "hadoop_fbdb249": {
        "bug_id": "hadoop_fbdb249",
        "commit": "https://github.com/apache/hadoop/commit/fbdb24946051f7abc2d065217ef8c52e0cf3f16f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=fbdb24946051f7abc2d065217ef8c52e0cf3f16f",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -81,6 +81,9 @@ Trunk (unreleased changes)\n     HADOOP-6603. Provide workaround for issue with Kerberos not resolving \n     cross-realm principal (Kan Zhang and Jitendra Pandey via jghoman)\n \n+    HADOOP-6620. NPE if renewer is passed as null in getDelegationToken.\n+    (Jitendra Pandey via jghoman)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/CHANGES.txt",
                "sha": "b54723d9bcc63a66f19a9a9369792b956995e7f4",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java?ref=fbdb24946051f7abc2d065217ef8c52e0cf3f16f",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java",
                "patch": "@@ -49,8 +49,16 @@ public AbstractDelegationTokenIdentifier() {\n   }\n   \n   public AbstractDelegationTokenIdentifier(Text owner, Text renewer, Text realUser) {\n-    this.owner = owner;\n-    this.renewer = renewer;\n+    if (owner == null) {\n+      this.owner = new Text();\n+    } else {\n+      this.owner = owner;\n+    }\n+    if (renewer == null) {\n+      this.renewer = new Text();\n+    } else {\n+      this.renewer = renewer;\n+    }\n     if (realUser == null) {\n       this.realUser = new Text();\n     } else {\n@@ -170,4 +178,14 @@ public void write(DataOutput out) throws IOException {\n     WritableUtils.writeVInt(out, sequenceNumber);\n     WritableUtils.writeVInt(out, masterKeyId);\n   }\n+  \n+  public String toString() {\n+    StringBuilder buffer = new StringBuilder();\n+    buffer\n+        .append(\"owner=\" + owner + \", renewer=\" + renewer + \", realUser=\"\n+            + realUser + \", issueDate=\" + issueDate + \", maxDate=\" + maxDate\n+            + \", sequenceNumber=\" + sequenceNumber + \", masterKeyId=\"\n+            + masterKeyId);\n+    return buffer.toString();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java",
                "sha": "d29ea324197a444d0bd7dbc5f58bfe1dea85035e",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java?ref=fbdb24946051f7abc2d065217ef8c52e0cf3f16f",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java",
                "patch": "@@ -178,6 +178,7 @@ private synchronized void removeExpiredKeys() {\n   \n   @Override\n   protected synchronized byte[] createPassword(TokenIdent identifier) {\n+    LOG.info(\"Creating password for identifier: \"+identifier);\n     int sequenceNum;\n     long now = System.currentTimeMillis();\n     sequenceNum = ++delegationTokenSequenceNumber;\n@@ -220,12 +221,13 @@ public synchronized long renewToken(Token<TokenIdent> token,\n     DataInputStream in = new DataInputStream(buf);\n     TokenIdent id = createIdentifier();\n     id.readFields(in);\n-\n+    LOG.info(\"Token renewal requested for identifier: \"+id);\n+    \n     if (id.getMaxDate() < now) {\n       throw new InvalidToken(\"User \" + renewer + \n                              \" tried to renew an expired token\");\n     }\n-    if (id.getRenewer() == null) {\n+    if ((id.getRenewer() == null) || (\"\".equals(id.getRenewer().toString()))) {\n       throw new AccessControlException(\"User \" + renewer + \n                                        \" tried to renew a token without \" +\n                                        \"a renewer\");\n@@ -271,13 +273,16 @@ public synchronized TokenIdent cancelToken(Token<TokenIdent> token,\n     DataInputStream in = new DataInputStream(buf);\n     TokenIdent id = createIdentifier();\n     id.readFields(in);\n+    LOG.info(\"Token cancelation requested for identifier: \"+id);\n+    \n     if (id.getUser() == null) {\n       throw new InvalidToken(\"Token with no owner\");\n     }\n     String owner = id.getUser().getUserName();\n     Text renewer = id.getRenewer();\n     if (!canceller.equals(owner)\n-        && (renewer == null || !canceller.equals(renewer.toString()))) {\n+        && (renewer == null || \"\".equals(renewer.toString()) || !canceller\n+            .equals(renewer.toString()))) {\n       throw new AccessControlException(canceller\n           + \" is not authorized to cancel the token\");\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/src/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java",
                "sha": "555dcff91126e91782c24b1d2eea0dc76624707e",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java?ref=fbdb24946051f7abc2d065217ef8c52e0cf3f16f",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "patch": "@@ -365,4 +365,24 @@ public void run() {\n       dtSecretManager.stopThreads();\n     }\n   }\n+  \n+  @Test \n+  public void testDelegationTokenNullRenewer() throws Exception {\n+    TestDelegationTokenSecretManager dtSecretManager = \n+      new TestDelegationTokenSecretManager(24*60*60*1000,\n+        10*1000,1*1000,3600000);\n+    dtSecretManager.startThreads();\n+    TestDelegationTokenIdentifier dtId = new TestDelegationTokenIdentifier(new Text(\n+        \"theuser\"), null, null);\n+    Token<TestDelegationTokenIdentifier> token = new Token<TestDelegationTokenIdentifier>(\n+        dtId, dtSecretManager);\n+    Assert.assertTrue(token != null);\n+    try {\n+      dtSecretManager.renewToken(token, \"\");\n+      Assert.fail(\"Renewal must not succeed\");\n+    } catch (IOException e) {\n+      //PASS\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/fbdb24946051f7abc2d065217ef8c52e0cf3f16f/src/test/core/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "sha": "22cfb3d84c24597849f6a45866ec0bb4165652ba",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6620. NPE if renewer is passed as null in getDelegationToken. Contributed by Jitendra Pandey.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@953896 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/6378822a67c0baa502d22201f5c2b478cbe1261c",
        "patched_files": [
            "CHANGES.java",
            "AbstractDelegationTokenIdentifier.java",
            "AbstractDelegationTokenSecretManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationToken.java"
        ]
    },
    "hadoop_fbf7e81": {
        "bug_id": "hadoop_fbf7e81",
        "commit": "https://github.com/apache/hadoop/commit/fbf7e81ca007e009b492e3b99060bbfb74394f46",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt?ref=fbf7e81ca007e009b492e3b99060bbfb74394f46",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "patch": "@@ -391,3 +391,6 @@\n \n     HDFS-8857. Erasure Coding: Fix ArrayIndexOutOfBoundsException in\n     TestWriteStripedFileWithFailure. (Li Bo)\n+\n+    HDFS-8827. Erasure Coding: Fix NPE when NameNode processes over-replicated\n+    striped blocks. (Walter Su and Takuya Fukudome via jing9)",
                "raw_url": "https://github.com/apache/hadoop/raw/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "sha": "45afd2cea912ae83159c1dab50a01d6ac577dab7",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=fbf7e81ca007e009b492e3b99060bbfb74394f46",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3135,14 +3135,13 @@ private void chooseExcessReplicates(\n     assert namesystem.hasWriteLock();\n     // first form a rack to datanodes map and\n     BlockCollection bc = getBlockCollection(storedBlock);\n-    final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(\n-        bc.getStoragePolicyID());\n-    final List<StorageType> excessTypes = storagePolicy.chooseExcess(\n-        replication, DatanodeStorageInfo.toStorageTypes(nonExcess));\n     if (storedBlock.isStriped()) {\n-      chooseExcessReplicasStriped(bc, nonExcess, storedBlock, delNodeHint,\n-          excessTypes);\n+      chooseExcessReplicasStriped(bc, nonExcess, storedBlock, delNodeHint);\n     } else {\n+      final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(\n+          bc.getStoragePolicyID());\n+      final List<StorageType> excessTypes = storagePolicy.chooseExcess(\n+          replication, DatanodeStorageInfo.toStorageTypes(nonExcess));\n       chooseExcessReplicasContiguous(bc, nonExcess, storedBlock,\n           replication, addedNode, delNodeHint, excessTypes);\n     }\n@@ -3216,8 +3215,7 @@ private void chooseExcessReplicasContiguous(BlockCollection bc,\n   private void chooseExcessReplicasStriped(BlockCollection bc,\n       final Collection<DatanodeStorageInfo> nonExcess,\n       BlockInfo storedBlock,\n-      DatanodeDescriptor delNodeHint,\n-      List<StorageType> excessTypes) {\n+      DatanodeDescriptor delNodeHint) {\n     assert storedBlock instanceof BlockInfoStriped;\n     BlockInfoStriped sblk = (BlockInfoStriped) storedBlock;\n     short groupSize = sblk.getTotalBlockNum();\n@@ -3237,6 +3235,14 @@ private void chooseExcessReplicasStriped(BlockCollection bc,\n       found.set(index);\n       storage2index.put(storage, index);\n     }\n+    // the number of target left replicas equals to the of number of the found\n+    // indices.\n+    int numOfTarget = found.cardinality();\n+\n+    final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(\n+        bc.getStoragePolicyID());\n+    final List<StorageType> excessTypes = storagePolicy.chooseExcess(\n+        (short)numOfTarget, DatanodeStorageInfo.toStorageTypes(nonExcess));\n \n     // use delHint only if delHint is duplicated\n     final DatanodeStorageInfo delStorageHint =",
                "raw_url": "https://github.com/apache/hadoop/raw/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "0ceb3921fb90c25921a589f5454c2c9d930d7888",
                "status": "modified"
            },
            {
                "additions": 151,
                "blob_url": "https://github.com/apache/hadoop/blob/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java?ref=fbf7e81ca007e009b492e3b99060bbfb74394f46",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "patch": "@@ -24,9 +24,14 @@\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.protocol.Block;\n+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n import org.apache.hadoop.hdfs.protocol.LocatedBlocks;\n import org.apache.hadoop.hdfs.protocol.LocatedStripedBlock;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n+import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset;\n import org.junit.After;\n import org.junit.Before;\n@@ -35,6 +40,7 @@\n import java.io.IOException;\n import java.util.Arrays;\n import java.util.HashSet;\n+import java.util.List;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n@@ -49,14 +55,16 @@\n   private final short PARITY_BLK_NUM = HdfsConstants.NUM_PARITY_BLOCKS;\n   private final short GROUP_SIZE = DATA_BLK_NUM + PARITY_BLK_NUM;\n   private final int CELLSIZE = HdfsConstants.BLOCK_STRIPED_CELL_SIZE;\n-  private final int NUM_STRIPE_PER_BLOCK = 1;\n+  private final int NUM_STRIPE_PER_BLOCK = 4;\n   private final int BLOCK_SIZE = NUM_STRIPE_PER_BLOCK * CELLSIZE;\n   private final int numDNs = GROUP_SIZE + 3;\n \n   @Before\n   public void setup() throws IOException {\n     Configuration conf = new Configuration();\n     conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);\n+    // disable block recovery\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_MAX_STREAMS_KEY, 0);\n     SimulatedFSDataset.setFactory(conf);\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDNs).build();\n     cluster.waitActive();\n@@ -113,4 +121,146 @@ public void testProcessOverReplicatedStripedBlock() throws Exception {\n         filePath.toString(), 0, fileLen);\n     DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE);\n   }\n+\n+  @Test\n+  public void testProcessOverReplicatedSBSmallerThanFullBlocks()\n+      throws Exception {\n+    // Create a EC file which doesn't fill full internal blocks.\n+    int fileLen = CELLSIZE * (DATA_BLK_NUM - 1);\n+    byte[] content = new byte[fileLen];\n+    DFSTestUtil.writeFile(fs, filePath, new String(content));\n+    LocatedBlocks lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    LocatedStripedBlock bg = (LocatedStripedBlock) (lbs.get(0));\n+    long gs = bg.getBlock().getGenerationStamp();\n+    String bpid = bg.getBlock().getBlockPoolId();\n+    long groupId = bg.getBlock().getBlockId();\n+    Block blk = new Block(groupId, BLOCK_SIZE, gs);\n+    cluster.triggerBlockReports();\n+    List<DatanodeInfo> infos = Arrays.asList(bg.getLocations());\n+\n+    // let a internal block be over replicated with 2 redundant blocks.\n+    // Therefor number of internal blocks is over GROUP_SIZE. (5 data blocks +\n+    // 3 parity blocks  + 2 redundant blocks > GROUP_SIZE)\n+    blk.setBlockId(groupId + 2);\n+    List<DataNode> dataNodeList = cluster.getDataNodes();\n+    for (int i = 0; i < numDNs; i++) {\n+      if (!infos.contains(dataNodeList.get(i).getDatanodeId())) {\n+        cluster.injectBlocks(i, Arrays.asList(blk), bpid);\n+        System.out.println(\"XXX: inject block into datanode \" + i);\n+      }\n+    }\n+\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+    // add to invalidates\n+    cluster.triggerHeartbeats();\n+    // datanode delete block\n+    cluster.triggerHeartbeats();\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+\n+    // verify that all internal blocks exists\n+    lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE - 1);\n+  }\n+\n+  @Test\n+  public void testProcessOverReplicatedAndCorruptStripedBlock()\n+      throws Exception {\n+    long fileLen = DATA_BLK_NUM * BLOCK_SIZE;\n+    DFSTestUtil.createStripedFile(cluster, filePath, null, 1,\n+        NUM_STRIPE_PER_BLOCK, false);\n+    LocatedBlocks lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    LocatedStripedBlock bg = (LocatedStripedBlock) (lbs.get(0));\n+    long gs = bg.getBlock().getGenerationStamp();\n+    String bpid = bg.getBlock().getBlockPoolId();\n+    long groupId = bg.getBlock().getBlockId();\n+    Block blk = new Block(groupId, BLOCK_SIZE, gs);\n+    BlockInfoStriped blockInfo = new BlockInfoStriped(blk,\n+        ErasureCodingSchemaManager.getSystemDefaultSchema(), CELLSIZE);\n+    for (int i = 0; i < GROUP_SIZE; i++) {\n+      blk.setBlockId(groupId + i);\n+      cluster.injectBlocks(i, Arrays.asList(blk), bpid);\n+    }\n+    cluster.triggerBlockReports();\n+\n+    // let a internal block be corrupt\n+    BlockManager bm = cluster.getNamesystem().getBlockManager();\n+    List<DatanodeInfo> infos = Arrays.asList(bg.getLocations());\n+    List<String> storages = Arrays.asList(bg.getStorageIDs());\n+    cluster.getNamesystem().writeLock();\n+    try {\n+      bm.findAndMarkBlockAsCorrupt(lbs.getLastLocatedBlock().getBlock(),\n+          infos.get(0), storages.get(0), \"TEST\");\n+    } finally {\n+      cluster.getNamesystem().writeUnlock();\n+    }\n+    assertEquals(1, bm.countNodes(blockInfo).corruptReplicas());\n+\n+    // let a internal block be over replicated with 2 redundant block.\n+    blk.setBlockId(groupId + 2);\n+    cluster.injectBlocks(numDNs - 3, Arrays.asList(blk), bpid);\n+    cluster.injectBlocks(numDNs - 2, Arrays.asList(blk), bpid);\n+\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+    // add to invalidates\n+    cluster.triggerHeartbeats();\n+    // datanode delete block\n+    cluster.triggerHeartbeats();\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+\n+    // verify that all internal blocks exists\n+    lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE);\n+  }\n+\n+  @Test\n+  public void testProcessOverReplicatedAndMissingStripedBlock()\n+      throws Exception {\n+    long fileLen = CELLSIZE * DATA_BLK_NUM;\n+    DFSTestUtil.createStripedFile(cluster, filePath, null, 1,\n+        NUM_STRIPE_PER_BLOCK, false);\n+    LocatedBlocks lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    LocatedStripedBlock bg = (LocatedStripedBlock) (lbs.get(0));\n+    long gs = bg.getBlock().getGenerationStamp();\n+    String bpid = bg.getBlock().getBlockPoolId();\n+    long groupId = bg.getBlock().getBlockId();\n+    Block blk = new Block(groupId, BLOCK_SIZE, gs);\n+    // only inject GROUP_SIZE - 1 blocks, so there is one block missing\n+    for (int i = 0; i < GROUP_SIZE - 1; i++) {\n+      blk.setBlockId(groupId + i);\n+      cluster.injectBlocks(i, Arrays.asList(blk), bpid);\n+    }\n+    cluster.triggerBlockReports();\n+\n+    // let a internal block be over replicated with 2 redundant blocks.\n+    // Therefor number of internal blocks is over GROUP_SIZE. (5 data blocks +\n+    // 3 parity blocks  + 2 redundant blocks > GROUP_SIZE)\n+    blk.setBlockId(groupId + 2);\n+    cluster.injectBlocks(numDNs - 3, Arrays.asList(blk), bpid);\n+    cluster.injectBlocks(numDNs - 2, Arrays.asList(blk), bpid);\n+\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+    // add to invalidates\n+    cluster.triggerHeartbeats();\n+    // datanode delete block\n+    cluster.triggerHeartbeats();\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+\n+    // Since one block is missing, when over-replicated blocks got deleted,\n+    // we are left GROUP_SIZE - 1 blocks.\n+    lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE - 1);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "sha": "337911d0cae11ce5e0338e4d8ffa2659b2cc1db4",
                "status": "modified"
            }
        ],
        "message": "HDFS-8827. Erasure Coding: Fix NPE when NameNode processes over-replicated striped blocks. Contributed by Walter Su and Takuya Fukudome.",
        "parent": "https://github.com/apache/hadoop/commit/8799363db1c0e0ce0abd4ab68b780092e7dc5263",
        "patched_files": [
            "CHANGES-HDFS-EC-7285.java",
            "BlockManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestAddOverReplicatedStripedBlocks.java"
        ]
    },
    "hadoop_fc5bd93": {
        "bug_id": "hadoop_fc5bd93",
        "commit": "https://github.com/apache/hadoop/commit/fc5bd930df793c3fab2964bb9ea2d0e9d412e493",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=fc5bd930df793c3fab2964bb9ea2d0e9d412e493",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -385,6 +385,9 @@ Release 2.1.2 - UNRELEASED\n     HDFS-5255. Distcp job fails with hsftp when https is enabled in insecure\n     cluster. (Arpit Agarwal)\n \n+    HDFS-5279. Guard against NullPointerException in NameNode JSP pages before\n+    initialization of FSNamesystem. (cnauroth)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "29b9be665d62eeb6bcc3f105e961c776291ef5cc",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java?ref=fc5bd930df793c3fab2964bb9ea2d0e9d412e493",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java",
                "patch": "@@ -30,6 +30,7 @@\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.Date;\n import java.util.Iterator;\n import java.util.List;\n@@ -210,6 +211,9 @@ static String getCorruptFilesWarning(FSNamesystem fsn) {\n \n   static void generateSnapshotReport(JspWriter out, FSNamesystem fsn)\n       throws IOException {\n+    if (fsn == null) {\n+      return;\n+    }\n     out.println(\"<div id=\\\"snapshotstats\\\"><div class=\\\"dfstable\\\">\"\n         + \"<table class=\\\"storage\\\" title=\\\"Snapshot Summary\\\">\\n\"\n         + \"<thead><tr><td><b>Snapshottable directories</b></td>\"\n@@ -652,7 +656,8 @@ static void redirectToRandomDataNode(ServletContext context,\n         .getAttribute(JspHelper.CURRENT_CONF);\n     // We can't redirect if there isn't a DN to redirect to.\n     // Lets instead show a proper error message.\n-    if (nn.getNamesystem().getNumLiveDataNodes() < 1) {\n+    FSNamesystem fsn = nn.getNamesystem();\n+    if (fsn == null || fsn.getNumLiveDataNodes() < 1) {\n       throw new IOException(\"Can't browse the DFS since there are no \" +\n           \"live nodes available to redirect to.\");\n     }\n@@ -688,6 +693,20 @@ static void redirectToRandomDataNode(ServletContext context,\n     resp.sendRedirect(redirectLocation);\n   }\n \n+  /**\n+   * Returns a descriptive label for the running NameNode.  If the NameNode has\n+   * initialized to the point of running its RPC server, then this label consists\n+   * of the host and port of the RPC server.  Otherwise, the label is a message\n+   * stating that the NameNode is still initializing.\n+   * \n+   * @param nn NameNode to describe\n+   * @return String NameNode label\n+   */\n+  static String getNameNodeLabel(NameNode nn) {\n+    return nn.getRpcServer() != null ? nn.getNameNodeAddressHostPortString() :\n+      \"initializing\";\n+  }\n+\n   static class NodeListJsp {\n     private int rowNum = 0;\n \n@@ -843,6 +862,9 @@ void generateNodesList(ServletContext context, JspWriter out,\n         HttpServletRequest request) throws IOException {\n       final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n       final FSNamesystem ns = nn.getNamesystem();\n+      if (ns == null) {\n+        return;\n+      }\n       final DatanodeManager dm = ns.getBlockManager().getDatanodeManager();\n \n       final List<DatanodeDescriptor> live = new ArrayList<DatanodeDescriptor>();\n@@ -1022,14 +1044,16 @@ private static String getLocalParentDir(INode inode) {\n     final BlockManager blockManager;\n     \n     XMLBlockInfo(FSNamesystem fsn, Long blockId) {\n-      this.blockManager = fsn.getBlockManager();\n+      this.blockManager = fsn != null ? fsn.getBlockManager() : null;\n \n       if (blockId == null) {\n         this.block = null;\n         this.inode = null;\n       } else {\n         this.block = new Block(blockId);\n-        this.inode = ((INode)blockManager.getBlockCollection(block)).asFile();\n+        this.inode = blockManager != null ?\n+          ((INode)blockManager.getBlockCollection(block)).asFile() :\n+          null;\n       }\n     }\n \n@@ -1103,8 +1127,10 @@ public void toXML(XMLOutputter doc) throws IOException {\n         } \n \n         doc.startTag(\"replicas\");\n-        for(final Iterator<DatanodeDescriptor> it = blockManager.datanodeIterator(block);\n-            it.hasNext(); ) {\n+        for (final Iterator<DatanodeDescriptor> it = blockManager != null ?\n+            blockManager.datanodeIterator(block) :\n+            Collections.<DatanodeDescriptor>emptyList().iterator();\n+            it.hasNext();) {\n           doc.startTag(\"replica\");\n \n           DatanodeDescriptor dd = it.next();\n@@ -1140,7 +1166,7 @@ public void toXML(XMLOutputter doc) throws IOException {\n     \n     XMLCorruptBlockInfo(FSNamesystem fsn, Configuration conf,\n                                int numCorruptBlocks, Long startingBlockId) {\n-      this.blockManager = fsn.getBlockManager();\n+      this.blockManager = fsn != null ? fsn.getBlockManager() : null;\n       this.conf = conf;\n       this.numCorruptBlocks = numCorruptBlocks;\n       this.startingBlockId = startingBlockId;\n@@ -1163,16 +1189,19 @@ public void toXML(XMLOutputter doc) throws IOException {\n       doc.endTag();\n       \n       doc.startTag(\"num_missing_blocks\");\n-      doc.pcdata(\"\"+blockManager.getMissingBlocksCount());\n+      doc.pcdata(\"\" + (blockManager != null ?\n+        blockManager.getMissingBlocksCount() : 0));\n       doc.endTag();\n       \n       doc.startTag(\"num_corrupt_replica_blocks\");\n-      doc.pcdata(\"\"+blockManager.getCorruptReplicaBlocksCount());\n+      doc.pcdata(\"\" + (blockManager != null ?\n+        blockManager.getCorruptReplicaBlocksCount() : 0));\n       doc.endTag();\n      \n       doc.startTag(\"corrupt_replica_block_ids\");\n-      final long[] corruptBlockIds = blockManager.getCorruptReplicaBlockIds(\n-          numCorruptBlocks, startingBlockId);\n+      final long[] corruptBlockIds = blockManager != null ?\n+        blockManager.getCorruptReplicaBlockIds(numCorruptBlocks,\n+        startingBlockId) : null;\n       if (corruptBlockIds != null) {\n         for (Long blockId: corruptBlockIds) {\n           doc.startTag(\"block_id\");",
                "raw_url": "https://github.com/apache/hadoop/raw/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeJspHelper.java",
                "sha": "1f3d328b8f0262b7256a7712419fb5f929608e22",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp?ref=fc5bd930df793c3fab2964bb9ea2d0e9d412e493",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp",
                "patch": "@@ -25,6 +25,7 @@\n \timport=\"org.apache.hadoop.fs.Path\"\n \timport=\"org.apache.hadoop.ha.HAServiceProtocol.HAServiceState\"\n \timport=\"java.util.Collection\"\n+\timport=\"java.util.Collections\"\n \timport=\"java.util.Arrays\" %>\n <%!//for java.io.Serializable\n   private static final long serialVersionUID = 1L;%>\n@@ -34,9 +35,10 @@\n   HAServiceState nnHAState = nn.getServiceState();\n   boolean isActive = (nnHAState == HAServiceState.ACTIVE);\n   String namenodeRole = nn.getRole().toString();\n-  String namenodeLabel = nn.getNameNodeAddressHostPortString();\n-  Collection<FSNamesystem.CorruptFileBlockInfo> corruptFileBlocks = \n-\tfsn.listCorruptFileBlocks(\"/\", null);\n+  String namenodeLabel = NamenodeJspHelper.getNameNodeLabel(nn);\n+  Collection<FSNamesystem.CorruptFileBlockInfo> corruptFileBlocks = fsn != null ?\n+    fsn.listCorruptFileBlocks(\"/\", null) :\n+    Collections.<FSNamesystem.CorruptFileBlockInfo>emptyList();\n   int corruptFileCount = corruptFileBlocks.size();\n %>\n \n@@ -48,7 +50,7 @@\n <h1><%=namenodeRole%> '<%=namenodeLabel%>'</h1>\n <%=NamenodeJspHelper.getVersionTable(fsn)%>\n <br>\n-<% if (isActive) { %> \n+<% if (isActive && fsn != null) { %> \n   <b><a href=\"/nn_browsedfscontent.jsp\">Browse the filesystem</a></b>\n   <br>\n <% } %> ",
                "raw_url": "https://github.com/apache/hadoop/raw/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/corrupt_files.jsp",
                "sha": "7c9050ddb1c16072c46b6e8299f44f7a3aaa77bf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp?ref=fc5bd930df793c3fab2964bb9ea2d0e9d412e493",
                "deletions": 11,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp",
                "patch": "@@ -34,29 +34,20 @@\n   boolean isActive = (nnHAState == HAServiceState.ACTIVE);\n   String namenodeRole = nn.getRole().toString();\n   String namenodeState = nnHAState.toString();\n-  String namenodeLabel = nn.getRpcServer() != null ?\n-    nn.getNameNodeAddressHostPortString() : null;\n+  String namenodeLabel = NamenodeJspHelper.getNameNodeLabel(nn);\n %>\n \n <!DOCTYPE html>\n <html>\n <head>\n <link rel=\"stylesheet\" type=\"text/css\" href=\"/static/hadoop.css\">\n-<% if (namenodeLabel != null) { %>\n <title>Hadoop <%=namenodeRole%>&nbsp;<%=namenodeLabel%></title>\n-<% } else { %>\n-<title>Hadoop <%=namenodeRole%></title>\n-<% } %>\n </head>    \n <body>\n-<% if (namenodeLabel != null) { %>\n <h1><%=namenodeRole%> '<%=namenodeLabel%>' (<%=namenodeState%>)</h1>\n-<% } else { %>\n-<h1><%=namenodeRole%> (<%=namenodeState%>)</h1>\n-<% } %>\n <%= NamenodeJspHelper.getVersionTable(fsn) %>\n <br />\n-<% if (isActive) { %> \n+<% if (isActive && fsn != null) { %> \n   <b><a href=\"/nn_browsedfscontent.jsp\">Browse the filesystem</a></b><br>\n <% } %> \n <b><a href=\"/logs/\"><%=namenodeRole%> Logs</a></b>",
                "raw_url": "https://github.com/apache/hadoop/raw/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.jsp",
                "sha": "10872a7af0957aed0e92337e1648b15b503dce3e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp?ref=fc5bd930df793c3fab2964bb9ea2d0e9d412e493",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp",
                "patch": "@@ -33,7 +33,7 @@ String namenodeRole = nn.getRole().toString();\n FSNamesystem fsn = nn.getNamesystem();\n HAServiceState nnHAState = nn.getServiceState();\n boolean isActive = (nnHAState == HAServiceState.ACTIVE);\n-String namenodeLabel = nn.getNameNodeAddressHostPortString();\n+String namenodeLabel = NamenodeJspHelper.getNameNodeLabel(nn);\n %>\n \n <!DOCTYPE html>\n@@ -46,7 +46,7 @@ String namenodeLabel = nn.getNameNodeAddressHostPortString();\n <h1><%=namenodeRole%> '<%=namenodeLabel%>'</h1>\n <%= NamenodeJspHelper.getVersionTable(fsn) %>\n <br />\n-<% if (isActive) { %> \n+<% if (isActive && fsn != null) { %> \n   <b><a href=\"/nn_browsedfscontent.jsp\">Browse the filesystem</a></b><br>\n <% } %> \n <b><a href=\"/logs/\"><%=namenodeRole%> Logs</a></b><br>",
                "raw_url": "https://github.com/apache/hadoop/raw/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfsnodelist.jsp",
                "sha": "3bb349860384cc7e65ad5c2032488da6d56ac700",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop/blob/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java?ref=fc5bd930df793c3fab2964bb9ea2d0e9d412e493",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java",
                "patch": "@@ -25,12 +25,15 @@\n import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.util.List;\n import java.util.regex.Pattern;\n \n+import javax.servlet.ServletContext;\n import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n import javax.servlet.jsp.JspWriter;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -45,6 +48,7 @@\n import org.junit.Before;\n import org.junit.Test;\n import org.mockito.ArgumentCaptor;\n+import org.znerd.xmlenc.XMLOutputter;\n \n public class TestNameNodeJspHelper {\n \n@@ -117,6 +121,75 @@ public void testGetRollingUpgradeText() {\n     Assert.assertEquals(\"\", NamenodeJspHelper.getRollingUpgradeText(null));\n   }\n \n+  /**\n+   * Tests for non-null, non-empty NameNode label.\n+   */\n+  @Test\n+  public void testGetNameNodeLabel() {\n+    String nameNodeLabel = NamenodeJspHelper.getNameNodeLabel(\n+      cluster.getNameNode());\n+    Assert.assertNotNull(nameNodeLabel);\n+    Assert.assertFalse(nameNodeLabel.isEmpty());\n+  }\n+\n+  /**\n+   * Tests for non-null, non-empty NameNode label when called before\n+   * initialization of the NameNode RPC server.\n+   */\n+  @Test\n+  public void testGetNameNodeLabelNullRpcServer() {\n+    NameNode nn = mock(NameNode.class);\n+    when(nn.getRpcServer()).thenReturn(null);\n+    String nameNodeLabel = NamenodeJspHelper.getNameNodeLabel(\n+      cluster.getNameNode());\n+    Assert.assertNotNull(nameNodeLabel);\n+    Assert.assertFalse(nameNodeLabel.isEmpty());\n+  }\n+\n+  /**\n+   * Tests that passing a null FSNamesystem to generateSnapshotReport does not\n+   * throw NullPointerException.\n+   */\n+  @Test\n+  public void testGenerateSnapshotReportNullNamesystem() throws Exception {\n+    NamenodeJspHelper.generateSnapshotReport(mock(JspWriter.class), null);\n+  }\n+\n+  /**\n+   * Tests that redirectToRandomDataNode does not throw NullPointerException if\n+   * it finds a null FSNamesystem.\n+   */\n+  @Test(expected=IOException.class)\n+  public void testRedirectToRandomDataNodeNullNamesystem() throws Exception {\n+    NameNode nn = mock(NameNode.class);\n+    when(nn.getNamesystem()).thenReturn(null);\n+    ServletContext context = mock(ServletContext.class);\n+    when(context.getAttribute(\"name.node\")).thenReturn(nn);\n+    NamenodeJspHelper.redirectToRandomDataNode(context,\n+      mock(HttpServletRequest.class), mock(HttpServletResponse.class));\n+  }\n+\n+  /**\n+   * Tests that XMLBlockInfo does not throw NullPointerException if it finds a\n+   * null FSNamesystem.\n+   */\n+  @Test\n+  public void testXMLBlockInfoNullNamesystem() throws IOException {\n+    XMLOutputter doc = new XMLOutputter(mock(JspWriter.class), \"UTF-8\");\n+    new NamenodeJspHelper.XMLBlockInfo(null, 1L).toXML(doc);\n+  }\n+\n+  /**\n+   * Tests that XMLCorruptBlockInfo does not throw NullPointerException if it\n+   * finds a null FSNamesystem.\n+   */\n+  @Test\n+  public void testXMLCorruptBlockInfoNullNamesystem() throws IOException {\n+    XMLOutputter doc = new XMLOutputter(mock(JspWriter.class), \"UTF-8\");\n+    new NamenodeJspHelper.XMLCorruptBlockInfo(null, mock(Configuration.class),\n+      10, 1L).toXML(doc);\n+  }\n+\n   /**\n    * Checks if the list contains any string that partially matches the regex.\n    * ",
                "raw_url": "https://github.com/apache/hadoop/raw/fc5bd930df793c3fab2964bb9ea2d0e9d412e493/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeJspHelper.java",
                "sha": "3207f0ccb01b1464913802e40b2fe376a2561f31",
                "status": "modified"
            }
        ],
        "message": "HDFS-5279. Guard against NullPointerException in NameNode JSP pages before initialization of FSNamesystem. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1528308 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/703838c59841952757d941df4414aa77d57fe492",
        "patched_files": [
            "dfshealth.java",
            "dfsnodelist.java",
            "CHANGES.java",
            "corrupt_files.java",
            "NamenodeJspHelper.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeJspHelper.java"
        ]
    },
    "hadoop_fc77ed1": {
        "bug_id": "hadoop_fc77ed1",
        "commit": "https://github.com/apache/hadoop/commit/fc77ed153e70dcf2bf33f8207a022155141a15d2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/fc77ed153e70dcf2bf33f8207a022155141a15d2/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=fc77ed153e70dcf2bf33f8207a022155141a15d2",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -237,6 +237,9 @@ Release 2.4.0 - UNRELEASED\n     MAPREDUCE-5656. bzip2 codec can drop records when reading data in splits\n     (jlowe)\n \n+    MAPREDUCE-5623. TestJobCleanup fails because of RejectedExecutionException\n+    and NPE. (jlowe)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/fc77ed153e70dcf2bf33f8207a022155141a15d2/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "08e1c2cc21d8c609f4fd3c04943dbc45aa44a2cb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/fc77ed153e70dcf2bf33f8207a022155141a15d2/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java?ref=fc77ed153e70dcf2bf33f8207a022155141a15d2",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java",
                "patch": "@@ -195,8 +195,7 @@ private void testFailedJob(String fileName,\n     RunningJob job = jobClient.submitJob(jc);\n     JobID id = job.getID();\n     job.waitForCompletion();\n-    Counters counters = job.getCounters();\n-    assertTrue(\"No. of failed maps should be 1\",counters.getCounter(JobCounter.NUM_FAILED_MAPS) == 1);\n+    assertEquals(\"Job did not fail\", JobStatus.FAILED, job.getJobState());\n \n     if (fileName != null) {\n       Path testFile = new Path(outDir, fileName);\n@@ -242,9 +241,7 @@ private void testKilledJob(String fileName,\n     job.killJob(); // kill the job\n \n     job.waitForCompletion(); // wait for the job to complete\n-    \n-    counters = job.getCounters();\n-    assertTrue(\"No. of killed maps should be 1\", counters.getCounter(JobCounter.NUM_KILLED_MAPS) == 1);\n+    assertEquals(\"Job was not killed\", JobStatus.KILLED, job.getJobState());\n \n     if (fileName != null) {\n       Path testFile = new Path(outDir, fileName);",
                "raw_url": "https://github.com/apache/hadoop/raw/fc77ed153e70dcf2bf33f8207a022155141a15d2/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestJobCleanup.java",
                "sha": "bf762d93d92fbdb9c9f93aac7e3ebd9c2575ef01",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5623. TestJobCleanup fails because of RejectedExecutionException and NPE. Contributed by Jason Lowe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551285 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/6c641339d1a3cd0c8ffa6a9181a893ce8edf8bf5",
        "patched_files": [
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestJobCleanup.java"
        ]
    },
    "hadoop_fdf7b18": {
        "bug_id": "hadoop_fdf7b18",
        "commit": "https://github.com/apache/hadoop/commit/fdf7b182475050aaf67765eb53aaf342ebaebe8b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=fdf7b182475050aaf67765eb53aaf342ebaebe8b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -131,3 +131,5 @@ HDFS-2804. Should not mark blocks under-replicated when exiting safemode (todd)\n HDFS-2807. Service level authorizartion for HAServiceProtocol. (jitendra)\n \n HDFS-2809. Add test to verify that delegation tokens are honored after failover. (jitendra and atm)\n+\n+HDFS-2838. NPE in FSNamesystem when in safe mode. (Gregory Chanan via eli)",
                "raw_url": "https://github.com/apache/hadoop/raw/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "c8a760336a9c9c95f075c4e2d894747b31d6207e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=fdf7b182475050aaf67765eb53aaf342ebaebe8b",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3623,11 +3623,10 @@ private void doConsistencyCheck() {\n       assert assertsOn = true; // set to true if asserts are on\n       if (!assertsOn) return;\n       \n-      \n-      int activeBlocks = blockManager.getActiveBlockCount();\n       if (blockTotal == -1 && blockSafe == -1) {\n         return; // manual safe mode\n       }\n+      int activeBlocks = blockManager.getActiveBlockCount();\n       if ((blockTotal != activeBlocks) &&\n           !(blockSafe >= 0 && blockSafe <= blockTotal)) {\n         throw new AssertionError(",
                "raw_url": "https://github.com/apache/hadoop/raw/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "b3b3dbdaf31bcb28f1c91d0cbdd4949a93c8f0fa",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java?ref=fdf7b182475050aaf67765eb53aaf342ebaebe8b",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import junit.framework.Assert;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.protocol.FSConstants;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -37,6 +38,7 @@\n   private static final String CLUSTER_1 = \"cluster1\";\n   private static final String CLUSTER_2 = \"cluster2\";\n   private static final String CLUSTER_3 = \"cluster3\";\n+  private static final String CLUSTER_4 = \"cluster4\";\n   protected String testDataPath;\n   protected File testDataDir;\n   @Before\n@@ -104,5 +106,21 @@ public void testDualClusters() throws Throwable {\n     }\n   }\n \n-\n+  @Test(timeout=100000)\n+  public void testIsClusterUpAfterShutdown() throws Throwable {\n+    Configuration conf = new HdfsConfiguration();\n+    File testDataCluster4 = new File(testDataPath, CLUSTER_4);\n+    String c4Path = testDataCluster4.getAbsolutePath();\n+    conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, c4Path);\n+    MiniDFSCluster cluster4 = new MiniDFSCluster.Builder(conf).build();\n+    try {\n+      DistributedFileSystem dfs = (DistributedFileSystem) cluster4.getFileSystem();\n+      dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_ENTER);\n+      cluster4.shutdown();\n+    } finally {\n+      while(cluster4.isClusterUp()){\n+        Thread.sleep(1000);\n+      }  \n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "sha": "0eec0d187746184179ed9f814d59dbd059c390e0",
                "status": "modified"
            }
        ],
        "message": "HDFS-2838. NPE in FSNamesystem when in safe mode. Contributed by Gregory Chanan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1236450 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/1c95060a720680aa1dfb14e08603aa5bb405ba65",
        "patched_files": [
            "MiniDFSCluster.java",
            "CHANGES.java",
            "FSNamesystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestMiniDFSCluster.java"
        ]
    },
    "hadoop_fe35103": {
        "bug_id": "hadoop_fe35103",
        "commit": "https://github.com/apache/hadoop/commit/fe35103591ece0209f8345aba5544313e45a073c",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/fe35103591ece0209f8345aba5544313e45a073c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java?ref=fe35103591ece0209f8345aba5544313e45a073c",
                "deletions": 18,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "patch": "@@ -585,33 +585,38 @@ public void setCheckpointID(TaskID taskId, TaskCheckpointID cid) {\n   private void coalesceStatusUpdate(TaskAttemptId yarnAttemptID,\n       TaskAttemptStatus taskAttemptStatus,\n       AtomicReference<TaskAttemptStatus> lastStatusRef) {\n-    boolean asyncUpdatedNeeded = false;\n-    TaskAttemptStatus lastStatus = lastStatusRef.get();\n-\n-    if (lastStatus == null) {\n-      lastStatusRef.set(taskAttemptStatus);\n-      asyncUpdatedNeeded = true;\n-    } else {\n-      List<TaskAttemptId> oldFetchFailedMaps =\n-          taskAttemptStatus.fetchFailedMaps;\n-\n-      // merge fetchFailedMaps from the previous update\n-      if (lastStatus.fetchFailedMaps != null) {\n+    List<TaskAttemptId> fetchFailedMaps = taskAttemptStatus.fetchFailedMaps;\n+    TaskAttemptStatus lastStatus = null;\n+    boolean done = false;\n+    while (!done) {\n+      lastStatus = lastStatusRef.get();\n+      if (lastStatus != null && lastStatus.fetchFailedMaps != null) {\n+        // merge fetchFailedMaps from the previous update\n         if (taskAttemptStatus.fetchFailedMaps == null) {\n           taskAttemptStatus.fetchFailedMaps = lastStatus.fetchFailedMaps;\n         } else {\n-          taskAttemptStatus.fetchFailedMaps.addAll(lastStatus.fetchFailedMaps);\n+          taskAttemptStatus.fetchFailedMaps =\n+              new ArrayList<>(lastStatus.fetchFailedMaps.size() +\n+                  fetchFailedMaps.size());\n+          taskAttemptStatus.fetchFailedMaps.addAll(\n+              lastStatus.fetchFailedMaps);\n+          taskAttemptStatus.fetchFailedMaps.addAll(\n+              fetchFailedMaps);\n         }\n       }\n \n-      if (!lastStatusRef.compareAndSet(lastStatus, taskAttemptStatus)) {\n-        // update failed - async dispatcher has processed it in the meantime\n-        taskAttemptStatus.fetchFailedMaps = oldFetchFailedMaps;\n-        lastStatusRef.set(taskAttemptStatus);\n-        asyncUpdatedNeeded = true;\n+      // lastStatusRef may be changed by either the AsyncDispatcher when\n+      // it processes the update, or by another IPC server handler\n+      done = lastStatusRef.compareAndSet(lastStatus, taskAttemptStatus);\n+      if (!done) {\n+        LOG.info(\"TaskAttempt \" + yarnAttemptID +\n+            \": lastStatusRef changed by another thread, retrying...\");\n+        // let's revert taskAttemptStatus.fetchFailedMaps\n+        taskAttemptStatus.fetchFailedMaps = fetchFailedMaps;\n       }\n     }\n \n+    boolean asyncUpdatedNeeded = (lastStatus == null);\n     if (asyncUpdatedNeeded) {\n       context.getEventHandler().handle(\n           new TaskAttemptStatusUpdateEvent(taskAttemptStatus.id,",
                "raw_url": "https://github.com/apache/hadoop/raw/fe35103591ece0209f8345aba5544313e45a073c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "sha": "556c90c4412173dbb9f9975be9ed53c29ef6dd77",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-7028. Concurrent task progress updates causing NPE in Application Master. Contributed by Gergo Repas",
        "parent": "https://github.com/apache/hadoop/commit/c9bf813c9a6c018d14f2bef49ba086ec0e60c761",
        "patched_files": [
            "TaskAttemptListenerImpl.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskAttemptListenerImpl.java"
        ]
    },
    "hadoop_fef596d": {
        "bug_id": "hadoop_fef596d",
        "commit": "https://github.com/apache/hadoop/commit/fef596df038112cbbc86c4dc49314e274fca0190",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=fef596df038112cbbc86c4dc49314e274fca0190",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -497,6 +497,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-6666. Abort NameNode and DataNode startup if security is enabled but\n     block access token is not enabled. (Vijay Bhat via cnauroth)\n \n+    HDFS-8055. NullPointerException when topology script is missing.\n+    (Anu Engineer via cnauroth)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "274e9cbc726982dffd42fbe6b820f787c0b90899",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java?ref=fef596df038112cbbc86c4dc49314e274fca0190",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "patch": "@@ -372,9 +372,17 @@ public void sortLocatedBlocks(final String targethost,\n     if (client == null) {\n       List<String> hosts = new ArrayList<String> (1);\n       hosts.add(targethost);\n-      String rName = dnsToSwitchMapping.resolve(hosts).get(0);\n-      if (rName != null)\n-        client = new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n+      List<String> resolvedHosts = dnsToSwitchMapping.resolve(hosts);\n+      if (resolvedHosts != null && !resolvedHosts.isEmpty()) {\n+        String rName = resolvedHosts.get(0);\n+        if (rName != null) {\n+          client = new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n+            targethost);\n+        }\n+      } else {\n+        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n+          \"awareness scripts are functional.\");\n+      }\n     }\n     \n     Comparator<DatanodeInfo> comparator = avoidStaleDataNodesForRead ?",
                "raw_url": "https://github.com/apache/hadoop/raw/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "sha": "65c57471aa501aeb25b02bc1fa11ae6b984ef8a3",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/hadoop/blob/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeManager.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeManager.java?ref=fef596df038112cbbc86c4dc49314e274fca0190",
                "deletions": 20,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeManager.java",
                "patch": "@@ -19,6 +19,10 @@\n package org.apache.hadoop.hdfs.server.blockmanagement;\n \n import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.Iterator;\n@@ -31,6 +35,7 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.StorageType;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n@@ -40,10 +45,10 @@\n import org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage;\n import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.util.Shell;\n import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n-\n import static org.hamcrest.core.Is.is;\n import static org.junit.Assert.*;\n \n@@ -224,26 +229,74 @@ public void reloadCachedMappings(List<String> names) {\n    * with the storage ids and storage types.\n    */\n   @Test\n-  public void testSortLocatedBlocks() throws IOException {\n+  public void testSortLocatedBlocks() throws IOException, URISyntaxException {\n+    HelperFunction(null);\n+  }\n+\n+  /**\n+   * Execute a functional topology script and make sure that helper\n+   * function works correctly\n+   *\n+   * @throws IOException\n+   * @throws URISyntaxException\n+   */\n+  @Test\n+  public void testgoodScript() throws IOException, URISyntaxException {\n+    HelperFunction(\"/\" + Shell.appendScriptExtension(\"topology-script\"));\n+  }\n+\n+\n+  /**\n+   * Run a broken script and verify that helper function is able to\n+   * ignore the broken script and work correctly\n+   *\n+   * @throws IOException\n+   * @throws URISyntaxException\n+   */\n+  @Test\n+  public void testBadScript() throws IOException, URISyntaxException {\n+    HelperFunction(\"/\"+ Shell.appendScriptExtension(\"topology-broken-script\"));\n+  }\n+\n+\n+  /**\n+   * Helper function that tests the DatanodeManagers SortedBlock function\n+   * we invoke this function with and without topology scripts\n+   *\n+   * @param scriptFileName - Script Name or null\n+   *\n+   * @throws URISyntaxException\n+   * @throws IOException\n+   */\n+  public void HelperFunction(String scriptFileName)\n+    throws URISyntaxException, IOException {\n     // create the DatanodeManager which will be tested\n+    Configuration conf = new Configuration();\n     FSNamesystem fsn = Mockito.mock(FSNamesystem.class);\n     Mockito.when(fsn.hasWriteLock()).thenReturn(true);\n+    if (scriptFileName != null && !scriptFileName.isEmpty()) {\n+      URL shellScript = getClass().getResource(scriptFileName);\n+      Path resourcePath = Paths.get(shellScript.toURI());\n+      FileUtil.setExecutable(resourcePath.toFile(), true);\n+      conf.set(DFSConfigKeys.NET_TOPOLOGY_SCRIPT_FILE_NAME_KEY,\n+        resourcePath.toString());\n+    }\n     DatanodeManager dm = new DatanodeManager(Mockito.mock(BlockManager.class),\n-        fsn, new Configuration());\n+      fsn, conf);\n \n     // register 5 datanodes, each with different storage ID and type\n     DatanodeInfo[] locs = new DatanodeInfo[5];\n     String[] storageIDs = new String[5];\n     StorageType[] storageTypes = new StorageType[]{\n-        StorageType.ARCHIVE,\n-        StorageType.DEFAULT,\n-        StorageType.DISK,\n-        StorageType.RAM_DISK,\n-        StorageType.SSD\n+      StorageType.ARCHIVE,\n+      StorageType.DEFAULT,\n+      StorageType.DISK,\n+      StorageType.RAM_DISK,\n+      StorageType.SSD\n     };\n-    for(int i = 0; i < 5; i++) {\n+    for (int i = 0; i < 5; i++) {\n       // register new datanode\n-      String uuid = \"UUID-\"+i;\n+      String uuid = \"UUID-\" + i;\n       String ip = \"IP-\" + i;\n       DatanodeRegistration dr = Mockito.mock(DatanodeRegistration.class);\n       Mockito.when(dr.getDatanodeUuid()).thenReturn(uuid);\n@@ -255,7 +308,7 @@ public void testSortLocatedBlocks() throws IOException {\n \n       // get location and storage information\n       locs[i] = dm.getDatanode(uuid);\n-      storageIDs[i] = \"storageID-\"+i;\n+      storageIDs[i] = \"storageID-\" + i;\n     }\n \n     // set first 2 locations as decomissioned\n@@ -280,18 +333,19 @@ public void testSortLocatedBlocks() throws IOException {\n     assertThat(sortedLocs.length, is(5));\n     assertThat(storageIDs.length, is(5));\n     assertThat(storageTypes.length, is(5));\n-    for(int i = 0; i < sortedLocs.length; i++) {\n-      assertThat(((DatanodeInfoWithStorage)sortedLocs[i]).getStorageID(), is(storageIDs[i]));\n-      assertThat(((DatanodeInfoWithStorage)sortedLocs[i]).getStorageType(), is(storageTypes[i]));\n+    for (int i = 0; i < sortedLocs.length; i++) {\n+      assertThat(((DatanodeInfoWithStorage) sortedLocs[i]).getStorageID(),\n+        is(storageIDs[i]));\n+      assertThat(((DatanodeInfoWithStorage) sortedLocs[i]).getStorageType(),\n+        is(storageTypes[i]));\n     }\n-\n     // Ensure the local node is first.\n     assertThat(sortedLocs[0].getIpAddr(), is(targetIp));\n-\n     // Ensure the two decommissioned DNs were moved to the end.\n-    assertThat(sortedLocs[sortedLocs.length-1].getAdminState(),\n-        is(DatanodeInfo.AdminStates.DECOMMISSIONED));\n-    assertThat(sortedLocs[sortedLocs.length-2].getAdminState(),\n-        is(DatanodeInfo.AdminStates.DECOMMISSIONED));\n+    assertThat(sortedLocs[sortedLocs.length - 1].getAdminState(),\n+      is(DatanodeInfo.AdminStates.DECOMMISSIONED));\n+    assertThat(sortedLocs[sortedLocs.length - 2].getAdminState(),\n+      is(DatanodeInfo.AdminStates.DECOMMISSIONED));\n   }\n }\n+",
                "raw_url": "https://github.com/apache/hadoop/raw/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestDatanodeManager.java",
                "sha": "bf167a58ba5d52b16527b780483007c9c8ed5504",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.cmd",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.cmd?ref=fef596df038112cbbc86c4dc49314e274fca0190",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.cmd",
                "patch": "@@ -0,0 +1,22 @@\n+@echo off\n+@rem Licensed to the Apache Software Foundation (ASF) under one or more\n+@rem contributor license agreements.  See the NOTICE file distributed with\n+@rem this work for additional information regarding copyright ownership.\n+@rem The ASF licenses this file to You under the Apache License, Version 2.0\n+@rem (the \"License\"); you may not use this file except in compliance with\n+@rem the License.  You may obtain a copy of the License at\n+@rem\n+@rem     http://www.apache.org/licenses/LICENSE-2.0\n+@rem\n+@rem Unless required by applicable law or agreed to in writing, software\n+@rem distributed under the License is distributed on an \"AS IS\" BASIS,\n+@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+@rem See the License for the specific language governing permissions and\n+@rem limitations under the License.\n+@rem\n+\n+@rem yes, this is a broken script, please don't fix this.\n+@rem This is used in a test case to verify that we can handle broken\n+@rem topology scripts.\n+\n+exit 1",
                "raw_url": "https://github.com/apache/hadoop/raw/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.cmd",
                "sha": "ec4f4ca5c1ff1be86ad80e5213ddcd9ff6e41c49",
                "status": "added"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.sh",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.sh?ref=fef596df038112cbbc86c4dc49314e274fca0190",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.sh",
                "patch": "@@ -0,0 +1,23 @@\n+#!/usr/bin/env bash\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+## yes, this is a broken script, please don't fix this.\n+## This is used in a test case to verify that we can handle broken\n+## topology scripts.\n+\n+exit 1\n+",
                "raw_url": "https://github.com/apache/hadoop/raw/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-broken-script.sh",
                "sha": "8e5cf00587d8ce90b73bd69792bac8bf0688a8d1",
                "status": "added"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.cmd",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.cmd?ref=fef596df038112cbbc86c4dc49314e274fca0190",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.cmd",
                "patch": "@@ -0,0 +1,18 @@\n+@echo off\n+@rem Licensed to the Apache Software Foundation (ASF) under one or more\n+@rem contributor license agreements.  See the NOTICE file distributed with\n+@rem this work for additional information regarding copyright ownership.\n+@rem The ASF licenses this file to You under the Apache License, Version 2.0\n+@rem (the \"License\"); you may not use this file except in compliance with\n+@rem the License.  You may obtain a copy of the License at\n+@rem\n+@rem     http://www.apache.org/licenses/LICENSE-2.0\n+@rem\n+@rem Unless required by applicable law or agreed to in writing, software\n+@rem distributed under the License is distributed on an \"AS IS\" BASIS,\n+@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+@rem See the License for the specific language governing permissions and\n+@rem limitations under the License.\n+@rem\n+\n+for /F \"delims=- tokens=2\" %%A in (\"%1\") do echo /rackID-%%A",
                "raw_url": "https://github.com/apache/hadoop/raw/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.cmd",
                "sha": "145df47ef7a2f2a1294b1cfa0896072d3358f8ea",
                "status": "added"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hadoop/blob/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.sh",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.sh?ref=fef596df038112cbbc86c4dc49314e274fca0190",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.sh",
                "patch": "@@ -0,0 +1,21 @@\n+#!/usr/bin/env bash\n+\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+echo $1 | awk -F'-' '{printf(\"/rackID-%s\",$2)}'\n+",
                "raw_url": "https://github.com/apache/hadoop/raw/fef596df038112cbbc86c4dc49314e274fca0190/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/topology-script.sh",
                "sha": "2a308e72c77191bbfb604d384479233025fa2648",
                "status": "added"
            }
        ],
        "message": "HDFS-8055. NullPointerException when topology script is missing. Contributed by Anu Engineer.",
        "parent": "https://github.com/apache/hadoop/commit/d45aa7647b1fecf81860ec7b563085be2af99a0b",
        "patched_files": [
            "topology-broken-script.java",
            "topology-script.java",
            "CHANGES.java",
            "DatanodeManager.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestDatanodeManager.java"
        ]
    },
    "hadoop_ff05110": {
        "bug_id": "hadoop_ff05110",
        "commit": "https://github.com/apache/hadoop/commit/ff0511019cd75a113a10940ef09b0d141a77c91a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ff0511019cd75a113a10940ef09b0d141a77c91a/common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/CHANGES.txt?ref=ff0511019cd75a113a10940ef09b0d141a77c91a",
                "deletions": 0,
                "filename": "common/CHANGES.txt",
                "patch": "@@ -246,6 +246,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    HADOOP-7327. FileSystem.listStatus() throws NullPointerException instead of\n+    IOException upon access permission failure. (mattf)\n+\n     HADOOP-7015. RawLocalFileSystem#listStatus does not deal with a directory\n     whose entries are changing (e.g. in a multi-thread or multi-process\n     environment). (Sanjay Radia via eli)",
                "raw_url": "https://github.com/apache/hadoop/raw/ff0511019cd75a113a10940ef09b0d141a77c91a/common/CHANGES.txt",
                "sha": "a74fa5daddc6d721f84d53e0b8e918efe0ad4df4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ff0511019cd75a113a10940ef09b0d141a77c91a/common/src/java/org/apache/hadoop/fs/FileSystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/java/org/apache/hadoop/fs/FileSystem.java?ref=ff0511019cd75a113a10940ef09b0d141a77c91a",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/fs/FileSystem.java",
                "patch": "@@ -1151,6 +1151,9 @@ public boolean accept(Path file) {\n   private void listStatus(ArrayList<FileStatus> results, Path f,\n       PathFilter filter) throws FileNotFoundException, IOException {\n     FileStatus listing[] = listStatus(f);\n+    if (listing == null) {\n+      throw new IOException(\"Error accessing \" + f);\n+    }\n \n     for (int i = 0; i < listing.length; i++) {\n       if (filter.accept(listing[i].getPath())) {",
                "raw_url": "https://github.com/apache/hadoop/raw/ff0511019cd75a113a10940ef09b0d141a77c91a/common/src/java/org/apache/hadoop/fs/FileSystem.java",
                "sha": "63954910962f851b19c9b06026dcdca22d0b7856",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/ff0511019cd75a113a10940ef09b0d141a77c91a/common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java?ref=ff0511019cd75a113a10940ef09b0d141a77c91a",
                "deletions": 4,
                "filename": "common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java",
                "patch": "@@ -25,6 +25,7 @@\n \n \n import org.apache.hadoop.fs.Options.Rename;\n+import org.apache.hadoop.fs.permission.FsPermission;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -252,8 +253,9 @@ public void testGetFileStatusThrowsExceptionForNonExistentFile()\n     }\n   } \n   \n+  @Test\n   public void testListStatusThrowsExceptionForNonExistentFile()\n-                                                    throws Exception {\n+  throws Exception {\n     try {\n       fSys.listStatus(getTestRootPath(fSys, \"test/hadoop/file\"));\n       Assert.fail(\"Should throw FileNotFoundException\");\n@@ -262,6 +264,27 @@ public void testListStatusThrowsExceptionForNonExistentFile()\n     }\n   }\n   \n+  // TODO: update after fixing HADOOP-7352\n+  @Test\n+  public void testListStatusThrowsExceptionForUnreadableDir()\n+  throws Exception {\n+    Path testRootDir = getTestRootPath(fSys, \"test/hadoop/dir\");\n+    Path obscuredDir = new Path(testRootDir, \"foo\");\n+    Path subDir = new Path(obscuredDir, \"bar\"); //so foo is non-empty\n+    fSys.mkdirs(subDir);\n+    fSys.setPermission(obscuredDir, new FsPermission((short)0)); //no access\n+    try {\n+      fSys.listStatus(obscuredDir);\n+      Assert.fail(\"Should throw IOException\");\n+    } catch (IOException ioe) {\n+      // expected\n+    } finally {\n+      // make sure the test directory can be deleted\n+      fSys.setPermission(obscuredDir, new FsPermission((short)0755)); //default\n+    }\n+  }\n+\n+\n   @Test\n   public void testListStatus() throws Exception {\n     Path[] testDirs = {\n@@ -315,6 +338,7 @@ public void testListStatusFilterWithNoMatches() throws Exception {\n     \n   }\n   \n+  @Test\n   public void testListStatusFilterWithSomeMatches() throws Exception {\n     Path[] testDirs = {\n         getTestRootPath(fSys, TEST_DIR_AAA),\n@@ -919,12 +943,13 @@ public void testRenameDirectoryToNonExistentParent() throws Exception {\n \n   @Test\n   public void testRenameDirectoryAsNonExistentDirectory() throws Exception {\n-    testRenameDirectoryAsNonExistentDirectory(Rename.NONE);\n+    doTestRenameDirectoryAsNonExistentDirectory(Rename.NONE);\n     tearDown();\n-    testRenameDirectoryAsNonExistentDirectory(Rename.OVERWRITE);\n+    doTestRenameDirectoryAsNonExistentDirectory(Rename.OVERWRITE);\n   }\n \n-  private void testRenameDirectoryAsNonExistentDirectory(Rename... options) throws Exception {\n+  private void doTestRenameDirectoryAsNonExistentDirectory(Rename... options) \n+  throws Exception {\n     if (!renameSupported()) return;\n     \n     Path src = getTestRootPath(fSys, \"test/hadoop/dir\");",
                "raw_url": "https://github.com/apache/hadoop/raw/ff0511019cd75a113a10940ef09b0d141a77c91a/common/src/test/core/org/apache/hadoop/fs/FSMainOperationsBaseTest.java",
                "sha": "6a9079c8ecef1b6fd7cdc9f6a504bb9ce3906f69",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7327. FileSystem.listStatus() throws NullPointerException instead of IOException upon access permission failure. Contributed by Matt Foley.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1143491 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/99f0b7d8cd01ce4fe9753049dca83009aa6df61d",
        "patched_files": [
            "CHANGES.java",
            "FileSystem.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "FSMainOperationsBaseTest.java",
            "TestFileSystem.java"
        ]
    },
    "hadoop_ff77582": {
        "bug_id": "hadoop_ff77582",
        "commit": "https://github.com/apache/hadoop/commit/ff7758299151e3b69c27314010b4ef3a9fda3b41",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=ff7758299151e3b69c27314010b4ef3a9fda3b41",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -88,6 +88,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2321. NodeManager web UI can incorrectly report Pmem enforcement\n     (Leitao Guo via jlowe)\n \n+    YARN-2273. NPE in ContinuousScheduling thread when we lose a node. \n+    (Wei Yan via kasha)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/CHANGES.txt",
                "sha": "2e69a756c6415598c7f26f304b969407987ee68e",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hadoop/blob/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=ff7758299151e3b69c27314010b4ef3a9fda3b41",
                "deletions": 30,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -970,37 +970,27 @@ private synchronized void nodeUpdate(RMNode nm) {\n     }\n   }\n \n-  private void continuousScheduling() {\n-    while (true) {\n-      List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n-      // Sort the nodes by space available on them, so that we offer\n-      // containers on emptier nodes first, facilitating an even spread. This\n-      // requires holding the scheduler lock, so that the space available on a\n-      // node doesn't change during the sort.\n-      synchronized (this) {\n-        Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n-      }\n+  void continuousSchedulingAttempt() {\n+    List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n+    // Sort the nodes by space available on them, so that we offer\n+    // containers on emptier nodes first, facilitating an even spread. This\n+    // requires holding the scheduler lock, so that the space available on a\n+    // node doesn't change during the sort.\n+    synchronized (this) {\n+      Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n+    }\n \n-      // iterate all nodes\n-      for (NodeId nodeId : nodeIdList) {\n-        if (nodes.containsKey(nodeId)) {\n-          FSSchedulerNode node = getFSSchedulerNode(nodeId);\n-          try {\n-            if (Resources.fitsIn(minimumAllocation,\n-                    node.getAvailableResource())) {\n-              attemptScheduling(node);\n-            }\n-          } catch (Throwable ex) {\n-            LOG.warn(\"Error while attempting scheduling for node \" + node +\n-                    \": \" + ex.toString(), ex);\n-          }\n-        }\n-      }\n+    // iterate all nodes\n+    for (NodeId nodeId : nodeIdList) {\n+      FSSchedulerNode node = getFSSchedulerNode(nodeId);\n       try {\n-        Thread.sleep(getContinuousSchedulingSleepMs());\n-      } catch (InterruptedException e) {\n-        LOG.warn(\"Error while doing sleep in continuous scheduling: \" +\n-                e.toString(), e);\n+        if (node != null && Resources.fitsIn(minimumAllocation,\n+            node.getAvailableResource())) {\n+          attemptScheduling(node);\n+        }\n+      } catch (Throwable ex) {\n+        LOG.error(\"Error while attempting scheduling for node \" + node +\n+            \": \" + ex.toString(), ex);\n       }\n     }\n   }\n@@ -1010,6 +1000,12 @@ private void continuousScheduling() {\n \n     @Override\n     public int compare(NodeId n1, NodeId n2) {\n+      if (!nodes.containsKey(n1)) {\n+        return 1;\n+      }\n+      if (!nodes.containsKey(n2)) {\n+        return -1;\n+      }\n       return RESOURCE_CALCULATOR.compare(clusterResource,\n               nodes.get(n2).getAvailableResource(),\n               nodes.get(n1).getAvailableResource());\n@@ -1234,7 +1230,16 @@ private synchronized void initScheduler(Configuration conf)\n           new Runnable() {\n             @Override\n             public void run() {\n-              continuousScheduling();\n+              while (!Thread.currentThread().isInterrupted()) {\n+                try {\n+                  continuousSchedulingAttempt();\n+                  Thread.sleep(getContinuousSchedulingSleepMs());\n+                } catch (InterruptedException e) {\n+                  LOG.error(\"Continuous scheduling thread interrupted. Exiting. \",\n+                      e);\n+                  return;\n+                }\n+              }\n             }\n           }\n       );",
                "raw_url": "https://github.com/apache/hadoop/raw/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "c0687bcbc249c470465c73ebeda16c64c623b9c6",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop/blob/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=ff7758299151e3b69c27314010b4ef3a9fda3b41",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -2763,7 +2763,43 @@ public void testContinuousScheduling() throws Exception {\n     Assert.assertEquals(2, nodes.size());\n   }\n \n-  \n+  @Test\n+  public void testContinuousSchedulingWithNodeRemoved() throws Exception {\n+    // Disable continuous scheduling, will invoke continuous scheduling once manually\n+    scheduler.init(conf);\n+    scheduler.start();\n+    Assert.assertTrue(\"Continuous scheduling should be disabled.\",\n+        !scheduler.isContinuousSchedulingEnabled());\n+\n+    // Add two nodes\n+    RMNode node1 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 1,\n+            \"127.0.0.1\");\n+    NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);\n+    scheduler.handle(nodeEvent1);\n+    RMNode node2 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 2,\n+            \"127.0.0.2\");\n+    NodeAddedSchedulerEvent nodeEvent2 = new NodeAddedSchedulerEvent(node2);\n+    scheduler.handle(nodeEvent2);\n+    Assert.assertEquals(\"We should have two alive nodes.\",\n+        2, scheduler.getNumClusterNodes());\n+\n+    // Remove one node\n+    NodeRemovedSchedulerEvent removeNode1 = new NodeRemovedSchedulerEvent(node1);\n+    scheduler.handle(removeNode1);\n+    Assert.assertEquals(\"We should only have one alive node.\",\n+        1, scheduler.getNumClusterNodes());\n+\n+    // Invoke the continuous scheduling once\n+    try {\n+      scheduler.continuousSchedulingAttempt();\n+    } catch (Exception e) {\n+      fail(\"Exception happened when doing continuous scheduling. \" +\n+        e.toString());\n+    }\n+  }\n+\n   @Test\n   public void testDontAllowUndeclaredPools() throws Exception{\n     conf.setBoolean(FairSchedulerConfiguration.ALLOW_UNDECLARED_POOLS, false);",
                "raw_url": "https://github.com/apache/hadoop/raw/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "df157e75001fd51b7687096383deb70b4eb25d4b",
                "status": "modified"
            }
        ],
        "message": "YARN-2273. NPE in ContinuousScheduling thread when we lose a node. (Wei Yan via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612720 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/853ed29f2df417878f896a30b68f7412baaa6bf8",
        "patched_files": [
            "FairScheduler.java",
            "CHANGES.java"
        ],
        "repo": "hadoop",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    }
}