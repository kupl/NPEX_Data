{
    "hadoop-ozone_245d335": {
        "repo": "hadoop-ozone",
        "message": "HDDS-2853. NPE in OzoneContainer Start. (#419)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/245d3358757eab7beb155a121dbcf61d89c7e7f1",
        "parent": "https://github.com/apache/hadoop-ozone/commit/34683e4bdad4a471787d3376d0a0cc2b17bc75f5",
        "bug_id": "hadoop-ozone_245d335",
        "file": [
            {
                "sha": "5229ae84144e08640e88f95c9a72f32bb4cb72aa",
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/245d3358757eab7beb155a121dbcf61d89c7e7f1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/245d3358757eab7beb155a121dbcf61d89c7e7f1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java",
                "status": "modified",
                "changes": 4,
                "additions": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/DatanodeStateMachine.java?ref=245d3358757eab7beb155a121dbcf61d89c7e7f1",
                "patch": "@@ -381,7 +381,9 @@ public void startDaemon() {\n    * be sent by datanode.\n    */\n   public void triggerHeartbeat() {\n-    stateMachineThread.interrupt();\n+    if (stateMachineThread != null) {\n+      stateMachineThread.interrupt();\n+    }\n   }\n \n   /**",
                "deletions": 1
            }
        ],
        "patched_files": [
            "DatanodeStateMachine.java"
        ],
        "unit_tests": [
            "TestDatanodeStateMachine.java"
        ]
    },
    "hadoop-ozone_9b72208": {
        "repo": "hadoop-ozone",
        "message": "HDDS-2630. NullPointerException in S3g. (#336)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/9b7220857129ba89b1a5ada01406b05d49430b9d",
        "parent": "https://github.com/apache/hadoop-ozone/commit/2a91bb64bf0c31ca889dac707ac2162f9ad95d15",
        "bug_id": "hadoop-ozone_9b72208",
        "file": [
            {
                "sha": "500eaeaa626eb292c6adc445c710ada1f8ab040b",
                "filename": "hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java",
                "status": "modified",
                "changes": 4,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/header/AuthenticationHeaderParser.java?ref=9b7220857129ba89b1a5ada01406b05d49430b9d",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.ozone.s3.header;\n \n import org.apache.hadoop.ozone.s3.exception.OS3Exception;\n+import org.apache.hadoop.ozone.s3.exception.S3ErrorTable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -37,6 +38,9 @@\n   private String accessKeyID;\n \n   public void parse() throws OS3Exception {\n+    if (authHeader == null) {\n+      throw S3ErrorTable.MALFORMED_HEADER;\n+    }\n     if (authHeader.startsWith(\"AWS4\")) {\n       LOG.debug(\"V4 Header {}\", authHeader);\n       AuthorizationHeaderV4 authorizationHeader = new AuthorizationHeaderV4(",
                "deletions": 0
            },
            {
                "sha": "86c040438fec4ddc2fd29d6b1d058b1e9a55f5df",
                "filename": "hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/9b7220857129ba89b1a5ada01406b05d49430b9d/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java",
                "status": "added",
                "changes": 114,
                "additions": 114,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/endpoint/TestBucketPut.java?ref=9b7220857129ba89b1a5ada01406b05d49430b9d",
                "patch": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.ozone.s3.endpoint;\n+\n+import javax.ws.rs.core.Response;\n+\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.client.OzoneClientStub;\n+\n+import org.apache.hadoop.ozone.s3.exception.OS3Exception;\n+import org.apache.hadoop.ozone.s3.header.AuthenticationHeaderParser;\n+\n+import static java.net.HttpURLConnection.HTTP_NOT_FOUND;\n+import static org.apache.hadoop.ozone.s3.AWSV4AuthParser.DATE_FORMATTER;\n+import static org.apache.hadoop.ozone.s3.exception.S3ErrorTable.MALFORMED_HEADER;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.time.LocalDate;\n+\n+/**\n+ * This class test Create Bucket functionality.\n+ */\n+public class TestBucketPut {\n+\n+  private String bucketName = OzoneConsts.BUCKET;\n+  private OzoneClientStub clientStub;\n+  private BucketEndpoint bucketEndpoint;\n+\n+  @Before\n+  public void setup() throws Exception {\n+\n+    //Create client stub and object store stub.\n+    clientStub = new OzoneClientStub();\n+\n+    // Create HeadBucket and setClient to OzoneClientStub\n+    bucketEndpoint = new BucketEndpoint();\n+    bucketEndpoint.setClient(clientStub);\n+  }\n+\n+  @Test\n+  public void testBucketFailWithAuthHeaderMissing() throws Exception {\n+\n+    bucketEndpoint.setAuthenticationHeaderParser(\n+        new AuthenticationHeaderParser());\n+    bucketEndpoint.getAuthenticationHeaderParser().setAuthHeader(null);\n+    try {\n+      bucketEndpoint.put(bucketName, null);\n+    } catch (OS3Exception ex) {\n+      Assert.assertEquals(HTTP_NOT_FOUND, ex.getHttpCode());\n+      Assert.assertEquals(MALFORMED_HEADER.getCode(), ex.getCode());\n+    }\n+  }\n+\n+  @Test\n+  public void testBucketPut() throws Exception {\n+    String auth = generateAuthHeader();\n+    bucketEndpoint.setAuthenticationHeaderParser(\n+        new AuthenticationHeaderParser());\n+    bucketEndpoint.getAuthenticationHeaderParser().setAuthHeader(auth);\n+    Response response = bucketEndpoint.put(bucketName, null);\n+    assertEquals(200, response.getStatus());\n+    assertNotNull(response.getLocation());\n+  }\n+\n+  @Test\n+  public void testBucketFailWithInvalidHeader() throws Exception {\n+    bucketEndpoint.setAuthenticationHeaderParser(\n+        new AuthenticationHeaderParser());\n+    bucketEndpoint.getAuthenticationHeaderParser().setAuthHeader(\"auth\");\n+    try {\n+      bucketEndpoint.put(bucketName, null);\n+    } catch (OS3Exception ex) {\n+      Assert.assertEquals(HTTP_NOT_FOUND, ex.getHttpCode());\n+      Assert.assertEquals(MALFORMED_HEADER.getCode(), ex.getCode());\n+    }\n+  }\n+\n+  /**\n+   * Generate dummy auth header.\n+   * @return auth header.\n+   */\n+  private String generateAuthHeader() {\n+    LocalDate now = LocalDate.now();\n+    String curDate = DATE_FORMATTER.format(now);\n+    return  \"AWS4-HMAC-SHA256 \" +\n+        \"Credential=ozone/\" + curDate + \"/us-east-1/s3/aws4_request, \" +\n+        \"SignedHeaders=host;range;x-amz-date, \" +\n+        \"Signature=fe5f80f77d5fa3beca038a248ff027\";\n+  }\n+\n+}",
                "deletions": 0
            }
        ],
        "patched_files": [
            "AuthenticationHeaderParser.java"
        ],
        "unit_tests": [
            "TestBucketPut.java"
        ]
    },
    "hadoop-ozone_d3021fb": {
        "repo": "hadoop-ozone",
        "message": "HDDS-2347 XCeiverClientGrpc's parallel use leads to NPE (#81)",
        "commit": "https://github.com/apache/hadoop-ozone/commit/d3021fb2bc895b0dfa25b0d562e4a2664164532a",
        "parent": "https://github.com/apache/hadoop-ozone/commit/27b6042f0e99ebcfdbbe21ee905084be3aadd6b1",
        "bug_id": "hadoop-ozone_d3021fb",
        "file": [
            {
                "sha": "236370b2ab9eb24e5001529c61a70ceb105e0188",
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "status": "modified",
                "changes": 38,
                "additions": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java?ref=d3021fb2bc895b0dfa25b0d562e4a2664164532a",
                "patch": "@@ -152,8 +152,11 @@ public void connect(String encodedToken) throws Exception {\n     connectToDatanode(dn, encodedToken);\n   }\n \n-  private void connectToDatanode(DatanodeDetails dn, String encodedToken)\n-      throws IOException {\n+  private synchronized void connectToDatanode(DatanodeDetails dn,\n+      String encodedToken) throws IOException {\n+    if (isConnected(dn)){\n+      return;\n+    }\n     // read port from the data node, on failure use default configured\n     // port.\n     int port = dn.getPort(DatanodeDetails.Port.Name.STANDALONE).getValue();\n@@ -208,7 +211,7 @@ private boolean isConnected(ManagedChannel channel) {\n   }\n \n   @Override\n-  public void close() {\n+  public synchronized void close() {\n     closed = true;\n     for (ManagedChannel channel : channels.values()) {\n       channel.shutdownNow();\n@@ -397,19 +400,9 @@ public XceiverClientReply sendCommandAsync(\n \n   private XceiverClientReply sendCommandAsync(\n       ContainerCommandRequestProto request, DatanodeDetails dn)\n-      throws IOException, ExecutionException, InterruptedException {\n-    if (closed) {\n-      throw new IOException(\"This channel is not connected.\");\n-    }\n-\n+      throws IOException, InterruptedException {\n+    checkOpen(dn, request.getEncodedToken());\n     UUID dnId = dn.getUuid();\n-    ManagedChannel channel = channels.get(dnId);\n-    // If the channel doesn't exist for this specific datanode or the channel\n-    // is closed, just reconnect\n-    String token = request.getEncodedToken();\n-    if (!isConnected(channel)) {\n-      reconnect(dn, token);\n-    }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Send command {} to datanode {}\",\n           request.getCmdType().toString(), dn.getNetworkFullPath());\n@@ -456,6 +449,21 @@ public void onCompleted() {\n     return new XceiverClientReply(replyFuture);\n   }\n \n+  private synchronized void checkOpen(DatanodeDetails dn, String encodedToken)\n+      throws IOException{\n+    if (closed) {\n+      throw new IOException(\"This channel is not connected.\");\n+    }\n+\n+    ManagedChannel channel = channels.get(dn.getUuid());\n+    // If the channel doesn't exist for this specific datanode or the channel\n+    // is closed, just reconnect\n+    if (!isConnected(channel)) {\n+      reconnect(dn, encodedToken);\n+    }\n+\n+  }\n+\n   private void reconnect(DatanodeDetails dn, String encodedToken)\n       throws IOException {\n     ManagedChannel channel;",
                "deletions": 15
            },
            {
                "sha": "648d22dbb064107f82662d4dead7e6f1cb10e1c8",
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "status": "modified",
                "changes": 2,
                "additions": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java?ref=d3021fb2bc895b0dfa25b0d562e4a2664164532a",
                "patch": "@@ -231,7 +231,6 @@ public XceiverClientSpi call() throws Exception {\n             case RATIS:\n               client = XceiverClientRatis.newXceiverClientRatis(pipeline, conf,\n                   caCert);\n-              client.connect();\n               break;\n             case STAND_ALONE:\n               client = new XceiverClientGrpc(pipeline, conf, caCert);\n@@ -240,6 +239,7 @@ public XceiverClientSpi call() throws Exception {\n             default:\n               throw new IOException(\"not implemented\" + pipeline.getType());\n             }\n+            client.connect();\n             return client;\n           }\n         });",
                "deletions": 1
            }
        ],
        "patched_files": [
            "XceiverClientManager.java"
        ],
        "unit_tests": [
            "TestXceiverClientManager.java"
        ]
    },
    "hadoop-ozone_89bdb6a": {
        "repo": "hadoop-ozone",
        "message": "HDDS-2521. Multipart upload failing with NPE\n\nCloses #206",
        "commit": "https://github.com/apache/hadoop-ozone/commit/89bdb6a69e6314384e79f76e509cf99437877780",
        "parent": "https://github.com/apache/hadoop-ozone/commit/8af5ab8dacdea4e882491ea0d9ebe81806ebf9fa",
        "bug_id": "hadoop-ozone_89bdb6a",
        "file": [
            {
                "sha": "9cb0eb16a78c0539a7012a04d3d4a34afae7fabb",
                "filename": "hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java",
                "status": "modified",
                "changes": 141,
                "additions": 68,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/ObjectEndpoint.java?ref=89bdb6a69e6314384e79f76e509cf99437877780",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -141,23 +141,17 @@ public Response put(\n       String copyHeader = headers.getHeaderString(COPY_SOURCE_HEADER);\n       String storageType = headers.getHeaderString(STORAGE_CLASS_HEADER);\n \n-      ReplicationType replicationType;\n-      ReplicationFactor replicationFactor;\n+      S3StorageType s3StorageType;\n       boolean storageTypeDefault;\n       if (storageType == null || storageType.equals(\"\")) {\n-        replicationType = S3StorageType.getDefault().getType();\n-        replicationFactor = S3StorageType.getDefault().getFactor();\n+        s3StorageType = S3StorageType.getDefault();\n         storageTypeDefault = true;\n       } else {\n-        try {\n-          replicationType = S3StorageType.valueOf(storageType).getType();\n-          replicationFactor = S3StorageType.valueOf(storageType).getFactor();\n-        } catch (IllegalArgumentException ex) {\n-          throw S3ErrorTable.newError(S3ErrorTable.INVALID_ARGUMENT,\n-              storageType);\n-        }\n+        s3StorageType = toS3StorageType(storageType);\n         storageTypeDefault = false;\n       }\n+      ReplicationType replicationType = s3StorageType.getType();\n+      ReplicationFactor replicationFactor = s3StorageType.getFactor();\n \n       if (copyHeader != null) {\n         //Copy object, as copy source available.\n@@ -214,10 +208,7 @@ public Response get(\n \n       if (uploadId != null) {\n         // When we have uploadId, this is the request for list Parts.\n-        int partMarker = 0;\n-        if (partNumberMarker != null) {\n-          partMarker = Integer.parseInt(partNumberMarker);\n-        }\n+        int partMarker = parsePartNumberMarker(partNumberMarker);\n         return listParts(bucketName, keyPath, uploadId,\n             partMarker, maxParts);\n       }\n@@ -233,16 +224,12 @@ public Response get(\n       String rangeHeaderVal = headers.getHeaderString(RANGE_HEADER);\n       RangeHeader rangeHeader = null;\n \n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"range Header provided value is {}\", rangeHeaderVal);\n-      }\n+      LOG.debug(\"range Header provided value: {}\", rangeHeaderVal);\n \n       if (rangeHeaderVal != null) {\n         rangeHeader = RangeHeaderParserUtil.parseRangeHeader(rangeHeaderVal,\n             length);\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"range Header provided value is {}\", rangeHeader);\n-        }\n+        LOG.debug(\"range Header provided: {}\", rangeHeader);\n         if (rangeHeader.isInValidRange()) {\n           throw S3ErrorTable.newError(\n               S3ErrorTable.INVALID_RANGE, rangeHeaderVal);\n@@ -261,20 +248,13 @@ public Response get(\n             .header(CONTENT_LENGTH, keyDetails.getDataSize());\n \n       } else {\n-        LOG.debug(\"range Header provided value is {}\", rangeHeader);\n         OzoneInputStream key = bucket.readKey(keyPath);\n \n         long startOffset = rangeHeader.getStartOffset();\n         long endOffset = rangeHeader.getEndOffset();\n-        long copyLength;\n-        if (startOffset == endOffset) {\n-          // if range header is given as bytes=0-0, then we should return 1\n-          // byte from start offset\n-          copyLength = 1;\n-        } else {\n-          copyLength = rangeHeader.getEndOffset() - rangeHeader\n-              .getStartOffset() + 1;\n-        }\n+        // eg. if range header is given as bytes=0-0, then we should return 1\n+        // byte from start offset\n+        long copyLength = endOffset - startOffset + 1;\n         StreamingOutput output = dest -> {\n           try (S3WrapperInputStream s3WrapperInputStream =\n               new S3WrapperInputStream(\n@@ -334,7 +314,8 @@ private void addLastModifiedDate(\n   @HEAD\n   public Response head(\n       @PathParam(\"bucket\") String bucketName,\n-      @PathParam(\"path\") String keyPath) throws Exception {\n+      @PathParam(\"path\") String keyPath) throws IOException, OS3Exception {\n+\n     OzoneKeyDetails key;\n \n     try {\n@@ -441,20 +422,14 @@ public Response initializeMultipartUpload(\n       OzoneBucket ozoneBucket = getBucket(bucket);\n       String storageType = headers.getHeaderString(STORAGE_CLASS_HEADER);\n \n-      ReplicationType replicationType;\n-      ReplicationFactor replicationFactor;\n+      S3StorageType s3StorageType;\n       if (storageType == null || storageType.equals(\"\")) {\n-        replicationType = S3StorageType.getDefault().getType();\n-        replicationFactor = S3StorageType.getDefault().getFactor();\n+        s3StorageType = S3StorageType.getDefault();\n       } else {\n-        try {\n-          replicationType = S3StorageType.valueOf(storageType).getType();\n-          replicationFactor = S3StorageType.valueOf(storageType).getFactor();\n-        } catch (IllegalArgumentException ex) {\n-          throw S3ErrorTable.newError(S3ErrorTable.INVALID_ARGUMENT,\n-              storageType);\n-        }\n+        s3StorageType = toS3StorageType(storageType);\n       }\n+      ReplicationType replicationType = s3StorageType.getType();\n+      ReplicationFactor replicationFactor = s3StorageType.getFactor();\n \n       OmMultipartInfo multipartInfo = ozoneBucket\n           .initiateMultipartUpload(key, replicationType, replicationFactor);\n@@ -541,9 +516,10 @@ private Response createMultipartKey(String bucket, String key, long length,\n     try {\n       OzoneBucket ozoneBucket = getBucket(bucket);\n       String copyHeader;\n-      OmMultipartCommitUploadPartInfo omMultipartCommitUploadPartInfo;\n-      try (OzoneOutputStream ozoneOutputStream = ozoneBucket.createMultipartKey(\n-          key, length, partNumber, uploadID)) {\n+      OzoneOutputStream ozoneOutputStream = null;\n+      try {\n+        ozoneOutputStream = ozoneBucket.createMultipartKey(\n+            key, length, partNumber, uploadID);\n         copyHeader = headers.getHeaderString(COPY_SOURCE_HEADER);\n         if (copyHeader != null) {\n           Pair<String, String> result = parseSourceHeader(copyHeader);\n@@ -570,9 +546,12 @@ private Response createMultipartKey(String bucket, String key, long length,\n         } else {\n           IOUtils.copy(body, ozoneOutputStream);\n         }\n-        omMultipartCommitUploadPartInfo = ozoneOutputStream\n-            .getCommitUploadPartInfo();\n+      } finally {\n+        IOUtils.closeQuietly(ozoneOutputStream);\n       }\n+\n+      OmMultipartCommitUploadPartInfo omMultipartCommitUploadPartInfo =\n+          ozoneOutputStream.getCommitUploadPartInfo();\n       String eTag = omMultipartCommitUploadPartInfo.getPartName();\n \n       if (copyHeader != null) {\n@@ -672,30 +651,28 @@ private CopyObjectResponse copyObject(String copyHeader,\n     try {\n       // Checking whether we trying to copying to it self.\n \n-      if (sourceBucket.equals(destBucket)) {\n-        if (sourceKey.equals(destkey)) {\n-          // When copying to same storage type when storage type is provided,\n-          // we should not throw exception, as aws cli checks if any of the\n-          // options like storage type are provided or not when source and\n-          // dest are given same\n-          if (storageTypeDefault) {\n-            OS3Exception ex = S3ErrorTable.newError(S3ErrorTable\n-                .INVALID_REQUEST, copyHeader);\n-            ex.setErrorMessage(\"This copy request is illegal because it is \" +\n-                \"trying to copy an object to it self itself without changing \" +\n-                \"the object's metadata, storage class, website redirect \" +\n-                \"location or encryption attributes.\");\n-            throw ex;\n-          } else {\n-            // TODO: Actually here we should change storage type, as ozone\n-            // still does not support this just returning dummy response\n-            // for now\n-            CopyObjectResponse copyObjectResponse = new CopyObjectResponse();\n-            copyObjectResponse.setETag(OzoneUtils.getRequestID());\n-            copyObjectResponse.setLastModified(Instant.ofEpochMilli(\n-                Time.now()));\n-            return copyObjectResponse;\n-          }\n+      if (sourceBucket.equals(destBucket) && sourceKey.equals(destkey)) {\n+        // When copying to same storage type when storage type is provided,\n+        // we should not throw exception, as aws cli checks if any of the\n+        // options like storage type are provided or not when source and\n+        // dest are given same\n+        if (storageTypeDefault) {\n+          OS3Exception ex = S3ErrorTable.newError(S3ErrorTable\n+              .INVALID_REQUEST, copyHeader);\n+          ex.setErrorMessage(\"This copy request is illegal because it is \" +\n+              \"trying to copy an object to it self itself without changing \" +\n+              \"the object's metadata, storage class, website redirect \" +\n+              \"location or encryption attributes.\");\n+          throw ex;\n+        } else {\n+          // TODO: Actually here we should change storage type, as ozone\n+          // still does not support this just returning dummy response\n+          // for now\n+          CopyObjectResponse copyObjectResponse = new CopyObjectResponse();\n+          copyObjectResponse.setETag(OzoneUtils.getRequestID());\n+          copyObjectResponse.setLastModified(Instant.ofEpochMilli(\n+              Time.now()));\n+          return copyObjectResponse;\n         }\n       }\n \n@@ -766,4 +743,22 @@ private CopyObjectResponse copyObject(String copyHeader,\n \n     return Pair.of(header.substring(0, pos), header.substring(pos + 1));\n   }\n+\n+  private static S3StorageType toS3StorageType(String storageType)\n+      throws OS3Exception {\n+    try {\n+      return S3StorageType.valueOf(storageType);\n+    } catch (IllegalArgumentException ex) {\n+      throw S3ErrorTable.newError(S3ErrorTable.INVALID_ARGUMENT,\n+          storageType);\n+    }\n+  }\n+\n+  private static int parsePartNumberMarker(String partNumberMarker) {\n+    int partMarker = 0;\n+    if (partNumberMarker != null) {\n+      partMarker = Integer.parseInt(partNumberMarker);\n+    }\n+    return partMarker;\n+  }\n }",
                "deletions": 73
            },
            {
                "sha": "83cb90799f1c7a67e7e7132e7ceaf46152d82369",
                "filename": "hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/89bdb6a69e6314384e79f76e509cf99437877780/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java",
                "status": "modified",
                "changes": 8,
                "additions": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/client/OzoneOutputStreamStub.java?ref=89bdb6a69e6314384e79f76e509cf99437877780",
                "patch": "@@ -32,6 +32,7 @@\n public class OzoneOutputStreamStub extends OzoneOutputStream {\n \n   private final String partName;\n+  private boolean closed;\n \n   /**\n    * Constructs OzoneOutputStreamStub with outputStream and partName.\n@@ -62,12 +63,15 @@ public synchronized void flush() throws IOException {\n   @Override\n   public synchronized void close() throws IOException {\n     //commitKey can be done here, if needed.\n-    getOutputStream().close();\n+    if (!closed) {\n+      getOutputStream().close();\n+      closed = true;\n+    }\n   }\n \n   @Override\n   public OmMultipartCommitUploadPartInfo getCommitUploadPartInfo() {\n-    return new OmMultipartCommitUploadPartInfo(partName);\n+    return closed ? new OmMultipartCommitUploadPartInfo(partName) : null;\n   }\n \n }",
                "deletions": 2
            }
        ],
        "patched_files": [
            "ObjectEndpoint.java"
        ],
        "unit_tests": [
            "TestObjectEndpoint.java"
        ]
    },
    "hadoop-ozone_2ea9afa": {
        "repo": "hadoop-ozone",
        "message": "HDDS-2900. Avoid logging NPE when space usage check is not configured\n\nCloses #454",
        "commit": "https://github.com/apache/hadoop-ozone/commit/2ea9afae1ef4ca911dcd05fa1d6fd6166c550f6d",
        "parent": "https://github.com/apache/hadoop-ozone/commit/5d27f45c2cf38a6c40c79f0f870080f8f235f969",
        "bug_id": "hadoop-ozone_2ea9afa",
        "file": [
            {
                "sha": "60e24f2273f91b406d856a0b65b0d4d60385fcf1",
                "filename": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/fs/SpaceUsageCheckFactory.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/2ea9afae1ef4ca911dcd05fa1d6fd6166c550f6d/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/fs/SpaceUsageCheckFactory.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/2ea9afae1ef4ca911dcd05fa1d6fd6166c550f6d/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/fs/SpaceUsageCheckFactory.java",
                "status": "modified",
                "changes": 17,
                "additions": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/fs/SpaceUsageCheckFactory.java?ref=2ea9afae1ef4ca911dcd05fa1d6fd6166c550f6d",
                "patch": "@@ -71,13 +71,16 @@ default SpaceUsageCheckFactory setConfiguration(Configuration conf) {\n   static SpaceUsageCheckFactory create(Configuration config) {\n     Conf conf = OzoneConfiguration.of(config).getObject(Conf.class);\n     Class<? extends SpaceUsageCheckFactory> aClass = null;\n-    try {\n-      aClass = config.getClassByName(conf.getClassName())\n-          .asSubclass(SpaceUsageCheckFactory.class);\n-    } catch (ClassNotFoundException | RuntimeException e) {\n-      Logger log = LoggerFactory.getLogger(SpaceUsageCheckFactory.class);\n-      log.warn(\"Error trying to create SpaceUsageCheckFactory: '{}'\",\n-          conf.getClassName(), e);\n+    String className = conf.getClassName();\n+    if (className != null && !className.isEmpty()) {\n+      try {\n+        aClass = config.getClassByName(className)\n+            .asSubclass(SpaceUsageCheckFactory.class);\n+      } catch (ClassNotFoundException | RuntimeException e) {\n+        Logger log = LoggerFactory.getLogger(SpaceUsageCheckFactory.class);\n+        log.warn(\"Error trying to create SpaceUsageCheckFactory: '{}'\",\n+            className, e);\n+      }\n     }\n \n     SpaceUsageCheckFactory instance = null;",
                "deletions": 7
            },
            {
                "sha": "09b8cc2ecec9dc4f8c50c105d0a1a14cf005b97a",
                "filename": "hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/fs/TestSpaceUsageFactory.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/2ea9afae1ef4ca911dcd05fa1d6fd6166c550f6d/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/fs/TestSpaceUsageFactory.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/2ea9afae1ef4ca911dcd05fa1d6fd6166c550f6d/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/fs/TestSpaceUsageFactory.java",
                "status": "modified",
                "changes": 31,
                "additions": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/fs/TestSpaceUsageFactory.java?ref=2ea9afae1ef4ca911dcd05fa1d6fd6166c550f6d",
                "patch": "@@ -18,19 +18,26 @@\n package org.apache.hadoop.hdds.fs;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.test.GenericTestUtils.LogCapturer;\n+import org.junit.Before;\n import org.junit.Test;\n+import org.slf4j.LoggerFactory;\n \n import java.io.File;\n \n import static org.apache.hadoop.hdds.fs.SpaceUsageCheckFactory.Conf.configKeyForClassName;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n import static org.junit.jupiter.api.Assertions.assertSame;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n /**\n  * Tests for {@link SpaceUsageCheckFactory}.\n  */\n public class TestSpaceUsageFactory {\n \n+  private LogCapturer capturer;\n+\n   /**\n    * Verifies that {@link SpaceUsageCheckFactory#create(Configuration)} creates\n    * the correct implementation if configured.  This should be called from each\n@@ -49,6 +56,12 @@\n     return factoryClass.cast(factory);\n   }\n \n+  @Before\n+  public void setUp() {\n+    capturer = LogCapturer.captureLogs(\n+        LoggerFactory.getLogger(SpaceUsageCheckFactory.class));\n+  }\n+\n   @Test\n   public void configuresFactoryInstance() {\n     SpyFactory factory = testCreateViaConfig(SpyFactory.class);\n@@ -81,6 +94,16 @@ public void returnsDefaultFactoryForClassThatDoesNotImplementInterface() {\n     testDefaultFactoryForWrongConfig(\"java.lang.String\");\n   }\n \n+  private void assertNoLog() {\n+    assertEquals(\"\", capturer.getOutput());\n+  }\n+\n+  private void assertLogged(String substring) {\n+    String output = capturer.getOutput();\n+    assertTrue(output.contains(substring), () -> \"Expected \" + substring + \" \" +\n+        \"in log output, but only got: \" + output);\n+  }\n+\n   private static <T extends SpaceUsageCheckFactory> Configuration configFor(\n       Class<T> factoryClass) {\n \n@@ -97,11 +120,17 @@ private static void testDefaultFactoryForBrokenImplementation(\n     assertCreatesDefaultImplementation(conf);\n   }\n \n-  private static void testDefaultFactoryForWrongConfig(String value) {\n+  private void testDefaultFactoryForWrongConfig(String value) {\n     Configuration conf = new Configuration();\n     conf.set(configKeyForClassName(), value);\n \n     assertCreatesDefaultImplementation(conf);\n+\n+    if (value == null || value.isEmpty()) {\n+      assertNoLog();\n+    } else {\n+      assertLogged(value);\n+    }\n   }\n \n   private static void assertCreatesDefaultImplementation(Configuration conf) {",
                "deletions": 1
            }
        ],
        "patched_files": [
            "SpaceUsageCheckFactory.java"
        ],
        "unit_tests": [
            "TestSpaceUsageFactory.java"
        ]
    },
    "hadoop-ozone_a6f80c0": {
        "repo": "hadoop-ozone",
        "message": "HDDS-2325. BenchMarkDatanodeDispatcher genesis test is failing with NPE\n\nCloses #60",
        "commit": "https://github.com/apache/hadoop-ozone/commit/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
        "parent": "https://github.com/apache/hadoop-ozone/commit/f7ba61684460dbeff67886c6b7cbfcda7688ec2c",
        "bug_id": "hadoop-ozone_a6f80c0",
        "file": [
            {
                "sha": "1feb5001dcabb06ac7fc9bb5fe7178cb5bcb0f39",
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java",
                "status": "modified",
                "changes": 5,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/HddsDispatcher.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -321,6 +321,11 @@ private ContainerCommandResponseProto dispatchRequest(\n         // Once container is marked unhealthy, all the subsequent write\n         // transactions will fail with UNHEALTHY_CONTAINER exception.\n \n+        if (container == null) {\n+          throw new NullPointerException(\n+              \"Error on creating containers \" + result + \" \" + responseProto\n+                  .getMessage());\n+        }\n         // For container to be moved to unhealthy state here, the container can\n         // only be in open or closing state.\n         State containerState = container.getContainerData().getState();",
                "deletions": 0
            },
            {
                "sha": "c623e1419b40d2e6a4bf7d01ae32255e42a3edab",
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java",
                "status": "modified",
                "changes": 60,
                "additions": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/interfaces/Handler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -18,25 +18,19 @@\n \n package org.apache.hadoop.ozone.container.common.interfaces;\n \n-\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.OutputStream;\n+import java.util.function.Consumer;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerCommandRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerCommandResponseProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerType;\n-import org.apache.hadoop.hdds.protocol.proto\n-    .StorageContainerDatanodeProtocolProtos.IncrementalContainerReportProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerType;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n import org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n-import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.transport.server.ratis.DispatcherContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n@@ -54,41 +48,46 @@\n   protected final VolumeSet volumeSet;\n   protected String scmID;\n   protected final ContainerMetrics metrics;\n+  protected String datanodeId;\n+  private Consumer<ContainerReplicaProto> icrSender;\n \n-  private final StateContext context;\n-  private final DatanodeDetails datanodeDetails;\n-\n-  protected Handler(Configuration config, StateContext context,\n+  protected Handler(Configuration config, String datanodeId,\n       ContainerSet contSet, VolumeSet volumeSet,\n-      ContainerMetrics containerMetrics) {\n+      ContainerMetrics containerMetrics,\n+      Consumer<ContainerReplicaProto> icrSender) {\n     this.conf = config;\n-    this.context = context;\n     this.containerSet = contSet;\n     this.volumeSet = volumeSet;\n     this.metrics = containerMetrics;\n-    this.datanodeDetails = context.getParent().getDatanodeDetails();\n+    this.datanodeId = datanodeId;\n+    this.icrSender = icrSender;\n   }\n \n   public static Handler getHandlerForContainerType(\n       final ContainerType containerType, final Configuration config,\n-      final StateContext context, final ContainerSet contSet,\n-      final VolumeSet volumeSet, final ContainerMetrics metrics) {\n+      final String datanodeId, final ContainerSet contSet,\n+      final VolumeSet volumeSet, final ContainerMetrics metrics,\n+      Consumer<ContainerReplicaProto> icrSender) {\n     switch (containerType) {\n     case KeyValueContainer:\n-      return new KeyValueHandler(config, context, contSet, volumeSet, metrics);\n+      return new KeyValueHandler(config,\n+          datanodeId, contSet, volumeSet, metrics,\n+          icrSender);\n     default:\n       throw new IllegalArgumentException(\"Handler for ContainerType: \" +\n-        containerType + \"doesn't exist.\");\n+          containerType + \"doesn't exist.\");\n     }\n   }\n \n   /**\n    * Returns the Id of this datanode.\n+   *\n    * @return datanode Id\n    */\n-  protected DatanodeDetails getDatanodeDetails() {\n-    return datanodeDetails;\n+  protected String getDatanodeId() {\n+    return datanodeId;\n   }\n+\n   /**\n    * This should be called whenever there is state change. It will trigger\n    * an ICR to SCM.\n@@ -97,12 +96,8 @@ protected DatanodeDetails getDatanodeDetails() {\n    */\n   protected void sendICR(final Container container)\n       throws StorageContainerException {\n-    IncrementalContainerReportProto icr = IncrementalContainerReportProto\n-        .newBuilder()\n-        .addReport(container.getContainerReport())\n-        .build();\n-    context.addReport(icr);\n-    context.getParent().triggerHeartbeat();\n+    ContainerReplicaProto containerReport = container.getContainerReport();\n+    icrSender.accept(containerReport);\n   }\n \n   public abstract ContainerCommandResponseProto handle(\n@@ -175,8 +170,9 @@ public abstract void closeContainer(Container container)\n    * Deletes the given container.\n    *\n    * @param container container to be deleted\n-   * @param force if this is set to true, we delete container without checking\n-   * state of the container.\n+   * @param force     if this is set to true, we delete container without\n+   *                  checking\n+   *                  state of the container.\n    * @throws IOException\n    */\n   public abstract void deleteContainer(Container container, boolean force)",
                "deletions": 32
            },
            {
                "sha": "5da0b8fcb8bcba414cb89b24c081720a0663dfa6",
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "status": "modified",
                "changes": 12,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Consumer;\n import java.util.function.Function;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -47,6 +48,7 @@\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n     .PutSmallFileRequestProto;\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Type;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n import org.apache.hadoop.hdds.scm.ByteStringConversion;\n import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n import org.apache.hadoop.hdds.scm.container.common.helpers\n@@ -60,7 +62,6 @@\n import org.apache.hadoop.ozone.container.common.interfaces.Container;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.interfaces.VolumeChoosingPolicy;\n-import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.transport.server.ratis\n     .DispatcherContext;\n import org.apache.hadoop.ozone.container.common.transport.server.ratis\n@@ -109,9 +110,10 @@\n   private final AutoCloseableLock containerCreationLock;\n   private final boolean doSyncWrite;\n \n-  public KeyValueHandler(Configuration config, StateContext context,\n-      ContainerSet contSet, VolumeSet volSet, ContainerMetrics metrics) {\n-    super(config, context, contSet, volSet, metrics);\n+  public KeyValueHandler(Configuration config, String datanodeId,\n+      ContainerSet contSet, VolumeSet volSet, ContainerMetrics metrics,\n+      Consumer<ContainerReplicaProto> icrSender) {\n+    super(config, datanodeId, contSet, volSet, metrics, icrSender);\n     containerType = ContainerType.KeyValueContainer;\n     blockManager = new BlockManagerImpl(config);\n     doSyncWrite =\n@@ -220,7 +222,7 @@ ContainerCommandResponseProto handleCreateContainer(\n \n     KeyValueContainerData newContainerData = new KeyValueContainerData(\n         containerID, maxContainerSize, request.getPipelineID(),\n-        getDatanodeDetails().getUuidString());\n+        getDatanodeId());\n     // TODO: Add support to add metadataList to ContainerData. Add metadata\n     // to container during creation.\n     KeyValueContainer newContainer = new KeyValueContainer(",
                "deletions": 5
            },
            {
                "sha": "328cd91e82afd59e50584c7676c3e7630f2733fc",
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java",
                "status": "modified",
                "changes": 59,
                "additions": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/ozoneimpl/OzoneContainer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -18,16 +18,21 @@\n \n package org.apache.hadoop.ozone.container.ozoneimpl;\n \n-import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.collect.Maps;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+\n import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n-import org.apache.hadoop.hdds.protocol.datanode.proto\n-    .ContainerProtos.ContainerType;\n-import org.apache.hadoop.hdds.protocol.proto\n-    .StorageContainerDatanodeProtocolProtos;\n-import org.apache.hadoop.hdds.protocol.proto\n-        .StorageContainerDatanodeProtocolProtos.PipelineReportsProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerType;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.IncrementalContainerReportProto;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.PipelineReportsProto;\n import org.apache.hadoop.hdds.security.token.BlockTokenVerifier;\n import org.apache.hadoop.hdds.security.x509.SecurityConfig;\n import org.apache.hadoop.hdds.security.x509.certificate.client.CertificateClient;\n@@ -42,24 +47,20 @@\n import org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis;\n import org.apache.hadoop.ozone.container.common.volume.HddsVolume;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n-\n import org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService;\n import org.apache.hadoop.ozone.container.replication.GrpcReplicationService;\n-import org.apache.hadoop.ozone.container.replication\n-    .OnDemandContainerReplicationSource;\n+import org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.Maps;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_INTERVAL;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_INTERVAL_DEFAULT;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_TIMEOUT;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_SERVICE_TIMEOUT_DEFAULT;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.*;\n-import java.util.ArrayList;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.TimeUnit;\n-\n-import static org.apache.hadoop.ozone.OzoneConfigKeys.*;\n-\n /**\n  * Ozone main class sets up the network servers and initializes the container\n  * layer.\n@@ -100,10 +101,22 @@ public OzoneContainer(DatanodeDetails datanodeDetails, OzoneConfiguration\n     buildContainerSet();\n     final ContainerMetrics metrics = ContainerMetrics.create(conf);\n     this.handlers = Maps.newHashMap();\n+\n+    Consumer<ContainerReplicaProto> icrSender = containerReplicaProto -> {\n+      IncrementalContainerReportProto icr = IncrementalContainerReportProto\n+          .newBuilder()\n+          .addReport(containerReplicaProto)\n+          .build();\n+      context.addReport(icr);\n+      context.getParent().triggerHeartbeat();\n+    };\n+\n     for (ContainerType containerType : ContainerType.values()) {\n       handlers.put(containerType,\n           Handler.getHandlerForContainerType(\n-              containerType, conf, context, containerSet, volumeSet, metrics));\n+              containerType, conf,\n+              context.getParent().getDatanodeDetails().getUuidString(),\n+              containerSet, volumeSet, metrics, icrSender));\n     }\n \n     SecurityConfig secConf = new SecurityConfig(conf);\n@@ -169,7 +182,6 @@ private void buildContainerSet() {\n \n   }\n \n-\n   /**\n    * Start background daemon thread for performing container integrity checks.\n    */\n@@ -240,13 +252,14 @@ public void stop() {\n     ContainerMetrics.remove();\n   }\n \n-\n   @VisibleForTesting\n   public ContainerSet getContainerSet() {\n     return containerSet;\n   }\n+\n   /**\n    * Returns container report.\n+   *\n    * @return - container report.\n    */\n ",
                "deletions": 23
            },
            {
                "sha": "9db11d03f5c4fab373fd12fcc193bc7e6beb59e9",
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java",
                "status": "modified",
                "changes": 16,
                "additions": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/impl/TestHddsDispatcher.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -36,6 +36,7 @@\n     .WriteChunkRequestProto;\n import org.apache.hadoop.hdds.protocol.proto\n     .StorageContainerDatanodeProtocolProtos.ContainerAction;\n+import org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos.ContainerReplicaProto;\n import org.apache.hadoop.ozone.common.Checksum;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerUtils;\n@@ -48,6 +49,7 @@\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n import org.apache.hadoop.test.GenericTestUtils;\n+\n import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -57,6 +59,7 @@\n import java.io.IOException;\n import java.util.Map;\n import java.util.UUID;\n+import java.util.function.Consumer;\n \n import static java.nio.charset.StandardCharsets.UTF_8;\n import static org.apache.hadoop.hdds.scm.ScmConfigKeys.HDDS_DATANODE_DIR_KEY;\n@@ -69,6 +72,9 @@\n  */\n public class TestHddsDispatcher {\n \n+  public static final Consumer<ContainerReplicaProto> NO_OP_ICR_SENDER =\n+      c -> {};\n+\n   @Test\n   public void testContainerCloseActionWhenFull() throws IOException {\n     String testDir = GenericTestUtils.getTempPath(\n@@ -98,8 +104,9 @@ public void testContainerCloseActionWhenFull() throws IOException {\n       Map<ContainerType, Handler> handlers = Maps.newHashMap();\n       for (ContainerType containerType : ContainerType.values()) {\n         handlers.put(containerType,\n-            Handler.getHandlerForContainerType(containerType, conf, context,\n-                containerSet, volumeSet, metrics));\n+            Handler.getHandlerForContainerType(containerType, conf,\n+                context.getParent().getDatanodeDetails().getUuidString(),\n+                containerSet, volumeSet, metrics, NO_OP_ICR_SENDER));\n       }\n       HddsDispatcher hddsDispatcher = new HddsDispatcher(\n           conf, containerSet, volumeSet, handlers, context, metrics, null);\n@@ -214,8 +221,9 @@ private HddsDispatcher createDispatcher(DatanodeDetails dd, UUID scmId,\n     Map<ContainerType, Handler> handlers = Maps.newHashMap();\n     for (ContainerType containerType : ContainerType.values()) {\n       handlers.put(containerType,\n-          Handler.getHandlerForContainerType(containerType, conf, context,\n-              containerSet, volumeSet, metrics));\n+          Handler.getHandlerForContainerType(containerType, conf,\n+              context.getParent().getDatanodeDetails().getUuidString(),\n+              containerSet, volumeSet, metrics, NO_OP_ICR_SENDER));\n     }\n \n     HddsDispatcher hddsDispatcher = new HddsDispatcher(",
                "deletions": 4
            },
            {
                "sha": "086b0fea9116db9b9497a88716131c752eacb880",
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java",
                "status": "modified",
                "changes": 8,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/interfaces/TestHandler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -19,16 +19,19 @@\n package org.apache.hadoop.ozone.container.common.interfaces;\n \n import com.google.common.collect.Maps;\n+\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Rule;\n@@ -69,7 +72,10 @@ public void setup() throws Exception {\n         ContainerProtos.ContainerType.values()) {\n       handlers.put(containerType,\n           Handler.getHandlerForContainerType(\n-              containerType, conf, context, containerSet, volumeSet, metrics));\n+              containerType, conf,\n+              context.getParent().getDatanodeDetails().getUuidString(),\n+              containerSet, volumeSet, metrics,\n+              TestHddsDispatcher.NO_OP_ICR_SENDER));\n     }\n     this.dispatcher = new HddsDispatcher(\n         conf, containerSet, volumeSet, handlers, null, metrics, null);",
                "deletions": 1
            },
            {
                "sha": "05b92cc825396d6a367ccf6f447fdc72e97a2a26",
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java",
                "status": "modified",
                "changes": 10,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandler.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -244,8 +244,10 @@ public void testVolumeSetInKeyValueHandler() throws Exception{\n       Mockito.when(stateMachine.getDatanodeDetails())\n           .thenReturn(datanodeDetails);\n       Mockito.when(context.getParent()).thenReturn(stateMachine);\n-      KeyValueHandler keyValueHandler = new KeyValueHandler(conf, context, cset,\n-          volumeSet, metrics);\n+      KeyValueHandler keyValueHandler = new KeyValueHandler(conf,\n+          context.getParent().getDatanodeDetails().getUuidString(), cset,\n+          volumeSet, metrics, c -> {\n+      });\n       assertEquals(\"org.apache.hadoop.ozone.container.common\" +\n           \".volume.RoundRobinVolumeChoosingPolicy\",\n           keyValueHandler.getVolumeChoosingPolicyForTesting()\n@@ -255,7 +257,9 @@ public void testVolumeSetInKeyValueHandler() throws Exception{\n       conf.set(HDDS_DATANODE_VOLUME_CHOOSING_POLICY,\n           \"org.apache.hadoop.ozone.container.common.impl.HddsDispatcher\");\n       try {\n-        new KeyValueHandler(conf, context, cset, volumeSet, metrics);\n+        new KeyValueHandler(conf,\n+            context.getParent().getDatanodeDetails().getUuidString(),\n+            cset, volumeSet, metrics, c->{});\n       } catch (RuntimeException ex) {\n         GenericTestUtils.assertExceptionContains(\"class org.apache.hadoop\" +\n             \".ozone.container.common.impl.HddsDispatcher not org.apache\" +",
                "deletions": 3
            },
            {
                "sha": "26603b6d130db9da9d857ec8374af9f481f92009",
                "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java",
                "status": "modified",
                "changes": 7,
                "additions": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueHandlerWithUnhealthyContainer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -24,9 +24,11 @@\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n+\n import org.junit.Assert;\n import org.junit.Test;\n import org.slf4j.Logger;\n@@ -41,7 +43,6 @@\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n-\n /**\n  * Test that KeyValueHandler fails certain operations when the\n  * container is unhealthy.\n@@ -147,10 +148,10 @@ private KeyValueHandler getDummyHandler() throws IOException {\n \n     return new KeyValueHandler(\n         new OzoneConfiguration(),\n-        context,\n+        stateMachine.getDatanodeDetails().getUuidString(),\n         mock(ContainerSet.class),\n         mock(VolumeSet.class),\n-        mock(ContainerMetrics.class));\n+        mock(ContainerMetrics.class), TestHddsDispatcher.NO_OP_ICR_SENDER);\n   }\n \n   private KeyValueContainer getMockUnhealthyContainer() {",
                "deletions": 3
            },
            {
                "sha": "0598f15e63f7afed848a26031d19304ff66b05fb",
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java",
                "status": "modified",
                "changes": 9,
                "additions": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/metrics/TestContainerMetrics.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -22,6 +22,7 @@\n import static org.apache.hadoop.test.MetricsAsserts.getMetrics;\n \n import com.google.common.collect.Maps;\n+\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.hdds.client.BlockID;\n import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n@@ -40,6 +41,7 @@\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n@@ -51,6 +53,7 @@\n import org.apache.hadoop.ozone.container.replication.GrpcReplicationService;\n import org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource;\n import org.apache.hadoop.test.GenericTestUtils;\n+\n import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n@@ -104,8 +107,10 @@ public void testContainerMetrics() throws Exception {\n       for (ContainerProtos.ContainerType containerType :\n           ContainerProtos.ContainerType.values()) {\n         handlers.put(containerType,\n-            Handler.getHandlerForContainerType(containerType, conf, context,\n-                containerSet, volumeSet, metrics));\n+            Handler.getHandlerForContainerType(containerType, conf,\n+                context.getParent().getDatanodeDetails().getUuidString(),\n+                containerSet, volumeSet, metrics,\n+                TestHddsDispatcher.NO_OP_ICR_SENDER));\n       }\n       HddsDispatcher dispatcher = new HddsDispatcher(conf, containerSet,\n           volumeSet, handlers, context, metrics, null);",
                "deletions": 2
            },
            {
                "sha": "e3827347dd6874d682a9db4de99cfc9dd1f6258c",
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java",
                "status": "modified",
                "changes": 7,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestContainerServer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n@@ -224,8 +225,10 @@ public void testClientServerWithContainerDispatcher() throws Exception {\n       for (ContainerProtos.ContainerType containerType :\n           ContainerProtos.ContainerType.values()) {\n         handlers.put(containerType,\n-            Handler.getHandlerForContainerType(containerType, conf, context,\n-                containerSet, volumeSet, metrics));\n+            Handler.getHandlerForContainerType(containerType, conf,\n+                context.getParent().getDatanodeDetails().getUuidString(),\n+                containerSet, volumeSet, metrics,\n+                TestHddsDispatcher.NO_OP_ICR_SENDER));\n       }\n       HddsDispatcher dispatcher = new HddsDispatcher(\n           conf, containerSet, volumeSet, handlers, context, metrics, null);",
                "deletions": 2
            },
            {
                "sha": "33f93e5288e74fe4b79086e671ca875c8243b75a",
                "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java",
                "status": "modified",
                "changes": 7,
                "additions": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/server/TestSecureContainerServer.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n+import org.apache.hadoop.ozone.container.common.impl.TestHddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine;\n@@ -155,8 +156,10 @@ private static HddsDispatcher createDispatcher(DatanodeDetails dd, UUID scmId,\n     for (ContainerProtos.ContainerType containerType :\n         ContainerProtos.ContainerType.values()) {\n       handlers.put(containerType,\n-          Handler.getHandlerForContainerType(containerType, conf, context,\n-              containerSet, volumeSet, metrics));\n+          Handler.getHandlerForContainerType(containerType, conf,\n+              dd.getUuid().toString(),\n+              containerSet, volumeSet, metrics,\n+              TestHddsDispatcher.NO_OP_ICR_SENDER));\n     }\n     HddsDispatcher hddsDispatcher = new HddsDispatcher(\n         conf, containerSet, volumeSet, handlers, context, metrics,",
                "deletions": 2
            },
            {
                "sha": "1d087f93d78c9a5e61660c73fcc5722e269daad3",
                "filename": "hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java",
                "status": "modified",
                "changes": 82,
                "additions": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkDatanodeDispatcher.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -17,54 +17,49 @@\n  */\n package org.apache.hadoop.ozone.genesis;\n \n-import com.google.common.collect.Maps;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hdds.HddsUtils;\n+import org.apache.hadoop.hdds.client.BlockID;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumData;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumType;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChunkInfo;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.GetBlockRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.PutBlockRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ReadChunkRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.WriteChunkRequestProto;\n import org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics;\n import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.impl.HddsDispatcher;\n import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n-import org.apache.hadoop.ozone.container.common.statemachine\n-    .DatanodeStateMachine.DatanodeStates;\n+import org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.DatanodeStates;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n import org.apache.hadoop.ozone.container.common.volume.VolumeSet;\n-import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n+\n+import com.google.common.collect.Maps;\n import org.apache.commons.codec.digest.DigestUtils;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.RandomStringUtils;\n import org.apache.commons.lang3.RandomUtils;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hdds.client.BlockID;\n-import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n-\n+import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n import org.openjdk.jmh.annotations.Benchmark;\n import org.openjdk.jmh.annotations.Level;\n import org.openjdk.jmh.annotations.Scope;\n import org.openjdk.jmh.annotations.Setup;\n import org.openjdk.jmh.annotations.State;\n import org.openjdk.jmh.annotations.TearDown;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.UUID;\n-import java.util.concurrent.atomic.AtomicInteger;\n-\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .PutBlockRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .GetBlockRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ContainerCommandRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .ReadChunkRequestProto;\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos\n-    .WriteChunkRequestProto;\n-\n-import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n-\n /**\n  * Benchmarks DatanodeDispatcher class.\n  */\n@@ -83,6 +78,7 @@\n   private static final int INIT_CONTAINERS = 100;\n   private static final int INIT_KEYS = 50;\n   private static final int INIT_CHUNKS = 100;\n+  public static final int CHUNK_SIZE = 1048576;\n \n   private List<Long> containers;\n   private List<Long> keys;\n@@ -94,15 +90,18 @@ public void initialize() throws IOException {\n     datanodeUuid = UUID.randomUUID().toString();\n \n     // 1 MB of data\n-    data = ByteString.copyFromUtf8(RandomStringUtils.randomAscii(1048576));\n-    random  = new Random();\n+    data = ByteString.copyFromUtf8(RandomStringUtils.randomAscii(CHUNK_SIZE));\n+    random = new Random();\n     Configuration conf = new OzoneConfiguration();\n     baseDir = System.getProperty(\"java.io.tmpdir\") + File.separator +\n         datanodeUuid;\n \n     // data directory\n     conf.set(\"dfs.datanode.data.dir\", baseDir + File.separator + \"data\");\n \n+    //We need 100 * container size minimum space\n+    conf.set(\"ozone.scm.container.size\", \"10MB\");\n+\n     ContainerSet containerSet = new ContainerSet();\n     volumeSet = new VolumeSet(datanodeUuid, conf);\n     StateContext context = new StateContext(\n@@ -111,9 +110,12 @@ public void initialize() throws IOException {\n     Map<ContainerProtos.ContainerType, Handler> handlers = Maps.newHashMap();\n     for (ContainerProtos.ContainerType containerType :\n         ContainerProtos.ContainerType.values()) {\n-      handlers.put(containerType,\n-          Handler.getHandlerForContainerType(\n-              containerType, conf, context, containerSet, volumeSet, metrics));\n+      Handler handler = Handler.getHandlerForContainerType(\n+          containerType, conf, \"datanodeid\",\n+          containerSet, volumeSet, metrics,\n+          c -> {});\n+      handler.setScmID(\"scm\");\n+      handlers.put(containerType, handler);\n     }\n     dispatcher = new HddsDispatcher(conf, containerSet, volumeSet, handlers,\n         context, metrics, null);\n@@ -217,11 +219,16 @@ private ContainerCommandRequestProto getReadChunkCommand(\n   private ContainerProtos.ChunkInfo getChunkInfo(\n       BlockID blockID, String chunkName) {\n     ContainerProtos.ChunkInfo.Builder builder =\n-        ContainerProtos.ChunkInfo.newBuilder()\n+        ChunkInfo.newBuilder()\n             .setChunkName(\n                 DigestUtils.md5Hex(chunkName)\n                     + \"_stream_\" + blockID.getContainerID() + \"_block_\"\n                     + blockID.getLocalID())\n+            .setChecksumData(\n+                ChecksumData.newBuilder()\n+                    .setBytesPerChecksum(4)\n+                    .setType(ChecksumType.CRC32)\n+                    .build())\n             .setOffset(0).setLen(data.size());\n     return builder.build();\n   }\n@@ -245,7 +252,7 @@ private ContainerCommandRequestProto getPutBlockCommand(\n   private ContainerCommandRequestProto getGetBlockCommand(BlockID blockID) {\n     GetBlockRequestProto.Builder readBlockRequest =\n         GetBlockRequestProto.newBuilder()\n-        .setBlockID(blockID.getDatanodeBlockIDProtobuf());\n+            .setBlockID(blockID.getDatanodeBlockIDProtobuf());\n     ContainerCommandRequestProto.Builder request = ContainerCommandRequestProto\n         .newBuilder()\n         .setCmdType(ContainerProtos.Type.GetBlock)\n@@ -274,7 +281,6 @@ public void createContainer(BenchMarkDatanodeDispatcher bmdd) {\n     bmdd.containerCount.getAndIncrement();\n   }\n \n-\n   @Benchmark\n   public void writeChunk(BenchMarkDatanodeDispatcher bmdd) {\n     bmdd.dispatcher.dispatch(getWriteChunkCommand(",
                "deletions": 38
            },
            {
                "sha": "436166645c3d188a389b1211ee528b8c1873eab3",
                "filename": "hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java",
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java",
                "status": "modified",
                "changes": 3,
                "additions": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/Genesis.java?ref=a6f80c096b5320f50b6e9e9b4ba5f7c7e3544385",
                "patch": "@@ -41,7 +41,8 @@\n \n   // After adding benchmark in genesis package add the benchmark name in the\n   // description for this option.\n-  @Option(names = \"-benchmark\", split = \",\", description =\n+  @Option(names = {\"-b\", \"-benchmark\", \"--benchmark\"},\n+      split = \",\", description =\n       \"Option used for specifying benchmarks to run.\\n\"\n           + \"Ex. ozone genesis -benchmark BenchMarkContainerStateMap,\"\n           + \"BenchMarkOMKeyAllocation.\\n\"",
                "deletions": 1
            }
        ],
        "patched_files": [
            "Handler.java",
            "BenchMarkDatanodeDispatcher.java",
            "KeyValueHandler.java",
            "OzoneContainer.java",
            "ContainerMetrics.java",
            "HddsDispatcher.java",
            "Genesis.java"
        ],
        "unit_tests": [
            "TestKeyValueHandler.java",
            "TestContainerMetrics.java",
            "TestKeyValueHandlerWithUnhealthyContainer.java",
            "TestSecureContainerServer.java",
            "TestOzoneContainer.java",
            "TestHddsDispatcher.java",
            "TestContainerServer.java",
            "TestHandler.java"
        ]
    }
}