{
    "accumulo_0e3fd3b": {
        "bug_id": "accumulo_0e3fd3b",
        "commit": "https://github.com/apache/accumulo/commit/0e3fd3b3cc26496f5a3e898c52f8fded16a5f203",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/accumulo/blob/0e3fd3b3cc26496f5a3e898c52f8fded16a5f203/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java?ref=0e3fd3b3cc26496f5a3e898c52f8fded16a5f203",
                "deletions": 1,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "patch": "@@ -52,7 +52,10 @@ private TabletBalancer constructNewBalancerForTable(String clazzName, String tab\n   }\n \n   protected String getLoadBalancerClassNameForTable(String table) {\n-    if (TableManager.getInstance().getTableState(table).equals(TableState.ONLINE))\n+    TableState tableState = TableManager.getInstance().getTableState(table);\n+    if (tableState == null)\n+      return null;\n+    if (tableState.equals(TableState.ONLINE))\n       return configuration.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n     return null;\n   }",
                "raw_url": "https://github.com/apache/accumulo/raw/0e3fd3b3cc26496f5a3e898c52f8fded16a5f203/server/base/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "sha": "f2478b1979b6d9a528556989d346ca8b9ec1b72c",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-2028 avoid NPE during a table delete when balancing",
        "parent": "https://github.com/apache/accumulo/commit/37c9a76017eac42577fbbc4c217f2af76ea84779",
        "repo": "accumulo",
        "unit_tests": [
            "TableLoadBalancerTest.java"
        ]
    },
    "accumulo_17d26bd": {
        "bug_id": "accumulo_17d26bd",
        "commit": "https://github.com/apache/accumulo/commit/17d26bda481a5b62295d9c5f56b58680005d6b14",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/accumulo/blob/17d26bda481a5b62295d9c5f56b58680005d6b14/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java?ref=17d26bda481a5b62295d9c5f56b58680005d6b14",
                "deletions": 1,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.accumulo.core.master.thrift.TabletServerStatus;\n import org.apache.accumulo.core.tabletserver.thrift.TabletStats;\n import org.apache.accumulo.server.conf.ServerConfiguration;\n+import org.apache.accumulo.server.conf.ServerConfigurationFactory;\n import org.apache.accumulo.server.master.state.TServerInstance;\n import org.apache.accumulo.server.master.state.TabletMigration;\n import org.apache.commons.lang.builder.ToStringBuilder;\n@@ -254,7 +255,7 @@ public int getMaxConcurrentMigrations() {\n   }\n \n   @Override\n-  public void init(ServerConfiguration conf) {\n+  public void init(ServerConfigurationFactory conf) {\n     super.init(conf);\n     parseConfiguration(conf);\n   }",
                "raw_url": "https://github.com/apache/accumulo/raw/17d26bda481a5b62295d9c5f56b58680005d6b14/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "sha": "6b26dc4c7ffee019527788b388d91fb94099469b",
                "status": "modified"
            }
        ],
        "message": "[ACCUMULO-4535] Fix NPE in HostRegexTableLoadBalancer\n\nSigned-off-by: Josh Elser <elserj@apache.org>",
        "parent": "https://github.com/apache/accumulo/commit/3bd701b883492f06766631af30e7b08f14d3454c",
        "repo": "accumulo",
        "unit_tests": [
            "HostRegexTableLoadBalancerTest.java"
        ]
    },
    "accumulo_1d608a8": {
        "bug_id": "accumulo_1d608a8",
        "commit": "https://github.com/apache/accumulo/commit/1d608a81f488c2bf371fc81f83e0022bd2943a36",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java?ref=1d608a81f488c2bf371fc81f83e0022bd2943a36",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.accumulo.core.conf.DefaultConfiguration;\n import org.apache.accumulo.core.conf.SiteConfiguration;\n import org.apache.accumulo.core.data.KeyExtent;\n+import org.apache.accumulo.server.client.HdfsZooInstance;\n \n public class ServerConfiguration {\n   ",
                "raw_url": "https://github.com/apache/accumulo/raw/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "sha": "86532745d69302069d67f08cc4fbc0d8f9edcaa6",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/accumulo/blob/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java?ref=1d608a81f488c2bf371fc81f83e0022bd2943a36",
                "deletions": 33,
                "filename": "server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java",
                "patch": "@@ -37,37 +37,46 @@\n \n public class TableConfiguration extends AccumuloConfiguration {\n   private static final Logger log = Logger.getLogger(TableConfiguration.class);\n-  \n+\n   // Need volatile keyword to ensure double-checked locking works as intended\n   private static volatile ZooCache tablePropCache = null;\n+  private static final Object initLock = new Object();\n+\n   private final String instanceId;\n+  private final Instance instance;\n   private final AccumuloConfiguration parent;\n-  \n+\n   private String table = null;\n   private Set<ConfigurationObserver> observers;\n-  \n+\n   public TableConfiguration(String instanceId, String table, AccumuloConfiguration parent) {\n+    this(instanceId, HdfsZooInstance.getInstance(), table, parent);\n+  }\n+\n+  public TableConfiguration(String instanceId, Instance instance, String table, AccumuloConfiguration parent) {\n     this.instanceId = instanceId;\n+    this.instance = instance;\n     this.table = table;\n     this.parent = parent;\n-    \n+\n     this.observers = Collections.synchronizedSet(new HashSet<ConfigurationObserver>());\n   }\n-  \n-  /**\n-   * @deprecated not for client use\n-   */\n-  @Deprecated\n-  private static ZooCache getTablePropCache() {\n-    Instance inst = HdfsZooInstance.getInstance();\n-    if (tablePropCache == null)\n-      synchronized (TableConfiguration.class) {\n-        if (tablePropCache == null)\n-          tablePropCache = new ZooCache(inst.getZooKeepers(), inst.getZooKeepersSessionTimeOut(), new TableConfWatcher(inst));\n+\n+  private void initializeZooCache() {\n+    synchronized (initLock) {\n+      if (null == tablePropCache) {\n+        tablePropCache = new ZooCache(instance.getZooKeepers(), instance.getZooKeepersSessionTimeOut(), new TableConfWatcher(instance));\n       }\n+    }\n+  }\n+\n+  private ZooCache getTablePropCache() {\n+    if (null == tablePropCache) {\n+      initializeZooCache();\n+    }\n     return tablePropCache;\n   }\n-  \n+\n   public void addObserver(ConfigurationObserver co) {\n     if (table == null) {\n       String err = \"Attempt to add observer for non-table configuration\";\n@@ -77,7 +86,7 @@ public void addObserver(ConfigurationObserver co) {\n     iterator();\n     observers.add(co);\n   }\n-  \n+\n   public void removeObserver(ConfigurationObserver configObserver) {\n     if (table == null) {\n       String err = \"Attempt to remove observer for non-table configuration\";\n@@ -86,53 +95,54 @@ public void removeObserver(ConfigurationObserver configObserver) {\n     }\n     observers.remove(configObserver);\n   }\n-  \n+\n   public void expireAllObservers() {\n     Collection<ConfigurationObserver> copy = Collections.unmodifiableCollection(observers);\n     for (ConfigurationObserver co : copy)\n       co.sessionExpired();\n   }\n-  \n+\n   public void propertyChanged(String key) {\n     Collection<ConfigurationObserver> copy = Collections.unmodifiableCollection(observers);\n     for (ConfigurationObserver co : copy)\n       co.propertyChanged(key);\n   }\n-  \n+\n   public void propertiesChanged(String key) {\n     Collection<ConfigurationObserver> copy = Collections.unmodifiableCollection(observers);\n     for (ConfigurationObserver co : copy)\n       co.propertiesChanged();\n   }\n-  \n+\n   public String get(Property property) {\n     String key = property.getKey();\n     String value = get(key);\n-    \n+\n     if (value == null || !property.getType().isValidFormat(value)) {\n       if (value != null)\n         log.error(\"Using default value for \" + key + \" due to improperly formatted \" + property.getType() + \": \" + value);\n       value = parent.get(property);\n     }\n     return value;\n   }\n-  \n+\n   private String get(String key) {\n     String zPath = ZooUtil.getRoot(instanceId) + Constants.ZTABLES + \"/\" + table + Constants.ZTABLE_CONF + \"/\" + key;\n+\n     byte[] v = getTablePropCache().get(zPath);\n     String value = null;\n     if (v != null)\n       value = new String(v, Constants.UTF8);\n     return value;\n   }\n-  \n+\n   @Override\n   public Iterator<Entry<String,String>> iterator() {\n     TreeMap<String,String> entries = new TreeMap<String,String>();\n-    \n+\n     for (Entry<String,String> parentEntry : parent)\n       entries.put(parentEntry.getKey(), parentEntry.getValue());\n-    \n+\n     List<String> children = getTablePropCache().getChildren(ZooUtil.getRoot(instanceId) + Constants.ZTABLES + \"/\" + table + Constants.ZTABLE_CONF);\n     if (children != null) {\n       for (String child : children) {\n@@ -141,22 +151,26 @@ private String get(String key) {\n           entries.put(child, value);\n       }\n     }\n-    \n+\n     return entries.entrySet().iterator();\n   }\n-  \n+\n   public String getTableId() {\n     return table;\n   }\n \n   @Override\n   public void invalidateCache() {\n     if (null != tablePropCache) {\n-      synchronized (TableConfiguration.class) {\n-        if (null != tablePropCache) {\n-          tablePropCache = null;\n-        }\n-      }\n+      tablePropCache.clear();\n     }\n+    // Else, if the cache is null, we could lock and double-check\n+    // to see if it happened to be created so we could invalidate it\n+    // but I don't see much benefit coming from that extra check.\n+  }\n+  \n+  @Override\n+  public String toString() {\n+    return this.getClass().getSimpleName();\n   }\n }",
                "raw_url": "https://github.com/apache/accumulo/raw/1d608a81f488c2bf371fc81f83e0022bd2943a36/server/src/main/java/org/apache/accumulo/server/conf/TableConfiguration.java",
                "sha": "7a3d6e44c493b950720b482aa9a6e33bd335e65c",
                "status": "modified"
            },
            {
                "additions": 152,
                "blob_url": "https://github.com/apache/accumulo/blob/1d608a81f488c2bf371fc81f83e0022bd2943a36/test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java?ref=1d608a81f488c2bf371fc81f83e0022bd2943a36",
                "deletions": 0,
                "filename": "test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java",
                "patch": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.test;\n+\n+import java.util.ArrayList;\n+import java.util.Random;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.accumulo.core.client.Connector;\n+import org.apache.accumulo.core.client.ZooKeeperInstance;\n+import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n+import org.apache.accumulo.core.conf.AccumuloConfiguration;\n+import org.apache.accumulo.core.conf.DefaultConfiguration;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.minicluster.MiniAccumuloCluster;\n+import org.apache.accumulo.server.conf.TableConfiguration;\n+import org.apache.log4j.Logger;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TableConfigurationUpdateTest {\n+  private static final Logger log = Logger.getLogger(TableConfigurationUpdateTest.class);\n+\n+  public static TemporaryFolder folder = new TemporaryFolder();\n+  private MiniAccumuloCluster accumulo;\n+  private String secret = \"secret\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    folder.create();\n+    accumulo = new MiniAccumuloCluster(folder.getRoot(), secret);\n+    accumulo.start();\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    accumulo.stop();\n+    folder.delete();\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    ZooKeeperInstance inst = new ZooKeeperInstance(accumulo.getInstanceName(), accumulo.getZooKeepers(), 60 * 1000);\n+    Connector conn = inst.getConnector(\"root\", new PasswordToken(secret));\n+\n+    String table = \"foo\";\n+    conn.tableOperations().create(table);\n+\n+    final DefaultConfiguration defaultConf = AccumuloConfiguration.getDefaultConfiguration();\n+\n+    // Cache invalidates 25% of the time\n+    int randomMax = 4;\n+    // Number of threads\n+    int numThreads = 2;\n+    // Number of iterations per thread\n+    int iterations = 100000;\n+    AccumuloConfiguration tableConf = new TableConfiguration(inst.getInstanceID(), inst, table, defaultConf);\n+    \n+    long start = System.currentTimeMillis();\n+    ExecutorService svc = Executors.newFixedThreadPool(numThreads);\n+    CountDownLatch countDown = new CountDownLatch(numThreads);\n+    ArrayList<Future<Exception>> futures = new ArrayList<Future<Exception>>(numThreads);\n+\n+    for (int i = 0; i < numThreads; i++) {\n+      futures.add(svc.submit(new TableConfRunner(randomMax, iterations, tableConf, countDown)));\n+    }\n+\n+    svc.shutdown();\n+    Assert.assertTrue(svc.awaitTermination(60, TimeUnit.MINUTES));\n+\n+    for (Future<Exception> fut : futures) {\n+      Exception e = fut.get();\n+      if (null != e) {\n+        Assert.fail(\"Thread failed with exception \" + e);\n+      }\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    log.debug(tableConf + \" with \" + iterations + \" iterations and \" + numThreads + \" threads and cache invalidates \"\n+        + ((1. / randomMax) * 100.) + \"% took \" + (end - start) / 1000 + \" second(s)\");\n+  }\n+\n+  public static class TableConfRunner implements Callable<Exception> {\n+    private static final Property prop = Property.TABLE_SPLIT_THRESHOLD;\n+    private AccumuloConfiguration tableConf;\n+    private CountDownLatch countDown;\n+    private int iterations, randMax;\n+\n+    public TableConfRunner(int randMax, int iterations, AccumuloConfiguration tableConf, CountDownLatch countDown) {\n+      this.randMax = randMax;\n+      this.iterations = iterations;\n+      this.tableConf = tableConf;\n+      this.countDown = countDown;\n+    }\n+\n+    @Override\n+    public Exception call() {\n+      Random r = new Random();\n+      countDown.countDown();\n+      try {\n+        countDown.await();\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        return e;\n+      }\n+\n+      String t = Thread.currentThread().getName() + \" \";\n+      try {\n+        for (int i = 0; i < iterations; i++) {\n+          // if (i % 10000 == 0) {\n+          // log.info(t + \" \" + i);\n+          // }\n+          int choice = r.nextInt(randMax);\n+          if (choice < 1) {\n+            tableConf.invalidateCache();\n+          } else {\n+            tableConf.get(prop);\n+          }\n+        }\n+      } catch (Exception e) {\n+        log.error(t, e);\n+        return e;\n+      }\n+\n+      return null;\n+    }\n+\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/accumulo/raw/1d608a81f488c2bf371fc81f83e0022bd2943a36/test/src/test/java/org/apache/accumulo/test/TableConfigurationUpdateTest.java",
                "sha": "30da268f747618c5fd76f563bc30230d58f41f3d",
                "status": "added"
            }
        ],
        "message": "ACCUMULO-2489 Fixes race condition in TableConfiguration where NPE may occur.\n\nMake invalidateCache much more efficient by calling clear on ZooCache\ninstead of creating a new one.",
        "parent": "https://github.com/apache/accumulo/commit/63d5e55a0b03910246b9b21efecfde5ac5e709f0",
        "repo": "accumulo",
        "unit_tests": [
            "TableConfigurationTest.java"
        ]
    },
    "accumulo_35a6746": {
        "bug_id": "accumulo_35a6746",
        "commit": "https://github.com/apache/accumulo/commit/35a67466b6685b6cb03e727585701b3029163cdf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/accumulo/blob/35a67466b6685b6cb03e727585701b3029163cdf/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java?ref=35a67466b6685b6cb03e727585701b3029163cdf",
                "deletions": 0,
                "filename": "server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "patch": "@@ -70,6 +70,7 @@ protected TabletBalancer getBalancerForTable(String table) {\n           if (newBalancer != null) {\n             balancer = newBalancer;\n             perTableBalancers.put(table, balancer);\n+            balancer.init(configuration);\n           }\n         } catch (Exception e) {\n           log.warn(\"Failed to load table balancer class \", e);\n@@ -89,6 +90,7 @@ protected TabletBalancer getBalancerForTable(String table) {\n         balancer = new DefaultLoadBalancer(table);\n       }\n       perTableBalancers.put(table, balancer);\n+      balancer.init(configuration);\n     }\n     return balancer;\n   }",
                "raw_url": "https://github.com/apache/accumulo/raw/35a67466b6685b6cb03e727585701b3029163cdf/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "sha": "c57e1e182941a30f979f0b69dba544eec8558df8",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-590 fixed NPE in Table load balancer\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1337149 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/accumulo/commit/8fad5e736750868dc563caeb9cf89c38005f034a",
        "repo": "accumulo",
        "unit_tests": [
            "TableLoadBalancerTest.java"
        ]
    },
    "accumulo_3c513c8": {
        "bug_id": "accumulo_3c513c8",
        "commit": "https://github.com/apache/accumulo/commit/3c513c894d711dd6d52e798a239b893ea94aa124",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/accumulo/blob/3c513c894d711dd6d52e798a239b893ea94aa124/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java?ref=3c513c894d711dd6d52e798a239b893ea94aa124",
                "deletions": 4,
                "filename": "server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "patch": "@@ -3118,9 +3118,15 @@ private SplitRowSpec findSplitRow(Collection<String> files) {\n       } else {\n         lastRow = extent.getEndRow();\n       }\n-      \n+\n+      // We expect to get a midPoint for this set of files. If we don't get one, we have a problem.\n+      final Key mid = keys.get(.5);\n+      if (null == mid) {\n+        throw new IllegalStateException(\"Could not determine midpoint for files\");\n+      }\n+\n       // check to see that the midPoint is not equal to the end key\n-      if (keys.get(.5).compareRow(lastRow) == 0) {\n+      if (mid.compareRow(lastRow) == 0) {\n         if (keys.firstKey() < .5) {\n           Key candidate = keys.get(keys.firstKey());\n           if (candidate.compareRow(lastRow) != 0) {\n@@ -3140,8 +3146,7 @@ private SplitRowSpec findSplitRow(Collection<String> files) {\n         \n         return null;\n       }\n-      Key mid = keys.get(.5);\n-      Text text = (mid == null) ? null : mid.getRow();\n+      Text text = mid.getRow();\n       SortedMap<Double,Key> firstHalf = keys.headMap(.5);\n       if (firstHalf.size() > 0) {\n         Text beforeMid = firstHalf.get(firstHalf.lastKey()).getRow();",
                "raw_url": "https://github.com/apache/accumulo/raw/3c513c894d711dd6d52e798a239b893ea94aa124/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "sha": "a1fc70747f46b6edb8ef3586e80aede239b1b34f",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-2870 Fail hard and fast if we don't calculate a midpoint.\n\nBy failing early, we catch the change in functionality from FileUtil.findMidPoint\nwhich always returns a mid-point. Also prevents an NPE from unexpectedly being thrown.",
        "parent": "https://github.com/apache/accumulo/commit/9f3cbb30de0fa1115d70cacdccda4e290ad657be",
        "repo": "accumulo",
        "unit_tests": [
            "TabletTest.java"
        ]
    },
    "accumulo_6a44215": {
        "bug_id": "accumulo_6a44215",
        "commit": "https://github.com/apache/accumulo/commit/6a442154a12151fed83de549ef398a27a72ac8ab",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 10,
                "filename": "core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java",
                "patch": "@@ -30,9 +30,10 @@\n import org.apache.accumulo.core.conf.ConfigurationTypeHelper;\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.core.security.ColumnVisibility;\n-import org.apache.accumulo.core.trace.TraceUtil;\n+import org.apache.htrace.NullScope;\n import org.apache.htrace.Sampler;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n import org.apache.log4j.Level;\n import org.apache.log4j.Logger;\n \n@@ -138,21 +139,15 @@ public void startDebugLogging() {\n   @Parameter(names = \"--keytab\", description = \"Kerberos keytab on the local filesystem\")\n   private String keytabPath = null;\n \n-  public void startTracing(String applicationName) {\n-    if (trace) {\n-      Trace.startSpan(applicationName, Sampler.ALWAYS);\n-    }\n-  }\n-\n-  public void stopTracing() {\n-    TraceUtil.off();\n+  public TraceScope parseArgsAndTrace(String programName, String[] args, Object... others) {\n+    parseArgs(programName, args, others);\n+    return trace ? Trace.startSpan(programName, Sampler.ALWAYS) : NullScope.INSTANCE;\n   }\n \n   @Override\n   public void parseArgs(String programName, String[] args, Object... others) {\n     super.parseArgs(programName, args, others);\n     startDebugLogging();\n-    startTracing(programName);\n   }\n \n   private Properties cachedProps = null;",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/cli/ClientOpts.java",
                "sha": "bc39e00fb330f71db76275e993f851fba12da125",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 2,
                "filename": "core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java",
                "patch": "@@ -83,6 +83,7 @@\n import org.apache.commons.collections.map.LRUMap;\n import org.apache.commons.lang.mutable.MutableLong;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.Trace;\n import org.apache.thrift.TApplicationException;\n import org.apache.thrift.TException;\n import org.apache.thrift.TServiceClient;\n@@ -328,7 +329,7 @@ private void queue(String location, TabletServerMutations<QCMutation> mutations)\n       serverQueue.queue.add(mutations);\n       // never execute more than one task per server\n       if (!serverQueue.taskQueued) {\n-        threadPool.execute(new LoggingRunnable(log, TraceUtil.wrap(new SendTask(location))));\n+        threadPool.execute(new LoggingRunnable(log, Trace.wrap(new SendTask(location))));\n         serverQueue.taskQueued = true;\n       }\n     }\n@@ -347,7 +348,7 @@ private void reschedule(SendTask task) {\n \n     synchronized (serverQueue) {\n       if (serverQueue.queue.size() > 0)\n-        threadPool.execute(new LoggingRunnable(log, TraceUtil.wrap(task)));\n+        threadPool.execute(new LoggingRunnable(log, Trace.wrap(task)));\n       else\n         serverQueue.taskQueued = false;\n     }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ConditionalWriterImpl.java",
                "sha": "0561c1917b5f0e98b8a1cefde65cc84afacca4c8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 2,
                "filename": "core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java",
                "patch": "@@ -715,7 +715,7 @@ else if (Tables.getTableState(context, tableId) == TableState.OFFLINE)\n     void queueMutations(final MutationSet mutationsToSend) {\n       if (mutationsToSend == null)\n         return;\n-      binningThreadPool.execute(TraceUtil.wrap(() -> {\n+      binningThreadPool.execute(Trace.wrap(() -> {\n         if (mutationsToSend != null) {\n           try {\n             log.trace(\"{} - binning {} mutations\", Thread.currentThread().getName(),\n@@ -777,7 +777,7 @@ private synchronized void addMutations(\n \n       for (String server : servers)\n         if (!queued.contains(server)) {\n-          sendThreadPool.submit(TraceUtil.wrap(new SendTask(server)));\n+          sendThreadPool.submit(Trace.wrap(new SendTask(server)));\n           queued.add(server);\n         }\n     }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/TabletServerBatchWriter.java",
                "sha": "ad7b29bf4d206229fad2e3d89196f9198b229aad",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 4,
                "filename": "core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java",
                "patch": "@@ -16,8 +16,6 @@\n  */\n package org.apache.accumulo.core.clientImpl;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n import java.io.IOException;\n import java.security.SecureRandom;\n import java.util.ArrayList;\n@@ -316,8 +314,7 @@ else if (log.isTraceEnabled())\n \n         try (TraceScope scanLocation = Trace.startSpan(\"scan:location\")) {\n           if (scanLocation.getSpan() != null) {\n-            scanLocation.getSpan().addKVAnnotation(\"tserver\".getBytes(UTF_8),\n-                loc.tablet_location.getBytes(UTF_8));\n+            scanLocation.getSpan().addKVAnnotation(\"tserver\", loc.tablet_location);\n           }\n           results = scan(loc, scanState, context);\n         } catch (AccumuloSecurityException e) {",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftScanner.java",
                "sha": "7ca36cfeeccf8d74dc667eb10c953eee0922857a",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 28,
                "filename": "core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java",
                "patch": "@@ -42,10 +42,8 @@\n import org.apache.htrace.Trace;\n import org.apache.htrace.TraceInfo;\n import org.apache.htrace.TraceScope;\n-import org.apache.htrace.Tracer;\n import org.apache.htrace.impl.CountSampler;\n import org.apache.htrace.impl.ProbabilitySampler;\n-import org.apache.htrace.wrappers.TraceRunnable;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -116,9 +114,7 @@ private static void enableTracing(String hostname, String service, String spanRe\n     if (service != null) {\n       htraceConfigProps.put(TRACE_SERVICE_PROPERTY, service);\n     }\n-    Trace.setProcessId(service);\n     ShutdownHookManager.get().addShutdownHook(() -> {\n-      TraceUtil.off();\n       disable();\n     }, 0);\n     synchronized (receivers) {\n@@ -163,19 +159,6 @@ public static void disable() {\n     }\n   }\n \n-  /**\n-   * Finish the current trace.\n-   */\n-  public static void off() {\n-    Span span = Trace.currentSpan();\n-    if (span != null) {\n-      span.stop();\n-      // close() will no-op, but ensure safety if the implementation changes\n-      // the main reason for doing the following is to remove the current span in the current thread\n-      Tracer.getInstance().continueSpan(null).close();\n-    }\n-  }\n-\n   /**\n    * Continue a trace by starting a new span with a given parent and description.\n    */\n@@ -184,17 +167,6 @@ public static TraceScope trace(TInfo info, String description) {\n         : Trace.startSpan(description, new TraceInfo(info.traceId, info.parentId));\n   }\n \n-  /**\n-   * Wrap a runnable in a TraceRunnable, if tracing.\n-   */\n-  public static Runnable wrap(Runnable runnable) {\n-    if (Trace.isTracing()) {\n-      return new TraceRunnable(Trace.currentSpan(), runnable);\n-    } else {\n-      return runnable;\n-    }\n-  }\n-\n   private static final TInfo DONT_TRACE = new TInfo(0, 0);\n \n   /**",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/trace/TraceUtil.java",
                "sha": "16c73fa802e6b3edfe8c8236d1a38c6de22c8a09",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/util/Merge.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/util/Merge.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 15,
                "filename": "core/src/main/java/org/apache/accumulo/core/util/Merge.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n import org.apache.accumulo.core.metadata.schema.TabletsMetadata;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -79,24 +80,25 @@ public Text convert(String value) {\n \n   public void start(String[] args) throws MergeException {\n     Opts opts = new Opts();\n-    opts.parseArgs(Merge.class.getName(), args);\n+    try (TraceScope clientTrace = opts.parseArgsAndTrace(Merge.class.getName(), args)) {\n \n-    try (AccumuloClient client = opts.createClient()) {\n+      try (AccumuloClient client = opts.createClient()) {\n \n-      if (!client.tableOperations().exists(opts.getTableName())) {\n-        System.err.println(\"table \" + opts.getTableName() + \" does not exist\");\n-        return;\n-      }\n-      if (opts.goalSize == null || opts.goalSize < 1) {\n-        AccumuloConfiguration tableConfig = new ConfigurationCopy(\n-            client.tableOperations().getProperties(opts.getTableName()));\n-        opts.goalSize = tableConfig.getAsBytes(Property.TABLE_SPLIT_THRESHOLD);\n-      }\n+        if (!client.tableOperations().exists(opts.getTableName())) {\n+          System.err.println(\"table \" + opts.getTableName() + \" does not exist\");\n+          return;\n+        }\n+        if (opts.goalSize == null || opts.goalSize < 1) {\n+          AccumuloConfiguration tableConfig = new ConfigurationCopy(\n+              client.tableOperations().getProperties(opts.getTableName()));\n+          opts.goalSize = tableConfig.getAsBytes(Property.TABLE_SPLIT_THRESHOLD);\n+        }\n \n-      message(\"Merging tablets in table %s to %d bytes\", opts.getTableName(), opts.goalSize);\n-      mergomatic(client, opts.getTableName(), opts.begin, opts.end, opts.goalSize, opts.force);\n-    } catch (Exception ex) {\n-      throw new MergeException(ex);\n+        message(\"Merging tablets in table %s to %d bytes\", opts.getTableName(), opts.goalSize);\n+        mergomatic(client, opts.getTableName(), opts.begin, opts.end, opts.goalSize, opts.force);\n+      } catch (Exception ex) {\n+        throw new MergeException(ex);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/core/src/main/java/org/apache/accumulo/core/util/Merge.java",
                "sha": "9865782ccee7dc97d7d22b5392e046bbcb0e06f3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/pom.xml?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 1,
                "filename": "pom.xml",
                "patch": "@@ -126,7 +126,7 @@\n     <hadoop.version>3.1.1</hadoop.version>\n     <hk2.version>2.5.0-b62</hk2.version>\n     <htrace.hadoop.version>4.1.0-incubating</htrace.hadoop.version>\n-    <htrace.version>3.1.0-incubating</htrace.version>\n+    <htrace.version>3.2.0-incubating</htrace.version>\n     <it.failIfNoSpecifiedTests>false</it.failIfNoSpecifiedTests>\n     <jackson.version>2.9.8</jackson.version>\n     <javax.el.version>2.2.4</javax.el.version>",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/pom.xml",
                "sha": "df5140346b35e4f8fc2afc90fe1c069767a1212f",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 10,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java",
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.htrace.TraceScope;\n import org.apache.zookeeper.ZooDefs.Ids;\n import org.apache.zookeeper.data.ACL;\n import org.apache.zookeeper.data.Stat;\n@@ -69,19 +70,21 @@ public static void main(String[] args) throws Exception {\n     argsList.add(\"--old\");\n     argsList.add(\"--new\");\n     argsList.addAll(Arrays.asList(args));\n-    opts.parseArgs(ChangeSecret.class.getName(), argsList.toArray(new String[0]));\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(ChangeSecret.class.getName(),\n+        argsList.toArray(new String[0]))) {\n \n-    ServerContext context = opts.getServerContext();\n-    verifyAccumuloIsDown(context, opts.oldPass);\n+      ServerContext context = opts.getServerContext();\n+      verifyAccumuloIsDown(context, opts.oldPass);\n \n-    final String newInstanceId = UUID.randomUUID().toString();\n-    updateHdfs(fs, newInstanceId);\n-    rewriteZooKeeperInstance(context, newInstanceId, opts.oldPass, opts.newPass);\n-    if (opts.oldPass != null) {\n-      deleteInstance(context, opts.oldPass);\n+      final String newInstanceId = UUID.randomUUID().toString();\n+      updateHdfs(fs, newInstanceId);\n+      rewriteZooKeeperInstance(context, newInstanceId, opts.oldPass, opts.newPass);\n+      if (opts.oldPass != null) {\n+        deleteInstance(context, opts.oldPass);\n+      }\n+      System.out.println(\"New instance id is \" + newInstanceId);\n+      System.out.println(\"Be sure to put your new secret in accumulo.properties\");\n     }\n-    System.out.println(\"New instance id is \" + newInstanceId);\n-    System.out.println(\"Be sure to put your new secret in accumulo.properties\");\n   }\n \n   interface Visitor {",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/ChangeSecret.java",
                "sha": "3b7f16415c74923055236971a63e51e447ca83e1",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 6,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.accumulo.core.security.Authorizations;\n import org.apache.accumulo.server.cli.ServerUtilOpts;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n \n public class CheckForMetadataProblems {\n   private static boolean sawProblems = false;\n@@ -165,13 +166,14 @@ public static void checkMetadataAndRootTableEntries(String tableNameToCheck, Ser\n \n   public static void main(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(CheckForMetadataProblems.class.getName(), args);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(CheckForMetadataProblems.class.getName(),\n+        args)) {\n \n-    checkMetadataAndRootTableEntries(RootTable.NAME, opts);\n-    checkMetadataAndRootTableEntries(MetadataTable.NAME, opts);\n-    opts.stopTracing();\n-    if (sawProblems)\n-      throw new RuntimeException();\n+      checkMetadataAndRootTableEntries(RootTable.NAME, opts);\n+      checkMetadataAndRootTableEntries(MetadataTable.NAME, opts);\n+      if (sawProblems)\n+        throw new RuntimeException();\n+    }\n   }\n \n }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/CheckForMetadataProblems.java",
                "sha": "0fe3fdf44ef7e7cd1d70fb62ccb95d125ad55705",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 3,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.accumulo.server.master.state.TabletLocationState;\n import org.apache.accumulo.server.master.state.TabletState;\n import org.apache.accumulo.server.master.state.ZooTabletStateStore;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -46,9 +47,10 @@\n \n   public static void main(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(FindOfflineTablets.class.getName(), args);\n-    ServerContext context = opts.getServerContext();\n-    findOffline(context, null);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(FindOfflineTablets.class.getName(), args)) {\n+      ServerContext context = opts.getServerContext();\n+      findOffline(context, null);\n+    }\n   }\n \n   static int findOffline(ServerContext context, String tableName) throws TableNotFoundException {",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/FindOfflineTablets.java",
                "sha": "eb8c2ec8d73dc5920a9bc34dee1223ad6f7c8dbd",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 28,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java",
                "patch": "@@ -37,46 +37,48 @@\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.htrace.TraceScope;\n \n public class LocalityCheck {\n \n   public int run(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(LocalityCheck.class.getName(), args);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(LocalityCheck.class.getName(), args)) {\n \n-    VolumeManager fs = opts.getServerContext().getVolumeManager();\n-    try (AccumuloClient accumuloClient = opts.createClient()) {\n-      Scanner scanner = accumuloClient.createScanner(MetadataTable.NAME, Authorizations.EMPTY);\n-      scanner.fetchColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME);\n-      scanner.fetchColumnFamily(DataFileColumnFamily.NAME);\n-      scanner.setRange(MetadataSchema.TabletsSection.getRange());\n+      VolumeManager fs = opts.getServerContext().getVolumeManager();\n+      try (AccumuloClient accumuloClient = opts.createClient()) {\n+        Scanner scanner = accumuloClient.createScanner(MetadataTable.NAME, Authorizations.EMPTY);\n+        scanner.fetchColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME);\n+        scanner.fetchColumnFamily(DataFileColumnFamily.NAME);\n+        scanner.setRange(MetadataSchema.TabletsSection.getRange());\n \n-      Map<String,Long> totalBlocks = new HashMap<>();\n-      Map<String,Long> localBlocks = new HashMap<>();\n-      ArrayList<String> files = new ArrayList<>();\n+        Map<String,Long> totalBlocks = new HashMap<>();\n+        Map<String,Long> localBlocks = new HashMap<>();\n+        ArrayList<String> files = new ArrayList<>();\n \n-      for (Entry<Key,Value> entry : scanner) {\n-        Key key = entry.getKey();\n-        if (key.compareColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME) == 0) {\n-          String location = entry.getValue().toString();\n-          String[] parts = location.split(\":\");\n-          String host = parts[0];\n-          addBlocks(fs, host, files, totalBlocks, localBlocks);\n-          files.clear();\n-        } else if (key.compareColumnFamily(DataFileColumnFamily.NAME) == 0) {\n+        for (Entry<Key,Value> entry : scanner) {\n+          Key key = entry.getKey();\n+          if (key.compareColumnFamily(TabletsSection.CurrentLocationColumnFamily.NAME) == 0) {\n+            String location = entry.getValue().toString();\n+            String[] parts = location.split(\":\");\n+            String host = parts[0];\n+            addBlocks(fs, host, files, totalBlocks, localBlocks);\n+            files.clear();\n+          } else if (key.compareColumnFamily(DataFileColumnFamily.NAME) == 0) {\n \n-          files.add(fs.getFullPath(key).toString());\n+            files.add(fs.getFullPath(key).toString());\n+          }\n+        }\n+        System.out.println(\" Server         %local  total blocks\");\n+        for (Entry<String,Long> entry : totalBlocks.entrySet()) {\n+          final String host = entry.getKey();\n+          final Long blocksForHost = entry.getValue();\n+          System.out.println(String.format(\"%15s %5.1f %8d\", host,\n+              (localBlocks.get(host) * 100.) / blocksForHost, blocksForHost));\n         }\n       }\n-      System.out.println(\" Server         %local  total blocks\");\n-      for (Entry<String,Long> entry : totalBlocks.entrySet()) {\n-        final String host = entry.getKey();\n-        final Long blocksForHost = entry.getValue();\n-        System.out.println(String.format(\"%15s %5.1f %8d\", host,\n-            (localBlocks.get(host) * 100.) / blocksForHost, blocksForHost));\n-      }\n+      return 0;\n     }\n-    return 0;\n   }\n \n   private void addBlocks(VolumeManager fs, String host, ArrayList<String> files,",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/LocalityCheck.java",
                "sha": "8998323739333a81dd425d05f4f062dffc1ecf3e",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 17,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.accumulo.core.data.Mutation;\n import org.apache.accumulo.core.data.Value;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -96,24 +97,26 @@ public static void main(String[] args) throws Exception {\n     Opts opts = new Opts(table_name);\n     opts.setPrincipal(\"root\");\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n-    opts.parseArgs(RandomWriter.class.getName(), args, bwOpts);\n-\n-    long start = System.currentTimeMillis();\n-    log.info(\"starting at {} for user {}\", start, opts.getPrincipal());\n-    try (AccumuloClient accumuloClient = opts.createClient()) {\n-      BatchWriter bw = accumuloClient.createBatchWriter(opts.getTableName(),\n-          bwOpts.getBatchWriterConfig());\n-      log.info(\"Writing {} mutations...\", opts.count);\n-      bw.addMutations(new RandomMutationGenerator(opts.count));\n-      bw.close();\n-    } catch (Exception e) {\n-      log.error(\"{}\", e.getMessage(), e);\n-      throw e;\n-    }\n-    long stop = System.currentTimeMillis();\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(RandomWriter.class.getName(), args,\n+        bwOpts)) {\n+\n+      long start = System.currentTimeMillis();\n+      log.info(\"starting at {} for user {}\", start, opts.getPrincipal());\n+      try (AccumuloClient accumuloClient = opts.createClient()) {\n+        BatchWriter bw = accumuloClient.createBatchWriter(opts.getTableName(),\n+            bwOpts.getBatchWriterConfig());\n+        log.info(\"Writing {} mutations...\", opts.count);\n+        bw.addMutations(new RandomMutationGenerator(opts.count));\n+        bw.close();\n+      } catch (Exception e) {\n+        log.error(\"{}\", e.getMessage(), e);\n+        throw e;\n+      }\n+      long stop = System.currentTimeMillis();\n \n-    log.info(\"stopping at {}\", stop);\n-    log.info(\"elapsed: {}\", (((double) stop - (double) start) / 1000.0));\n+      log.info(\"stopping at {}\", stop);\n+      log.info(\"elapsed: {}\", (((double) stop - (double) start) / 1000.0));\n+    }\n   }\n \n }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomWriter.java",
                "sha": "56f60aa5012b6e54f381c9351babcedef0e864ea",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 8,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java",
                "patch": "@@ -46,6 +46,7 @@\n import org.apache.accumulo.server.fs.VolumeManager;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -54,14 +55,15 @@\n \n   public static void main(String[] args) {\n     ServerUtilOnRequiredTable opts = new ServerUtilOnRequiredTable();\n-    opts.parseArgs(RandomizeVolumes.class.getName(), args);\n-    ServerContext context = opts.getServerContext();\n-    try {\n-      int status = randomize(context, opts.getTableName());\n-      System.exit(status);\n-    } catch (Exception ex) {\n-      log.error(\"{}\", ex.getMessage(), ex);\n-      System.exit(4);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(RandomizeVolumes.class.getName(), args)) {\n+      ServerContext context = opts.getServerContext();\n+      try {\n+        int status = randomize(context, opts.getTableName());\n+        System.exit(status);\n+      } catch (Exception ex) {\n+        log.error(\"{}\", ex.getMessage(), ex);\n+        System.exit(4);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RandomizeVolumes.java",
                "sha": "e4cb695a9eab94fc611e2c81d2f8d34eb449a4f0",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 3,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java",
                "patch": "@@ -48,6 +48,7 @@\n import org.apache.accumulo.server.fs.VolumeManager;\n import org.apache.commons.collections.map.LRUMap;\n import org.apache.hadoop.fs.Path;\n+import org.apache.htrace.TraceScope;\n \n import com.beust.jcommander.Parameter;\n \n@@ -205,8 +206,9 @@ public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n     ScannerOpts scanOpts = new ScannerOpts();\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n-    opts.parseArgs(RemoveEntriesForMissingFiles.class.getName(), args, scanOpts, bwOpts);\n-\n-    checkAllTables(opts.getServerContext(), opts.fix);\n+    try (TraceScope clientSpan = opts\n+        .parseArgsAndTrace(RemoveEntriesForMissingFiles.class.getName(), args, scanOpts, bwOpts)) {\n+      checkAllTables(opts.getServerContext(), opts.fix);\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/RemoveEntriesForMissingFiles.java",
                "sha": "ebc51135035cc4e9948a54056bf694f3c46d29df",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 4,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java",
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.accumulo.server.fs.VolumeManager;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.Path;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -296,10 +297,12 @@ public static void printDiskUsage(Collection<String> tableNames, VolumeManager f\n \n   public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n-    opts.parseArgs(TableDiskUsage.class.getName(), args);\n-    try (AccumuloClient client = opts.createClient()) {\n-      VolumeManager fs = opts.getServerContext().getVolumeManager();\n-      org.apache.accumulo.server.util.TableDiskUsage.printDiskUsage(opts.tables, fs, client, false);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(TableDiskUsage.class.getName(), args)) {\n+      try (AccumuloClient client = opts.createClient()) {\n+        VolumeManager fs = opts.getServerContext().getVolumeManager();\n+        org.apache.accumulo.server.util.TableDiskUsage.printDiskUsage(opts.tables, fs, client,\n+            false);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/TableDiskUsage.java",
                "sha": "fa36a5d6b3adae6e62674418f5e55a80f6c09fb2",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 5,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java",
                "patch": "@@ -53,6 +53,7 @@\n import org.apache.accumulo.core.util.HostAndPort;\n import org.apache.accumulo.server.cli.ServerUtilOpts;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.thrift.TException;\n import org.apache.thrift.TServiceClient;\n import org.slf4j.Logger;\n@@ -71,11 +72,12 @@\n \n   public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n-    opts.parseArgs(VerifyTabletAssignments.class.getName(), args);\n-\n-    try (AccumuloClient client = opts.createClient()) {\n-      for (String table : client.tableOperations().list())\n-        checkTable((ClientContext) client, opts, table, null);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(VerifyTabletAssignments.class.getName(),\n+        args)) {\n+      try (AccumuloClient client = opts.createClient()) {\n+        for (String table : client.tableOperations().list())\n+          checkTable((ClientContext) client, opts, table, null);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/base/src/main/java/org/apache/accumulo/server/util/VerifyTabletAssignments.java",
                "sha": "4346eba41feb30bf2421488c9327b02fc37dceaf",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 49,
                "filename": "server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java",
                "patch": "@@ -522,67 +522,65 @@ private void run() {\n         .probabilitySampler(getConfiguration().getFraction(Property.GC_TRACE_PERCENT));\n \n     while (true) {\n-      Trace.startSpan(\"gc\", sampler);\n+      try (TraceScope gcOuterSpan = Trace.startSpan(\"gc\", sampler)) {\n+        try (TraceScope gcSpan = Trace.startSpan(\"loop\")) {\n+          tStart = System.currentTimeMillis();\n+          try {\n+            System.gc(); // make room\n \n-      try (TraceScope gcSpan = Trace.startSpan(\"loop\")) {\n-        tStart = System.currentTimeMillis();\n-        try {\n-          System.gc(); // make room\n+            status.current.started = System.currentTimeMillis();\n \n-          status.current.started = System.currentTimeMillis();\n+            new GarbageCollectionAlgorithm().collect(new GCEnv(RootTable.NAME));\n+            new GarbageCollectionAlgorithm().collect(new GCEnv(MetadataTable.NAME));\n \n-          new GarbageCollectionAlgorithm().collect(new GCEnv(RootTable.NAME));\n-          new GarbageCollectionAlgorithm().collect(new GCEnv(MetadataTable.NAME));\n+            log.info(\"Number of data file candidates for deletion: {}\", status.current.candidates);\n+            log.info(\"Number of data file candidates still in use: {}\", status.current.inUse);\n+            log.info(\"Number of successfully deleted data files: {}\", status.current.deleted);\n+            log.info(\"Number of data files delete failures: {}\", status.current.errors);\n \n-          log.info(\"Number of data file candidates for deletion: {}\", status.current.candidates);\n-          log.info(\"Number of data file candidates still in use: {}\", status.current.inUse);\n-          log.info(\"Number of successfully deleted data files: {}\", status.current.deleted);\n-          log.info(\"Number of data files delete failures: {}\", status.current.errors);\n+            status.current.finished = System.currentTimeMillis();\n+            status.last = status.current;\n+            status.current = new GcCycleStats();\n \n-          status.current.finished = System.currentTimeMillis();\n-          status.last = status.current;\n-          status.current = new GcCycleStats();\n+          } catch (Exception e) {\n+            log.error(\"{}\", e.getMessage(), e);\n+          }\n \n-        } catch (Exception e) {\n-          log.error(\"{}\", e.getMessage(), e);\n-        }\n+          tStop = System.currentTimeMillis();\n+          log.info(String.format(\"Collect cycle took %.2f seconds\", ((tStop - tStart) / 1000.0)));\n+\n+          /*\n+           * We want to prune references to fully-replicated WALs from the replication table which\n+           * are no longer referenced in the metadata table before running\n+           * GarbageCollectWriteAheadLogs to ensure we delete as many files as possible.\n+           */\n+          try (TraceScope replSpan = Trace.startSpan(\"replicationClose\")) {\n+            CloseWriteAheadLogReferences closeWals = new CloseWriteAheadLogReferences(context);\n+            closeWals.run();\n+          } catch (Exception e) {\n+            log.error(\"Error trying to close write-ahead logs for replication table\", e);\n+          }\n \n-        tStop = System.currentTimeMillis();\n-        log.info(String.format(\"Collect cycle took %.2f seconds\", ((tStop - tStart) / 1000.0)));\n-\n-        /*\n-         * We want to prune references to fully-replicated WALs from the replication table which are\n-         * no longer referenced in the metadata table before running GarbageCollectWriteAheadLogs to\n-         * ensure we delete as many files as possible.\n-         */\n-        try (TraceScope replSpan = Trace.startSpan(\"replicationClose\")) {\n-          CloseWriteAheadLogReferences closeWals = new CloseWriteAheadLogReferences(context);\n-          closeWals.run();\n-        } catch (Exception e) {\n-          log.error(\"Error trying to close write-ahead logs for replication table\", e);\n+          // Clean up any unused write-ahead logs\n+          try (TraceScope waLogs = Trace.startSpan(\"walogs\")) {\n+            GarbageCollectWriteAheadLogs walogCollector = new GarbageCollectWriteAheadLogs(context,\n+                fs, isUsingTrash());\n+            log.info(\"Beginning garbage collection of write-ahead logs\");\n+            walogCollector.collect(status);\n+          } catch (Exception e) {\n+            log.error(\"{}\", e.getMessage(), e);\n+          }\n         }\n \n-        // Clean up any unused write-ahead logs\n-        try (TraceScope waLogs = Trace.startSpan(\"walogs\")) {\n-          GarbageCollectWriteAheadLogs walogCollector = new GarbageCollectWriteAheadLogs(context,\n-              fs, isUsingTrash());\n-          log.info(\"Beginning garbage collection of write-ahead logs\");\n-          walogCollector.collect(status);\n+        // we just made a lot of metadata changes: flush them out\n+        try {\n+          AccumuloClient accumuloClient = getClient();\n+          accumuloClient.tableOperations().compact(MetadataTable.NAME, null, null, true, true);\n+          accumuloClient.tableOperations().compact(RootTable.NAME, null, null, true, true);\n         } catch (Exception e) {\n-          log.error(\"{}\", e.getMessage(), e);\n+          log.warn(\"{}\", e.getMessage(), e);\n         }\n       }\n-\n-      // we just made a lot of metadata changes: flush them out\n-      try {\n-        AccumuloClient accumuloClient = getClient();\n-        accumuloClient.tableOperations().compact(MetadataTable.NAME, null, null, true, true);\n-        accumuloClient.tableOperations().compact(RootTable.NAME, null, null, true, true);\n-      } catch (Exception e) {\n-        log.warn(\"{}\", e.getMessage(), e);\n-      }\n-\n-      TraceUtil.off();\n       try {\n         long gcDelay = getConfiguration().getTimeInMillis(Property.GC_CYCLE_DELAY);\n         log.debug(\"Sleeping for {} milliseconds\", gcDelay);",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/gc/src/main/java/org/apache/accumulo/gc/SimpleGarbageCollector.java",
                "sha": "4f9e72ac0aab1a7e869ccc875121505093353f78",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 34,
                "filename": "server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.accumulo.fate.util.UtilWaitThread;\n import org.apache.accumulo.master.Master;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n import org.apache.htrace.impl.ProbabilitySampler;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -70,43 +71,44 @@ public void run() {\n         rcrr = new RemoveCompleteReplicationRecords(client);\n       }\n \n-      Trace.startSpan(\"masterReplicationDriver\", sampler);\n+      try (TraceScope replicationDriver = Trace.startSpan(\"masterReplicationDriver\", sampler)) {\n+\n+        // Make status markers from replication records in metadata, removing entries in\n+        // metadata which are no longer needed (closed records)\n+        // This will end up creating the replication table too\n+        try {\n+          statusMaker.run();\n+        } catch (Exception e) {\n+          log.error(\"Caught Exception trying to create Replication status records\", e);\n+        }\n+\n+        // Tell the work maker to make work\n+        try {\n+          workMaker.run();\n+        } catch (Exception e) {\n+          log.error(\"Caught Exception trying to create Replication work records\", e);\n+        }\n+\n+        // Update the status records from the work records\n+        try {\n+          finishedWorkUpdater.run();\n+        } catch (Exception e) {\n+          log.error(\n+              \"Caught Exception trying to update Replication records using finished work records\",\n+              e);\n+        }\n+\n+        // Clean up records we no longer need.\n+        // It must be running at the same time as the StatusMaker or WorkMaker\n+        // So it's important that we run these sequentially and not concurrently\n+        try {\n+          rcrr.run();\n+        } catch (Exception e) {\n+          log.error(\"Caught Exception trying to remove finished Replication records\", e);\n+        }\n \n-      // Make status markers from replication records in metadata, removing entries in\n-      // metadata which are no longer needed (closed records)\n-      // This will end up creating the replication table too\n-      try {\n-        statusMaker.run();\n-      } catch (Exception e) {\n-        log.error(\"Caught Exception trying to create Replication status records\", e);\n-      }\n-\n-      // Tell the work maker to make work\n-      try {\n-        workMaker.run();\n-      } catch (Exception e) {\n-        log.error(\"Caught Exception trying to create Replication work records\", e);\n       }\n \n-      // Update the status records from the work records\n-      try {\n-        finishedWorkUpdater.run();\n-      } catch (Exception e) {\n-        log.error(\n-            \"Caught Exception trying to update Replication records using finished work records\", e);\n-      }\n-\n-      // Clean up records we no longer need.\n-      // It must be running at the same time as the StatusMaker or WorkMaker\n-      // So it's important that we run these sequentially and not concurrently\n-      try {\n-        rcrr.run();\n-      } catch (Exception e) {\n-        log.error(\"Caught Exception trying to remove finished Replication records\", e);\n-      }\n-\n-      TraceUtil.off();\n-\n       // Sleep for a bit\n       long sleepMillis = conf.getTimeInMillis(Property.MASTER_REPLICATION_SCAN_INTERVAL);\n       log.debug(\"Sleeping for {}ms before re-running\", sleepMillis);",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/replication/ReplicationDriver.java",
                "sha": "f648006c5c7a956a07c9296516a7204fcf93e441",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 16,
                "filename": "server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java",
                "patch": "@@ -46,6 +46,7 @@\n import org.apache.accumulo.server.master.state.TabletState;\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.zookeeper.data.Stat;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -260,24 +261,25 @@ private boolean verifyMergeConsistency(AccumuloClient accumuloClient, CurrentSta\n \n   public static void main(String[] args) throws Exception {\n     ServerUtilOpts opts = new ServerUtilOpts();\n-    opts.parseArgs(MergeStats.class.getName(), args);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(MergeStats.class.getName(), args)) {\n \n-    try (AccumuloClient client = opts.createClient()) {\n-      Map<String,String> tableIdMap = client.tableOperations().tableIdMap();\n-      ZooReaderWriter zooReaderWriter = opts.getServerContext().getZooReaderWriter();\n-      for (Entry<String,String> entry : tableIdMap.entrySet()) {\n-        final String table = entry.getKey(), tableId = entry.getValue();\n-        String path = ZooUtil.getRoot(client.instanceOperations().getInstanceID())\n-            + Constants.ZTABLES + \"/\" + tableId + \"/merge\";\n-        MergeInfo info = new MergeInfo();\n-        if (zooReaderWriter.exists(path)) {\n-          byte[] data = zooReaderWriter.getData(path, new Stat());\n-          DataInputBuffer in = new DataInputBuffer();\n-          in.reset(data, data.length);\n-          info.readFields(in);\n+      try (AccumuloClient client = opts.createClient()) {\n+        Map<String,String> tableIdMap = client.tableOperations().tableIdMap();\n+        ZooReaderWriter zooReaderWriter = opts.getServerContext().getZooReaderWriter();\n+        for (Entry<String,String> entry : tableIdMap.entrySet()) {\n+          final String table = entry.getKey(), tableId = entry.getValue();\n+          String path = ZooUtil.getRoot(client.instanceOperations().getInstanceID())\n+              + Constants.ZTABLES + \"/\" + tableId + \"/merge\";\n+          MergeInfo info = new MergeInfo();\n+          if (zooReaderWriter.exists(path)) {\n+            byte[] data = zooReaderWriter.getData(path, new Stat());\n+            DataInputBuffer in = new DataInputBuffer();\n+            in.reset(data, data.length);\n+            info.readFields(in);\n+          }\n+          System.out.println(String.format(\"%25s  %10s %10s %s\", table, info.getState(),\n+              info.getOperation(), info.getExtent()));\n         }\n-        System.out.println(String.format(\"%25s  %10s %10s %s\", table, info.getState(),\n-            info.getOperation(), info.getExtent()));\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/master/src/main/java/org/apache/accumulo/master/state/MergeStats.java",
                "sha": "efd9422934e2e7c4f58a741e4911a665002a3ef7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 2,
                "filename": "server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java",
                "patch": "@@ -26,8 +26,8 @@\n \n public class ShowTraceLinkTypeTest {\n   private static RemoteSpan rs(long start, long stop, String description) {\n-    return new RemoteSpan(\"sender\", \"svc\", 0L, 0L, 0L, start, stop, description,\n-        Collections.emptyMap(), Collections.emptyList());\n+    return new RemoteSpan(\"sender\", \"svc\", 0L, 0L, Collections.singletonList(0L), start, stop,\n+        description, Collections.emptyMap(), Collections.emptyList());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/monitor/src/test/java/org/apache/accumulo/monitor/ShowTraceLinkTypeTest.java",
                "sha": "54b44a10628b0a385ffc9e1c37846662254cb7b7",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 3,
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java",
                "patch": "@@ -43,6 +43,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.primitives.Longs;\n+\n /**\n  * Deliver Span information periodically to a destination.\n  * <ul>\n@@ -175,7 +177,7 @@ public void receiveSpan(Span s) {\n       return;\n     }\n \n-    Map<String,String> data = convertToStrings(s.getKVAnnotations());\n+    Map<String,String> data = s.getKVAnnotations();\n \n     SpanKey dest = getSpanKey(data);\n     if (dest != null) {\n@@ -192,8 +194,8 @@ public void receiveSpan(Span s) {\n         return;\n       }\n       sendQueue.add(new RemoteSpan(host, service == null ? processId : service, s.getTraceId(),\n-          s.getSpanId(), s.getParentId(), s.getStartTimeMillis(), s.getStopTimeMillis(),\n-          s.getDescription(), data, annotations));\n+          s.getSpanId(), Longs.asList(s.getParents()), s.getStartTimeMillis(),\n+          s.getStopTimeMillis(), s.getDescription(), data, annotations));\n       sendQueueSize.incrementAndGet();\n     }\n   }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/AsyncSpanReceiver.java",
                "sha": "fde4a326e9e1bbab6987ec1bc6b669cf0f1df5bb",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 7,
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java",
                "patch": "@@ -24,25 +24,27 @@\n import java.util.Set;\n \n import org.apache.accumulo.tracer.thrift.RemoteSpan;\n-import org.apache.htrace.Span;\n \n public class SpanTree {\n   final Map<Long,List<Long>> parentChildren = new HashMap<>();\n   public final Map<Long,RemoteSpan> nodes = new HashMap<>();\n+  private final List<Long> rootSpans = new ArrayList<>();\n \n   public void addNode(RemoteSpan span) {\n     nodes.put(span.spanId, span);\n-    if (parentChildren.get(span.parentId) == null)\n-      parentChildren.put(span.parentId, new ArrayList<>());\n-    parentChildren.get(span.parentId).add(span.spanId);\n+    if (span.getParentIdsSize() == 0) {\n+      rootSpans.add(span.spanId);\n+    }\n+    for (Long parentId : span.getParentIds()) {\n+      parentChildren.computeIfAbsent(parentId, id -> new ArrayList<>()).add(span.spanId);\n+    }\n   }\n \n   public Set<Long> visit(SpanTreeVisitor visitor) {\n     Set<Long> visited = new HashSet<>();\n-    List<Long> root = parentChildren.get(Span.ROOT_SPAN_ID);\n-    if (root == null || root.isEmpty())\n+    if (rootSpans.isEmpty())\n       return visited;\n-    RemoteSpan rootSpan = nodes.get(root.iterator().next());\n+    RemoteSpan rootSpan = nodes.get(rootSpans.iterator().next());\n     if (rootSpan == null)\n       return visited;\n     recurse(0, rootSpan, visitor, visited);",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/SpanTree.java",
                "sha": "90dec989849b04036a9c267e9aaed49ad39e645b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 2,
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java",
                "patch": "@@ -36,7 +36,6 @@\n import org.apache.accumulo.core.data.Value;\n import org.apache.accumulo.tracer.thrift.RemoteSpan;\n import org.apache.hadoop.io.Text;\n-import org.apache.htrace.Span;\n \n import com.beust.jcommander.Parameter;\n \n@@ -126,7 +125,7 @@ public static int printTrace(Scanner scanner, final Printer out) {\n       RemoteSpan span = TraceFormatter.getRemoteSpan(entry);\n       tree.addNode(span);\n       start = min(start, span.start);\n-      if (span.parentId == Span.ROOT_SPAN_ID)\n+      if (span.getParentIdsSize() == 0)\n         count++;\n     }\n     if (start == Long.MAX_VALUE) {",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceDump.java",
                "sha": "76e1a7e48f94a261c33c37727be1701310a1d76a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 1,
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.util.Date;\n import java.util.Iterator;\n import java.util.Map.Entry;\n+import java.util.stream.Collectors;\n \n import org.apache.accumulo.core.data.Key;\n import org.apache.accumulo.core.data.Value;\n@@ -83,7 +84,10 @@ public String next() {\n       result.append(String.format(\" %12s:%s%n\", \"trace\", Long.toHexString(span.traceId)));\n       result.append(String.format(\" %12s:%s%n\", \"loc\", span.svc + \"@\" + span.sender));\n       result.append(String.format(\" %12s:%s%n\", \"span\", Long.toHexString(span.spanId)));\n-      result.append(String.format(\" %12s:%s%n\", \"parent\", Long.toHexString(span.parentId)));\n+      String parentString = span.getParentIdsSize() == 0 ? \"\"\n+          : span.getParentIds().stream().map(x -> Long.toHexString(x)).collect(Collectors.toList())\n+              .toString();\n+      result.append(String.format(\" %12s:%s%n\", \"parent\", parentString));\n       result.append(String.format(\" %12s:%s%n\", \"start\", dateFormatter.format(span.start)));\n       result.append(String.format(\" %12s:%s%n\", \"ms\", span.stop - span.start));\n       if (span.data != null) {",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceFormatter.java",
                "sha": "9700482e69eecd61be2b1d32156d2386651ccf88",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 5,
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java",
                "patch": "@@ -28,6 +28,7 @@\n import java.util.Map.Entry;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n \n import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.Accumulo;\n@@ -63,7 +64,6 @@\n import org.apache.accumulo.tracer.thrift.SpanReceiver.Iface;\n import org.apache.accumulo.tracer.thrift.SpanReceiver.Processor;\n import org.apache.hadoop.io.Text;\n-import org.apache.htrace.Span;\n import org.apache.thrift.TByteArrayOutputStream;\n import org.apache.thrift.TException;\n import org.apache.thrift.protocol.TCompactProtocol;\n@@ -149,14 +149,14 @@ public void span(RemoteSpan s) throws TException {\n       ByteArrayTransport transport = new ByteArrayTransport();\n       TCompactProtocol protocol = new TCompactProtocol(transport);\n       s.write(protocol);\n-      String parentString = Long.toHexString(s.parentId);\n-      if (s.parentId == Span.ROOT_SPAN_ID)\n-        parentString = \"\";\n+      String parentString = s.getParentIdsSize() == 0 ? \"\"\n+          : s.getParentIds().stream().map(x -> Long.toHexString(x)).collect(Collectors.toList())\n+              .toString();\n       put(spanMutation, \"span\", parentString + \":\" + Long.toHexString(s.spanId), transport.get(),\n           transport.len());\n       // Map the root span to time so we can look up traces by time\n       Mutation timeMutation = null;\n-      if (s.parentId == Span.ROOT_SPAN_ID) {\n+      if (s.getParentIdsSize() == 0) {\n         timeMutation = new Mutation(new Text(\"start:\" + startString));\n         put(timeMutation, \"id\", idString, transport.get(), transport.len());\n       }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/TraceServer.java",
                "sha": "da0f74ebf48d6c1efffaf31263ad37d3a5daf4bb",
                "status": "modified"
            },
            {
                "additions": 155,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java",
                "changes": 253,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 98,
                "filename": "server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java",
                "patch": "@@ -31,7 +31,7 @@\n   private static final org.apache.thrift.protocol.TField SVC_FIELD_DESC = new org.apache.thrift.protocol.TField(\"svc\", org.apache.thrift.protocol.TType.STRING, (short)2);\n   private static final org.apache.thrift.protocol.TField TRACE_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"traceId\", org.apache.thrift.protocol.TType.I64, (short)3);\n   private static final org.apache.thrift.protocol.TField SPAN_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"spanId\", org.apache.thrift.protocol.TType.I64, (short)4);\n-  private static final org.apache.thrift.protocol.TField PARENT_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"parentId\", org.apache.thrift.protocol.TType.I64, (short)5);\n+  private static final org.apache.thrift.protocol.TField PARENT_IDS_FIELD_DESC = new org.apache.thrift.protocol.TField(\"parentIds\", org.apache.thrift.protocol.TType.LIST, (short)11);\n   private static final org.apache.thrift.protocol.TField START_FIELD_DESC = new org.apache.thrift.protocol.TField(\"start\", org.apache.thrift.protocol.TType.I64, (short)6);\n   private static final org.apache.thrift.protocol.TField STOP_FIELD_DESC = new org.apache.thrift.protocol.TField(\"stop\", org.apache.thrift.protocol.TType.I64, (short)7);\n   private static final org.apache.thrift.protocol.TField DESCRIPTION_FIELD_DESC = new org.apache.thrift.protocol.TField(\"description\", org.apache.thrift.protocol.TType.STRING, (short)8);\n@@ -45,7 +45,7 @@\n   public java.lang.String svc; // required\n   public long traceId; // required\n   public long spanId; // required\n-  public long parentId; // required\n+  public java.util.List<java.lang.Long> parentIds; // required\n   public long start; // required\n   public long stop; // required\n   public java.lang.String description; // required\n@@ -58,7 +58,7 @@\n     SVC((short)2, \"svc\"),\n     TRACE_ID((short)3, \"traceId\"),\n     SPAN_ID((short)4, \"spanId\"),\n-    PARENT_ID((short)5, \"parentId\"),\n+    PARENT_IDS((short)11, \"parentIds\"),\n     START((short)6, \"start\"),\n     STOP((short)7, \"stop\"),\n     DESCRIPTION((short)8, \"description\"),\n@@ -86,8 +86,8 @@ public static _Fields findByThriftId(int fieldId) {\n           return TRACE_ID;\n         case 4: // SPAN_ID\n           return SPAN_ID;\n-        case 5: // PARENT_ID\n-          return PARENT_ID;\n+        case 11: // PARENT_IDS\n+          return PARENT_IDS;\n         case 6: // START\n           return START;\n         case 7: // STOP\n@@ -140,9 +140,8 @@ public short getThriftFieldId() {\n   // isset id assignments\n   private static final int __TRACEID_ISSET_ID = 0;\n   private static final int __SPANID_ISSET_ID = 1;\n-  private static final int __PARENTID_ISSET_ID = 2;\n-  private static final int __START_ISSET_ID = 3;\n-  private static final int __STOP_ISSET_ID = 4;\n+  private static final int __START_ISSET_ID = 2;\n+  private static final int __STOP_ISSET_ID = 3;\n   private byte __isset_bitfield = 0;\n   public static final java.util.Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;\n   static {\n@@ -155,8 +154,9 @@ public short getThriftFieldId() {\n         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n     tmpMap.put(_Fields.SPAN_ID, new org.apache.thrift.meta_data.FieldMetaData(\"spanId\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n-    tmpMap.put(_Fields.PARENT_ID, new org.apache.thrift.meta_data.FieldMetaData(\"parentId\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n-        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n+    tmpMap.put(_Fields.PARENT_IDS, new org.apache.thrift.meta_data.FieldMetaData(\"parentIds\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n+        new org.apache.thrift.meta_data.ListMetaData(org.apache.thrift.protocol.TType.LIST, \n+            new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64))));\n     tmpMap.put(_Fields.START, new org.apache.thrift.meta_data.FieldMetaData(\"start\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));\n     tmpMap.put(_Fields.STOP, new org.apache.thrift.meta_data.FieldMetaData(\"stop\", org.apache.thrift.TFieldRequirementType.DEFAULT, \n@@ -182,7 +182,7 @@ public RemoteSpan(\n     java.lang.String svc,\n     long traceId,\n     long spanId,\n-    long parentId,\n+    java.util.List<java.lang.Long> parentIds,\n     long start,\n     long stop,\n     java.lang.String description,\n@@ -196,8 +196,7 @@ public RemoteSpan(\n     setTraceIdIsSet(true);\n     this.spanId = spanId;\n     setSpanIdIsSet(true);\n-    this.parentId = parentId;\n-    setParentIdIsSet(true);\n+    this.parentIds = parentIds;\n     this.start = start;\n     setStartIsSet(true);\n     this.stop = stop;\n@@ -220,7 +219,10 @@ public RemoteSpan(RemoteSpan other) {\n     }\n     this.traceId = other.traceId;\n     this.spanId = other.spanId;\n-    this.parentId = other.parentId;\n+    if (other.isSetParentIds()) {\n+      java.util.List<java.lang.Long> __this__parentIds = new java.util.ArrayList<java.lang.Long>(other.parentIds);\n+      this.parentIds = __this__parentIds;\n+    }\n     this.start = other.start;\n     this.stop = other.stop;\n     if (other.isSetDescription()) {\n@@ -251,8 +253,7 @@ public void clear() {\n     this.traceId = 0;\n     setSpanIdIsSet(false);\n     this.spanId = 0;\n-    setParentIdIsSet(false);\n-    this.parentId = 0;\n+    this.parentIds = null;\n     setStartIsSet(false);\n     this.start = 0;\n     setStopIsSet(false);\n@@ -356,27 +357,43 @@ public void setSpanIdIsSet(boolean value) {\n     __isset_bitfield = org.apache.thrift.EncodingUtils.setBit(__isset_bitfield, __SPANID_ISSET_ID, value);\n   }\n \n-  public long getParentId() {\n-    return this.parentId;\n+  public int getParentIdsSize() {\n+    return (this.parentIds == null) ? 0 : this.parentIds.size();\n+  }\n+\n+  public java.util.Iterator<java.lang.Long> getParentIdsIterator() {\n+    return (this.parentIds == null) ? null : this.parentIds.iterator();\n+  }\n+\n+  public void addToParentIds(long elem) {\n+    if (this.parentIds == null) {\n+      this.parentIds = new java.util.ArrayList<java.lang.Long>();\n+    }\n+    this.parentIds.add(elem);\n+  }\n+\n+  public java.util.List<java.lang.Long> getParentIds() {\n+    return this.parentIds;\n   }\n \n-  public RemoteSpan setParentId(long parentId) {\n-    this.parentId = parentId;\n-    setParentIdIsSet(true);\n+  public RemoteSpan setParentIds(java.util.List<java.lang.Long> parentIds) {\n+    this.parentIds = parentIds;\n     return this;\n   }\n \n-  public void unsetParentId() {\n-    __isset_bitfield = org.apache.thrift.EncodingUtils.clearBit(__isset_bitfield, __PARENTID_ISSET_ID);\n+  public void unsetParentIds() {\n+    this.parentIds = null;\n   }\n \n-  /** Returns true if field parentId is set (has been assigned a value) and false otherwise */\n-  public boolean isSetParentId() {\n-    return org.apache.thrift.EncodingUtils.testBit(__isset_bitfield, __PARENTID_ISSET_ID);\n+  /** Returns true if field parentIds is set (has been assigned a value) and false otherwise */\n+  public boolean isSetParentIds() {\n+    return this.parentIds != null;\n   }\n \n-  public void setParentIdIsSet(boolean value) {\n-    __isset_bitfield = org.apache.thrift.EncodingUtils.setBit(__isset_bitfield, __PARENTID_ISSET_ID, value);\n+  public void setParentIdsIsSet(boolean value) {\n+    if (!value) {\n+      this.parentIds = null;\n+    }\n   }\n \n   public long getStart() {\n@@ -557,11 +574,11 @@ public void setFieldValue(_Fields field, java.lang.Object value) {\n       }\n       break;\n \n-    case PARENT_ID:\n+    case PARENT_IDS:\n       if (value == null) {\n-        unsetParentId();\n+        unsetParentIds();\n       } else {\n-        setParentId((java.lang.Long)value);\n+        setParentIds((java.util.List<java.lang.Long>)value);\n       }\n       break;\n \n@@ -622,8 +639,8 @@ public void setFieldValue(_Fields field, java.lang.Object value) {\n     case SPAN_ID:\n       return getSpanId();\n \n-    case PARENT_ID:\n-      return getParentId();\n+    case PARENT_IDS:\n+      return getParentIds();\n \n     case START:\n       return getStart();\n@@ -659,8 +676,8 @@ public boolean isSet(_Fields field) {\n       return isSetTraceId();\n     case SPAN_ID:\n       return isSetSpanId();\n-    case PARENT_ID:\n-      return isSetParentId();\n+    case PARENT_IDS:\n+      return isSetParentIds();\n     case START:\n       return isSetStart();\n     case STOP:\n@@ -726,12 +743,12 @@ public boolean equals(RemoteSpan that) {\n         return false;\n     }\n \n-    boolean this_present_parentId = true;\n-    boolean that_present_parentId = true;\n-    if (this_present_parentId || that_present_parentId) {\n-      if (!(this_present_parentId && that_present_parentId))\n+    boolean this_present_parentIds = true && this.isSetParentIds();\n+    boolean that_present_parentIds = true && that.isSetParentIds();\n+    if (this_present_parentIds || that_present_parentIds) {\n+      if (!(this_present_parentIds && that_present_parentIds))\n         return false;\n-      if (this.parentId != that.parentId)\n+      if (!this.parentIds.equals(that.parentIds))\n         return false;\n     }\n \n@@ -799,7 +816,9 @@ public int hashCode() {\n \n     hashCode = hashCode * 8191 + org.apache.thrift.TBaseHelper.hashCode(spanId);\n \n-    hashCode = hashCode * 8191 + org.apache.thrift.TBaseHelper.hashCode(parentId);\n+    hashCode = hashCode * 8191 + ((isSetParentIds()) ? 131071 : 524287);\n+    if (isSetParentIds())\n+      hashCode = hashCode * 8191 + parentIds.hashCode();\n \n     hashCode = hashCode * 8191 + org.apache.thrift.TBaseHelper.hashCode(start);\n \n@@ -868,12 +887,12 @@ public int compareTo(RemoteSpan other) {\n         return lastComparison;\n       }\n     }\n-    lastComparison = java.lang.Boolean.valueOf(isSetParentId()).compareTo(other.isSetParentId());\n+    lastComparison = java.lang.Boolean.valueOf(isSetParentIds()).compareTo(other.isSetParentIds());\n     if (lastComparison != 0) {\n       return lastComparison;\n     }\n-    if (isSetParentId()) {\n-      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.parentId, other.parentId);\n+    if (isSetParentIds()) {\n+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.parentIds, other.parentIds);\n       if (lastComparison != 0) {\n         return lastComparison;\n       }\n@@ -972,8 +991,12 @@ public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.\n     sb.append(this.spanId);\n     first = false;\n     if (!first) sb.append(\", \");\n-    sb.append(\"parentId:\");\n-    sb.append(this.parentId);\n+    sb.append(\"parentIds:\");\n+    if (this.parentIds == null) {\n+      sb.append(\"null\");\n+    } else {\n+      sb.append(this.parentIds);\n+    }\n     first = false;\n     if (!first) sb.append(\", \");\n     sb.append(\"start:\");\n@@ -1084,10 +1107,20 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, RemoteSpan struct)\n               org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);\n             }\n             break;\n-          case 5: // PARENT_ID\n-            if (schemeField.type == org.apache.thrift.protocol.TType.I64) {\n-              struct.parentId = iprot.readI64();\n-              struct.setParentIdIsSet(true);\n+          case 11: // PARENT_IDS\n+            if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {\n+              {\n+                org.apache.thrift.protocol.TList _list0 = iprot.readListBegin();\n+                struct.parentIds = new java.util.ArrayList<java.lang.Long>(_list0.size);\n+                long _elem1;\n+                for (int _i2 = 0; _i2 < _list0.size; ++_i2)\n+                {\n+                  _elem1 = iprot.readI64();\n+                  struct.parentIds.add(_elem1);\n+                }\n+                iprot.readListEnd();\n+              }\n+              struct.setParentIdsIsSet(true);\n             } else { \n               org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);\n             }\n@@ -1119,15 +1152,15 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, RemoteSpan struct)\n           case 9: // DATA\n             if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {\n               {\n-                org.apache.thrift.protocol.TMap _map0 = iprot.readMapBegin();\n-                struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map0.size);\n-                java.lang.String _key1;\n-                java.lang.String _val2;\n-                for (int _i3 = 0; _i3 < _map0.size; ++_i3)\n+                org.apache.thrift.protocol.TMap _map3 = iprot.readMapBegin();\n+                struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map3.size);\n+                java.lang.String _key4;\n+                java.lang.String _val5;\n+                for (int _i6 = 0; _i6 < _map3.size; ++_i6)\n                 {\n-                  _key1 = iprot.readString();\n-                  _val2 = iprot.readString();\n-                  struct.data.put(_key1, _val2);\n+                  _key4 = iprot.readString();\n+                  _val5 = iprot.readString();\n+                  struct.data.put(_key4, _val5);\n                 }\n                 iprot.readMapEnd();\n               }\n@@ -1139,14 +1172,14 @@ public void read(org.apache.thrift.protocol.TProtocol iprot, RemoteSpan struct)\n           case 10: // ANNOTATIONS\n             if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {\n               {\n-                org.apache.thrift.protocol.TList _list4 = iprot.readListBegin();\n-                struct.annotations = new java.util.ArrayList<Annotation>(_list4.size);\n-                Annotation _elem5;\n-                for (int _i6 = 0; _i6 < _list4.size; ++_i6)\n+                org.apache.thrift.protocol.TList _list7 = iprot.readListBegin();\n+                struct.annotations = new java.util.ArrayList<Annotation>(_list7.size);\n+                Annotation _elem8;\n+                for (int _i9 = 0; _i9 < _list7.size; ++_i9)\n                 {\n-                  _elem5 = new Annotation();\n-                  _elem5.read(iprot);\n-                  struct.annotations.add(_elem5);\n+                  _elem8 = new Annotation();\n+                  _elem8.read(iprot);\n+                  struct.annotations.add(_elem8);\n                 }\n                 iprot.readListEnd();\n               }\n@@ -1186,9 +1219,6 @@ public void write(org.apache.thrift.protocol.TProtocol oprot, RemoteSpan struct)\n       oprot.writeFieldBegin(SPAN_ID_FIELD_DESC);\n       oprot.writeI64(struct.spanId);\n       oprot.writeFieldEnd();\n-      oprot.writeFieldBegin(PARENT_ID_FIELD_DESC);\n-      oprot.writeI64(struct.parentId);\n-      oprot.writeFieldEnd();\n       oprot.writeFieldBegin(START_FIELD_DESC);\n       oprot.writeI64(struct.start);\n       oprot.writeFieldEnd();\n@@ -1204,10 +1234,10 @@ public void write(org.apache.thrift.protocol.TProtocol oprot, RemoteSpan struct)\n         oprot.writeFieldBegin(DATA_FIELD_DESC);\n         {\n           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.data.size()));\n-          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter7 : struct.data.entrySet())\n+          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter10 : struct.data.entrySet())\n           {\n-            oprot.writeString(_iter7.getKey());\n-            oprot.writeString(_iter7.getValue());\n+            oprot.writeString(_iter10.getKey());\n+            oprot.writeString(_iter10.getValue());\n           }\n           oprot.writeMapEnd();\n         }\n@@ -1217,9 +1247,21 @@ public void write(org.apache.thrift.protocol.TProtocol oprot, RemoteSpan struct)\n         oprot.writeFieldBegin(ANNOTATIONS_FIELD_DESC);\n         {\n           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.annotations.size()));\n-          for (Annotation _iter8 : struct.annotations)\n+          for (Annotation _iter11 : struct.annotations)\n           {\n-            _iter8.write(oprot);\n+            _iter11.write(oprot);\n+          }\n+          oprot.writeListEnd();\n+        }\n+        oprot.writeFieldEnd();\n+      }\n+      if (struct.parentIds != null) {\n+        oprot.writeFieldBegin(PARENT_IDS_FIELD_DESC);\n+        {\n+          oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, struct.parentIds.size()));\n+          for (long _iter12 : struct.parentIds)\n+          {\n+            oprot.writeI64(_iter12);\n           }\n           oprot.writeListEnd();\n         }\n@@ -1255,7 +1297,7 @@ public void write(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct)\n       if (struct.isSetSpanId()) {\n         optionals.set(3);\n       }\n-      if (struct.isSetParentId()) {\n+      if (struct.isSetParentIds()) {\n         optionals.set(4);\n       }\n       if (struct.isSetStart()) {\n@@ -1286,8 +1328,14 @@ public void write(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct)\n       if (struct.isSetSpanId()) {\n         oprot.writeI64(struct.spanId);\n       }\n-      if (struct.isSetParentId()) {\n-        oprot.writeI64(struct.parentId);\n+      if (struct.isSetParentIds()) {\n+        {\n+          oprot.writeI32(struct.parentIds.size());\n+          for (long _iter13 : struct.parentIds)\n+          {\n+            oprot.writeI64(_iter13);\n+          }\n+        }\n       }\n       if (struct.isSetStart()) {\n         oprot.writeI64(struct.start);\n@@ -1301,19 +1349,19 @@ public void write(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct)\n       if (struct.isSetData()) {\n         {\n           oprot.writeI32(struct.data.size());\n-          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter9 : struct.data.entrySet())\n+          for (java.util.Map.Entry<java.lang.String, java.lang.String> _iter14 : struct.data.entrySet())\n           {\n-            oprot.writeString(_iter9.getKey());\n-            oprot.writeString(_iter9.getValue());\n+            oprot.writeString(_iter14.getKey());\n+            oprot.writeString(_iter14.getValue());\n           }\n         }\n       }\n       if (struct.isSetAnnotations()) {\n         {\n           oprot.writeI32(struct.annotations.size());\n-          for (Annotation _iter10 : struct.annotations)\n+          for (Annotation _iter15 : struct.annotations)\n           {\n-            _iter10.write(oprot);\n+            _iter15.write(oprot);\n           }\n         }\n       }\n@@ -1340,8 +1388,17 @@ public void read(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct) t\n         struct.setSpanIdIsSet(true);\n       }\n       if (incoming.get(4)) {\n-        struct.parentId = iprot.readI64();\n-        struct.setParentIdIsSet(true);\n+        {\n+          org.apache.thrift.protocol.TList _list16 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());\n+          struct.parentIds = new java.util.ArrayList<java.lang.Long>(_list16.size);\n+          long _elem17;\n+          for (int _i18 = 0; _i18 < _list16.size; ++_i18)\n+          {\n+            _elem17 = iprot.readI64();\n+            struct.parentIds.add(_elem17);\n+          }\n+        }\n+        struct.setParentIdsIsSet(true);\n       }\n       if (incoming.get(5)) {\n         struct.start = iprot.readI64();\n@@ -1357,29 +1414,29 @@ public void read(org.apache.thrift.protocol.TProtocol prot, RemoteSpan struct) t\n       }\n       if (incoming.get(8)) {\n         {\n-          org.apache.thrift.protocol.TMap _map11 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());\n-          struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map11.size);\n-          java.lang.String _key12;\n-          java.lang.String _val13;\n-          for (int _i14 = 0; _i14 < _map11.size; ++_i14)\n+          org.apache.thrift.protocol.TMap _map19 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());\n+          struct.data = new java.util.HashMap<java.lang.String,java.lang.String>(2*_map19.size);\n+          java.lang.String _key20;\n+          java.lang.String _val21;\n+          for (int _i22 = 0; _i22 < _map19.size; ++_i22)\n           {\n-            _key12 = iprot.readString();\n-            _val13 = iprot.readString();\n-            struct.data.put(_key12, _val13);\n+            _key20 = iprot.readString();\n+            _val21 = iprot.readString();\n+            struct.data.put(_key20, _val21);\n           }\n         }\n         struct.setDataIsSet(true);\n       }\n       if (incoming.get(9)) {\n         {\n-          org.apache.thrift.protocol.TList _list15 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());\n-          struct.annotations = new java.util.ArrayList<Annotation>(_list15.size);\n-          Annotation _elem16;\n-          for (int _i17 = 0; _i17 < _list15.size; ++_i17)\n+          org.apache.thrift.protocol.TList _list23 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());\n+          struct.annotations = new java.util.ArrayList<Annotation>(_list23.size);\n+          Annotation _elem24;\n+          for (int _i25 = 0; _i25 < _list23.size; ++_i25)\n           {\n-            _elem16 = new Annotation();\n-            _elem16.read(iprot);\n-            struct.annotations.add(_elem16);\n+            _elem24 = new Annotation();\n+            _elem24.read(iprot);\n+            struct.annotations.add(_elem24);\n           }\n         }\n         struct.setAnnotationsIsSet(true);",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/java/org/apache/accumulo/tracer/thrift/RemoteSpan.java",
                "sha": "819185a96eda7c98602b1a27adf43d1e635634b5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/thrift/tracer.thrift",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/main/thrift/tracer.thrift?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 1,
                "filename": "server/tracer/src/main/thrift/tracer.thrift",
                "patch": "@@ -29,7 +29,7 @@ struct RemoteSpan {\n   2:string svc\n   3:i64 traceId\n   4:i64 spanId\n-  5:i64 parentId\n+  11:list<i64> parentIds\n   6:i64 start\n   7:i64 stop\n   8:string description",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/main/thrift/tracer.thrift",
                "sha": "a66479df2a811abe96febbe9d97b54a3d73f1d40",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 8,
                "filename": "server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.htrace.SpanReceiver;\n import org.apache.htrace.Trace;\n import org.apache.htrace.TraceScope;\n+import org.apache.htrace.Tracer;\n import org.apache.htrace.wrappers.TraceProxy;\n import org.apache.thrift.protocol.TBinaryProtocol;\n import org.apache.thrift.server.TServer;\n@@ -51,16 +52,18 @@\n import org.junit.Before;\n import org.junit.Test;\n \n+import com.google.common.primitives.Longs;\n+\n import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n \n public class TracerTest {\n   static class SpanStruct {\n-    public SpanStruct(long traceId, long spanId, long parentId, long start, long stop,\n-        String description, Map<byte[],byte[]> data) {\n+    public SpanStruct(long traceId, long spanId, List<Long> parentIds, long start, long stop,\n+        String description, Map<String,String> data) {\n       super();\n       this.traceId = traceId;\n       this.spanId = spanId;\n-      this.parentId = parentId;\n+      this.parentIds = parentIds;\n       this.start = start;\n       this.stop = stop;\n       this.description = description;\n@@ -69,11 +72,11 @@ public SpanStruct(long traceId, long spanId, long parentId, long start, long sto\n \n     public long traceId;\n     public long spanId;\n-    public long parentId;\n+    public List<Long> parentIds;\n     public long start;\n     public long stop;\n     public String description;\n-    public Map<byte[],byte[]> data;\n+    public Map<String,String> data;\n \n     public long millis() {\n       return stop - start;\n@@ -88,7 +91,7 @@ public TestReceiver() {}\n     @Override\n     public void receiveSpan(Span s) {\n       long traceId = s.getTraceId();\n-      SpanStruct span = new SpanStruct(traceId, s.getSpanId(), s.getParentId(),\n+      SpanStruct span = new SpanStruct(traceId, s.getSpanId(), Longs.asList(s.getParents()),\n           s.getStartTimeMillis(), s.getStopTimeMillis(), s.getDescription(), s.getKVAnnotations());\n       if (!traces.containsKey(traceId))\n         traces.put(traceId, new ArrayList<>());\n@@ -134,13 +137,14 @@ public void testTrace() throws Exception {\n     assertEquals(2, tracer.traces.get(traceId).size());\n     assertTrue(tracer.traces.get(traceId).get(1).millis() >= 100);\n \n-    Thread t = new Thread(TraceUtil.wrap(() -> assertTrue(Trace.isTracing())), \"My Task\");\n+    Thread t = new Thread(Trace.wrap(() -> assertTrue(Trace.isTracing())), \"My Task\");\n     t.start();\n     t.join();\n \n     assertEquals(3, tracer.traces.get(traceId).size());\n     assertEquals(\"My Task\", tracer.traces.get(traceId).get(2).description);\n-    TraceUtil.off();\n+    Trace.currentSpan().stop();\n+    Tracer.getInstance().continueSpan(null);\n     assertFalse(Trace.isTracing());\n   }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tracer/src/test/java/org/apache/accumulo/tracer/TracerTest.java",
                "sha": "df23b7500e2be267a2ca2a45659405a540ddf686",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 18,
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java",
                "patch": "@@ -16,7 +16,6 @@\n  */\n package org.apache.accumulo.tserver.replication;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n import static java.util.Objects.requireNonNull;\n import static org.apache.accumulo.fate.util.UtilWaitThread.sleepUninterruptibly;\n \n@@ -209,10 +208,9 @@ public Status replicate(final Path p, final Status status, final ReplicationTarg\n   private Status _replicate(final Path p, final Status status, final ReplicationTarget target,\n       final ReplicaSystemHelper helper, final AccumuloConfiguration localConf,\n       final ClientContext peerContext, final UserGroupInformation accumuloUgi) {\n-    try {\n-      double tracePercent = localConf.getFraction(Property.REPLICATION_TRACE_PERCENT);\n-      ProbabilitySampler sampler = TraceUtil.probabilitySampler(tracePercent);\n-      Trace.startSpan(\"AccumuloReplicaSystem\", sampler);\n+    double tracePercent = localConf.getFraction(Property.REPLICATION_TRACE_PERCENT);\n+    ProbabilitySampler sampler = TraceUtil.probabilitySampler(tracePercent);\n+    try (TraceScope replicaSpan = Trace.startSpan(\"AccumuloReplicaSystem\", sampler)) {\n \n       // Remote identifier is an integer (table id) in this case.\n       final String remoteTableId = target.getRemoteIdentifier();\n@@ -277,8 +275,6 @@ private Status _replicate(final Path p, final Status status, final ReplicationTa\n \n       // We made no status, punt on it for now, and let it re-queue itself for work\n       return status;\n-    } finally {\n-      TraceUtil.off();\n     }\n   }\n \n@@ -337,7 +333,7 @@ protected Status replicateLogs(ClientContext peerContext, final HostAndPort peer\n       log.debug(\"Skipping unwanted data in WAL\");\n       try (TraceScope span = Trace.startSpan(\"Consume WAL prefix\")) {\n         if (span.getSpan() != null) {\n-          span.getSpan().addKVAnnotation(\"file\".getBytes(UTF_8), p.toString().getBytes(UTF_8));\n+          span.getSpan().addKVAnnotation(\"file\", p.toString());\n         }\n         // We want to read all records in the WAL up to the \"begin\" offset contained in the Status\n         // message,\n@@ -358,15 +354,11 @@ protected Status replicateLogs(ClientContext peerContext, final HostAndPort peer\n         try (TraceScope span = Trace.startSpan(\"Replicate WAL batch\")) {\n           if (span.getSpan() != null) {\n             // Set some trace context\n-            span.getSpan().addKVAnnotation(\"Batch size (bytes)\".getBytes(UTF_8),\n-                Long.toString(sizeLimit).getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"File\".getBytes(UTF_8), p.toString().getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"Peer instance name\".getBytes(UTF_8),\n-                peerContext.getInstanceName().getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"Peer tserver\".getBytes(UTF_8),\n-                peerTserver.toString().getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"Remote table ID\".getBytes(UTF_8),\n-                remoteTableId.getBytes(UTF_8));\n+            span.getSpan().addKVAnnotation(\"Batch size (bytes)\", Long.toString(sizeLimit));\n+            span.getSpan().addKVAnnotation(\"File\", p.toString());\n+            span.getSpan().addKVAnnotation(\"Peer instance name\", peerContext.getInstanceName());\n+            span.getSpan().addKVAnnotation(\"Peer tserver\", peerTserver.toString());\n+            span.getSpan().addKVAnnotation(\"Remote table ID\", remoteTableId);\n           }\n \n           // Read and send a batch of mutations\n@@ -634,7 +626,7 @@ protected ClientContext getContextForPeer(AccumuloConfiguration localConf,\n   public DataInputStream getWalStream(Path p, FSDataInputStream input) throws IOException {\n     try (TraceScope span = Trace.startSpan(\"Read WAL header\")) {\n       if (span.getSpan() != null) {\n-        span.getSpan().addKVAnnotation(\"file\".getBytes(UTF_8), p.toString().getBytes(UTF_8));\n+        span.getSpan().addKVAnnotation(\"file\", p.toString());\n       }\n       DFSLoggerInputStreams streams = DfsLogger.readHeaderAndReturnStream(input, conf);\n       return streams.getDecryptingInputStream();",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/replication/AccumuloReplicaSystem.java",
                "sha": "fe85ab49e1c11db4f807476bd4b848f2047f6605",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 8,
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java",
                "patch": "@@ -16,8 +16,6 @@\n  */\n package org.apache.accumulo.tserver.tablet;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n import java.io.IOException;\n \n import org.apache.accumulo.core.metadata.schema.DataFileValue;\n@@ -94,12 +92,10 @@ public void run() {\n         }\n \n         if (minorCompaction.getSpan() != null) {\n-          minorCompaction.getSpan().addKVAnnotation(\"extent\".getBytes(UTF_8),\n-              tablet.getExtent().toString().getBytes(UTF_8));\n-          minorCompaction.getSpan().addKVAnnotation(\"numEntries\".getBytes(UTF_8),\n-              Long.toString(this.stats.getNumEntries()).getBytes(UTF_8));\n-          minorCompaction.getSpan().addKVAnnotation(\"size\".getBytes(UTF_8),\n-              Long.toString(this.stats.getSize()).getBytes(UTF_8));\n+          minorCompaction.getSpan().addKVAnnotation(\"extent\", tablet.getExtent().toString());\n+          minorCompaction.getSpan().addKVAnnotation(\"numEntries\",\n+              Long.toString(this.stats.getNumEntries()));\n+          minorCompaction.getSpan().addKVAnnotation(\"size\", Long.toString(this.stats.getSize()));\n         }\n       }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/MinorCompactionTask.java",
                "sha": "7aaa150f1a5270743c4a59b57c5a3e1b9d9d4964",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 12,
                "filename": "server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java",
                "patch": "@@ -1998,12 +1998,9 @@ public RateLimiter getWriteLimiter() {\n           CompactionStats mcs = compactor.call();\n \n           if (span.getSpan() != null) {\n-            span.getSpan().addKVAnnotation(\"files\".getBytes(UTF_8),\n-                (\"\" + smallestFiles.size()).getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"read\".getBytes(UTF_8),\n-                (\"\" + mcs.getEntriesRead()).getBytes(UTF_8));\n-            span.getSpan().addKVAnnotation(\"written\".getBytes(UTF_8),\n-                (\"\" + mcs.getEntriesWritten()).getBytes(UTF_8));\n+            span.getSpan().addKVAnnotation(\"files\", (\"\" + smallestFiles.size()));\n+            span.getSpan().addKVAnnotation(\"read\", (\"\" + mcs.getEntriesRead()));\n+            span.getSpan().addKVAnnotation(\"written\", (\"\" + mcs.getEntriesWritten()));\n           }\n           majCStats.add(mcs);\n \n@@ -2124,13 +2121,10 @@ CompactionStats majorCompact(MajorCompactionReason reason, long queued) {\n             .enqueueMasterMessage(new TabletStatusMessage(TabletLoadState.CHOPPED, extent));\n       }\n       if (span.getSpan() != null) {\n-        span.getSpan().addKVAnnotation(\"extent\".getBytes(UTF_8),\n-            (\"\" + getExtent()).getBytes(UTF_8));\n+        span.getSpan().addKVAnnotation(\"extent\", (\"\" + getExtent()));\n         if (majCStats != null) {\n-          span.getSpan().addKVAnnotation(\"read\".getBytes(UTF_8),\n-              (\"\" + majCStats.getEntriesRead()).getBytes(UTF_8));\n-          span.getSpan().addKVAnnotation(\"written\".getBytes(UTF_8),\n-              (\"\" + majCStats.getEntriesWritten()).getBytes(UTF_8));\n+          span.getSpan().addKVAnnotation(\"read\", (\"\" + majCStats.getEntriesRead()));\n+          span.getSpan().addKVAnnotation(\"written\", (\"\" + majCStats.getEntriesWritten()));\n         }\n       }\n       success = true;",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/server/tserver/src/main/java/org/apache/accumulo/tserver/tablet/Tablet.java",
                "sha": "d8b10e26a65f432b25b9c5f0ee75a874c23e49a3",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 5,
                "filename": "shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java",
                "patch": "@@ -26,27 +26,33 @@\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.data.Range;\n import org.apache.accumulo.core.security.Authorizations;\n-import org.apache.accumulo.core.trace.TraceUtil;\n import org.apache.accumulo.core.util.BadArgumentException;\n import org.apache.accumulo.shell.Shell;\n import org.apache.accumulo.tracer.TraceDump;\n import org.apache.commons.cli.CommandLine;\n import org.apache.hadoop.io.Text;\n import org.apache.htrace.Sampler;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n \n public class TraceCommand extends DebugCommand {\n \n+  private TraceScope traceScope = null;\n+\n   @Override\n   public int execute(final String fullCommand, final CommandLine cl, final Shell shellState)\n       throws IOException {\n     if (cl.getArgs().length == 1) {\n       if (cl.getArgs()[0].equalsIgnoreCase(\"on\")) {\n-        Trace.startSpan(\"shell:\" + shellState.getAccumuloClient().whoami(), Sampler.ALWAYS);\n+        if (traceScope == null) {\n+          traceScope = Trace.startSpan(\"shell:\" + shellState.getAccumuloClient().whoami(),\n+              Sampler.ALWAYS);\n+        }\n       } else if (cl.getArgs()[0].equalsIgnoreCase(\"off\")) {\n-        if (Trace.isTracing()) {\n-          final long trace = Trace.currentSpan().getTraceId();\n-          TraceUtil.off();\n+        if (traceScope != null) {\n+          final long trace = traceScope.getSpan().getTraceId();\n+          traceScope.close();\n+          traceScope = null;\n           StringBuilder sb = new StringBuilder();\n           int traceCount = 0;\n           for (int i = 0; i < 30; i++) {",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/shell/src/main/java/org/apache/accumulo/shell/commands/TraceCommand.java",
                "sha": "67e2a5b0089e14ada1e1533832b97b773958a3ec",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/TestIngest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/TestIngest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 4,
                "filename": "test/src/main/java/org/apache/accumulo/test/TestIngest.java",
                "patch": "@@ -54,6 +54,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.log4j.Level;\n import org.apache.log4j.Logger;\n \n@@ -188,12 +189,10 @@ public static void main(String[] args) {\n \n     Opts opts = new Opts();\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n-    opts.parseArgs(TestIngest.class.getName(), args, bwOpts);\n \n     String name = TestIngest.class.getSimpleName();\n     TraceUtil.enableClientTraces(null, name, new Properties());\n-    try {\n-      opts.startTracing(name);\n+    try (TraceScope clientSpan = opts.parseArgsAndTrace(name, args, bwOpts)) {\n       if (opts.debug)\n         Logger.getLogger(TabletServerBatchWriter.class.getName()).setLevel(Level.TRACE);\n \n@@ -203,7 +202,6 @@ public static void main(String[] args) {\n     } catch (Exception e) {\n       throw new RuntimeException(e);\n     } finally {\n-      opts.stopTracing();\n       TraceUtil.disable();\n     }\n   }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/TestIngest.java",
                "sha": "c2b390080fcfcf9dfb6483525ddab1353b7099e0",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/VerifyIngest.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/VerifyIngest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 13,
                "filename": "test/src/main/java/org/apache/accumulo/test/VerifyIngest.java",
                "patch": "@@ -16,8 +16,6 @@\n  */\n package org.apache.accumulo.test;\n \n-import static java.nio.charset.StandardCharsets.UTF_8;\n-\n import java.util.Arrays;\n import java.util.Iterator;\n import java.util.Map.Entry;\n@@ -40,6 +38,7 @@\n import org.apache.htrace.Sampler;\n import org.apache.htrace.Span;\n import org.apache.htrace.Trace;\n+import org.apache.htrace.TraceScope;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -68,23 +67,20 @@ public static void main(String[] args) throws Exception {\n     Opts opts = new Opts();\n     ScannerOpts scanOpts = new ScannerOpts();\n     opts.parseArgs(VerifyIngest.class.getName(), args, scanOpts);\n-    try {\n-      if (opts.trace) {\n-        String name = VerifyIngest.class.getSimpleName();\n-        TraceUtil.enableClientTraces(null, null, new Properties());\n-        Trace.startSpan(name, Sampler.ALWAYS);\n-        Span span = Trace.currentSpan();\n-        if (span != null)\n-          span.addKVAnnotation(\"cmdLine\".getBytes(UTF_8),\n-              Arrays.asList(args).toString().getBytes(UTF_8));\n-      }\n+    if (opts.trace) {\n+      TraceUtil.enableClientTraces(null, null, new Properties());\n+    }\n+    try (TraceScope clientSpan = Trace.startSpan(VerifyIngest.class.getSimpleName(),\n+        Sampler.ALWAYS)) {\n+      Span span = clientSpan.getSpan();\n+      if (span != null)\n+        span.addKVAnnotation(\"cmdLine\", Arrays.asList(args).toString());\n \n       try (AccumuloClient client = opts.createClient()) {\n         verifyIngest(client, opts, scanOpts);\n       }\n \n     } finally {\n-      TraceUtil.off();\n       TraceUtil.disable();\n     }\n   }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/VerifyIngest.java",
                "sha": "f099fa8be56443522ae6087ed2876a5585b07839",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 71,
                "filename": "test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.Text;\n+import org.apache.htrace.TraceScope;\n import org.apache.htrace.wrappers.TraceProxy;\n \n public class ContinuousIngest {\n@@ -82,103 +83,104 @@ public static void main(String[] args) throws Exception {\n     ContinuousOpts opts = new ContinuousOpts();\n     BatchWriterOpts bwOpts = new BatchWriterOpts();\n     ClientOnDefaultTable clientOpts = new ClientOnDefaultTable(\"ci\");\n-    clientOpts.parseArgs(ContinuousIngest.class.getName(), args, bwOpts, opts);\n+    try (TraceScope clientSpan = clientOpts.parseArgsAndTrace(ContinuousIngest.class.getName(),\n+        args, bwOpts, opts)) {\n \n-    initVisibilities(opts);\n+      initVisibilities(opts);\n \n-    if (opts.min < 0 || opts.max < 0 || opts.max <= opts.min) {\n-      throw new IllegalArgumentException(\"bad min and max\");\n-    }\n-    try (AccumuloClient client = clientOpts.createClient()) {\n-\n-      if (!client.tableOperations().exists(clientOpts.getTableName())) {\n-        throw new TableNotFoundException(null, clientOpts.getTableName(),\n-            \"Consult the README and create the table before starting ingest.\");\n+      if (opts.min < 0 || opts.max < 0 || opts.max <= opts.min) {\n+        throw new IllegalArgumentException(\"bad min and max\");\n       }\n+      try (AccumuloClient client = clientOpts.createClient()) {\n \n-      BatchWriter bw = client.createBatchWriter(clientOpts.getTableName(),\n-          bwOpts.getBatchWriterConfig());\n-      bw = TraceProxy.trace(bw, TraceUtil.countSampler(1024));\n-\n-      Random r = new SecureRandom();\n-\n-      byte[] ingestInstanceId = UUID.randomUUID().toString().getBytes(UTF_8);\n-\n-      System.out.printf(\"UUID %d %s%n\", System.currentTimeMillis(),\n-          new String(ingestInstanceId, UTF_8));\n-\n-      long count = 0;\n-      final int flushInterval = 1000000;\n-      final int maxDepth = 25;\n+        if (!client.tableOperations().exists(clientOpts.getTableName())) {\n+          throw new TableNotFoundException(null, clientOpts.getTableName(),\n+              \"Consult the README and create the table before starting ingest.\");\n+        }\n \n-      // always want to point back to flushed data. This way the previous item should\n-      // always exist in accumulo when verifying data. To do this make insert N point\n-      // back to the row from insert (N - flushInterval). The array below is used to keep\n-      // track of this.\n-      long prevRows[] = new long[flushInterval];\n-      long firstRows[] = new long[flushInterval];\n-      int firstColFams[] = new int[flushInterval];\n-      int firstColQuals[] = new int[flushInterval];\n+        BatchWriter bw = client.createBatchWriter(clientOpts.getTableName(),\n+            bwOpts.getBatchWriterConfig());\n+        bw = TraceProxy.trace(bw, TraceUtil.countSampler(1024));\n \n-      long lastFlushTime = System.currentTimeMillis();\n+        Random r = new SecureRandom();\n \n-      out: while (true) {\n-        // generate first set of nodes\n-        ColumnVisibility cv = getVisibility(r);\n+        byte[] ingestInstanceId = UUID.randomUUID().toString().getBytes(UTF_8);\n \n-        for (int index = 0; index < flushInterval; index++) {\n-          long rowLong = genLong(opts.min, opts.max, r);\n-          prevRows[index] = rowLong;\n-          firstRows[index] = rowLong;\n+        System.out.printf(\"UUID %d %s%n\", System.currentTimeMillis(),\n+            new String(ingestInstanceId, UTF_8));\n \n-          int cf = r.nextInt(opts.maxColF);\n-          int cq = r.nextInt(opts.maxColQ);\n+        long count = 0;\n+        final int flushInterval = 1000000;\n+        final int maxDepth = 25;\n \n-          firstColFams[index] = cf;\n-          firstColQuals[index] = cq;\n+        // always want to point back to flushed data. This way the previous item should\n+        // always exist in accumulo when verifying data. To do this make insert N point\n+        // back to the row from insert (N - flushInterval). The array below is used to keep\n+        // track of this.\n+        long prevRows[] = new long[flushInterval];\n+        long firstRows[] = new long[flushInterval];\n+        int firstColFams[] = new int[flushInterval];\n+        int firstColQuals[] = new int[flushInterval];\n \n-          Mutation m = genMutation(rowLong, cf, cq, cv, ingestInstanceId, count, null,\n-              opts.checksum);\n-          count++;\n-          bw.addMutation(m);\n-        }\n+        long lastFlushTime = System.currentTimeMillis();\n \n-        lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n-        if (count >= opts.num)\n-          break out;\n+        out: while (true) {\n+          // generate first set of nodes\n+          ColumnVisibility cv = getVisibility(r);\n \n-        // generate subsequent sets of nodes that link to previous set of nodes\n-        for (int depth = 1; depth < maxDepth; depth++) {\n           for (int index = 0; index < flushInterval; index++) {\n             long rowLong = genLong(opts.min, opts.max, r);\n-            byte[] prevRow = genRow(prevRows[index]);\n             prevRows[index] = rowLong;\n-            Mutation m = genMutation(rowLong, r.nextInt(opts.maxColF), r.nextInt(opts.maxColQ), cv,\n-                ingestInstanceId, count, prevRow, opts.checksum);\n+            firstRows[index] = rowLong;\n+\n+            int cf = r.nextInt(opts.maxColF);\n+            int cq = r.nextInt(opts.maxColQ);\n+\n+            firstColFams[index] = cf;\n+            firstColQuals[index] = cq;\n+\n+            Mutation m = genMutation(rowLong, cf, cq, cv, ingestInstanceId, count, null,\n+                opts.checksum);\n             count++;\n             bw.addMutation(m);\n           }\n \n           lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n           if (count >= opts.num)\n             break out;\n-        }\n \n-        // create one big linked list, this makes all of the first inserts\n-        // point to something\n-        for (int index = 0; index < flushInterval - 1; index++) {\n-          Mutation m = genMutation(firstRows[index], firstColFams[index], firstColQuals[index], cv,\n-              ingestInstanceId, count, genRow(prevRows[index + 1]), opts.checksum);\n-          count++;\n-          bw.addMutation(m);\n+          // generate subsequent sets of nodes that link to previous set of nodes\n+          for (int depth = 1; depth < maxDepth; depth++) {\n+            for (int index = 0; index < flushInterval; index++) {\n+              long rowLong = genLong(opts.min, opts.max, r);\n+              byte[] prevRow = genRow(prevRows[index]);\n+              prevRows[index] = rowLong;\n+              Mutation m = genMutation(rowLong, r.nextInt(opts.maxColF), r.nextInt(opts.maxColQ),\n+                  cv, ingestInstanceId, count, prevRow, opts.checksum);\n+              count++;\n+              bw.addMutation(m);\n+            }\n+\n+            lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n+            if (count >= opts.num)\n+              break out;\n+          }\n+\n+          // create one big linked list, this makes all of the first inserts\n+          // point to something\n+          for (int index = 0; index < flushInterval - 1; index++) {\n+            Mutation m = genMutation(firstRows[index], firstColFams[index], firstColQuals[index],\n+                cv, ingestInstanceId, count, genRow(prevRows[index + 1]), opts.checksum);\n+            count++;\n+            bw.addMutation(m);\n+          }\n+          lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n+          if (count >= opts.num)\n+            break out;\n         }\n-        lastFlushTime = flush(bw, count, flushInterval, lastFlushTime);\n-        if (count >= opts.num)\n-          break out;\n-      }\n \n-      bw.close();\n-      clientOpts.stopTracing();\n+        bw.close();\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/main/java/org/apache/accumulo/test/performance/ContinuousIngest.java",
                "sha": "43b09e25d404051744380aadceb5e2db08fb7a5d",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/accumulo/blob/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java?ref=6a442154a12151fed83de549ef398a27a72ac8ab",
                "deletions": 1,
                "filename": "test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java",
                "patch": "@@ -58,7 +58,6 @@ public void paramsSetThreadsTest() {\n     assertEquals(\"Check tablename is set\", 0, tablename.compareTo(opts.getTableName()));\n     assertEquals(\"Check numThreads is set\", 99, opts.numThreads);\n \n-    System.out.println(opts.columns);\n   }\n \n }",
                "raw_url": "https://github.com/apache/accumulo/raw/6a442154a12151fed83de549ef398a27a72ac8ab/test/src/test/java/org/apache/accumulo/test/performance/scan/CollectTabletStatsTest.java",
                "sha": "a5c71ac62965bc8b7ff5cda0fcfcf151b6bce3be",
                "status": "modified"
            }
        ],
        "message": "Update to HTrace 3.2\n\nAPI fixes specifically for HTrace version 3.2:\n\n* Use String-based annotations, instead of deprecated byte arrays API\n* Update thrift and other utils to support multiple span parents and\nmore explicit tracking of root spans (those without parents)\n* Remove broken API call to set the Tracer processId field\n\nGeneral fixes found broken, or generally lacking with 3.2, but were in\nneed of fix anyway:\n\n* Properly close spans, via containing TraceScope instead of trying to\nmanually stop and set it to null by messing with HTrace internal\ntracking (avoid doing Tracer.getInstance().continueSpan(null), since the\nresulting Closeable can't be closed, because it throws an NPE)\n* Ensure TraceScopes are closed via try-with-resources wherever we were\nmanually trying to stop the span\n* Explicitly support client-side tracing in various command-line tools\nby parsing arguments with an option to return a TraceScope object, which\nis then put in a try-with-resources block\n\nTrivial unrelated changes:\n\n* Remove random print statement of a null field in CollectTabletStatsTest",
        "parent": "https://github.com/apache/accumulo/commit/426c93b83f16dbf638713d3b704f5a4a63f9c30e",
        "repo": "accumulo",
        "unit_tests": [
            "TestClientOpts.java",
            "ThriftScannerTest.java",
            "MergeTest.java",
            "SimpleGarbageCollectorTest.java",
            "AsyncSpanReceiverTest.java",
            "AccumuloReplicaSystemTest.java",
            "TabletTest.java"
        ]
    },
    "accumulo_7918299": {
        "bug_id": "accumulo_7918299",
        "commit": "https://github.com/apache/accumulo/commit/79182991d0da0f7b7836a901cec78b3ea570808c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/accumulo/blob/79182991d0da0f7b7836a901cec78b3ea570808c/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java?ref=79182991d0da0f7b7836a901cec78b3ea570808c",
                "deletions": 0,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "patch": "@@ -320,6 +320,9 @@ public long balance(SortedMap<TServerInstance,TabletServerStatus> current, Set<K\n             }\n             try {\n               List<TabletStats> outOfBoundsTablets = getOnlineTabletsForTable(e.getKey(), tid);\n+              if (null == outOfBoundsTablets) {\n+                continue;\n+              }\n               Random random = new Random();\n               for (TabletStats ts : outOfBoundsTablets) {\n                 KeyExtent ke = new KeyExtent(ts.getExtent());",
                "raw_url": "https://github.com/apache/accumulo/raw/79182991d0da0f7b7836a901cec78b3ea570808c/server/base/src/main/java/org/apache/accumulo/server/master/balancer/HostRegexTableLoadBalancer.java",
                "sha": "d19ab82d078bbced21ea832f34ed8ec46640f020",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-4196: Protect against NPE",
        "parent": "https://github.com/apache/accumulo/commit/e77bdd7de25af87c53da6e5a3727c028956c1e00",
        "repo": "accumulo",
        "unit_tests": [
            "HostRegexTableLoadBalancerTest.java"
        ]
    },
    "accumulo_80f8afb": {
        "bug_id": "accumulo_80f8afb",
        "commit": "https://github.com/apache/accumulo/commit/80f8afb10c69bf337ffd7301e21f729c1c0ce27b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/accumulo/blob/80f8afb10c69bf337ffd7301e21f729c1c0ce27b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java?ref=80f8afb10c69bf337ffd7301e21f729c1c0ce27b",
                "deletions": 0,
                "filename": "src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java",
                "patch": "@@ -78,6 +78,10 @@ static TabletTime getInstance(String metadataValue) {\n   }\n   \n   public static String maxMetadataTime(String mv1, String mv2) {\n+    if (mv1 == null && mv2 == null) {\n+      return null;\n+    }\n+    \n     if (mv1 == null) {\n       checkType(mv2);\n       return mv2;",
                "raw_url": "https://github.com/apache/accumulo/raw/80f8afb10c69bf337ffd7301e21f729c1c0ce27b/src/server/src/main/java/org/apache/accumulo/server/tabletserver/TabletTime.java",
                "sha": "3ba84459f4307a74790f48d84549a55811121080",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-2523 TabletTime.maxMetadataTime NPE if both arguments are null\n\nSigned-off-by: Bill Havanki <bhavanki@cloudera.com>",
        "parent": "https://github.com/apache/accumulo/commit/92c41719bfc9cce7cb222c96155650bc11a288fb",
        "repo": "accumulo",
        "unit_tests": [
            "TabletTimeTest.java"
        ]
    },
    "accumulo_8fad5e7": {
        "bug_id": "accumulo_8fad5e7",
        "commit": "https://github.com/apache/accumulo/commit/8fad5e736750868dc563caeb9cf89c38005f034a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/8fad5e736750868dc563caeb9cf89c38005f034a/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java?ref=8fad5e736750868dc563caeb9cf89c38005f034a",
                "deletions": 7,
                "filename": "server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.accumulo.core.data.KeyExtent;\n import org.apache.accumulo.core.master.thrift.TabletServerStatus;\n import org.apache.accumulo.server.client.HdfsZooInstance;\n-import org.apache.accumulo.server.conf.ServerConfiguration;\n import org.apache.accumulo.server.master.state.TServerInstance;\n import org.apache.accumulo.server.master.state.TabletMigration;\n import org.apache.accumulo.server.security.SecurityConstants;\n@@ -44,11 +43,6 @@\n   private static final Logger log = Logger.getLogger(TableLoadBalancer.class);\n   \n   Map<String,TabletBalancer> perTableBalancers = new HashMap<String,TabletBalancer>();\n-  ServerConfiguration config;\n-  \n-  public void init(ServerConfiguration config) {\n-    this.config = config;\n-  }\n   \n   private TabletBalancer constructNewBalancerForTable(String clazzName, String table) throws Exception {\n     Class<? extends TabletBalancer> clazz = AccumuloClassLoader.loadClass(clazzName, TabletBalancer.class);\n@@ -57,7 +51,7 @@ private TabletBalancer constructNewBalancerForTable(String clazzName, String tab\n   }\n   \n   protected String getLoadBalancerClassNameForTable(String table) {\n-    return config.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n+    return configuration.getTableConfiguration(table).get(Property.TABLE_LOAD_BALANCER);\n   }\n   \n   protected TabletBalancer getBalancerForTable(String table) {",
                "raw_url": "https://github.com/apache/accumulo/raw/8fad5e736750868dc563caeb9cf89c38005f034a/server/src/main/java/org/apache/accumulo/server/master/balancer/TableLoadBalancer.java",
                "sha": "3598504ac332bb057a06bb914905e15a2936db89",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-590 fixed NPE in Table load balancer\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1337131 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/accumulo/commit/4a7d4b91461454ac047611dd51e0d20a51e92a2b",
        "repo": "accumulo",
        "unit_tests": [
            "TableLoadBalancerTest.java"
        ]
    },
    "accumulo_c3ca5e7": {
        "bug_id": "accumulo_c3ca5e7",
        "commit": "https://github.com/apache/accumulo/commit/c3ca5e770792368311f55717d8c4b0a0bdc5b7e7",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/c3ca5e770792368311f55717d8c4b0a0bdc5b7e7/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java?ref=c3ca5e770792368311f55717d8c4b0a0bdc5b7e7",
                "deletions": 8,
                "filename": "src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "patch": "@@ -1388,10 +1388,6 @@ private Tablet(final TabletServer tabletServer,\n \t\ttabletTime = TabletTime.getInstance(time);\n \t\tpersistedTime = tabletTime.getTime();\n \t\t\n-\t\tconstraintChecker.set(ConstraintLoader.load(extent.getTableId().toString()));\n-\n-\t\t\n-\n \t\tacuTableConf.addObserver(configObserver = new ConfigurationObserver(){\n \n \t\t\tprivate void reloadConstraints(){\n@@ -3049,6 +3045,9 @@ private CompactionStats _majorCompact(MajorCompactionReason reason) throws IOExc\n \t\tMap<String, Long> filesToCompact;\n \t\t\n \t\tint maxFilesToCompact = acuTableConf.getCount(Property.TSERV_MAJC_THREAD_MAXOPEN);\n+\t\t\n+\t\tCompactionStats majCStats = new CompactionStats();\n+\n \t\tsynchronized(this) {\n \t\t\t//plan all that work that needs to be done in the sync block... then do the actual work\n \t\t\t//outside the sync block\n@@ -3078,7 +3077,7 @@ private CompactionStats _majorCompact(MajorCompactionReason reason) throws IOExc\n \t\t\tCompactionTuple ret = getFilesToCompact(reason, falks);\n \t\t\tif(ret == null){\n \t\t\t\t//nothing to compact\n-\t\t\t\treturn null;\n+        return majCStats;\n \t\t\t}\n \t\t\tfilesToCompact = ret.getFilesToCompact();\n \n@@ -3105,8 +3104,6 @@ private CompactionStats _majorCompact(MajorCompactionReason reason) throws IOExc\n \t\t\t\t//compacting everything, so update the compaction id in !METADATA\n \t\t\t\tcompactionId = getCompactionID();\n \t\t\t}\n-\t\t\t\n-\t\t\tCompactionStats majCStats = new CompactionStats();\n \n \t\t\t//need to handle case where only one file is being major compacted\n \t\t\twhile(filesToCompact.size() > 0) {\n@@ -3252,7 +3249,7 @@ private CompactionStats majorCompact(MajorCompactionReason reason) {\n \t\t\t\tmajorCompactionInProgress = false;\n \t\t\t\tthis.notifyAll();\n \t\t\t}\n-\t\t\t//TODO majCStats could be null\n+\n \t\t\tSpan curr = Trace.currentTrace();\n \t\t\tcurr.data(\"extent\",  \"\"+getExtent());\n \t\t\tcurr.data(\"read\",    \"\"+majCStats.getEntriesRead());",
                "raw_url": "https://github.com/apache/accumulo/raw/c3ca5e770792368311f55717d8c4b0a0bdc5b7e7/src/server/src/main/java/org/apache/accumulo/server/tabletserver/Tablet.java",
                "sha": "3f5fbeef7c4af295bb9368a32b9934913fbdc3b2",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-20 committing patch that fixes NPE and removes redundant reload of contraints\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/accumulo/trunk@1182972 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/accumulo/commit/f13ce13b70ef0f5858738ab4f3ec08483fe227c3",
        "repo": "accumulo",
        "unit_tests": [
            "TabletTest.java"
        ]
    },
    "accumulo_c5aac49": {
        "bug_id": "accumulo_c5aac49",
        "commit": "https://github.com/apache/accumulo/commit/c5aac49ed299b7c6eee534333cf080b638bcdecd",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/c5aac49ed299b7c6eee534333cf080b638bcdecd/server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java?ref=c5aac49ed299b7c6eee534333cf080b638bcdecd",
                "deletions": 6,
                "filename": "server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java",
                "patch": "@@ -148,12 +148,7 @@ public boolean process(TProtocol in, TProtocol out) throws TException {\n         metrics.add(ThriftMetrics.idle, (now - idleStart));\n       }\n       try {\n-        try {\n-          return other.process(in, out);\n-        } catch (NullPointerException ex) {\n-          // THRIFT-1447 - remove with thrift 0.9\n-          return true;\n-        }\n+        return other.process(in, out);\n       } finally {\n         if (metrics.isEnabled()) {\n           idleStart = System.currentTimeMillis();",
                "raw_url": "https://github.com/apache/accumulo/raw/c5aac49ed299b7c6eee534333cf080b638bcdecd/server/src/main/java/org/apache/accumulo/server/util/TServerUtils.java",
                "sha": "f185661dcfd8a70e69616ec228dbe65a544c3908",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-2410 TServerUtils no longer needs to catch NPE as a thrift bug workaround\n\nSigned-off-by: John Vines <vines@apache.org>",
        "parent": "https://github.com/apache/accumulo/commit/53ec68999a416d96a0bd3c8d92a6311c8b69987b",
        "repo": "accumulo",
        "unit_tests": [
            "TServerUtilsTest.java"
        ]
    },
    "accumulo_c79cb06": {
        "bug_id": "accumulo_c79cb06",
        "commit": "https://github.com/apache/accumulo/commit/c79cb06b345dbf43ebe34ab17bf60f517f27ca5f",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/accumulo/blob/c79cb06b345dbf43ebe34ab17bf60f517f27ca5f/src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java?ref=c79cb06b345dbf43ebe34ab17bf60f517f27ca5f",
                "deletions": 7,
                "filename": "src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java",
                "patch": "@@ -305,7 +305,7 @@ public static boolean isLockHeld(ZooKeeper zk, LockID lid) throws KeeperExceptio\n     \n     List<String> children = zk.getChildren(lid.path, false);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return false;\n     }\n     \n@@ -323,7 +323,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n     \n     List<String> children = zc.getChildren(lid.path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return false;\n     }\n     \n@@ -341,7 +341,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n   public static byte[] getLockData(ZooKeeper zk, String path) throws KeeperException, InterruptedException {\n     List<String> children = zk.getChildren(path, false);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return null;\n     }\n     \n@@ -356,7 +356,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n     \n     List<String> children = zc.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return null;\n     }\n     \n@@ -381,7 +381,7 @@ public static boolean isLockHeld(ZooCache zc, LockID lid) {\n   public static long getSessionId(ZooCache zc, String path) throws KeeperException, InterruptedException {\n     List<String> children = zc.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       return 0;\n     }\n     \n@@ -406,7 +406,7 @@ public static void deleteLock(String path) throws InterruptedException, KeeperEx\n     IZooReaderWriter zk = ZooReaderWriter.getInstance();\n     children = zk.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       throw new IllegalStateException(\"No lock is held at \" + path);\n     }\n     \n@@ -428,7 +428,7 @@ public static boolean deleteLock(String path, String lockData) throws Interrupte\n     IZooReaderWriter zk = ZooReaderWriter.getInstance();\n     children = zk.getChildren(path);\n     \n-    if (children.size() == 0) {\n+    if (children == null || children.size() == 0) {\n       throw new IllegalStateException(\"No lock is held at \" + path);\n     }\n     ",
                "raw_url": "https://github.com/apache/accumulo/raw/c79cb06b345dbf43ebe34ab17bf60f517f27ca5f/src/server/src/main/java/org/apache/accumulo/server/zookeeper/ZooLock.java",
                "sha": "06d136f69de16962d3eb382c15caab4676be8b8d",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-93: tighten up the lock check, ensure lock checking does not throw NPE\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/accumulo/trunk@1190505 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/accumulo/commit/75202271ee82e1149c0140915f6389621be8271c",
        "repo": "accumulo",
        "unit_tests": [
            "ZooLockTest.java"
        ]
    },
    "accumulo_e3a743c": {
        "bug_id": "accumulo_e3a743c",
        "commit": "https://github.com/apache/accumulo/commit/e3a743cb445723a3d5664a4bf1ebf37833152aae",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 4,
                "filename": "core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java",
                "patch": "@@ -60,16 +60,15 @@ public static Volume getDefaultVolume(Configuration conf, AccumuloConfiguration\n   }\n \n   /**\n-   * @see org.apache.accumulo.core.volume.VolumeConfiguration#getVolumeUris(AccumuloConfiguration)\n+   * @see org.apache.accumulo.core.volume.VolumeConfiguration#getVolumeUris(AccumuloConfiguration,Configuration)\n    */\n   @Deprecated\n-  public static String getConfiguredBaseDir(AccumuloConfiguration conf) {\n+  public static String getConfiguredBaseDir(AccumuloConfiguration conf, Configuration hadoopConfig) {\n     String singleNamespace = conf.get(Property.INSTANCE_DFS_DIR);\n     String dfsUri = conf.get(Property.INSTANCE_DFS_URI);\n     String baseDir;\n \n     if (dfsUri == null || dfsUri.isEmpty()) {\n-      Configuration hadoopConfig = CachedConfiguration.getInstance();\n       try {\n         baseDir = FileSystem.get(hadoopConfig).getUri().toString() + singleNamespace;\n       } catch (IOException e) {\n@@ -85,15 +84,20 @@ public static String getConfiguredBaseDir(AccumuloConfiguration conf) {\n \n   /**\n    * Compute the URIs to be used by Accumulo\n+   * \n    */\n   public static String[] getVolumeUris(AccumuloConfiguration conf) {\n+    return getVolumeUris(conf, CachedConfiguration.getInstance());\n+  }\n+  \n+  public static String[] getVolumeUris(AccumuloConfiguration conf, Configuration hadoopConfig) {\n     String ns = conf.get(Property.INSTANCE_VOLUMES);\n \n     String configuredBaseDirs[];\n \n     if (ns == null || ns.isEmpty()) {\n       // Fall back to using the old config values\n-      configuredBaseDirs = new String[] {getConfiguredBaseDir(conf)};\n+      configuredBaseDirs = new String[] {getConfiguredBaseDir(conf, hadoopConfig)};\n     } else {\n       String namespaces[] = ns.split(\",\");\n       configuredBaseDirs = new String[namespaces.length];",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/volume/VolumeConfiguration.java",
                "sha": "8d56d9e991142ef85cd1ce8e61faafbe53fe24f0",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 7,
                "filename": "core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java",
                "patch": "@@ -25,31 +25,34 @@\n import org.apache.accumulo.core.conf.AccumuloConfiguration;\n import org.apache.accumulo.core.util.CachedConfiguration;\n import org.apache.accumulo.core.volume.VolumeConfiguration;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.log4j.Logger;\n \n public class ZooUtil extends org.apache.accumulo.fate.zookeeper.ZooUtil {\n-  \n+\n   private static final Logger log = Logger.getLogger(ZooUtil.class);\n-  \n+\n   public static String getRoot(final Instance instance) {\n     return getRoot(instance.getInstanceID());\n   }\n-  \n+\n   public static String getRoot(final String instanceId) {\n     return Constants.ZROOT + \"/\" + instanceId;\n   }\n-  \n+\n   /**\n    * Utility to support certain client side utilities to minimize command-line options.\n    */\n-\n   public static String getInstanceIDFromHdfs(Path instanceDirectory, AccumuloConfiguration conf) {\n-    try {\n+    return getInstanceIDFromHdfs(instanceDirectory, conf, CachedConfiguration.getInstance());\n+  }\n \n-      FileSystem fs = VolumeConfiguration.getVolume(instanceDirectory.toString(), CachedConfiguration.getInstance(), conf).getFileSystem();\n+  public static String getInstanceIDFromHdfs(Path instanceDirectory, AccumuloConfiguration conf, Configuration hadoopConf) {\n+    try {\n+      FileSystem fs = VolumeConfiguration.getVolume(instanceDirectory.toString(), hadoopConf, conf).getFileSystem();\n       FileStatus[] files = null;\n       try {\n         files = fs.listStatus(instanceDirectory);",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/core/src/main/java/org/apache/accumulo/core/zookeeper/ZooUtil.java",
                "sha": "9ee7c992aa3b1273f57141d326d05e51ba1c9c5b",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 2,
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java",
                "patch": "@@ -17,6 +17,7 @@\n package org.apache.accumulo.minicluster;\n \n import java.io.File;\n+import java.io.IOException;\n import java.util.Map;\n \n import org.apache.accumulo.minicluster.impl.MiniAccumuloConfigImpl;\n@@ -95,8 +96,7 @@ public MiniAccumuloConfig setZooKeeperPort(int zooKeeperPort) {\n   }\n \n   /**\n-   * Configure the time to wait for ZooKeeper to startup.\n-   * Calling this method is optional. The default is 20000 milliseconds\n+   * Configure the time to wait for ZooKeeper to startup. Calling this method is optional. The default is 20000 milliseconds\n    * \n    * @param zooKeeperStartupTime\n    *          Time to wait for ZooKeeper to startup, in milliseconds\n@@ -252,4 +252,19 @@ public MiniAccumuloConfig setNativeLibPaths(String... nativePathItems) {\n     impl.setNativeLibPaths(nativePathItems);\n     return this;\n   }\n+\n+  /**\n+   * Informs MAC that it's running against an existing accumulo instance. It is assumed that it's already initialized and hdfs/zookeeper are already running.\n+   *\n+   * @param accumuloSite\n+   *          a File representation of the accumulo-site.xml file for the instance being run\n+   * @param hadoopConfDir\n+   *          a File representation of the hadoop configuration directory containing core-site.xml and hdfs-site.xml\n+   *\n+   * @since 1.6.2\n+   */\n+  public MiniAccumuloConfig useExistingInstance(File accumuloSite, File hadoopConfDir) throws IOException {\n+    impl.useExistingInstance(accumuloSite, hadoopConfDir);\n+    return this;\n+  }\n }",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloConfig.java",
                "sha": "68e30fa4eb8dfd0add7dd74a47927162cdbe4325",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 1,
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java",
                "patch": "@@ -21,7 +21,6 @@\n import java.io.IOException;\n import java.io.InputStream;\n import java.net.ServerSocket;\n-import java.net.Socket;\n import java.util.Date;\n import java.util.HashMap;\n import java.util.Map;\n@@ -31,6 +30,7 @@\n import org.apache.accumulo.core.cli.Help;\n import org.apache.accumulo.core.util.Pair;\n import org.apache.commons.io.FileUtils;\n+\n import com.beust.jcommander.IStringConverter;\n import com.beust.jcommander.Parameter;\n import com.google.common.io.Files;",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloRunner.java",
                "sha": "22eca84ef0b91e28e09278c54bc7e82e699f36e8",
                "status": "modified"
            },
            {
                "additions": 138,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java",
                "changes": 208,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 70,
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java",
                "patch": "@@ -50,6 +50,7 @@\n import java.util.concurrent.TimeoutException;\n \n import org.apache.accumulo.cluster.AccumuloCluster;\n+import org.apache.accumulo.core.Constants;\n import org.apache.accumulo.core.client.AccumuloException;\n import org.apache.accumulo.core.client.AccumuloSecurityException;\n import org.apache.accumulo.core.client.ClientConfiguration;\n@@ -59,22 +60,31 @@\n import org.apache.accumulo.core.client.impl.MasterClient;\n import org.apache.accumulo.core.client.impl.thrift.ThriftSecurityException;\n import org.apache.accumulo.core.client.security.tokens.PasswordToken;\n+import org.apache.accumulo.core.conf.ConfigurationCopy;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.core.master.thrift.MasterClientService;\n import org.apache.accumulo.core.master.thrift.MasterGoalState;\n import org.apache.accumulo.core.master.thrift.MasterMonitorInfo;\n+import org.apache.accumulo.core.util.CachedConfiguration;\n import org.apache.accumulo.core.util.Daemon;\n import org.apache.accumulo.core.util.Pair;\n import org.apache.accumulo.core.util.StringUtil;\n import org.apache.accumulo.core.util.UtilWaitThread;\n+import org.apache.accumulo.core.zookeeper.ZooUtil;\n+import org.apache.accumulo.fate.zookeeper.IZooReaderWriter;\n import org.apache.accumulo.gc.SimpleGarbageCollector;\n import org.apache.accumulo.master.Master;\n import org.apache.accumulo.master.state.SetGoalState;\n import org.apache.accumulo.minicluster.ServerType;\n+import org.apache.accumulo.server.Accumulo;\n+import org.apache.accumulo.server.fs.VolumeManager;\n+import org.apache.accumulo.server.fs.VolumeManagerImpl;\n import org.apache.accumulo.server.init.Initialize;\n import org.apache.accumulo.server.security.SystemCredentials;\n+import org.apache.accumulo.server.util.AccumuloStatus;\n import org.apache.accumulo.server.util.PortUtils;\n import org.apache.accumulo.server.util.time.SimpleTimer;\n+import org.apache.accumulo.server.zookeeper.ZooReaderWriterFactory;\n import org.apache.accumulo.start.Main;\n import org.apache.accumulo.start.classloader.vfs.MiniDFSUtil;\n import org.apache.accumulo.trace.instrument.Tracer;\n@@ -84,12 +94,17 @@\n import org.apache.commons.vfs2.FileObject;\n import org.apache.commons.vfs2.impl.VFSClassLoader;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n-import org.apache.log4j.Logger;\n import org.apache.thrift.TException;\n+import org.apache.zookeeper.KeeperException;\n+import org.apache.zookeeper.data.Stat;\n import org.apache.zookeeper.server.ZooKeeperServerMain;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import com.google.common.base.Predicate;\n import com.google.common.collect.Maps;\n@@ -101,7 +116,7 @@\n  * @since 1.6.0\n  */\n public class MiniAccumuloClusterImpl implements AccumuloCluster {\n-  private static final Logger log = Logger.getLogger(MiniAccumuloClusterImpl.class);\n+  private static final Logger log = LoggerFactory.getLogger(MiniAccumuloClusterImpl.class);\n \n   public static class LogWriter extends Daemon {\n     private BufferedReader in;\n@@ -219,6 +234,9 @@ private String getClasspath() throws IOException {\n       StringBuilder classpathBuilder = new StringBuilder();\n       classpathBuilder.append(config.getConfDir().getAbsolutePath());\n \n+      if (config.getHadoopConfDir() != null)\n+        classpathBuilder.append(File.pathSeparator).append(config.getHadoopConfDir().getAbsolutePath());\n+\n       if (config.getClasspathItems() == null) {\n \n         // assume 0 is the system classloader and skip it\n@@ -268,7 +286,8 @@ private Process _exec(Class<?> clazz, List<String> extraJvmOpts, String... args)\n     for (Entry<String,String> sysProp : config.getSystemProperties().entrySet()) {\n       argList.add(String.format(\"-D%s=%s\", sysProp.getKey(), sysProp.getValue()));\n     }\n-    argList.addAll(Arrays.asList(\"-XX:+UseConcMarkSweepGC\", \"-XX:CMSInitiatingOccupancyFraction=75\", \"-Dapple.awt.UIElement=true\", Main.class.getName(), className));\n+    argList.addAll(Arrays.asList(\"-XX:+UseConcMarkSweepGC\", \"-XX:CMSInitiatingOccupancyFraction=75\", \"-Dapple.awt.UIElement=true\", Main.class.getName(),\n+        className));\n     argList.addAll(Arrays.asList(args));\n \n     ProcessBuilder builder = new ProcessBuilder(argList);\n@@ -290,6 +309,8 @@ private Process _exec(Class<?> clazz, List<String> extraJvmOpts, String... args)\n     builder.environment().put(\"ACCUMULO_CONF_DIR\", config.getConfDir().getAbsolutePath());\n     // hadoop-2.2 puts error messages in the logs if this is not set\n     builder.environment().put(\"HADOOP_HOME\", config.getDir().getAbsolutePath());\n+    if (config.getHadoopConfDir() != null)\n+      builder.environment().put(\"HADOOP_CONF_DIR\", config.getHadoopConfDir().getAbsolutePath());\n \n     Process process = builder.start();\n \n@@ -339,13 +360,16 @@ public MiniAccumuloClusterImpl(MiniAccumuloConfigImpl config) throws IOException\n     this.config = config.initialize();\n \n     config.getConfDir().mkdirs();\n-    config.getAccumuloDir().mkdirs();\n-    config.getZooKeeperDir().mkdirs();\n     config.getLogDir().mkdirs();\n-    config.getWalogDir().mkdirs();\n     config.getLibDir().mkdirs();\n     config.getLibExtDir().mkdirs();\n \n+    if (!config.useExistingInstance()) {\n+      config.getZooKeeperDir().mkdirs();\n+      config.getWalogDir().mkdirs();\n+      config.getAccumuloDir().mkdirs();\n+    }\n+\n     if (config.useMiniDFS()) {\n       File nn = new File(config.getAccumuloDir(), \"nn\");\n       nn.mkdirs();\n@@ -378,6 +402,8 @@ public MiniAccumuloClusterImpl(MiniAccumuloConfigImpl config) throws IOException\n       siteConfig.put(Property.INSTANCE_DFS_URI.getKey(), dfsUri);\n       siteConfig.put(Property.INSTANCE_DFS_DIR.getKey(), \"/accumulo\");\n       config.setSiteConfig(siteConfig);\n+    } else if (config.useExistingInstance()) {\n+      dfsUri = CachedConfiguration.getInstance().get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY);\n     } else {\n       dfsUri = \"file://\";\n     }\n@@ -394,20 +420,22 @@ public boolean apply(Entry<String,String> v) {\n     File siteFile = new File(config.getConfDir(), \"accumulo-site.xml\");\n     writeConfig(siteFile, config.getSiteConfig().entrySet());\n \n-    zooCfgFile = new File(config.getConfDir(), \"zoo.cfg\");\n-    FileWriter fileWriter = new FileWriter(zooCfgFile);\n-\n-    // zookeeper uses Properties to read its config, so use that to write in order to properly escape things like Windows paths\n-    Properties zooCfg = new Properties();\n-    zooCfg.setProperty(\"tickTime\", \"2000\");\n-    zooCfg.setProperty(\"initLimit\", \"10\");\n-    zooCfg.setProperty(\"syncLimit\", \"5\");\n-    zooCfg.setProperty(\"clientPort\", config.getZooKeeperPort() + \"\");\n-    zooCfg.setProperty(\"maxClientCnxns\", \"1000\");\n-    zooCfg.setProperty(\"dataDir\", config.getZooKeeperDir().getAbsolutePath());\n-    zooCfg.store(fileWriter, null);\n-\n-    fileWriter.close();\n+    if (!config.useExistingInstance()) {\n+      zooCfgFile = new File(config.getConfDir(), \"zoo.cfg\");\n+      FileWriter fileWriter = new FileWriter(zooCfgFile);\n+\n+      // zookeeper uses Properties to read its config, so use that to write in order to properly escape things like Windows paths\n+      Properties zooCfg = new Properties();\n+      zooCfg.setProperty(\"tickTime\", \"2000\");\n+      zooCfg.setProperty(\"initLimit\", \"10\");\n+      zooCfg.setProperty(\"syncLimit\", \"5\");\n+      zooCfg.setProperty(\"clientPort\", config.getZooKeeperPort() + \"\");\n+      zooCfg.setProperty(\"maxClientCnxns\", \"1000\");\n+      zooCfg.setProperty(\"dataDir\", config.getZooKeeperDir().getAbsolutePath());\n+      zooCfg.store(fileWriter, null);\n+\n+      fileWriter.close();\n+    }\n \n     // disable audit logging for mini....\n     InputStream auditStream = this.getClass().getResourceAsStream(\"/auditLog.xml\");\n@@ -445,58 +473,98 @@ private void writeConfigProperties(File file, Map<String,String> settings) throw\n    */\n   @Override\n   public synchronized void start() throws IOException, InterruptedException {\n+    if (config.useExistingInstance()) {\n+      Configuration acuConf = config.getAccumuloConfiguration();\n+      Configuration hadoopConf = config.getHadoopConfiguration();\n+\n+      ConfigurationCopy cc = new ConfigurationCopy(acuConf);\n+      VolumeManager fs;\n+      try {\n+        fs = VolumeManagerImpl.get(cc, hadoopConf);\n+      } catch (IOException e) {\n+        throw new RuntimeException(e);\n+      }\n+      Path instanceIdPath = Accumulo.getAccumuloInstanceIdPath(fs);\n \n-    if (!initialized) {\n+      String instanceIdFromFile = ZooUtil.getInstanceIDFromHdfs(instanceIdPath, cc, hadoopConf);\n+      IZooReaderWriter zrw = new ZooReaderWriterFactory().getZooReaderWriter(cc.get(Property.INSTANCE_ZK_HOST),\n+          (int) cc.getTimeInMillis(Property.INSTANCE_ZK_TIMEOUT), cc.get(Property.INSTANCE_SECRET));\n \n-      Runtime.getRuntime().addShutdownHook(new Thread() {\n-        @Override\n-        public void run() {\n-          try {\n-            MiniAccumuloClusterImpl.this.stop();\n-          } catch (IOException e) {\n-            e.printStackTrace();\n-          } catch (InterruptedException e) {\n-            e.printStackTrace();\n+      String rootPath = ZooUtil.getRoot(instanceIdFromFile);\n+\n+      String instanceName = null;\n+      try {\n+        for (String name : zrw.getChildren(Constants.ZROOT + Constants.ZINSTANCES)) {\n+          String instanceNamePath = Constants.ZROOT + Constants.ZINSTANCES + \"/\" + name;\n+          byte[] bytes = zrw.getData(instanceNamePath, new Stat());\n+          String iid = new String(bytes, Constants.UTF8);\n+          if (iid.equals(instanceIdFromFile)) {\n+            instanceName = name;\n           }\n         }\n-      });\n-    }\n+      } catch (KeeperException e) {\n+        throw new RuntimeException(\"Unable to read instance name from zookeeper.\", e);\n+      }\n+      if (instanceName == null)\n+        throw new RuntimeException(\"Unable to read instance name from zookeeper.\");\n \n-    if (zooKeeperProcess == null) {\n-      zooKeeperProcess = _exec(ZooKeeperServerMain.class, ServerType.ZOOKEEPER, zooCfgFile.getAbsolutePath());\n-    }\n+      config.setInstanceName(instanceName);\n+      if (!AccumuloStatus.isAccumuloOffline(zrw, rootPath))\n+        throw new RuntimeException(\"The Accumulo instance being used is already running. Aborting.\");\n+    } else {\n+      if (!initialized) {\n+        Runtime.getRuntime().addShutdownHook(new Thread() {\n+          @Override\n+          public void run() {\n+            try {\n+              MiniAccumuloClusterImpl.this.stop();\n+            } catch (IOException e) {\n+              e.printStackTrace();\n+            } catch (InterruptedException e) {\n+              e.printStackTrace();\n+            }\n+          }\n+        });\n+      }\n+\n+      if (zooKeeperProcess == null) {\n+        zooKeeperProcess = _exec(ZooKeeperServerMain.class, ServerType.ZOOKEEPER, zooCfgFile.getAbsolutePath());\n+      }\n \n-    if (!initialized) {\n-      // sleep a little bit to let zookeeper come up before calling init, seems to work better\n-      long startTime = System.currentTimeMillis();\n-      while (true) {\n-        Socket s = null;\n-        try {\n-          s = new Socket(\"localhost\", config.getZooKeeperPort());\n-          s.getOutputStream().write(\"ruok\\n\".getBytes());\n-          s.getOutputStream().flush();\n-          byte buffer[] = new byte[100];\n-          int n = s.getInputStream().read(buffer);\n-          if (n >= 4 && new String(buffer, 0, 4).equals(\"imok\"))\n-            break;\n-        } catch (Exception e) {\n-          if (System.currentTimeMillis() - startTime >= config.getZooKeeperStartupTime()) {\n-            throw new ZooKeeperBindException(\"Zookeeper did not start within \" + (config.getZooKeeperStartupTime() / 1000) + \" seconds. Check the logs in \"\n-                + config.getLogDir() + \" for errors.  Last exception: \" + e);\n+      if (!initialized) {\n+        // sleep a little bit to let zookeeper come up before calling init, seems to work better\n+        long startTime = System.currentTimeMillis();\n+        while (true) {\n+          Socket s = null;\n+          try {\n+            s = new Socket(\"localhost\", config.getZooKeeperPort());\n+            s.getOutputStream().write(\"ruok\\n\".getBytes());\n+            s.getOutputStream().flush();\n+            byte buffer[] = new byte[100];\n+            int n = s.getInputStream().read(buffer);\n+            if (n >= 4 && new String(buffer, 0, 4).equals(\"imok\"))\n+              break;\n+          } catch (Exception e) {\n+            if (System.currentTimeMillis() - startTime >= config.getZooKeeperStartupTime()) {\n+              throw new ZooKeeperBindException(\"Zookeeper did not start within \" + (config.getZooKeeperStartupTime() / 1000) + \" seconds. Check the logs in \"\n+                  + config.getLogDir() + \" for errors.  Last exception: \" + e);\n+            }\n+          } finally {\n+            if (s != null)\n+              s.close();\n           }\n-          UtilWaitThread.sleep(250);\n-        } finally {\n-          if (s != null)\n-            s.close();\n         }\n+        Process initProcess = exec(Initialize.class, \"--instance-name\", config.getInstanceName(), \"--password\", config.getRootPassword());\n+        int ret = initProcess.waitFor();\n+        if (ret != 0) {\n+          throw new RuntimeException(\"Initialize process returned \" + ret + \". Check the logs in \" + config.getLogDir() + \" for errors.\");\n+        }\n+        initialized = true;\n       }\n-      Process initProcess = exec(Initialize.class, \"--instance-name\", config.getInstanceName(), \"--password\", config.getRootPassword());\n-      int ret = initProcess.waitFor();\n-      if (ret != 0) {\n-        throw new RuntimeException(\"Initialize process returned \" + ret + \". Check the logs in \" + config.getLogDir() + \" for errors.\");\n-      }\n-      initialized = true;\n     }\n+    \n+    log.info(\"Starting MAC against instance {} and zookeeper(s) {}.\", config.getInstanceName(), config.getZooKeepers());\n+    \n     synchronized (tabletServerProcesses) {\n       for (int i = tabletServerProcesses.size(); i < config.getNumTservers(); i++) {\n         tabletServerProcesses.add(_exec(TabletServer.class, ServerType.TABLET_SERVER));\n@@ -744,11 +812,11 @@ protected ExecutorService getShutdownExecutor() {\n \n   private int stopProcessWithTimeout(final Process proc, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {\n     FutureTask<Integer> future = new FutureTask<Integer>(new Callable<Integer>() {\n-        @Override\n-        public Integer call() throws InterruptedException {\n-          proc.destroy();\n-          return proc.waitFor();\n-        }\n+      @Override\n+      public Integer call() throws InterruptedException {\n+        proc.destroy();\n+        return proc.waitFor();\n+      }\n     });\n \n     executor.execute(future);\n@@ -757,9 +825,9 @@ public Integer call() throws InterruptedException {\n   }\n \n   /**\n-   * Get programmatic interface to information available in a normal monitor.\n-   * XXX the returned structure won't contain information about the metadata table until there is data in it.\n-   * e.g. if you want to see the metadata table you should create a table.\n+   * Get programmatic interface to information available in a normal monitor. XXX the returned structure won't contain information about the metadata table\n+   * until there is data in it. e.g. if you want to see the metadata table you should create a table.\n+   * \n    * @since 1.6.1\n    */\n   public MasterMonitorInfo getMasterMonitorInfo() throws AccumuloException, AccumuloSecurityException {",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloClusterImpl.java",
                "sha": "d5dc1e9704efc7f509c19f83dc9dd2bcb4044be3",
                "status": "modified"
            },
            {
                "additions": 132,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java",
                "changes": 145,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 13,
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java",
                "patch": "@@ -18,13 +18,15 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.MalformedURLException;\n import java.util.HashMap;\n import java.util.Iterator;\n import java.util.Map;\n import java.util.Map.Entry;\n \n import org.apache.accumulo.cluster.AccumuloConfig;\n import org.apache.accumulo.core.conf.CredentialProviderFactoryShim;\n+import org.apache.accumulo.core.conf.DefaultConfiguration;\n import org.apache.accumulo.core.conf.Property;\n import org.apache.accumulo.minicluster.MemoryUnit;\n import org.apache.accumulo.minicluster.ServerType;\n@@ -55,18 +57,20 @@\n   private File libDir;\n   private File libExtDir;\n   private File confDir;\n+  private File hadoopConfDir = null;\n   private File zooKeeperDir;\n   private File accumuloDir;\n   private File logDir;\n   private File walogDir;\n \n   private int zooKeeperPort = 0;\n   private int configuredZooKeeperPort = 0;\n-  private long zooKeeperStartupTime = 20*1000;\n+  private long zooKeeperStartupTime = 20 * 1000;\n \n   private long defaultMemorySize = 128 * 1024 * 1024;\n \n   private boolean initialized = false;\n+  private Boolean existingInstance = null;\n \n   private boolean useMiniDFS = false;\n \n@@ -76,6 +80,10 @@\n \n   private String[] nativePathItems = null;\n \n+  // These are only used on top of existing instances\n+  private Configuration hadoopConf;\n+  private Configuration accumuloConf;\n+\n   /**\n    * @param dir\n    *          An empty or nonexistant directory that Accumulo and Zookeeper can store data in. Creating the directory is left to the user. Java 7, Guava, and\n@@ -109,18 +117,22 @@ MiniAccumuloConfigImpl initialize() {\n       logDir = new File(dir, \"logs\");\n       walogDir = new File(dir, \"walogs\");\n \n-      // TODO ACCUMULO-XXXX replace usage of instance.dfs.{dir,uri} with instance.volumes\n-      setInstanceLocation();\n+      // Never want to override these if an existing instance, which may be using the defaults\n+      if (existingInstance == null || !existingInstance) {\n+        existingInstance = false;\n+        // TODO ACCUMULO-XXXX replace usage of instance.dfs.{dir,uri} with instance.volumes\n+        setInstanceLocation();\n+        mergeProp(Property.INSTANCE_SECRET.getKey(), DEFAULT_INSTANCE_SECRET);\n+        mergeProp(Property.LOGGER_DIR.getKey(), walogDir.getAbsolutePath());\n+        mergeProp(Property.TRACE_TOKEN_PROPERTY_PREFIX.getKey() + \"password\", getRootPassword());\n+      }\n \n-      mergeProp(Property.INSTANCE_SECRET.getKey(), DEFAULT_INSTANCE_SECRET);\n       mergeProp(Property.TSERV_PORTSEARCH.getKey(), \"true\");\n-      mergeProp(Property.LOGGER_DIR.getKey(), walogDir.getAbsolutePath());\n       mergeProp(Property.TSERV_DATACACHE_SIZE.getKey(), \"10M\");\n       mergeProp(Property.TSERV_INDEXCACHE_SIZE.getKey(), \"10M\");\n       mergeProp(Property.TSERV_MAXMEM.getKey(), \"50M\");\n       mergeProp(Property.TSERV_WALOG_MAX_SIZE.getKey(), \"100M\");\n       mergeProp(Property.TSERV_NATIVEMAP_ENABLED.getKey(), \"false\");\n-      mergeProp(Property.TRACE_TOKEN_PROPERTY_PREFIX.getKey() + \"password\", getRootPassword());\n       // since there is a small amount of memory, check more frequently for majc... setting may not be needed in 1.5\n       mergeProp(Property.TSERV_MAJC_DELAY.getKey(), \"3\");\n       mergeProp(Property.GENERAL_CLASSPATHS.getKey(), libDir.getAbsolutePath() + \"/[^.].*[.]jar\");\n@@ -138,10 +150,13 @@ MiniAccumuloConfigImpl initialize() {\n         updateConfigForCredentialProvider();\n       }\n \n-      // zookeeper port should be set explicitly in this class, not just on the site config\n-      if (zooKeeperPort == 0)\n-        zooKeeperPort = PortUtils.getRandomFreePort();\n-      siteConfig.put(Property.INSTANCE_ZK_HOST.getKey(), \"localhost:\" + zooKeeperPort);\n+      if (existingInstance == null || !existingInstance) {\n+        existingInstance = false;\n+        // zookeeper port should be set explicitly in this class, not just on the site config\n+        if (zooKeeperPort == 0)\n+          zooKeeperPort = PortUtils.getRandomFreePort();\n+        siteConfig.put(Property.INSTANCE_ZK_HOST.getKey(), \"localhost:\" + zooKeeperPort);\n+      }\n       initialized = true;\n     }\n     return this;\n@@ -244,6 +259,15 @@ public MiniAccumuloConfigImpl setInstanceName(String instanceName) {\n    */\n   @Override\n   public MiniAccumuloConfigImpl setSiteConfig(Map<String,String> siteConfig) {\n+    if (existingInstance != null && existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set set config info when using an existing instance.\");\n+\n+    this.existingInstance = Boolean.FALSE;\n+\n+    return _setSiteConfig(siteConfig);\n+  }\n+\n+  private MiniAccumuloConfigImpl _setSiteConfig(Map<String,String> siteConfig) {\n     this.siteConfig = new HashMap<String,String>(siteConfig);\n     this.configuredSiteConig = new HashMap<String,String>(siteConfig);\n     return this;\n@@ -259,14 +283,19 @@ public MiniAccumuloConfigImpl setSiteConfig(Map<String,String> siteConfig) {\n    */\n   @Override\n   public MiniAccumuloConfigImpl setZooKeeperPort(int zooKeeperPort) {\n+    if (existingInstance != null && existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set zookeeper info when using an existing instance.\");\n+\n+    this.existingInstance = Boolean.FALSE;\n+\n     this.configuredZooKeeperPort = zooKeeperPort;\n     this.zooKeeperPort = zooKeeperPort;\n     return this;\n   }\n \n   /**\n-   * Configure the time to wait for ZooKeeper to startup.\n-   * Calling this method is optional. The default is 20000 milliseconds\n+   * <<<<<<< HEAD Configure the time to wait for ZooKeeper to startup. Calling this method is optional. The default is 20000 milliseconds ======= Configure the\n+   * time to wait for ZooKeeper to startup. Calling this method is optional. The default is 20000 milliseconds >>>>>>> ACCUMULO-2984\n    *\n    * @param zooKeeperStartupTime\n    *          Time to wait for ZooKeeper to startup, in milliseconds\n@@ -275,6 +304,11 @@ public MiniAccumuloConfigImpl setZooKeeperPort(int zooKeeperPort) {\n    */\n   @Override\n   public MiniAccumuloConfigImpl setZooKeeperStartupTime(long zooKeeperStartupTime) {\n+    if (existingInstance != null && existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set zookeeper info when using an existing instance.\");\n+\n+    this.existingInstance = Boolean.FALSE;\n+\n     this.zooKeeperStartupTime = zooKeeperStartupTime;\n     return this;\n   }\n@@ -557,7 +591,8 @@ public boolean isUseCredentialProvider() {\n   }\n \n   /**\n-   * @param useCredentialProvider the useCredentialProvider to set\n+   * @param useCredentialProvider\n+   *          the useCredentialProvider to set\n    */\n   public void setUseCredentialProvider(boolean useCredentialProvider) {\n     this.useCredentialProvider = useCredentialProvider;\n@@ -567,4 +602,88 @@ public void setUseCredentialProvider(boolean useCredentialProvider) {\n   public MiniAccumuloClusterImpl build() throws IOException {\n     return new MiniAccumuloClusterImpl(this);\n   }\n+\n+  /**\n+   * Informs MAC that it's running against an existing accumulo instance. It is assumed that it's already initialized and hdfs/zookeeper are already running.\n+   *\n+   * @param accumuloSite\n+   *          a File representation of the accumulo-site.xml file for the instance being run\n+   * @param hadoopConfDir\n+   *          a File representation of the hadoop configuration directory containing core-site.xml and hdfs-site.xml\n+   * \n+   * @return MiniAccumuloConfigImpl which uses an existing accumulo configuration\n+   *\n+   * @since 1.6.2\n+   *\n+   * @throws IOException\n+   *           when there are issues converting the provided Files to URLs\n+   */\n+  public MiniAccumuloConfigImpl useExistingInstance(File accumuloSite, File hadoopConfDir) throws IOException {\n+    if (existingInstance != null && !existingInstance.booleanValue())\n+      throw new UnsupportedOperationException(\"Cannot set to useExistingInstance after specifying config/zookeeper\");\n+\n+    this.existingInstance = Boolean.TRUE;\n+\n+    System.setProperty(\"org.apache.accumulo.config.file\", \"accumulo-site.xml\");\n+    this.hadoopConfDir = hadoopConfDir;\n+    hadoopConf = new Configuration(false);\n+    accumuloConf = new Configuration(false);\n+    File coreSite = new File(hadoopConfDir, \"core-site.xml\");\n+    File hdfsSite = new File(hadoopConfDir, \"hdfs-site.xml\");\n+\n+    try {\n+      accumuloConf.addResource(accumuloSite.toURI().toURL());\n+      hadoopConf.addResource(coreSite.toURI().toURL());\n+      hadoopConf.addResource(hdfsSite.toURI().toURL());\n+    } catch (MalformedURLException e1) {\n+      throw e1;\n+    }\n+\n+    Map<String,String> siteConfigMap = new HashMap<String,String>();\n+    for (Entry<String,String> e : accumuloConf) {\n+      siteConfigMap.put(e.getKey(), e.getValue());\n+    }\n+    _setSiteConfig(siteConfigMap);\n+    \n+    for (Entry<String,String> entry : DefaultConfiguration.getDefaultConfiguration())\n+      accumuloConf.setIfUnset(entry.getKey(), entry.getValue());\n+\n+    return this;\n+  }\n+\n+  /**\n+   * @return MAC should run assuming it's configured for an initialized accumulo instance\n+   *\n+   * @since 1.6.2\n+   */\n+  public boolean useExistingInstance() {\n+    return existingInstance != null && existingInstance;\n+  }\n+\n+  /**\n+   * @return hadoop configuration directory being used\n+   *\n+   * @since 1.6.2\n+   */\n+  public File getHadoopConfDir() {\n+    return this.hadoopConfDir;\n+  }\n+\n+  /**\n+   * @return accumulo Configuration being used\n+   * \n+   * @since 1.6.2\n+   */\n+  public Configuration getAccumuloConfiguration() {\n+    return accumuloConf;\n+  }\n+\n+  /**\n+   * @return hadoop Configuration being used\n+   * \n+   * @since 1.6.2\n+   */\n+  public Configuration getHadoopConfiguration() {\n+    return hadoopConf;\n+  }\n }",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/minicluster/src/main/java/org/apache/accumulo/minicluster/impl/MiniAccumuloConfigImpl.java",
                "sha": "2d7103e200978e7a68875f2c5acdcc58f11ce98b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 1,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "patch": "@@ -98,5 +98,4 @@ public synchronized AccumuloConfiguration getConfiguration() {\n   public Instance getInstance() {\n     return scf.getInstance();\n   }\n-\n }",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/conf/ServerConfiguration.java",
                "sha": "904f4839b39b7571815e379f1f5717a1132aefb9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 2,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java",
                "patch": "@@ -391,11 +391,14 @@ public static VolumeManager get() throws IOException {\n   static private final String DEFAULT = \"\";\n \n   public static VolumeManager get(AccumuloConfiguration conf) throws IOException {\n+    return get(conf, CachedConfiguration.getInstance());\n+  }\n+\n+  public static VolumeManager get(AccumuloConfiguration conf, final Configuration hadoopConf) throws IOException {\n     final Map<String,Volume> volumes = new HashMap<String,Volume>();\n-    final Configuration hadoopConf = CachedConfiguration.getInstance();\n \n     // The \"default\" Volume for Accumulo (in case no volumes are specified)\n-    for (String volumeUriOrDir : VolumeConfiguration.getVolumeUris(conf)) {\n+    for (String volumeUriOrDir : VolumeConfiguration.getVolumeUris(conf, hadoopConf)) {\n       if (volumeUriOrDir.equals(DEFAULT))\n         throw new IllegalArgumentException(\"Cannot re-define the default volume\");\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/fs/VolumeManagerImpl.java",
                "sha": "54d7e2a3f5af285f3ad99d19e340b99b990a0dd6",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 0,
                "filename": "server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java",
                "patch": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.server.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.accumulo.core.Constants;\n+import org.apache.accumulo.core.zookeeper.ZooUtil;\n+import org.apache.accumulo.fate.zookeeper.IZooReader;\n+import org.apache.accumulo.server.client.HdfsZooInstance;\n+import org.apache.zookeeper.KeeperException;\n+\n+public class AccumuloStatus {\n+  /**\n+   * Determines if there could be an accumulo instance running via zookeeper lock checking\n+   *\n+   * @param reader\n+   *\n+   * @return true iff all servers show no indication of being registered in zookeeper, otherwise false\n+   * @throws IOException\n+   *           if there are issues connecting to ZooKeeper to determine service status\n+   */\n+  public static boolean isAccumuloOffline(IZooReader reader) throws IOException {\n+    String rootPath = ZooUtil.getRoot(HdfsZooInstance.getInstance());\n+    return isAccumuloOffline(reader, rootPath);\n+  }\n+\n+  /**\n+   * Determines if there could be an accumulo instance running via zookeeper lock checking\n+   *\n+   * @param reader\n+   * @param rootPath\n+   *\n+   * @return true iff all servers show no indication of being registered in zookeeper, otherwise false\n+   * @throws IOException\n+   *           if there are issues connecting to ZooKeeper to determine service status\n+   */\n+  public static boolean isAccumuloOffline(IZooReader reader, String rootPath) throws IOException {\n+    try {\n+      for (String child : reader.getChildren(rootPath + Constants.ZTSERVERS)) {\n+        if (!reader.getChildren(rootPath + Constants.ZTSERVERS + \"/\" + child).isEmpty())\n+          return false;\n+      }\n+      if (!reader.getChildren(rootPath + Constants.ZTRACERS).isEmpty())\n+        return false;\n+      if (!reader.getChildren(rootPath + Constants.ZMASTER_LOCK).isEmpty())\n+        return false;\n+      if (!reader.getChildren(rootPath + Constants.ZMONITOR_LOCK).isEmpty())\n+        return false;\n+      if (!reader.getChildren(rootPath + Constants.ZGC_LOCK).isEmpty())\n+        return false;\n+    } catch (KeeperException e) {\n+      throw new IOException(\"Issues contacting ZooKeeper to get Accumulo status.\", e);\n+    } catch (InterruptedException e) {\n+      throw new IOException(\"Issues contacting ZooKeeper to get Accumulo status.\", e);\n+    }\n+    return true;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/server/base/src/main/java/org/apache/accumulo/server/util/AccumuloStatus.java",
                "sha": "7e1cc9714f5fdd0fec118cb2be95fdb9036a248d",
                "status": "added"
            },
            {
                "additions": 152,
                "blob_url": "https://github.com/apache/accumulo/blob/e3a743cb445723a3d5664a4bf1ebf37833152aae/test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java?ref=e3a743cb445723a3d5664a4bf1ebf37833152aae",
                "deletions": 0,
                "filename": "test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java",
                "patch": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.accumulo.test;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.Collection;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+\n+import org.apache.accumulo.core.client.BatchWriter;\n+import org.apache.accumulo.core.client.BatchWriterConfig;\n+import org.apache.accumulo.core.client.Connector;\n+import org.apache.accumulo.core.client.Scanner;\n+import org.apache.accumulo.core.conf.Property;\n+import org.apache.accumulo.core.data.Key;\n+import org.apache.accumulo.core.data.Mutation;\n+import org.apache.accumulo.core.data.Value;\n+import org.apache.accumulo.core.metadata.MetadataTable;\n+import org.apache.accumulo.core.metadata.RootTable;\n+import org.apache.accumulo.core.security.Authorizations;\n+import org.apache.accumulo.core.util.UtilWaitThread;\n+import org.apache.accumulo.minicluster.MiniAccumuloCluster;\n+import org.apache.accumulo.minicluster.MiniAccumuloConfig;\n+import org.apache.accumulo.minicluster.ServerType;\n+import org.apache.accumulo.minicluster.impl.MiniAccumuloConfigImpl;\n+import org.apache.accumulo.minicluster.impl.ProcessReference;\n+import org.apache.accumulo.test.functional.ConfigurableMacIT;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class ExistingMacIT extends ConfigurableMacIT {\n+  @Override\n+  public void configure(MiniAccumuloConfigImpl cfg, Configuration hadoopCoreSite) {\n+    cfg.setProperty(Property.INSTANCE_ZK_TIMEOUT, \"5s\");\n+\n+    // use raw local file system so walogs sync and flush will work\n+    hadoopCoreSite.set(\"fs.file.impl\", RawLocalFileSystem.class.getName());\n+  }\n+\n+  private void createEmptyConfig(File confFile) throws IOException {\n+    Configuration conf = new Configuration(false);\n+    OutputStream hcOut = new FileOutputStream(confFile);\n+    conf.writeXml(hcOut);\n+    hcOut.close();\n+  }\n+\n+  @Test\n+  public void testExistingInstance() throws Exception {\n+\n+    Connector conn = getCluster().getConnector(\"root\", ROOT_PASSWORD);\n+\n+    conn.tableOperations().create(\"table1\");\n+\n+    BatchWriter bw = conn.createBatchWriter(\"table1\", new BatchWriterConfig());\n+\n+    Mutation m1 = new Mutation(\"00081\");\n+    m1.put(\"math\", \"sqroot\", \"9\");\n+    m1.put(\"math\", \"sq\", \"6560\");\n+\n+    bw.addMutation(m1);\n+    bw.close();\n+\n+    conn.tableOperations().flush(\"table1\", null, null, true);\n+    // TOOD use constants\n+    conn.tableOperations().flush(MetadataTable.NAME, null, null, true);\n+    conn.tableOperations().flush(RootTable.NAME, null, null, true);\n+\n+    Set<Entry<ServerType,Collection<ProcessReference>>> procs = getCluster().getProcesses().entrySet();\n+    for (Entry<ServerType,Collection<ProcessReference>> entry : procs) {\n+      if (entry.getKey() == ServerType.ZOOKEEPER)\n+        continue;\n+      for (ProcessReference pr : entry.getValue())\n+        getCluster().killProcess(entry.getKey(), pr);\n+    }\n+\n+    // TODO clean out zookeeper? following sleep waits for ephemeral nodes to go away\n+    UtilWaitThread.sleep(10000);\n+\n+    File hadoopConfDir = createTestDir(ExistingMacIT.class.getSimpleName() + \"_hadoop_conf\");\n+    FileUtils.deleteQuietly(hadoopConfDir);\n+    hadoopConfDir.mkdirs();\n+    createEmptyConfig(new File(hadoopConfDir, \"core-site.xml\"));\n+    createEmptyConfig(new File(hadoopConfDir, \"hdfs-site.xml\"));\n+\n+    File testDir2 = createTestDir(ExistingMacIT.class.getSimpleName() + \"_2\");\n+    FileUtils.deleteQuietly(testDir2);\n+\n+    MiniAccumuloConfig macConfig2 = new MiniAccumuloConfig(testDir2, \"notused\");\n+    macConfig2.useExistingInstance(new File(getCluster().getConfig().getConfDir(), \"accumulo-site.xml\"), hadoopConfDir);\n+\n+    MiniAccumuloCluster accumulo2 = new MiniAccumuloCluster(macConfig2);\n+    accumulo2.start();\n+\n+    conn = accumulo2.getConnector(\"root\", ROOT_PASSWORD);\n+\n+    Scanner scanner = conn.createScanner(\"table1\", Authorizations.EMPTY);\n+\n+    int sum = 0;\n+    for (Entry<Key,Value> entry : scanner) {\n+      sum += Integer.parseInt(entry.getValue().toString());\n+    }\n+\n+    Assert.assertEquals(6569, sum);\n+\n+    accumulo2.stop();\n+  }\n+\n+  @Test\n+  public void testExistingRunningInstance() throws Exception {\n+    File hadoopConfDir = createTestDir(ExistingMacIT.class.getSimpleName() + \"_hadoop_conf_2\");\n+    FileUtils.deleteQuietly(hadoopConfDir);\n+    hadoopConfDir.mkdirs();\n+    createEmptyConfig(new File(hadoopConfDir, \"core-site.xml\"));\n+    createEmptyConfig(new File(hadoopConfDir, \"hdfs-site.xml\"));\n+\n+    File testDir2 = createTestDir(ExistingMacIT.class.getSimpleName() + \"_3\");\n+    FileUtils.deleteQuietly(testDir2);\n+\n+    MiniAccumuloConfig macConfig2 = new MiniAccumuloConfig(testDir2, \"notused\");\n+    macConfig2.useExistingInstance(new File(getCluster().getConfig().getConfDir(), \"accumulo-site.xml\"), hadoopConfDir);\n+\n+    System.out.println(\"conf \" + new File(getCluster().getConfig().getConfDir(), \"accumulo-site.xml\"));\n+\n+    MiniAccumuloCluster accumulo2 = new MiniAccumuloCluster(macConfig2);\n+    try {\n+      accumulo2.start();\n+      Assert.fail();\n+    } catch (RuntimeException e) {\n+      // TODO check message or throw more explicit exception\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/accumulo/raw/e3a743cb445723a3d5664a4bf1ebf37833152aae/test/src/test/java/org/apache/accumulo/test/ExistingMacIT.java",
                "sha": "5d1978e1f27a4637bd432edf289a54ceb57774c8",
                "status": "added"
            }
        ],
        "message": "ACCUMULO-2984 adding ability to run MAC against a permanent accumulo instance\n\nAddressing NPEs with existingInstance\n\nACCUMULO-2984 add test for existing mac using existing instance\n\nFixing my NPE fix and integrating Keith's test\n\nAdding ability to clean out singleton stuff\n\nNow no longer doing weird things with statics\n\nWarning cleanup\n\nRemoving added features no longer needed\n\nCleaning up minor ticket items\n\nFixing another Configuration plumbing issue and adding some info statements",
        "parent": "https://github.com/apache/accumulo/commit/758a364bda0b47520daf251d88e0057187884d66",
        "repo": "accumulo",
        "unit_tests": [
            "MiniAccumuloClusterImplTest.java",
            "MiniAccumuloConfigImplTest.java",
            "VolumeManagerImplTest.java"
        ]
    },
    "accumulo_fb2c0c7": {
        "bug_id": "accumulo_fb2c0c7",
        "commit": "https://github.com/apache/accumulo/commit/fb2c0c75141177da429bc9094d6d5b15ba12f05e",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/accumulo/blob/fb2c0c75141177da429bc9094d6d5b15ba12f05e/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java?ref=fb2c0c75141177da429bc9094d6d5b15ba12f05e",
                "deletions": 5,
                "filename": "minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java",
                "patch": "@@ -244,11 +244,14 @@ public MiniAccumuloCluster(MiniAccumuloConfig config) throws IOException {\n \n     File nativeMap = new File(config.getLibDir().getAbsolutePath() + \"/native/map\");\n     nativeMap.mkdirs();\n-    String testRoot = new File(new File(System.getProperty(\"user.dir\")).getParent() + \"/server/src/main/c++/nativeMap\").getAbsolutePath();\n-    for (String file : new File(testRoot).list()) {\n-      File src = new File(testRoot, file);\n-      if (src.isFile() && file.startsWith(\"libNativeMap\"))\n-        FileUtils.copyFile(src, new File(nativeMap, file));\n+    File testRoot = new File(new File(new File(System.getProperty(\"user.dir\")).getParent() + \"/server/src/main/c++/nativeMap\").getAbsolutePath());\n+    \n+    if (testRoot.exists()) {\n+      for (String file : testRoot.list()) {\n+        File src = new File(testRoot, file);\n+        if (src.isFile() && file.startsWith(\"libNativeMap\"))\n+          FileUtils.copyFile(src, new File(nativeMap, file));\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/accumulo/raw/fb2c0c75141177da429bc9094d6d5b15ba12f05e/minicluster/src/main/java/org/apache/accumulo/minicluster/MiniAccumuloCluster.java",
                "sha": "c492e1b0ead9a0339987a788d187f1be7c1b1c30",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-1537 fixed NPE in MAC\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1502381 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/accumulo/commit/720e27a59a3a12ca70ee20d85eaf7c6afab83429",
        "repo": "accumulo",
        "unit_tests": [
            "MiniAccumuloClusterTest.java"
        ]
    },
    "accumulo_fc34457": {
        "bug_id": "accumulo_fc34457",
        "commit": "https://github.com/apache/accumulo/commit/fc34457d480a03b5df064ee0b26db34b8337a37e",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/accumulo/blob/fc34457d480a03b5df064ee0b26db34b8337a37e/core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/accumulo/contents/core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java?ref=fc34457d480a03b5df064ee0b26db34b8337a37e",
                "deletions": 1,
                "filename": "core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java",
                "patch": "@@ -17,6 +17,7 @@\n package org.apache.accumulo.core.util.shell;\n \n import java.io.File;\n+import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n \n@@ -80,7 +81,7 @@ public AuthenticationToken convert(String value) {\n   private boolean hdfsZooInstance;\n   \n   @Parameter(names = {\"-z\", \"--zooKeeperInstance\"}, description = \"use a zookeeper instance with the given instance name and list of zoo hosts\", arity = 2)\n-  private List<String> zooKeeperInstance;\n+  private List<String> zooKeeperInstance = new ArrayList<String>();\n   \n   @Parameter(names = \"--auth-timeout\", description = \"minutes the shell can be idle without re-entering a password\")\n   private int authTimeout = 60; // TODO Add validator for positive number",
                "raw_url": "https://github.com/apache/accumulo/raw/fc34457d480a03b5df064ee0b26db34b8337a37e/core/src/main/java/org/apache/accumulo/core/util/shell/ShellOptionsJC.java",
                "sha": "33b3eacd06849ad2aa91098d1afa3756fce4fa39",
                "status": "modified"
            }
        ],
        "message": "ACCUMULO-1478 prevent NPE when starting the shell\n\ngit-svn-id: https://svn.apache.org/repos/asf/accumulo/trunk@1490335 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/accumulo/commit/98261a88635f6e481cd439843e4ffd310b57e486",
        "repo": "accumulo",
        "unit_tests": [
            "ShellOptionsJCTest.java"
        ]
    }
}